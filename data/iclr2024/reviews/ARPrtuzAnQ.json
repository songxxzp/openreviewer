[
    {
        "id": "CcXKfWf3zW",
        "forum": "ARPrtuzAnQ",
        "replyto": "ARPrtuzAnQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2889/Reviewer_kFXh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2889/Reviewer_kFXh"
        ],
        "content": {
            "summary": {
                "value": "The authors study the computational hardness of learning equivariant networks using gradient descent.  They show that enforcing symmetries like permutation invariance does not make learning any substantially easier, and that their hardness results hold even for shallow 1-layer GNNs and CNNs.  They provide statistical query (SQ) lower bounds that scale exponentially with feature dimensions for various architectures.  Additionally, the authors provide an efficient non-gradient based algorithm for learning sparse invariant polynomials, separating SQ and correlational SQ complexity.  Lastly, they perform numerous experiments to verify their results."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "Originality:\n\nThe authors prove numerous new results on the sample complexity of learning in neural networks, and provide ample empirical support for their work.\n\nQuality/clarity:\n\nThe authors sketch their proofs using careful, clear technical arguments.  Additionally, their experiments are simple, but clear demonstrations of the practical difficulty of learning networks within the families they authors study.\n\nSignificance:\n\nThe author's work significantly advances progress on the hardness of learning symmetric networks, opening the door to clear avenues of future, follow-up work."
            },
            "weaknesses": {
                "value": "I would've liked a _slightly_ more thorough empirical treatment, if only to make sure that the failure to learn was not due to poor hyperparameter choices / poor initialization etc."
            },
            "questions": {
                "value": "Could the authors comment more on the applicability of \"worst-case\" reasoning re: the likelihood of these function classes to well-describe nature?  It seems plausible that the worst case could be significantly harder than the typical case for problems that we care about.  In practice, these sorts of hardness results don't seem to impact practitioner's usage of these model classes much at all!"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2889/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698443844685,
        "cdate": 1698443844685,
        "tmdate": 1699636232281,
        "mdate": 1699636232281,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6zP8ThkuVD",
        "forum": "ARPrtuzAnQ",
        "replyto": "ARPrtuzAnQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2889/Reviewer_Fgas"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2889/Reviewer_Fgas"
        ],
        "content": {
            "summary": {
                "value": "This work considers the problem of learning symmetric Neural Networks.  The\nauthors provide hardness results for Statistical Query (SQ) and Correlational\nStatistical Query (CSQ) algorithms (and an NP-Hardness result for properly\nlearning Graph Neural Networks (GNNs)).  An example of a symmetric neural\nnetwork is a 2-layer GNN that maps an input graph $A$ to $g(f(A))$, where\n$f:\\{0, 1\\}^{n \\times n} \\mapsto R^k$ first aggregates $k$ permutation\ninvariant features of the input graph $A$ and $g$ is a one-hidden layer MLP.\n\nThe first result is an SQ hardness result for two-layer GNNs showing that for\nthe above class of GNNs $\\tau^2 2^{n^{\\Omega(1)}}$ queries of tolerance $\\tau$\nare required.  The result follows by designing a 2-layer GNN where the\n$i$-output of the first layer counts how many nodes have $i-1$ outgoing edges\nand the second layer selects a subset of those counts and computes its parity.\nBy using properties of GNP graphs, the authors reduce the problem to the\nwell-known hard problem of learning parity functions over the uniform\ndistribution on the $n$-dimensional Boolean hypercube.\n\nThe second result considers GNNs that take as input a $n \\times d$ feature\nmatrix $X$ and then compute $1_n^T \\sigma(A(G) X W) a$, for an adjacency matrix\n$A(G) \\in \\{0,1\\}^n$,a weight $d \\times 2 k$ matrix $W$ and a $2k$-dimensional\nweight vector $a$.  They give a $d^k$ CSQ lower bound for this problem.  This\nresult follows from adapting the hard instances of the CSQ lower bound\nconstruction of [2].\n\nThe third result shows that for CNNs (and more general frame-averaged networks)\nof the form $f(X) = 1/|G| \\sum_{g \\in G} a^T \\sigma (W^T g^{-1} X) 1_d$ where\n$X$ is a $n \\times d$ input matrix, $G$ is a group acting on $R^n$ (e.g., could\nbe cyclic shifts) either requires $2^{n^{\\Omega(1)}}/ |G|^2$ queries or a query\nwith precision $|G| 2^{-n^{\\Omega(1)}} + \\sqrt{|G|} n^{-\\Omega(k)}$.  The proof\nof this results also adapts the construction of [2] For more general\nframe-averaged networks, the authors use the techniques developed in [1] to\nshow a super-polynomial CSQ lower-bound (for any constant c either $n^{\\log n}$\nqueries are needed or a query with accuracy $n^{-c}$).\n\n\n[1] Surbhi Goel, Aravind Gollakota, Zhihan Jin, Sushrut Karmalkar, and Adam Klivans. Superpolynomial lower bounds for learning one-layer neural networks using gradient descent.\nICML 2020.\n\n[2] Ilias Diakonikolas, Daniel M Kane, Vasilis Kontonis, and Nikos Zarifis. Algorithms and sq lower bounds for pac learning one-hidden-layer relu networks. COLT 2020."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The problem considered in this work is interesting and well-motivated. Most theoretical prior works on learning neural networks focused on fully connected shallow networks; investigating the learnability of popular and practically relevant classes of neural networks such as GNNs and CNNs (that have more restricted symmetric structure) is a natural\nnext step.\n\n2. The paper provides hardness results for various classes of ``symmetric'' neural networks in the SQ and CSQ models that are general models of computation capturing, for example, stochastic gradient descent algorithms.\n\n3. I found the paper to be well-organized and written. The authors clearly state what results of prior works they rely on to get their results."
            },
            "weaknesses": {
                "value": "1. The novelty of the technics and arguments used in the lower bounds provided in this work may be limited in the sense that most of the claimed results rely heavily on machinery developed in the prior works [1,2]."
            },
            "questions": {
                "value": "1. See weaknesses. \n\n2. While the authors clearly state which lemmas and proofs of the prior works they are using, I think a more detailed high-level explanation of the arguments and the differences from prior work should appear in the main body of the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2889/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2889/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2889/Reviewer_Fgas"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2889/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1700039826806,
        "cdate": 1700039826806,
        "tmdate": 1700039826806,
        "mdate": 1700039826806,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xvgzdc1Rz7",
        "forum": "ARPrtuzAnQ",
        "replyto": "ARPrtuzAnQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2889/Reviewer_aiE7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2889/Reviewer_aiE7"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the hardness of learning certain two-layer or one-hidden-layer neural networks under symmetrized architecture/algorithmic designs on Gaussian inputs, via the Statistical Query (SQ) lower bound techniques. It provides several results characterizing the hardness of learning GNNs and CNNs via leveraging correlational SQ (CSQ) lower bounds for learning boolean functions and by connecting them with learning parity functions. It also discussed when CSQ lower bounds can be different than SQ lower bounds."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The question studied in this paper is closely related to a core question in understanding deep learning, that is: can deep learning benefit from symmetry-inspired algorithmic designs? In this sense I deem the question studied in the paper valuable and this paper's attempt to deal with it respectful.\n2. The technical contribution of this paper, although still depended on some prior works, is novel enough to my understanding to be nontrivial. This paper constructed function classes that were not studied before to specifically deal with their problems, and proved hardness of learning these classes, which is a notable effort. \n3. This paper covers both GNN and CNNs, and discussed the difference between CSQ and SQ in certain scenarios, which is good for completeness."
            },
            "weaknesses": {
                "value": "The weaknesses listed below are, in my opinion, secondary to the contributions of this paper.\nThe approach of this paper in studying the hardness of learning symmetry-enhanced neural networks has certain limitations. It cannot account for all neural architectures at once and requires specific construction whenever the problem formulation changes by a little bit. And the hard function classes, although are well designed for the proof, are not very intuitive in terms of broader impact to people who are outside of the learning theoretic community. Perhaps there could be more illustrative explanation for the intuition behinds the constructions and also its possible impacts outside pure theory."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2889/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2889/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2889/Reviewer_aiE7"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2889/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1700525770740,
        "cdate": 1700525770740,
        "tmdate": 1700525770740,
        "mdate": 1700525770740,
        "license": "CC BY 4.0",
        "version": 2
    }
]