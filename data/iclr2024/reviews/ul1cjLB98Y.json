[
    {
        "id": "H8bGTW3JQs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7698/Reviewer_T6g2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7698/Reviewer_T6g2"
        ],
        "forum": "ul1cjLB98Y",
        "replyto": "ul1cjLB98Y",
        "content": {
            "summary": {
                "value": "This work initiates a theoretical study on the unimodal bias in multimodal learning, by analyzing the training dynamics. In particular, for linear networks, factors including a deeper fusion layer, stronger correlations between modalities and disparities in input-output correlations are identified as the causes of the unimodal bias."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The problem of building up a theoretical understanding of multimodal learning is urgent and significant.\n\nAnalyzing the training dynamic is an interesting and promising avenue for understanding unimodal bias. Such direction is intuitive due to relevant works on the implicit bias of neural networks, particularly on the training dynamics after zero training loss."
            },
            "weaknesses": {
                "value": "**Unfocused writing:** the writing of this work is unfocused. From the title and the contents, I presume this work is theoretical paper. However, there is no formal presentation of the theoretical results (propositions/lemmas/theorems). It greatly obstructs a smooth understanding of the results for readers. After I read the main-text, I still can't tell which part is prioritized. There should be rigorous summaries of the theoretical results. Even a heuristic-level summary can be useful.\n\n**Restricted results:** the dynamic analysis only concerns linear networks, which is not popular in practice. Tools from the study of implicit bias of neural networks (for example, [1]) might be useful to deal with nonlinear networks.\n\n[1] What Happens after SGD Reaches Zero Loss? \u2013 A Mathematical Framework, Li et al 2021"
            },
            "questions": {
                "value": "Though the contents might be interesting, the current writing style is certainly non-standard and disadvantageous to readability. In my opinion, the work needs a thorough rewriting for a clear summary and presentation of the theoretical results.\n\nStill, I'm curious the reason of the current writing. There seems to be plenty rigorous mathematical arguments in the appendix. Is there any difficulty preventing you from summarizing them as theorems?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7698/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697142708005,
        "cdate": 1697142708005,
        "tmdate": 1699636937610,
        "mdate": 1699636937610,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Tijv65cDFd",
        "forum": "ul1cjLB98Y",
        "replyto": "ul1cjLB98Y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7698/Reviewer_H94d"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7698/Reviewer_H94d"
        ],
        "content": {
            "summary": {
                "value": "This paper theoretically study the unimodal bias in deep multimodal linear networks. They also derived the duration of the unimodal phase in terms of network configuration, and dataset statistics. The theoretical findings are supported by numerical simulations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is clear and well-written \n2. The results for early fusion and intermediate fusion are novel\n3.  This paper explicitly characterizes an analytical relationship between unimodal bias, network configuration, and dataset statistics under simplified linear settings, and the implication from theory, fast-to-learn modality, is interesting.\n4.  The results are validated by numerical simulations."
            },
            "weaknesses": {
                "value": "1. The author claims \"there is a scarce theoretical understanding of how unimodal bias arises and how it is affected by the network configuration, dataset statistics, and initialization\". However,  the work [1] mentioned in this paper already theoretically explored the rise of unimodal bias in a more realistic neural network setting, and their results somewhat reveal the relationship between the inferior performance of late-fusion networks with initialization and modality correlations. \n\n2. Moreover, there is another work [2] that has provided some analysis about insufficient learning of uni-modal features and proposed some methods to overcome the limitations of late-fusion networks.  In their study, they also discuss the effect of easy-to-learn features.\n\nTherefore, the novelty and contribution of this paper compared to the previous analysis is not clear to me, considering they studied more complex and realistic settings.\n\n[1] Modality Competition: What Makes Joint Training of Multi-modal Network Fail in Deep Learning? (Provably),  Huang et al, ICML 2022\n\n[2] On Uni-Modal Feature Learning in Supervised Multi-Modal Learning, Du et al, ICML 2023"
            },
            "questions": {
                "value": "See weakness.\n\n1. For the data generation process,  it appears that only the correlation matrices are required. Are there specific assumptions made regarding $y$ that need to be clarified?\n2. The author claims \"we develop a theory of unimodal bias with deep multimodal linear networks.\" However, the frequent use of the approximation symbol ($\\approx$) in the appendix when deriving mean results raises questions about the rigor and precision of the theoretical justifications provided."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7698/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7698/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7698/Reviewer_H94d"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7698/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698445077680,
        "cdate": 1698445077680,
        "tmdate": 1699636937491,
        "mdate": 1699636937491,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qch0oBSkCj",
        "forum": "ul1cjLB98Y",
        "replyto": "ul1cjLB98Y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7698/Reviewer_8NBJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7698/Reviewer_8NBJ"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on a theoretical understanding of unimodal bia and  examines the effect of network architecture, dataset characteristics, and initialization factors. It reveals that while early fusion networks do not exhibit unimodal bias, this bias is noticeable in networks with intermediate and late fusion. Additionally, the paper quantifies the duration of the unimodal phase in these settings. To support these findings, the paper presents experimental data using numerical simulations conducted on two-layer ReLU networks and deep linear networks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper tackles an important problem of unimodal bias by investigating the unimodal bias theoretically and understanding the impact of various components such as network configuration, dataset statistics and initialization, which would be of interest to the community.\n- The paper was clear and well-written.\n- The supporting experimental evidence provides interesting insights into intermediate and late fusion for multimodal learning."
            },
            "weaknesses": {
                "value": "- The paper presents an interesting study of unimodal bias in intermediate and late fusion contexts, yet the evidence supporting the absence of unimodal bias in early fusion remains unconvincing. The reliance on the Frobenius norm of weights as a metric for understanding unimodal bias seems reasonable, but the experiments regarding early fusion are limited to simplistic scenarios. These toy settings are insufficient to assert the absence of unimodal bias in early fusion.\n- All experiments are conducted with linearly separable data. The inclusion of experiments with XOR data, where early fusion fails to perform effectively, further casts doubt on the claims for early fusion. Considering that the main focus of the paper was to investigate the interplay between unimodal bias, network configuration, and dataset statistics, the scope and depth of the study become critical. When this research is contrasted with prior studies that have identified unimodal bias across a diverse range of datasets, the robustness of the current findings for complex, real-world scenarios appears uncertain.\n\n- Minor suggestions\n   - In equation 2, did you intend to use W^{tot} alongside y?.\n   - The latter part of the caption for figure 2, particularly the description following parts a-c, is somewhat confusing and could benefit from a more straightforward explanation."
            },
            "questions": {
                "value": "Apart from the review, I have some additional questions:\n- Can the authors provide more details with respect to Fig 3e) \n- \u201cTwo-layer early fusion ReLU networks do not learn XOR features and can even fail to learn this task\u201d \u2013 can the authors comment and provide more reasoning about this observation wirth respect to the XoR experiment?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7698/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698785059106,
        "cdate": 1698785059106,
        "tmdate": 1699636937346,
        "mdate": 1699636937346,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zfKunw2hsp",
        "forum": "ul1cjLB98Y",
        "replyto": "ul1cjLB98Y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7698/Reviewer_W8DV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7698/Reviewer_W8DV"
        ],
        "content": {
            "summary": {
                "value": "The paper studies how deep linear networks with multiple pathways learn from data to produce a scalar output  when starting from small weights. The paper studies how the learning dynamics depend on layer in the network architechture when the modalities is fused in an additive manner, and find that with early fusion both modalities are learned (approx) simultaenously, whereas with late fusion the modality more correlated with the output is learned earlier in training."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is well-written, the figures are clear, and the mathematical results appear to be sound. I did not carefully check the derivation of the time ratios for learning from the different modalities in the Appendix."
            },
            "weaknesses": {
                "value": "The motivating phenomonom (unimodal bias) that one modality dominates at convergence is not addressed in the deep linear multimodal settings, as the manuscript studies the transient dynamics for when these modalities get learned (which the authors directly acknowledge in the intro). While the motivating phenomenon is well motivated, developing an improved understanding of the transient dynamics was not well-motivated. It would be helpful if the manuscript could comment/discuss how the analysis of the transient in the deep linear network setting could inform the phenomemon of unimodal bias at convergence in practice. I also feel that the paper title could better reflect the contents of the paper.\n\nDo the results extend to the multitask case where output y is a vector?"
            },
            "questions": {
                "value": "The authors considered architectures of equivalent depth between pathways. How do the result change if these depths differ? \n\nHow do the weights evolve in the pre and post fusion layers?\n\nWhen the paper says: \"In essence, an early fusion point allows the weaker modality to benefit from the stronger modality's learning in the post-fusion layers:\" Are there settings where this can be harmful as well, or would the larger scale of the weights always help learning?\n\nMinor: \n\nWhat matrix norm is being used throughout the paper? It should be clarified (For example in Eq 7, Eq 9 etc). Apologies if I missed it.\nDo the results apply to more complicated covariance matrices? It seems like diagonal input covarainces were studied, and 2x2 matrices.\n\nDefine the product notation used for a product over weight matrices (for example in Eq 2)\n\nIt was unclear the experimental details used. For example, were there a finite amount of inputs used, or were inputs drawn according to the covariance structure every batch (In Fig 2,3; for example). The paper mentioned full-batch SGD but the details were not provided (and could not find in appendix.)\n\nIn sect. 3.2.3 unclear why it is ideal for modality learned first to lead to larger decrease in loss.\n\nWording \" a smaller initialization scale exacerbates the impediment to learning modality A compared to modality\nB, yielding a larger time ratio\" is unclear.\n\nIt was a bit strange to add in new results in the discussion section.\n\nWhy do the results require a small initialization?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7698/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7698/Reviewer_W8DV",
                    "ICLR.cc/2024/Conference/Submission7698/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7698/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698870110255,
        "cdate": 1698870110255,
        "tmdate": 1700689453328,
        "mdate": 1700689453328,
        "license": "CC BY 4.0",
        "version": 2
    }
]