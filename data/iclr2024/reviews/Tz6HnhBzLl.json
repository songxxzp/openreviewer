[
    {
        "id": "hdx0QzG92R",
        "forum": "Tz6HnhBzLl",
        "replyto": "Tz6HnhBzLl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3129/Reviewer_Uwp8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3129/Reviewer_Uwp8"
        ],
        "content": {
            "summary": {
                "value": "This paper studies Robust Reinforcement Learning (RRL) within the framework of positional differential game theory, presenting a novel approach to understanding agents' robust policies and deterministic payoff values. This paper introduces two algorithms: Isaacs Deep Q-Networks (IDQN) and Decomposed Isaacs Deep Q-Networks (DIDQN). Through theoretical proofs, under the Isaacs\u2019s condition, it shows that a centralized Q-learning approach can be developed for these problems. Empirical results underscore the effectiveness of these algorithms against other RRL and Multi-Agent RL baselines, also proposing a new framework for evaluating the robustness of trained policies."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Firstly, it offers an exhaustive literature review, meticulously drawing contrasts between the current work and existing literature. Moreover, the clarity of writing facilitates comprehension, making the paper accessible to a broad spectrum of readers. Lastly, the experimental results provided is convincing, adding weight to the authors' arguments and hypotheses."
            },
            "weaknesses": {
                "value": "1. While the authors adeptly show the significance of considering robustness in RL in the introduction, they do not clearly state why there's a pressing need to study robustness in the framework of positional differential game theory. This oversight makes the paper's motivation less pronounced and leaves readers questioning the specific choice of this framework.\n\n2. One point of confusion for me is in Section 3 where the authors state that \"to solve differential games by RL algorithms, it is necessary to discretize them in time\uff0cwe describe such a discretization...\". If employing RL to address differential games still necessitates discretization, then why consider robust RL problems within the framework of differential games? Wouldn't it be more straightforward to tackle the issue directly within the framework of discrete robust MDPs? What additional advantages does considering robustness in differential games provide? Furthermore, how does the efficacy of using discretization to address differential games compare with directly solving a discrete robust MDP?"
            },
            "questions": {
                "value": "Please see the Weaknesses, I will finalize my rating after rebuttal."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Non"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3129/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3129/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3129/Reviewer_Uwp8"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3129/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698344723146,
        "cdate": 1698344723146,
        "tmdate": 1700517355137,
        "mdate": 1700517355137,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6rtwoNuOWP",
        "forum": "Tz6HnhBzLl",
        "replyto": "Tz6HnhBzLl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3129/Reviewer_BWN5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3129/Reviewer_BWN5"
        ],
        "content": {
            "summary": {
                "value": "This paper explores robust adversarial reinforcement learning (RARL) through the lens of positional differential game theory, ensuring the worst-case deterministic payoff. Leveraging the positional differential game theory, the authors formulated multi-agent reinforcement learning (MARL) to solve the RARL problem. These techniques were then benchmarked against multiple MARL baseline methods. Finally, the authors analyze the learned policies, highlighting the superior performance of the introduced algorithms."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper provides a novel perspective of RARL in the context of positional differential games."
            },
            "weaknesses": {
                "value": "* There is substantial room for improvement in terms of writing. There is a lack of overall organization of sections and smooth transitions between paragraphs. Some examples are:\n    * In section 2, the authors could provide more insights into what is the difference between the standard formulation and differential game and why it is important to have a deterministic payoff. How the fact of the PDG is important to this paper and when the Isaacs's condition is met or not met.\n    * In the experiment section, the metric of 'stability' appears without proper definition and explanation. For example, is 'stability' equivalent to 'deterministic payoff'? If so, why not just stick to the latter? \n    * In the experiment section again, some analysis seems to be out of place and is unclear how it is related to the topic. For example, \"it is more efficient for agents...\" What is efficiency exactly, and how is it compared across CounterDQN and MADQN?\n* The reasoning and motivation for using positional differential games instead of Markov games as the framework are unclear. Markov games can be deterministic.\n* The discretization of action space makes it unclear under what conditions the main theorem still holds.\n* The effects of time discretization are unclear. How does it affect the estimation error or the convergence?"
            },
            "questions": {
                "value": "* In Equ. 3, should it be $t_{m+1}$ instead of $\\tau_{m+1}$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3129/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698793829185,
        "cdate": 1698793829185,
        "tmdate": 1699636259635,
        "mdate": 1699636259635,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5vrcKYDwQU",
        "forum": "Tz6HnhBzLl",
        "replyto": "Tz6HnhBzLl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3129/Reviewer_h5zf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3129/Reviewer_h5zf"
        ],
        "content": {
            "summary": {
                "value": "Robust Reinforcement Learning (RRL) treats uncertainty as actions of an adversarial agent, and in this paper, the authors propose a novel approach by applying positional differential game theory to develop a centralized Q-learning method. The authors demonstrate that this method can approximate solutions to both minimax and maximin Bellman equations, and they introduce two algorithms, Isaacs Deep Q-Networks (IDQN) and Decomposed Isaacs Deep Q-Networks (DIDQN). The algorithms are tested in various environments and outperform other baseline RRL and Multi-Agent RL algorithms in the experiments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is well-organized and involving RRL in potential differential games is a neat idea. The ideas for the experiments all seem very natural to me. The paper is also nicely organized."
            },
            "weaknesses": {
                "value": "I think the authors address the limitations of their own work as is."
            },
            "questions": {
                "value": "Q1) In Section 5, when the environments is described, what is the difference between $R$ and $\\mathbb{R}$?\n\nQ2) What are the specifications of the system used to run the experiments?\n\nMinor comments:\n\nI think you should name your main theorem. At the moment, you referred to it as \"Theorem\" and I think it would help if you name it. Either Thereom 1, Main Theorem, or some other name. Similarly, I would enumerate the remark on page 5 and maybe call it a corollary. \n\nI was unfamiliar with some of your notation, but if it is common in the field then you should keep it, of course. In particular, $\\overline{a,b}$  to refer to the integers $a,a+1,\\ldots, b$ was new to me. Similarly, I had to look up $\\lim_{\\delta\\downarrow0}$. I merely wanted to point out that I wasn't familiar with the notation, though I guess it is not so hard to figure out from context."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3129/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698906310018,
        "cdate": 1698906310018,
        "tmdate": 1699636259514,
        "mdate": 1699636259514,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CWm3cgQPAH",
        "forum": "Tz6HnhBzLl",
        "replyto": "Tz6HnhBzLl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3129/Reviewer_o2s5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3129/Reviewer_o2s5"
        ],
        "content": {
            "summary": {
                "value": "The paper discusses RRL, Robust reinforcement learning, a recent method to incorporate physics and other constraints into the RL paradigm, such as disturbances and perturbations. Finding algorithms for RRL (modeled as an extra adversary in an multi-agent RL setting) is difficult due to non-stationarity. \n\nThe paper introduces a shared-Q-function to compute policies, and compares their new algorithm (IDQN) to other algorithms, on a range of suitable games.\n\nThe contribution/text of the paper is mostly theoretical, proving theorems on why such an algorithm may work."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Robust RL is a new and under-studied problem, mostly due to the non-stationary state space. It is nice to see the problem being studied. Also nice is the part on the Isaac condition, and the derivation of the shared Q function algorithm. The experimental results comapring the performance to other algorithms is also nice"
            },
            "weaknesses": {
                "value": "A shared-Q function algorithm is compared against non-shared Q MARL algorihtms, or to standard single agent RL algorithms such as DDQN and PPO. This is comparing apples and oranges. Of course  shared Q function algorithms outperforms the other options. As such the experimental results are not very meaningful.\nSharing the Q function is not really addressing the MARL problem of non-stationarity, it is a kind of cheating."
            },
            "questions": {
                "value": "I find the paper sympathetic in that it addresses Robust RL. Using a shared Q function transforms the MARL problem into a single agent problem. The pages of proofs do not contribute much to deeper understanding of the problem."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3129/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699386748130,
        "cdate": 1699386748130,
        "tmdate": 1699636259458,
        "mdate": 1699636259458,
        "license": "CC BY 4.0",
        "version": 2
    }
]