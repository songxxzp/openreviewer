[
    {
        "id": "KWZ29We4ow",
        "forum": "bvw9H80jyW",
        "replyto": "bvw9H80jyW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission676/Reviewer_YuZu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission676/Reviewer_YuZu"
        ],
        "content": {
            "summary": {
                "value": "This work introduces a new unsupervised skill discovery method, ComSD. The main idea of ComSD is to decompose mutual information into $I(\\tau; z) = H(\\tau) - H(\\tau|z)$ and to introduce a skill-dependent coefficient $\\beta(z)$ to weight the second term. The authors employ a CIC-like contrastive estimator for the second term and a particle-based entropy estimator for the first term. They show that ComSD outperforms previous skill discovery methods (BeCL, CIC, APS, SMM, and DIAYN) in DMC locomotion environments in terms of both fine-tuning and hierarchical RL performances."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The final objective in Eq. (12) is simple and intuitive.\n- The authors evaluate ComSD on both fine-tuning and hierarchical RL settings, where ComSD achieves better performance compared to five previous approaches."
            },
            "weaknesses": {
                "value": "- The contribution of the proposed method (ComSD) appears incremental compared to CIC. Especially, [this previous version](https://openreview.net/pdf?id=Z12zA99EFEi) of CIC is almost identical to ComSD, with the same contrastive objective for $H(\\tau|z)$ and the same particle-based entropy estimator for $H(\\tau)$. The only difference is that ComSD additionally uses a skill-dependent linear coefficient (i.e., using $\\beta(z) \\alpha H(\\tau|z)$ instead of $\\alpha H(\\tau|z)$), which, in my view, is too incremental to justify an ICLR publication.\n- Given the high similarity to CIC, I believe this work requires more thorough comparisons with CIC. Since ComSD uses $\\beta(z)$ and $\\alpha$ individually tuned for each environment, it should be compared against (the $H(\\tau) - \\alpha H(\\tau|z)$ version of) CIC with individually tuned $\\alpha$ to ensure a fair comparison.\n- There is a mismatch in CIC performance between this paper and [the original CIC paper](https://arxiv.org/pdf/2202.00161.pdf). Weirdly, their performances are the same in `walker walk` and `walker stand`, but different in `walker run` and `quadruped run`. Could the authors clarify these differences?\n\nMinor issues\n\n- \"(Liu & Abbeel, 2021a) first employs the second MI decomposition Eq. 2 for a better MI estimation.\" in Appendix B: (1) I would recommend using `\\citet` for inline citations, and (2) to the best of my knowledge, DADS (Sharma et al., 2020) is the first such method.\n- Nitpick: $\\propto$ can't be used in Eq. (4) because the right-hand side is not technically proportional to the left-hand side."
            },
            "questions": {
                "value": "- As far as I understand, the authors use a fixed latent vector of $z = [0, 0.5, 0.5, \\dots, 0.5]$ for fine-tuning experiments. In this case, why don't we just use a fixed coefficient of $\\beta = w_{low}$ during training?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission676/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698120625710,
        "cdate": 1698120625710,
        "tmdate": 1699635994908,
        "mdate": 1699635994908,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NCRJ2Orz2Y",
        "forum": "bvw9H80jyW",
        "replyto": "bvw9H80jyW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission676/Reviewer_wqjS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission676/Reviewer_wqjS"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a unsupervised skill discovery method named ComSD. With ComSD, the mutual information between states (or pairs of consecutive states) and skill vectors is estimated with the NCE-style contrastive learning loss and particle-based entropy estimation, and the coefficient for weighting the two intrinsic reward terms is designed to differ across skill vectors. They also empirically compare ComSD with baseline methods in two representative skill discovery evaluation settings, skill combination and skill fine-tuning on locomotion simulation tasks including DMControl and provide further quantitative and qualitative analyses."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The empirical evaluation is done in various settings. Also, compared to the selected set of baselines, the proposed method shows fair performance on the tasks.\n- The manuscript basically easy to follow, and Fig. 1 and 2 help readers to understand the proposed approach more quickly."
            },
            "weaknesses": {
                "value": "- Skill-based Multi-objective Weighting (SMW), which is the main contribution of this paper in my view, needs rationale for it. Apart from empirical confirmation that it improves the resulting performance, I don't think why the weighting coefficient should be different for different skill vectors and why it needs to be structured like that are not clear. I'm not trying to argue that just a constant coefficient is enough, but I'm rather asking the following questions: Exactly what benefits does that form provide and why? How does having different coefficients for different skills like that affect the overall learning objective?\n- I believe the statement about the proposed method's difference from CIC and APS (Sec. 4.1) needs further clarification. If the \"contrastive results\" employed for the state entropy maximization mean the state representations, CIC uses the state representations from the contrastive learning for estimating the state entropy.\n- Some presentation issue: the use of \"reasonable\" and \"unreasonable\" for describing the MI estimation methods doesn't seem technical/scientific and is without appropriate backup."
            },
            "questions": {
                "value": "Please check out the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission676/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698819995170,
        "cdate": 1698819995170,
        "tmdate": 1699635994820,
        "mdate": 1699635994820,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vH6QhZJJTN",
        "forum": "bvw9H80jyW",
        "replyto": "bvw9H80jyW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission676/Reviewer_U8Ne"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission676/Reviewer_U8Ne"
        ],
        "content": {
            "summary": {
                "value": "This paper points out that one of the major challenges in MI-based skill discovery is balancing two intrinsic reward terms, the state entropy for exploration and the negative conditioned state entropy for exploitation or state-skill alignment. On top of the prior skill discovery method, CIC, this paper proposes Skill-based Multi-objectives Weighting (SMW) to dynamically weight these two reward terms for different skill vectors. The proposed method outperforms prior MI-based skill discovery approaches in both skill composition and skill finetuning experiments on 4 URLB locomotion domains."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The paper points out the challenge of balancing exploration and exploitation in skill discovery and then provides a simple practical solution, Skill-based Multi-objectives Weighting (SMW).\n\n* The exhaustive experiments demonstrate that the proposed method, ComSD, can discover diverse skills and adapt better to downstream tasks. Especially, ComSD significantly outperforms other methods on most skill combination tasks.\n\n* The paper is well-organized and easy to follow."
            },
            "weaknesses": {
                "value": "* In Section 3.3, it is clear that the weighting between two intrinsic reward terms is challenging. However, it is not straightforward to get how the proposed SMW resolves this issue. The choice of the dynamic weighting term in Equation 11 is not justified and explained sufficiently. As this is the main contribution of this paper, it has to be clearly stated and examined in the paper.\n\n* The approach of ComSD seems very similar to CIC except for the dynamic weighting (SWM) and it is unclear how ComSD is different from CIC other than SWM. In Section 4.1, the paper says \"For CIC, ComSD follows it in state entropy estimation but first proposes to employ contrastive results for explicit state entropy maximization.\" but I cannot follow the \"explicit state entropy maximization\" part. Could the authors elaborate on this more?\n\n* Many MI-based methods show their limited applicability to domains other than simple locomotion environments. Although the strong skill discovery performances on the locomotion tasks are impressive, the proposed approach could overfit to the specific domain. Comparisons on manipulation tasks, as in (Park 2023) would make the claim of this paper much stronger.\n\n* Although the proposed method outperforms prior MI-based skill discovery approaches, recent non-MI-based skill discovery methods (Park 2021, Park 2023) have shown much diverse skill sets. Thus, it is important to compare with these non-MI-based approaches.\n\n\n---\n\nAlthough the author response did not address all my concerns, it is clear that ComSD has resolved some issues in CIC implementation, which seems useful for future research. Thus, I increased my rating to borderline accept."
            },
            "questions": {
                "value": "Please address the weaknesses mentioned above.\n\n\n### Minor questions and suggestions\n\n* In Equation 11, $w_{high}$ at the end should be $w_{low}$.\n\n* In Section 4.1, Evaluations, \"skill combination in URLB\" should be \"skill finetuning in URLB\".\n\n* Section 4.2 mentions that 6-10 different random seeds are used in skill combination and finetuning experiments. Does this mean a pre-trained skill policy and meta- or finetuned- policy for each random seed? Or, does this mean one pre-trained skill policy and 6-10 different meta- or finetuned- policies? The variances in Figure 3 and Table 1 seem a bit smaller than expected given the high-variance nature of skill discovery methods."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission676/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission676/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission676/Reviewer_U8Ne"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission676/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699177573669,
        "cdate": 1699177573669,
        "tmdate": 1700633434879,
        "mdate": 1700633434879,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "nyd5V7eZuR",
        "forum": "bvw9H80jyW",
        "replyto": "bvw9H80jyW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission676/Reviewer_uFyJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission676/Reviewer_uFyJ"
        ],
        "content": {
            "summary": {
                "value": "In this work, the authors present a novel unsupervised skill discovery algorithm called ComSD (Contrastive Multi-Objective Skill Discovery) that uses contrastive learning and entropy estimation to learn skills in an unsupervised fashion in simulated environments. The primary insight from the authors are twofold: using contrastive learning to learn a similarity metric between skill latents z and trajectories tau, and using a coefficient to balance the aspect of quality of the policy vs. the diversity of the explorations.\n\nIn this work, the authors first explain their algorithm, which is to first learn an estimated lower bound on the p(tau, z) by using an NCE loss function to find the entropy of a skill, and using a particle based entropy estimator to estimate the entropy of the trajectories. Then, the authors explain their automatically rebalanced multi-objective weighting (SMW) which help the learned skills balance between the quality and the diversity of the learned skills.\n\nThe authors then show some experiments on standard unsupervised skill discovery environments from DM-control. Their experiments contain two experiments first for skill finetuning and skill combination using a hierarchical controller. Then, in an ablation experiment they show that both the dynamic weighting and the contrastive encoding are both important for ComSD to perform well. Finally, they show that ComSD also outperforms other comparable algorithms in state diversity metrics."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The work shows a number of positive qualities:\n\n+ The authors motivate their algorithm well; there are only so many ways of doing unsupervised skill discovery, but the authors identify an approach that differs from the baseline approaches and execute on it.\n+ The trade-off between diversity and consistency is not explored often enough in the literature, but the authors identify it as an important factor, and consciously optimize for this trade-off across their skills.\n+ They evaluate their algorithm on a good number of environments, and against a good number of baselines.\n+ The authors also evaluate ComSD on two state diversity metric, which is also quite important for an unsupervised skill discovery metric."
            },
            "weaknesses": {
                "value": "The primary flaw with this work is the incomplete comparison with prior state of the art. Unsupervised skill discovery is a crowded field of research, so it is natural that they may miss certain previous works during their literature review. However, a quite relevant work that the authors seem to have missed is [1]. [1]  seems relevant to the authors work because of the (a) information gain expansion used in the work, (b) use of the point-based entropy estimator to compute the reward, and most importantly (c) use balancing coefficient to trade off between diversity and consistency between different skills. Moreover, [1] seem to outperform the standard unsupervised skill discovery algorithms at the time, so it would be good for the authors to add the work as a baseline and/or explain the differences between ComSD and [1] and why they are not compatible for direct comparison.\n\nApart from that, there are some other issues of the work:\n1. The notation r_exploration and r_exploitation look quite similar to each other, and I was thrown off multiple times while reading the explanations. If they could change the notation it may be much easier to read.\n2. The SMW objective is not motivated well. It seems like it was pulled out of nowhere. More motivation for this would be apt.\n3. The skills are trained for 2M steps, however, there is no clear reasoning for why this number was picked. How does this look in the limit, at say 10M steps?\n\n[1] Shafiullah, Nur Muhammad Mahi, and Lerrel Pinto. \"One After Another: Learning Incremental Skills for a Changing World.\" International Conference on Learning Representations. 2021."
            },
            "questions": {
                "value": "See above. Specifically, what is similar vs not about previous works, more motivation for the SMW module, and the behavior in limiting number of environment steps would be good."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission676/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699223233142,
        "cdate": 1699223233142,
        "tmdate": 1699635994699,
        "mdate": 1699635994699,
        "license": "CC BY 4.0",
        "version": 2
    }
]