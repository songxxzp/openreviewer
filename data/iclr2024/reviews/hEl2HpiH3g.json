[
    {
        "id": "6guvzdfYzk",
        "forum": "hEl2HpiH3g",
        "replyto": "hEl2HpiH3g",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4174/Reviewer_Na6T"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4174/Reviewer_Na6T"
        ],
        "content": {
            "summary": {
                "value": "The paper uses Mixture of Experts architecture with a gating function to select \"the most relevant\" experts for each client data \"just-in-time\" for federated learning. They also take advantage of a pretrained model as \"common expert\". The authors aim i) global generalization ii) enhance global model via personalized models ii) solve \"cold-start\" problem."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- It is good to see that the authors have used MoE for federated learning, differently from the FedMix paper.\n- Reducing communication costs is critical and very good.\n- Using anchor users seems useful."
            },
            "weaknesses": {
                "value": "- The contribution lacks novelty, as using a gating function and common expert is not new. Even there are multi-gate mixture of experts architectures in the literature [1].\n- The architecture is similar to STAR model in paper [2] without anchor users.\n- There are couple of places mention that our approach lower communication costs but there is no experimental results that show that how much improvement is there as in FedMix paper.\n- There are no experiments for the cold-start problem as claimed in the paper ( this is not same as unseen new users for testing. Testing, of course, should be unseen). \n\n[1] Modeling Task Relationships in Multi-task Learning with\nMulti-gate Mixture-of-Experts (https://dl.acm.org/doi/pdf/10.1145/3219819.3220007)\n[2] One Model to Serve All: Star Topology Adaptive Recommender\nfor Multi-Domain CTR Prediction (https://dl.acm.org/doi/pdf/10.1145/3459637.3481941?casa_token=X928_yKMvcsAAAAA:WKNfD3i-ELk5CTxjIqs8t6MxMN0LSmwwhIvbEY7lvKaoqp8BC0zQdUOuZHXQKUkMUH1poak8ZFxZ)"
            },
            "questions": {
                "value": "- The claim \"we partition the data samples by classes to turn full datasets into non-i.i.d. subsets\", how do you make sure that samples with different class labels with same data is non - i.i.d ? \n- This work also very similar to multi-task learning, one of the main problem is conflicting gradients. Since you claim the data is non i.i.d. have you ever encountered this problem as in these papers [3] [4]\n\n[3] MAMDR: A Model Agnostic Learning Framework\nfor Multi-Domain Recommendation (https://dl.acm.org/doi/pdf/10.1145/3459637.3481941?casa_token=X928_yKMvcsAAAAA:WKNfD3i-ELk5CTxjIqs8t6MxMN0LSmwwhIvbEY7lvKaoqp8BC0zQdUOuZHXQKUkMUH1poak8ZFxZ)\n[4] Gradient Surgery for Multi-Task Learning (https://proceedings.neurips.cc/paper/2020/file/3fe78a8acf5fda99de95303940a2420c-Paper.pdf)\n[5] Conflict-Averse Gradient Descent\nfor Multi-task Learning (https://proceedings.neurips.cc/paper_files/paper/2021/file/9d27fdf2477ffbff837d73ef7ae23db9-Paper.pdf)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4174/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4174/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4174/Reviewer_Na6T"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4174/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698347044708,
        "cdate": 1698347044708,
        "tmdate": 1699636383306,
        "mdate": 1699636383306,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Rb9o7sOC4J",
        "forum": "hEl2HpiH3g",
        "replyto": "hEl2HpiH3g",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4174/Reviewer_k6Nb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4174/Reviewer_k6Nb"
        ],
        "content": {
            "summary": {
                "value": "The paper studies a federated learning setting where the goal is to fine-tune the models. The main framework FedJETs is given a pretrained model and contains multiple ``expert'' models and a gating function. When new client data comes in, the gating function utilizes the representation from the pre-trained model to decide which K experts to update. Then, using the client's data, FedJETs obtain updates for the gating function as well as the K experts and send them back to the server. The server aggregates and updates the new weights."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper presented the main idea as well as the FEDJETs algorithm in a clear and intuitive manner. The idea of having individual expert models and a gating function to select experts is intuitive and reasonable. The authors also discussed the technical difficulties coming with this design. The experimental results suggest the efficacy of the proposed method."
            },
            "weaknesses": {
                "value": "My biggest concern is the novelty of the proposed method. The general framework of having individualized models and selecting a subset of experts for performing ensemble learning is a traditional topic. The specific setting of having a pre-trained model along with a gating function to select a subset of experts to update is new. I am not entirely familiar with the current federated learning literature, so I will leave other reviewers to decide on the novelty of the paper to the federated learning community. \nIn addition to the concern about the novelty of the work, another concern I have is the applicability of the method when expert models need to be very large. It seems to be inefficient to use the common expert (a large pre-trained model) to just perform expert selection. Would it be more reasonable, computation-wise at least, to not have individual expert models but different expert heads so that the pretrained common expert can be used to extract a common representation to pass into different experts?"
            },
            "questions": {
                "value": "- How should the number of experts scale with the number of clients? How should one choose the ``K'' hyperparameter?\n- Could the authors comment on the computation and memory costs of having individual experts? How big should the expert model be compared to the pretrained common expert model? \n- Other than using the pre-trained model for obtaining representation for the gating function, is it used in some other ways, e.g., is there a way to combine its output with the expert model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4174/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698806440783,
        "cdate": 1698806440783,
        "tmdate": 1699636383223,
        "mdate": 1699636383223,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SUCxsOeio3",
        "forum": "hEl2HpiH3g",
        "replyto": "hEl2HpiH3g",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4174/Reviewer_pX49"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4174/Reviewer_pX49"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes FedJETs, a distributed system that connects and extends Mixture of Experts in FL setting. The system features multiple independent models as experts, in contrast to common MoE settings where different parts of a model is considered as experts. The authors introduce a pretrained common expert and a novel gating functionality to guide the specialization of experts during training. The authors claim that the combined system can exploit the characteristics of each client\u2019s dataset and adaptively select experts suitable during training. FedJETs also claims to be able to dynamically select experts and adjust to unseen clients on-site."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper proposes FedJETs, a distributed system that connects and extends Mixture of Experts in FL setting. The system features multiple independent models as experts, in contrast to common MoE settings where different parts of a model is considered as experts. The authors introduce a pretrained common expert and a novel gating functionality to guide the specialization of experts during training. The authors claim that the combined system can exploit the characteristics of each client\u2019s dataset and adaptively select experts suitable during training. FedJETs also claims to be able to dynamically select experts and adjust to unseen clients on-site."
            },
            "weaknesses": {
                "value": "The paper proposes FedJETs, a distributed system that connects and extends Mixture of Experts in FL setting. The system features multiple independent models as experts, in contrast to common MoE settings where different parts of a model is considered as experts. The authors introduce a pretrained common expert and a novel gating functionality to guide the specialization of experts during training. The authors claim that the combined system can exploit the characteristics of each client\u2019s dataset and adaptively select experts suitable during training. FedJETs also claims to be able to dynamically select experts and adjust to unseen clients on-site."
            },
            "questions": {
                "value": "It would be appreciated, considering the nature of this paper, if more results regarding Non-IID datasets other than the CIFAR data suite could be demonstrated. For FL scenarios, there are plenty of available datasets beyond the CIFAR data suite with more obvious levels of Non-IID features (e.g., the LEAF benchmark datasets).\n\nBesides, the ablation study regarding the anchor client ratio might not be sufficient as to determine the claimed \u201coptimal\u201d ratio. It served the proposal to address the significance of anchor clients, but there seems to be more to explore regarding such a key component of the entire method. Is it possible for a higher anchor-normal client ratio to achieve faster convergence or even a better overall performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4174/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4174/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4174/Reviewer_pX49"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4174/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698847960957,
        "cdate": 1698847960957,
        "tmdate": 1699636383135,
        "mdate": 1699636383135,
        "license": "CC BY 4.0",
        "version": 2
    }
]