[
    {
        "id": "h8ybid6q86",
        "forum": "hp4yOjhwTs",
        "replyto": "hp4yOjhwTs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6200/Reviewer_1poU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6200/Reviewer_1poU"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a curriculum design method that produces causally aligned curriculums. Given a causal graph, a curriculum generator, and a target task, the proposed method creates a causally aligned curriculum using two main ideas. First, the FindMaxEdit algorithm, which constructs a set of causally aligned source tasks based on the modifying the editable variables of the target task. Importantly, optimal decision rules can be transported across causally aligned source tasks. Second, the authors propose ordering causally aligned source tasks with an expansion criterion on the optimal decision rules (i.e. tasks should be ordered such that the set of optimal decision rules expands with each task)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "i. The method is well-founded and rigorously specified. Further, the main idea of this paper - using knowledge of causal relationships to improve curriculum generation methods - is clearly useful to the community. \nii. The paper is clear, with all details necessary to reproduce the work present in the paper. \niii. The experimental results on Colored Sokoban and Button Maze are strong."
            },
            "weaknesses": {
                "value": "Most of my concerns + comments are related to the motivation + presentation of the paper. \n\n- Motivation: Example 1's motivation for the paper is very contrived. The only reason a curriculum designer would design such a source task (where the box color is fixed to yellow) is if the curriculum designer was completely unaware of the causal dependencies of the reward. While this is possible in the case of causally-unaware automated curriculum generation methods, I find it a bit unlikely unless the crucial confounder variable U_i was either excluded from the state space (which would violate the usual Markovian reward assumption) or it is included in teh state space, but the agent cannot observe it (a partial observability assumption).  This leads to my questions: \n\t1)  Can the authors experimentally verify how often existing non-causal curriculum generation methods generate causally unaligned tasks? \n\t2) Can the authors comment on the expected benefit of their method in a domain with Markovian reward, or a fully observable domain w/no unobserved confounders? \n\n-  Acknowledging limitations and situating their work in presence of related work \n\t1) I saw that the related works section was relegated to the Appendix. There should at least be a pointer in the main paper to the related works. I found it helpful to read it, to understanding the positioning of this work, and think that it should be included in the main paper. The paper provides almost too many details about the method in the main paper, and at places is rigorous to the point of being pedantic. I think that the space would be better spent on situating the work properly. \n\t2) The inputs/outputs of the authors' method is not presented in plain language. As I understand it, the authors' method assumes access to the SCM of the task, and a curriculum generator such as GoalGAN. Such details should be made much clearer, perhaps through a summary methods figure or by adding a discussion of limitations. \n\n- Figures are not standalone: it is common practice to read a paper by skimming the figures + captions first, but I found that it was not possible to get the main idea of the figure, method and experimental results by doing so. Even after reading most of the accompanying text, I still found the figures + captions alone ambiguous. I had to find the specific part of the text referencing the figures -- sometimes needing to jump to other parts of the paper-- and read very carefully to understand what was happening. The authors should rewrite the figure captions and perhaps add a new \"methods\" figure summarizing the flow of their method. My specific comments on two figures are below, but all figures can be improved: \n\t1) Figure 5: Specifically, I was confused about why the curves labelled \"causal\" performed differently across the four columns? Also, the names of \"causal\" vs \"original\" were confusing. I think this is because the fact that the proposed method augments existing curriculum generation methods was presented only in a single sentence towards the end of the intro, and the knowledge was assumed in the rest of the paper. Seeing as this information is crucial to understand the paper, perhaps the authors can add this crucial piece \n\t2) Figure 3: It's confusing that figure (a) and (b) are identical except for the edit indicators. I needed to visually trace all arrows of both figures to verify this. The presence/absence of edit indicators should be explained in the caption, and the figure can be modified so that it is much quicker for the reader to notice that the only addition is the edit indicators (perhaps through the use of opacity / shading)."
            },
            "questions": {
                "value": "See the weaknesses section for the most substantial questions and comments. \n\nMinor clarity comments are below: \n- Exogenous vs endogenous should be defined at some point in the preliminaries section on SCMs. \n- \"che\" \"de\" \"an\" meanings were not immediately clear to me. \n- In Def 1, the SCM M* is labelled with a *, yet this notation does not appear elsewhere in the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6200/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6200/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6200/Reviewer_1poU"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6200/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698090376924,
        "cdate": 1698090376924,
        "tmdate": 1699636675396,
        "mdate": 1699636675396,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LQS6NlrHVK",
        "forum": "hp4yOjhwTs",
        "replyto": "hp4yOjhwTs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6200/Reviewer_eG2G"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6200/Reviewer_eG2G"
        ],
        "content": {
            "summary": {
                "value": "The paper tackled the problem of curriculum design in multi-task Reinforcement Learning. A challenge in the previous curriculum learning literature is that they all assume that the optimal decision rules are shared across different tasks. From a causal point of view, this expectation may not hold when the underlying environment contains unobserved confounders. To tackle this issue, the paper proposed a causal framework based on structural causal models and rigorously defined the notion of aligned tasks. The paper then proposed an algorithm that only generates aligned tasks. Simulation studies on Maze and Sokoban environments showed the advantage of the proposed algorithms."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper finds a significant issue in the current literature of automatic curriculum learning and provides a clear framework to discuss the issue of non-alignment. Thus, the paper has great significance.\n\nThe paper is well-written and all the important messages are clearly delivered."
            },
            "weaknesses": {
                "value": "1. The authors mentioned that curriculum learning could overcome the curse of dimensionality. However, all the examples discussed in this paper are tabular. There seems to be a mismatch. It would be interesting to discuss and experiment on an example with exponentially large state and action space.\n\n2. What is the computation-complexity of FindCausalCurriculum? Does it scale with the size of state and action space? If that is the case, it seems to be against the motivation of applying curriculum learning.\n\n3. Comments on experiments: In general, I believe the environment tested in this paper are rather naive.\n\nThe authors implement ALP-GMM by fixing the color. I don't think this is a good way to construct baseline. One can run ALP-GMM as a task sampler on a fixed set of tasks. ALP-GMM essentially assigns a probability for different tasks at the each round. If one task has the underlying U that is not aligned with the target task, ALP-GMM will adaptively reduce the weight for that task. \n\nI understand that fixing color matches the general story of the whole paper. However, I don't think this is a natural choice in examples where we have access to a simulator. It would be interesting to demonstrate a non-trivial example where C is a proxy of the underlying U and choosing C by running ALP-GMM could be harmful. \n\n4. An important way to improve the paper is to propose an algorithm for large (or continuous) state and action space. The current version seems to only work with tabular MDPs, which limits the complexity of environments the algorithm can be applied to."
            },
            "questions": {
                "value": "1. I was a bit confused by the wordings in abstract. The authors first state that invariant\noptimal decision rules does not necessarily hold, then they propose condition characterizing causally aligned source tasks, i.e., the invariance of optimal decision rules holds. The flow does not seem right here.\n\n2. Need extra clarifications on Figure 1 and 2. What does it mean by fixing the color? When the different algorithms are trained, are they only allowed to choose tasks from misaligned source tasks? Should this example be too artificial? It would be interesting to see the results when ALP-GMM can generate tasks with unknown color, while the algorithm may reuse these tasks. In this way, it is possible for ALP-GMM to discover that certain tasks can be more helpful for learning the target task. This is in fact more aligned with the situations in the real-world."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6200/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698630944436,
        "cdate": 1698630944436,
        "tmdate": 1699636675282,
        "mdate": 1699636675282,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4cUSTdDVCO",
        "forum": "hp4yOjhwTs",
        "replyto": "hp4yOjhwTs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6200/Reviewer_StU4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6200/Reviewer_StU4"
        ],
        "content": {
            "summary": {
                "value": "Curriculum reinforcement learning is to train the agent on a sequence of simpler, related tasks in order to gradually progress the learning towards a difficult, often sparse-reward target task. The hope is that if there are common optimal decisions among these simpler intermediate tasks and the target task, the agent will learn transferrable skills during the process and accelerate the learning process. However, this assumption may not hold if there are unobserved confounders in the environment. This paper delves into this problem from a causal perspective, defining conditions under which the optimal decisions remain consistent. It also introduces a method to create a curriculum that aligns causally. The method is tested in two grid-world environments with unseen confounders to validate the effectiveness of the proposed algorithms."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The structure and flow of the paper are easy to follow. Several simplified examples are provided to support the argument and illustration. To my knowledge, the perspective of confounders in curriculum reinforcement learning is novel."
            },
            "weaknesses": {
                "value": "* Toyish experiments. The experiments conducted are limited to grid-world environments, which significantly narrows the scope of application. For the conclusions to be generalized, the experiments should ideally be extended to a more diverse set of environments.\n* Scalability. The paper does not adequately address how the proposed method scales to continuous environment variables. This is a significant concern as many practical applications in continuous or high-dimensional environment variable space. By not tackling this challenge, the authors leave a gap in understanding the full potential and limitations of their method.\n* Missing related work:\n    * Hu, Xing, et al. \"Causality-driven Hierarchical Structure Discovery for Reinforcement Learning.\" Advances in Neural Information Processing Systems 35 (2022): 20064-20076.\n    * Cho, Daesol, Seungjae Lee, and H. Jin Kim. \"Outcome-directed Reinforcement Learning by Uncertainty & Temporal Distance-Aware Curriculum Goal Generation.\" arXiv preprint arXiv:2301.11741 (2023).\n    * Huang, Peide, et al. \"Curriculum reinforcement learning using optimal transport via gradual domain adaptation.\" Advances in Neural Information Processing Systems 35 (2022): 10656-10670."
            },
            "questions": {
                "value": "* In Algorithm 3, how is the actions sequence $\\{X_1, \\ldots, X_H\\}$ obtained in the first place? If we have this optimal action sequence before the learning starts, why do we need RL?\n* In Theorem 3, one of the conditions is that: For every $j=1, \\ldots, N-1$, actions $\\boldsymbol{X}^{(j)} \\subseteq \\boldsymbol{X}^{(j+1)}$. Does this mean that the transition must be deterministic and there only exists one unique optimal solution?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6200/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698775840017,
        "cdate": 1698775840017,
        "tmdate": 1699636675148,
        "mdate": 1699636675148,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bfEzo3VzGP",
        "forum": "hp4yOjhwTs",
        "replyto": "hp4yOjhwTs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6200/Reviewer_C4uc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6200/Reviewer_C4uc"
        ],
        "content": {
            "summary": {
                "value": "The paper delves into the challenges of designing a curriculum of source tasks to tackle a complex target task in the presence of unobservable confounding variables within the environment. The authors leverage the structural causal model framework (Pearl, 2009) to define causally aligned source tasks for a given target task. They propose a causally aligned curriculum that incorporates qualitative causal knowledge of the target environment, and they validate their approach through experiments in two confounded environments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper addresses an important problem in curriculum reinforcement learning, highlighting the potential negative impact of inadequate task space design on target task performance. \n\nThe writing quality is commendable, even though the paper features heavy notation due to its nature. The repeated use of the Sokoban game aids comprehension and clarity regarding the paper's contributions and claims."
            },
            "weaknesses": {
                "value": "The strategy for avoiding misaligned source tasks, as proposed in this work, relies on causal knowledge about the underlying data-generating mechanisms within the environment. This requirement limits the broader applicability of the strategy, making it dependent on domain-specific knowledge. The practicality of obtaining a causal diagram G for a general target task remains uncertain. \n\nIt remains unclear whether the proposed causal curriculum strategy can be extended to domains with continuous state and action spaces."
            },
            "questions": {
                "value": "Re. the Colored Sokoban example:\n\nIn the baseline, the context/task space is restricted to tasks where the box color remains constant throughout the game. However, considering a more extensive task space that encompasses all possible combinations of box colors might lead the state-of-the-art curriculum strategies to automatically select relevant/aligned tasks during training. In contrast, the proposed causal curriculum limits the task space to initial agent and box positions, with the environment determining box colors based on intrinsic randomness. \n\nFormally, let us denote the initial positions of the agent and the box as $(a_0^x, a_0^y)$ and $(b_0^x, b_0^y)$, respectively. Furthermore, let $c_t$ represent the color of the box at time step $t$, and designate $H$ as the maximum number of steps allowable in the game. In the results presented, the baseline methodology confines the context/task space to instances of the form $[a_0^x, a_0^y, b_0^x, b_0^y, c_0 = c_1 = \\cdots = c_H]$. The extensive task space contains all possible task configurations denoted as $[a_0^x, a_0^y, b_0^x, b_0^y, c_0, c_1, \\dots, c_H]$. The proposed causal curriculum imposes a task space restriction, characterized by tasks of the form $[a_0^x, a_0^y, b_0^x, b_0^y]$, delegating the selection of $[c_1, c_2, \\dots, c_H]$ to the environment, predicated upon intrinsic randomness. \n\nPlease provide your insights on this aspect."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6200/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699140909406,
        "cdate": 1699140909406,
        "tmdate": 1699636675042,
        "mdate": 1699636675042,
        "license": "CC BY 4.0",
        "version": 2
    }
]