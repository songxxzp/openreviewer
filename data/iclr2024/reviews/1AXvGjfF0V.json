[
    {
        "id": "yHLaxnOXdV",
        "forum": "1AXvGjfF0V",
        "replyto": "1AXvGjfF0V",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3414/Reviewer_svpT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3414/Reviewer_svpT"
        ],
        "content": {
            "summary": {
                "value": "A Chinese hallucination question-answering dataset named HalluQA is introduced for evaluating hallucination issues in large Chinese language models. The paper also provides a detailed description of the dataset's construction process and evaluation methodology.The experimental results demonstrate that all models exhibit non-hallucination rates of less than 70% on HalluQA, highlighting the dataset's challenging nature. The paper also discusses the primary types of hallucinations exhibited by different models and offers recommendations for model improvement."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "At present, there is a significant difference in capabilities between open-source Chinese large models and English large models. Meanwhile, there are fewer people focusing on hallucinations in Chinese large models. This paper introduces a Chinese dataset for hallucination benchmarking and evaluates and analyzes current Chinese large models. It is of great significance. Meanwhile,this paper has ample experiments, and the data presented in the text is also very sufficient."
            },
            "weaknesses": {
                "value": "1\u3001Based on the Figure 2 included in the work, question examples  look not natural. For example, in real world scenario, no one would ask a question like \u201c?\u201d  In short, the reviewer has doubt about the similarity between such generated queries and human written queries. Although I know this is to ask difficult questions to test LLM, is this kind of question really meaningful?\n\n2\u3001There is limited description of how the human filtering is performed. Is there any training process for those annotators? Quantitatively, how much data are removed in the process? Why are they being removed? Is there a list of examples for removed cases? Are more than one annotators working on the same datapoint? What is the agreement?\n\n3\u3001Regarding the evaluation issue, as a reviewer, what I would like to see more is a practical offline evaluation method. As we know, the GPT4 API is very expensive, and using GPT4 to evaluate the illusion of other LLMs does not seem feasible from a practical application perspective."
            },
            "questions": {
                "value": "Could you clearly introduce factors such as the price of using GPT4 evaluation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3414/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698817134763,
        "cdate": 1698817134763,
        "tmdate": 1699636292938,
        "mdate": 1699636292938,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "N0ZkCS8L1v",
        "forum": "1AXvGjfF0V",
        "replyto": "1AXvGjfF0V",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3414/Reviewer_DCpV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3414/Reviewer_DCpV"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a benchmark called HalluQA, which aims to measure the hallucination phenomenon in Chinese large language models. HalluQA consists of meticulously designed adversarial questions that cover various domains and take into account Chinese historical culture, customs, and social phenomena. The authors identify two types of hallucinations: imitative falsehoods and factual errors, and construct adversarial samples accordingly with LLMs. An automated evaluation method using GPT-4 is designed to judge whether a model's output is hallucinated."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper built an adversarial evaluation benchmark aligned with the Chinese-specific context"
            },
            "weaknesses": {
                "value": "The details of human expert evaluations are not provided in this paper, so it is difficult to determine the reliability of its high correlation with GPT-4 evaluations. Furthermore, a richer variety of LLMs can be used to generate examples, ensuring coverage of various forms of hallucination and fairness of evaluations."
            },
            "questions": {
                "value": "1. In Figure 4, the non-hallucination rate performance of GPT-4 is not optimal. Is it appropriate to use it for evaluating the existence of potential issues?\n2. In the GPT-4 automated evaluation method, if the temperature of the GPT-4 evaluator is set to 0, are its outputs still random? And how does the voting part work if the outputs are deterministic?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3414/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698972412469,
        "cdate": 1698972412469,
        "tmdate": 1699636292865,
        "mdate": 1699636292865,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3RC4ee12gm",
        "forum": "1AXvGjfF0V",
        "replyto": "1AXvGjfF0V",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3414/Reviewer_fQ9L"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3414/Reviewer_fQ9L"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a benchmark named HalluQA to measure the hallucination phenomenon in Chinese LLMs. HalluQA contains 450 adversarial questions, covering various Chinese historical cultures, customs, and social phenomena. Both imitative falsehoods and factual errors are considered. GPT-4 is integrated into an automated framework to judge whether a model output is hallucinated. Extensive experiments on 24 large language models are presented, and18 achieved non-hallucination rates lower than 50%, showing that HalluQA is quite difficult. Some insights on causes are also provided."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The authors made serious efforts in conducting a comprehensive study on hallucinations in Chinese LLMs.\n\n2. Some interesting insights are provided. \n\n3. It is important to establish some benchmark for studying hallucinations in Chinese LLMs, and this work is quite timely in this sense."
            },
            "weaknesses": {
                "value": "The novelty of this work is not very clear to me. The results are kind of expected."
            },
            "questions": {
                "value": "1. Can the authors clarify the unique novelty of this work? On the conceptual and technical levels?\n\n2. Is any part of the results particularly surprising to the authors?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3414/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699289813728,
        "cdate": 1699289813728,
        "tmdate": 1699636292800,
        "mdate": 1699636292800,
        "license": "CC BY 4.0",
        "version": 2
    }
]