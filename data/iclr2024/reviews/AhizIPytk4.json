[
    {
        "id": "DrODpbuLWX",
        "forum": "AhizIPytk4",
        "replyto": "AhizIPytk4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission742/Reviewer_osvs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission742/Reviewer_osvs"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a novel dataset (IMAGENETCT-9K) containing 9,262 CT volumes along with their respective voxel-level masks. Furthermore, the paper pretrains various models on this dataset and fine-tunes it on other publicly available benchmarks, achieving SOTA performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The dataset appears to be comprehensive and holds great promise. I believe that making this dataset and the pretrained weights publicly available can contribute to advancements in the field."
            },
            "weaknesses": {
                "value": "The weakness could relate to the details of the pretraining strategy. Typically, image-wise [1, 2] or pixel-wise [3] pre-training relies on the InfoNCE loss for clustering embedding samples in the latent space, rather than directly applying penalties based on labels via cross-entropy loss. It would be more interesting to observe results achieved through category-guided InfoNCE loss [4, 5] pre-training using this dataset.\n\nAdditionally, there exist many promising domain transfer methods, yet the paper appears to lack exploration in this area. The current approach appears to be straightforward fine-tuning on other datasets, such as TotalSegmentator and JHH.\n\n[1] Self-training with Noisy Student improves ImageNet classification\n\n[2] Unsupervised Learning of Visual Features by Contrasting Cluster Assignments\n\n[3] Dense Contrastive Learning for Self-Supervised Visual Pre-Training\n\n[4] Supervised Contrastive Learning\n\n[5] Exploring Cross-Image Pixel Contrast for Semantic Segmentation"
            },
            "questions": {
                "value": "I don't have a lot questions since the paper's primary contribution lies in the dataset. I would like to confirm whether the dataset and the pre-trained weights will be made accessible to the public."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Responsible research practice (e.g., human subjects, data release)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "I missed the data privacy of the patients throughout the processes of data collection, storage, and sharing in the paper. I have observed a 'pending' status in Table 5 of your Appendix A and I believe it is essential for the authors to address this issue appropriately."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission742/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission742/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission742/Reviewer_osvs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission742/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698502902760,
        "cdate": 1698502902760,
        "tmdate": 1699636001221,
        "mdate": 1699636001221,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "N3RHFQ9taA",
        "forum": "AhizIPytk4",
        "replyto": "AhizIPytk4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission742/Reviewer_mSzX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission742/Reviewer_mSzX"
        ],
        "content": {
            "summary": {
                "value": "The paper investigates the transfer learning ability of self-supervised and fully-supervised foundational models for medical image segmentation. For this purpose, the authors collect a very large, labeled 3D CT scans to be used for pre-trainining. The results show that supervised models have better transfer learning ability compared to self-supervised counterparts, by also saving a significant GPU time. Moreover, since the collected data is very diverse and large, it generalizes well to OOD dataset even surpasses the performance of the models trained on the OOD datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Lack of large annotated datasets is a huge problem in medical imaging due to the cost of collecting labelled data. Publicly available pretrained models on such large datasets are huge assets for medical imaging. \n\n- The paper presents extensive experiments showing the benefit of supervised pre-training compared to the unsupervised counterparts."
            },
            "weaknesses": {
                "value": "- UniverSeg [1] is another paper that trains networks on a very large dataset, 22K scans, which is even larger than this paper. Although Arxiv version of UniverSeg is available since April 2023, I see this work and UniverSeg as concurrent works since UniverSeg is recently presented in ICCV. However, I still think that mentioning UniverSeg and discussing the similarities/differences in the final version would be useful.\n\n[1] https://universeg.csail.mit.edu/\n\n- How do the models trained on the collected large CT dataset generalize to novel modalities such as MRI?"
            },
            "questions": {
                "value": "Please address my concerns in the weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission742/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698872212794,
        "cdate": 1698872212794,
        "tmdate": 1699636001154,
        "mdate": 1699636001154,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rYEpxAPsYf",
        "forum": "AhizIPytk4",
        "replyto": "AhizIPytk4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission742/Reviewer_bY7Q"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission742/Reviewer_bY7Q"
        ],
        "content": {
            "summary": {
                "value": "The authors collected publicly available and private CT data, and obtained over 9000 CT data cases by manually correcting pseudo-labels. These data can support the segmentation of 32 organs and a small number of tumors. Through experiments, it was discovered that the supervised pre-training method outperforms self-supervised pre-training and supervised pre-training with fewer samples."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The authors collected over 9000 cases of publicly available and private 3D CT datasets, and manually corrected annotation errors, making it the largest dataset for multi-organ segmentation currently available."
            },
            "weaknesses": {
                "value": "1. The paper explores the transferability of supervised learning by combining multiple 3D CT segmentation datasets. Similar work has been done in various fields, such as training various tasks and data in computer vision, which has shown improved results across tasks. This paper incorporates 3D CT data and tasks, with the only difference being the collection of more publicly available and private data, without bringing new insights or technological innovations to the community.\n2. The conclusion that supervised pre-training has an advantage over other pre-training methods in 3D medical imaging is generalized to general 3D vision tasks in an inconsistent manner. The introduction discusses pre-training strategies for 3D vision tasks, but the experiments are all conducted on medical images. It is well known that there are significant differences between medical images and natural images, and whether the experimental conclusions on 3D CT data can be extended to other 3D vision tasks is not explored in the paper.\n3. The fused dataset of over 9000 CT cases collected by the authors includes segmentation of 32 organs and tumors, but the evaluation in the experimental section focuses more on organs, lacking an evaluation of tumor segmentation performance. Comparatively, organ segmentation is less challenging in terms of generalization, while tumor segmentation is more complex. In the external dataset, more focus should be given to tumor segmentation, as it is more susceptible to a series of generalization issues caused by differences in populations, devices, and diseases in practical application scenarios.\n4. Unfair comparison in the experimental section is a fatal flaw. Although the authors claim that collecting 9000 data is their contribution, the same 9000 data were not used for pre-training when comparing with other self-supervised/supervised pre-training methods. Therefore, it is not rigorous to conclude that SPT is superior to other supervised/self-supervised methods.\n5. The results of fine-tuning SPT on 63 novel classes are not impressive. Although there is no comparison with totalsegmentator, the performance of totalsegmentator trained on 1000 data seems to surpass what is reported in Table 4. For example, totalsegmentator can achieve over 95% Dice on iliopsoas, while it is less than 90% in the paper."
            },
            "questions": {
                "value": "1. The paper does not clearly explain how the three expert radiologists collaborated to clean the data, including how they worked together, established uniform standards, and how they corrected tumor masks in the pseudo labels. What was the time cost involved, and so on?\n2. The \"novel datasets\" claimed in Table 3 is inappropriate. It should be referred to as the external dataset."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "This paper uses internal medical data for training, so it may need ethical approval for use."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission742/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission742/Reviewer_bY7Q",
                    "ICLR.cc/2024/Conference/Submission742/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission742/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698921491329,
        "cdate": 1698921491329,
        "tmdate": 1700749072834,
        "mdate": 1700749072834,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "T6KM3fkRwU",
        "forum": "AhizIPytk4",
        "replyto": "AhizIPytk4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission742/Reviewer_esgA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission742/Reviewer_esgA"
        ],
        "content": {
            "summary": {
                "value": "This paper evaluates supervised and self-supervised feature learning approaches on 3D CT data. The work first contributes a dataset of CT scans (9000 samples) of different organs, and taken from different hospitals. Next, the authors train different segmentation learning models on this data, both supervised and self-supervised, and report general trends such as sample size efficiency and transferability. The main conclusion is that there is a benefit to having supervised 3D datasets, and that self-supervision can be inferior despite the recent successes of self-supervised models in the vision community. The authors will make the dataset and trained models public."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The dataset that the authors collected seems like it will be a valuable resource for researchers, particularly in medical imaging. The data can serve to train general-purpose 3D features and comes with annotations.\n\n- I do like the point that self-supervision has its limits and that there is a benefit to simply having large supervised data. This is particularly relevant with the current interest in the vision community on self-supervised learning representations.\n\n- Experiments are reasonable and include sufficient prior models."
            },
            "weaknesses": {
                "value": "- The title may be a little misleading. If I understood it correctly, the word \"transfer\" in the title is not referring to transfer learning in this case, but simply asking whether supervision is also good for 3D data as it has been for 2D data. When first reading the title, I thought you were exploring transferring 2D supervised models to 3D tasks. Others may also make that incorrect assumption.\n\n- The use of the word \"ImageNet\" in the dataset name may want to be reconsidered. Besides being a large dataset with a variety of anatomy, the link to ImageNet is a bit weak and may also suggest properties that the dataset does not have (e.g., per-image classification labels). \n\n- CTs are very specific types of 3D data -- it's difficult to make a claim for all 3D data from these experiments alone. I think the paper could be improved by focusing the message more narrowly, perhaps on medical imaging segmentation. \n\n- To me, the results are not surprising -- if you have a such a large dataset with rich segmentation labels, then it should do better than self-supervision. This is in contrast to classification, where this may not be true. I think the main point of the paper should be that this is a new dataset that will be valuable to the community, not that rich supervision is useful in segmentation."
            },
            "questions": {
                "value": "1. Is there anything surprising from the results? I think it is clear that if you have rich segmentation labels you can learn a better segmentation model than via self-supervised learning."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission742/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698930927208,
        "cdate": 1698930927208,
        "tmdate": 1699636000994,
        "mdate": 1699636000994,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CauaXiMXvW",
        "forum": "AhizIPytk4",
        "replyto": "AhizIPytk4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission742/Reviewer_fMfJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission742/Reviewer_fMfJ"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a new large-scale computed tomography (CT) dataset for medical image segmentation, which is so-far the one with the highest number of annotated scans. The authors, employing this benchmark, draw several insights where most of them are revealed for the first time. More specifically, the authors show that supervised pre-training is more effective and efficient compared with self-supervised counterpart given similar training circumstances. Their released models can effectively serve as a foundation model for transfer learning, helping to reduce the computational load and improve the segmentation accuracy."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I really enjoy reading this paper and the contribution it brings. First, the authors would release a large-scale volumetric segmentation dataset with unprecedented number of pixelwisely labeled ground truth. This dataset comes from some publically available dataset and self-constructed ones with semi-annotated tools and interactive segmentation with radiologists. Second, the paper concluded on the debate of whether self-supervised or supervised pre-training lead to better performance and data efficiency. This debate had not be resolved without the invention of a fully-annotation dataset of such scale. Third, the authors release their pre-trained models so that one can easily fine-tune the model efficiently. This can also have a good impact for the whole community. \n\nThis paper is well structure and written. The notation is clear, and the experimental setups are carefully noted. \n\nExperimental results are intensive and convincing. I checked the attached code and it seems to be solid."
            },
            "weaknesses": {
                "value": "I do not remark any major issue as the drawback of this paper."
            },
            "questions": {
                "value": "I haven't had many questions regarding this paper. \n\n- Could the authors explain why the performance \"scratch\" out-performed most of the pre-training method in Tab. 2?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission742/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698995022421,
        "cdate": 1698995022421,
        "tmdate": 1699636000928,
        "mdate": 1699636000928,
        "license": "CC BY 4.0",
        "version": 2
    }
]