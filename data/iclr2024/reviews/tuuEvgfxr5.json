[
    {
        "id": "QvWxBOZXrv",
        "forum": "tuuEvgfxr5",
        "replyto": "tuuEvgfxr5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4804/Reviewer_nxjv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4804/Reviewer_nxjv"
        ],
        "content": {
            "summary": {
                "value": "The paper discusses the topic of Bayesian pseudo-coresets: a type of distillation for probabilistic models, where a synthetic pseudo-dataset is selected such that the posterior conditioned on them is close to that conditioned on the full dataset. The original optimization for Bayesian pseudo-coreset attempts to minimize a discrepancy measure between the pseudo-posterior $\\pi_{\\tilde{x}}$ and real posterior $\\pi_{x}$, but each gradient step w.r.t. $\\tilde{x}$ involves inefficient re-sampling of $\\pi_{\\tilde{x}}$. The paper proposes to replace the canonical KL divergence measure with the contrastive divergence (CD), such that one could approximate $\\pi_{\\tilde{x}}$ in a biased fashion via a warm-start simulation of a short MCMC chain. The paper presents sensible empirical results showing that the pseudo-corsets selected via CD minimization yields better results in the distilled dataset."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- Soundness: The problem of selecting Bayesian pseudo-coresets is notably similar to training an energy-based model. The method of using contrastive divergence in replacement of KL divergence is well-established and justified. \n- Experiments: The paper presents better results compared to other pseudo-coreset selection methods. \n- Presentation: The paper is well-written and easy to understand, with a rich literature survey that I find helpful."
            },
            "weaknesses": {
                "value": "In lieu of the problem of a manual inflation of test accuracy, I'm modifying my view of this paper from the standpoint of integrity. \n\nHere is my original characterization:\n\nI am mainly concerned about the significance of this contribution: the only contribution of this paper is the application of contrastive divergence to this specific problem, which is a natural and incremental next step compared to previous works. \n\nMoreover, there are numerous other divergence measures that could be applied to this problem, many of which share the same advantage as contrastive divergence. I believe that the wholesale introduction and investigation of divergence measures with no / little reliance on long MCMC simulation of $\\pi_{\\tilde{x}}$ is a significant way to bolster the significance of the paper. I lay out my thoughts in the \"questions\" section."
            },
            "questions": {
                "value": "Barp et al. [1] include divergence measures of such sort. As mentioned by this paper, contrastive divergence has been shown to be an approximation of the score-matching divergence, and score-matching divergence can be further smoothed/kernelized into kernel Stein discrepancy - a discrepancy that measures the divergence between 2 distributions, where 1 of them can be represented by the score function. In my opinion, while the introduction of CD can be seen as trivial, a more comprehensive look is an interesting and significant next step for tackling this question. \n\n1. Barp A, Briol FX, Duncan A, Girolami M, Mackey L. Minimum Stein Discrepancy Estimators. In: Advances in Neural Information Processing Systems."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Research integrity issues (e.g., plagiarism, dual submission)"
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4804/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4804/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4804/Reviewer_nxjv"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "As pointed out by Reviewer N1MW, the paper's implementation seems to give their method an unfair advantage."
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4804/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698620663057,
        "cdate": 1698620663057,
        "tmdate": 1699636463670,
        "mdate": 1699636463670,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "41eigQnKZL",
        "forum": "tuuEvgfxr5",
        "replyto": "tuuEvgfxr5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4804/Reviewer_N1MW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4804/Reviewer_N1MW"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method that distills a synthetic dataset (Bayesian Pseudocoreset) whose NN weight posterior approximates the posterior over neural network weights, conditional on an entire training dataset. The proposed method approximates the max. likelihood (=forward kl between target posterior and pseudocoreset posterior) with the contrastive divergence objective and applies several approximations to make the procedure more computationally tractable."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The authors provide plenty of empirical evidence with their work, with a lot of additional results containing experimentation with different architectures, parameterizations and on OOD data.\n- Related work section gives a very good and extensive summary of other works in the field."
            },
            "weaknesses": {
                "value": "- The contributions of this paper are very limited, especially when compared to [1]. The BPC-fKL objective in [1] is very closely related to contrastive divergence, something which is even explained in section 3.3 of [1]. The authors of this paper claim that their method is more efficient because it samples from the pseudocoreset posterior with a finite number of MCMC steps initialized from a sample of the target posterior. However, when comparing their Algorithm 1 to BPC-fKL as described in Algorithm 1 of [1] (and their code), both methods are equivalent apart from the proposed method using noised SGD as the short horizon MCMC kernel instead of SGD in BPC-fKL and not adding any noise to $\\theta^-$.\n\n- I see some issues with the provided code, where the code does not align with what is presented in the paper. Line 11 of Algorithm 1 for example is computed with an additional `meta_loss` term in line 464 and 466 of `distill.py`. Furthermore, no noise is added to the pseudocoreset sampler in the code (line 402 of `distill.py`), in contrast to line 9 of Algorithm 1 in the appendix. Additionally, no noise is added to $\\theta^+$ (line 351 of `distill.py`), in contrast to line 5 of Algorithm 1. Consequently, the code actually shows an procedure which is even more similar to BPC-fKL from [1] than described in the paper.\n\n- As a result of the mentioned observations in the code, and how the procedure in the code is almost the same as BPC-fKL, I have some doubts about the provided experimental results. Specifically, the margin with which the proposed algorithm beats BPC-fKL seems rather large to me, considering how similar the procedures are. Furthermore, I want to point out the reported accuracy of BPC-fKL in Table 1 for MNIST (sghmc), which decreases to 40.63% for ipc=50, after being 82.98 for ipc=1. In other experiments, I also see the baselines decrease their accuracy for ipc=50 over ipc=10, something which should not happen.\n\nIf these weaknesses cannot be refuted by the authors, I would consider this a clear reject.\n\n[1] Kim, B., Choi, J., Lee, S., Lee, Y., Ha, J. W., & Lee, J. (2022). On divergence measures for bayesian pseudocoresets. Advances in Neural Information Processing Systems, 35, 757-767."
            },
            "questions": {
                "value": "- What are the specific differences between the proposed method and BPC-fKL (as described in Algorithm 1 of [1])?\n- Please comment on some of my comments on the code, mentioned above. For example, what is `meta_loss`? \n- Why does the accuracy for some of the baselines decrease from ipc=10 to ipc=50 and does not decrease for your proposed method?\n- Why does the proposed method perform better with such a large margin over BPC-fKL while being almost equivalent? Can you explain this?\n- How does the performance compare for each experiment to a random subset of the data (an \u2018untrained\u2019 initialized coreset)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4804/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4804/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4804/Reviewer_N1MW"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4804/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698779529619,
        "cdate": 1698779529619,
        "tmdate": 1699636463456,
        "mdate": 1699636463456,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OI7l63NJtH",
        "forum": "tuuEvgfxr5",
        "replyto": "tuuEvgfxr5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4804/Reviewer_m79B"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4804/Reviewer_m79B"
        ],
        "content": {
            "summary": {
                "value": "Bayesian pseudo-coresets are a promising methodology for scaling Bayesian inference to large datasets. For this, the inclusive KL divergence has been a dominant objective function but is hard to minimize and obtain gradients. Instead, the paper proposes to use contrastive divergence as an alternative."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* To my understanding, the idea of using the contrastive divergence for pseudo-coresets is novel, and the motivation is sound.\n* The writing is good, with a thorough study of the literature."
            },
            "weaknesses": {
                "value": "* In terms of technique, applying the contrastive divergence to pseudocoresets is rather incremental. And the theoretical analysis and insight is limited. To increase the impact of the idea, sufficient empirical analysis should have follows. However...\n* During review, an issue was raised by a separate reviewer regarding the code implementation. In particular, it appears that the authors manually increase the test accuracy in Line 420 of utils.py. Due to this, the reliability of the experimental results is unclear. Therefore, I did not review the experimental section at this point.\n\n### Minor Comments\n* I think an important related work is [1], which introduces contrastive divergences for variational inference. I think this paper could be discussed together with Hinton 2002. \n* Section 1 second paragraph: \"the KL-divergence between the (optimal) coreset posterior and true posterior increases with data dimension. Meaning that for large data dimension, even with the optimal coresets, the KL-divergence is far from optima.\" I'm not sure if this is a good motivation. The fact that the KL-divergence at the optimum increase with dimensionality does not imply that the solution will be worse with dimension, because the KL values with different dimensions are not exactly comparable.\n* Section 1 fifth paragraph first sentence: This is the first mention of the contrastive divergence; cite the relevant papers.\n* Section 2.1 first paragraph: This paragraph is too general in scope, in my opinion. It does not necessarily build a context specific to the proposed methodology. Therefore, I think this paragraph could be removed entirely or shortened to a few sentences. \n* Section 2.1 first paragraph, \"Recent methods ... black box approach\": Black box variational inference was also independently developed by [2] around the same time as Ranganath *et al.* 2014.\n\n### References\n1. Ruiz, Francisco, and Michalis Titsias. \"A contrastive divergence for combining variational inference and mcmc.\" International Conference on Machine Learning. PMLR, 2019.\n2. Titsias, Michalis, and Miguel L\u00e1zaro-Gredilla. \"Doubly stochastic variational Bayes for non-conjugate inference.\" International conference on machine learning. PMLR, 2014."
            },
            "questions": {
                "value": "no questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4804/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4804/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4804/Reviewer_m79B"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4804/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698786010135,
        "cdate": 1698786010135,
        "tmdate": 1699636463362,
        "mdate": 1699636463362,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8WS08iGycS",
        "forum": "tuuEvgfxr5",
        "replyto": "tuuEvgfxr5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4804/Reviewer_T54i"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4804/Reviewer_T54i"
        ],
        "content": {
            "summary": {
                "value": "The authors tackle the problem of learning Bayesian Pseudocoresets, a representation of the posterior based on a (small) set of learned datapoints that match the posterior of the full match as well as possible, akin to inducing points in Gaussian Processes.\nIn their paper, they focus on a fundamentally interesting inference approach, namely contrastive divergence.\nThe authors use CD to propose a loss function to train their corsets and propose this both allows them to learn well-performing coresets as well as reduces the inference complexity and \"heaviness\" inherent in many of these coreset algorithms.\n\nIndeed, empirical performance seems to be good across a wide range of tasks and the authors share results and visualizations of their learned coresets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "A key strength in this paper is empirical:\nThe empirical results in this paper are strong for this domain in a purely quantitative way.\n\nAlso, the idea of utilizing contrastive divergence per se is interesting and appealing.\nElecting the reverse KL, KL(p||q), as a starting point also seems to be a good call since it leads to simpler objectives in this case."
            },
            "weaknesses": {
                "value": "I have a few core problems with the setup, theory, attribution, and execution of this paper.\nSome of them it inherits from other literature, so I will elaborate.\n\nSome of the statements are just mathematically incorrect.\nSpecifically, the authors both in their background section as well as inter technique keep saying they avoid sampling from the data-posterior by performing variational inference.\nIt sounds good, but that's not what the objective calls for. \nThe objective the authors aim to optimize is KL(p||q_cs), for q_cs denoting the coreset posterior.\nIf the authors first perform variational inference on the full data to arrive at a variational family q, then they factually compute KL(q||q_cs).\nThis objective no longer cleanly globally approximates what they claim it does, since they have not accounted for that change and are not training all these approximations jointly in such a way as to guarantee a convergence. This happens in multiple stages of pre-training and freezing pieces here, which is just not sufficiently principled for work suggesting a new inference algorithm.\nThey inherit this problem from Kim et al 2022, which is unfortunate, since the variational inference plug-in to the global posterior is also factually incorrectly applied in that paper without a correction term.\nThe problem gets compounded by the fact that the authors do a lot of pipelining for their inference.\nThey retrain MAP weights for VI, plug them in in lieu of the true posterior, and then run their Monte Carlo trajectories over the full data rather than the coreset data, and plug in the coreset data when convenient to evaluate it.\nOverall there are a lot of inconsistencies here between what the math aims to achieve and what is actually happening.\n\nAs far as the sampling from the coreset posterior issue is concerned, there is a Neurips paper from 2022 \"Black Box Coreset Variational Inference\" by Manousakas, Ritter, and Karaletsos, which tackles exactly these problems and cleans ups the theory to be able to do VI on this. Not only do the authors here not cite this as clearly related work, which could be an omission that is forgivable and easily fixed, but they do not follow the insight that it is necessary to maintain guarantees and do the math work that shows how these objectives still optimize the desired quantity and how the approximations made impact that learning.\n\nAnother severe problem in this paper is that the authors just write up their contrastive divergence loss, but do not derive in main paper or appendix how this still optimizes their desired objective. While I have re-derived CD myself in the past and understand its link to stochastic maximum likelihood, I find the paper entirely insufficiently technically disseminated without explaining how this links to their desired objective, KL(p||q_cs). I'd expect to see a derivation that unambiguously shows that the difference of two divergence achieves what we want, so that the reader can go beyond just faith that this is true. That is a pity because evoking CD is interesting per se, but not well executed here.\n\nLast, the fact that the authors alternative between training different parts of their model not he full data (i.e. VI AND their Monte Carlo chains) and then evaluate their coresets just for the gradient step is hard to digest empirically: \nI currently do not know if I can trust their good results, because it is unclear if they even came to pass through application of the objective, or if initializing with real data and full data MCMC yielded good enough solutions that then following their objective just didn't break it."
            },
            "questions": {
                "value": "I would really appreciate if the authors could clarify their experiments and the details of the many choices for the \"pipelining\" of pertaining things on real data vs coresets and hopefully can clean that up. It would bring back some more understanding of the good performance here, which currently I cannot help but be suspicious about but would be very happy to link to the actual objective -even a non-principled one-  if given the evidence.\n\nOn the objectives side, while I think there are many severe weaknesses in the core mathematical setup and dissemination as noted above that will be hard to overcome in a rebuttal, but I would still really be curious if the authors could share how CD solves the objective they want to solve, since that abstraction can still be correct if they derive it well independently of the problems with the VI plugins I mentioned.\nIt would significantly improve the presentation of this paper and up level that piece, since CD is not an obvious choice and something I credit the authors with as a creative choice, but that credit is tainted by lack of derivation.\n\nLastly, the authors would do well to cite the appropriate work that has tackled some the core challenges in coreset inference that they chose to \"sweep under a rug\" . Even if they choose not to tackle those themselves, it is important for the reader to not just hide these issues but openly explain choices where the execution is not based no the proposed math but maybe loosely inspired."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4804/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698983128082,
        "cdate": 1698983128082,
        "tmdate": 1699636463285,
        "mdate": 1699636463285,
        "license": "CC BY 4.0",
        "version": 2
    }
]