[
    {
        "id": "5f2hIL139x",
        "forum": "MIuimtOu0T",
        "replyto": "MIuimtOu0T",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4352/Reviewer_QvRs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4352/Reviewer_QvRs"
        ],
        "content": {
            "summary": {
                "value": "This paper targets on the fairness problem in knowledge distillation. The proposed method, BIRD, collects the feedback from the student model through a meta-learning-based approach and selectively distill teacher knowledge. BIRD is orthogonal with existing methods and computationally effective. Extensive experiment results show that BIRD can enhance the fairness remarkably."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper is overall well-written and easy to follow. \n2. This paper targets on an interesting problem, the fairness in knowledge distillation. \n3. The proposed method is orthogonal with existing methods, enhancing the fairness remarkably."
            },
            "weaknesses": {
                "value": "1. I am curious about the results when we apply BIRD to existing methods, and test their performance according to the conventional criteria? eg. The Top-1 and Top-5 accuracy on CIFAR-100 and ImageNet. Will it degrade when pursuing fairness? I want to see more comparison results."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4352/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4352/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4352/Reviewer_QvRs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4352/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698131230526,
        "cdate": 1698131230526,
        "tmdate": 1699636407675,
        "mdate": 1699636407675,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lFqd34t429",
        "forum": "MIuimtOu0T",
        "replyto": "MIuimtOu0T",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4352/Reviewer_qpK3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4352/Reviewer_qpK3"
        ],
        "content": {
            "summary": {
                "value": "This study introduces a \"student-aware selective feature distillation\" approach, drawing inspiration from meta-learning, which enables the teacher to impart unbiased information effectively to the student."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The authors have lucidly articulated both the problem definition and the corresponding solution formulation.\n\n2. Distinguishing itself from prior research, this study emphasizes adapting the teacher's predictions to remain unbiased. This adjustment, in turn, facilitates the training of a student that naturally inherits this unbiased character due to the teacher's modified features.\n\n3. The introduction of a meta-learning inspired transfer approach is both well-conceived and aptly presented, offering a compelling solution for unbiased (or fair) knowledge transfer.\n\n4. In the results section, the authors comprehensively address pivotal concerns, including the problem's justification (5.2.1), the efficacy of their proposed method (5.2.2), the technique's adaptability across various KD frameworks (5.2.3), and insightful ablation studies that dissect various components of their framework (5.2.4 and 5.2.5)."
            },
            "weaknesses": {
                "value": "1. While the paper's overarching framework appears to draw heavily from the \"learning how to teach\" paradigm (as detailed in Park et al., 2021; Liu et al., 2021; Zhou et al., 2021), its overall contribution may be perceived as somewhat incremental. This perception arises from the adaptation of a pre-established framework to address knowledge distillation. Despite this, the proposed solution stands out due to its technical novelty and apt alignment with the problem statement. A deeper dive by the authors into the technical distinctions between their work and prior studies would further solidify their contribution.\n\n2. For the benefit of practitioners, the authors might consider expanding their ablation section to detail not just the memory overhead, but also the time overhead associated with the distillation process. This is particularly relevant given the well-documented time-intensive nature of meta-learning-based approaches."
            },
            "questions": {
                "value": "Can the authors detail how the CLIP-Resnet-50 to ResNet 18 KD is performed. How is the KD distillation performed in the absence of the class-probability distribution of CLIP?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4352/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4352/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4352/Reviewer_qpK3"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4352/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698718391012,
        "cdate": 1698718391012,
        "tmdate": 1699636407566,
        "mdate": 1699636407566,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GTbylmdE0G",
        "forum": "MIuimtOu0T",
        "replyto": "MIuimtOu0T",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4352/Reviewer_UAGV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4352/Reviewer_UAGV"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates making student models with less gender & racial bias in knowledge distillation. The meta-learning framework is used to achieve this goal, and the experiments are done on CelebA and UTK datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The motivation of this paper is sound, fairness in KD is indeed an important topic in the era of large models. This paper is well-presented."
            },
            "weaknesses": {
                "value": "1. This paper proposes a meta-learning framework to solve the fairness issue in KD. I don't see why meta-learning can be used to resolve fairness here; the intuition and motivation are unclear. \n\n\n2. The experiments are mainly conducted on CelebA and UTKFace, two small datasets. I think it is necessary to evaluate a larger benchmark, such as a dataset that a mixup of gender/racial images with other non-biased images, i.e., animals,  to verify the effectiveness of the proposed method on a larger scale setting. The same issue is on the choice of student models; more models should be evaluated, such as ViT (not as a teacher but as a student model).\n\n3. It is disappointing that this paper is only focused on the fairness of classification. The application of classification is very limited and has been extensively explored over the past decades. I think it is essential to test KD fairness on more settings, such as multi-turn vqa. Otherwise, I suggest changing the title to a more specific topic, i.e. \"Towards Fair Knowledge Distillation on Image Classification\"."
            },
            "questions": {
                "value": "See weakness. My main concerns are the following: 1) it is not clear why the meta-learning framework is effective for **fairness** (not the overall results.), 2) the experiments are insufficient regarding the size of the dataset, the model size of the students, and the tasks."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4352/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4352/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4352/Reviewer_UAGV"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4352/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698733145891,
        "cdate": 1698733145891,
        "tmdate": 1699636407219,
        "mdate": 1699636407219,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ap1mqxYEbm",
        "forum": "MIuimtOu0T",
        "replyto": "MIuimtOu0T",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4352/Reviewer_qaWv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4352/Reviewer_qaWv"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new fairness-aware KD method, BIRD (BIas-awaRe Distillation), for distilling foundation models. The main idea is to use a proposed FAIRDISTILL operator to collect feedback from the student through a meta-learning-based approach and selectively distill teacher knowledge.  This method can be used with several existing base KD methods for improved performance. Extensive experiments across three fairness datasets show the efficacy of the proposed method over other counterparts."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. As FMs prevail day by day, distilling FMs is more important as well. The fairness problem of the distilled model is also of interest. This paper contributes to this axis.\n\n2. The idea of selecting part of the teacher's feature for debiased distillation under the meta learning framework is technically sound and intuitive.\n\n3. Empirically, the method is effective (\"Results show that BIRD improves the fairness of the knowledge distillation framework by 40.91%\") and it is ready to be used along with existing KD methods to enhance the fairness of the distilled student."
            },
            "weaknesses": {
                "value": "1. One major problem with the proposed method is that it needs the internal features of the teacher model for distillation (Eq. 4). However, as the paper mentioned in the motivation, most of the FMs in the real world only provide APIs. I.e., their features are barely accessible. This limitation seems to undermine the practical value of the proposed method severely.\n\n2. Another concern is that the fairness is improved but in many cases at a price of degraded accuracy. E.g., CLIP- ResNet50 with UTKFace in Tab. 1\uff0c CLIP-ViT-32 \u2212\u2192ResNet18 with UTKFace in Tab. 2, CLIP-R50 \u2212\u2192ResNet18 with UTKFace in Tab. 2. Namely, the proposed method is not very strong. The fairness issue may be a concern, while accuracy also matters. \n\nA side concern is that, as seen above, the method does not perform well on the UTKFace dataset. Why? \n\n3. Presentation: Some of the results are mistakenly highlighted. In Tab. 1, \u2206mean-DEO, the highlighted results are sometimes not the best, which are quite confusing. \n\n4. Minor issues.\n- This paper seems quite relevant: https://aclanthology.org/2022.gebnlp-1.27.pdf. It reports a similar observation to Sec. 5.2 that KD amplifies biases."
            },
            "questions": {
                "value": "NAN"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4352/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698898963188,
        "cdate": 1698898963188,
        "tmdate": 1699636406493,
        "mdate": 1699636406493,
        "license": "CC BY 4.0",
        "version": 2
    }
]