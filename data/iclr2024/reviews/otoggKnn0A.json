[
    {
        "id": "Ok97AfRq2Q",
        "forum": "otoggKnn0A",
        "replyto": "otoggKnn0A",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9109/Reviewer_kr7t"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9109/Reviewer_kr7t"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a dataset for fine-grained hand action recognition in kitchen scenes. It include three tasks, i.e., detection, recognition, and domain generalization."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.Dataset contribution is meaningful to the community.  \n2.Human action understanding is worth studying.  \n3.This paper is easy to read.  \n4. 'Verb' categories are abundant."
            },
            "weaknesses": {
                "value": "1. There are also fine-grained human action datasets, such as fine-gym. \n2. In this dataset, action categories are highly related with objects. Then object bias may appear, which means object will help the model understand actions.  I wonder about the task difficulty. Furthermore, it may cause researcher focus on objects rather than actions.    \n3. Following the point 2, can you just classify the 'verb' category?\n4. No correponding baseline model is proposed."
            },
            "questions": {
                "value": "Refer to the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9109/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697620455646,
        "cdate": 1697620455646,
        "tmdate": 1699637146362,
        "mdate": 1699637146362,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SfHrBXz7pl",
        "forum": "otoggKnn0A",
        "replyto": "otoggKnn0A",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9109/Reviewer_9rBA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9109/Reviewer_9rBA"
        ],
        "content": {
            "summary": {
                "value": "The paper presents \"FHA-Kitchens\", a new dataset tailored for fine-grained hand action recognition within kitchen environments. This proposed dataset emphasizes human hand interactions in the kitchen, providing a more detailed perspective than traditional datasets that often focus on broader, full-body actions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The FHA-Kitchens dataset is meticulously curated, with videos spanning diverse dish preparations, offering a holistic view of kitchen interactions.\n2. The comprehensive benchmarking across three distinct paradigms provides valuable insights into the dataset's utility and the challenges inherent in hand action recognition."
            },
            "weaknesses": {
                "value": "1. While the dataset's focus is niche, the total number of videos (30) might be considered limited, especially when compared to other large-scale datasets.\n2. The exclusive focus on kitchen environments might limit the dataset's applicability to broader hand action recognition scenarios outside of kitchen contexts.\n3. The proposed dataset is quite similar to the EPIC-KITCHENS-100 dataset [1]. Upon comparison, it appears that the proposed dataset is of a smaller scale and offers fewer tracks than the EPIC-KITCHENS-100. I would appreciate further clarification on its unique contributions to the community.\n\n\n\n[1] Damen D, Doughty H, Farinella G M, et al. Rescaling egocentric vision: Collection, pipeline and challenges for epic-kitchens-100[J]. International Journal of Computer Vision, 2022: 1-23."
            },
            "questions": {
                "value": "1. In the process of generating object segment information, could you elaborate on how SAM is utilized to produce the object mask? To my understanding, SAM typically outputs a class-agnostic object mask. How do you subsequently determine the class label?\n2. The proposed dataset is quite similar to the EPIC-KITCHENS-100 dataset [1]. Upon comparison, it appears that the proposed dataset is of a smaller scale and offers fewer tracks than the EPIC-KITCHENS-100. I would appreciate further clarification on its unique contributions to the community."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9109/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9109/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9109/Reviewer_9rBA"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9109/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698763228084,
        "cdate": 1698763228084,
        "tmdate": 1699637146247,
        "mdate": 1699637146247,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LIXjLhWbPD",
        "forum": "otoggKnn0A",
        "replyto": "otoggKnn0A",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9109/Reviewer_hqWJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9109/Reviewer_hqWJ"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a video dataset for hand action recognition. Specifically, the videos are specific to the Kitchen environment. They provide labels for fine-grained action classes and bounding box annotations for hands. Based on such data and annotations, the paper benchmarks three tasks: hand interaction and object detection, fine-grained hand action recognition, and domain generalization for hand region detection."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well-written and easy to follow.\n- The proposed dataset is quality-controlled and has extensive annotations. \n- The dataset could be helpful to the computer vision community to address hand interaction understanding. \n- The annotations provided in the form of hand-action triplets are novel and fine-grained. \n- The authors benchmark the dataset on three important tasks."
            },
            "weaknesses": {
                "value": "- In Figure 1, why is there no annotation for the chopping board in O-O interaction? Also, there is carrot & chopping board interaction. Should there be bounding box annotations to highlight this? While the authors mention that they want to perform fine-grained hand action recognition, it appears that many objects are not annotated. \n- The number of frames in relatively low compared to modern hand datasets such as EPIC-Kitchens/ VISOR. There are only around 30K frames. Also, the dataset captures only eight dish categories.\n- Do you provide contact annotations between hands and objects?\n- Are there segmentation mask annotations for all the bounding boxes?\n- There are important missing comparisons with existing relevant datasets such as EPIC-Kitchens-VISOR [1]. The VISOR benchmarks have high-quality annotations for hands and objects. The authors should compare their dataset against VISOR regarding annotations and cross-dataset evaluation. That is, there should be a comparison in evaluation performance of methods trained on FHA-Kitchen on VISOR, and vice-versa. This will help to understand the benefits of the proposed dataset.\n\n[1] EPIC-KITCHENS VISOR Benchmark VIdeo Segmentations and Object Relations, NeurIPS 2022"
            },
            "questions": {
                "value": "Please see the Weakness section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9109/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9109/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9109/Reviewer_hqWJ"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9109/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698772537393,
        "cdate": 1698772537393,
        "tmdate": 1699637146132,
        "mdate": 1699637146132,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wKEHoLb03O",
        "forum": "otoggKnn0A",
        "replyto": "otoggKnn0A",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9109/Reviewer_wEMj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9109/Reviewer_wEMj"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a fine-grained hand action annotation dataset that has 2,377 video clips and 30,047 images from 8 different types of dishes. The hand action is a triplet with a total number of 878 action triplets. Based on the proposed dataset, this paper benchmarked 3 tasks: (1) supervised learning for hand interaction region and object detection, (2) supervised learning for fine-grained hand action recognition, and (3) intra- and inter- class domain generalization for hand interaction region detection. The experiments show the performance of existing methods on the proposed dataset."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper proposes a dataset with fine-grained hand action annotation. In the hand-object interaction part, it also includes the object-object interaction annotations which are useful for interaction learning tasks.\n- It benchmarks on 3 tracks using existing methods."
            },
            "weaknesses": {
                "value": "- There is no method to sufficiently use the annotations collected in this paper, e.g. the 9-dim action labels.\n- There is no method in the paper. In the experiments section, it runs exiting methods and ablates the backbones.\n- The whole paper is about a dataset and three benchmarks thus it fits more to a dataset and benchmark track."
            },
            "questions": {
                "value": "- Table 1: This paper focuses on annotating hand action. There are many hand datasets and newer datasets that need to be incorporated into Table 1 for a comprehensive and fair comparison. For example,\n    - Ego4D: Around the World in 3000 Hours of Egocentric Video. Grauman et al.\n    - EPIC-KITCHENS VISOR Benchmark: VIdeo Segmentations and Object Relations. Darkhalil et al.\n    - Understanding Human Hands in Contact at Internet Scale. Shan et al.\n- In Table 1, there are more statistics about the dataset, for example, the number of hands, and objects. These are basic numbers of annotations and thus authors should show these numbers.\n- In 3.1, it mentions that the authors collected 30 videos to make a balanced dataset. It is not clear how the 30 videos are collected, from YouTube or captured by the authors.\n- In 3.2 Bounding Box Annotation of Hand Action: the same object, such as the knife in R-O and O-O in Figure 1 are annotated twice separately. The overlapping object bbox here is for the same object, the location annotation should be the same to maintain consistency of annotations. Why not only annotate the same object once?\n- The paper emphasizes the contribution of annotating a high dimension of action label which has 9 dimensions. However, there is no experiments to use the 9-dim annotation or to support that these annotations are more helpful than existing ones.\n- In the experiment section, Results on the SL-AR Track, the author mentioned that \u201cHowever, all the models achieve much worse accuracy on our FHA-Kitchens than the coarse-grained Kinetics 400, and unsatisfactory performance even using the large models.\u201d There is no results Kinetics evaluation set for comparison given that Table 4 is on the FHA Kitchens val set."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9109/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698967108453,
        "cdate": 1698967108453,
        "tmdate": 1699637146027,
        "mdate": 1699637146027,
        "license": "CC BY 4.0",
        "version": 2
    }
]