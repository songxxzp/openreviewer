[
    {
        "id": "iXQBBwKaAE",
        "forum": "ptmeLzcNyB",
        "replyto": "ptmeLzcNyB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2747/Reviewer_82Fi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2747/Reviewer_82Fi"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a communication strategy for cooperative multi-agent reinforcement learning that is task-agnostic and environment-specific. The authors pre-train the communication strategy in a self-supervised manner using a set autoencoder, enabling seamless adaptation to new tasks and scaling to more agents. The proposed strategy is compared to task-specific methods and a baseline that does not use communication and is shown to outperform both in different MARL scenarios. The authors also demonstrate that the proposed method can handle variable numbers of agents and can detect out-of-distribution events in the environment. Overall, the paper presents an approach to reducing the inefficiency of task-specific communication strategies in MARL."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "One strength of this paper is the proposed task-agnostic communication strategy, which is shown to outperform task-specific methods and a baseline that does not use communication in diverse MARL scenarios. The self-supervised and pretrained autoencoder is assumed to capture the representation of a given environment, and each of the policies of agents rollout actions based on the representation. This representation also accelerates policy adaptation to new tasks making the learning more efficient compared to task-specific MARL methods. The proposed framework is also tackling the OOD events in MARL. Code is written satisfyingly, and easy to reproduce."
            },
            "weaknesses": {
                "value": "The weakness of the approach lies in the method of generating offline datasets, which involves applying a random policy in different environments. In such a setup, the offline dataset often contains episodes with low returns, and with these episodes, it is hard to learn an informative representation due to this inefficient exploration strategy. It is essential to acknowledge that the quality of the dataset is crucial, given that the encoder aims to distill intrinsic information from the environment.\n\nAnother potential issue is dataset bias. The authors have not explicitly stated that the dataset was generated across different tasks. In a task-agnostic setting, the learned latent variable should ideally capture data from an environment with a distribution of tasks, facilitating the learning of environment-specific representations.\n\nThe adaptability in the task-agnostic setting is not well defined. While it is important to evaluate the task-agnostic setting across various tasks in each environment, the description of how different tasks are generated is not clear. For instance, in the \"Melting Pot\" collaborative cooking environment, it's unclear whether different tasks involve cooking various recipes or collaborating in different pre-defined ways. Moreover, the shared global reward structure might lead to suboptimal cooperation, with a few agents doing most of the work while others contribute little.\n\nThe same for the Discovery, where the differences between tasks appear to be primarily related to the differently generated locations of targets. If I am correct, this might not provide a strong basis for presenting distinct tasks.\n\nThe evaluation presented is incomplete. The author only demonstrates results for a single task rather than a distribution of tasks. A robust task-agnostic method should ideally outperform other baselines across a range of tasks within a specific environment.\nOOD setting is interesting, but I was wondering if it possible to unfreeze the parameters of the autoencoder when training policy under the OOD setting.This would shed light on how adaptive the communication strategy truly is.\n\nEnvironments are limited to 2-D environments. Some more complicated 3D environments are truly expected."
            },
            "questions": {
                "value": "Q1. Could you clarify how you distinguish between tasks within each of the environments?\nQ2 Would it be possible to construct an expert offline dataset?\nQ3 Could you provide an explanation of what the \"no-comms baseline\" entails?\nQ4 Would it be possible to illustrate the trajectory of the Center of Mass moves in policy training phase, and also compare it to the scenario where the policy is trained with randomized s_t? I believe it gives a more intuitive demonstration how s_t influences learning."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2747/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698742170971,
        "cdate": 1698742170971,
        "tmdate": 1699636217513,
        "mdate": 1699636217513,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VHW9P0i5kj",
        "forum": "ptmeLzcNyB",
        "replyto": "ptmeLzcNyB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2747/Reviewer_NAUT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2747/Reviewer_NAUT"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on solving multi-agent dec-MDPs. Specifically, the paper proposes a method that uses the local observations of nearby agents to estimate a latent global Markov state. The paper shows that the method can be used to execute unseen tasks within the same environment."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is well-written. It is well-organized and the ideas are exposed simply. The problem of multi-agent task execution is clearly important and the idea of having task-agnostic communication should be important for efficiently solving unseen tasks within an environment. The used set auto-encoder that is permutation invariant and can handle variable number of agents is also interesting and promising."
            },
            "weaknesses": {
                "value": "The problem being addressed is on the execution of multi-agent tasks in unseen environments. In those environments, the agents can communicate their local observations with each other if they are closer than a certain threshold. The method proposed encodes these observations into a latent state. The encoder was learned in a different task than the test. \n\nThe paper compares the performance of their task-agnostic method with one where the agent don't communicate at all (no-comms) and one where the encoder was learned on a different task (task-specific). The task-specific method is not explained in the main text (how and what does it learn?), so it is hard to understand how it can be compared with the task-agnostic method. The no-comms baseline is also not expected to be a strong competitor since it has less information than the other too. There is a clear baseline missing, which is a method that receives the observations of the agents that are close but does not use the set autoencoder, but rather a standard neural network architecture. Otherwise, there is no way to say that the set autoencoder, the main contribution within the method, is useful. It is also important to know how many times the agents are communicating. If the agents are always close by each other, than the Markov state is always being observed. Some metrics would be useful. \n\nThe method is build in the hope that the set autoencoder is able to predict the Markov state from the local observations. The Markov state is the joint observations of all agents. Therefore, the agents must be able to predict the other agents observations from their own local observations and the ones of their closeby neighbors. However, the other agents observations depends on their policy: if the agents are acting randomly, their observations will be some; if they are acting with a different policy, their observations will be other. Therefore, pre-training the method purely offline with random policies may be troublesome and can lead to serious problems in some environments. It would be important to see how the set autoencoder reacts to fine-tuning during task execution to accomodate for this, also compared with the baselines.\n\nThe convergence and regret results can be seen as out-of-scope of the work, or distracting, as the method does not contribute in the reinforcement learning, rather the encoder of the observations. It is typical to have encoders in partially observable environments for reinforcement learning so it is not particularly interesting to analyse the convergence and regret in this work in my opinion. The contributions in 4.2 are also much more aligned with the focus and relevant than the ones in 4.3 and 4.4.\n\nThe limitation of acting on a Markov state is not discussed and it should. Specifically, if the estimated Markov state is useful, it encodes the information from the observations of all the agents. Even though the dimension of the latent vector can be fixed, as the number of agents grows, the information scales exponentially. Methods that prune the communication or communicate only what is important for the task do not have this scale limitation. The limitation that multiple agents also need to coordinate (the Markov state is not sufficient) in multi-agent task executions is also not discussed.\n\nThe set autoencoder is not explained in detail in section 3.2 and is a core element of the paper.\n\nMinor: the method does not learn to communicate (neither how to \"speak\" nor how to \"listen\"), rather learns to encode observations that are available. The title and the exposition may hint the reader otherwise."
            },
            "questions": {
                "value": "- why is the focus restricted to dec.MDPs, in opposition to dec-POMDPs? Even though in the latter the joint observation is not necessarily the state, it is a more common setting in multi-agent reinforcement learning and the method proposed can be used to estimate the joint observation instead of the state. The generalization would also allow to use more benchmark environments, for example the ones that are more common in the literature (SMAC, MPE, LBF, ...).\n- what would be necessary for the method to be used with heterogeneous agents? By heterogeneous, I mean different action and observation spaces.\n- in the experiments, do the policies share parameters, or they learn independently?\n- in figure 2, $N$ is the number of agents, and should be $n$ or is the number of closeby neighbors?\n- in the conclusions, what does it mean that you use \"full connectivity between agents\"?\n- what is meant by \"on-policy\" throughout the work? It does not appear to be the usual sense in reinforcement learning (Sutton 2018) where the target policy is the same  the behavior policy."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2747/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698828853306,
        "cdate": 1698828853306,
        "tmdate": 1699636217439,
        "mdate": 1699636217439,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GmG5zIslrT",
        "forum": "ptmeLzcNyB",
        "replyto": "ptmeLzcNyB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2747/Reviewer_FP8p"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2747/Reviewer_FP8p"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a universal communication strategy for cooperative multi-agent reinforcement learning (MARL), applicable across various tasks within a specific environment. The authors employ a set autoencoder for pre-training without relying on task-specific rewards, ensuring policy convergence with latent representations. The approach scales to accommodate additional agents beyond the training setup and effectively identifies out-of-distribution environmental events, with empirical results across diverse MARL scenarios confirming its efficacy."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is easy to follow with concise expressions and clear logic."
            },
            "weaknesses": {
                "value": "1. The experimental environment is overly simplistic; I recommend employing more convincing and commonly used benchmarks such as SMAC and GRF.\n\n2. In essence, the paper models the global environment in advance through the use of an autoencoder. If the global state can be reconstructed from the partial state, it implies that the invisible state can be deduced from the visible state. In this case, there would be no need for communication.\n\nSome typos:\n\npage3 As the the global observation"
            },
            "questions": {
                "value": "Please refer to the weakness section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2747/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698839406999,
        "cdate": 1698839406999,
        "tmdate": 1699636217374,
        "mdate": 1699636217374,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pfGoJdAR1B",
        "forum": "ptmeLzcNyB",
        "replyto": "ptmeLzcNyB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2747/Reviewer_CDxU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2747/Reviewer_CDxU"
        ],
        "content": {
            "summary": {
                "value": "This paper decouples \"environment\" from \"task\" for MARL by introducing a task-agnostic, environment-specific communication strategy. This addresses the issue of sampling inefficiency and enables adaptation to novel tasks, more agents, and out-of-distribution events without relearning or fine-tuning the communication strategy."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "In the experiments, the task-agnostic version outperforms others."
            },
            "weaknesses": {
                "value": "1. Task-agnostic communication for MARL is not a new thing, especially the ones using natural languages (e.g. https://arxiv.org/pdf/2107.09356.pdf). This work did not show comparisons with those methods.\n\n2. Population invariant feature aggregation in MARL is not a new thing, e.g. Evolutionary Population Curriculum\uff08EPC\uff09achieved this using attention mechanisms. Population-invariant communication is also mentioned in papers like https://arxiv.org/pdf/2302.03429.pdf.\n\n3. Theorem 3.1 and 3.2 seem to be trivial."
            },
            "questions": {
                "value": "1. What are the detailed structures of the autoencoder?\n\n2. What is the exact version of the task-specific algorithm used in experiments?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2747/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698884709640,
        "cdate": 1698884709640,
        "tmdate": 1699636217314,
        "mdate": 1699636217314,
        "license": "CC BY 4.0",
        "version": 2
    }
]