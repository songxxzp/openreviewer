[
    {
        "id": "WiTOEhn6tT",
        "forum": "hWF4KWeNgb",
        "replyto": "hWF4KWeNgb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4336/Reviewer_v6Sb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4336/Reviewer_v6Sb"
        ],
        "content": {
            "summary": {
                "value": "The authors proposed a normalizing flow-based unified anomaly detection method, i.e., Hierarchical Gaussian Anomaly Detection (HGAD). By designing a loss function, the proposed method attempts to handle the intra-class diversity and inter-class separation."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The proposed method can model normalizing flows with hierarchical Gaussian mixture priors on the multi-class anomaly detection task.\n* The design of the proposed method can model the inter-class Gaussian mixture priors, learn class centers, and maximize mutual information."
            },
            "weaknesses": {
                "value": "* The presentation and the layout of the manuscript are bad. For examples,\n  * Figure 2(b)/(d): The authors need to clarify the colors associated with the classes.\n  * What are the $\\lambda_1, \\lambda_2$ in Section 4.2? It is confusing that the authors list several separate loss functions in Section 3.3 without any articulation about how to deal with these equations to achieve the goal(s). I finally found the objective function of the target goal after checking with the appendix, but the authors didn't mention anything in the main paper.\n  * In Section 3.4: What did the authors mean by level-k? Additionally, since there is no access to the label in the test, which $y$ in $\\mu_i^y$ will be used for the test point?\n  * The limited explanation between problem formulation and the experiment setup: \n    * In Section 3.2, since the authors pointed out that Eq. (2) is used to maximize the log-likelihood of normal features, why do the normalizing flows present a large log-likelihood for both normal and abnormal features? In other words, are both normal and abnormal/anomaly observations used in this loss function?\n    * In Section 4, what is the partition for the data in experiments? What are the normal classes? What are the anomaly classes? If label information (including anomalies) is used in the training, why do we call this multi-class anomaly detection? What is the difference between this with the regular multi-class classification?\n\n* Since there are multiple goals contained in the objective function and different training strategies in the experiments, to clearly summarize the work, it would be better to use pseudocode to outline the algorithm.\n\n* The weak support of the necessity of the intra-class centers: From Figure 3(b), I cannot see there is a significant difference among different numbers of intra-centers."
            },
            "questions": {
                "value": "* Section 3.1: Why $p_\\theta$ is a probability rather than a density? If it is a density function, why did the authors subtract that from 1 (any motivation)? \n\n* Figure 3: Why is the positional encoding added to the normalizing flow? Is this necessary? Did the authors conduct the ablation study of this design?\n\n* Bottom in Page 5: Why do not just use sample class priors to estimate $p(Y)$? Which part of the architecture in Figure 3 is used to estimate $p(Y)$? Could the authors explain in detail?\n\n* The notations in (8), and (9) are bad. What is $\\mu\\_{y^\\prime}$? Do you mean the center vector? Is loss (9) necessary? Why there is no penalty cost before this loss in the final object function? Did the authors conduct the ablation study for this loss function?\n\n* The discussion in Section 3.3:\n  * Could the authors further clarify this sentence: \"Because our method only explicitly uses class labels, while they implicitly use class labels (see App. A)\".\n  * I see one loss function is designed to maximize the log-likelihood of normal observations. Why did the author claim that using a label should not be a strict condition? Did the author conduct the experiment to support this conclusion?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4336/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698425236022,
        "cdate": 1698425236022,
        "tmdate": 1699636403622,
        "mdate": 1699636403622,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fjrOUjkoLg",
        "forum": "hWF4KWeNgb",
        "replyto": "hWF4KWeNgb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4336/Reviewer_tw2T"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4336/Reviewer_tw2T"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a normalizing flow model with hierarchical Gaussian mixture prior for unified anomaly detection, HGAD. This method achieves the SOTA unified AD performance on four datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The analysis and discussion of the proposed model are detailed.\n\n2. The experiment results are superior to the comparison methods. \n\n3. The experiments in both the formal paper and appendix are relatively thorough."
            },
            "weaknesses": {
                "value": "1. The abstraction is somewhat lengthy. Please polish the abstraction and make it concise.\n\n2. The size of coordinate/legend in Figure 2 is too small to recognize.\n\n3. The representation should be improved to be more professional. The explanations of some equations (eg. Eq6 and Eq9 ) are not easy-understood."
            },
            "questions": {
                "value": "1. Is the homogeneous mapping issue intrinsically equal to the well-known identical shortcut problem?\n\n2. The citations might be wrong. Many citations should be placed in the brackets. Please pay attention to the difference between '\\citep' and '\\citet'.\n\n3. The full name of HGAD should be listed.\n\n4. Why the performance of multi-class case is lower than the unified case, as shown in Table 1.\n\n5. The best performance on MVTec in Table 1 are 98.4/97.9, but 97.7/97.6 in Table 3."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4336/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698765440899,
        "cdate": 1698765440899,
        "tmdate": 1699636403536,
        "mdate": 1699636403536,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iQHZ38XTnA",
        "forum": "hWF4KWeNgb",
        "replyto": "hWF4KWeNgb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4336/Reviewer_37tJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4336/Reviewer_37tJ"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the problem of *supervised* *multiclass* anomaly detection, where the \"normal\" samples may belong to a pre-defined set of classes Y, and the goal is to detect anomalous samples that do not belong to any class in Y. The authors point to drawbacks with prior reconstruction-based and normalizing-flow (NF) based approaches to multiclass anomaly detection. They then propose a new approach for alleviating these drawbacks by building on existing NF-based methods replacing their unimodal Gaussian prior with a hierarchical Gaussian mixture prior. Experimental results and ablation studies demonstrate that the proposed approach is better on average compared to prior methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Extending existing NF-based approaches with a mixture of Gaussian prior looks like a natural approach to take for multiclass anomaly detection\n- Fairly extensive experimentation with ablation studies that attempt to show the role of individual loss components"
            },
            "weaknesses": {
                "value": "- One of my main concerns is that most of the methods compared to (e.g. UniAD, FastFlow, etc) are *unsupervised* whereas the proposed method is a *supervised* approach explicitly requiring class labels to be provided (see e.g. discussion in Appendix A.1). On the face of it, this does not seem like a fair comparison to make. It is important that the authors explicitly summarize what supervision each method uses and justify why theirs is a better approach despite requiring explicit label information to be provided during training.\n\n- The writing and presentation is at places hard to follow. The authors are urged to present the high-level approach first before dwelling into the details of the individual loss components. Having an explicit pseudo-code stating what the supervision is for the algorithm, and how the overall optimization objective looks like would be very helpful.\n\n- The proposed approach appears to have a lot of moving parts: there are four loss components (one for a inter-class Gaussian mixture, one for an intra-class Gaussian mixture, a mutual information based and an entropy-based loss for class diversity), with two hyper-parameters for weighting them (Appendix C). Although the authors do conduct some analysis of different hyper-parameter combinations, one if left with a feeling that the approach is highly heuristic in nature, with the gains coming largely from heavy engineering effort. Improving the writing and presentation may help boost the reader's confidence in the proposed method."
            },
            "questions": {
                "value": "Of the methods discussed, it appears that BGAD is supervised, but not compared to. Are there other methods you compare to in experiments which like your method are also supervised?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4336/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4336/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4336/Reviewer_37tJ"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4336/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1700706004397,
        "cdate": 1700706004397,
        "tmdate": 1700706004397,
        "mdate": 1700706004397,
        "license": "CC BY 4.0",
        "version": 2
    }
]