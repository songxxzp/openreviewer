[
    {
        "id": "0r61usZx0U",
        "forum": "OKf6JtXtoy",
        "replyto": "OKf6JtXtoy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4991/Reviewer_L3aD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4991/Reviewer_L3aD"
        ],
        "content": {
            "summary": {
                "value": "In the paper \"MAP IT to visualize representations\", the authors put forward a novel 2D visualisation method (MAP IT), based on t-SNE. There are two main differences between MAP IT and t-SNE: (1) MAP IT uses unnormalized affinities and replaces KL divergence with Cauchy-Schwarz (CS) divergence which is scale-invariant; (2) MAP IT applies the divergence to the \"marginal\" affinities obtained by summing up the rows of the pairwise affinity matrix. The authors claim that this results in a visualization method which is conceptually different from all t-SNE-like predecessors. The authors use several small datasets (n <= 2000) including a subset of MNIST to argue that MAP IT outperforms t-SNE and UMAP."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "I initially found the paper interesting, as it shows a good understanding of existing 2D visualization methods (t-SNE/UMAP) and develops what seems to be a novel alternative approach. I also agree with the authors that their MAP IT visualization of n=2000 subset of MNIST looks interesting and very different from t-SNE/UMAP."
            },
            "weaknesses": {
                "value": "Unfortunately, in the end I was confused by the presentation and not convinced by the paper. One big issue is that the paper is hard to understand: formulas have typos, many terms are not properly defined, overall section structure is sometimes confusing, etc. Another big issue is that the authors only show results on tiny datasets and do not seem to have a working scalable implementation that would allow embedding even a modest-sized full MNIST, let alone datasets with millions of points. The third issue is complete lack of quantitative evaluations."
            },
            "questions": {
                "value": "MAJOR ISSUES\n\n* section 3.1: Z_p and Z_q are never formally defined. While I understand that Z_q is the sum over all N^2 Cauchy kernels, I am not sure what Z_p is in t-SNE. Is it simply 2n?\n\n* MOST IMPORTANT QUESTION: The authors define marginal \\tilde p_j as sum over rows of the unnormalized t-SNE affinity matrix. They don't actually define \\tilde p_ij, but I assume that it's just p_ij * 2n. Note that in t-SNE p_{i|j} values sum to exactly 1 in each row, by construction. p_{ij} are obtained by symmetrizing p_{i|j}, but approximately they still have constant row and column sums. So when the authors define marginal p_j probabilities, to me it seems they are constant (??!) or at least near-constant. So I don't undestand how this can be useful for any dimensionality reduction algorithm. What am I missing?\n\n* Conceptual question 1. The authors put a lot of emphasis on using unnormalized affinities (and replacing KL with CS) and also on using marginal probabilities instead of pairwise probabilties. Are these two things somehow related? Could one use unnormalized affinities and CS loss with the pairwise affinities? Could one use marginal probabilities in the KL loss? Are these two independent suggestions and MAP IT just happens to implement both, or are these two suggestions somehow related and follow from each other?\n\n* Conceptual question 2. The authors put a lot of emphasis on using unnormalized affinities, but their CS loss function performs normalization within the loss function (Equation 5). This normalization leads to the N^2 repulsion term similar to t-SNE (Equation 8). To me this seems like the authors simply \"moved\" the normalization from one place (affinities) to another place (loss function), but nothing much changed compared to t-SNE. What am I missing?\n\n* The beginning of section 4 says that MAP IT uses perplexity 15. But in caption of Figure 3 and later in the text (esp. Appendix C), the authors mention k=10 value as if MAP IT uses kNN graph with k=10. How does perplexity 15 relate to k=10? This is very confusing.\n\n* Major limitation: the authors only have implementation that allows them to embed n=2000 data sets. That was fine 25 years ago, but in 2023 this is a major limitation. What is confusing to me, is that in Figure 8 the authors show that they can reduce the runtime 100x fold, and still obtain nearly the same result, but they only show it on the same n=2000 subset of MNIST. Why not run this 100x-sped-up approximation on the entire MNIST? That would be interesting to see!\n\n* Major limitation: no quantitative evaluation. E.g. for MNIST one could do kNN classification in the 2D space, and compare MAP IT with t-SNE/UMAP. One could also compute various NN preservation metrics. Would of course be much more interesting to do this on the entire MNIST and not on the n=2000 subset...\n\n\nMINOR ISSUES\n\n* page 2, formula (1): the minus sign is missing after the last equality sign\n\n* page 2, formula for p_ij in t-SNE: there should be 2n in the denominator, not just 2.\n\n* page 3, \"the role of normalization\": see Damrich et al 2023 https://openreview.net/forum?id=B8a1FcY0vi for parametric t-SNE. This paper would be important to cite in various places, including in the Appendix A\n\n* page 3: \"marginal probabilities\" appear in Definition 2 but have not been properly defined yet.\n\n* section 3.2, second line: one of the p_j = should be q_j =.\n\n* section 3.3: this section needs some introduction. By the end of section 3.2 it seems that the method is already defined. So why do you need another treatment in section 3.3? This needs more motivation.\n\n* page 14: relationship between t-SNE and Lapl. Eigenmaps was discussed in https://jmlr.org/papers/v23/21-0055.html and https://epubs.siam.org/doi/10.1137/18M1216134, it seems one should cite them here. And \"not to have been discussed much in the literature\" is not exactly right."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4991/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4991/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4991/Reviewer_L3aD"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4991/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698527560649,
        "cdate": 1698527560649,
        "tmdate": 1700757218572,
        "mdate": 1700757218572,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RYiTTmpZUe",
        "forum": "OKf6JtXtoy",
        "replyto": "OKf6JtXtoy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4991/Reviewer_vQ36"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4991/Reviewer_vQ36"
        ],
        "content": {
            "summary": {
                "value": "The authors introduce a new visualization method, called MAP IT.  The\nmethod supposedly improves over other visualization methods that are\ncommonly used, such as t-SNE, UMAP, etc."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The visualizations show some features that are better highlighted\ncompared to other methods.\n\nThe method seems straightforward to optimize and does not rely on too\nmany hyperparameters."
            },
            "weaknesses": {
                "value": "The method does not seem scalable, contrary to the authors claim.  The\nCS divergence includes a summation over all p_j and q_j, hence you\nhave quadratic complexity when optimizing.\n\nContinuing the point, there are no large-scale visualizations.  While\nthe authors claim that their approach could be scaled, I am sceptical\nof it previsely because of the definition of the CS divergence.\n\nMisc:\n\nsome citations could be added, e.g. on p. 3 (The role of\nnormalization) you could cite NCVis (Artemenkov & Panov, 2021,\nhttps://arxiv.org/abs/2001.11411); an approach that estimates the\nnorm. const. via NCE (Gutmann & Hyvarinen, 2012).  The role of the\nnormalization constant is also discussed in B\u00f6hm et al. (2022, JMLR)\nas well as Damrich et al. (2023, ICLR).\n\nTalking about the scalability of the method is fine, but there is no\nlarge-scale visualization that demonstrates that the method scales\nbeyond what other methods can esily visualize today.\n\nIn the same vein, there are no timings reported for any of the\nmethods."
            },
            "questions": {
                "value": "Could you comment on the computational complexity?\n\nWhy did you not chosse to use commonly used optimization parameters\nfor t-SNE?  In Belkina et al. 2019 they highlight what approaches work\nwell and they considerably improve the visualization quality.  This\nwould also improve the comparison that you draw from Figure 1.\n\nWhy did you choose the delta-bar-delta method for optimization?\n\nWhy do you think current approaches are not sufficient for the\ndatasets that you show in the paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4991/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4991/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4991/Reviewer_vQ36"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4991/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698768240636,
        "cdate": 1698768240636,
        "tmdate": 1699636486675,
        "mdate": 1699636486675,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Qg7CkpR98x",
        "forum": "OKf6JtXtoy",
        "replyto": "OKf6JtXtoy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4991/Reviewer_LbLf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4991/Reviewer_LbLf"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces an approach to dimensionality reduction by Cauchy-Schwarz projective divergence called MAP IT. This methodology aligns discrete marginal probability distributions rather than individual data points."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* This paper presents a new perspective on dimension reduction using projective divergence. \n* The manuscript is well written, with most equations explained clearly and easy to follow. \n* The authors demonstrate their method across various data sets, showing its ability to discover high-dimensional structures and visualize them in a lower dimension. \n* The commitment to release the code publicly after the review process is admirable."
            },
            "weaknesses": {
                "value": "The paper presents projective divergence as a method to simplify high-dimensional data, with its main advantage being the removal of weight normalization. However, from the authors experiments, it's unclear how this could be an improvement compared to existing methods."
            },
            "questions": {
                "value": "There are some typos in the manuscript:\n1. Sixth paragraph: normaliztion \n1. After equation 18: neigborhood \n1. After equation 19 second line: also i the\n1. After figure 3: neightbors\n$D(P||Q) = D(P\u02dc||Q\u02dc), \\forall Z_p,Z_q\\neq0$, could be better to understand over the mention of \"$Z_p$ and $Z_q$ being normalizing constants\" which implies $Z_p$ and $Z_q$ need to be some specific values."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4991/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698818403257,
        "cdate": 1698818403257,
        "tmdate": 1699636486568,
        "mdate": 1699636486568,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "51cZjqGhhy",
        "forum": "OKf6JtXtoy",
        "replyto": "OKf6JtXtoy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4991/Reviewer_BmiL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4991/Reviewer_BmiL"
        ],
        "content": {
            "summary": {
                "value": "The authors propose MAP IT, a new algorithm for manifold learning / dimensionality reduction for visualization. They compare qualitatively against previous approaches, including UMAP and T-SNE.\nTheir approach follows t-SNE but replaces the KL divergence with the Cauchy Schwarz Divergence, which removes the need for normalization, resulting in an approach that is theoretically more scalable.\n\nFWIW, I did not have the time to check the proofs, and I don't think I am familiar enough with the literature on relating t-SNE and UMAP to properly appreciate the details in the paper."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper provides a new algorithm for a common task, visualizing high dimensional datasets by projecting to two dimensions. They provide an in-depth discussion of how their approach relates to previous approaches, in terms of core ideas, computation, and results. The appendix contains helpful discussion and additional experiments, and all experiments are outlined in great detail."
            },
            "weaknesses": {
                "value": "I have two main critiques of the paper:\n- It does not provide quantitative results. It's common in this area to report 1NN results over various datasets in the projected space. While this is a somewhat arbitrary metric, it seems to be the best there is so far, and I think it would be prudent to include it.\n- The paper claims two conceptual novelties, the consideration of neighborhoods and the lack of normalization. Neither of these seem to me as new as the paper claims. Normalization is also not required in UMAP (as the paper mentions). Considering local neighborhoods is done explicitly in the less recent LLE, and implicitly in spectral embedding and laplacian eigenmaps. I was under the impression that t-SNE also has a similar \"dual\" interpretation in terms of kernel density, but I might be mistaken.\n\nMinor:\nAt the bottom of page 1, \"normalization\" is misspelled \"normaliztion\"."
            },
            "questions": {
                "value": "Can you explain why the kernel smoothing view doesn't apply to t-SNE?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4991/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698965112306,
        "cdate": 1698965112306,
        "tmdate": 1699636486487,
        "mdate": 1699636486487,
        "license": "CC BY 4.0",
        "version": 2
    }
]