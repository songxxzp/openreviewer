[
    {
        "id": "mn6wl9d2C5",
        "forum": "5COCYDObes",
        "replyto": "5COCYDObes",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7085/Reviewer_UBnq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7085/Reviewer_UBnq"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes the leader-follower bilevel framework, for learning relevant prompts for task-relevant actions. To verify the effectiveness of the proposed method, experiments are conducted on the Overcooked and FourRoom environments."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The paper is well organized.\n2. Figure 1 provides an intuitive illustration of the proposed method."
            },
            "weaknesses": {
                "value": "1. [Major] The presentation of this paper is vague and confusing. For example, this paper introduces many notations in the POMDP setting, but as far as the reviewer understands, the main usages of these notations only help the definition of policies $\\pi_\\phi,\\pi^{re},\\pi_\\theta$. However, $\\pi_\\phi,\\pi^{re},\\pi_\\theta$ themselves still remain unclear to the reviewer, even with the fancy math notations.  See other detailed examples in the \u201cQuestion\u201d section.\n2. [Major] The experimental results are not convincing. After reading Section 4 (Experiments) and Appendix 8 (Detailed descriptions of experiments), the reviewer still could not understand how the experiments are conducted (e.g., what are the inputs and outputs of each environment, and what are the reward functions). The reviewer would like to see the inputs, outputs, and reward functions of each environment clearly introduced, not being vaguely described in the text. See detailed questions in the \u201cQuestions\u201d.\n3. [Minor] The paper seems to be overclaiming the contribution. In the first sentence of page 2, the author claims that \u201cwe take the first step towards a fully unified LLM framework that learns to perform complex tasks\u201d. The reviewer does not believe the current paper is the \u201cfirst\u201d (see e.g., [1]), nor the Overvooked or FourRoom are complex enough (compared to [MineDojo](https://minedojo.org/) studied in [1]).\n\n[1] Wang, Guanzhi, et al. \"Voyager: An open-ended embodied agent with large language models.\" arXiv preprint arXiv:2305.16291 (2023)."
            },
            "questions": {
                "value": "> The presentation of this paper is vague and confusing.\n\n1. What are the exact inputs and outputs (e.g., are they symbolic vectors of the tasks texts) of $\\pi_\\phi,\\pi^{re},\\pi_\\theta$? The reviewer has carefully read through section 2, and the reviewer guesses that (1) $\\pi_\\phi$ is a text-to-text mapping; (2) $\\pi^{te}$ is a state-to-text mapping, but what exactly is the state space, is it text or images, or vectors? (3) $\\pi_\\phi$ is a mapping from (observation, text) to action, but what are the observation and action spaces (text, symbolic vectors, or others)?\n2. In the last paragraph **CoT reasoning with Prompts**, the authors mentioned \u201c$\\pi^{re}$ is severed by an LLM such as GPT3.5\u201d, what exactly is $\\pi^{re}$? If it is GPT3.5, which version of GPT3.5 (`turbo`, `turbo-16k`, or others)? If not GPT3.5, what exactly is it?\n3. In paragraph **Action policy training via PPO with LLM** of page 6, the authors mentioned \u201cwe use the pre-trained LLM, FLAN-T5 small\u2026 as the action policy\u201d. Does this suggest that $\\pi_\\theta$ itself is also an LM? If yes, could the author clarify how the state space serves as text inputs to $\\pi_\\theta$ and how the text outputs of $\\pi_\\theta$ are post-processed into actions? \n\n> The experimental results are not convincing.\n\n1. What is the reward function in the `overcooked` environment? In Section 8.1 (Environment), the authors have introduced the reward functions for ChainWorld and FourRoom. But for Overcooked, the author mentioned in the last sentence of paragraph \u201cOvercooked\u201d that: \u201cwe use an incremental reward shaping scheme where we give a small reward to agent for\u2026\u201d What is the incremental reward shaping scheme, and how is the reward function actually defined in this case?\n2. The experimental results presented in Figures 2 and 3 only slightly surpass the previous SOTA GFlan by a small margin, given the fact that only 5 random seeds are selected, it is hard for the reviewer to believe that the proposed method is actually better than GFlan. Note that in the original paper (Figure 3 of GFlan [1]), GFlan actually improves the prior baselines by a huge margin.\n\n\n[1] Carta, Thomas, et al. \"Grounding large language models in interactive environments with online reinforcement learning.\" arXiv preprint arXiv:2302.02662 (2023)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7085/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7085/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7085/Reviewer_UBnq"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7085/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698357104680,
        "cdate": 1698357104680,
        "tmdate": 1700683264772,
        "mdate": 1700683264772,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RYez6LuNwp",
        "forum": "5COCYDObes",
        "replyto": "5COCYDObes",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7085/Reviewer_g6Ja"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7085/Reviewer_g6Ja"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes the Bilevel-LLM approach for solving complex RL tasks. Bilevel-LLM (1) trains a promp-generating policy, (2) applies chain-of-thought reasoning to the prompt, and (3) trains an action policy conditioned on the final chain-of-thought output. The authors evaluate on several simple tasks (ChainWorld, FourRooms) and a more complex Overcooked task, where the authors demonstrate that their approach works better than several baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is generally clearly written, with the algorithms and objective functions clearly written out. \n- I believe the approach of using a policy to generate prompts for a CoT process that conditions an action policy is novel.\n- The experiments and ablation studies convincingly show the value of the method. In particular, the ablation study over a random prompt generation baseline demonstrates the value of training the prompt generating policy, and the small performance increase over GFLan demonstrates the benefit of the more complicated bi-level approach."
            },
            "weaknesses": {
                "value": "- Some of the notation is complicated and I believe it could be simplified. In particular, there are many subscripts (t+, I, etc.) that I think could be removed that could make the presentaiton of the technical method a bit easier to read.\n\n- The performance of the method is not that much higher than GFLan, which is surprising given that GFLan has no chain-of-thought reasoning. This implies that perhaps that chain-of-thought reasoning is not very effective for the tasks (given that it is a much more complex process), or that the tasks are too simple for the method to demonstrate it's benefits.\n\nMinor:\nThe \"t+\" notation seems a bit unnecessary. I believe it can be removed and the equations would still make sense."
            },
            "questions": {
                "value": "- For reproducibility purposes, it would be great if the authors could report all learning rates & hyperparameters used in the experiments (also for baselines), as well as the hyperparameter sweeping strategy.\n- What does the subscript \"I\" mean in the \"\\gamma_I\" discount factor in Eq 1 & 4? I did not find this notation explained previously."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7085/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698559486404,
        "cdate": 1698559486404,
        "tmdate": 1699636835791,
        "mdate": 1699636835791,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6RKwqRcla3",
        "forum": "5COCYDObes",
        "replyto": "5COCYDObes",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7085/Reviewer_WqYj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7085/Reviewer_WqYj"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a bilevel framework that consists of the prompt-generation policy, thought reasoning policy, and action policy. In particular, at each timestep, the prompt-generation policy generates a prompt, which is then used by the thought reasoning policy to produce the CoT. The produced thought is used by the action policy to select an action in the environment. The proposed bilevel-LLM method is evaluated in ChainWorld, FourRoom, and Overcooked domains and shows better performance than baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The bilevel optimization with the prompt-generation policy to minimize the actor policy's uncertainty is new & interesting. \n2. The paper is well-written and addresses the important challenge of automated prompt engineering for solving decision-making tasks."
            },
            "weaknesses": {
                "value": "1. While the proposed bilevel-LLM generally achieves better performance than baselines, the performance gap is marginal compared to the GFlan baseline (the difference may not be statistically significant).\n2. The novelty could be limited with respect to prior work: learning to select a prompt based on policy gradient is similar to PromptPG, and the use of Chain-of-thought prompting to act in the environment is similar to ReAct. As such, this paper could be viewed as combining these two directions.\n3. While one of the paper's objectives is to avoid the expensive prompt crafting by humans, the framework would still need human-constructed questions. In Figure 3(c), the paper presents the Bilevel-LLM-Auto that does not rely on human-constructed prompts, but it is unclear whether this method applies to other domains, including Overcooked (only the ChainWorld(Full) performance is shown)."
            },
            "questions": {
                "value": "1. I hope to ask the authors' responses to my concerns (please refer to the weaknesses section for details).\n2. The prompt-generation policy aims to minimize the actor policy's entropy. However, I am unsure whether this is the correct objective to optimize for because 1) the actor policy may be certain but certain about incorrect actions (i.e., low uncertainty but convergence to sub-optimal actions), 2) for some domains, an optimal policy could be stochastic not deterministic, and 3) a positive entropy could help exploration. \n3. The proposed bilevel optimization could be difficult because the actor policy is learning and thus keeps changing its behavior over time. Theoretically, this non-stationary actor's behavior makes the reward function (i.e., the entropy reward) non-stationary from the prompt-generation policy's perspective, which could render the Markov property invalid and induce unstable training of the prompt-generation policy. Would it be possible to ask for more discussion about this possible non-stationarity issue, which is one of the main challenges in multi-agent learning?\n4. Because the domain is POMDP (Section 2), would the Vanilla PPO use RNN/LSTM/transformer architecture instead of MLP architecture (i.e., no memory)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7085/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7085/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7085/Reviewer_WqYj"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7085/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698694711816,
        "cdate": 1698694711816,
        "tmdate": 1700679304182,
        "mdate": 1700679304182,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CmpA5iCncq",
        "forum": "5COCYDObes",
        "replyto": "5COCYDObes",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7085/Reviewer_5KxL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7085/Reviewer_5KxL"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a bi-level framework which learns to select a set of prompts to enable effective operation on downstream tasks. A set of prompts is generated using chatGPT 3.5, which are then selected from using a learned policy. Generated chain of thought skeleton are then input into a action policy which acts in the environment. Experiments show the efficacy of the approach."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The proposed bi-level mechanism to select from a set of prompts and then act in the environment is novel to my understanding\n- The method in the paper I generally clear and understandable"
            },
            "weaknesses": {
                "value": "- The formatting of the paper is odd, for example the distance from the subsection heading from text is way too small in page 5\n- The results in the paper would be more readable if there were more illustrations of the process (for example in the introduction)\n- The LLM takes as input only text. In this case, for embodied domains with image observations, it seems like there is no way for the LLM to really know the current state, which means that the method essentially just trains a low-level image based policy given some set of thoughts (since the high-level policy is invariant to task completion).\n- As a result, this framework doesn't really make sense for decision-making in my opinion -- it makes much more sense in the setting of reasoning and I would like to see evaluation in that setting."
            },
            "questions": {
                "value": "- Given the setting described above, why is that the approach actually improve performance over baselines?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7085/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698867368783,
        "cdate": 1698867368783,
        "tmdate": 1699636835576,
        "mdate": 1699636835576,
        "license": "CC BY 4.0",
        "version": 2
    }
]