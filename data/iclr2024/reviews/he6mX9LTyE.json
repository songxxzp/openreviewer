[
    {
        "id": "PNPZEtkquG",
        "forum": "he6mX9LTyE",
        "replyto": "he6mX9LTyE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8897/Reviewer_J1rb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8897/Reviewer_J1rb"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a framework that combines MLLM and SD to perform image generation/editing with multimodal input. To better bridge the MLLM output space and SD input space, AlignerNet is introduced for feature alignment. Additionally, a large-scale object compositional image generation data is collected and used for training."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The idea of bridging MLLM and SD for versatile image generation is interesting. MLLM naturally can accept both image and text input, which can provide a more diverse signal to the image generation module and therefore enable new applications.\n2. The newly collected compositional image generation dataset should be useful to the community."
            },
            "weaknesses": {
                "value": "1. I don't see much novelty from AlignerNet.  Compared with GlueNet, AlignerNet merely replaces the MLP with encoder-decoder Transformers but they have the same loss and the same domains (both aligning text embedding). AlignerNet is useful from the experiments, but not novel IMO.\n2. Since the training data includes the image editing dataset from InstructPix2Pix. A comparison between previous works on image editing benchmarks should also be conducted. Similarly how is kosmos-g compared with GILL in visual storytelling?\n3. In AlignerNet, both MSE and REC losses are used. However, no ablation is done about those two losses. \n4. In Tab2, it seems the E2E Fine-tuning fails. However, recent works such as BLIP-Diffusion, EMU, and MGIE can successfully connect MLLM with SD via E2E fine-tuning without any specific alignment. Why Kosmos-G's behavior is different from others and relies on additional alignment?"
            },
            "questions": {
                "value": "1. When constructing the compositional generation dataset, what if multiple objects of the same class exist in the same image? Would the corresponding segmentation mask cover multiple instances in the same mask?\n\n-------------- After rebuttal ----------------\nThank the authors for the last-minute efforts. I have raised the rating to 6. Please stick to this new manuscript and the new title in the camera-ready version if accepted, and further improvement in corresponding writing is also encouraged."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8897/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8897/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8897/Reviewer_J1rb"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8897/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698822146752,
        "cdate": 1698822146752,
        "tmdate": 1700764777177,
        "mdate": 1700764777177,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8HHYBUN6YG",
        "forum": "he6mX9LTyE",
        "replyto": "he6mX9LTyE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8897/Reviewer_VSX4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8897/Reviewer_VSX4"
        ],
        "content": {
            "summary": {
                "value": "This paper explores image generation from generalized vision-language inputs, especially involving multiple images. Named KOSMOS-G, a model that leverages the advanced perception capabilities of MLLMs, and aligns the output space of MLLM with CLIP using the textual modality as an anchor and performs compositional instruction tuning on curated data."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The problem of image generation conditioning on generalized vision-language inputs is an interesting problem, and the proposed approach seems to show some promising results.\nThe idea of aligning KOSMOS-G Space with the CLIP-T Space and then directly leveraging the stable diffusion models seems a valid approach.\nThe qualitative results show some good capabilities of the proposed method."
            },
            "weaknesses": {
                "value": "The ablation study seems not very comprehensive, for example, if the goal is to align the two representation spaces, there should be other options to achieve the alignment design, so why is the current AlignerNet design the best, maybe more justification and ablation study are needed here."
            },
            "questions": {
                "value": "see the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8897/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698908518284,
        "cdate": 1698908518284,
        "tmdate": 1699637119450,
        "mdate": 1699637119450,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NIU5SmWmPy",
        "forum": "he6mX9LTyE",
        "replyto": "he6mX9LTyE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8897/Reviewer_hrQd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8897/Reviewer_hrQd"
        ],
        "content": {
            "summary": {
                "value": "Kosmos-G aligns the outputs of MLLM to the embedding space of CLIP text encoder, which can be fed into Stable Diffusion model for image generation with context of any form."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The KOSMOS-G's ability to achieve zero-shot multi-entity subject-driven generation is notable. The method addresses an underexplored area in image generation by focusing on generalized vision-language inputs and multiple images, the method leverage existing advancements in both multimodal language models and image generation. \n\n2. By the alignment of the output space of MLLMs with CLIP and Score distillation instruction tuning, KOSMOS-G can achieve subject-driven generation and image editing without any training on diffusion models, highlighting its potential for integration into different frameworks."
            },
            "weaknesses": {
                "value": "1. The paper repeatedly mentions KOSMOS-G's ability to master zero-shot multi-entity generation and handle interleaved image-text input. However, the practical cases presented in the paper seem to focus primarily on image editing capabilities. Look forward to showing more cases with complex and rich scenarios to further illustrate the capabilities of the model. : 1\uff09the paper only demonstrates cases with a maximum of two images, failing to showcases with more than two images as inputs.  2) the paper predominantly showcases and evaluates image-text-image input scenarios, leaving more diverse multi-image and text interleaving cases unexplored. \n\n2. Section 2.3 discusses the \"Score distillation instruction tuning\" technique, but the description lacks clarity. The paper should provide a more precise definition of the entities involved in calculating the KL divergence, along with any specific mathematical formulas or equations for better understanding.  In addition, is KL divergence loss necessary? Is it feasible to directly apply diffusion model's loss for training?\n\n3. The paper highlights the exceptional subject-driven generation capabilities of KOSMOS-G, particularly when not training the diffusion model.   I would like to ask if the authors have explored the possibility of further enhancing subject-driven generation, like training the diffusion model."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8897/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8897/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8897/Reviewer_hrQd"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8897/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699534602875,
        "cdate": 1699534602875,
        "tmdate": 1699637119297,
        "mdate": 1699637119297,
        "license": "CC BY 4.0",
        "version": 2
    }
]