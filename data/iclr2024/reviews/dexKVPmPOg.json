[
    {
        "id": "xpydA0QXLS",
        "forum": "dexKVPmPOg",
        "replyto": "dexKVPmPOg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8700/Reviewer_yLdo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8700/Reviewer_yLdo"
        ],
        "content": {
            "summary": {
                "value": "This paper describes a method to incorporate extra training data to learn a particular prediction tasks using Gaussian processes. The proposed method consists in following a particular approach to decide whether a particular data instance should be incorporated to the training data or not. The criterion followed consists in using the negative log likelihood given by the predictive distribution of the GP after incorporating a particular training instance. This is equivalent to changing the prior GP to another GP that is expected to perform better (since it gives a better marginal likelihood estimation). The proposed method is expensive if a naive implementation is followed, with O(N^4) cost on the number of data points or iterations to follow. The authors proposed a clever implementation that takes into account partial updates of Cholesky factors. The method is validated on synthetic datasets both in terms of performance and in terms of computational cost."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper is very well written and clearly explained. Apart from that, I cannot find any other particular strength. My overall impression is that the paper is still in an early stage and needs more work before it can be accepted for publication. In particular, the authors should address the weaknesses described below."
            },
            "weaknesses": {
                "value": "The experimental section is too weak. It only considers synthetic datasets. It is not clear at all if the proposed method has a practical utility since no real world problems are considered in the paper. This questions the significance of the results. The authors should given particular examples of the expected utility of the proposed approach in a real-world setting.\n\n        The paper lacks a solid related work section. It is not clear at all if this problem has been already studied in the literature and if some methods have already been devised for it. In the introduction there are some related methods described. However, they seem methods proposed for a different setting that may be adapted to the particular setting considered by the authors.\n\n        The proposed method has a very large computational cost that is cubic w.r.t. to the number of training points or the points to be added to the training set. This is a limitation since only a few thousand points may be considered at most. The authors should try to scale the method to larger experimental settings, considering e.g., approaches for sparse GPs.\n\n        The use of Cholesky factors that are updated efficiently is not new within the GP literature."
            },
            "questions": {
                "value": "Why do not each method in Fig. 1 start from the same initial value?\n\nCould you approach be extended to take advantage of sparse GPs approaches to scale to large datasets?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8700/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8700/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8700/Reviewer_yLdo"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8700/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698660768730,
        "cdate": 1698660768730,
        "tmdate": 1700665019627,
        "mdate": 1700665019627,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3hmRcca4nF",
        "forum": "dexKVPmPOg",
        "replyto": "dexKVPmPOg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8700/Reviewer_1Da7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8700/Reviewer_1Da7"
        ],
        "content": {
            "summary": {
                "value": "This work proposes to use the negative log marginal likelihood of the Gaussian process as a criterion when selectively adding simulator-generated data to the training data. Since evaluating each candidate training data point using the negative log marginal likelihood can be time-consuming, the authors propose a method for fast computation by considering the so-called Cholesky update and take advantage of the dependencies between matrix elements."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Originality: Probably, the faster re-computation of the marginal likelihood might be the originality of the work. \n\nQuality: The experiments provided in the paper are useful to provide an idea of the approach, though they are limited to just presenting synthetic scenarios. \n\nClarity: The methodology sections are generally well written and not difficult to follow, though they present some inconsistencies in the mathematical notation.\n\nSignificance (importance): The work has its strength in the efficient computation of the marginal likelihood."
            },
            "weaknesses": {
                "value": "-The idea of accepting data to be added as part of the training set by improving the marginal likelihood was previously explored by Titsias M. in \"Variational Learning of Inducing Variables in Sparse Gaussian Processes\" section 3.1. In the context of Titsias' work was used to generated pseudo-inputs (or inducing points).\n\n-The experiments provided are limited to just presenting synthetic scenarios. \n\n-The introduction does not properly motives the research problem to engage the reader with the work. Also, the introduction lacks of references to better support different phrases or claims. The methodology sections are generally well written and not difficult to follow, though they present some inconsistencies in the mathematical notation.\n\n-The work has its strength in the efficient computation of the marginal likelihood, but the main aim of the work was not to compare such an algorithm with other approaches that improve such computation, but to introduce a direct method of selectively adding simulator-generated data to training data when using Gaussian processes."
            },
            "questions": {
                "value": "---Specific comments---\n\n-In Abstract, it sounds contradictory to say that we rely on knowledge that is unreliable: \"construct models based on their knowledge of the modeling target and, as training data increases, choose more flexible models with reduced dependence on that knowledge if that knowledge is unreliable.\"\nMaybe the last sentence would be better understood if read as:\"...if that knowledge becomes unreliable\" or simply get rid of last part and leave: \"construct models based on their knowledge of the modeling target and, as training data increases, choose more flexible models with reduced dependence on that knowledge.\"\n\n-In Abstract, it is not clear what it is the intention of \"We propose a faster method considering the Cholesky factor and matrix element dependencies.\" There is something missing to properly connect with all the previous text.\n\n-In the Introduction, there is probably a sentence missing at the very beginning regarding modelling issues or modelling challenges than allows the reader understand where the idea or problem of bias-variance trade-off comes from. Also, it is necessary to include a strong reference regarding \"bias-variance trade-off\" to support the text.\n\n-In the introduction, the phrase that reads: \"On the other hand, the method of selectively adding generated\nsimulator data to the training data only requires that data can be generated from the simulator\" seems ambiguous or needs rewording.\n\n-Please include references to support: \"The criterion for selecting important data is the diversity of the training data. Various methods to measure this\ndiversity have been proposed.\"\n\n-Please include references to support: \"The negative log marginal likelihood is a metric that measures the model\u2019s \ufb01t to the training data and has a theoretical foundation that it matches, on average, the KL divergence between the true distribution and the model\u2019s distribution.\"\n\n-Where it reads: \"Within this category, although\nAuto Data Augmentation is efficient, The knowledge transferred\", lower case \"..., The knowledge...\" to \"..., the knowledge...\"\n\n-Introduce the acronyms KL, BIC, GPs, NLL and NML!\n\nIn section 2.1: \n-There seems to be inconsistency in the notation. I do not see the benefit of referring to $\\mathbf{X}$ as a random variable. There is no information or specification of the distribution that $\\mathbf{X}$ follows. I would suggest to refer as an input variable $\\mathbf{x} \\in \\mathbb{R}^d$ instead of $\\mathbf{X} \\in \\mathbb{R}^d$. Also $y \\in \\mathbb{R}$ instead of $\\mathbf{y} \\in \\mathbb{R}^1$, these to be congruent with Eq. (1).\n\n-Also, I suggest to use $\\mathbf{y}^N=(y_1,y_2,...,y_N)^\\top$ and $\\mathbf{X}^N=(\\mathbf{x}_1,\\mathbf{x}_2,...,\\mathbf{x}_N)^\\top$ to be more consistent instead of the current notation in the paper.\n\nIn Eq. (1), the Covariance matrices $\\mathbf{K}_{N,m^*}$ \n\nand $\\mathbf{K}^{\\top}_{N,m^*}$ \n\nmight be swapped of quadrant. \n\nIt is more intuitive to think that the pair $N,m^*$ refers to rows,columns respectively. \n\nAdd period \".\" at the end of the equation. \n\nPutting the $\\mathbf{x}_1...\\mathbf{x}_N$ and \n\n$\\mathbf{x}_{1^*}$ ...  \n\n$\\mathbf{x}_{m^*}$ \n\ninside the equation looks strange as if a vector were multiplying the covariance matrix. Maybe a footnote should be added to avoid confusion.\n\n-If $\\mathbf{K}_{N,m^*}$ \n\nis swapped by $\\mathbf{K}^{\\top}_{N,m^*}$ then the equations that use these matrices should be corrected.\n\n-Similar comment to the one before applies to Eq. (2).\n\n-After Eq. (2) in $F_{m+1^*}$ the $y^{m+1}$ is missing \"*\".\n\n-In Eq. (2) and (3) the identity matrices $\\mathbf{I}$ should be different at each quadrant since they do not have the same dimensions.\n\nIn Eq (3) the negative sign is not applied, previously it was introduced $F_{m+1^*}=-\\log \\mathcal{N}...$. Also, as per Eq. (4) the operation in Eq. (3) should be \n\n$(\\mathbf{y}^{m+1}-\\boldsymbol{\\mu}_{m+1})$\n\ninstead of \n\n$(\\mathbf{y}^N-\\boldsymbol{\\mu}_{m+1})$\n\n-Add a comma \",\" after Eq. (3) and (4), then period \".\" after Eq. (5).\n\n-In section 3: write $(m+1)\\times(m+1)$ instead of $m+1 \\times m+1$. Indeed, in the equations should be better to write, say, \n\n$\\mathbf{y}_{(m+1)^*}$ or \n\n$\\mathbf{K}_{(m+1)^*}$.\n\n-In section 3: it reads: \"with a total cost of \n\n$\\mathcal{O}(M^2N + MN^2)$, \n\nkeeping it within the cubic order\", shouldn't it be within the quadratic order?\n\n-Before Eq. (6): what is $\\mathbf{K}_{+m}$? typo?\n\n-Before section 3.2: \n\n$(\\mathbf{L}_{m+1}$ \n\n$\\mathbf{L}^{\\top}_{m+1})^{-1}\\mathbf{y}^{(m+1)^*}$ \n\ninstead of \n\n$(\\mathbf{L}_{m+1}$\n\n$\\mathbf{L}^{\\top}_{m+1})^{-1}\\mathbf{y}^{m+1}$\n\n-Where it reads: \"Lalchand \\& Faul\n(2018) described in Section 1, promote diversity of training data.\" should be \"promotes\" since you are referring to the method or work.\n\n-Typo where it reads: \"then using the likelihood of the all output data y\", should be \"...of all the output data...\"\n\n-In section 4.2: \"the number of training data candidates generated from the simulator was 1,000,\", you mean \"1000\" or 1?\n\n-In the figure 3: it is not possible to visualise the Training data (brown-ish colour) for SoD. \n\n-In the conclusion: \"the algorithm we proposed is specialized for regression models\", not regression models in general, but a regression model particularly with a Gaussian likelihood.\n\n-Generally, there is either a comma or period missing after the equations.\n\n-Initial capital letter in the bibliography, words like: Gaussian and Cholesky. \n\n-Why is there a distribution $q(\\mathbf{X}^N)$ in appendix H for Eq. (21)? Aren't we saying in $KL(q(.|\\mathbf{X}^N)||p(.|\\mathbf{X}^N))$ that $\\mathbf{X}^N$ is given? I do not think the Eq. (21) is correct.\n\n---Other Questions---\n\nIs this method feasible to different statistical data types for the outs $y$ or we should assume that $y$ is always in the real values?\n\nWe fit the GP hyperparameters with the training data, but are those hyperparameters tuned again when adding simulator data?\n\n-The experiments shown seem to have an appropriate number of N data observation so that the GP model fits quite well for the range of input data $\\mathbf{x}$, so due to the conditioning properties of a Normal distribution it is expected to only accept data that could improve the conditional distribution $p(\\mathbf{y}^N|\\mathbf{X}^N,\\mathbf{y}^{m^*},\\mathbf{X}^{m^*})$. What would it happen if the GP has a smaller number of data observation, or lack of data in regions such that the predictive distribution was less uncertain? How would the acceptance and rejection would behave in such a region?\n\n-It seems that the Log marginal likelihood metric gives priority to the model fitting, so when do we trust the simulator?\n\n-What if the simulator is actually quite close to the true distribution, but we have a small number of data observations for which we fit a GP with the hyper-parameters tuning a distribution not that close to the true distribution?\n\n\nWhat ways to measure a trade-off, as mentioned in the introduction, to achieve an appropriate bias-variance in our last model that contains training and simulator data? \n\n-If I fit the GP and generate data from such a GP and use it as simulator data, wouldn't I expect to achieve improvements in the Log marginal likelihood? Wouldn't the fitted GP be simply the best data simulator?\n\n-The work is missing to show a real world application to additionally assess the performance of the approach, for instance an example as claimed in appendix C.\n\n-What would be the effect of using different data simulators? For instance, a simulator less similar to the real distribution. \n\n-For the practitioner, How is a data simulator generally built or where does it come from?\n\n-What if the dataset we are fitting presents a heteroscedastic noise, how could this affect the method approach for accepting training data candidates?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8700/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8700/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8700/Reviewer_1Da7"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8700/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698789323833,
        "cdate": 1698789323833,
        "tmdate": 1699637090435,
        "mdate": 1699637090435,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BPqy54UZhC",
        "forum": "dexKVPmPOg",
        "replyto": "dexKVPmPOg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8700/Reviewer_q1Av"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8700/Reviewer_q1Av"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on selectively adding data for training a low-variance model, which is an important topic. The authors propose to deploy GP along with marginal likelihood as the metric to evaluate the quality of simulated data samples. The paper first talks about the method to selectively add more training data using GP. Then, it introduces the algorithm for faster implementation.  The experiments show the improvement."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The GP for adding simulated data seems to better perform than other baseline methods. \n- The algorithm is faster. \n- The discussion is well-rounded."
            },
            "weaknesses": {
                "value": "- The novelty of GP on this topic is a bit limited. GP is not a new method at all. The algorithm that makes it faster is more interesting but no major breakthrough. \n- It seems no real data set is experimented."
            },
            "questions": {
                "value": "1. Why gray points are not adopted in Figure 3?\n2. It is said that the hyperparameters of GP are learned from initial training data. Do those hyperparameters change after it is learned? If it is not, does the initial training data affect the selection process? If it is not, how does it change?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8700/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699495414473,
        "cdate": 1699495414473,
        "tmdate": 1699637090235,
        "mdate": 1699637090235,
        "license": "CC BY 4.0",
        "version": 2
    }
]