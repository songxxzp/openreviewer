[
    {
        "id": "WyvxpZMvBR",
        "forum": "mHQEyXaULY",
        "replyto": "mHQEyXaULY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6266/Reviewer_szNh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6266/Reviewer_szNh"
        ],
        "content": {
            "summary": {
                "value": "The study introduces a method for efficient concept-driven image generation using text-to-image diffusion models. It represents concepts by adjusting a pretrained model's weights and utilizes localized attention-guided sampling. This approach captures concept identity quickly on a single GPU, using fewer parameters than previous models, while maintaining the generative potential of the original model for most of the image."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Efficiency: The method offers a more efficient approach to personalized image generation, using fewer parameters and avoiding the need for regularization images, resulting in faster and simpler training.\n2. Domain Flexibility: It can be applied to arbitrary domains and concepts, making it versatile and adaptable to a wide range of image generation tasks.\n3. Improved Sampling: The localized attention-guided (LAG) sampling approach enhances the generation process by focusing on areas where the concept is localized, combining concept identity with the strengths of the original model for better results."
            },
            "weaknesses": {
                "value": "1. Limited Novelty: The paper is criticized for its limited novelty compared to existing work, suggesting that it may not significantly advance the state-of-the-art in the field.\n2. Lack of Quality Improvement: Reviewers note that the method does not substantially improve the quality of generated images when compared to existing approaches, as evidenced by Figure 3 and Table 2.\n3. Insufficient Baseline Comparisons: The paper is faulted for not including thorough discussions or experiments comparing its method against strong baselines, such as IP-adapter or ControlNet reference-only mode, which makes it challenging to assess its relative performance.\n4. Control Support Unclear: There is ambiguity regarding whether this method supports adding control, like IP-adapter also supports ControlNet, as this is not adequately addressed or demonstrated in the paper.\n\nThese weaknesses suggest that the paper may lack a comprehensive evaluation and may not provide substantial advancements over existing methods."
            },
            "questions": {
                "value": "See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6266/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698171323474,
        "cdate": 1698171323474,
        "tmdate": 1699636685896,
        "mdate": 1699636685896,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IwtOliSkQV",
        "forum": "mHQEyXaULY",
        "replyto": "mHQEyXaULY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6266/Reviewer_98Kb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6266/Reviewer_98Kb"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a T2I personalization approach based on low-rank residuals, which uses fewer parameters and does not rely on class regularization. A localized attention-guided sampling approach is proposed such that the learned concept only influences (the subject) parts of the image, preventing overfitting the learned concepts while generating subject-irrelevant content (like background)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper leverages LoRA to learn personalized concepts, despite being widely used by the community [1, 2], and discovers that such usages can preserve the existing diffusion prior, thus eliminating the need for class regularization. The paper also experiments with the optimal values of the LoRA rank, which may be useful to the community."
            },
            "weaknesses": {
                "value": "My primary concerns about the papers are two-fold. \n1. Method-wise, the main contributions of this paper are to use LoRA for fewer trainable parameters and getting rid of regularization, and use attention maps to mask out the subject foreground. However, as mentioned in the strength part, using LoRA for training diffusion models is widely adopted in the communities [1], as well as for training personalized diffusion models [2]. Despite the paper exploring that leveraging LoRA preserves diffusion prior thus avoiding regularization, the reduction in trainable parameters is mainly contributed by the use of LoRA, thus the contribution is very incremental in my opinion. Secondly, previous works have also adopted cross-attention maps to guide the diffusion model to focus on the subject token such that the attention maps associated with each attended token will be activated [3]. More similarly, [4] leverages attention maps to suggest personalized models for foreground subject generation, which shares the same motivation and almost identical solution as this paper, while according to the time stamp it was released around half a year ago with their code available (June 1st, 2023 according to https://arxiv.org/abs/2306.00971).\n\n2. Performance-wise, it is hard to distinguish the superiority of the proposed approach against DreamBooth. The comparison might be unfair qualitatively, however, as shown in Table 1, the proposed method does not lead to a compelling improvement against baselines like DreamBooth on CLIP text and image alignment metrics. Furthermore, as suggested in Table 2, more human evaluations indicate preferences over DreamBooth for better text alignment as well.\nAs above, from both the perspectives of presented methods and experimental evaluations, the paper does not show a clear distinction between the existing approaches.\n\n[1] https://github.com/cloneofsimo/lora\n\n[2] https://github.com/huggingface/peft/tree/main/examples/lora_dreambooth\n\n[3] Chefer, Hila, et al. \"Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models.\" ACM Transactions on Graphics (TOG) 42.4 (2023): 1-10.\n\n[4] Hao, Shaozhe, et al. \"ViCo: Detail-Preserving Visual Condition for Personalized Text-to-Image Generation.\" arXiv preprint arXiv:2306.00971 (2023)."
            },
            "questions": {
                "value": "Please see the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6266/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698769114965,
        "cdate": 1698769114965,
        "tmdate": 1699636685733,
        "mdate": 1699636685733,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xFUMI2uThN",
        "forum": "mHQEyXaULY",
        "replyto": "mHQEyXaULY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6266/Reviewer_mRPT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6266/Reviewer_mRPT"
        ],
        "content": {
            "summary": {
                "value": "This work addresses the challenge in concept-driven text-to-image generation motivated by a low-rank personalization approach.\nThe authors introduced a personalized residual computation for each transformer block.\n\nThey resolved the issue of concept forgetting in fine-tuning by leveraging their approach and also reduced the computational cost traditionally associated with fine-tuning a new set of parameters for each concept.\n\nThey proposed residual computations based on localized attention to enable fine-tuning with a small number of parameters, ensuring that the image adheres closely to the intended concept.\n\nThey utilized LAG (Localized Attention-Guided sampling) to improve the precision of image generation, particularly in personalization scenarios where specific local changes are required in the generated image.\n\nThe performance of their method showed competitive results against established baselines with fewer parameters and the user study showed a preference for their method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The paper is overall well-written and easy to follow. \n\nThe authors proposed a novel concept-driven text-to-image generation technique, grounded in a low-rank personalization approach, that addresses challenges associated with traditional fine-tuning methods.\n\nThe paper demonstrates that localized attention-guided sampling effectively mitigates the overfitting to specific concepts. This design reflects a thoughtful integration of both attention and residuals. Additionally, a detailed analysis is presented, highlighting the advantages and limitations of LAG compared to conventional sampling technique.\n\nThe methods are robust and supported by empirical results. They introduce a unique solution to address the issue of concept forgetting in fine-tuning, making the approach both scalable and efficient."
            },
            "weaknesses": {
                "value": "Robustness: As the author mentioned, any shortcomings in the attention maps could significantly impair the overall performance of the model. \n\nMacro class sensitivity: The choice of macro class can influence the performance of the model and its general applicability. Selecting the optimal macro class for certain datasets or domains, where the macro class is ambiguous, might require extensive fine-tuning or human trial-and-errors to select the right macro class.\n\nMinor details: There is a typo in the introduction where \u201can\u201d is misspelled as \u201cam\u201d."
            },
            "questions": {
                "value": "Given instances where the output of the model significantly diverges from the expected outcomes, can you propose or discuss any introspective mechanisms within the paper to determine which specific component or step might be responsible for the observed discrepancy?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6266/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698816329091,
        "cdate": 1698816329091,
        "tmdate": 1699636685598,
        "mdate": 1699636685598,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qZsv36nt66",
        "forum": "mHQEyXaULY",
        "replyto": "mHQEyXaULY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6266/Reviewer_dZ91"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6266/Reviewer_dZ91"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors proposed a method for subject-driven/personalized text-to-image generation. The proposed method is a combination of LoRA training for the output projection layers and constrains the effect of personalized weight on specific areas determined by the cross attention maps. The method shows comparable performance to competing baselines with relatively low computational requirements."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "This paper is well written and very easy to follow. The qualitative examples shown in the paper demonstrate comparable performance against the baselines while providing some improvements on the training computation requirement and time."
            },
            "weaknesses": {
                "value": "1. My main concern about this paper is its novelty. \n\n     (1) The paper can be summarized as \u201cDreamBooth + LoRA + Paint-by-Words\u201d and none of these components is new. The authors claim that their localized attention guidance (LAG) is a new method. However, the essence of this algorithm is to edit the cross attention maps using binary masks. This technique has been widely applied to many papers like Paint-by-Word and Prompt2Prompt which are two papers that the authors cited, and others like \u201cDiffusion Self-Guidance for Controllable Image Generation\u201d and \u201cLocalized Text-to-Image Generation for Free via Cross Attention Control\u201d which the authors did not cite. \n\n    (2) The authors also label the \u201cDreamBooth + LoRA\u201d method as \u201cOurs\u201d, but the only design choices that the authors made are the layers to apply LoRA and dropping the prior preservation loss. They also claim that the benefits from LoRA \u201celiminate the burden on the user to determine an effective set of regularization images\u201d. However, DreamBooth uses ancestral sampling to generate the set of regularization images and it does not require the users to determine that. As a result, I don\u2019t see the so-called \u201cburden\u201d being a very big problem.\n\n2. The evaluation of the results does not contain any metrics for fidelity, which is a standard evaluation criterion adopted by most of the image generation tasks and the baselines the authors chose.\n\n3. The authors did not mention any discussion about the limitations and failure case analysis of their method.\n\n4. The quantitative improvement of the method is not very significant."
            },
            "questions": {
                "value": "It would be great if the authors can write out the learning objectives that they used in the paper"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6266/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6266/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6266/Reviewer_dZ91"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6266/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698877374521,
        "cdate": 1698877374521,
        "tmdate": 1699636685490,
        "mdate": 1699636685490,
        "license": "CC BY 4.0",
        "version": 2
    }
]