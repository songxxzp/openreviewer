[
    {
        "id": "rGvKhs976j",
        "forum": "u2as4lHoyl",
        "replyto": "u2as4lHoyl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2055/Reviewer_xtbe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2055/Reviewer_xtbe"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on an interesting task, updating outdated knowledge in text-to-image generative models.\nTo this end, this paper introduces a simple method namely ReFACT to edit factual associations without relaying on explicit input from end-users or costly re-training.\nSpecifically, ReFACT only modifies a tiny of model's parameters in the text encoder.\nExperiments show that ReFACT achieves superior performance in both generalization to related concepts and preservation of unrelated concepts."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "See summary."
            },
            "weaknesses": {
                "value": "1. Although this paper focuses on a very interesting task, its technique contributions are very limited. The proposed ReFACT is more like an application of [1] in text-to-image generative models.\n2. In my opinion, the number of negative samples $N$ is very large in contrastive loss. Could the authors provide ablation experiments on this hyperparameter?\n3. The proposed method may not be very effective in real-world scenarios, since each mistaken concept requires feedback from human and additional fine-tuning. Furthermore, it cannot handle with unseen visual concepts, either.\n\n\n\n\n\n\n[1] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual\nassociations in gpt. Advances in Neural Information Processing Systems, 35:17359\u201317372, 2022a."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2055/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2055/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2055/Reviewer_xtbe"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2055/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698467278068,
        "cdate": 1698467278068,
        "tmdate": 1700714858353,
        "mdate": 1700714858353,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OZ4jjuKzXt",
        "forum": "u2as4lHoyl",
        "replyto": "u2as4lHoyl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2055/Reviewer_AyFi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2055/Reviewer_AyFi"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an approach to update the text encoder of a text-to-image model to update factual knowledge within the text encoder (e.g., map \"president of USA\" from Donal Trump to Joe Biden). The update to the text encoder doesn't need additional training data and only requires very few parameters to be updated."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is well written and well explained. The approach seems to be easy to implement, doesn't require additional training data, is done in closed form, and only changes a minimal amount of parameters.\n\nThe problem statement is also a realistic one in the sense that we don't want to retrain large models more often than absolutely necessary, so being able to update specific parts of them is useful."
            },
            "weaknesses": {
                "value": "I think a very simple baseline that is missing from the quantitative evaluation is to simply replace a concept/word with its intended new meaning, e.g., replace \"President of the USA\" with \"Joe Biden\" (or \"phone\" with \"smart phone\" etc). For most of the examples shown in the paper this would be a pretty straight forward approach to implement at large scale and wouldn't need any updates to the model parameters at all.\n\nAlso, it would be interesting to see how well the model handles more complicated real-world scenarios, e.g., what happens if someone uses another description for the president of the US (e.g., \"American president\", \"head of the military\", etc). Basically, it's not clear to me how well this approach translates to the complexities of the real world where it's not simply replacing one phrase with another phrase (which can already be achieved by the simple baseline I mentioned above). The generalization evaluation takes a step in that direction but I don't think it's general enough.\n\nThe same holds for specificity, for which I think a more general evaluation is necessary (again, sticking with the example above, what if the caption is simply \"a photo of the president\", would it show Joe Biden even though it doesn't specify that it should be the American president)?"
            },
            "questions": {
                "value": "From a practitioner's point of view I wonder how well this scales to even more edits. Specifically, some edits might affect very similar parts of the text encoder, e.g., I might want to edit who is the president of multiple countries, would that still work?\n\nAlso, what are your thoughts on making the edits more context driven, e.g., apply a specific edit only if another condition is true (e.g., leaves of trees are green, unless the caption specifies it's autumn, in which case leaves should be brown)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2055/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698801582731,
        "cdate": 1698801582731,
        "tmdate": 1699636137460,
        "mdate": 1699636137460,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "u1R5KNWOwm",
        "forum": "u2as4lHoyl",
        "replyto": "u2as4lHoyl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2055/Reviewer_sFcd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2055/Reviewer_sFcd"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes to optimize a representation in text encoder to change the knowledge of text-to-image diffusion models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. It is an interesting task and show that it is possible to hack the text-to-image diffusion model and change the knowledge.\n\n2. The generated results show that the method is able to replace the old concept with new ones."
            },
            "weaknesses": {
                "value": "1. The approach. The method carefully select the positive and negative prompts to update the representation layer to change the knowledge. The main problem is the selection. The objective may overfits the prompt and may not perform good at other cases. For instance, the second rows  in figure 1. show good results of the positive prompt in figure 3. It is unclear what would happen to other unshown prompts? For instance, can we generate \"prince of wales is drinking coffee\" and the sentence is not seen by the model? This result is necessary to the approach. Otherwise, we may just choose to add oracle desciption \"William\" to generate images rather than finetuning the model.\n\n2. The evaluation. When authors perform evaluation on ROADS or TIME dataset, is the text-encoder updated every time once a new image is presented to the model? Or you copy the original model finetune the model for each concept? In addition, although authors show that the FID and CLIP is almost identical to the baseline model on the new datasets. It is necessary to include the FID and CLIP results on some benchmark text2image datasets to show that the model is able to generate other images after the tuning. \n\n\nI'm happy to raise the score if above concerns are addressed."
            },
            "questions": {
                "value": "as above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2055/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698809483009,
        "cdate": 1698809483009,
        "tmdate": 1699636137351,
        "mdate": 1699636137351,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "90MHJTJcXP",
        "forum": "u2as4lHoyl",
        "replyto": "u2as4lHoyl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2055/Reviewer_c2qA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2055/Reviewer_c2qA"
        ],
        "content": {
            "summary": {
                "value": "The text-to-image models often store factual information that can become outdated, limiting their usefulness. The authors proposed a new method -- ReFACT that can address this challenge by updating specific parts of the model without requiring direct user input or expensive re-training. The approach is evaluated on existing and newly created datasets and outperforms other methods in terms of adapting to related concepts while preserving unrelated ones."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The key strengths of the proposed method, ReFACT can be listed as follow\n\n1.\tEfficient Factual Updates: ReFACT efficiently updates factual information in text-to-image models without the need for extensive retraining, ensuring that the models stay up-to-date.\n\n2.\tPrecision and Control: It allows for precise and controlled editing of facts, ensuring the accuracy of the desired factual updates.\n\n3.\tSuperior Performance: ReFACT outperforms other editing methods, maintains image generation quality, and demonstrates strong generalization to related concepts, making it a highly effective tool for text-to-image model editing.\n\nThe paper is well-organized and the proposed method is easy to reproduce."
            },
            "weaknesses": {
                "value": "1.\tThe evaluation dataset is relatively small, and it would be beneficial to include a wider variety of prompts to evaluate ReFACT. For instance, additional prompts could involve questions about the current President of the United States or synonyms of the target prompts. This expanded evaluation would provide a more comprehensive assessment of ReFACT's performance and its ability to handle a diverse range of factual associations.\n2.\tThe proposed method, ReFACT, appears to be straightforward in its approach to updating factual information in text-to-image models. However, the authors should clearly establish the differences between ReFACT and existing methods, such as \"textual inversion.\" It is essential to provide a detailed comparison to highlight how ReFACT distinguishes itself."
            },
            "questions": {
                "value": "See the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2055/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2055/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2055/Reviewer_c2qA"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2055/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699497471870,
        "cdate": 1699497471870,
        "tmdate": 1699636137280,
        "mdate": 1699636137280,
        "license": "CC BY 4.0",
        "version": 2
    }
]