[
    {
        "id": "cgbKYmD4zQ",
        "forum": "unE3TZSAVZ",
        "replyto": "unE3TZSAVZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6027/Reviewer_VLQY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6027/Reviewer_VLQY"
        ],
        "content": {
            "summary": {
                "value": "The paper seeks to understand scaling laws for modular neural networks and proposes a method for training them. Modular neural network here refers to models that sum the output of their constituent modules each of which receive (different) low-dimensional projections of the input. The paper theoretically shows that when the modules are linear neural networks that receive a linear projection of the input into a fixed dimensional space, and the data comes from a model of the same form, sample complexity is independent of the task intrinsic dimension $m$ (in contrast to the monolithic case where it is exponential in $m$). The paper then proposes a kernel-based rule to learn the initializations of the input projections from data and test the empirical performance on a sine wave regression task and compositional CIFAR."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Understanding the sample complexity of training modular vs. monolithic neural networks is an important open problem for which a theoretical contribution is potentially impactful.\nThe theory identifies a reasonable setting for a tractable analysis and is overall convincing (without having checked the proofs in the appendix).\nOverall the paper is well presented and transparent about the merits and limitations of the analysis."
            },
            "weaknesses": {
                "value": "The scaling behaviour is studied theoretically in the case of linear neural networks for tractability. A more thorough empirical investigation to what extent this scaling law is practically relevant in the nonlinear setting would have been useful. As far as I understand the experiments conducted do not reflect the theoretical result of constant sample complexity in the input dimension. I was missing a discussion on this point.\n\nI am a bit worried about the reproducibility of the empirical part of the paper since no code was provided as part of the submission. I also encourage the authors to specify the exact number of seeds per experiment in Figure 3b as \"up to five seeds\" as stated in the caption could technically mean only one seed is reported."
            },
            "questions": {
                "value": "1. The modular learning rule minimizing the norm of the $\\theta_i$ is applied as a pretraining step assuming that the $\\varphi(X;\\hat{U}_i$ are sufficiently expressive. Since this is before training, can you elaborate why this assumption might be justified and to what extent the algorithm is robust to a violation of it? \n2. There are discrepancies between the theory and toy model in Figure 5 as the paper points out in App A.2. Can you elaborate why this is not a matter of concern for the theory, i.e. what exactly causes the mismatch?\n3. Figure 5 is missing labels and the caption is a bit sparse. Could you specify how exactly the four plots differ? Maybe adding a colour bar to indicate the values of the light lines could be helpful? How do the theoretical predictions look like for individual (light) lines?\n4. Figure 12 is missing a legend for what the colours encode. Could you please clarify?\n\nSuggestions / typos:\n- I think it would be useful to show both the theoretical prediction and empirical validation in Figure 2 (similar to Figure 5 in Appendix A).\n- Page 7 \"and the test loss and dependence of the test loss\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6027/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6027/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6027/Reviewer_VLQY"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6027/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698684142962,
        "cdate": 1698684142962,
        "tmdate": 1699636647779,
        "mdate": 1699636647779,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "i1UhWlFIrT",
        "forum": "unE3TZSAVZ",
        "replyto": "unE3TZSAVZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6027/Reviewer_MGmV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6027/Reviewer_MGmV"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a theoretical model of NN learning, specifically predicts that while the sample complexity of non-modular NNs varies exponentially with task dimension, sample complexity of modular NNs is independent of task dimension. The authors then develop a learning rule to align NN modules to modules underlying high-dimensional modular tasks, and presents empirical results which demonstrate improved performance of modular learning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper presents the first theoretical model to explicitly compute non-asymptotic expressions for generalization error in modular architectures, develops new modular learning rules based on the theory and empirically demonstrated the improved performance of the new method."
            },
            "weaknesses": {
                "value": "Validation of theoretical results is only shown in the appendix, with large discrepancy between theoretical predictions and numerics, I think more empirical evaluations are needed to verify the theoretical result."
            },
            "questions": {
                "value": "1. What causes the large deviation of the test loss between actual and predicted in Figure 5?\n2. In figure 4 (also figure 3b), the total range of the similarity score is quite small, it is therefore difficult to say whether the result is a significant improvement from baseline."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6027/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6027/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6027/Reviewer_MGmV"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6027/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698721898802,
        "cdate": 1698721898802,
        "tmdate": 1699636647663,
        "mdate": 1699636647663,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bnmINjC8VE",
        "forum": "unE3TZSAVZ",
        "replyto": "unE3TZSAVZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6027/Reviewer_Jw6v"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6027/Reviewer_Jw6v"
        ],
        "content": {
            "summary": {
                "value": "This paper analyzes the sample complexity of modular neural networks and shows theoretically how the sample complexity of modular networks doesn't depend on the intrinsic dimensionality of the input. This is proven for linear models. The theory is supported by experiments on 1) sin wave regression and 2) compositional CIFAR10. The paper further proposes a learning rule to ensure the modularity of the task is aligned with the modularity of the network."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This is the first paper to conduct a rigorous theoretical analysis of modular neural networks. Understanding the empirical success of modular neural networks is an important open problem. \n\n2. The theoretic analysis and the effect of different terms in the generalization bound are presented clearly. \n\n3. Assumptions for the theoretical analysis are presented clearly. \n\n4. Related work is covered well and in thorough detail."
            },
            "weaknesses": {
                "value": "1. Including synthetic experiments in the linear model to demonstrate how the sample complexity changes for modular and non-modular networks in a specific setting."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6027/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698816218313,
        "cdate": 1698816218313,
        "tmdate": 1699636647539,
        "mdate": 1699636647539,
        "license": "CC BY 4.0",
        "version": 2
    }
]