[
    {
        "id": "WMANAhXeoU",
        "forum": "J562Q8Hjut",
        "replyto": "J562Q8Hjut",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9354/Reviewer_1ymx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9354/Reviewer_1ymx"
        ],
        "content": {
            "summary": {
                "value": "The authors present \u201cPEACH: Pretrained-Embedding Explanation across contextual and hierarchical structure\u201d in which they propose an approach for finding a human-understandable interpretation of text classification with pre-trained language models.\nThe proposed approach works as follows: At first, PLMs are fine-tuned to the text classification tasks. The resulting vector representations of the CLS tokens are taken as Document embedding and are processed further. For this second feature processing step, the authors propose three variants, a Pearsons correlation based clustering, a k-means clustering based method, and a CNN. The goal of all three are to reduce the dimensions of the resulting feature matrix, and to identify the most informative dimensions for those features. \nThese features are then used to create a Decision Tree (based on the commonly used algorithms) that are built to solve the text classification task. Since the feature dimensions and the resulting decision tree nodes by themselves do not provide a human interpretable representation, the crucial task is to provide an interpretable decision tree node. This is accomplished by collecting all documents that pass through a specific node and use their tf-idf word-cloud representation as a visualization of this node. This procedure is done with all nodes, so that every node can be visually represented by a specific word cloud built from the documents that pass through them.\nTheir approach is then evaluated on a performance level, and on an interpretability level. In the performance evaluation, they compare their feature based decision tree approaches with the fine-tuned models. The interpretability is evaluated by human participants that evaluate how much they trust a given classification and a corresponding explanation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "-\tThe authors adopt an interpretability model from the computer vision domain to pre-trained language models and evaluate it with human study participants\n\n-\tThe presentation quality makes the paper easy to follow"
            },
            "weaknesses": {
                "value": "-\tIt is unclear to me how the improvements over the baselines are related to the interpretability aspect. It would be helpful to explain whether this is just a byproduct of the main research, or if it was a goal by itself.\n\n-\tAlthough the related work is listed and categorized, a little more explanation \u2013 especially for the LIME approach, which is compared to the proposed PEACH approach \u2013 would provide more background for the human evaluation task."
            },
            "questions": {
                "value": "-\tTable 1: separate the results from Peach and the baseline models; maybe just add a horizontal line for easier comparison \n-\tAre the feature extraction methods crucial for the increased performance, or just happen to work well?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9354/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698744818938,
        "cdate": 1698744818938,
        "tmdate": 1699637176881,
        "mdate": 1699637176881,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uYbaWkF8In",
        "forum": "J562Q8Hjut",
        "replyto": "J562Q8Hjut",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9354/Reviewer_HiZh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9354/Reviewer_HiZh"
        ],
        "content": {
            "summary": {
                "value": "- This paper introduces a tree-based explanation technique, titled PEACH (Pretrained-embedding Explanation Across Contextual and Hierarchical Structure), designed to elucidate the classification process of text-based documents through a tree-structured methodology.\n- The authors demonstrate the utility of these explanations by employing word-cloud-based trees.\n- Experimental results showcase that the classification outcomes achieved by PEACH either surpass or are on par with those derived from pretrained models."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper attempts to apply decision tree-based interpretations used in the CV field to the NLP domain. Theoretically, it is possible to combine certain explanatory elements under specific conditions to provide a more comprehensive, logical, and human-cognitively consistent interpretation."
            },
            "weaknesses": {
                "value": "This article asserts that the proposed word-cloud-based trees are human-interpretable and can clearly identify model mistakes and assist in dataset debugging. However, based on the examples illustrated in the figures of this paper (such as Figure 26), the word-cloud-based trees do not exhibit strong human-interpretability. They also do not align well with the semantics of the input sentences. From a human perspective, they do not provide clear explanations that would reduce cognitive load.\n\nThe experiments related to interpretability only compare the effectiveness of PEACH against LIME and Anchor, but do not conduct comparisons with the more human-intelligible Rationalization series of methods (Rationalizing Neural Predictions and subsequent series of studies).\n\nThere are several aspects of the paper that require improvement. The paper contains minor writing errors. The explanation of PEACH's interpretability in the introduction is not clear, the rationale behind word clouds is not analyzed, there is no systematic diagram, which hinders readability.\n\n- In the description of MR in the DATASETS section, it mentions \"two training documents,\" and it's unclear if this is an error.In the description of IMDB, there is a grammatical error; the word \"and\" should probably be changed to \"has.\"\n- In the IMDB column of Table 1, there are two bolded numbers, but it appears that there should be only one bolded number.\n- In the article, the figures and tables are located far from their corresponding references, which hinders readability.\n- The focus of PEACH should primarily be on interpretability, specifically on whether it can enhance human understanding of the text classification process. The article should emphasize this point in the introduction. However, the introduction lacks a direct comparison and analysis with previous methods, making it less intuitive. Figure 1 is placed in the introduction but is not explained, making it difficult to understand just by looking at it. In comparison, Figure 26 seems to be more fitting for inclusion in the introduction.\n- The paper solely utilizes TF-IDF to construct word clouds without conducting an interpretability analysis. In the provided examples, it is challenging to discern the specific meanings of word clouds within the nodes, and there is limited overlap between the words in the word clouds and the sentences that need to be explained. This approach does not appear to offer stronger interpretability compared to previous methods.\n- PEACH lacks a systematic diagram."
            },
            "questions": {
                "value": "In CV, decision tree nodes commonly employ specific image segments and representative patterns. In NLP, the choice of what to use as nodes is a topic of discussion. For the use of decision tree methods, employing word clouds as nodes in NLP doesn't seem as intuitive as using image segments as nodes in CV. At least, the examples provided in this article demonstrate this point."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9354/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698792671943,
        "cdate": 1698792671943,
        "tmdate": 1699637176654,
        "mdate": 1699637176654,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Z7iOIL4lx0",
        "forum": "J562Q8Hjut",
        "replyto": "J562Q8Hjut",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9354/Reviewer_TT7T"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9354/Reviewer_TT7T"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a method for explaining the decisions made by a pretrained language model like BERT finetuned on a text classification dataset. The method consists of 4 steps: First, the pretrained model is finetuned in the typical way, i.e., by adding a classifier head on top of the CLS token. The CLS token representation at the last layer then constitutes the text embedding. Secondly, there is a feature dimensionality reduction step by either a) grouping individual dimensions by their correlation or b) by applying K-Means or c) by training a CNN on top. Third, a decision tree is trained on top of the reduced feature set. Finally, the nodes of the resulting decision tree are annotated with TFIDF statistics collected from the text examples associated with each node. These statistics are visualized as word clouds.\nThe method is evaluated on 5 text classification tasks with various datasets. The results indicate that the classification performance of the decision tree classifier reaches comparable performance to the finetuned model alone. A human evaluation is performed to compare the interpretability of the proposed methods to LIME and Anchor, two common baselines, which indicates that the proposed method performs drastically better. Additionally, some ablations and qualitative evaluations are performed."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* the paper proposes an intuitive and reasonable method\n* the method is effective, both in the sense that the added interpretability doesn't come at the cost of classification performance and in the sense that the interpretability is much better than for the baseline methods according to human raters.\n* the paper is relevant to the community"
            },
            "weaknesses": {
                "value": "* It is not clear what problem the paper tries to solve. While it is motivated by the lack of trustworthyness of attention-score methods, there is no comparison in the end. Concrete research questions are missing. The introduction asks prominently \"What should be considered a node of the decision tree in NLP tasks?\", but this question is not revisited again.\n* The datasets used during evaluation are quite easy. Many of them are just topic classification, which can be solved by extracting a few keywords. No advanced reasoning is required, which puts the value of the method in question.\n* It is not clear how local explanations are obtained without manual inspection. No automatic procedure is described that would explain the annotations in the example figures.\n* The choice of baselines for the interpretability evaluation is not motivated. Explanations of how the baselines work are missing. There is no explanation of why the proposed method works better by such a large margin, which means that the reader doesn't learn much from the paper.\n* Individual components of the method's pipeline are well known techniques without any novelty apart from their straight forward combination."
            },
            "questions": {
                "value": "* visualizations are computed based on word statistics in the documents corresponding to a note. However, those are global statistics, so is there any guarantee that these can serve as local interpretations?\n* since the visualizations are done based on TF-IDF, I wonder what is the added value of training the decision tree based on features from PLMs? What would be the performance if the decision trees were trained on word clusters directly?\n* are the score differences in Figure 2 statistically significant? They seem quite small, and somewhat random.\n* Section 4.3: Why did you choose LIME and Anchor as baselines? There is no description of how they work or how they were trained. The advantage of PEACH over them is very high, which remains unexplained without given further context of how these methods work different.\n* How are the local explanations (Figure 3) generated? Some of the highlighted words have no exact representation in the word cloud. E.g., for ELMo, engaging is highlighted but not present in the word cloud. Conversely, entertaining is highlighted in the word cloud but not present in the text. Are these local explanations generated automatically?\n* The low trustability of attention-score based interpretation methods are referenced as one of the main motivations in the introduction. Why are they not compared against as baselines?\n* A study with human evaluators was conducted. Details regarding this study are missing, and it is not clear whether approval from an ethics board was seeked."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Responsible research practice (e.g., human subjects, data release)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "A study involving human evaluators was conducted in this paper. Details regarding the implementation of the study and the potential approval through an ethics board are missing."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9354/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698854935938,
        "cdate": 1698854935938,
        "tmdate": 1699637176531,
        "mdate": 1699637176531,
        "license": "CC BY 4.0",
        "version": 2
    }
]