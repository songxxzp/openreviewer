[
    {
        "id": "7xx05cH1Fu",
        "forum": "Pzir15nPfc",
        "replyto": "Pzir15nPfc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5863/Reviewer_Y1Xv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5863/Reviewer_Y1Xv"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces Contextual Vision Transformers (ContextViT), a method to address structured variations and distribution shifts in image datasets. It leverages context tokens and token inference models to enable robust feature representation learning across groups with shared characteristics. The paper provides evidence of ContextViT's effectiveness through experiments in gene perturbation classification and pathology image classification."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper introduces a novel method, ContextViT, to address structured variations and distribution shifts in image datasets. It brings a unique perspective to the problem of improving feature representations for vision transformers.\n- The paper is well-written and provides clear explanations of the methodology, experiments, and results.\n- ContextViT is extensively evaluated in different tasks, showcasing its effectiveness in improving out-of-distribution generalization and resilience to batch effects."
            },
            "weaknesses": {
                "value": "- How to chose and define the \"in-context\" prompt is unclear.\n- While the paper is well-structured and well-written, it would be beneficial to include more detailed comparisons with related work to highlight the novelty of the proposed approach.\n- In the \"Out-of-Distribution Generalization (Pathology Images)\" section, it's not entirely clear what \"linear probing accuracy\" means and how it relates to out-of-distribution generalization. A more in-depth explanation of this metric would improve the clarity of the paper."
            },
            "questions": {
                "value": "- Are there any specific use cases or domains where ContextViT is particularly well-suited, and are there any limitations or scenarios where it may not perform as effectively?\n- Could the authors provide more insights into how ContextViT's approach to handling structured variations and distribution shifts could be applied in practical applications outside of the ones discussed in the paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5863/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698648431725,
        "cdate": 1698648431725,
        "tmdate": 1699636621015,
        "mdate": 1699636621015,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Vqkd072sYX",
        "forum": "Pzir15nPfc",
        "replyto": "Pzir15nPfc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5863/Reviewer_hyoq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5863/Reviewer_hyoq"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a Contextual Vision Transformers (ContextViT) based on ViT. ContextViT is designed for adapting ViTs to OOD data with varying latent factors. This work is inspired by in-context learning and prepends tokens to input sequences for alleviating model performance. This paper finds out that standard context tokens might not be able to generalize to unseen domains, therefore it proposes a context inference network that estimates context tokens from input images. The proposed method is evaluated with cell-imaging and histopathology datasets and achieves performance improvements under distribution shifts.\n\nPros:\n\n- This paper is well-written and easy to follow.\n- Figure 1 is well drawn to illustrate the overall idea of this work.\n- Layer-wise context conditioning is well-motivated and makes sense.\n\nCons:\n\nThe novelty of this work is limited.\n- The intrinsic difference between this work and visual prompting [1] is unclear. It seems that visual prompting can also fit this OOD scenario. \n- The key idea of this work is similar to [2], which also uses a network to predict the context/domain tokens.\n- The comparison in experiment section is insufficient.\n- Lack of visualization of the learned context token, which shows the difference of context tokens of different groups.\n\nThe paper is simple and effective, but its novelty is unfortunately limited, and analysis for the insight of this approach is absent.\n\n[1] Jia, Menglin, et al. \"Visual prompt tuning.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.\n[2] Zhang, Xin, et al. \"Domain Prompt Learning for Efficiently Adapting CLIP to Unseen Domains.\" arXiv preprint arXiv:2111.12853 (2021)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This paper is well-written and easy to follow.\n- Figure 1 is well drawn to illustrate the overall idea of this work.\n- Layer-wise context conditioning is well-motivated and makes sense."
            },
            "weaknesses": {
                "value": "The novelty of this work is limited.\n- The intrinsic difference between this work and visual prompting [1] is unclear. It seems that visual prompting can also fit this OOD scenario. \n- The key idea of this work is similar to [2], which also uses a network to predict the context/domain tokens.\n- The comparison in experiment section is insufficient.\n- Lack of visualization of the learned context token, which shows the difference of context tokens of different groups.\n\nThe paper is simple and effective, but its novelty is unfortunately limited, and analysis for the insight of this approach is absent."
            },
            "questions": {
                "value": "-"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5863/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5863/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5863/Reviewer_hyoq"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5863/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698668871432,
        "cdate": 1698668871432,
        "tmdate": 1699636620915,
        "mdate": 1699636620915,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZJxLfFrgQv",
        "forum": "Pzir15nPfc",
        "replyto": "Pzir15nPfc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5863/Reviewer_4Dw1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5863/Reviewer_4Dw1"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes ContextViT to address the distribution shift between different datasets. ContextViT uses a context inference model taking the dataset as input to get a context embedding for the dataset, and predicts the label conditioned on the context embedding (token). It also makes this process layer-wise to capture different-scale distribution shift."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper presents a method to mitigate the distribution gap between different datasets. Based on their experimental results, the proposed method, ContextViT, has the ability to improve the performance under distribution shift."
            },
            "weaknesses": {
                "value": "- The paper mentioned that the proposed method applies the concept of in-context learning in vision transformer. However, in my opinion, in-context learning is a kind of few-shot learning, which predicts based on the (data, label) pair of a few samples, unlike the usage of all the dataset-c data (or a batch of the data) in this paper. The method looks like a summarization of the dataset information and then makes the prediction based on that summarization.\n\n- The method requires a lot of distribution-c data at the inference stage and increases the inference overhead. \n\n- The oracle-context model is very similar to some prompt tuning works, like Visual Prompt Tuning & Prompt Learning for Vision-Language Models, but these works are not discussed in the paper."
            },
            "questions": {
                "value": "Please see weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5863/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698815668878,
        "cdate": 1698815668878,
        "tmdate": 1699636620798,
        "mdate": 1699636620798,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "y6HZ20MhzI",
        "forum": "Pzir15nPfc",
        "replyto": "Pzir15nPfc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5863/Reviewer_kTos"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5863/Reviewer_kTos"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces an improved ViT where some group-specific context information from the sub-groups in datasets is collected and generated from those images in a group. The network generates the context token from those images and appends them to image patch embeddings. Their experiments show some improvement of this ViT on some group-specific datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The idea of capturing context information from the datasets is interesting.\nThe writing of this method is clear and easy to follow.\nThe experiments demonstrate the efficiency of their proposed framework on both the dataset with the same distribution and other datasets with different distributions."
            },
            "weaknesses": {
                "value": "The view and impact of this paper are limited. It seems the method focuses on improving the performance of the datasets that contain several distinct groups. Although the authors demonstrate improvements on some specific datasets, the improvement in general image tasks is still unclear. It is suggested to widely evaluate their framework on other popular datasets and tasks or extend related techniques to improve the capability of transfer learning from one task to some other tasks. It should also be compared with more related works.\n\n\nDespite the proposed contextual learning paradigm, the technical contributions in this paper are limited and not novel enough.\n\n\nSome unclear presentations:\n\n1. Figure 1 is unclear and somehow misleading. The source of the context (where those images come from) and the function (input, output) of the inference model should be labeled. I strongly suggest redoing this figure.\n\n2. The end of page 5 is missing.\n\n3. Table 2 looks messy and should be redesigned."
            },
            "questions": {
                "value": "What would the performance be if we want to apply this framework to a large dataset that was combined with several small datasets?\n\nIf we don't know the sub groups of the data, is there anyway to benefit from the proposed framework?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5863/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698862685115,
        "cdate": 1698862685115,
        "tmdate": 1699636620699,
        "mdate": 1699636620699,
        "license": "CC BY 4.0",
        "version": 2
    }
]