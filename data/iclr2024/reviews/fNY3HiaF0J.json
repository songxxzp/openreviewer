[
    {
        "id": "b9mI41Hpwm",
        "forum": "fNY3HiaF0J",
        "replyto": "fNY3HiaF0J",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1600/Reviewer_Y1yT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1600/Reviewer_Y1yT"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to solve the problem that existing text-to-image diffusion models sometimes fail to generate realistic human faces and hands. To this end, they collect several human face/hand specific datasets and use them to finetune the pretrained diffusion model. LoRA and MoE techniques are adopted for reducing updated-parameter scale and merging different trained modules respectively."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This work moves a successful step towards generating more natural hands (especially) and faces.\n- The proposed MoE strategy to allow different LoRA modules to work together is novel and makes sense."
            },
            "weaknesses": {
                "value": "It is obvious that recent progress in text-to-image generation, including this work, largely relies on the collected datasets. My main concern is about whether the collected datasets in this work would have ethical issues thus affect the model preference, given that it is a sensitive human-centric research topic, and unfortunately the collection and curation process is not comprehensively discussed. Since the \"Human-in-the-scene images\" and part of the \"Close-up of hand images\" are web-downloaded, they undoubtedly contain questionable contents. I am afraid I am not fully qualified to review this work from the ethical perspective and I recommend an ethics expert to assess this aspect. Nevertheless, I will try to list some concerns below:\n\n- Is the web-collected dataset biased in terms of any subpopulations (e.g. race, age)?\n- Do the generated image captions contain sensitive information of people (e.g. location, race, health situation, financial situation)?\n- Are the raw data of original images logged (e.g. URL) for supporting community investigation and further improvement?"
            },
            "questions": {
                "value": "- The scope of this work is somewhat over-claimed. \"Human-centric\" is a large scope where all human parts should be contained. But this work only considers the face and hand generation issue, leaving others like feet and leg not mentioned.\n\n- The chosen SD v1.5 is somewhat an outdated baseline, since the stable diffusion team is also continually updating their model toward better generation of human content. In SD v2.1 (https://stability.ai/blog/stablediffusion2-1-release7-dec-2022, https://huggingface.co/stabilityai/stable-diffusion-2-1, released about 10 months ago), they claim that \"the new release delivers improved anatomy and hands\" by reducing the number of the filtered-out people in the dataset. It would be better to see whether the proposed method could still boost the human content generation quality upon this release.\n\n- I have some concerns about reproducibility: whether the good results presented in the paper are carefully selected few ones. This can be addressed by\n  - either providing the trained model and code (I haven't seen them on the provided anonymous webpage near the deadline of the review)\n  - or conducting larger-scale quantitive experiments in Fig. 5 with SD XL or SD v2.1."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Discrimination / bias / fairness concerns",
                    "Yes, Privacy, security and safety"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Please refer to the \"weaknesses\" part for my concerns."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1600/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698637896851,
        "cdate": 1698637896851,
        "tmdate": 1699636088620,
        "mdate": 1699636088620,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zL1524huve",
        "forum": "fNY3HiaF0J",
        "replyto": "fNY3HiaF0J",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1600/Reviewer_39sw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1600/Reviewer_39sw"
        ],
        "content": {
            "summary": {
                "value": "Stable Diffusion Models (SDMs) often struggle to generate accurate facial and hand details, leading to unrealistic artefacts like ghost fingers. This paper attempts to tackle this problem and optimise SDMs for human-centric generation applications. \nTo achieve this, a dataset of one million human-focused images is collected, which includes three subsets of text-image pairs, close-up face images and close-up hand images.A multi-stage Mixture of Low-rank Experts training framework is proposed to leverage the different subsets and to adaptively combine the predictions from face and hand experts.\nThe results show that the proposed method empowers SDMs to create images with faithful facial and hand details. Moreover, the numerical metrics, measured by Human Performance Scores and ImageReward scores, surpass those of other text-image generation models, establishing a new state-of-the-art."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The dataset and benchmark contributions are valuable for developing data-hungry image-text generative models like diffusion models.\nThe paper is well-written and easy to follow.\nThe visualisations and accompanied website offer abundant visual examples.\nCode, data, and benchmarks are promised to be open-sourced."
            },
            "weaknesses": {
                "value": "The proposed dataset has significant contributions, but lacks details and insights. Descriptions of the dataset distributions lack numerical support. For example, the authors claim \"These images are diverse w.r.t. occasions, activities, gestures, ages, genders, and racial backgrounds\" but there is no evidence to support this making the claim unsubstantiated.\n\nThe preprocessing of the human-in-the-scene dataset is vaguely explained. For example, they seem to \"train a VGG19 to filter out images that contain little information about people\" but how such images were identified and how the VGG19 was trained are not present.\n\nBased on the description in Section 4.1 without diving into references, the classifiers used in evaluation metrics could inherit bias from training dataset. Such bias is especially harmful for human-centric applications. The bias is also present in visualisations where most generated humans have light skin colour.\n\nThe methodology section has limited contributions, with components being adaptations of existing works and lacking insightful studies of why the adapted components are necessary than other alternatives. It may be more appropriate to present these findings at a conference with a dedicated Dataset and Benchmark's track.\n\nLastly, the usefulness of the fine-tuned SDM in generic image generation is not addressed."
            },
            "questions": {
                "value": "* Is there any study on failure cases? For instance, in Figure 10, \"a young man skateboarding while listening to music,\" an unrealistic half-man is generated, even though the hand and face appear natural. What role does each component play in causing such failures?\n* As mentioned in the weaknesses, is there any quantitative evidence to support the diversity of the proposed dataset, apart from qualitative visualisations? For example, a gesture classifier and an object detector could be used to explore the variety within the hand dataset. A similar approach can be applied to other subsets as well.\n* What does \"adding stage 2\" mean in section 4.3.1? There are two low-rank experts trained on two separate close-up datasets (one for faces and the other for hands). Are both experts utilised in the \"stage 2\" ablation, and if so, how are their predictions combined? It would be beneficial to compare this approach with a simpler mixture method, such as directly adding the two predictions.\n* As there is already large amount of AI-generated images online, how was it guaranteed that all the collected images are real during the web-crawling stage?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Discrimination / bias / fairness concerns",
                    "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "- The constructed human-centric dataset may contain unfair distributions and no fairness study has been conducted.\n- The authors do not have consents for distributing the images that contain human faces. This may not comply with GDPR or other local laws and the images could be misused by companies if published."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1600/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1600/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1600/Reviewer_39sw"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1600/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698660283906,
        "cdate": 1698660283906,
        "tmdate": 1699636088543,
        "mdate": 1699636088543,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "toeNYbJ8QW",
        "forum": "fNY3HiaF0J",
        "replyto": "fNY3HiaF0J",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1600/Reviewer_FSG6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1600/Reviewer_FSG6"
        ],
        "content": {
            "summary": {
                "value": "In the human-centric text-to-image generation, particularly in the context of faces and hands, the results often lack naturalness due to insufficient training priors. The authors address this problem from two perspectives. Firstly, on the data front, they create a human-centric dataset comprising approximately one million high-quality person-scene images, along with two distinct sets of close-up facial and hand images. Secondly, in terms of methodology, they propose MoLE, which involves incorporating low-rank modules trained separately on close-up hand and facial images as experts."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper focuses on addressing the issue of stable diffusion models having subpar generation results for human faces and hands, a problem of significant practical importance. \n2. The high-quality human-centric dataset constructed in this work will effectively advances research in this area.\n3. As shown in Figure 7, Mixture-of-experts make low-rank modules work together well. The assembly of two separate modules, each excelling in distinct functionalities, results in a natural and effective approach."
            },
            "weaknesses": {
                "value": "1. The method has obvious limitations, as it requires the collection of substantial amounts of data. How to measure the quality of such data, what about the computational burden of using such data, etc, remain unresolved issues. Besides, the method is ineffective in scenarios involving multiple individuals would also limit its practical value.\n2. Many methods employed in this paper are already in existence, with the primary innovation residing in the \"mixture of low-rank experts\" (MoLE). However, the paper lacks a comprehensive elaboration on the distinctions between MoLE and other mixture-of-experts approaches.\n3. I am curious on the results in Table 1. The numbers are hard to evaluate on the performance improvement of this work."
            },
            "questions": {
                "value": "Please check the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1600/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1600/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1600/Reviewer_FSG6"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1600/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698771964896,
        "cdate": 1698771964896,
        "tmdate": 1699636088451,
        "mdate": 1699636088451,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4fWZcfRPhI",
        "forum": "fNY3HiaF0J",
        "replyto": "fNY3HiaF0J",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1600/Reviewer_B5SW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1600/Reviewer_B5SW"
        ],
        "content": {
            "summary": {
                "value": "Since an open-sourced text-to-image (T2I) model, Stable Diffusion v1.5, has a limitation in generating human-centric images, this study has tried to alleviate this problem in terms of data and model aspects. The authors carefully collect human-centric images, including publicly available datasets such as CelebA and FFHQ, and then fine-tune Stable Diffusion v1.5 on the collected dataset. In addition, this study proposes a Mixture of Low-rank Experts to further fine-tune the T2I model, while a method of soft MoE is adopted especially for increasing the generation quality of faces and hands. On the proposed benchmark, the fine-tuning improves the quality of generated images with respect to human preferences such as HPS and IR scores."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "S1. This study aims to resolve a well-known limitation of open-sourced T2I models, which lead to low-quality of human-centric images including faces and hands.\n\nS2. The experimental results show that fine-tuning on human-centric datasets can improve the quality of generated images in terms of human preference.\n\nS3. The main idea, global and Local assignments of two LoRA experts, is novel and makes sense, while having potential to be further improved and expanded."
            },
            "weaknesses": {
                "value": "W1. The detailed information and statistics of collected data are missing. To ensure the quality of collected human-centric images, the authors need to analyze the characteristics and attributes of the dataset in a systematic and quantitative manner. For example, the distribution of racial groups and gender, the diversity of text prompts including semantic context, or the distribution of face size and the number of faces have to be provided to understand the characteristics of the dataset.\n\nW2. The organization of presentation of this paper should be improved. This paper includes some types and grammatical errors. In addition, the detailed explanation of the proposed method, MoLE, is not provided, while presenting only the overall idea. For example, I cannot find the detailed explanation about how the LoRA experts and gating model $G$ are formulated. \n\nW3. Although the design of the proposed method, MoLE, makes sense, its design is not well-motivated. Please refer to the questions below for the details.\n\nW4. The experimental results are limited to demonstrate the effectiveness of the proposed idea, while the experiments can be considered unfair. Please refer to the questions below for the details.\n\nW5. This paper does not include a discussion about negative social impacts."
            },
            "questions": {
                "value": "Q1. In Section 1, the authors postulate \u201cthe absence of comprehensive and high-quality human centric data within the training dataset LAION5B\u2026\u201d, but the claim does not have supporting analysis. Is there any specific analysis to show that LAION5B does not include comprehensive and high-quality human centric images?  \n\nQ2. In Section 1, the authors also claim \u201cfaces and hands represent the two most complicated parts due to high variability, making them challenging to be generated naturally.\u201d However, although MoLE adopts a soft mixture of LoRAs, the capability of T2I model is not much increased compared with Stable Diffusion v1.5. Can the proposed method synthesize naturally generated faces and hands, while significantly outperforming previous methods? Figure 1 and Figure 8 still show that the proposed method provides unnaturally generated eyes and hands.\n\nQ3. In my opinion, one of the main reasons why a T2I model cannot generate high-quality faces and hands is that complex attributes and details of faces and hands are often located in a small region, considering synthesizing high-quality images with small objects is difficult for T2I models. I wonder the authors\u2019 opinion and the reason why the authors focus on close-up faces and hands for fine-tuning in Stage 2 and Stage 3. \n\nQ4. Can the proposed MoE exploit numerous LoRA experts more than two? Can the proposed method support the adoption of multiple LoRA experts for face and hand, respectively?\n\nQ5. What are the details of each low-rank expert $E_\\text{face}$ and $E_\\text{hand}$? Since they are not formally defined and described, I wonder the exact operation (including mathematical formulations) of each expert. In addition, the operation of the learnable gating layer $G$ is not formally defined. \n\nQ6. My major question is whether the comparison of experimental results is fair or not. First, since MoLE further trains SD v1.5, the results that outperforms SD v1.5 are natural and do not support the effectiveness of MoLE. In addition, I wonder why the CLIP text encoder is further trained together with U-Net. How about the user study to compare MoLE and fine-tuned SD v1.5?\n\nQ7. After fine-tuning of SD v1.5 in Stage 1, is the model capable of generating high quality and diverse images including non-human-centric entities such as animals, foods, landscapes, and etc? \n\nQ8. Why is the LoRA applied to the U-Net blocks? What if the LoRA is applied to the (key, value) projection layers and MLP layers in self- and cross-attention blocks of SD?\n\nQ9. The authors claim that the degradation of performance in Stage 2 results from overfitting. That is, does the training loss decrease but validation loss increase?  In addition, can the model after Stable 1 successfully generate images for the prompts in Figure 6?\n\nQ10. This paper describes that 3K prompts are used in Section 4.2 and 1K prompts are used in Section 4.3.2. However, why are the reported performances (mean, std)  of MoLE in Table 1 and Table 2 (+Stage 3) exactly the same? Since Table 2 uses random sampling of 1K prompts among the 3K prompts, the HPS and IR scores cannot be exactly the same with the reported values in Table 1, considering the standard deviations (0.07, 1.49). \n\nQ11. The authors first use local assignments and then use global assignments in Figure 4. Does the ordering of the two affect the experimental results?\n\nQ12. Instead of using Stage 2 and Stage 3, which use 30K+60K and 50K training steps, respectively, how about the results of Stage 1 when we train the model longer than 300K steps?\n\nQ13. There are minor questions about the experiments.  \n- The authors should also report CLIP similarity, FID, and aesthetic score to evaluate the overall quality of generated images in addition to human preferences.  \n- In Table 1, how the standard deviations are measured?  \n- Can the proposed approach also improve the performance of SD-XL?  \n- I wonder why the authors do not present generated images on user-provided text prompts that rarely appear in the training and benchmark prompts.   \n- In Figure 7(d), why do the face and hand experts also highlight non-human-centric regions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1600/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1600/Reviewer_B5SW",
                    "ICLR.cc/2024/Conference/Submission1600/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1600/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698948237906,
        "cdate": 1698948237906,
        "tmdate": 1700639923877,
        "mdate": 1700639923877,
        "license": "CC BY 4.0",
        "version": 2
    }
]