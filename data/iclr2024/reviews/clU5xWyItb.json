[
    {
        "id": "8MnoPYR9Lg",
        "forum": "clU5xWyItb",
        "replyto": "clU5xWyItb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8010/Reviewer_vdwn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8010/Reviewer_vdwn"
        ],
        "content": {
            "summary": {
                "value": "The authors propose an agent-based scientific multiple choice QA system, mostly powered by LLMs and search APIs. They also contribute LitQA - 50 multiple choice questions written by experts."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "I have read the author responses and have increased the score by 1 point.\n-------------------------------------------------------\n\nThanks to the authors for their hard work on this agent and dataset. \n\nStrengths:\n- Good performance on the proposed dataset\n- Clear writing\n- Extensive ablations"
            },
            "weaknesses": {
                "value": "- Is it likely you overfit the entire PaperQA system? There are only 50 questions. Was PaperQA developed after the data was collected? Did you make important system choices based on outcomes on the 50 questions? I think you need a development and test set split and I worry that the 50 questions in LitQA are actually the development set. Table 5 suggests that your system's advantage over GPT + search is actually much smaller than suggested by the deltas in Table 2.\n- Multiple-choice is not realistic. Scientists don't already know what they're trying to solve when they are doing research. I think the entire paper should be the \"No MC options\" row from Table 3. I realize that MC means evaluation is easier, but with only 50 questions, manual evaluation of the various systems in Table 2 is feasible. Please consider rerunning the entirety of table 2 with the \"No MC options\". \n- How *did* you evaluate the No MC options? Exact string overlap? Manually? I think it should be the latter.\n- As I'm sure you know, Table 4 is hard to believe. Could it be because the MC options are included? What happens to the hallucination rate when the MC options are removed? I have done a lot of manual evaluation of many LLMs over the past year and have never seen a 0 hallucination rate. This needs to be more thoroughly understood as a part of this paper."
            },
            "questions": {
                "value": "- Will you be open sourcing your agent system? \n- In table 3, what is the \"Samples\" column? Is this how many times you ran the entire end-to-end experiment? If so, can you include the standard deviations?\n- Are Perplexity, Scite, and Elicit made to answer multiple-choice questions? I don't believe so. How do they compare in the \"No MC options\" setting?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8010/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8010/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8010/Reviewer_vdwn"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8010/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698090590009,
        "cdate": 1698090590009,
        "tmdate": 1700850098546,
        "mdate": 1700850098546,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "j9F1pNGBQr",
        "forum": "clU5xWyItb",
        "replyto": "clU5xWyItb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8010/Reviewer_CSpW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8010/Reviewer_CSpW"
        ],
        "content": {
            "summary": {
                "value": "This paper describes PaperQA, an agent that answers questions about scientific literature according to the search results. The agent is composed of three tools: search, gather evidence, and answer the question. It can find and parse relevant full-text research papers, identify specific sections in the paper that help answer the question, summarize those sections with the context of the question (called evidence), and then generate an answer based on the evidence. Compared to a standard retrieval-augmented generative (RAG) agent, PaperQA decomposes parts of a RAG and provides them as tools to an agent. It can adjust the input to paper searches, gather evidence with different phrases, and assess if an answer is complete."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. PaperQA decomposes parts of a RAG and provides them as tools to an agent, and it can adjust the input to paper searches, gather evidence with different phrases, and assess if an answer is complete. \n2. PaperQA makes use of a priori and a posteriori prompting, tapping into the latent knowledge in LLMs.\n3. PaperQA outperforms all models tested and commercial tools, and is comparable to human experts on LitQA on performance and time."
            },
            "weaknesses": {
                "value": "1. The paper has some innovation, but it still feels limited. Firstly, the dynamic use of the three tools is quite similar to the ReAct framework, all of which are dynamically autonomous in determining whether to retrieve them again. Secondly, the number of benchmarks constructed is relatively small, with only 50 questions and a multiple-choice format. Existing research has shown that the form of multiple choice questions has limitations in evaluating model performance, and the model is more often used to generate longer texts. Therefore, there is a significant gap between the form of multiple choice questions and practical applications.\n2. In the experiment, there is a lack of comparison with some advanced agent frameworks, which often consider the dynamic nature of intermediate steps. Therefore, it is necessary to increase the comparison with these frameworks, such as ReAct and Reflexion. The main experiment is conducted on multiple choice questions, which has limitations because hallucinations typically occur when the generated text is long. During the hallucination evaluation experiment, some details were not clearly written, such as whether other LLMs used search tools."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8010/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698504484648,
        "cdate": 1698504484648,
        "tmdate": 1699636987484,
        "mdate": 1699636987484,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cQs6IGInBd",
        "forum": "clU5xWyItb",
        "replyto": "clU5xWyItb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8010/Reviewer_afKA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8010/Reviewer_afKA"
        ],
        "content": {
            "summary": {
                "value": "This paper presents PaperQA, a tool developed with retrieval-augmented generation (RAG) technique to answer science questions. They also proposed LitQA, new benchmark to assess the performance of RAG models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* Authors introduce new components to the standard RAG pipeline (e.g., search, map-reduce the summary, repeat for more evidence)\n* Adaptive and modular framework and an implementation with open source libraries."
            },
            "weaknesses": {
                "value": "* This paper appeared to be more product or application specific than focused on the underlying research problems. Unfortunately, no research problem was mentioned in the text.\n* Ask LLM prompt assess the parametric knowledge, which is feed to the evidence contexts. Authors found this knowledge is helpful. But I do not agree with the reasons provided. For example, what would happen if there are knowledge conflicts raised with the parametric knowledge and retrieved knowledge?\n> \u201cSurprisingly enough, not using the LLM\u2019s latent knowledge (no ask LLM) also hurts performance, despite the benchmark being based on information after the cutoff date \u2013 we suggest that the useful latent knowledge we find LLMs to possess in Table 5 helps the agent use the best pieces of evidence.\u201d \n* I don\u2019t think the following claim is true. Having a low rate of incorrect answers does not suggest that the model is certain, in fact, one needs to measure the uncertainty in the generated answers to make such a claim.\n\u201cFurthermore, we see the lowest rate of incorrectly answered questions out of all tools, which rivals that of humans. This highlight\u2019s PaperQA\u2019s ability to be certain about its answers.\u201d\n* This is a bad analogy, I don\u2019t think that human judgmental time should need to correlate with the time taken to complete OpenAI API calls.\n> \u201cIt took PaperQA on average about 2.4 hours to answer all questions, which is also on par with humans who were given 2.5 hours.\u201d"
            },
            "questions": {
                "value": "* What kind of reasoning required in this case? I can only find the task is to measure the relevance of the query to the retrieved passages. It is intriguing why authors opt out the model to explain the score.\n> \u201cthe LLM\u2019s ability to reason over text and provide numerical relevance scores for each chunk of text to the query question.\u201d\n> \u201cAt the end of your response, provide a score from 1-10 on a newline indicating relevance to question. Do not explain your score\u201d\n* How authors reliably make the claim of the GPT4 cut off date? Which GPT4 version used in the study?\n> \u201cWe take special care to only collect questions from papers published after the GPT-3.5/4 cutoff date in September 2021\u201d\n* Is it expected that biomedical researchers cannot answer these question without internet? Or is this setup introduced to mimic the RAG styled QA by asking only to answer given what they find on the internet?\n> \u201cWe recruited five biomedical researchers with an undergraduate degree or higher to solve LitQA. They were given access to the internet and given three minutes per question (2.5 hours in total) to answer all questions\u201d\n* This is unexpected, any reasons? Is the context length a factor to correlate with the summarization performance?\n> \u201cInterestingly, we observe that using GPT-4 as the summary LLM worsens overall performance.\u201d"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8010/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698797491655,
        "cdate": 1698797491655,
        "tmdate": 1699636987361,
        "mdate": 1699636987361,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lY7aRHT4TI",
        "forum": "clU5xWyItb",
        "replyto": "clU5xWyItb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8010/Reviewer_1BNP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8010/Reviewer_1BNP"
        ],
        "content": {
            "summary": {
                "value": "The paper presents PaperQA, a Retrieval-Augmented Generation (RAG) agent developed to enhance question-answering in the scientific domain by mitigating issues of hallucinations and uninterpretability associated with Large Language Models (LLMs). Unlike other LLMs, PaperQA searches and retrieves information from full-text scientific papers to generate more accurate and interpretable responses. The authors showcase PaperQA's performance over existing LLMs on science QA benchmarks and introduce a new benchmark called LitQA, designed to simulate the complex task of human literature research. PaperQA is said to perform on par with expert human researchers when evaluated against the LitQA benchmark."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The concept introduced in the paper is promising, as it aims to develop a framework for retrieving literature to facilitate the answering of questions within scientific texts. The authors propose a novel approach that breaks down the QA task into three primary components: identifying relevant papers from online databases, extracting text from these papers, and synthesizing the information into a coherent final answer.\n\nThe paper introduces a new dataset, LitQA, which necessitates the retrieval and synthesis of information from full-text scientific papers. This is a notable effort to replicate the complexity of real-world scientific inquiry.\n\nThe study compares the proposed PaperQA system against multiple baselines. The results indicate that PaperQA outperforms these baselines and is on par with human experts."
            },
            "weaknesses": {
                "value": "Lack of Novelty:\n\nThe methodology presented in this paper follows the established pipeline of retrieval, reading, and answering, which has been extensively explored in prior literature. The paper does not adequately differentiate the proposed model from existing work in the field. For this approach to be considered a substantial contribution, it would require either a novel application of these methods or significant improvements over existing models, neither of which are sufficiently demonstrated in the current paper.\n\nInsufficient Dataset Size:\n\nThe introduction of the LitQA dataset is an interesting addition; however, with only 50 examples, it is far too limited to serve as a robust benchmark for this area of research. Benchmarks require extensive and diverse examples to evaluate the generalizability and effectiveness of the proposed approach and to provide a reliable comparison with other baselines. The dataset, as it stands, does not meet these criteria.\n\nTechnical Feasibility and Lack of Detail:\n\nThere are concerns regarding the technical feasibility of some experimental settings described. Specifically, the instruction for the summary LLM to score relevance from 1 to 10 is not grounded in a clearly defined metric, raising questions about the model's capacity to interpret and apply these scores accurately.\n\nMoreover, the paper omits crucial details necessary for the reproducibility of the results and the clarity of the methods used. For instance, the base LLM utilized for PaperQA is not specified, leaving a gap in understanding the foundation upon which the system is built. Similarly, the engines powering GPT-3.5 and GPT-4 are not clearly defined. The configurations and model setups for the tools Elicit, Scite_, Perplexity, and Perplexity (Co-pilot) are insufficiently detailed. This lack of clarity hinders the assessment of the methods and the comparison of the results."
            },
            "questions": {
                "value": "Missing related work:\n\n- Are You Smarter Than a Sixth Grader? Textbook Question Answering for Multimodal Machine Comprehension\n- Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering\n\nTypos:\n- \u201cThis implementation decision is explained\u201d -> \u201cThe implementation details are explained\u201d"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8010/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698888505265,
        "cdate": 1698888505265,
        "tmdate": 1699636987260,
        "mdate": 1699636987260,
        "license": "CC BY 4.0",
        "version": 2
    }
]