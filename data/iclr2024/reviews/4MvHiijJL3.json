[
    {
        "id": "i9hK94HjvF",
        "forum": "4MvHiijJL3",
        "replyto": "4MvHiijJL3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5610/Reviewer_nvHG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5610/Reviewer_nvHG"
        ],
        "content": {
            "summary": {
                "value": "In this work, the authors propose to leverage model interpretability methods to explore fairness problems. Specifically, based on local feature importance methods, they introduced a novel notion, FID (feature importance disparity), as a bias indicator under rich group fairness scenarios. Furthermore, they designed an oracle-efficient algorithm to search the large FID subgroups and claimed the potential utility of the proposed concept in fairness research."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper is informative and well-written.\n    \n- Combining interpretability with the fairness problem, the idea is interesting."
            },
            "weaknesses": {
                "value": "- Not discussing specific definitions of fairness in the context of rich group fairness can sidestep a certain amount of risk, but does it affect the accuracy of the article and the related discussions?\n    \n- Section 3 and the proposed algorithm are a little confusing; please refer to the questions.\n    \n- The main body of the article exceeded the required number of pages."
            },
            "questions": {
                "value": "- I'm curious if the assumption about being locally separable is too strong.\n    \n- In algorithm 1, what is the definition of L?\n    \n- Are there any specific application scenarios or examples for the proposed FID metrics?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5610/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5610/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5610/Reviewer_nvHG"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5610/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698814453295,
        "cdate": 1698814453295,
        "tmdate": 1699636578721,
        "mdate": 1699636578721,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5IxzYn8pGZ",
        "forum": "4MvHiijJL3",
        "replyto": "4MvHiijJL3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5610/Reviewer_tQK6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5610/Reviewer_tQK6"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces the notion of feature importance disparity (FID) to capture fairness of a model in terms of whether a feature importance value differs significantly for a subgroup compared to the entire population. Then to assess the fairness of a given model, the authors present an algorithm to find a feature and a subgroup, characterized by a function over the protected features, that exhibits a large FID. The algorithm involves iteratively solving constrained optimization problems using a polynomial number of calls to an oracle for cost-sensitive classification. The proposed method is evaluated empirically on four datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The characterization of fairness in terms of disparities in feature importance scores is novel and interesting.\n\nThe proposed approach can be used with many feature importance notions and (somewhat) model-agnostic (for some feature importance scores), and thus can potentially be widely applicable.\n\nThe paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "The proposed method returns real-valued subgroup functions (instead of binary) which are not really interpretable. Binary subgroup functions are indicators for some population groups that would be affected disparately. On the other hand, while real-valued subgroup functions can be interpreted as fractional membership in some demographic groups, their use in characterizing bias against/towards certain subgroups is questionable.\n\nThe number of calls to the CSC oracle required by Algorithm 1 to maximize AVG-SEPFID is quadratic in the size of data, for a single feature. This is quite computationally expensive, which would limit the applicability of the proposed method to large datasets with a large number of features.\n\nEmpirical evaluation is lacking comparison with any baseline. For example, how do the subgroups identified by the proposed method compare to existing `rich subgroup\u2019 discovery methods?"
            },
            "questions": {
                "value": "1. How do the subgroups identified by the proposed method compare to existing `rich subgroup\u2019 discovery methods on the four datasets?\n\n2. What were the runtimes for optimizing AVG-SEPFID for the experiments?\n\n3. Given a subgroup (with fractional membership) and a feature with high FID, how do the authors suggest the model or domain expert address this? Moreover, how do you decide what value of AVG-SEPFID is significantly high to signal bias?\n\n4. The approximation bound given by Theorem 1 seems quite weak. Even though the difference between the optimum and the expected FID over the distribution p_g is bounded, the expected difference for each g (E[FID(j,g*)-FID(j,g)]) may still be large. Is it possible to bound such expected difference or give a probabilistic guarantee?\n\nMinor comments / questions: \n- Definition 3: typo: $\\frac{1}{n|g|}$ -> $\\frac{1}{|g|}$\n- What does bound B in Algorithm 1 represent intuitively?\n- According to the experimental results regarding coefficients of subgroup functions, it appears that the subgroup functions g are linear functions. Is this a restriction imposed by the proposed algorithm?\n- While the FID is useful in the algorithm to optimize for AVG-SEPFID, I don\u2019t think it makes much sense as a fairness notion itself. According to Definitions 1 and 2, separable FID just ends up being $|\\sum_X (g(x)-1) F(f_j,X,h)|$, which roughly corresponds to the sum of feature importance of non-group data points."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5610/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699171447016,
        "cdate": 1699171447016,
        "tmdate": 1699636578641,
        "mdate": 1699636578641,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JCclIfiT8w",
        "forum": "4MvHiijJL3",
        "replyto": "4MvHiijJL3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5610/Reviewer_SE4V"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5610/Reviewer_SE4V"
        ],
        "content": {
            "summary": {
                "value": "The paper explores feature importance methods and their potential for disparities in attributing feature importance values across different subgroups. The authors introduce the concept of Feature Importance Disparity (FID), which measures whether local feature importance methods attribute different feature importance values on average in protected subgroups compared to the entire population as a proxy to indicate bias in the model or data generation process. The paper designs an efficient algorithm to identify subgroups with large FID and provides empirical results across 4 public datasets and 4 common feature importance methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper does a good job at motivating the connection between explainability and fairness and why more exploration in this direction is warranted\n- The experiments to showcase the results in terms of explainability for FID are extensive\n- The convergence proof for Algorithm 1 appears correct to me"
            },
            "weaknesses": {
                "value": "For completeness, I do not have any experience in explainability, but I do in algorithmic fairness as well as the optimization literature.\nThe main weaknesses of this paper to me appear to be (a) the presentation of FID and Algorithm 1 and (b) the implications and connections to downstream fairness.\n\n(a) The technical presentation of the optimization algorithm is too abrupt for the reader, and some lingering questions never get addresses fully. \n-What is the rich subgroup class we are considering, for practical examples? \n-What do the size violations correspond to?\n-The notion of CSC being relegated to the appendix really subtract clarity from the work, as the reader struggles to bounce back and forth from the appendix for almost every step of Algorithm 1.\nI appreciate the intricacies of presenting an algorithm such as this one, but one could maybe shorten the \"Introduction\" section to provide either a (i) simple example or (ii) dedicate some space to go through each of the single ingredients of the algorithm. Personally, I think this would go a long way towards improving the understanding of AVG-SEPFID and appreciate why solving it is tricky.\n\n(b) The main drawback of this paper is the lack of connections with fairness metrics (such demographic parity or equalized odds) to connect the results in e.g., Table 1, with the disparities of the classifiers with respect to different subgroups. While pointing out biases in the original data is useful, FID seems to be applicable to a given group or family of classifiers $\\mathcal{H}$, while in practice what modelers are interested in is to be able to analyse and find the pathways of discrimination in a given model. I believe that an extra set of experiments where the connection between natural fairness metrics and the proposed FID would make this paper meaningful and interesting to the community. As of now, from an algorithmic fairness standpoint, the paper ends too abruptly."
            },
            "questions": {
                "value": "Please see \"Weaknesses\" section above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5610/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699493991471,
        "cdate": 1699493991471,
        "tmdate": 1699636578527,
        "mdate": 1699636578527,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7HHgx61OSF",
        "forum": "4MvHiijJL3",
        "replyto": "4MvHiijJL3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5610/Reviewer_ZZsY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5610/Reviewer_ZZsY"
        ],
        "content": {
            "summary": {
                "value": "This paper formally introduces the notion of feature importance disparity in the context of ``rich'' subgroups, which could be used as a potential indicator of bias in the model/data generation process. The proposed algorithm finds (feature, subgroup) pairs that: (i) have subgroup feature importance that is often an order of magnitude different than the importance on the whole dataset, generalize out of sample."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper tackles an important question in machine learning literature on understanding how a specific feature contributes to a model's prediction, focusing on feature importance. The objective is clearly stated, definitions are well defined, and the optimization problem is intuitive and well-defined. Overall it uses existing methods in learning theory to solve an interesting fairness problem."
            },
            "weaknesses": {
                "value": "I find algorithm 1 hard to interpret; in particular, it involves in exponential gradient update, but it's not clear to me what certain parameters mean. In addition, it is not clear how this algorithm is related to Kivinen & Warmuth in terms of who is the max player, who is the min player, and what their objectives are."
            },
            "questions": {
                "value": "1. In definition 1, what does the expectation over? namely, what is $X$?\n2. How should I intuitively understand the notion of ``Locally Separable''? In reality, how easy or hard is it to satisfy this condition?\n\nOthers:\n1. The citation style is weird; the author might consider changing it."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5610/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699597333136,
        "cdate": 1699597333136,
        "tmdate": 1699636578430,
        "mdate": 1699636578430,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NDgI2kMdCN",
        "forum": "4MvHiijJL3",
        "replyto": "4MvHiijJL3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5610/Reviewer_5Fo2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5610/Reviewer_5Fo2"
        ],
        "content": {
            "summary": {
                "value": "The work introduces the problem of quantifying disparities in the output of a feature importance method across groups in the data. It finds subgroups for which the average feature importance for a given feature differs significantly compared to the feature importance in the whole population. The main contribution is that the subgroups need not be enumerated beforehand. The proposed method is able to search for groups represented as functions on an arbitrary set of features, called \u2018rich subgroups\u2019 in the fairness literature. To do so, the work formalizes a constrained optimization problem and solves it via online learning algorithms through a reduction that leverages oracle access to cost-sensitive classifiers. Theoretical result shows the solution is close to the maximum disparity group. Experiments on multiple datasets show the applicability of the method for different feature importance methods.\n\n---\nUpdating the score after the rebuttal which addresses concerns on significance of the problem and presentation of the algorithm."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The ideas are presented clearly with adequate explanation and clear notation. \n2. I like the generality of the problem formulation and how it is presented in Section 2 and 3 which can then be readily made specific to different feature importances.\n3. Experiments are thorough - multiple explanation methods, datasets, tests on hold-out set - and are presented concisely."
            },
            "weaknesses": {
                "value": "1. (Major) Utility of the algorithm outputs could be discussed more thoroughly. As discussed in limitations, all differences in explanation outputs need not imply a discriminatory model. On the contrary, sometimes a model will differ in its logic in the protected group to account for differences in data. For example, the feature of not having a health insurance will be much more predictive of health outcomes in the unhoused population (say, a protected group) in comparison to whole population. So, the model logic and the feature importance will differ in the protected group. Reasons for finding such groups should then be discussed to motivate the utility of the method, along with suggestions for diagnosing the disparities. Introduction and empirical results can be edited to convey the significance of the explanation disparity problem.\n2. (Minor) Algorithm for finding maximal group can be described in more detail. Reduction to CSC and the specifics of Algorithm 1 like what is theta, lambda, and so on can be briefly explained, even if this is a standard method for solving constrained optimization."
            },
            "questions": {
                "value": "1. Please discuss relation to the work Balagopalan et al. 2022 (The Road to Explainability is Paved with Bias: Measuring the Fairness of Explanations https://dl.acm.org/doi/abs/10.1145/3531146.3533179) which also investigates disparities in explanation outputs. Advantages of the proposed method such as rich subgroups should be highlighted.\n2. Please include more discussion on what can be done after observing disparities in explanation output. How can the user interpret the relation between explanation disparities and the underlying social biases, and decide how to update the model?\n\n---\n## Minor (no response is requested)\n\nIn Definition 1 of FID, consider denoting if the expectation is over different samples of the whole dataset X^n.\n\nThe scope of prediction models, sensitive features, and feature importances supported by the method was not clear to me. Describe the assumptions needed on the prediction model (for CSC to give good solution) and sensitive features (mix of categorical, real variables) if any. Consider listing feature importance methods that satisfy separability and any other required conditions for the method, perhaps in a table format.\n\nConsider discussing extensions to non-separable importances other than weights in linear regression such as feature-permutation based importances in decision trees.\n\nPlease increase font size in Figures 2-6."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5610/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5610/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5610/Reviewer_5Fo2"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5610/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699657593718,
        "cdate": 1699657593718,
        "tmdate": 1701063706805,
        "mdate": 1701063706805,
        "license": "CC BY 4.0",
        "version": 2
    }
]