[
    {
        "id": "yFJzZ87oY0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4525/Reviewer_b3Zi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4525/Reviewer_b3Zi"
        ],
        "forum": "6vtGG0WMne",
        "replyto": "6vtGG0WMne",
        "content": {
            "summary": {
                "value": "This paper introduces a precise definition to quantify the extent of model imbalance and presents a bias adjustment technique that effectively fine-tunes the model for specific imbalance scenarios. The proposed approach involves training the backbone model using conventional training methods and then updating the model's bias term to achieve class balance. The authors demonstrate performance enhancements compared to baseline models on datasets like CIFAR-10, SST-2, and AG. Additionally, the paper discusses additional metrics, such as F1 score and G-means, which previous studies did not address."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The strengths of this paper include:\n\n1. It provides a solid mathematical rationale for addressing class imbalance, which lends credibility to the proposed method. The approach is well-grounded in mathematical derivations, reinforcing its validity.\n\n2. The proposed algorithm stands out for its simplicity. It leverages traditional training methods and only requires minimal adjustments, specifically focusing on updating the bias term. This simplicity makes it practical and straightforward to implement.\n\n3. The proposed method consistently demonstrates performance improvements across a range of datasets, further highlighting its effectiveness and versatility."
            },
            "weaknesses": {
                "value": "Despite the improved performance demonstrated by the proposed method in comparison to other baselines, assessing its overall impact presents certain challenges.\n\n1. The dataset employed in the study does not align with typical choices for class imbalance tasks. For instance, the authors utilize a subsampled version of CIFAR-10, which is not commonly used for this purpose. Larger datasets like the full CIFAR-10 or CIFAR-100 are typically preferred to evaluate the method's compatibility with scenarios involving more than two classes.\n\n\n2. There is a notable absence of comparison with recent works such as RIDE [1], CMO [2], CUDA [3], Balanced-softmax [4], BCL [5], and NCL[6], which are relevant in the context of class imbalance. These omissions are not addressed in the related work section, leaving a gap in the comparison with contemporary methodologies.\n\n[1] Long-tailed Recognition by Routing Diverse Distribution-Aware Experts, ICLR 2021  \n[2] The Majority Can Help the Minority: Context-rich Minority Oversampling for Long-tailed Classification, CVPR 2022  \n[3] CUDA: Curriculum of Data Augmentation for Long-Tailed Recognition, ICLR 2023  \n[4] Balanced Meta-Softmax for Long-Tailed Visual Recognition, NeurIPS 2020  \n[5] Balanced Contrastive Learning for Long-Tailed Visual Recognition, CVPR 2022  \n[6] Nested Collaborative Learning for Long-Tailed Visual Recognition, CVPR 2022"
            },
            "questions": {
                "value": "1. It would be beneficial if the authors could provide a comparison of their method's effectiveness against recent works published within the last two years. This would help establish the relevance and competitiveness of their approach in the current research landscape.\n\n2. Additionally, extending the evaluation to datasets like CIFAR-10/100 without subsampling of classes and including large real-world datasets such as ImageNet or iNaturalist2018 would offer a more comprehensive assessment of the method's applicability and generalization capabilities. This broader range of datasets would provide a better understanding of the method's performance in diverse and real-world scenarios."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "I do not have ethics concerns about this paper."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4525/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4525/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4525/Reviewer_b3Zi"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4525/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697135294872,
        "cdate": 1697135294872,
        "tmdate": 1699636429800,
        "mdate": 1699636429800,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5pGm5k7Lku",
        "forum": "6vtGG0WMne",
        "replyto": "6vtGG0WMne",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4525/Reviewer_TH73"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4525/Reviewer_TH73"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the challenge of imbalanced problems in deep learning. The authors introduce two primary contributions:\n1. **Model Imbalance State (MIS)**: A definition to quantify the imbalance of a model. This metric essentially measures the model's prediction probability for a given label.\n2. **Bias Adjustment (BA)**: An optimization method that adjusts the bias of the model's last layer to make the imbalanced model reach an optimal state. In addition, BA employs a search strategy for the best bias values and uses gradient descent to find the optimal bias.\n\nExperiments show that their proposed method achieves significant improvements and demonstrates superior effectiveness."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper tackles the prevalent issue of imbalanced problems in deep learning, which is important especially when dealing with real-world data.\n2. Experiments show that the bias adjustment method seems computationally efficient.\n3. Experiments looks thorough and demonstrate the improvement and effectivenss of proposed method."
            },
            "weaknesses": {
                "value": "The issue addressed in this paper is undeniably significant, and the authors present an intuitive approach backed by promising results. However, the central section (i.e., sec.3) requires further refinement, as its current state suggests an incomplete work. My concerns are as follows:\n\n1. The statement \"$P_i$ records the model prediction probability of label $C_i$\" can benefit from a detailed explanation, preferably supplemented by an example for clarity.\n2. The assertion \"Eq.(4) shows Pi can be estimated...\" is misleading, given the absence of $P_i$ in Eq.(4).\n3. Regarding Eq.(4), the terms $p(C_i | \\Phi)$ and $p(x)$ need explicit definitions within this section. Given that the MIS is formally defined, readers should not be expected to reference prior sections for notation clarification. \n4. The portion \"In practice, the class probability distribution r is generally unknown in applications. BA employs a search strategy to pinpoint the optimal $r^\u2217$ rooted in imbalance metrics\" raises questions. Specifically, are there any theoretical guarantees to ensure a bounded error between the searched $r^*$? Additionally, are there assurances that the BA method remains effective even if the discovered $r^*$ lacks accuracy? Expanding on this could fortify the paper's foundation, preventing it from being solely experimental.\n5. Section 3.3 is somehow sparse. I recommend elaborating on the process and possibly incorporating a figure for enhanced comprehension.\n6. If the problem pertains to multi-class classification rather than just binary, would the proposed method still be applicable? This aspect warrants discussion in your paper, particularly as your focus is on addressing imbalanced datasets found in real-world scenarios, which are often not limited to binary classification.\n7. Is there potential for this method to be extended to address imbalanced regression scenarios? For instance, rather than having the model predict strict values of 0 or 1, could it be adapted to regress responses ranging from 0 to 1? It might be beneficial to address this point in the paper, even if it falls outside the scope of Section 3.\n\nIn conclusion, I urge the authors to consider these suggestions earnestly. Improvements to section 3 could greatly elevate the overall quality of this paper."
            },
            "questions": {
                "value": "See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4525/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4525/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4525/Reviewer_TH73"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4525/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698471094273,
        "cdate": 1698471094273,
        "tmdate": 1699636429703,
        "mdate": 1699636429703,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dRHHEmxr9b",
        "forum": "6vtGG0WMne",
        "replyto": "6vtGG0WMne",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4525/Reviewer_HH3t"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4525/Reviewer_HH3t"
        ],
        "content": {
            "summary": {
                "value": "This paper explores the backbone-and-classifier bifurcation of a neural network to propose a change in the way that biases are assimilated into the decision. In doing so, an observation on the class predictions by a learner on an imbalanced classification is made, that the majority class dominates the predictions, which is reported as a measure. This is absorbed into a bias correction mechanism that optimizes  relative entropy between the distribution of predictive bias and the ideal bias with no imbalance. The later quantity being unknown, an approximative step is introduced following every training  epoch."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The proposed method is straightforward. Evaluations are however, fairly extensive, and are done across a variety of data types - tabular, image and text. A per-measure comparison is also provided. This kind of papers are persuasive with broad-based evaluations, usually on even more datasets."
            },
            "weaknesses": {
                "value": "The hypothesis of just the bias on the softmax layer meant for classification making the classification swing by these amounts us astounding. In fact, a study on the values of the bias versus predictive performance is required in my opinion before bias adjustment is presented."
            },
            "questions": {
                "value": "At 500:1 imbalance ratios, multiple benchmark methods show numbers decimated to zero. A previous study, MMM, by Mirza et al. '21 appears to suggest that even at such a ratio, classical resampling and even baselines report an above zero performance. Could you reason about the disparity?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4525/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698684396598,
        "cdate": 1698684396598,
        "tmdate": 1699636429574,
        "mdate": 1699636429574,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IoLGU6W0WT",
        "forum": "6vtGG0WMne",
        "replyto": "6vtGG0WMne",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4525/Reviewer_tgc4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4525/Reviewer_tgc4"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a solution to an important issue of class imbalance observed frequently during classification. Imbalanced classification problems are generally handled using decoupled models where we have a backbone module that learns the input feature representation and a classifier module that takes the learned feature representation and performs the classification task. In this work, authors have developed user-specified metrics as an optimization strategy to resolve class imbalance in classification tasks. Towards this, they formally define the Model Imbalance State (MIS) based on how biased the classifier is towards the particular class and used Bias Adjustment (BA) method to minimize the bias of the classifier towards the minor class based on MIS value, which in general is biased towards the majority class. Using BA, authors have employed the KL divergence-based objective to minimize the difference between the class probability distribution observed during MIS and the original class probability distribution using a naive search strategy and optimization using gradient descent. Unlike traditional training strategies to handle imbalanced data classification where the backbone module is taken care of, this work keeps the backbone module constant and works on bias adjustment to minimize the impact of class imbalance over classification performance. The proposed approach is evaluated over binary text and image classification datasets compared with six approaches, including class weighting, two-stage methods, and a SOTA approach. The results are reported using the accuracy, minority class F1, and G-means evaluation metrics. The results reported under different imbalance ratios suggest that the proposed approach is capable of handling the imbalance scenarios better than all the compared approaches."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The idea of BA to minimize the difference between MIS and original class distribution to achieve better classification results over imbalanced datasets sounds very intuitive.  \n2. The proposed approach is able to handle the extreme imbalance scenarios better.\n3. The proposed approach is more efficient than two-stage methods in terms of the time taken to tune the epochs and is ten times faster than SOTA.\n4. The proposed approach is much more efficient than class-level weighting methods, where the time taken for tuning the weight values takes as much as 2-3 orders of magnitude than the proposed approach."
            },
            "weaknesses": {
                "value": "1. The evaluation metrics chosen to report the performance reflect the overall performance rather than being class-specific, except for F1 over the minority class. Towards this, authors should have considered precision, recall, and F1 value over minority class and macro performance utilizing the same metrics to check if the performance is not biased towards any particular metrics or a particular class, respectively.\n2. The results on CIFAR-10 as can be observed from Figure 3(b) reflects a high variance in F1 value. What could be the possible reason behind this?"
            },
            "questions": {
                "value": "1. Why have authors considered only BERT and ResNet-32 for text and image datasets, respectively, out of so many models widely available? \n2. Can we expect the same behavior over other backbone models? Why?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4525/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4525/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4525/Reviewer_tgc4"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4525/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699423672849,
        "cdate": 1699423672849,
        "tmdate": 1699636429479,
        "mdate": 1699636429479,
        "license": "CC BY 4.0",
        "version": 2
    }
]