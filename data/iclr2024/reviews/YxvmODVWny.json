[
    {
        "id": "sGsloVnrBd",
        "forum": "YxvmODVWny",
        "replyto": "YxvmODVWny",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6698/Reviewer_rbwp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6698/Reviewer_rbwp"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces an innovative approach to directing end-to-end robot manipulation tasks using sketches. The proposed model, referred to as RT-Sketch, interprets sketches of varying specificity, processes current and previous visual states, and predicts the corresponding robot actions. To obtain training data, a unique image-to-sketch model is utilized to convert terminal images from the Robot Transformer-1 (RT-1) into sketches, leading to the creation of the RT-Sketch dataset."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The proposed task is novel and interesting, potentially enhancing the efficiency of human-robot interactions through sketches. RT-Sketch can interpret and act upon sketches with varied levels of specificity, which suggests a degree of flexibility and adaptability in the model. The thorough experimental work effectively demonstrates the system's proficiency in executing the tasks assigned and its performance with the specific robot used."
            },
            "weaknesses": {
                "value": "## \n\n1. The model, like its predecessor, the Robot Transformer (RT), remains an end-to-end agent. Although it showcases impressive performance, task comprehension is tied to a specific robot, hindering the system's ability to undertake out-of-domain tasks or generalize across various robots without necessitating retraining.\n2. In terms of communication, a sketch represents more than just an enhanced image; it abstracts visual information and fosters the emergence of graphical conventions to boost efficiency (Qiu et al., 2022; Chen et al., 2023). However, the sketches in this work, even at the lowest specificity, seem to be merely processed images rather than abstractions, making this work more like an augmented version of RT-1 to sketch images.\n\nThe above limitations, in my opinion, restrict the work's contribution to addressing fundamental issues in human-robot collaboration and robot manipulation.\n\nReferences:\n\n- Qiu, S., Xie, S., Fan, L., Gao, T., Joo, J., Zhu, S. C., & Zhu, Y. (2022). Emergent graphical conventions in a visual communication game.\u00a0*Advances in Neural Information Processing Systems*,\u00a0*35*, 13119-13131.\n- Lei, Z., Zhang, Y., Xiong, Y., & Chen, S. (2023). Emergent Communication in Interactive Sketch Question Answering.\u00a0*arXiv [Cs.AI]*. Retrieved from http://arxiv.org/abs/2310.15597"
            },
            "questions": {
                "value": "1. How does the model handle variations in sketch quality or style? Are there specific requirements for the sketches used to instruct the model?\n2. I wonder if you can consider potential benefits of combining sketch and text descriptions to enhance task specification and promote more effective collaboration?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6698/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698340286323,
        "cdate": 1698340286323,
        "tmdate": 1699636768949,
        "mdate": 1699636768949,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8uoz1keqkw",
        "forum": "YxvmODVWny",
        "replyto": "YxvmODVWny",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6698/Reviewer_GDgQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6698/Reviewer_GDgQ"
        ],
        "content": {
            "summary": {
                "value": "This work addresses a challenge in how humans specify the goal for robotics tasks. In related works, typically a goal-image or a natural language goal is given. The authors highlight the disadvantages that using these goals can cause. The authors propose another route, using hand-drawn sketches as a goal for the robotics task. RT-Sketch is proposed as an extension to RT1 as a goal-conditioned model to solve manipulation tasks when given a sketch of the final goal state. Experiments are first performed via a survey by using the Likert scores for perceived semantic and spatial alignment. After this table-top manipulation experiments are performed across 6 skills and ablations on visual distractors and language ambiguity."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The direction the authors are going with this work, thinking about different ways of specifying a goal, will be useful for improving accessibility and has intellectual merits. \n- The authors take steps to perform surveys to study human preference in specifying goals rather than just quantitative analysis through robot performance.\n- Creation of the RT-Sketch dataset and the Image-To-Sketch model have technical merits.\n- The limitations and failure modes are honest from a methodology perspective and are appreciated.\n- Overall this paper is well written, Figure 2 does a good job of demonstrating the architecture of RT-Sketch and Figure 3 does a great job of conveying the survey results."
            },
            "weaknesses": {
                "value": "The main concerns with this manuscript come from two sources. The first is with respect to the motivation of using sketches to specify a goal and the second is concerning the quantitative robotics results.\n## Motivation\nOverall the reviewer is not convinced that the examples and arguments given motivate the superiority of using a sketch as a goal over using natural language.\n- Regarding the granularity argument that the authors use in the introduction including the examples of \"put utensils, ..., on the table\" and \"put the fork 2cm to the right...\". Doesn't this demonstrate the flexibility that language has as a goal? Even if a human had to communicate the placement of utensils on a table, this would still seem easier than drawing a corresponding representation for an entire table.\n- While language can be ambiguous, so can sketches. As an example, if a sketch was given with an empty table, am I telling the agent that I want it to throw out the garbage on the table or am I telling it to ignore the garbage? It would seem like the desirable solution would be to create more intelligent agents to create reasonable solutions or ask for clarifications when given an ambiguous problem.\n\n## Robotics Results\n- The metrics, \"Spatial Precision\" and \"Failure Occurrence\" are not carefully defined or motivated. Failure occurrence is not defined at all. The spatial precision metric is at best defined as \"the distance (in pixels) between object centroids in achieved and ground truth goal states, using manual keypoint annotation\". However, how these centroids are obtained and why manual keypoint annotation is necessary over using an off-the-shelf image classifier should have been mentioned. \n- It is not obvious how big of a difference the errors are when looking at the RMSE in pixels. Could the errors when finding the centroid and the manual keypoint annotations be an issue in measuring this? Can a visualization be created to show this?\n- Typically bolded numbers in a column/row represent the method with best performance. However, in the column for failure occurrences, RT-Goal-Image is bolded despite having the highest failure occurrence.\n- The meaning of the shading of the cells of Table 1 adds a lot of confusion and should have been defined in the caption to improve readability. This confusion comes because, in one portion of the table, darker gray colors represent lower centroid distance and in another portion it represents the frequency of failures. This frequency of failure metric is also not well defined, how is this different than failure occurrence?"
            },
            "questions": {
                "value": "Beyond the concerns and questions given in the weakness section.\n\n- How many people took the survey? How was this survey conducted?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6698/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6698/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6698/Reviewer_GDgQ"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6698/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698448677979,
        "cdate": 1698448677979,
        "tmdate": 1699636768824,
        "mdate": 1699636768824,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mds1Gj1Syq",
        "forum": "YxvmODVWny",
        "replyto": "YxvmODVWny",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6698/Reviewer_fq96"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6698/Reviewer_fq96"
        ],
        "content": {
            "summary": {
                "value": "The authors present RT-Sketch, a goal-conditioned policy that takes a hand-drawn sketch of the desired scene as input and outputs actions. They train RT-Sketch on a dataset of paired trajectories and corresponding synthetically generated goal sketches. The experimental results show that RT-Sketch performs on a similar level to image or language-conditioned agents in straightforward settings, while achieving greater robustness when language goals are ambiguous or visual distractors are present."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This study introduces a novel method that employs sketches as the target for conditioned imitation learning. This approach is well-motivated as sketches can be more advantageous than language and natural images in situations involving language ambiguity and visual distractors.\n\n- The proposed method was validated through experimental settings that aimed to address four hypotheses. These hypotheses were concerned with whether sketches are expressive enough (H1), if the proposed method can handle various abstraction levels of sketches (H2), whether sketches are robust enough to tackle distractors compared to goal images (H3), and if sketches outperform ambiguous language (H4). The results of these experiments were convincing to me.\n\n- The paper has been skillfully crafted and is presented in a manner that is both clear and concise."
            },
            "weaknesses": {
                "value": "To obtain the sketch that can precisely convey the goal is essential to the success of the proposed method, I have the following concerns which may limit the practical usage of the proposed method:\n\n1. Given the potential (vast) cost associated with collecting human sketches for various scenarios, perhaps scalability could be a concern when applying this approach to broader scenarios, rather than solely relying on the benchmark presented in this study.\n\n2. The authors did not conduct experiments to determine how various image-to-sketch generation methods can affect the final results of RT-Sketch, which could potentially create a bottleneck in the entire pipeline."
            },
            "questions": {
                "value": "Please see the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6698/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698769776833,
        "cdate": 1698769776833,
        "tmdate": 1699636768687,
        "mdate": 1699636768687,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qEcv6zWIWN",
        "forum": "YxvmODVWny",
        "replyto": "YxvmODVWny",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6698/Reviewer_1JK3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6698/Reviewer_1JK3"
        ],
        "content": {
            "summary": {
                "value": "This study introduces RT-Sketch, a novel approach to visual imitation learning that utilizes hand-drawn sketches for goal specification. Unlike ambiguous natural language or overly detailed images, sketches strike a balance by being user-friendly and spatially aware. RT-Sketch achieves performance similar to image or language-conditioned agents in straightforward scenarios but excels in handling ambiguity and visual distractions. It demonstrates the capacity to interpret and act upon sketches of varying specificity, highlighting their versatility in goal representation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ Overall, this work opens a new direction for leveraging sketches for goal-conditioned imitation learning. \n+ The motivation is clear and reasonable. \n+ Use of Contour Drawing Dataset is logical and helps to mitigate the gap between synthetic sketches and real freehand sketches."
            },
            "weaknesses": {
                "value": "## Missing subsection in the related work:  \nIn recent years, there has been a significant body of work at the intersection of sketches for visual understanding. I would suggest the author add one separate subsection discussing a few major works on 'Sketch for Visual Understanding' in the related work parts. The authors could use this for their reference: https://github.com/MarkMoHR/Awesome-Sketch-Based-Applications/blob/master/README.md. Some representative works include: \n- a) https://arxiv.org/abs/2303.15149, CVPR'23\n- b) https://arxiv.org/pdf/2302.05543.pdf (ControlNet uses sketch for image generation), ICCV'23\n- c) https://arxiv.org/pdf/2303.11502.pdf, CVPR'23\n- d) https://arxiv.org/abs/2203.14843, CVPR'22. \n- e) https://arxiv.org/abs/2204.11964, CVPR'23\n\n## Freehand sketches vs Synthetic Sketches/Edgemaps\nFree-hand sketches and edge maps are different, and many existing works on sketches have claimed that models trained from edge maps do not generalize well to free-hand sketches. Some relevant works are https://openaccess.thecvf.com/content/CVPR2023/papers/Koley_Picture_That_Sketch_Photorealistic_Image_Generation_From_Abstract_Sketches_CVPR_2023_paper.pdf and https://github.com/mtli/PhotoSketch. Some discussions around that could be helpful. \n\n## Minor\n- Some self-contained caption could be helpful for Fig. 2."
            },
            "questions": {
                "value": "1. The experiment section could be made a little more self-contained so that it would be easier to digest for readers from a broader background. I wonder if the authors could pay some attention to that. \n2. Figure 3 could benefit from a more comprehensive caption to guide the reader through the observations, making it easier to understand.\n3. Is it possible to add more visual examples where a sketch is found to be better than a text-only counterpart?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6698/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6698/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6698/Reviewer_1JK3"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6698/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698778738551,
        "cdate": 1698778738551,
        "tmdate": 1699636768524,
        "mdate": 1699636768524,
        "license": "CC BY 4.0",
        "version": 2
    }
]