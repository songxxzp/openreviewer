[
    {
        "id": "GKa1ynmvzk",
        "forum": "PDL5A6facN",
        "replyto": "PDL5A6facN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission210/Reviewer_y1oq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission210/Reviewer_y1oq"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a novel RL algorithm based on Nesterov Accelerated Gradient (NAG) to improve the convergence rate. Specifically, the authors adapt the NAG into policy gradient to develop APG and mathematically show the global convergence rate of $\\tilde{\\mathcal{O}}(1/t^2)$, which is faster than the existing $\\mathcal{O}(1/t)$., with the true gradient and softmax policy parameterization. The authors also show that regardless of initialization, APG is able to reach a locally nearly-concave regime, within finite iterations. To validate the theory, the authors use two simple benchmarks to demonstrate that APG outperforms the standard PG."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The investigated topic of convergence rate improvement for RL is important and interesting. This work seems theoretically strong in analysis by combining two well-known methods, NAG and PG to improve the rate. Such a combination is simple and straightforward. The paper is well written and easy to follow."
            },
            "weaknesses": {
                "value": "1. The novelty is incremental. APG is not novel in terms of the algorithmic framework. \n\n2. Some assumptions to characterize the analysis are strong and not justified well. Why is Assumption 1 required to guarantee the convergence? Please justify in the paper. Otherwise, it might be a strong condition in the work. How likely is the assumption satisfied in the various real-world scenarios? What is the point to have Assumption 2? Not a more generic range [-R, R] in many existing works? Why is Assumption 4 is required for the convergence? I understand it could be attained in practice.\n\n3. In Theorem 1, the authors mentioned that the softmax parameterized policy is tabular. What does it mean by tabular here? Would it mean the policy acts like a lookup table? Do the conclusions still apply if removing tabular? It is confusing in the paper.\n\n4. The experimental results are not promising. The benchmark models are quite simple. The authors should present more complex benchmark models. Continuous environment should be utilized to showcase APG's superiority. \n\n5. Definitions 1 and 2 are a bit ah-hoc for the convergence proof in this work. If they are existing, the authors should cite references. Otherwise, the author should justify why they are needed.\n\n6. Section 6.2 did not really present anything new on the lower bound. How to relate Theorem 3 to APG? The authors only said due to Theorem 3, there was no tighter lower bound for APG. This seems to me quite simple. They should show the contradiction if there existed a tighter lower bound for APG.\n\nI think the paper still requires a substantial amount of work to make it technically solid and sound."
            },
            "questions": {
                "value": "Please see the questions in the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission210/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698723411492,
        "cdate": 1698723411492,
        "tmdate": 1699635946501,
        "mdate": 1699635946501,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4V8RMXhGsF",
        "forum": "PDL5A6facN",
        "replyto": "PDL5A6facN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission210/Reviewer_txPi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission210/Reviewer_txPi"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the Nesterov accelerated gradient method in the context of reinforcement learning. The authors theoretically show that with the true gradient, the proposed accelerated policy gradient with softmax policy parametrization converges to an optimal policy at a $O(1/t^2)$ rate. The empirical evaluation further demonstrates that APG exhibits $O(1/t^2)$ rate and APG could significantly improve the convergence behavior over the standard policy gradient."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "In general, this paper is well-structured, and the main idea of this work is easy to follow. This paper proposes a novel accelerated policy gradient method with a fast $O(1/t^2)$ convergence rate, which is the first Nesterov accelerated method with a provable guarantee in reinforcement learning. The authors of this paper also develop a new technical analysis for the proposed method to prove its convergence rate. The authors also conduct experiments to verify the efficiency of the proposed method empirically."
            },
            "weaknesses": {
                "value": "(1) The major concern about this work is that the authors did not present a detailed discussion of the work [1]. The work [1] has shown that a linear convergence rate, which is faster than $O(1/t^2)$, can be achieved by the policy gradient with an exponentially increasing step size. Moreover, the result in [1] is also based on the non-regularized MDP, which is the same setting as in this submission. The authors need to provide a detailed comparison of the theoretical results in [1] and this submission and also discuss the significance of the result in this submission, given the linear convergence rate in [1].  \n\n\n(2) Additionally, since this work discusses the lower bound of policy gradient, it is interesting to show why the linear convergence rate in [1] does not conflict with the lower bound provided in this submission.\n\n\n(3) The upper bound in this paper has a dependence on the factor $||\\frac{1}{\\mu}||\\_\\infty$. The recent work on policy gradient, e.g., [1] [2], has a convergence rate dependent on a tighter factor $||\\frac{d\\_{\\rho}^{\\pi\\_*}}{\\mu}||\\_\\infty$. Is it possible to sharpen such a factor in the result of this submission?\n\n\n[1] Lin Xiao. On the convergence rates of policy gradient methods. Journal of Machine Learning Research, 23(282):1\u201336, 2022.\n\n[2] Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy gradient methods: Optimality, approximation, and distribution shift. Journal of Machine Learning Research, 22(1):4431\u20134506, 2021."
            },
            "questions": {
                "value": "Please see the above section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission210/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699359787093,
        "cdate": 1699359787093,
        "tmdate": 1699635946423,
        "mdate": 1699635946423,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jfN3n3ES5v",
        "forum": "PDL5A6facN",
        "replyto": "PDL5A6facN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission210/Reviewer_6oea"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission210/Reviewer_6oea"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the convergence of Accelerated Policy Gradient (APG) with restart as shown in Algorithm 2. The main results show that this algorithm achieves a $\\tilde{O}(1/t^2)$ convergence rate toward globally optimal policy in terms of value sub-optimality. The technical innovation includes showing that the value function is nearly $C$-concave when optimal action's probability is large enough (locally around optimal policy), as well as using AGD's $\\tilde{O}(1/t^2)$ convergence results, and asymptotic global convergence in Agarwal et al."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Answering whether Nesterov's acceleration can be used in policy gradient is an interesting question.\n2. The technical challenges are well explained and real, including using momentum in non-convexity, and unbounded parameters.\n3. The simulations verify the proved rates."
            },
            "weaknesses": {
                "value": "1. There already exist acceleration methods for policy gradient, including natural policy gradient, and normalization which both lead to an exponential convergence rate, which might make this slower acceleration not that attractive to the community."
            },
            "questions": {
                "value": "1. How do you compare the acceleration provided by momentum with faster acceleration methods for policy gradient, such as natural policy gradient and normalization, as well as regularization?\n\n2. Any idea of using generalizing the methods to stochastic settings, where the policy gradient has to be estimated from samples?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission210/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699530378738,
        "cdate": 1699530378738,
        "tmdate": 1699635946364,
        "mdate": 1699635946364,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uwK9B9XTdJ",
        "forum": "PDL5A6facN",
        "replyto": "PDL5A6facN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission210/Reviewer_aHMD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission210/Reviewer_aHMD"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the use of Nesterov\u2019s accelerated gradient onto policy gradient. The work terms this methods accelerated\npolicy gradient. This improves the convergence rate from $O(1/t)$ to $O(1/t^2)$, provided that one has access to the true gradient. The work points out the intuition how the acceleration is benefited from the momentum. \n\nThe results are based on several assumptions on the RL problem structure. First, the surrogate initial state distribution has to be strictly positive for every state. Second, the optimal action has to be unique at every state.\n\nSome numerical test are provided in the manuscript."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The work uses Nestrov acceleration on policy gradient and improves the convergence rate.\n2. Techniques used in the proofs could be of independent interest."
            },
            "weaknesses": {
                "value": "One major concern is on the assumptions, especially Assumption 3 and Assumption 4. I believe these assumptions are way too strong, and drastically reduce the complexity of reinforcement learning problems and make the optimization landscape much easier to tackle with. The setting of true gradient and initial state distribution are also strong, though acceptable. I would prefer if the work, that claims they are the first to achieve Nestrov acceleration on PG, to be under a much more general setting."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission210/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699532344091,
        "cdate": 1699532344091,
        "tmdate": 1699635946281,
        "mdate": 1699635946281,
        "license": "CC BY 4.0",
        "version": 2
    }
]