[
    {
        "id": "QHjVJbOXOx",
        "forum": "Sx038qxjek",
        "replyto": "Sx038qxjek",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4343/Reviewer_hutJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4343/Reviewer_hutJ"
        ],
        "content": {
            "summary": {
                "value": "The papers proposes CRITIC, a framework for composing programs involving LMs self-correcting themselves using external tools. The authors conduct experiments with question answering, program synthesis and toxicity reduction show that CRITIC consistently improves the performance of LLMs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is written clearly and easy to follow.\n2. I found the comparison with ReAct interesting, i.e. the role of parameter knowledge vs language feedback."
            },
            "weaknesses": {
                "value": "1. I'm not convinced the CRITIC framework is novel enough to count as a contribution. The idea idea of using natural language feedback [1, 2, 3, 4] that guides LMs in revising their responses is pretty old as is the idea of using tools [5,6]. I agree the authors provide a nice unifying framework and some new downstream tasks (e.g. toxicity with PerspectiveAPI), but these don't seem to be pass the bar for ICLR. \n\n2. The authors don't compare with other frameworks endowing LMs with self-correction and tool use, like the ones listed above.\n\n3. I think the claim that tool use \"mimic[s] human thinking and behavior\" is overblown. Humans use think and work with tools very differently, typically not through a text-only interface. \n\n[1] https://arxiv.org/abs/2204.14146\n\n[2] https://arxiv.org/abs/2303.11366\n\n[3] https://arxiv.org/abs/2212.08073\n\n[4] https://arxiv.org/abs/2303.16749\n\n[5] https://arxiv.org/abs/2207.14502\n\n[6] https://arxiv.org/abs/2302.04761\n\n[7] https://openai.com/blog/function-calling-and-other-api-updates"
            },
            "questions": {
                "value": "How does the paper compare with other frameworks endowing LMs with self-correction and tool use?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4343/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698778711238,
        "cdate": 1698778711238,
        "tmdate": 1699636404720,
        "mdate": 1699636404720,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ia0G4Dfd33",
        "forum": "Sx038qxjek",
        "replyto": "Sx038qxjek",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4343/Reviewer_rWYL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4343/Reviewer_rWYL"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a framework called CRITIC to progressively validate and revise the output based on the feedback from tools. Six different external tools are used including Knowledge base, code interpreter, Text APIs, Wiki, Calculator and Search Engine. Evaluations are done on free-form question answering, mathematical program synthesis, and toxicity reduction. CRITIC was shown to have superior performance on these benchmarks compared to strong baselines including CoT, Self-Consistency, ReAct, and PoT."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. LLM Tool Use is a very timely research topic, and it is an important research area to use external feedback for the self-improvement of LLMs. This paper covers a wider range of tools compared to many prior works which typically employ one single type of tools.\n2. The results are rather strong with universal improvements across most tasks evaluated with several different model families and sizes.\n3. The ablation against CRITIC w/o Tool shows the importance of external feedback from Tools, which is an important learning for the community.\n4. The paper is very well written and is easy to understand with comprehensive comparisons to strong baselines."
            },
            "weaknesses": {
                "value": "Error analysis is missing on what are the failure modes after using Tools for feedback."
            },
            "questions": {
                "value": "1. It is unclear how important each Tool is to each task. Such analysis will provide further insight into where the improvements come from.\n2. The authors used different sampling config for the experiments for different tasks: e.g. p=0.9 was used for section 4.3 which is different from p=0.5 in 4.1 and 4.2."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4343/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698801621943,
        "cdate": 1698801621943,
        "tmdate": 1699636404639,
        "mdate": 1699636404639,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yzGMvIU2WY",
        "forum": "Sx038qxjek",
        "replyto": "Sx038qxjek",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4343/Reviewer_dDfr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4343/Reviewer_dDfr"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a framework called CRITIC that enables large language models (LLMs) to self-verify and self-correct their outputs by interacting with external tools. The authors demonstrate the effectiveness of CRITIC in improving the performance of LLMs across multiple tasks, including free-form question answering, mathematical program synthesis, and toxicity reduction. The paper highlights the importance of external feedback in promoting the ongoing self-improvement of LLMs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1) The paper introduces a novel framework, CRITIC, which addresses the limitations of LLMs by allowing them to verify and correct their outputs through interaction with external tools.\n\n2) The authors provide comprehensive evaluations of CRITIC on different tasks and datasets, demonstrating its consistent performance improvement over baseline methods.\n\n3) The paper highlights the crucial role of external feedback in the self-improvement of LLMs and emphasizes the unreliability of LLMs in self-verification."
            },
            "weaknesses": {
                "value": "1) I think this is a good paper. The motivation is strong: utilizing external feedback to enhance the model's ability. However, some recent studies [1] reported that large language models cannot self-correct themselves.  I acknowledge that [1] did not involve external tools, which is different from CRITIC's setting and it is a paper after CRITIC which is not necessarily be included, but it would be more comprehensive to include a discussion with these new studies in such a fast-moving field.\n\n2) How much of the additional costs? Since calling external tools costs money. The authors should report the cost for each experiment.\n\n3) In Appendix C.2, an important work active-prompt [2] should be included, which applies uncertainty estimation to chain-of-thought prompting.\n\n[1] Large Language Models Cannot Self-Correct Reasoning Yet\n[2] Active Prompting with Chain-of-Thought for Large Language Models"
            },
            "questions": {
                "value": "How much of the additional costs? Since calling external tools costs money. The authors should report the cost for each experiment."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4343/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698851580839,
        "cdate": 1698851580839,
        "tmdate": 1699636404581,
        "mdate": 1699636404581,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SZaAqbweE7",
        "forum": "Sx038qxjek",
        "replyto": "Sx038qxjek",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4343/Reviewer_ASEM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4343/Reviewer_ASEM"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces CRITIC, a method for improving  the outputs of language models by leveraging external feedback from various tools. The idea is to generate an initial output with the language model and then refine this output using feedback from an external tool, such as a Python interpreter, search engines, or toxicity detection APIs. Notably, this approach relies solely on in-context learning without the need for specialized training. Results across various tasks, including question answering, mathematical reasoning, and toxicity reduction, show that CRITIC improves over baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The experiments demonstrate the effectiveness of the proposed approach across a diverse set of tasks, indicating its potential to significantly improve the performance of large language models (LLMs).\n\n- Utilizing external feedback as a means of improving LLM outputs is practical. The simplicity of the approach is a plus, as it facilitates widespread application."
            },
            "weaknesses": {
                "value": "- The primary concern with this work is its novelty. Several studies have previously demonstrated that external feedback can be instrumental in correcting LLM outputs. In fact, there is existing work within each domain addressed in this paper, such as Self-Correct ([1], using external APIs), Self-Ask ([2], employing a search engine), and Self-Debug ([3], via a Python interpreter). Notably, Self-Debug and Self-Ask have a striking resemblance to CRITIC but are not referenced.\n\n\n- The settings that rely on an oracle are somewhat idealistic, and detract from the core message of the paper. It may be more appropriate to move these results to an appendix (as done by other works) to facilitate a clearer understanding.\n\n\n\n[1] Welleck, Sean, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. \"Generating Sequences by Learning to Self-Correct.\" In The Eleventh International Conference on Learning Representations. 2022.\n\n[2] Press, Ofir, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. \"Measuring and narrowing the compositionality gap in language models.\" arXiv preprint arXiv:2210.03350 (2022).\n\n\n[3] Chen, Xinyun, Maxwell Lin, Nathanael Sch\u00e4rli, and Denny Zhou. \"Teaching large language models to self-debug.\" arXiv preprint arXiv:2304.05128 (2023)."
            },
            "questions": {
                "value": "- The emphasis in Table 1 seems inconsistent. For instance, the AmbigNQ EM score of 50.0 is highlighted for Text-Davinci-003, but it is not the highest. Is this a bug or am I missing something?\n\n- Regarding the GSM task in a non-oracle setting, it appears that feedback from the interpreter is limited to syntactic correctness. Given the improvements, it suggests that many of the programs were initially syntactically wrong. Is this the case?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4343/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699056582158,
        "cdate": 1699056582158,
        "tmdate": 1699636404517,
        "mdate": 1699636404517,
        "license": "CC BY 4.0",
        "version": 2
    }
]