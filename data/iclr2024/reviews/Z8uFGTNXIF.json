[
    {
        "id": "2JPQSLrBnX",
        "forum": "Z8uFGTNXIF",
        "replyto": "Z8uFGTNXIF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7672/Reviewer_sg3G"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7672/Reviewer_sg3G"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new task called referred visual search. A new dataset is also created to achieve this task. This paper uses contrastive learning for extracting referred embeddings. Experiments achieve promising results in different tasks in LRVS-F dataset."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper proposes a challenging task for image similarity search in the context of fashion. A new dataset is also proposed at the same time.\n2. Conditional embedding is properly used to achieve this task. Experiments demonstrate the effectiveness of the method.\n3. This paper can be a baseline to do more relevant work."
            },
            "weaknesses": {
                "value": "1.This task is similar to composed image retrieval. Composed image retrieval aims to find the target image based on the reference image and text description. I have some doubts about the contribution and meaning of the task. \n2.The structure of the model is simple. It lacks innovation. The description of the model is not specific enough. Contrastive learning is often used in the task of composed image retrieval, so it is not an innovative method.\n3.Experiments are basically a comparison with other models, but the ablation experiment of your own model and visualization is lacking."
            },
            "questions": {
                "value": "1. You say it extracts referred embedding using weakly-supervised training. Why it is a weakly-supervised training?\n2. In fig2, it shares weight between the two vision transformers. In this model, it is whether all parameters are shared or only parts of the parameters are shared. And I want to know why to share the weight.\n3. In table1 and table2, you say you report bootstrapped means and standards deviations for 0K distractors, but I don\u2019t see the result of the 0K distractors. In addition, your model is similar to some models that used in the task of composed image retrieval. FashionIQ is also a dataset about clothes. Do you try this dataset using your method?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7672/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698759853840,
        "cdate": 1698759853840,
        "tmdate": 1699636933365,
        "mdate": 1699636933365,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DLOYiYFPWl",
        "forum": "Z8uFGTNXIF",
        "replyto": "Z8uFGTNXIF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7672/Reviewer_vVJq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7672/Reviewer_vVJq"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new task, named Referred Visual Search (RVS). It aims to search the specific part under the condition of category. The new task sounds good. The authors also introduce a corresponding dataset and framework. Some experimental results look good."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "## Strengths\n\n1. The writing is good. It is easy to follow this paper.\n\n2. The motivation sounds reasonable.\n\n3. Some experimental results look good."
            },
            "weaknesses": {
                "value": "## Weaknesses\n\n1. It may be not appropriate to use the entire image (even given the conditions) to search for an part area, such as pants.\n\n2. Why not crop the part area according to the given condition and then use it to search?\n\n3. How to collect the LAION-RVS-Fashion dataset? How to ensure the accuracy of the labels? The original labels are not clear.\n\n4. The main content of this paper has 10 pages."
            },
            "questions": {
                "value": "Please see above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7672/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698983518368,
        "cdate": 1698983518368,
        "tmdate": 1699636933122,
        "mdate": 1699636933122,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Snn56C0TYr",
        "forum": "Z8uFGTNXIF",
        "replyto": "Z8uFGTNXIF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7672/Reviewer_F8wp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7672/Reviewer_F8wp"
        ],
        "content": {
            "summary": {
                "value": "This paper aims at fashion retrieval conditioned on images and texts. Particularly, the text conditions can be categories and captions. This task is tackled via learning the joint embedding of texts and images, similar to conventional multi-modal metric learning methods. A dataset that is extracted from the publicly available dataset LAION-5B is constructed to validate the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed dataset can facilitate research on multi-modal fashion retrieval.\n2. Extensive experiments have been conducted to provide insightful information on this task.\n3. The writing of this paper is excellent and easy to follow."
            },
            "weaknesses": {
                "value": "1. Although this paper claims its target task is new, I still consider it to belong to multi-modal fashion retrieval. That is, one can include classes, attributes, captions, or even negative prompts in the textual conditions, and then leverage LLMs to process them uniformly. \n2. The failure cases suggest image features are dominant, and hence the proposed method or the task might not be as convenient as it claims. For example, what if the user wants to find clothes with similar styles but different colors, or of the same brand? Moreover, the text conditions seem rather simple, so whether the proposed method can handle fine-grained queries is unclear.\n3. The comparison between the proposed method and SOTAs might be unfair, e.g., ASEN is implemented partially and it only uses attributes. Other baselines with similar architectures, like FashionBert should be considered as well. Besides, how is the performance of the proposed method on other fashion retrieval benchmarks?"
            },
            "questions": {
                "value": "Please refer to the weakness part. I will adjust my score according if my concerns can be addressed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7672/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7672/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7672/Reviewer_F8wp"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7672/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699091099365,
        "cdate": 1699091099365,
        "tmdate": 1699636933026,
        "mdate": 1699636933026,
        "license": "CC BY 4.0",
        "version": 2
    }
]