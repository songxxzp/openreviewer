[
    {
        "id": "VXQtyXdxU2",
        "forum": "sFQe52N40m",
        "replyto": "sFQe52N40m",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7196/Reviewer_5U9r"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7196/Reviewer_5U9r"
        ],
        "content": {
            "summary": {
                "value": "This paper considers the label shift between training data and testing data, and focuses on the online case, where several batches of testing data arrive sequentially. To this end, the authors propose a method called Online Label Shift adaptation with Online Feature Updates (OLS-OFU), the main idea of which is to utilize a self-supervised learning (SSL) technique to refine the feature extraction process of existing OLS algorithms. Experimental results are presented to verify the performance of their method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1) Label shift is a common phenomenon in real applications, especially those with streaming data. So, the motivation of this study is clear.\n2) It seems that previous studies mainly focused on the online label shift (OLS) problem, and this paper is the first one to study the online generalized label shift problem, which introduces an unknown mapping $h(\\cdot)$ between the original data to some latent spaces.\n3) It seems that this paper is the first one to utilize the self-supervised learning (SSL) technique to improve the feature extraction process of existing OLS algorithms, which can further take advantage of unlabeled data to improve the testing performance."
            },
            "weaknesses": {
                "value": "1) The proposed method, namely Online Label Shift adaptation with Online Feature Updates (OLS-OFU), is a straightforward combination of any existing self-supervised learning (SSL) technique and any existing OLS algorithm, the novelty of which is limited. Moreover, it seems that there does not exist any challenge in this combination.\n2) Although the authors provide some theoretical guarantees for the proposed method, it seems that these results can be simply derived by following previous studies.\n3) Although the application of SSL is reasonable, the authors do not provide theoretical guarantees on its performance of learning the implicit feature mapping."
            },
            "questions": {
                "value": "As discussed in the above weaknesses, the authors should explain the novelty of their method and theoretical results. Moreover, the authors should also discuss the theoretical guarantees of SSL."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7196/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698577407889,
        "cdate": 1698577407889,
        "tmdate": 1699636854640,
        "mdate": 1699636854640,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9sMjtqQuob",
        "forum": "sFQe52N40m",
        "replyto": "sFQe52N40m",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7196/Reviewer_xmwo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7196/Reviewer_xmwo"
        ],
        "content": {
            "summary": {
                "value": "The paper study the problem of online label shift adaptation. Different from existing methods that focus on adjusting or updating the final layer of the pre-trained model, the paper proposes to update the feature representation layers by using self-supervised learning techniques."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The motivation of this paper is clear. Considering that most of existing methods only consider updating the final layer of classifier, the paper uses the self-supervised learning techniques to learn good feature representations and improve model performance.\n\n2. The implementation is very simple and easy to follow."
            },
            "weaknesses": {
                "value": "1. The idea is not very novel. Considering that self-supervised learning techniques are originally designed to boost feature representation learning, the idea presented in this paper does not appear very novel. Improving model performance by incorporating self-supervised techniques seems intuitive and not a very surprising and insightful finding.\n\n2. The technical contribution is limited. This paper hardly introduces any new technique the proposed method is merely a combination of existing techniques.\n\n3. The theoretical results in the paper seem to be derived from existing works.\n\n4. Experiments are weak in the current version.\n\n1) Only the results from one dataset are reported in the main paper. These results are insufficient to validate the effectiveness of the proposed method.\n\n2) The experimental results presented in the main paper provide limited information and lack many ablation experiments, which are crucial to supporting the conclusions of the paper. For example, how self-supervised learning techniques improve the model performance?"
            },
            "questions": {
                "value": "How does self-supervised learning work in the studied learning scenarios? Is there any difference from the original way it works?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7196/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7196/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7196/Reviewer_xmwo"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7196/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698577747690,
        "cdate": 1698577747690,
        "tmdate": 1699636854530,
        "mdate": 1699636854530,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "87hqTqaQAO",
        "forum": "sFQe52N40m",
        "replyto": "sFQe52N40m",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7196/Reviewer_VDvw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7196/Reviewer_VDvw"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the problem of label shift in the online setting, where the data distributions vary in a non-stationary environment. Unlike the previous which focused on re-weighting the pretrained classifier or re-training the final linear layer of the classifier, this paper proposed to enhance the feature representation by leveraging the unlabeled data at test time. Specifically, a novel Online Label Shift (OLS) adaptation with Online Feature Updates (OFU), named OLS-OFU, was proposed by refining the feature extraction process through self-supervised learning (SSL). Theoretical analysis indicated that the proposed OLS-OFU could reduce the algoritmic regret by introducing the SSL. And the empirical results showed the effectiveness and the robustness of the proposed method under both the OLS setting and the online generalized label shift (OGLS) setting."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The setting that this paper considered, online (generalized) label shift, is interesting and realistic in the practical scenarios. The investigation of the adaptation methods in such scenarios is valuable.\n2. The method proposed in this work is simple. It can be flexibly combined with different existing online label shift adaptation methods, such as FLHFTL, ROGD, UOGD, etc.\n3. Most parts of this paper are well-written and easy to follow."
            },
            "weaknesses": {
                "value": "1. In my opinion, the motivation of introducing feature extraction refinement is not strong. In the Introduction part, the authors claimed that \"feature extractors can still be improved, even during test-time and in the absence of labels\". And the authors hypothesized that \"a similar effect can be leveraged in the generalized label shift setting\". However, I did not see strong relations between the drawbacks of the existing methods in this field and the lack of feature representation improvement. \n2. The theoretical support in this work is not well verified by the empirical results. In Eq.(5) of the draft, the math showed that the online updates yield improvements. However, this point cannot be theoretically guaranteed. Furthermore, existing empirical results cannot reveal the relationship between the larger improvement and the better performance.\n3. Some algorithmic steps are a little confusing for the readers.\n\n\nSee Questions part for more details."
            },
            "questions": {
                "value": "1. About the motivation of introducing the feature refinement via self-supervised learning. I did not clearly get why we should focus on the feature improvement. For example, why did you believe the feature representation learning has not been enough in the existing methods? Why should we introduce the feature representation improvement, and is there any empirical support? If we cannot explain this point, introducing self-supervised learning just looks like a naive combination of the online label shift adaptation with the popular topic, self-supervised learning.\n2. About the theoretical analysis in Eq. (5). I believe that the motivation of introducing the online feature updates comes from the assumption in this inequality. 5And the authors tried to provide empirical evaluations in Sec. 4 (and Appendix D.6) to verify the holdness of this inequality. However, there is no further demonstrations of the quantitative relationship between the amount of feature representation refinement (or in other words the tightness of Eq. (5)) and the final performance. I will describe my question in the mathematical way:\n\nSuppose $\\mathcal{M}$ denote the OLS methods (e.g., $\\mathcal{M}\\in${FLHFTL, FTH, ROGD,...}). Take $\\mathcal{M}=$FLHFTL as an example. \nLet \n$$X^{\\mathcal{M}}=\\mathbb{E}[ \\frac{1}{T} \\Sigma_{t=1}^{T}\\ell(f_{t}^{\\mathcal{M}-ofu}; \\mathcal{P}_{t}^{test})]$$\n\n$$Y=\\mathbb{E}[ \\frac{1}{T} \\Sigma_{t=1}^{T}\\ell(g(\\cdot; f_{t}^{\\prime\\prime}, q_t/q_0); \\mathcal{P}_{t}^{test})]$$ \n\nand \n$$Z=\\frac{1}{T} \\Sigma_{t=1}^{T}\\ell(g(\\cdot; f_{0}, q_t/q_0); \\mathcal{P}_{t}^{test})$$\n\nAccording to [1],  Then, the Eq.(3) can be rewrote as \n$$ X^{\\mathcal{M}} - Y  \\leq \\mathcal{O}(\\frac{K^{1/6}V_{T}^{1/3}}{\\sigma^{2/3}T^{1/3}} + \\frac{K}{\\sigma \\sqrt{T}})$$\n\nIf we define $\\Delta = Z- Y$  as **the amount of improvement** ($\\Delta \\geq 0$ if we admit Eq. (5)). \nThen, introducing feature update can make the original bound in Eq. (4) tighter by $\\Delta$:\n$$X^{\\mathcal{M}} - Z \\leq \\mathcal{O}(\\frac{K^{1/6}V_{T}^{1/3}}{\\sigma^{2/3}T^{1/3}} + \\frac{K}{\\sigma \\sqrt{T}}) - \\Delta$$\n\nThe larger improvement we make by feature update (in other words larger $\\Delta$), the tighter bound we can derive from the original version.\nThus, my question is: **could you please verify this point with empirical results to support your motivation in Eq. (5) in a quantitative manner?** Maybe we can fix an OLS method $\\mathcal{M}$, and then choose different ways to obtain $f_{t}^{\\prime\\prime}$ (e.g., different $\\ell_{ssl}$), then give a quantitative measure on the loss of $\\mathcal{M}$-OFU (i.e., $X^{\\mathcal{M}}$ defined above) with respect to the value of $\\Delta$?\n\n3. It seems the organization of the descriptions in Algorithm 1&2 is a little confusing and make the core steps of your methods less readable. For example, if we start Algorithm 1 from $t=1$, the step-1 of Algorithm 1 aims to return $f_{2}^{\\prime} \\leftarrow \\textit{OLS-R}$. However, in the details of this step shown in Algorithm 2, it seems we need $f_{1}^{\\prime\\prime}$ returned by the step-3 of Algorithm of the previous loop. I failed to get what is the exact definition of $f_{1}^{\\prime\\prime}$.\n\nReferences:\n\n[1] Online label shift: Optimal dynamic regret meets practical algorithms. NeurIPS 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "no"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7196/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7196/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7196/Reviewer_VDvw"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7196/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698675143654,
        "cdate": 1698675143654,
        "tmdate": 1699636854421,
        "mdate": 1699636854421,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9CHtBY5zwf",
        "forum": "sFQe52N40m",
        "replyto": "sFQe52N40m",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7196/Reviewer_TVnp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7196/Reviewer_TVnp"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel method called OLS-OFU for improving online generalized label shift adaptation. The proposed method builds upon previous OLS methods by additionally leveraging self-supervised learning to improve feature representations and enhance predictive models. The paper provides theoretical analyses and empirical tests to demonstrate the effectiveness and robustness of OLS-OFU, especially in cases of domain shifts. The empirical tests were conducted on CIFAR-10 and CIFAR-10C datasets."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is well written.\n2. This paper addresses a seldom encountered yet highly realistic scenario called online generalized label shift. The proposed method builds upon previous OLS methods by additionally leveraging self-supervised learning to improve feature representations and enhance predictive models, offering a promising approach for addressing this problem.\n3. This paper has conducted thorough experiments on various OLS methods and has also derived theoretical bounds separately for each method. The experimental results validate the theoretical analysis."
            },
            "weaknesses": {
                "value": "1. The proposed method doesn't introduce many novel ideas; it mainly involves modifying previous OLS methods under the setting of online generalized label shift and iteratively optimizing the model by incorporating additional self-supervised loss.\n2. The experimental results are presented in the form of graphs and charts. It might be beneficial to include some quantitative tables for better clarity and to facilitate comparisons by others. Besides, the experimental dataset is relatively small. Conducting experiments on a larger dataset, such as ImageNet-C, would be more convincing."
            },
            "questions": {
                "value": "1. Can the online generalized label shift scenario be simplified as a combination of domain shift and label shift?\n2. The selected self-supervised learning methods often require an additional branch, and their computational cost is not insignificant. Have you explored alternative lightweight methods for implementation, such as BN adaptation or entropy minimization?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7196/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698827674645,
        "cdate": 1698827674645,
        "tmdate": 1699636854312,
        "mdate": 1699636854312,
        "license": "CC BY 4.0",
        "version": 2
    }
]