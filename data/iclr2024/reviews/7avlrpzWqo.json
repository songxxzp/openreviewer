[
    {
        "id": "ubcIodcuS7",
        "forum": "7avlrpzWqo",
        "replyto": "7avlrpzWqo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6879/Reviewer_Es7u"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6879/Reviewer_Es7u"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a novel robust aggregator, i.e., Flag Aggregator (FA), to deal with Byzantine faults in distributed computation. FA is an optimization-based subspace estimator that formulates aggregation as a maximum likelihood estimation procedure using Beta densities. In distributed training setups where vanilla mean aggregators are replaced by robust aggregators without additional tricks, FA consistently outperforms many other existing robust aggregators in extensive experiments with various batch sizes, fractions of Byzantine nodes, and number of nodes."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed FA is novel in the distributed optimization literature. FA makes use of dependence among distributed gradients, while most existing aggregators only exploit the pairwise distance or moment conditions. This is beneficial in that if one can design aggregator that utilizes the data dependence, this aggregator is actually adaptive to this specific problem automatically, and thus one can improve performance. The authors provide insights on the intuitions, formulations, and approximate solutions to FA.\n3. Consistent empirical performance improvement against many existing aggregators is achieved, among extensive experiments that investigate many facets of the Byzantine distribution optimization problem."
            },
            "weaknesses": {
                "value": "1. There are no specific examples that can show FA is theoretically better than existing methods, and thus makes the claim not persuasive to some audience. It can be, possibly, FA only performs well in the specific datasets, models, the way of distributing datasets among compute nodes in the experiments presented in this work. Even a naive toy example would help people understand why FA is theoretically better. \n2. There are no comparisons between FA and existing aggregators in terms of computation complexity. It can be that, FA is too expensive to use in every iteration. \n3. The intuition behind the development of FA is not clear enough to me, this goes to the first bullet, can the authors provide an example or a simple problem model to illustrate this? And why is FA optimal as claimed in line 83? Can the authors elaborate on this point?"
            },
            "questions": {
                "value": "1. In line 22, the description of quadratic function is not clear to me, can the authors put it simpler? \n2. How will the augmented data and stable diffusion influence the mutual dependence of distributed data, can the authors comment on that?\n3. In line 72, what does it mean for noise to be nonlinear? \n4. In line 73 & 74, how does the discrete hyper parameters hamper convergence of the overall training procedure, can you elaborate on that?\n5. In line 76, the authors mention 'sparse', does that correspond to the choice of the second dimension of $Y$, i.e., $m$, how is $m$ chosen? \n6. In line 115, it seems that $g$ should $g_i$, and there is no need to use trace on a scalar right? \n7. In figure 3, I don't understand what is optimal subspace, do you mean optimality in the sense of formulation (5)?\n8. Why is beta distribution used in the formulation of FA? \n9. In the experiment starting from line 300, what attacks are being used?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6879/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6879/Reviewer_Es7u",
                    "ICLR.cc/2024/Conference/Submission6879/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6879/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698797729392,
        "cdate": 1698797729392,
        "tmdate": 1700712825349,
        "mdate": 1700712825349,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jrGqwvxsPw",
        "forum": "7avlrpzWqo",
        "replyto": "7avlrpzWqo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6879/Reviewer_PqPx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6879/Reviewer_PqPx"
        ],
        "content": {
            "summary": {
                "value": "This work proposes an aggregation function based on low-rank projection. In particular, for a given matrix $G$, this work proposes to perform the aggregation by projecting it to a low-dimensional space, $YY^TG1$, where $Y$ is the subspace chosen based on low-rank factorization of $G$. The connection between the algorithm and the Maximum Likelihood Estimation procedure using Beta densities is presented. Experimental results show improvements in communication efficiency and accuracy compared to previous works."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed method is simple yet effective. Theoretical analysis is also provided to backup the algorithm.\n\n2. Detailed experimental results are provided, and better accuracy has been shown compared to the previous works."
            },
            "weaknesses": {
                "value": "Here are a few comments regarding the experimental section:\n\n1. The choice of setting the subspace rank $m$ to $(p+1)/2$ in all experiments raises a question. Is this decision rooted in theoretical analysis or other considerations?\n\n2. The paper introduces a general framework in Section 2, considering a general norm and suggesting SDP for solving the system. However, the focus shifts to $\\ell_1$ regularization later due to its ease of optimization. An experimental comparison of different regularization terms could bridge the apparent gap between Sections 2 and 3.\n\n3. It would be useful to explore whether the proposed algorithm offers advantages even when $f=0$."
            },
            "questions": {
                "value": "n/a"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6879/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6879/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6879/Reviewer_PqPx"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6879/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698819694123,
        "cdate": 1698819694123,
        "tmdate": 1699636799809,
        "mdate": 1699636799809,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YADMInOQuX",
        "forum": "7avlrpzWqo",
        "replyto": "7avlrpzWqo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6879/Reviewer_YEpb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6879/Reviewer_YEpb"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes Flag Aggregator, a simple Maximum Likelihood Based estimation procedure for aggregation purposes. They show that any procedure used to solve Flag Optimization can be directly used to obtain the optimal summary statistic $Y^*$.The authors also show the approach is resilient against Byzantine attacks for gradient aggregation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Gradient aggregation is a critical design choice in many of the distributed training applications, and is ubiquitous. The proposed method seems promising and useful for this space."
            },
            "weaknesses": {
                "value": "- I am not sure how much overhead the SVD might bring in practice, could you provide some real-world measurement? So far all the empirical results are epoch-wise measuring.\n- Most of the baseline compared in the experiments seem to be from at least five years ago (2018); I wonder if the authors can compare their approach with latest algorithms? For instance  Allouah et al. (2023a;b); Farhadkhani et al. (2022) as mentioned in the related work."
            },
            "questions": {
                "value": "I'll increase my rating if comparison to more recent algorithms is provided."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6879/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699212856124,
        "cdate": 1699212856124,
        "tmdate": 1699636799685,
        "mdate": 1699636799685,
        "license": "CC BY 4.0",
        "version": 2
    }
]