[
    {
        "id": "sOzYFG5NiQ",
        "forum": "RCKoQGpPEN",
        "replyto": "RCKoQGpPEN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission971/Reviewer_tzju"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission971/Reviewer_tzju"
        ],
        "content": {
            "summary": {
                "value": "This paper tackles the well-known video task, video panoptic segmentation, and presents MaXTron. From the inherent property of the VPS task that comprises other video segmentation tasks such as VSS and VIS, MaXTron can be considered as a general video segmentation framework.\nThe authors points out a couple of challenges of the video segmentation tasks, and target to alleviate such challenges. Specifically, per-clip segmentation methods (which MaXTron also belongs to) have put efforts in improving inter-clip and intra-clip predictions. In order to improve inter-clip predictions, the authors suggest the Within-Clip Tracking Module, which consists of a stack of multi-scale deformable attention followed by Axial-Trajectory Attentions. For intra-clip association, MaXTron fully leverages the object queries that possess object-level information, and insert into the Cross-Clip Tracking Module that has Trajectory Attention and Temporal ASPP.\nFinally, utilizing the presented modules, MaXTron achieves compelling results, demonstrating state-of-the-art accuracy."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "This paper has a clear structure that ease the readers to follow and understand which components are being used. \nThe authors points out important problems of the video segmentation tasks and each module is designed with a specific goal.\nCombining them all, MaXTron achieves improvements in the accuracy on multiple benchmarks, highlighting the effectiveness of each module."
            },
            "weaknesses": {
                "value": "The major weakness of this paper is lack of novelty. Each component used in the design of MaXTron is mostly brought from existing literatures or with a subtle modification.\nFor instance, Multi-level Deformable Attention, Transformer Decoder, and ASPP are brought from previous works.\n\nThe used modules with slight changes are 1) Axial-Trajectory Attention and 2) Cross Clip Tracking Module.\nThe Axial-Trajectory Attention manipulates the set of tokens fed into transformer attentions, which has been widely used in lots of different areas.\nAdditionally, Cross Clip Tracking Module is a simple modification to VITA, using clip-level outputs instead of frame-level outputs.\n\nAs this paper used a number of components that are already proven effective, it is rather expected to see the gain in the accuracy."
            },
            "questions": {
                "value": "How much are the FLOPs and FPS of MaXTron compared to other methods?\nWhat's the statistical significance, i.e. how many runs were executed for reporting the numbers? Are the numbers mean/median of multiple trials?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission971/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission971/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission971/Reviewer_tzju"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission971/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698034161558,
        "cdate": 1698034161558,
        "tmdate": 1699636023084,
        "mdate": 1699636023084,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "e8OQyVLNyO",
        "forum": "RCKoQGpPEN",
        "replyto": "RCKoQGpPEN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission971/Reviewer_LGcv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission971/Reviewer_LGcv"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a novel panoptic segmentation method, namely MaXTron, which enhances temporal consistency by the proposed within-clip and cross-clip tracking modules. Axial-trajectory attention is the essential component of the introduced tracking modules, which aims at associating objects meanwhile reducing computational complexity.Experimental results have shown state-of-the-art performance on video segmentation benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "It sounds interesting to conduct in-clip tracking and cross-clip tracking via axial-trajectory attention.\n\nAssociation with non-overlapping clips is more efficient than previous overlapping-based methods. \n\nThe results are promising."
            },
            "weaknesses": {
                "value": "The writing of sec.3 (method) should be improved. It\u2018s a bit confusing about the implementation details.\n\nBesides the performance, it is suggested to provide the cost, e.g. training cost and inference speed, of integrating the proposed model into existing methods.\n\nBesides the overall performance, a deeper analysis is expected. For instance, how does the association capability improve after integrating the proposed modules into an off-the-shelf method?"
            },
            "questions": {
                "value": "see weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission971/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698240903848,
        "cdate": 1698240903848,
        "tmdate": 1699636023016,
        "mdate": 1699636023016,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lbBP7GSLCt",
        "forum": "RCKoQGpPEN",
        "replyto": "RCKoQGpPEN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission971/Reviewer_tzTu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission971/Reviewer_tzTu"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors proposed trajectory attention based mask transformer for video panoptic segmentation. Specifically, two types of tracking modules (within-clip and cross-clip tracking) are proposed to improve the temporal consistency by leveraging trajectory attention. The within-clip tracking module, an axial-trajectory attention is proposed for effectively computing the trajectory attention for tracking dense pixels sequentially along the height- and width-axes, while the cross-clip tracking module is used to capture the long-term temporal connections by applying trajectory attention to object queries. The experimental shows that the proposed solution is able to help boost the performance of existing solutions (e.g., Video-kMax and Tube-Link) on multiple datasets."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed solution sounds solid. (1) Using trajectory attention to force the model pay more attention spatially and temporally on trajectories (maybe simply on pixel trajectories) while doing video segmentation sound solid in theory. The attention should be able to provide extra useful information to the model. (2) Splitting the trajectory attention along different axes (horizontal and vertical) indeed helps reduce the complexity while calculating attention. \n2. The experimental results on multiple datasets and models prove that the proposed solution works in varying application scenarios. \n3. This paper is well-organized, which help readers easy to read and understand. Expecially, there are more implementation details and results reported in the appendix, which helps readers better understand their work and the performance."
            },
            "weaknesses": {
                "value": "1. It will be better to report some failure cases. It will be helpful if the authors could report some failure cased that caused by applying the proposed MaxTron. In this case, readers will better understand their work and the performance, which may inspire more ideas along this direction. \n2. The proposed solution sounds like an add-on to the existing solutions, which was inspired by other works (e.g., Patrick et al. 2021). The novelty may be incremental."
            },
            "questions": {
                "value": "1. What if we change the number of frames within one video clip? Is there any positive / negative impact on the model performance? Is there any guidances (or suggestion) of how many frames should be selected while spliting the video?\n2. Does the proposed solution perform differently if we (1) process all continuous frames or (2) only process key frames (with some down-sampling temporally)? The later operation will speed up motions in videos."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission971/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission971/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission971/Reviewer_tzTu"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission971/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698271135732,
        "cdate": 1698271135732,
        "tmdate": 1700512802448,
        "mdate": 1700512802448,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "N2TuJWBJmx",
        "forum": "RCKoQGpPEN",
        "replyto": "RCKoQGpPEN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission971/Reviewer_TBSQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission971/Reviewer_TBSQ"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors mainly study the clip-level video panoptic segmentation. They propose a new framework using Mask Xformer with trajectory attention, named MaXTron. It includes within-clip and cross-clip tracking modules, to use trajectory attention. The experimental results show the effectiveness of their proposed model."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper is well-written and easy to follow. The idea of using trajectory information to help the segmentation and decompose the attention into height and width in two directions, greatly reducing the computational complexity. They have done comparison and ablation studies to validate their proposed components."
            },
            "weaknesses": {
                "value": "The figures might not be easy to follow. For example, in Fig. 3, they show H and W-axis attention maps of one point. It would be much better, if they also show how to get the probabilistic path of a point between frames and what the whole attention maps for static and dynamic points. Besides, the authors should show the details in trajectory attention module and temporal ASPP in Fig. 4 and it can help readers to understand. \n\nThe main contribution of this work is the trajectory based within-clip and cross-clip module, which might be limited and insufficient for this conference, even if the authors could clearly introduce their modules using Fig. 3 and 4 after revision. \n\nIn the experiment, the authors are suggested to add some examples to show the attention maps and how to get the trajectories or the trajectories might not be perfect."
            },
            "questions": {
                "value": "The main contribution of this work is the trajectory-based within-clip and cross-clip modules, which might be limited and insufficient for this conference, even if the authors could clearly introduce their modules using Fig. 3 and 4 after revision.\n\nIn the experiment, the authors are suggested to add some examples to show the attention maps and how to get the trajectories or the trajectories might not be perfect.\n\nIt would be much better if they also showed how to get the probabilistic path of a point between frames and what the whole attention maps for static and dynamic points."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission971/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission971/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission971/Reviewer_TBSQ"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission971/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698498136663,
        "cdate": 1698498136663,
        "tmdate": 1700451157058,
        "mdate": 1700451157058,
        "license": "CC BY 4.0",
        "version": 2
    }
]