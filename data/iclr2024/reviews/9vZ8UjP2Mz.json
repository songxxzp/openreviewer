[
    {
        "id": "UOdLP9vxUn",
        "forum": "9vZ8UjP2Mz",
        "replyto": "9vZ8UjP2Mz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7010/Reviewer_FTaq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7010/Reviewer_FTaq"
        ],
        "content": {
            "summary": {
                "value": "This paper studies optimization algorithms for the approximate implicit differentiation (AID) based bilevel optimization problems, which own an inner and an outer objective function. Existing studies mainly consider the development of optimization algorithms and convergence analysis, and there seems merely single work on the algorithmic stability-based generalization assessment [1] as described in this paper. Instead, the paper considers the argument-level uniformly stability and generalization analysis for the complex AID algorithms where the upper level problem is non-convex. Especially, there are several intermediate variables in the so-called AID algorithms, which may bring challenges to the analysis.\n\tIncluding the convergence analysis, this paper further gives a similar result on algorithmic stability bounds as compared to existing bilevel work [1] and single-level one [2]. However, this may not obviously highlight the advantages to [1] or contributions of this work to the community.\n\n[1] Stability and generalization of bilevel programming in hyperparameter optimization, NeurIPS\u201921\n[2] Bilevel Optimization: Convergence Analysis and Enhanced Design, ICML\u201921"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(1) This paper considers the AID based optimization algorithm, which is commonly used in bilevel problems. There are several intermediate variables in the so-called AID algorithms, which may bring challenges to the analysis. \n(2) And the AID algorithm indeed is more complex than classical iterative differentiation (ITD) algorithms. Furthermore, the convergence analysis of the AID algorithm (Algorithm 1) is also provided. The generalization and convergence analysis provide some theoretical guarantees to the practice. \n(3) The settings of step sizes for updating these variables seems practically feasible, which may further provide guidance to the realistic practice to improve and balance the convergence rate and generalization performance. \n(4) This paper is fairly\u00a0well-written, the representation of the learning algorithms (like the commonly used decomposition strategy) is helpful for readers to better understand the learning objects."
            },
            "weaknesses": {
                "value": "I checked parts of the proof in the appendix, and not check all the proof. There may exist some typos and technical flaws in the analysis that I have checked as follows:\n\n(1) Even though the algorithm is complex with several intermediate variables involved in the analysis, the proof techniques for each are analogous to [1] and some existing stability-based work [3-5]. And the employed argument-based stability tool indeed has been used previously [4].\n\tThe authors are suggested to further add some descriptions of the employed assumptions (e.g., the e-accuracy from [2], Assumption 3 from \u2026), existing stability-based work and provide some discussions on the differences with them. Besides, it is kindly suggested to highlight the challenges and difficulties of the proof, so as to help the readers understand the contributions of this work.\n\n[1] Stability and generalization of bilevel programming in hyperparameter optimization, NeurIPS\u201921\n[2] Bilevel Optimization: Convergence Analysis and Enhanced Design, ICML\u201921\n[3] Stability and generalization, JMLR\u201902\n[4] Algorithmic stability and hypothesis complexity, ICML\u201917\n\n\n(2) As described in the contribution, I\u2019d like to know from which perspective of this paper, the robustness of the AID algorithm can be demonstrated. And the so-called robustness is the adversary robustness or some statistical robustness [1] ?\n\n[1] Huber P J. Robust statistics. \n\n(3) In Definition 1, only perturbation in the validation (or outer) dataset is considered. Why not consider perturbation in the inner-level training dataset? Some discussions of the reasons for this definition are needed.\n\tIn fact, there are prior works that consider perturbation on both datasets for a special case of bilevel learning, meta learning. See [1]. \n\n[1] Generalization of Model-Agnostic Meta-Learning Algorithms: Recurring and Unseen Tasks, NeurIPS 2021.\n\n(4) However, Algorithms 1&2 are not the same as the previous works that I\u2019ve read [1-3]. It is understandable that approximation algorithms are not unique for AID algorithms (see Table 1 in [2]). Please indicate in detail which related works has proposed or used these algorithms?\n\n[1] Bilevel Optimization: Convergence Analysis and Enhanced Design\n[2] Optimizing Millions of Hyperparameters by Implicit Differentiation\n[3] Approximation Methods for Bilevel Programming\n\n\n(5) Although the authors give an example of non-convex upper problem, it could be still interesting and necessary to further analyze other conditions. For some examples in convergence analysis, an asymptotic analysis are conduct under the assumptions that the lower and upper functions are convex and strongly convex [1,2]. [3] studied the setting where average version \\Fai() is strongly convex or convex, and g(x,.) is strongly convex.\nIt would be better if this issue is considered, but not necessarily required.\n\n[1] A generic first-order algorithmic framework for bi-level programming beyond lower-level singleton\n[2] Improved bilevel model: Fast and optimal algorithm with theoretical guarantee\n[3] Approximation methods for bilevel programming.\n\n(6) In Corollary 1, the constants \\alpha and \\beta are involved in the learning rate without detailed definition? Is the \\beta here the same as the stability definition in Definition 1 or 2?\n\n(7) In the Appendix A, the conclusion of Lemma 8 seems to be wrongly used in following lemmas and Theorem 1. Dz indeed could only bound z^K, which does not hold for z^0 or z^k where k<K.\t\n\tThis could be the most serious weakness and will render some arguments in the paper invalid.\n\n(8) In the proof of Lemma 11 (see Appendix, P.14), the rules for updating variable \u2018m\u2019 seems contractive with the Algorithm 1. Besides, the mixture usage of y and y*(x) indeed cause confusion (see Lemma 16 and Theorem 2). \n\nThe following are the potential typos or mistakes that have been found:\n\tIn Remark 3, the \\Theta -> O\n\tIn the proof of Lemma 8 (P.12), A is undefined\n\tIn the proof of Lemma 9 (P.12), symbol \u2018+\u2019 before \\eta_z is ignored; (P.13 Line.3) symbol \u2018=\u2019 \n\tIn the proof of Lemma 10 (P.13), symbol \u2018<=\u2019 is ignored; the description of \u201cBy selecting \\eta_y such that \\mu \\eta_y \u2265 L_2 \\eta_y\u201d seems wrong; (P.14) x should lie in [0,0.5] in the first line; \u20182\u2019-> \u2018K+1\u2019 in the 4-th line. \n\tIn the proof of Lemma 11 (P.14), the rules for updating m seems contractive with the Algorithm 1.\n\tIn the proof of Lemma 15 (P.18), \u2018X\u2019 or \u2018x\u2019?\n\tIn the proof of Lemma 16 (P.18), \u2018\\nabla_t\u2019 -> \u2018\\nabla_y\u2019; single \u2018\\nabla\u2019 -> \u2018\\nabla^2_yy\u2019\n\nThe authors are kindly asked to check these issues when conducting the revision."
            },
            "questions": {
                "value": "Please see Weaknesses stated as above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7010/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698452541124,
        "cdate": 1698452541124,
        "tmdate": 1699636821586,
        "mdate": 1699636821586,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "meSXzwg9Vg",
        "forum": "9vZ8UjP2Mz",
        "replyto": "9vZ8UjP2Mz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7010/Reviewer_JTQs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7010/Reviewer_JTQs"
        ],
        "content": {
            "summary": {
                "value": "The paper studies bilevel optimization problems. It aims at understanding the generalization behavior AID method.  They provide excess uniform stability bounds on validation data under certain assumptions, the stability bound has a scaling of $O(T^q/n)$."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The main strength of the paper is that it investigates the stability of the AIM method in Bilevel optimization.\n2. The paper is well-written and theoretically solid."
            },
            "weaknesses": {
                "value": "1. The influence of the inner iteration K is not discussed. In Theorem 2, the paper only gives 'K is large enough'. However, in the experiments, the author only uses K=2. I think this is confumsing. Is there any relation between K and T? [1] proposed a method where K=1, so may be 'K is large enough' is not needed.\n2. If K is needed to be large, i think the performance with different K should be given.\n[1] Dagr\u00e9ou M, Ablin P, Vaiter S, et al. A framework for bilevel optimization that enables stochastic and global variance reduction algorithms[J]. Advances in Neural Information Processing Systems, 2022, 35: 26698-26710."
            },
            "questions": {
                "value": "1. The influence of the inner iteration K is not discussed. In Theorem 2, the paper only gives 'K is large enough'. However, in the experiments, the author only uses K=2. I think this is confumsing. Is there any relation between K and T? [1] proposed a method where K=1, so may be 'K is large enough' is not needed.\n2. If K is needed to be large, i think the performance with different K should be given.\n[1] Dagr\u00e9ou M, Ablin P, Vaiter S, et al. A framework for bilevel optimization that enables stochastic and global variance reduction algorithms[J]. Advances in Neural Information Processing Systems, 2022, 35: 26698-26710."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7010/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7010/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7010/Reviewer_JTQs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7010/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698667388760,
        "cdate": 1698667388760,
        "tmdate": 1699636821472,
        "mdate": 1699636821472,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cXxX3OKd2h",
        "forum": "9vZ8UjP2Mz",
        "replyto": "9vZ8UjP2Mz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7010/Reviewer_CjiH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7010/Reviewer_CjiH"
        ],
        "content": {
            "summary": {
                "value": "The paper first presents a novel analysis framework for examining multi-level variables within the stability of bi-level optimization. It provides a stability analysis for non-convex optimization with various learning rate configurations. The paper offers convergence results for AID-based methods and highlights the trade-off between convergence and stability of these algorithms. The authors conduct an ablation study of the parameters and assess the performance of these methods on real-world tasks, providing practical guidance on managing and minimizing gaps in bi-level optimization. The experimental results corroborate the theoretical findings, demonstrating the effectiveness and potential applications of AID-based bi-level optimization methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(1) The authors first present an innovative analysis framework aimed at systematically examining the behavior of multi-level variables within the stability of bi-level optimization, which derives theoretical stability bounds for AID-based bi-level methods.\n\n(2) This study illustrates the uniform stability of AID-based techniques, even under mild conditions. The stability bounds observed are comparable to those encountered in nonconvex single-level optimization and ITD-based bi-level methods.\n\n(3) Their research reveals generalization gap results related to optimization errors, offering a deeper understanding of the trade-offs between convergence and stability of the AID-based bi-level optimization, ultimately improving the e\ufb00iciency and effectiveness of the AID-based methods."
            },
            "weaknesses": {
                "value": "(1) This analysis in this paper appears heavily reliant on smoothness assumptions for the inner and outer functions, the theoretical results may be not suited to loss function with L1 penalty. Therefore, the results in this paper do not yet seem generalizable to the broader field of artificial intelligence.\n\n(2) The paper does not seem to consider the generalization ability of the inner problem, but sometimes the solution to the inner problem is equally important. Therefore, it would be better if the generalization ability of the lower problem could also be analyzed.\n\n(3) The paper considers the trade-off between convergence and stability of AID based algorithms, but only gives the cases of constant learning rate and diminishing learning rate, without discussing the impact of decaying learning rate at different rates to choose better learning rate."
            },
            "questions": {
                "value": "(1) The paper has discussed the relation between generalization error and learning rate, could other critical factors, such as batch size, impact the generation capabilities of the proposed AID algorithms?\n\n(2) Might it be feasible to incorporate more challenging datasets and tasks in order to validate the theoretical analysis, such as CIFAR?\n\n(3) Will there be more significant challenges in extending the conclusions of this paper to AID-based bi-level optimization variants?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7010/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698677217827,
        "cdate": 1698677217827,
        "tmdate": 1699636821364,
        "mdate": 1699636821364,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ASSF2urHoB",
        "forum": "9vZ8UjP2Mz",
        "replyto": "9vZ8UjP2Mz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7010/Reviewer_R6J2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7010/Reviewer_R6J2"
        ],
        "content": {
            "summary": {
                "value": "The authors investigate the stability of a specific double-loop AID-based algorithm for bilevel optimization. They demonstrate that, under specific conditions, this algorithm achieves $O(T^q/n)$-stable, which is of a similar order as ITD-based methods. The authors also conduct a convergence analysis under specific conditions of stepsizes. By combining the stability and convergence results, they determine the generalization ability of the proposed algorithm. Their experimental findings show that the generalization ability for diminishing learning rates outperforms the generalization ability for constant rates."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "S1. The exploration of stability and generalization in bi-level optimization is an under-explored area, and the topic addressed in this work is both interesting and significant. \n\nS2. A novel analytical framework for examining the stability of bilevel optimization has been developed.\n\nS3. The paper is easy to follow."
            },
            "weaknesses": {
                "value": "W1. I didn't examine all the proofs of the main results in depth, but I did identify some errors in certain proofs, including the proof of Theorem 1 (specifically, details in Q3 and minor comments below). It would be advisable for the authors to review and validate all their results for accuracy.\n\nW2. The paper doesn't adequately cover closely related papers on AID-based bi-level optimization algorithms, including:\n\n[1] K. Ji, J. Yang, and Y. Liang. ``Bilevel Optimization: Convergence Analysis and Enhanced Design.\u201d ICML 2021.\n\n[2] M. Dagreou et al. ``A framework for bilevel optimization that enables stochastic and global variance reduction algorithms.\u201d NeurIPS 2022.\n\nW3. The convergence analysis in this paper lacks novelty, as the approach presented in Chen et al. (2022) can be readily extended from constant learning rates to time-evolving learning rates in a standard manner.\n\nW4. There are numerous unnecessary typos, details in Minor Comments below."
            },
            "questions": {
                "value": "Q1. Why do the authors focus on the specific double-loop AID-based bi-level optimization algorithm in Algorithm 1? There are several single-loop AID-based bi-level optimization algorithms available, as demonstrated in Dagr\u00e9ou et al.'s work ``A framework for bilevel optimization that enables stochastic and global variance reduction algorithms\" in NeurIPS (2022), as well as the related references.\n\nQ2. Why are the solutions of bi-level optimization problems in Section 3.2 unique, considering the definitions of $(\\bar{x}, \\bar{y})$ and $(x^*, y^*)$? Note that Remark 1 mentions that the bi-level optimization problem is likely to have a nonconvex objective with respect to $x$.\n\nQ3. Why can we use the same sample for different datasets? Note that there is an internal randomness of algorithm $\\mathcal{A}$ (i.e., Algorithm 1). The proof of Theorem 1 should be more rigorous. The proof of Theorem 2.2 in Hardt et al. (2016) may provide valuable insights.\n\nMinor Comments:\n\n(1)The sample spaces for $\\xi_i$ and $\\zeta_i$ can differ; for instance, refer to the toy example in Section 5.1.\n\n(2)In Algorithm 2, where the sample $\\zeta_t^{(K+1)}$ is used?\n\n(3)There are numerous unnecessary typos, such as:\n \n(i)the long inequality of generalization error on page 4: $A$ should be $\\mathcal{A}$, $N$ should be $n$, and $\\mathbb{E}_{z, D_v}$ should be $\\mathbb{E}_{z}$ in (IV). \n\n(ii)line 2 before Proposition 2: $x_i$ should be $\\xi_i$. \n\n(iii)Proposition 2: $\\mathbb{E}_{\\mathcal{A}, D_z}$ should be $\\mathbb{E}_{\\mathcal{A}, D_v}$.\n\n(iv)Definition 2: check and correct the statement such as clarify the meaning of \n$D_v$ and $D_v\u2019$, and so on.\n\n(v)In the beginning of Section 4.5, what follows the word ``However,\u201d?\n\n(vi)After the toy example in Section 5.1, swap the positions of $A_1$ and $A_2$.\n\n(vii)Proof of Lemma 8 on Page 12: $\\frac{L_0}{\\eta_z}$ should be $L_0 \\eta_z$. There are also other unnecessary typos in the proofs on Lemma 9, Corollary 1 and elsewhere.\n\n(4)Where is the proof of Proposition 3? According to Definition 2, it's not immediately obvious, as there are different samples in $f$, and the assumptions on $f$ are made for the same sample.\n\n(5)Equation (2) in Theorem 2: $L_{\\Phi}$ is not defined before, so clarify its meaning.\n\n(6)How large is $K$ in Theorem 2? Does it have any impact on the experiments?\n\n(7)Where is the proof of Corollary 2? At the end of the proof of Corollary 3, how does $e^{\\frac{\\alpha}{\\epsilon \\gamma}}$ become $\\mathcal{O}(e^{1/\\epsilon})$? Is this statement correct?\n\n(8)Toy example in Section 5.1: Why is $(\\hat{X}, \\hat{y})$ considered the ground truth? Please note that there is an $L_2$ regularization term in the context.\n\n(9)Does the lower-level objective in Section 5.2 is strongly convex for $y$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7010/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7010/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7010/Reviewer_R6J2"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7010/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699115436726,
        "cdate": 1699115436726,
        "tmdate": 1699636821261,
        "mdate": 1699636821261,
        "license": "CC BY 4.0",
        "version": 2
    }
]