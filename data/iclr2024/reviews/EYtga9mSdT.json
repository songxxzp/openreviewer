[
    {
        "id": "9pR79bWHph",
        "forum": "EYtga9mSdT",
        "replyto": "EYtga9mSdT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5063/Reviewer_E3Wj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5063/Reviewer_E3Wj"
        ],
        "content": {
            "summary": {
                "value": "This paper pays attention to the patch-based few-shot learning methods. Motivated by the scenario of not fully leveraging the non-annotated patches in existing methods, this paper proposes to learn all the patches in an image and designs a cascaded multi-stage learning framework, including a direct patch learning strategy, a Gaussian mixup strategy as well as the self-supervised learning strategy. Experiments on multiple benchmark datasets show the effectiveness of the proposed method to some extent."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThis manuscript is well organized and easy to follow. \n2.\tThe motivation is reasonable and experiments are abundant."
            },
            "weaknesses": {
                "value": "Several main concerns are as follows:\n1. The details of the proposed framework and strategies are not clear, which makes it difficult to convince the readers. Some concerns are as below,\n   - The whole objective function to be optimized of the proposed framework is not clear.\n   - The details (e.g., architecture and total number of parameters) of the first encoder and following encoders are not clear. \n   - How to use and optimize Eq. (1) is not clear.\n   - The detail of the self-supervised learning loss used is not clear.\n   - The definitions of some notations are not clear. For example, what\u2019s the meaning of z in the Algorithm 1. Is it the same notation used in Figure 4?\n   - How to optimize the loss function used in the patch-level label?\n\n2.\tFrom the description in Table 3, this paper focuses on using self-supervised learning to address the few-shot learning problem without using any labels. However, in Figure 4, it seems that the class label is used to obtain the patch-level label. This part should be clear. In addition, the definition of Baseline in the experiments is not clear. Is it just the DINOv2?\n3.\tThe training details of this work is not clear. For example, is the proposed model trained from scratch or trained based on a pre-trained backbone with DINOv2?\n4.\tThe comparisons in Tables 1 and 2 are not strict and fair. For example, many existing FSL methods only use a resolution of 84* 84 for the training images of miniImageNet, while this work uses a much larger resolution. The authors may say they have conducted an experiment in Figure 6. However, how many epochs are used and if the multi-crop strategy is used for other competitors are not clear. \n5.\tThe review of existing works in the literature is not sufficient. For example, the representative patch-based FSL methods, such as [1-4], are not reviewed. Also, for the cascading learning, some existing FSL methods, such as [5-6], have tried the similar ideas. \n\n- [1] Revisiting Local Descriptor based Image-to-Class Measure for Few-shot Learning. CVPR 2019.\n- [2] Dense Classification and Implanting for Few-Shot Learning. CVPR 2019.\n- [3] Cross attention network for few-shot classification. NeurIPS 2019.\n- [4] Joint Distribution Matters: Deep Brownian Distance Covariance for Few-Shot Classification. CVPR 2022.\n- [5] Rethinking Few-Shot Image Classification: a Good Embedding Is All You Need? ECCV 2020.\n- [6] Self-supervised Knowledge Distillation for Few-shot Learning. BMVC 2021."
            },
            "questions": {
                "value": "Please refer to the above comments."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5063/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698583568527,
        "cdate": 1698583568527,
        "tmdate": 1699636496338,
        "mdate": 1699636496338,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VUvwuQXFDb",
        "forum": "EYtga9mSdT",
        "replyto": "EYtga9mSdT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5063/Reviewer_EnYW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5063/Reviewer_EnYW"
        ],
        "content": {
            "summary": {
                "value": "The authors present a new approach for few-shot classification based on image patches, which is based upon two main components: 1) Balanced learning of foreground features to obtain a rich representation of the \u2018main\u2019 object corresponding to the label, and 2) leveraging the remaining background information to further extract potentially helpful information. To this end, the authors introduce a method to automatically split the tokenised image into \u2018foreground\u2019 and \u2018background\u2019, followed by individual processing of both sets.  \nPotential shortcuts in the classification process of foreground objects are reduced via a balanced \u2018direct patch learning\u2019 method combined with a Gaussian MixUp Augmentation method to achieve 1), while a cascaded pipeline leveraging self-supervised learning is introduced to tackle 2). \nThe authors support their method by providing convincing results on the four major few-shot benchmarks (as well as some cross-domain results in the appendix)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "### Originality & Significance:\n-  The idea of selecting a \u2018suitable\u2019 subset representing the labelled entity and enforcing a balanced way of learning on this reduced set is original and seems intuitive; Additionally trying to leverage (potentially helpful) information from the \u2018background\u2019 area has been shown to help, but is here realized in an interesting iterative way; \n\n### Quality: \n-  Approach well motivated, experiments demonstrate convincing results on a variety of benchmarks with two different backbones; \n-  Most individual components are well motivated and underlying intuitions for their use are convincing;\n-  The presented results and especially ablation (Table 3) indicate the individual contributions of this approach\u2019s components; \n\n### Clarity:\n-  Paper is generally easy to read and follow; appropriate use of illustrations\n-  Motivation is clearly stated, both for using a balancing strategy for \u2018target\u2019 patches to avoid shortcuts, as well as for incorporating the background patches for additional information."
            },
            "weaknesses": {
                "value": "**Note**: I do think the paper is interesting and \u2018almost there\u2019, and I\u2019m happy to update my score if the authors can clarify my questions and provide some insights on these aspects of their work!\n\n---\n### Assumptions and Limitations should be clearly stated:\n-\tThe approach heavily relies on the fact that the \u2018top tokens\u2019 will actually correspond to the object of interest (\u2018foreground\u2019), which does not automatically have to be the case -> Imagine a picture of a human with a dog in the park: Both human and/or dog could be \u2018foreground\u2019, and might therefore not necessarily correspond to the label;\n-\tAdditionally, what exactly is considered \u2018foreground\u2019 and \u2018background\u2019 can highly depend on the context in Few-shot settings (i.e. other samples in the support set of the same class) -> See e.g. Hou et al., 2019, _Cross Attention Network for Few-Shot Classification_\n\n&#8594; I'd suggest to clearly state these underlying assumptions and potential limitations in their revised draft;\n\n---\n### Ablation & Justification of Token Selection:\nToken selection is performed via a concept inspired by PageRank (Sec 3.2), while using the attention matrix as probability transition matrix; \n\n-  Given the provided motivation that DINO\u2019s attention maps focus on the \u2018main foreground objects\u2019, this seems quite elaborate/complicated; Is this necessary to obtain a reasonable ranking? What would happen if simply the top-x tokens would be used? And how does the chosen \u2018threshold\u2019 of 50% affect results (e.g. vs. 75% or 30%)?\n-  The stated motivation holds for ViTs trained using DINO \u2013 however, the authors here use a CNN backbone to extract features; This is worth pointing out, and some visualisation (e.g. appendix) of how an attention matrix looks like would be interesting (i.e. does it still hold for representations from CNN-backbones and only few transformer layers, and does it differ between DINO, iBOT and DINOv2 and/or backbones?)\n-  Please see the \u2018Questions\u2019 part for further questions\n\n---\n### Ablation on GMIX: \n- Since the authors introduce this \u2018new\u2019 mix-up strategy which should generally be applicable as long as two images are available, the question arises how it compares to existing MixUp methods. Ablation is in my opinion required here to provide insight why this method should be used \u2013 and would additionally provide insights regarding its value and potentially use beyond the few-shot use case, which could significantly strengthen this contribution.\n\n---\n### Minor point: Context of related work\n\nStatements are slightly misleading in some places regarding related work, and might benefit from rewording/clarification: E.g. it is neglected that other works like \u2018Doersch et al. (CrossTransformers)\u2019 or \u2018Hiller et al.\u2019 are similarly motivated to learn richer representations by using background information (although with the slightly different motivation of using self-supervision in pre-training and/or mixing some episodes to prevent supervision collapse of the representation space); \n\n&#8594;   Note that the paper's introduction makes it sound like the use of background information hasn\u2019t really successfully been explored in the context of (patch-based) few-shot methods \u2013 while the related work section doesn\u2019t further clarify this either (and simply mentions the fact that self-supervised has been observed to outperform supervised). \n\n&#8594;  Also the contributions state \u201cprove the potential value of non-annotated parts\u201d \u2013 which is true, but still not entirely novel insight."
            },
            "questions": {
                "value": "Please also see the questions/concerns raised in the 'Weaknesses' part; \n\n---\nToken selection via a concept inspired by PageRank (Sec 3.2): \n-  I\u2019d like the authors to clarify what exactly is used when setting \u201cs=3\u201d: Is Equation 1 simply used and the Attention matrix cubed, as 's' is shown in the exponent? Or is the state refined using different attention matrices across several layers? &#8594;  Some clarification would be helpful here.\n\nUse of self-supervision: \n-  The authors state that \u201ca self-supervised method and loss\u201d is used for the cls token \u201clike DINO, iBOT and DINOv2\u201d; Note that a main difference in iBOT is that a loss is applied to the actual token sequence, not only the \u2018cls-token\u2019 \u2013 is this done here as well, or not? \n\nClassification at inference time:\n-  The authors use \u201conly the output of the first encoder\u201d, and concatenate the \u201caverage of foreground tokens and the cls-token\u201d: Is this \u2018essential\u2019, or does this simply further boost the performance? And how would only one of them perform (cls vs. avg across patches)?\n-  Also: What's the intuition behind this? Usually, we use either an average over the patch tokens (commonly when no cls-token is available) OR the cls-token itself. \n\nAdditionally introduced parameters/FLOPs:\n-  I\u2019d be curious how many additional parameters & FLOPs are introduced through this method, just to get an impression how it compares to the backbone itself; Some info regarding backbone parameter count vs. the method\u2019s total parameters would be interesting (e.g. in appendix), maybe split into training and inference time. \n\nMinor comments regarding notation & references:\n-  For sake of consistency with other (seminal) works and to avoid confusion, it might be easier to stick to the well-established \u2018N-way K-shot\u2019 notation (as introduced by Finn et al.) \u2013 instead of \u2018K-way N-shot\u2019.\n-  There exist some duplicates in your references (e.g. Snell et al., Vinyals et al.) that should be corrected."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5063/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5063/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5063/Reviewer_EnYW"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5063/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698644279121,
        "cdate": 1698644279121,
        "tmdate": 1699636496245,
        "mdate": 1699636496245,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pJkt506D8L",
        "forum": "EYtga9mSdT",
        "replyto": "EYtga9mSdT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5063/Reviewer_yR4m"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5063/Reviewer_yR4m"
        ],
        "content": {
            "summary": {
                "value": "This work proposes an end-to-end learning framework for few-shot learning that considers the whole image from a multi-level perspective. The framework includes Direct Patch Learning (DPL) and Gaussian Mixup (GMIX) for balanced learning of annotated subjects, as well as a cascading token selection strategy and self-supervised learning for utilizing knowledge from the non-annotated parts of the image. As the paper illustrates, the proposed method outperforms previous methods in inductive few-shot learning and demonstrates the value of non-annotated parts. \nAll its motivations are strongly based on the assumption that any patch of the image is beneficial to learning, and it divides the tokens from patches into two sets named top tokens and bottom tokens via a token selection method. And then supervised and self-supervised learning are separately applied to two kinds of tokens."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is well organized, tables and figures are easy to read and the results are clearly shown to some extent."
            },
            "weaknesses": {
                "value": "- The motivation of this work contradicts the findings of a previous paper [1] which shows that image background is harmful to few-shot learning with evidence found by well-designed experiments. Unless the authors prove that the results in [1] are wrong, the motivation of this paper is questionable.\n- The techniques taken in this paper are irrelevant to the **few-shot** nature of few-shot learning problems. That is to say, I see nothing not suitable for the proposed methods to be applied to general visual representation learning and classification problems. Thus we cannot learn anything from this paper that can improve our understanding of few-shot learning problems.\n- Self-supervised pretraining has proven to be extremely helpful for few-shot learning [2, 3]. As most of the other methods compared in this paper do not use self-supervised pretraining, the comparisons are not fair. \n- The resolution of ResNet-12 (or 18?) used in the paper is much higher than previous paper, making comparisons unfair.\n- The DINO baseline in this paper is significantly worse than reported in previous paper [3], which shows that DINO has a very high performance on few-shot learning, even approaching the visual encoder of CLIP. This raises the suspicion that all experimental results in this paper may be unreliable.\n\n[1] Rectifying the shortcut learning of background for few-shot learning. NeurIPS 2021.\n\n[2] Self-Supervision Can Be a Good Few-Shot Learner. ECCV 2022.\n\n[3] A Closer Look at Few-shot Classification Again. ICML 2023."
            },
            "questions": {
                "value": "- What backbone of the ResNet family are you using? ResNet-12 or ResNet-18? They are said in two different places.\n- Why use the Markov chain when doing token selection? Actually, the average attention score of cls token is not a strict probability distribution, so why do you insist on using the Markov chain for $\\pi_s$?\n- In section 3.1, $D_{val}$ data split seems to be ignored when you design the framework and the authors do not state how to determine the hyperparameters.\n- Equation (7) is confusing. Do you mean $L^{n^{th}} = L^{{n-1}^{th}}+L_{DPL}+\\alpha \\times L_{ssl}^{ n^{th}}$?\n- In section 4.1, tieredImageNet has 608 classes, not 600. In Figure 4, $w_2$, not $w_x$.\n- I cannot evaluate the DPL\u2019s effectiveness owing to an unclear DINO V2 SSL baseline. Why do you choose a dino v2 as the baseline in Table 3 to verify your component effectiveness? Dino-v2 is not comparable because of its dissimilarity with your model.\n- Figure 6 shows DeepEmd and feat have lower performance when image resolution is increasing but it is puzzling. Is it because the backbones used for them are still ResNet, not ViT? Or is it because they do not use self-supervised pretraining? What will the results be like if they are equipped with these components?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5063/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5063/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5063/Reviewer_yR4m"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5063/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698644306848,
        "cdate": 1698644306848,
        "tmdate": 1699636496139,
        "mdate": 1699636496139,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xpIyE5b1nW",
        "forum": "EYtga9mSdT",
        "replyto": "EYtga9mSdT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5063/Reviewer_HMyX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5063/Reviewer_HMyX"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the author introduces a few-shot classification framework designed to address the limitations of the current patch-wise learning approach. The author presents the \"Gaussian-mixup\" and \"direct patch learning\" modules to balance diverse foreground features. Concurrently, a token selection strategy is introduced to extract more information from previously overlooked background details. Ultimately, the proposed method demonstrates a significant improvement over baseline approaches."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- For Table 3, the improvement of DPL module over baseline is significant;\n- The idea of the paper is simple and effective, and overall speaking, it's technically sound."
            },
            "weaknesses": {
                "value": "- The sections covering literature review and introduction seem to lack depth in terms of providing sufficient background on patch learning methods. A more comprehensive overview of these methods would aid the reader's understanding and set the context for your subsequent discussions. The paper mentions the process of learning a model using image-level labels for few-shot classification, especially as highlighted in Sec 3.1 \"Definition\". There appears to be a gap in explaining the transition from image-level to object-level  information utilization for classification. While one can deduce that certain methods might automatically discern important patch tokens from the image and utilize these tokens as feature representation for the ultimate classification loss, this inference should not be left to the reader's assumption. It would enhance the clarity of the paper to provide this background explicitly. \n- The subsections on Self-Supervised Learning (SSL) for FSL and cascading learning appear to be cursory. To aid the reader's comprehension and to offer a robust foundation, it's crucial to delve deeper into these areas.\n- For Figure 2, the distinction between dashed and solid lines in Figure 2 is unclear. The notations f^{(t)} and f^{(s)} are introduced in the figure but lack an accompanying explanation. The current caption for Figure 2 seems to fall short of giving readers a full understanding of the depicted pipeline. In summary, while Figure 2 appears to be central to the paper's methodology, its current presentation lacks clarity in some aspects. \n- Your token selection strategy brings forth the concern of potential noise or irrelevant information being chosen. In methods aiming to identify significant features or tokens, there's always an inherent risk of inadvertently selecting non-representative or misleading tokens.  E.g.,in  Figure 1 Row 2, Using the given example where the class label is \"chimpanzee\", the inclusion of the human boy in the selected tokens could indeed pose a problem. This selection can mislead the classifier, making it associate features of the human boy with the chimpanzee class. It's essential to clarify how your proposed method deals with such scenarios. Does the method have a mechanism to filter out or down-weight such irrelevant information?\n\nOverall speaking, I have several confusion and concerns of the paper in current stage."
            },
            "questions": {
                "value": "See weakness part"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N.A."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5063/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698682447485,
        "cdate": 1698682447485,
        "tmdate": 1699636496008,
        "mdate": 1699636496008,
        "license": "CC BY 4.0",
        "version": 2
    }
]