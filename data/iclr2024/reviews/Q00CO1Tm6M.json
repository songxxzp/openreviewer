[
    {
        "id": "GL1Fy9U9Qq",
        "forum": "Q00CO1Tm6M",
        "replyto": "Q00CO1Tm6M",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5723/Reviewer_VorK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5723/Reviewer_VorK"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the partially observable reinforcement learning with certain feedback structure (partial online state information, POSI). Given that POSI is not sufficient for statistical learnability, the authors identify two learnable problem classes of interest with POSI and extra structural assumptions (independent sub-states / revealing condition) by establishing regret upper bounds and lower bounds."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The hardness result for POSI (Theorem 1) uses a novel and interesting construction based on combinatorics lock. The clever construction demonstrates that partially observable RL is still hard even the agent can actively query (part of) the latent state information, which is itself a meaningful message.\n\n- The motivation of considering class 1 & 2 is very clear, as the authors provide various empirical works where the structure of class 1 / 2 naturally arises.\n\n- Algorithm 1 adopts a novel strategy for determining the substates to be queried based on exponential weight update, which seems promising."
            },
            "weaknesses": {
                "value": "The two problem classes are studied in a case-by-case manner (both algorithms and analysis are very different). It would be a natural question whether there is a connection between these two. Such a connection is important because it will provide a more unified understanding of partially observable RL with POSI. In this paper, there lacks an investigation (or at least a discussion) on this connection (either positive or negative)."
            },
            "questions": {
                "value": "1. For problem class 1, the structure of the optimal policy is not very clear. If my understanding is correct, the guarantee of algorithm 1 implicitly implies that there is an optimal policy that proceed as follows: for each step h, sample a substate i from a fixed distribution p, query i and take an action according to \\phi_i(s_h). In my opinion, this result is non-trivial, and stating this result can help the reader better understand the motivation of Algorithm 1.\n\n2. For theorem 3, the authors highlight that the upper bound is \"decreases exponentially as $\\tilde{d}$ increases\". I am a little bit confused of this point, because there is a possibility that such a decreasing upper bound is an artifact of the algorithm design or the analysis. More concretely, it is possible that the minimax optimal regret actually scales with $|\\tilde{\\mathbb{S}}|^d$ (as the lower bound), which does not decrease as $\\tilde{d}$ increases. There also lacks a discussion of the revealing condition $\\alpha$ which only appears in the upper bound side, yet it is known that for revealing POMDP the minimax optimal regret actually scales with $\\alpha^{-1}$."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5723/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5723/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5723/Reviewer_VorK"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5723/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698723515071,
        "cdate": 1698723515071,
        "tmdate": 1699636599106,
        "mdate": 1699636599106,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HvznO9JtQd",
        "forum": "Q00CO1Tm6M",
        "replyto": "Q00CO1Tm6M",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5723/Reviewer_oftj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5723/Reviewer_oftj"
        ],
        "content": {
            "summary": {
                "value": "This paper delves into the realm of partially observable reinforcement learning. The authors initially establish that achieving a near-optimal policy solution for POMDPs necessitates an exponentially increasing sample complexity unless full online state information (OSI) is readily available. Moreover, the authors demonstrate that POMDPs with only partial OSI become tractable when certain additional assumptions are made, such as independent sub-states or the revealing assumption."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The domain of partially observable reinforcement learning holds significant interest for the RL theory community, with the identification of new tractable POMDPs being of paramount importance. This paper aims to present a comprehensive set of results for POMDPs with online state information."
            },
            "weaknesses": {
                "value": "One of the principal issues with this paper is its writing quality, marred by numerous typographical errors that impede comprehension. Some concerns and questions are outlined in the questions part.\n\n\nWhile I can grasp the main storyline and contributions of this work to some extent, its subpar writing hinders my ability to follow and validate its accuracy. Therefore, I firmly believe that this paper requires a comprehensive revision to enhance its clarity and coherence."
            },
            "questions": {
                "value": "- The reward function $r_h$ remains independent of the episode index $k$,\" but the reward $r_h^k$ is frequently referenced (e.g., Figure 1, Section 2.1, Section 2.2...).\n\n- In Section 2.3, it is unclear whether the authors intend to use $\\hat{\\Phi}_h$ or $\\hat{\\Phi}_h^k$.\"\n\n- Regarding the definition of $V^{\\pi^k}$, the expectation is seemingly taken with respect to $\\pi_q^k$ rather than $\\pi_p^k$. The authors should also provide clarity that $\\pi^k = (\\pi_q^k, \\pi_a^k)$.\n\n- The definition of regret raises questions. Why do the authors employ $Reg^{\\pi}(K)$ instead of $Reg(K)?$ If the authors aim to emphasize the dependency on the executed policies, it should be expressed as $Reg^{\\pi^{1:K}}(K).$\n\n- The statement of Theorem 1 appears non-rigorous as it lacks a precise definition of \"with only partial online state information.\"\n\n- The definition of $\\mathbb{P}\\_h^k$ in (3) is unclear. The counter $\\mathcal{N}^k$ also seems to depend on $h.$ The definition of $\\mathcal{N}^k(\\phi_{\\hat{i}}(s), a, \\phi_{\\hat{i}}(s'))$ should be provided;  and the $\\mathbb{P}\\_h^k$ should be replaced by $\\mathbb{P}\\_h^k(\\phi_{\\hat{i}}(s') | \\phi_{\\hat{i}}(s), a)$? \n\n\n\n- The proposed algorithms appear to be a fusion of the strategic querying mechanism and existing methods, which include value iteration and OMLE. It's evident that the strategic querying mechanism stands out as the primary novelty in this work. However, the paper lacks a detailed and thorough elaboration of this mechanism. For instance, deferring the definition of $\\hat{r}_h^k$ to the appendix without accompanying explanation seems to be a suboptimal choice, as it hinders a clear understanding of this critical component. Providing more clarity and detail on this novel approach would greatly enhance the paper's readability.\n\n\n- Regarding Theorem 6: Why can't the cases of $\\tilde{d} = 1$ and $\\tilde{d} \\ge 2$ be unified? Why is Theorem 6 unable to encompass the results for the case of $\\tilde{d}=1$\"? Perhaps presenting the general case for $\\tilde{d}$ would better illustrate the contributions of this paper.\n\n- Another minor comment: it would be much more interesting if the lower bound in Theorem 4 depends on $\\tilde{d}$.\n\nThere are a few issues I identified in the main paper. I hope the authors will thoroughly review the paper (including the Appendix) and make comprehensive revisions, aiming to make the paper easy to follow."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5723/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5723/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5723/Reviewer_oftj"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5723/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698768200874,
        "cdate": 1698768200874,
        "tmdate": 1700711388381,
        "mdate": 1700711388381,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5z0YLMIGGa",
        "forum": "Q00CO1Tm6M",
        "replyto": "Q00CO1Tm6M",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5723/Reviewer_yDo3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5723/Reviewer_yDo3"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on the benefits of online state information (OSI) in the learning of POMDPs. Previous work showed that full OSI in hindsight helps accelerate the exploration of unknown POMDPs. The main problem studied in the paper is whether partial OSI is beneficial for POMDPs, and to what extend are the benefits if the answer is yes. The partial OSI setting means the agent can actively query part of the state at each step. For the negative results, the paper shows that partial OSI cannot prevent exponential sample complexity in general by constructing a hard family of POMDPs. For positive results, it identifies two subclasses of partial OSI POMDPs that is tractable and provides sample efficient algorithms to learn the optimal policy with $\\sqrt{K}$ regret ($K$ is the number of episodes)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The setting studied in the paper is common in some real-world problems. The paper also provides some motivating examples for the partial OSI model.\n\n2. The techniques to develop the lower bound instances and the algorithmic design of pessimistic-optimistic exploration strategy for the query policy and execution policy are appreciated."
            },
            "weaknesses": {
                "value": "1. The exact setting studied in the paper seems to be inconsistent with the motivation of introducing OSI in POMDPs. As in previous work, the OSI in the hindsight is common some applications, such as data center scheduling, where the state is revealed after the action is take (so the effect of the action can be observed), However, the query model in this paper allows the agent to query part of the state before the action is taken. Even the motivating examples (e.g., the autonomous driving) seems to advocate the former setup, which reveals the state after taking actions. Although the conclusions of the paper may still hold in the case that the query happens after taking actions, it is appreciated to discuss about the model that is better aligned with the real world. \n\n2. Some of the claims and proofs in the paper are not clear enough, leaving potential risks for the soundness of the paper. In the definition of tractable class 1, the transition kernel can be factored as the multiplication of $d$ different subtransition according to each substate. Algorithm 3 does not clearly show how to estimate those $d$ subtransitions, instead it looks like the subtransitions are the same for each $i \\in [d]$, which enables us to estimate this transition using all the adjacent revealed substates within one episode no matter whether the adjacent steps choose the same dimension of substate or not. This is not achievable when the subtransition for each dimension is different (so the final regret bound would be worse). Another subtle point is the proof of Lemma 1 and Lemma 5. It is not clear with what the expectation in Eqn. 40, Eqn. 42, and Eqn. 43 is taken. It seems that the current definition of the expectation does not make sense for proving the regret bound, but I believe this problem can be fixed and the conclusion is still correct. Due to this caveats in the paper, I cannot ensure the correctness of the conclusions. I strongly recommend the authors to revise the statement and proofs in the paper."
            },
            "questions": {
                "value": "The tractable class 1 is similar to the factored MDPs, where the transition (and reward) of a large MDP can be factored into several subtransitions with regards to different part of the state. The optimal regret of factored MDPs only has square root dependency on the size of factored state space and action space. Can you provide some comparisons with tractable class 1 with factored MDPs?\n\nTypo: \nSection 2.3 Line 3: $\\Phi_h \\to \\hat{\\Phi}_h$"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5723/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5723/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5723/Reviewer_yDo3"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5723/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698789938504,
        "cdate": 1698789938504,
        "tmdate": 1700698880411,
        "mdate": 1700698880411,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MyoNDpWHbk",
        "forum": "Q00CO1Tm6M",
        "replyto": "Q00CO1Tm6M",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5723/Reviewer_Z1AA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5723/Reviewer_Z1AA"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the theoretical hardness and tractability of partially observable Markov decision processes (POMDPs) when only partial online state information (OSI) is available. The setting is different from the standard POMDP case, and also different from the recent work on POMDP with hindsight state information. The authors also provide motivating exampling to justify the proposed framework.\n\nThe authors then establish a lower bound that shows the exponential scaling of sample complexity is required to find an optimal policy solution for POMDPs without full OSI in general. However, they also identify two tractable sub-classes of POMDPs with partial OS. New algorithms are also proposed to solve the identified classes."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1 POMDP is a useful framework of the interactive decision-making problems, but is also intractable in general. Therefore, identifying interesting tractable sub-class of POMDP with interesting and reasonable structures is an important problem in theoretical RL study. The topic itself is thus very relevant to the community of neurips. Meanwhile, the proposed framework introduces partial side information before the agent makes decision, which is natural in practice and supported by the practical applications, and also is a great complement to existing works.\n\n2 I find that the story is complete: the authors start with the proposed frameworks and its motivating examples, and also the connections with existing frameworks; and then a pessimistic lower bound to motivate further structural assumptions; finally, two algorithms are proposed to solve the identified tractable problems.\n\n3 The proposed algorithm 1 is distinct from the recent popular OMLE/MOPS algorithms, which are popular since GOLF and Bilin-ucb. Instead, the algorithms are more related to the classic algorithms that are crafted to exploit the observable operator structure, but with distinct new ideas to handle the partial side information. To the best my knowledge, the algorithmic designs and some of the analysis techniques are new.\n\nOverall, I feel that the authors have presented a reasonable framework of tractable POMDP problems, with a complete story, thus hitting the bar of acceptance."
            },
            "weaknesses": {
                "value": "1 The authors mention that the proposed framework can be placed under the general decision-making framework studied in [1,2]. I am wondering whether the identified problems can also be proved to be a sub-class of the tractable problems identified in these works. For instance, do the two classes of problems admit a low dec?\n\n2 I think it would be better if you could explicitly instantiate the motivating examples in section 2 with the proposed framework (e.g. explicitly write down the state mapping with the physical quantities). Also, some of the superscripts are missing in the main paper (e.g. in section 2.3). It is also suggested to provide a notation table for the readers to improve readability.\n\n\n[1] The statistical complexity of interactive decision making\n[2] Unified algorithms for rl with decision-estimation coefficients: No-regret, pac, and reward-free learning"
            },
            "questions": {
                "value": "see weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5723/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699053033601,
        "cdate": 1699053033601,
        "tmdate": 1699636598799,
        "mdate": 1699636598799,
        "license": "CC BY 4.0",
        "version": 2
    }
]