[
    {
        "id": "s8dwyyZpg9",
        "forum": "QmZKc7UZCy",
        "replyto": "QmZKc7UZCy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4730/Reviewer_ePEX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4730/Reviewer_ePEX"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel pretraining paradigm called LanguageBind, which takes the language as the ind across different modalities. To this end, authors curate a large-scale multimodal dataset. Extensive experiments for different modalities demonstrate the effectiveness of the proposed paradigm."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is clearly written and contains sufficient details and thorough descriptions of the experimental design. \n2. Extensive experiments are conducted to verify the effectiveness of the proposed method and dataset."
            },
            "weaknesses": {
                "value": "1. In table 2, while authors demonstrate the improvements over ImageBind on T2V and V2T tasks, these two models are trained with different backbones, model initializations, finetuning techniques, and training data. This leads to an unfair comparison, especially considering the proposed model is leveraging more video data. \n\n2. Based on my understanding, LanguageBind is initialized from OpenCLIP and continues to train on the VIDAL-10M dataset. Compared to OpenCLIP, it is difficult to tell whether the performance improvement comes from the proposed dataset or the new pretraining paradigm. \n\n3. In table 4, do the authors have any intuition why raw caption works best for the Infrared modality?"
            },
            "questions": {
                "value": "See the above weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4730/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4730/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4730/Reviewer_ePEX"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4730/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698806627187,
        "cdate": 1698806627187,
        "tmdate": 1699636455168,
        "mdate": 1699636455168,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3ylHEwF1ZG",
        "forum": "QmZKc7UZCy",
        "replyto": "QmZKc7UZCy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4730/Reviewer_Bowz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4730/Reviewer_Bowz"
        ],
        "content": {
            "summary": {
                "value": "The paper presents multi-modal pretraining approach with modalities N=5 (video, text, audio, depth, infrared) by using language as bind across different modalities. A frozen text encoder from a pretrained VL model is used as the feature extractor for the text modality and aligned with other modalities (pair-wise) using contrastive loss. It also introduces a dataset called VIDAL-10M with 10M data pairs from VL, DL, IL,and AL. The dataset and method is evaluated on standard retrieval benchmarks to show the effectiveness of the pretraining data as well as the technique."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1) The paper attempts to learn a unified embedding space for 5 modalities where the modalities are guided by language during pre-training. Such an embedding space can be very useful for tasks involving: i) multi-modal data for ex: video containing audio, ii) tasks where paired data is not available, for ex: Video-Infrared, Video-depth etc \n\n2) The paper introduces a dataset with 10M paired data from AL, VL, IL and DL which is important for driving research in the multimodal learning area as many of the real-world applications contain multimodal data. It follows a careful approach by leveraging existing vision and language models (OFA, mPLUG-owl, chatgpt etc) to collect a balanced (in topic) and diverse (in semantic) data-pairs.\n\n3) The introduced dataset and the pretrained model is shown to be useful for:\na) cross-modality video retrieval task where it outperforms its counterparts (ImageBind, CLIP-straight, CLIP4clip).\nb) AL, DL, IL zero-shot classification tasks.\nThis shows that the model has learned good representations in the joint embedding space."
            },
            "weaknesses": {
                "value": "1) It is not clear from the text or Table2, the size of the pretraining data used for MSR-VTT and MSVD datasets. For a fair comparison, all methods should be pretrained with same amount of data but here CLIP-Straight is trained with WIT400M only (initialized from CLIP but no fine-tuning), CLIP4clip is trained with WIT400M+HT100M-380k whereas the proposed technique (although CLIP4clip technique is used) is pretrained with WIT400M+VIDAL-10M. It would be a fair comparison if all methods use similar sized data, i.e what would be the performance of other technique like CLIP4clip if additional data (not VIDAL-10M) is used for training.\n\n2) One of the goals of learning a model from multimodal data is  that the data can use all available modalities to learn stronger representations but there are no experiments to demonstrate this, for ex: instead of using just video -> text retrieval, it would interesting to show that video+audio -> text retrieval has better performance. \n\n3) There are few other advantages of multimodal learning in situations where:\na) one of the modalities is corrupted \nb) one of the modalities has some weaknesses (videos taken in the dark, OR audio from multiple sources) \nc) one of the modality undergoes a domain change while the other doesn't (eg: videos under weather changes etc) \nbut none of these has been addressed in this paper. It would be interesting to see results on at least one of the above scenarios.\n\n4) It would also be interesting to see an experiment where the model is evaluated on retrieval task where the modalities doesn't contain text. For ex (video<->audio, video<->infrared). This will evaluate the quality of learned representations."
            },
            "questions": {
                "value": "I would like authors to discuss all the points described above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4730/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699292484551,
        "cdate": 1699292484551,
        "tmdate": 1699636455005,
        "mdate": 1699636455005,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lVKAdlkTuo",
        "forum": "QmZKc7UZCy",
        "replyto": "QmZKc7UZCy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4730/Reviewer_ZKDg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4730/Reviewer_ZKDg"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes LanguageBind, a method for training encoders of multiple modalities in a joint embedding space by aligning them to a frozen text encoder. Additionally, the authors introduce VIDAL-10M, a multimodal dataset that includes data for 4 modalities with paired textual descriptions. The authors rely on multiple third-party tools for creating VIDAL like OFA, mPLUG-Owl, and ChatGPT, in addition to modality-specific generation modules to collect data for the infrared and depth modalities. Various techniques are utilized to train LanguageBind like LoRA tuning, masking, and initializing from pre-trained CLIP checkpoints. The authors provide zero-shot retrieval and recognition experiments to showcase the effectiveness of their method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The VIDAL dataset is potentially interesting. In particular the utilization of multiple captioning models to enhance the textual descriptions for spatial and temporal information (OFA and mPLUG) as well as ChatGPT for refining the descriptions. This is further confirmed by the results of the video modality in Table.4\n2. Overall, the results reported by the authors for the different modalities+text benchmarks are strong and reflect a good performance of the model."
            },
            "weaknesses": {
                "value": "1. The authors try to draw parallels to ImageBind (method name, comparisons, frequent mentions in the abstract and throughout the paper). However, LanguageBind is much closer to standard CLIP training where a joint encoder is trained between textual descriptions and sensory data for a certain modality. There have been various examples of such methods ever since CLIP was introduced for image-text pre-training such as AudioCLIP, PointCLIP, VideoCLIP (only to mention a few). This is very important because the paper only includes evaluations testing the performance of each modality and text which is different than ImageBind's proposal of testing alignment that emerged indirectly by training the modalities jointly. For LanguageBind, the benefit of training all modalities in a joint embedding space is not showcased. \n2. The technical contributions of the paper are weak in terms of novelty and potential interest to the wider research community. The method is more a bag of well-established tricks (e.g. masking from FLIP, LoRA tuning, fine-tuning openCLIP checkpoints)\n3. While the VIDAL dataset is potentially interesting, the fact that all modalities other than Video and Audio are automatically generated by off-the-shelf models is concerning in terms of its long-term impact. (the dataset will likely be outdated once higher fidelity generation models are developed)."
            },
            "questions": {
                "value": "- The paper only includes LoRA results. Why are not there any full fine-tuning results given the authors collected a decent-sized dataset (10M across modalities) ?\n- Similar to the previous point, what happens if the modality encoders are not initialized with openCLIP checkpoints similar to ImageBind where only the text and image encoders are pre-trained ?\n- In the ablation, the performance drops when moving from 0.5 -> 0.3 masking which is counter-intuitive. What is the authors' explanation? What happens with no masking is used?\n- As stated above, what is the value of training all modalities in a joint embedding space if all use cases have to do with text only?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4730/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4730/Reviewer_ZKDg",
                    "ICLR.cc/2024/Conference/Submission4730/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4730/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699476430645,
        "cdate": 1699476430645,
        "tmdate": 1700572447226,
        "mdate": 1700572447226,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vHedvDpdIA",
        "forum": "QmZKc7UZCy",
        "replyto": "QmZKc7UZCy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4730/Reviewer_fQRK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4730/Reviewer_fQRK"
        ],
        "content": {
            "summary": {
                "value": "The authors introduce LanguageBind, an alternative approach to ImageBind where language is the primary modality that all other modalities are aligned to (instead of images). They also introduce a new dataset called VIDAL-10M that contains language-aligned data for visual, infrared, depth, and audio modalities. LanguageBind achieves state-of-the-art zero-shot classification on various infrared, depth, and audio benchmarks, as well as zero-shot video-text retrieval. Finally, they analyze the impact of scaling their dataset size on MSR-VTT R@1 and provide some training ablations that measure changes in zero-shot classification on NYU-D as a function of training epochs, batch size, LoRA rank, loss temperature, and masking ratio."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* While ImageBind was able to perform zero-shot classification by pairing its text encoder with other modalities, even if they hadn't been observed together during training, LanguageBind takes an alternative approach by obtaining the text-aligned data via synthesizing the rarer modalities from visual information in their collected text-paired data, and then training the model to align each modality separately to text. This is a subtle but interesting distinction. \n* The authors describe their data collection pipeline and state they will release upon publication, which would be valuable for the broader community. \n* The authors overcome the lack of pair infrared/depth data with other modalities by utilizing pretrained generative models for synthesizing a large-scale dataset is an interesting research direction that is currently gaining popularity. The VIDAL-10M dataset could be a fruitful playground for future research on scaling synthetic data generation. \n* The scaling curves presented in Figure 5 are promising. They suggest one could continue scaling the techniques in this paper to continue advancing the state-of-the-art. \n* Typically, models like CLIP are trained with the goal of producing a strong image encoder. It is interesting that LanguageBind is able to achieve competitive results by aligning to the text encoder instead."
            },
            "weaknesses": {
                "value": "* __Lack of ablations__: The authors only provide a limited set of ablations for a single modality (depth) on a single dataset (NYU-D). It is not clear whether the ablated decisions would impact other datasets or modalities. This is especially true because of the results in Table 4, which suggest each modality and dataset responds differently to the kinds of text annotations used, as stated by the authors. Providing a small amount of ablations on just one of these combinations makes the paper seem incomplete. Furthermore, since this paper is explicitly comparing to ImageBind, which provides extensive ablations, this paper would be much more convincing with a broader set of ablations to match. \n* __Model release__: it is unclear whether the authors intend to release their models, which is a bit unexpected since they state they will release the dataset, and the model weights should be fairly small since they are mostly LoRA modules applied to OpenCLIP.\n\n**Update**: the authors have incorporated more ablations in the rebuttal, along with a statement on model release, that adequately address my concerns. I have increased my score on \"soundness\" to \"good\" and my overall rating to \"accept, good paper\" to reflect this."
            },
            "questions": {
                "value": "### Audio Processing\n\nI don't understand how the authors are processing the audio. \n\nThey state \"For example, a 4-second spectrogram would be repeated twice and then padded with zero for an additional 2 seconds.\" Why isn't a 4-second spectrogram simply padded with zero for the remaining 6 seconds? ImageBind does not repeat spectrograms. From ImageBind Appendix B.1: \"For audio, we process each raw audio waveform by sampling it at 16KHz followed by extracting a log mel spectrogram with 128 frequency bins using a 25ms Ham- ming window with hop length of 10ms. Hence, for a t second audio we get a 128 \u00d7100t dimensional input.\" \n\nI'm also confused about this sentence: \"If the duration exceeds 10 seconds, we randomly sample three 10-second audio segments, each from the front 1/3, middle 1/3, and back 1/3 of the original audio, and finally stack them together.\". What is being stacked along what dimension exactly? \n\n### VIDAL-10M\n\n* How are the multi-view text annotations used during training? Randomly sampled at each step? There's also ambiguous wording later on like in section 6.1: \"allowing for flexibility in selecting an appropriate text source that caters to diverse task requirements.\" How are the authors selecting \"an appropriate text source\" during training?  \n* There already exist short-video datasets like WebVid-10M, as the authors mention. Why not just use the sRGB-TIR/GLPN models, as well as OFA/mPLUG-Owl on those existing datasets instead of constructing this new one? At first, I thought the motivation for VIDAL-10M was to obtain a multimodal dataset with more modalities than existing datasets, but the \"new\" modalities (infrared, depth) are just generated by these models. Not clear why you need to collect the audio/video in the first place if that's the case. WebVid videos have an average duration of 18 seconds, which seems similar to VIDAL-10M. Perhaps I'm missing some of the details here, but if so it might be beneficial to highlight these differences more clearly.  \n\n### Miscellaneous \n\n* Section 3.2: this does not say what pooling method is used to go from text logics of length L to single normalized text vector. Typically this is done with either CLS token pooling, max pooling, or mean pooling, but the authors do not mention that here. \n* Table 3: ImageBind uses OpenCLIP. How are their numbers on LLVIP (63.4) worse than the reported OpenCLIP numbers in this table (82.2)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4730/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4730/Reviewer_fQRK",
                    "ICLR.cc/2024/Conference/Submission4730/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4730/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699494340661,
        "cdate": 1699494340661,
        "tmdate": 1700495704624,
        "mdate": 1700495704624,
        "license": "CC BY 4.0",
        "version": 2
    }
]