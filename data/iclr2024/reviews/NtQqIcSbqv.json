[
    {
        "id": "HFWqNnJFH1",
        "forum": "NtQqIcSbqv",
        "replyto": "NtQqIcSbqv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2955/Reviewer_RFKG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2955/Reviewer_RFKG"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the problem of learning representations for modeling visual and tactile sensor data. A large multi-modal visual-tactile dataset is presented, and a straightforward pipeline is proposed for learning the data. Experiments are performed on cross-modal prediction tasks to validate the idea."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper introduces a substantial dataset comprising paired visual and tactile sensor data, which holds the potential for significant advancements in cross-modal research; \n\nThe paper's organization is clear and easy to follow."
            },
            "weaknesses": {
                "value": "The proposed approach has been exclusively assessed in the context of cross-modal prediction tasks, with no concrete verification of its applicability in downstream manipulation tasks; \n\nMoreover, it is worth noting that a single video observation could potentially correspond to a wide range of tactile signals, such as variations in the force applied when touching dough. Regrettably, the study does not appear to account for the inherent multimodality in the distribution of data in this respect; \n\nThe paper lacks technical details, e.g., the learning rate, batch size, etc. \n\nThe video prediction results for new instances and new categories seem not promising."
            },
            "questions": {
                "value": "Providing additional technical details would enhance the reproducibility of the work, e.g., model architecture, training details, etc. \n\nIt would be interesting to see how the learned representations can be applied to downstream manipulation tasks, adding such results would further strengthen the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2955/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2955/Reviewer_RFKG",
                    "ICLR.cc/2024/Conference/Submission2955/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2955/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698030685247,
        "cdate": 1698030685247,
        "tmdate": 1700803696435,
        "mdate": 1700803696435,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "da5r30XzoL",
        "forum": "NtQqIcSbqv",
        "replyto": "NtQqIcSbqv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2955/Reviewer_h9Bd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2955/Reviewer_h9Bd"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on understanding everyday objects and tools from a manipulation standpoint. The authors have constructed a multi-modal visual-tactile dataset, consisting of paired full-hand force pressure maps and manipulation videos. Additionally, they introduce a unique method to learn a cross-modal latent manifold. This manifold facilitates cross-modal prediction and uncovers latent structures in various data modalities. The extensive experiments establish the efficacy of the proposed approach approach."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper tackles a common issue, manifold learning, through a pragmatic lens within robotics applications. The study aims to address multi-modal learning problems in the visual-tactile sensory observation context, a highly practical setup for manipulation tasks. The proposed representation learning method can be beneficial for a multitude of downstream applications within visuotactile learning in robotics.\n\n2. The method proposed in the paper is straightforward, suggesting that it does not place a heavy computational load on the system.\n\n3. The authors have gathered a substantial paired dataset for visual and tactile signals. If made publicly accessible, this dataset could prove to be a valuable resource for further research."
            },
            "weaknesses": {
                "value": "1. The method operates under the assumption that the sum of shape and tactile information equates to visual information. This assumption is manifested in the authors' approach of creating video latents by combining the latents of manipulation and the latents of canonical shapes. However, the tactile sequence may also encapsulate the object's geometric information. As suggested in the referenced paper 'Learning human\u2013environment interactions using conformal tactile textiles,' tactile information can be employed to classify object geometry. Consequently, it's worth questioning the efficacy of combining shape and tactile embeddings to produce the video embedding.\n\n2. The cross-modality query necessitates an optimization process. Therefore, it's crucial to provide information regarding the time cost of these experiments. For instance, how much time would be required to employ the neural field in this inverse manner?\n\n3. The absence of videos in the paper is a notable limitation. Including video content could significantly enhance the understanding of the tasks and experiments conducted in the study."
            },
            "questions": {
                "value": "1. Do you want to claim the dataset as one of the contribution? In another word, would you open source the dataset once the paper is accepted?\n\n2. Could you clarify the symbol $\\gamma$ used in Equation 2? I was unable to locate a definition for it within the text.\n\n3. Your elaboration on $I_i$ and $I_j$ would be appreciated, specifically in relation to the following sentence. How is the distance within the space of $I$ quantified, and how is the subtraction operation defined in Eq3 for $I_i$ and $I_j$, given that $I$ is a three-modality tuple? While I recognize that the manifold isometry loss is a standard loss in the manifold learning field, I would like to confirm if the subtraction operation is a simple reduction operation in the raw data format.\n> any two samples sampled from the signal agnostic manifold $\\{z_i, z_j \\} \u2286 M$ respects the distance between the samples $I_i, I_j$ \n\nMinor issues:\n1. There appears to be a typographical error in the first line of the 'Dataset: Data Collection Setup' section \u2013 the citation parentheses are empty.\n2. It would enhance clarity if Figure 1 was referenced in Section 4.2 and if further details about the components in the figure were provided. This issue is also applicable to Figure 2.\n3. Please use last names when citing authors. For instance, it should be 'Chen et al.' instead of 'Peter et al.'."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2955/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2955/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2955/Reviewer_h9Bd"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2955/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698101356302,
        "cdate": 1698101356302,
        "tmdate": 1699636239431,
        "mdate": 1699636239431,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iC17Qng9Kd",
        "forum": "NtQqIcSbqv",
        "replyto": "NtQqIcSbqv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2955/Reviewer_u4UQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2955/Reviewer_u4UQ"
        ],
        "content": {
            "summary": {
                "value": "This paper purposed a force maps and RGB paird visual-tactile dataset. And further purpose to first represent each signal in a shared latent space, and then project the global manifold to local submanifold for each signal for reconstruction. The results demonstrate the effectiveness of purposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper tactles a more challenging visual-tactile prediction task which is harder compare to previous works.\n2. The design of preoject global manifold to local submanifold force the model to capture different signals, and to futher incorporate with the test time optimization method to improve the prediction results.\n3. The experiments are comprehensive.The TSNE results also show the model learned with some semantic meaningful infomation."
            },
            "weaknesses": {
                "value": "1. My major concern is since the training set : testing set is aroud 12:1, and only include 4 categoreis, can this method really generalize to unseen objects? How is the diversity of the training and testing set?\n2. Presentation with Figure1: It would be better to also draw the process of getting shared latent space in the Figure1 for better understanding, it's quiet hard to undertand how to get a shared latent space from signals that are different dimension, could the author illustrate more about this ? Also projection layer of x seems missing in Figure 1."
            },
            "questions": {
                "value": "1. If as the author statement, force maps and RGB id many-to-many mapping, what is the advantage of using force maps as tactile signal?  Why different object will not have similar surface texture property? And the challenge of disparity in spatial scale of different signals seems also exist even if it is one-one mapping?\n2. How is the robustness of the tactile glove, will it need to calibrate a lot to make sure the tactile data is accurate?\n3. How long will it take for test time optimization? Since this might be important for robotic application?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics concerns."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2955/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2955/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2955/Reviewer_u4UQ"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2955/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698568428996,
        "cdate": 1698568428996,
        "tmdate": 1700655760727,
        "mdate": 1700655760727,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WXH0WIXrAU",
        "forum": "NtQqIcSbqv",
        "replyto": "NtQqIcSbqv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2955/Reviewer_ksmF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2955/Reviewer_ksmF"
        ],
        "content": {
            "summary": {
                "value": "The authors have curated a unique visual-tactile dataset and introduced a manifold algorithm to explore the cross-modal relationship between objects and their manipulation. By visualizing the cross-modal latent structures, they showcase that their approach outperforms current methods and effectively generalizes manipulations to unfamiliar objects."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is articulately written, offering clarity and ease of comprehension, making it accessible even to readers unfamiliar with the subject matter.\n2. A significant contribution of this research is the introduction of a novel visual-tactile dataset, especially noteworthy given the limited datasets available in this domain.\n3. The innovative manifold learning approach presented has the potential to pave the way for subsequent research.\n4. Through experiments, the paper effectively showcases the promise of the cross-modal retrieval, prediction, and the latent structure. Compared to existing methodologies, the proposed approach holds considerable promise."
            },
            "weaknesses": {
                "value": "1. The dataset would benefit from enhanced visualization and in-depth details, possibly within the appendix.\n2. There's a typographical error on page 5 after equation 2; \"Additioanlly\" should be corrected to \"Additionally.\"\n3. Based on observations from figures 3 and 4, the sequences appear to have minimal variation across different frames. Displaying greater variation would add value. Additionally, considering a baseline that utilizes only the initial frame, as opposed to the entire sequence data, could provide intriguing insights."
            },
            "questions": {
                "value": "Please see the weakness above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2955/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2955/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2955/Reviewer_ksmF"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2955/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698650775658,
        "cdate": 1698650775658,
        "tmdate": 1699636239273,
        "mdate": 1699636239273,
        "license": "CC BY 4.0",
        "version": 2
    }
]