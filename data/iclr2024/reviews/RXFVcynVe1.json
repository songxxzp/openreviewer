[
    {
        "id": "cHYscOybi9",
        "forum": "RXFVcynVe1",
        "replyto": "RXFVcynVe1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8930/Reviewer_Dm45"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8930/Reviewer_Dm45"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the challenge of leveraging Large Language Models (LLMs) such as GPT to enhance the performance of Graph Neural Networks (GNNs) on Text-Attributed Graphs (TAGs). The authors propose an innovative approach called GRAPHTEXT, which combines LLMs' textual modeling abilities with GNNs' structural learning capabilities. The key innovation lies in using explanations generated by LLMs as features to boost GNN performance on downstream tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed method presents a novel and effective way to integrate the power of LLMs, like GPT, with GNNs to handle text-attributed graphs. By using LLMs to generate explanations and converting them into informative features for GNNs, it bridges the gap between textual information and graph structure, enabling more sophisticated graph reasoning.\n\n2. The experimental results demonstrate that the proposed method achieves state-of-the-art performance on well-established TAG datasets, including Cora, PubMed, ogbn-arxiv, and a newly introduced dataset, arXiv-2023. This underscores the effectiveness of the proposed approach in improving the accuracy of downstream tasks on TAGs.\n\n3.The proposed method not only enhances performance but also significantly speeds up training. It achieves a 2.88 times improvement over the closest baseline on the ogbn-arxiv dataset. This computational efficiency is crucial for practical applications and scalability."
            },
            "weaknesses": {
                "value": "1. The time analysis and money estimation are lacking. The paper utilizes the chatgpt API, nonetheless, there is a large restraint on the word limitation per day, and the cost of one dataset should also be taken into consideration. \n\n2. The robustness of the prompt is lacking. The paper proposed a single prompt for the node classification. Nonetheless, is another similar prompt can achieve a similar performance. \n\n3. There may lack of ablation study on the LM. Current LM focuses on BERT, while other LM are ignored. Recent paper demonstrates that they can achieve satisfying performance with only SentenceBert Embedding\n\n4. There lack some experimental results in table 1, especially Giant. \n\n[1] Duan, Keyu, et al. \"Simteg: A frustratingly simple approach improves textual graph learning.\" arXiv preprint arXiv:2308.02565 (2023)."
            },
            "questions": {
                "value": "See the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8930/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8930/Reviewer_Dm45",
                    "ICLR.cc/2024/Conference/Submission8930/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8930/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697761869194,
        "cdate": 1697761869194,
        "tmdate": 1700492331306,
        "mdate": 1700492331306,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bqSDLBvRGL",
        "forum": "RXFVcynVe1",
        "replyto": "RXFVcynVe1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8930/Reviewer_HEW1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8930/Reviewer_HEW1"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a framework for representation learning on text-attributed networks. The authors first prompt the LLM to obtain explanations and predictions for each node based on its textual information. Then they finetune language models on different textual attributes and obtain feature embeddings. Finally, the graph neural network adopts the learned embeddings from the second step to conduct the final prediction."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The author proposes a method to augment LM with LLM-generated features.\n2. The paper is easy to follow."
            },
            "weaknesses": {
                "value": "1. Lack of baselines. Many important baselines that conduct learning with language models on TAGs including GraphFormers [1] and Patton [2] are missing in the experimental section.\n2. The theorem is not specific to this problem. The theorem in 4.4 is not particular for LLM and LM for TAGs, and needs strong assumptions.\n3. Lack of evaluation tasks. The paper claims to do representation learning on TAGs but only evaluates the node classification task. It would be better to add experiments on other tasks such as link prediction to evaluate the quality of the representations.\n4. Lack of ablation studies. 1) If we need to finetune the LM in step 2 or not?; 2) Why do we need to have different LM for original text encoding and explanation encoding?\n5. Limit novelty. While I appreciate the introduction of the new arxiv-2023 dataset, the technique contribution of this work is very limited, which basically introduces LLM to conduct data augmentation for node classification on TAGs.\n\n[1] J. Yang, et al. GraphFormers: GNN-nested Transformers for Representation Learning on Textual Graph. NeurIPs 2021. \n\n[2] B. Jin, et al. Patton: Language Model Pretraining on Text-Rich Networks. ACL 2023."
            },
            "questions": {
                "value": "Please refer to the comments raised in the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8930/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8930/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8930/Reviewer_HEW1"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8930/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698079186890,
        "cdate": 1698079186890,
        "tmdate": 1700494533129,
        "mdate": 1700494533129,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2iJJkcKwnO",
        "forum": "RXFVcynVe1",
        "replyto": "RXFVcynVe1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8930/Reviewer_NoEV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8930/Reviewer_NoEV"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method to solve TAG problems. The method uses an LLM to generate category prediction and explanation of the target node, then the explanation is used as an enhancing part of the node feature. An LM is used to encode the raw text feature and the additional explanation into hidden space. A GNN predictor receives the hidden features, the prediction of LLM, and the shallow embeddings to give the final classification results. Experiments on several TAGs show the effectiveness of the method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is clearly written and easy to follow.\n2. Bagging LLM prediction, LM features and shallow features is reasonable for the node classification task.\n3. The experiments show the method can achieve SOTA performance on several benchmark datasets."
            },
            "weaknesses": {
                "value": "1. The main concern is that the method has little to do with graph. It seems like an application attempt of LLMs on natural language tasks, and TAG is just a scenario. Thus the contribution of the method is limited.\n2. Since the Debera need fully fine-tuning, training the method cost much more memory than pure GNN methods."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8930/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698838844252,
        "cdate": 1698838844252,
        "tmdate": 1699637124331,
        "mdate": 1699637124331,
        "license": "CC BY 4.0",
        "version": 2
    }
]