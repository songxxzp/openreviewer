[
    {
        "id": "ap7NsAOLke",
        "forum": "pXt2EP0PW1",
        "replyto": "pXt2EP0PW1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1250/Reviewer_6gnc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1250/Reviewer_6gnc"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a novel algorithm called FedFaiREE, designed to tackle fairness concerns in federated learning. Existing fairness algorithms primarily cater to centralized data environments, relying on large samples and distributional assumptions. This paper addresses the pressing need for fairness techniques tailored to decentralized systems with finite samples and distribution-free guarantees. FedFaiREE is specifically developed for distribution-free fair learning in decentralized settings with small samples. The algorithm considers the unique challenges posed by decentralized environments, including client heterogeneity, communication costs, and small sample sizes commonly encountered in practical scenarios. The paper offers rigorous theoretical guarantees for both fairness and accuracy, and the experimental results provide strong empirical validation of these theoretical claims."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The strengths of FedFaiREE are as follows:\n\n1. **Effective Fairness in Challenging Conditions:** FedFaiREE offers a simple yet highly effective approach for ensuring fairness in scenarios with limited samples and distribution-free conditions. This is particularly important in real-world applications where such conditions are prevalent.\n\n2. **Theoretical Fairness Guarantees:** The paper provides theoretical guarantees that FedFaiREE can achieve nearly optimal fairness when the input prediction function is appropriate. This adds a level of confidence in the algorithm's ability to deliver on its fairness objectives."
            },
            "weaknesses": {
                "value": "My primary concern regarding this paper centers on its contribution when compared with previous work, particularly the FaiREE algorithm designed for centralized learning. Based on my interpretation, the overall process appears quite similar, with the primary distinction being the distributed aggregation process. I would greatly value it if the authors could delve deeper into the algorithmic variances and the technical challenges associated with the algorithm's design and theoretical analysis. This deeper exploration would enhance the paper's clarity and help readers better understand the specific advancements and innovations brought about by FedFaiREE in relation to its predecessor, FaiREE."
            },
            "questions": {
                "value": "see above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1250/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697992887235,
        "cdate": 1697992887235,
        "tmdate": 1699636051492,
        "mdate": 1699636051492,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lO8PmZzfLN",
        "forum": "pXt2EP0PW1",
        "replyto": "pXt2EP0PW1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1250/Reviewer_7R45"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1250/Reviewer_7R45"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a versatile post-processing method that can be used together with other pre-processing and in-processing techniques to ensure fairness in federated learning. The method is distribution-free and only requires small samples and communication costs. Under a binary label prediction task, the authors derived theoretical guarantees for both model fairness and performance under Equality of Opportunity, after which was further extended to a distribution shift setting and an extended Equalized Odds fairness notion. Empirical experiments on Adults and Compas datasets were also carried out to demonstrate the superior performance and fairness of the proposed FedFaiREE method when compared to other baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is clearly written and has strong motivation paragraphs with well-categorized related works.\n2. The authors further study estimation and approximation methods for better adoption in practice.\n3. The theoretical results and analyses are sound. The guarantees derived are further extended to a setting with test distribution shifts and also a stronger extended notion of fairness.\n4. The method proposed is versatile since it can be used in combination with other pre-processing and in-processing techniques to ensure better fairness."
            },
            "weaknesses": {
                "value": "1. The setting of the paper may be restrictive since it only applies to tabular data with binary prediction labels. Nevertheless, simpler settings might be needed for the ease of analysis.\n2. The empirical validation of the method and performance is not comprehensive enough.\n\nSome other details are given below in the Questions section."
            },
            "questions": {
                "value": "1. Help me understand the theoretical results: How tight is the bound that is derived in (7)? If the optimal classifier is indeed unfair, how can you still achieve an optimal misclassification error with the DEOO constraint?\n2. How difficult is the extension to multiple labels? The current empirical validation section still appears not as convincing due to the simplistic setting and well-behaved binary label prediction datasets used. A larger-scale setting also supports the necessity of federated learning.\n3. Why is the same $\\alpha$ used for all experiments on a dataset (i.e., 0.1 for Adult and 0.15 for Compas)? How should this alpha be set in practice?\n4. What is the trend of accuracy (ACC) when we shrink $\\alpha$? What is a good point to stop (for $\\alpha$) in order to balance accuracy and fairness? What is the lowest value of $\\alpha$ we can go?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1250/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698396301024,
        "cdate": 1698396301024,
        "tmdate": 1699636051389,
        "mdate": 1699636051389,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Y1R1iHzyfP",
        "forum": "pXt2EP0PW1",
        "replyto": "pXt2EP0PW1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1250/Reviewer_1atA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1250/Reviewer_1atA"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a distribution-free post-processing approach to impose (approximate) equality of opportunity on a binary classifier with binary sensitive labels. The classifier is built as a sensitive-label-dependent threshold over a sensitive-label-dependent scoring function. This calibration procedure has finite sample guarantees."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The calibration procedure is distribution free and has finite sample guarantees. As a post-processing approach, it produces a simple pair of thresholds. To alleviate the communication costs of computing the quantile distribution across all clients, an efficient distributed quantile algorithm (Q-digest) is used."
            },
            "weaknesses": {
                "value": "Though conceptually simple, I found the presentation and the notation to be exceedingly hard to follow. I am also unsure on why, exactly, is there a need to maintain per-client score rankings other than to update the per-client sketches prior to aggregation. \n\nSince the distributed quantile learning algorithm is not a contribution of this paper, and the notation is hard to follow, it is hard to evaluate the contribution and the insight of this work."
            },
            "questions": {
                "value": "what exactly is the use of the per-client score sorting?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1250/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699318504328,
        "cdate": 1699318504328,
        "tmdate": 1699636051322,
        "mdate": 1699636051322,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vdg8u4QGTf",
        "forum": "pXt2EP0PW1",
        "replyto": "pXt2EP0PW1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1250/Reviewer_NaPJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1250/Reviewer_NaPJ"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a fairness Post-hoc approach that estimates a decision threshold for a classifier scoring function per sensitive attribute in a federated setting. This is done by computing a quantile estimate of the score function in a distributed manner such that the decision of the classifier achieves the best error subject to satisfying a confidence upper bound on the desired level of fairness. Note that the classifier needs to have access to the sensitive attribute at inference time since the learned decision threshold depends on it."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The idea seems promising, well grounded theoretically and the experiments show good performance for the proposed solution."
            },
            "weaknesses": {
                "value": "I find the presentation a bit hard to follow and should be simplified. I think the paper is hard to follow in terms of notation and procedure. The clients step is clear and the overall goal of having one threshold per sensitive attribute based on the desired fairness level seems reasonable. Even though this means that the sensitive attribute needs to be accessible at inference time.\nHowever, update on the server and related notation is hard to follow. For instance it seems that K (Eq.4) is obtained based on the desired probability of not satisfying the fairness tolerance (|DEOO|> \\alpha in Prop 3.2). Then the final pair k_0,k_1 are chosen to minimize the misclassification error (Eq 5) from the set of K. However, it is not clear to me how do you derive a single threshold for the scoring function from k_0,k_1 which seem to be two vectors of size S.\n\nEven though I find the approach interesting I think the paper should be improved in terms of clarity of presentation. At least I find that a better presentation would clarify the contribution and attract more attention."
            },
            "questions": {
                "value": "Is the minimization of Eq. 5 done in a greedy manner? Do you evaluate all of the possibilities in K?\n\nWhat is the global rank of a rank (i.e., last line in Step 2 Section 3.1 )\n\nHow does your method connect with Hardt et al 2016 which is a centralized post-processing fairness technique that also relies on finding a threshold per sensitive group to satisfy a fairness criteria."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1250/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1250/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1250/Reviewer_NaPJ"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1250/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699405952205,
        "cdate": 1699405952205,
        "tmdate": 1699636051250,
        "mdate": 1699636051250,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Y1yxG3Tto9",
        "forum": "pXt2EP0PW1",
        "replyto": "pXt2EP0PW1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1250/Reviewer_XnRQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1250/Reviewer_XnRQ"
        ],
        "content": {
            "summary": {
                "value": "This work presents FedFaiREE, an extension of the FaiREE post-processing method for fair classification, and its formal guarantees to the federated learning settings, leveraging the Q-digest method. Akin to FaiREE, the proposed framework works with any score-based function and outputs the best threshold to correct fairness violations. The provided experimental results illustrate that the proposed method shows promising performance on two Adult and Compas datasets."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "* Addressing group fairness in federated learning is a very important and currently popular problem. Most existing approaches are in-process methods and the proposed one is a post-process method that can be combined with other methods.\n\n* The authors provided experiments with baselines with and without applying FedFaiREE and showed improvements in the final models' performance.\n\n* The paper presents its ideas in a clear and easy-to-follow manner."
            },
            "weaknesses": {
                "value": "* I find the novelty and contributions of this work to be limited, given that the main objective, formal guarantees, and algorithm are very similar to [1]. Moreover, the algorithm operates under the assumption of full client participation, a condition that may not always align with reality in FL settings.\n\n*  The proposed problem and guarantees rely on the assumption of binary target and attribute/group variables, which is a restrictive assumption. This raises questions about the actual utility of this approach and what are its guarantees in more realistic scenarios (e.g., multiple sensitive groups and multiclass problems). I note that extending the work to multiclass problems is also identified by the authors, but there are existing works that address multiple attributes for these fairness metrics, e.g., [3].\n\n* The paper misses discussion and comparison to other works proposing the same idea -- i.e., how to optimize a fairness metric and produce results akin to centralized ML using the local information from clients (e.g., [2] and [4]). Also, while the authors briefly mention [5], [4] a more explicit discussion of these proposed method's conceptual differences would benefit the paper. \n\n* The experimental section requires enhancements: (1) the paper performs experiments using only two datasets, (2) important experimental details are missing, (e.g., the number of clients used in the experiments, standard deviation for each result, Dirichlet distribution parameter values that were explored, what is the sensitive group for each dataset etc.), (3) the comparison to AFL which optimizes for client-fairness (i.e. a different fairness concept in FL) should be justified.\n \n\n[1] Puheng Li, James Zou, and Linjun Zhang. Fairee: fair classification with finite-sample and distribution-free guarantee. In The Eleventh International Conference on Learning Representations, 2022.\n\n[2] Papadaki, A., Martinez, N., Bertran, M., Sapiro, G., and Rodrigues, M. (2022). Minimax demographic group fairness in federated learning.\n\n[3] Y. Zeng, H. Chen, and K. Lee (2021). Improving fairness via federated learning. \n\n\n[4] Hu, S., Wu, Z. S., and Smith, V. (2022). Fair federated learning via bounded group loss.\n\n[5] Yahya Ezzeldin, Shen Yan, Chaoyang He, Emilio Ferrara, and A. Avestimehr. Fairfed: Enabling\ngroup fairness in federated learning. Proceedings of the AAAI Conference on Artificial Intelligence, 37:7494\u20137502, 06 2023"
            },
            "questions": {
                "value": "**Major**\n*  Can you please give more insights (than the ones at the beginning of section 3.2) on why and how the proposed method differs from [1]?\n   \n* How does this work compare with the related works [2],[3],[4],[5] mentioned above? Is the group fairness definition studied here different from [2] and [4]? Please also revise the related work. \n\n* How does FedFaiREE perform for different fairness constraint parameter $\\alpha$, different levels of data heterogeneity across clients and confidence level $\\beta$? My understanding is that only $\\alpha=0.1$ for the adult dataset, $\\alpha=0.15$ for compas, and $\\beta=0.95$ for all experiments.\n\n*  I'm interested in understanding how heterogeneity and imbalancedness are introduced across clients for these datasets, using the Dirichlet distribution. Why the parameter for adult was set to 1 and for compas was set to 10? Additionally, what is the standard deviation for each result reported in the tables (both supplementary and main)? This should be included in the results. \n\n* What's the number of clients studied per dataset? If the number is low you should empirically examine how this approach scales for a larger number of clients. To illustrate that, you can consider for example the  ACSIncome dataset, where the data are naturally noniid and partitioned into 50 states and Puerto Rico and treat each place as a client (i.e., 51 clients). \n\n\n **Minor:** \n\n* It would be good for the authors to acknowledge the concerns regarding the COMPAS dataset within the fairness community. \n\n* The margins around Figure 1 around the figure require editing. The main text touches on the figure's description."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1250/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1250/Reviewer_XnRQ",
                    "ICLR.cc/2024/Conference/Submission1250/Senior_Area_Chairs"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1250/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699552496841,
        "cdate": 1699552496841,
        "tmdate": 1700738925741,
        "mdate": 1700738925741,
        "license": "CC BY 4.0",
        "version": 2
    }
]