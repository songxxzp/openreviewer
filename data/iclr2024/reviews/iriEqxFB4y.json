[
    {
        "id": "IasujqZBne",
        "forum": "iriEqxFB4y",
        "replyto": "iriEqxFB4y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7386/Reviewer_BvLD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7386/Reviewer_BvLD"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes to use a diverse sampling to draw OOD samples from auxiliary datasets for training OOD models. The sampling consists of two steps. First, they apply K-means to cluster the auxiliary samples in a normalized feature space. Second, the samples close to the decision boundary of in-distribution are selected from each cluster. The clustering ensures the diversity of the selected samples. In training, the selected samples are considered OOD samples. They evaluate the proposed method on the common benchmark, i.e., CIFAR100, and show that the proposed diverse sampling improves over greedy sampling and other baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper studies a well-motivated problem in OOD detection. Finding high-quality samples in the large auxiliary dataset improves the performance of the trained model and reduces the training cost.\n\n2. The proposed diverse sampling is simple and effective. \n\n3. The proposed method achieves impressive empirical results on the common benchmark.\n\n4. The authors provide extensive ablation studies to demonstrate the robustness of the method."
            },
            "weaknesses": {
                "value": "1. Diverse sampling is a well-known method in active learning [1,2]. Diverse sampling leading to superior performance is not surprising. It indeed improves the performance of OOD models. Critically speaking, the novelty in terms of the method is limited. If the authors could provide some deeper theoretical analysis, e.g., how diverse sampling improves the generalization bound, it would make the paper more solid.\n\n2. There are typos in equation 2. I understand that the authors use a cross-entropy loss. However, equation 2 is just the entropy. $p(y)$ or $y$ is missing inside the expectation.\n\n[1] Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set approach. In\nInternational Conference on Learning Representations, 2018.\n\n[2] Jordan T Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal. Deep batch active learning by diverse, uncertain gradient lower bounds. In International Conference on Learning Representations, 2020"
            },
            "questions": {
                "value": "How important is the clustering step in diverse sampling? [1] proposed to use a K-means++ seeding algorithm based diverse sampling for semi-supervised anomaly detection. The K-means++ seeding algorithm doesn't require to predefine the clusters and the number of clusters. I think it can be also easily combined with the absent category probability. How would the proposed diverse sampling compare to the K-means++ seeding algorithm based diverse sampling?\n\n[1] Li, Aodong, Chen Qiu, Marius Kloft, Padhraic Smyth, Stephan Mandt, and Maja Rudolph. \"Deep anomaly detection under labeling budget constraints.\" In International Conference on Machine Learning, pp. 19882-19910. PMLR, 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7386/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7386/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7386/Reviewer_BvLD"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7386/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698553852574,
        "cdate": 1698553852574,
        "tmdate": 1700756356339,
        "mdate": 1700756356339,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "e89WYPFHSD",
        "forum": "iriEqxFB4y",
        "replyto": "iriEqxFB4y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7386/Reviewer_HJAj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7386/Reviewer_HJAj"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on outlier sampling for out-of-distribution detection (OOD detection) tasks. To be specific, this paper follows the setting of outlier exposure that utilizes a surrogate outlier dataset to regularize the model during training, trying to make the model better recognize those OOD inputs. This work points out that previous outlier sampling methods are solely based on predictive uncertainty which may fail to capture the full outlier distribution. Motivated by the empirical evidence which shows the criticality of diversity, this work proposes Diverse Outlier Sampling (DOS) to select diverse and informative outliers via clustering the normalized features at each iteration. The proposed method achieves an efficient way to shape the decision boundary between ID and OOD data. Experiments from different perspective are conducted to demonstrate the effectiveness of DOS."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper focuses on an important and practical question on outlier exposure, i.e., the auxiliary outliers may fail to capture the full outlier distribution.  \n2. This paper proposes a new method, namely DOS, which clusters the normalized features at each iteration and samples the informative outlier from each cluster to realize the diversified outlier selection. The technical design for clustering with normalized features is noval to the knowledge of the reviewer and shows promising empirical achievement towards the target.\n3. Comprehensive experiments compared with both post-hoc OOD detection scores and also several representative sampling methods are conducted to demonstrate the effectiveness of the proposed DOS.\n4. The overall presentation is clear and the method is easy to understand."
            },
            "weaknesses": {
                "value": "Overall, this work presents a concise and effective way to conduct diverse sampling in outlier exposure. Here are the major concerns for the current version of this paper, and hope it can help to improve the paper better.\n1. Although the overall presentation is clear, some critical definitions and claims are questionable and lack of convincing support.\n2. Technically, the proposed method (DOS) is based on an empirical demonstration of \"diversity\" with the OOD detection performance. However, the detailed definition of diversity is under-defined and the underlying mechanism of the clustering-based sampling scheme is not clearly explained. \n3. Since the previous greedy strategy will continually sample those outliers that are easy to be recognized as ID data, why do we need to utilize the diverse sampling method if the newly proposed method will sample some outliers that are already recognized as OOD data by the model?\n4. The experimental part can include more results conducted in other ID datasets, as well as the large benchmark dataset (like ImageNet) to demonstrate the effectiveness and efficiency of the proposed DOS.\n\n---- Acknowledgement ----\nAfter reading the response, further clarification and discussion addressed most of the concerns in the following perspectives,\n- Provided the specific definition of diversity and explained the relationship with clustering-based sampling; Complete the analysis on the outliers sampled by diversification target; Revised some unrigorous statements; Provided the large-scale experimental verification on the ImageNet dataset;\n\nOverall, regarding the quality and the technical novelty of this work, the reviewer decided to raise the score accordingly."
            },
            "questions": {
                "value": "Please also refer to the weakness part for the general concerns. The following questions are more specific to the clarification and the reviewer hope these question can help the authors to improve the writing and presentation of this work. \n1. As for the critical motivation (\"However, the OOD samples selected solely based on uncertainty can be biased towards certain classes or\ndomains, which may fail to capture the full distribution of the auxiliary OOD dataset.\"), the intuitive illustration is based on a toy example in 2D space, it could be better to refer to some convincing results which also show the same problem with Figure 1c.\n2. For the \"imbalanced performance of OOD detection\" pointed out in the same sentence, could the authors provide some empirical evidence?\n3. The critical observation shows that \"outlier subset comprising data from more clusters results in better OOD detection\", but what is the relationship between the cluster with diversity? \n4. Based on the previous question, could the authors provide a more detailed or specific conceptual definition of diversity? and clearly state how to measure the diversity in the motivation part to make that part more convincing.\n5. It seems that the current version of the work does not provide the corresponding experimental part for supporting the claimed \"efficient\" of the proposed DOS, like the sampling efficiency if I understand it correctly.\n6. In addition to using the CIFAR-100 as the ID dataset, it could be better to use the ImageNet dataset as ID dataset and use a large-scale ImageNet 21k as auxiliary outliers to verify the scalability of the proposed DOS."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7386/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7386/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7386/Reviewer_HJAj"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7386/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698637243051,
        "cdate": 1698637243051,
        "tmdate": 1700547298242,
        "mdate": 1700547298242,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9OhMMdqtVT",
        "forum": "iriEqxFB4y",
        "replyto": "iriEqxFB4y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7386/Reviewer_bYrq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7386/Reviewer_bYrq"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes Diverse Outlier Sampling (DOS), a simple sampling approach for choosing diverse and informative outliers to use as an auxiliary OOD training dataset. The basic idea is simple: use k-means on normalized features to cluster the data and then per cluster identify the most informative outlier. Diversity is achieved since the k clusters collectively explain the full possible auxiliary OOD data but per cluster, we only choose the most informative outlier, which altogether ensures that the size of the auxiliary dataset can be controlled to be small (i.e., based on the number of clusters). Experimental results show that DOS works extremely well in practice."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is easy to follow.\n- The basic idea of the proposed approach is very simple and elegant.\n- The experimental results are compelling."
            },
            "weaknesses": {
                "value": "- By normalizing the feature vectors, if I understand correctly, Euclidean norm is used so that the normalized vector resides on the unit hypersphere. Is there any benefit to using specialized versions of k-means (and k-means related) algorithms for the hypersphere? For reference, there are versions of k-means and the Gaussian mixture model that are restricted to the hypersphere (technically, mixtures of von Mises-Fisher distributions). See, for instance, the papers by Banerjee et al (2005) and Kim (2021). More generally, some sort of sensitivity analysis with respect to using different clustering algorithms could be helpful.\n- Figuring out how to set up experiments so that you could report error bars would be very helpful.\n\nReferences:\n- A Banerjee, I S Dhillon, J Ghosh, S Sra. Clustering on the Unit Hypersphere using von Mises-Fisher Distributions. JMLR 2005.\n- M Kim. On PyTorch Implementation of Density Estimators for von Mises-Fisher and Its Mixture. arXiv 2021."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7386/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7386/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7386/Reviewer_bYrq"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7386/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698809762269,
        "cdate": 1698809762269,
        "tmdate": 1700629691498,
        "mdate": 1700629691498,
        "license": "CC BY 4.0",
        "version": 2
    }
]