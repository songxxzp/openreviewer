[
    {
        "id": "SJ2qy5yLDB",
        "forum": "gzT61ziSCu",
        "replyto": "gzT61ziSCu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1974/Reviewer_VvN5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1974/Reviewer_VvN5"
        ],
        "content": {
            "summary": {
                "value": "The authors show that automatic functional differentiation (AutoFD) can be implemented in the same vein as automatic differentiation (AD) in JAX.\nThe authors introduce operators, namely, compose, \u2207, linearize, linear transpose, integrate, JVP, and transpose rules.\nThe authors provide two applications of AutoFD: Solving the brachistochrone problem and density functional theory."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Automatic functional differentiation is required in many research areas, such as physics, chemistry, mathematics, and machine learning. The topic is very relevant."
            },
            "weaknesses": {
                "value": "- Presentation is poor (see Questions and Comments below for details) and much more clarification is needed.\n\n- The experiments are not reproducible.\n\n- The baseline of the experiments is too weak."
            },
            "questions": {
                "value": "# [Question (major)] Infinite dimensional generalization of arrays\nIn Abstract (and Introduction),\n> By representing functions as infinite dimensional generalization of arrays, we seamlessly use JAX\u2019s existing primitive system to implement higher-order functions.\n- How was it realized in the proposed method? Could you point the part of the manuscript that concretely explain it?\n\n# [Comment (minor)] Reference \nIn Introduction,\n> To this date, functional differentiation with respect to functions are usually done in one of the following ways: (1) manually derived by human and explicitly implemented; (2) for semi-local functionals, convert it to the canonical Euler-Lagrange form, which can be implemented generally using existing AD tools; (3) use parametric functions and convert the problem to parameter space.\n- Could you add some reference papers about these three approaches for readers who are not that familiar with functional differentiation?\n\n# [Comment (minor)] Reference\nIn Section 3,\n> The Schwartz kernel theorem states that any linear operators can be expressed in this form.\n- Could you add a reference here? It is more reader-friendly.\n\n# [Question] Complex numbers\nIn Section 4.2,\n> The primitive operators considered in this work are focused to realize the most used types\nof operators and functionals described in Section 3.\n- [Question] Does your proposed program support complex numbers?\n\n# [Comment (major)] More definitions\nIn Section 4,\n- It would improve the clarity of the present paper to add a rigorous and/or intuitive definition of the Frechet derivative, cotangent space, and transpose rule and also an illustration of the primitive operations used in the present paper, because not all of the readers are familiar with both functional analysis and computer programming. Changing the order of explanations can also be an option (e.g., the begging of Section 4.2.3 can be an intuitive explanation of the Frechet derivative as a generalized directional derivative).\n\n# [Question] \nIn Section 4.2.1,\n> The function inverse on the right hand side of the $T_f (\\hat{C})$ rule is not implemented...\n- Does it restrict the operations of the proposed AutoFD? Could you take some examples?\n\n# [Question (major)] Grid points\nIn Section 4.2.5 and Experiment\n> We implement the integrate operator by supplying a numerical grid for the integrand function.\n- How did you choose the grid points?\n- How critical is the numerical error?\n- The proposed integral scheme looks like nothing but the conventional numerical integral. Is there any difference?\n\n# [Comment (minor)] Font\nIn Section 4.3 and Eq. (7--8),\n> We denote them as undefined because...\n- Changing the font of \"undefined\" would be good, e.g., mathtt.\n\n# [Comment (major)] Efficienty\n- For most of the statements in Section 4.4, I would like to see quantitative results.\n\n# [Questions and Comments (major)] Experiment: Solving variational problem\nIn Section 5.1, the authors performed an experiment to solve the brachistochrone problem.\n- The authors simplify the problem to a parametric fitting of $y^{\\theta}(x)$. This is how conventional methods in the numerical analysis of functionals does, as is stated in Section 1 and 2. What does the proposed program enable us to do, or what is the difference from the conventional methods?\n- What is the difference between Eq. (17) and (18--19)? Is it whether the Euler-Lagrange equation is used?  If yes, the performance gap given in Figure 2 may come from it.\n\n> It is worth highlighting that the directly minimize in the parameter space is limited by the numerical integration, it easily overfits to the numerical grid.\n- Taking random grids in every iteration is often done in learning integrals, which would fill the gap between red and the other curves. I would like to see the performance of such a more rational baseline.\n\n> Better functional optimal are found as can be seen in Figure 2 (right) that the functional gradient is closer to zero.\n- The authors should use log scales.\n\n# [Question and Comment (major)] Experiment: Density functional theory\nIn Figure 3 and 4,\n- I could not understand what Figure 4 means, potentially because Figure 4 is not a complete code like Figure 3. Could you provide more details of it? \n- Could you clarify how difficult it is to implement higher order derivatives? \n- Is Figure 3 simply a wrapper of Figure 4?\n- I would like to see numerical results about DFT using AutoFD.\n\n> In the SCF loop of the DFT calculation, ...\n- What is SCF?\n\n# [Comment (major)] Detailed experimental settings\n- Could you add more details of the experimental settings for reproducibility?\n\n# [Comment (major)] Code submission\n- Could you show the code for reproducibility?\n\n# [Question] PDE and FDE\n- Can we use the proposed AutoFD to solve a partial differential equations that include functional derivatives?\n- Can we use the proposed AutoFD to solve a functional differential equation that include functional derivatives (the task is to get the functional that satisfies the given equation that include functional derivatives)?\n\n# [Comment] Other journals and conferences \n- Automatic differentiation is actively discussed in, e.g., MFPS, TOMS, ICFP, TOPLA, POPL, and FoSSaCS. These community might have much more interest in the present paper.\n\n\n# [Comment (major)] Typos\nPlease proofread the manuscript before submission.\n- In Section 4.1, \"...one level of generalization, the term function in...\" should be. e.g., \"...one level of generalization. That is,  the term function in...\" \n- In Section 4.2, \"... for consistency. i.e. ...\" should be \"... for consistency; i.e. ...\"\n- In Section 4.2, \". Which\" should be \", which\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1974/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1974/Reviewer_VvN5",
                    "ICLR.cc/2024/Conference/Submission1974/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1974/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698048906041,
        "cdate": 1698048906041,
        "tmdate": 1700046192186,
        "mdate": 1700046192186,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HHBILQGVha",
        "forum": "gzT61ziSCu",
        "replyto": "gzT61ziSCu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1974/Reviewer_RHkE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1974/Reviewer_RHkE"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors extend the JAX automatic differentiation system to support higher-order derivatives -- i.e. derivatives over functions (often called functionals), rather than arrays.\n\nSome background: the core JAX autodiff system is based on a first order functional programming language.  The only data type that it supports is the Array (or tuples/dictionaries of Arrays), and all of the built-in differentiable primitive operations are first-order functions over arrays.  Consequently, JAX autodiff can only compute gradients with respect to Arrays. \n\nIt is important to note that the larger JAX library has plenty of higher-order API calls (e.g. vmap, linearize, jvp etc.), and python itself supports higher-order functions.  However, the core autodiff system works by *tracing* python functions to a first-order computation graph, and then computing gradients on that graph.\n\nIn this paper, the authors extend JAX type system to support functions as first-class citizens in the computation graph, and they introduce a set of differentiable primitive operations that are higher-order (e.g. function composition).  The extended autodiff \nsystem uses the same JAX autodiff machinery, but it can compute gradients with respect to functions/functionals as well as Arrays.\n\nThe authors describe the differentiation rules for a core set of higher-order primitive operations, and give a few examples, primarily drawn from engineering and physics, for doing autodiff over functionals."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is reasonably well-written, and the idea is mathematically interesting."
            },
            "weaknesses": {
                "value": "Although the paper is reasonably well-written, I had a very hard time following it.  Part of the problem is my fault.  Although I have deep knowledge of both JAX and automatic differentiation systems in general (having implemented several of them myself), I am not \nparticularly familiar with the underlying mathematics of functional analysis, Fr\u00e9chet derivatives, and so on, which are used in this paper.   I have not checked the math, and I am happy to defer to other reviewers who have a deeper understanding.\n\nSince this paper is being submitted to ICLR, rather than a computational mathematics conference, I had expected the authors to provide a gentle introduction to some of the underlying concepts, suitable for ML practitioners.  Sadly, they do not, and I suspect that most ICLR readers will have the same problems understanding it that I did.\n\nThe authors claim that \"functions are represented as infinite dimensional generalizations of arrays\", but they do not explain how, nor do they even cite any source for this claim.  (Perhaps a functional analysis textbook?)  Moreover, I assume the \"infinite dimensional array\" is simply a mathematical abstraction, and it is unclear to me why it is even relevant.  In the body of the paper, functions actually seem to be represented in the usual way as symbolic programs.  As one would expect from a practical algorithm, infinite dimensions do not appear.\n\nThe actual differentiation rules are written in terms of forward derivatives and *transpose* operations.  The use of transpose is due to recent work by Radul et al., but will likely be unfamiliar to ML practitioners who are used to traditional backpropagation.  It would have been helpful if the authors had given an example of how these two operators work in a conventional (non-higher-order) setting, before diving into the higher-order case.  \n\nEven after doing my best to read this paper carefully, I am still unsure about how higher-order functions (functionals) are actually defined and represented in the core language. The differentiation rules given here do not seem to constitute what I would consider to \nbe a core programming language.  E.g. the authors provide differentiation rules for function composition, but not for function application or function definition.  That leads to me believe that the code for functionals would have to be defined in a point-free functional programming style, as is used by some other autodiff systems in the literature.  However, the examples in the paper just show ordinary python.  Is the python code traced, and then translated into the primitive operations, as is usual for Jax?  What are the details of this translation, since going from python to a point-free representation is not necessarily trivial?\n\nThe differentiation rules for function composition $f \\circ g$ require that $g$ is invertible.  If composition is the basic mechanism used to build complex programs (as is usually the case in point-free languages), then this would seem to be a very severe limitation.\n\nMy final point of confusion is that in most cases of practical interest, the functional that we are computing a derivative for is really just an ordinary symbolic function that is parameterized by some array $A$.  The value that we actually want to solve for is $A$ -- the higher-order derivative is just an intermediary step.  In this situation, ordinary Jax works just fine without higher-order automatic differentiation; the higher-order operations are eliminated by tracing and partial evaluation, and the first-order autodiff system then solves for the gradient of $A$.\n\nThe authors actually allude to this fact, but do not provide a detailed discussion of the tradeoffs between solving directly for $A$ using tracing and first-order autodiff, and doing something more complicated with higher-order autodiff.  Given the various limitations and restrictions on the higher-order methods, the former seems decidedly simpler.  Why would I want to use this system?  Can you explain why some problems can't be solved with ordinary Jax?\n\nFinally there are some typos which added to my confusion.  E.g. page 5 second paragraph $C(x, y \\to x + y, f, g)$ needs extra parens: $C((x, y) \\to x + y, f, g)$ otherwise it makes no sense."
            },
            "questions": {
                "value": "Please see weaknesses, above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1974/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1974/Reviewer_RHkE",
                    "ICLR.cc/2024/Conference/Submission1974/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1974/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698820453917,
        "cdate": 1698820453917,
        "tmdate": 1700516614675,
        "mdate": 1700516614675,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hpboRV2z2h",
        "forum": "gzT61ziSCu",
        "replyto": "gzT61ziSCu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1974/Reviewer_4ztS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1974/Reviewer_4ztS"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces functional differentiation (i.e., derivatives of functionals, which are functions that take other functions rather than values as inputs). The package directly builds on JAX's automatic differentiation machinery, introducing a new datatype for functions (section 4.1) and then implementing the appropriate linearization and transposition rules for each primitive operation. The primitive operations implemented are differentiation (nabla), linearization (Fr\u00e9chet derivative), linear transposition, and integration, as well as some utility operators (composition, permutation, zipping). Caching is used to perform common subexpression elimination while the computation graph is being built. Finally, some experiments using functional differentiation are performed (brachistochrone problem, exchange-correlation functionals) as showcase."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "I think this paper is quite straightforward, but not in a bad way: The text is clearly structured, and a good balance is found between the theory behind and the implementation of the framework. I appreciate how the authors were able to re-use JAX's machinery, which allows them to benefit from a lot of JAX's strength (compilation backends, debugging tools, etc.) and the simplicity of the brachistochrone in code is quite compelling."
            },
            "weaknesses": {
                "value": "The main weaknesses of this paper are some of the limitations discussed in section 6. In particular, not having any approach for function inversion seems like a shortcoming."
            },
            "questions": {
                "value": "Could the machinery used to register derivatives not also be used to register function inverses?\n\nI would also love to see a more thorough discussion of the integration trade-offs: How should the user know what numerical grid to provide? And how sensitive should they expect the outcome to be to the grid provided?\n\nAs this is a software package, I would also like to know some of the details regarding public release: Will the code be on GitHub (or some other platform)? Under what license? Will there be documentation, tutorials, or notebooks? Does the code take the form of a separate Python package, or is it a fork of JAX?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1974/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698928106669,
        "cdate": 1698928106669,
        "tmdate": 1699636128957,
        "mdate": 1699636128957,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "z4aOXPpvt4",
        "forum": "gzT61ziSCu",
        "replyto": "gzT61ziSCu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1974/Reviewer_Mbhg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1974/Reviewer_Mbhg"
        ],
        "content": {
            "summary": {
                "value": "In this work the authors introduce a package for the machine learning framework JAX, to enhance JAX with functional differentiation, stemming from functional calculus / the calculus of variations. This enables the easier expression of e.g. neural operators, as they implicitly rely on the functional framework. To this end, the authors introduce 5 new language primitives, namely `compose`, `nabla`, `linearize`, `linear transpose`, and `integrate`. All implemented in pure Python, and complemented by the theoretical derivation of the individual operators.\n\nThe introduced extension is subsequently validated on examples from particle dynamics with the brachistochrone problem, and the exchange-correlation functional stemming from density functional theory."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Where this paper shines is its connection with a very strong theoretical background stemming from functional calculus, and how it derives its proposed extensions to JAX from said theoretical motivation. The two chosen examples only underline this further.\n\nIt removes previous constraints from a machine learning framework, hence acting in large part as an enable of future work using functional calculus, such as for learned operators."
            },
            "weaknesses": {
                "value": "At the same time, the work suffers from a number of unclear treatments of JAX, and a failure to establish the usage of the proposed framework for neural operators, such as the Fourier Neural Operator.\n\nJAX as a framework:\n- The authors explain the tracing into a DAG, and the mapping of primitives to XLA, the compilation backend underpinning JAX. What they miss in this instance though is the intermediate layer of JAXPR, JAX's internal representation, and the mentioning of operation-tracing, and XLA-compilation, only leads to a lack of clarity. I'd suggest to add a diagram of the pieces of JAX's architecture you rely on, and remove mentions of XLA-mapping of ops etc. from the latter parts of the paper.\n- While provided for some introduced operations, a number of operations do not have their implementation code attached to them. The addition of the code in the main paper, or the appendix would contribute greatly to further the clarity of the exposition.\n\nNeural Operators:\n- While neural operators, and most specifically Fourier Neural Operators, are presented as a clear motivation for the work, they are sadly not used in the experimental evaluation of the presented extension. Addition of a functioning Fourier neural operator based on `autofd` would significantly strengthen the paper's claims. Evaluation of a Fourier neural operator could for example be in the form of a normal JAX-based implementation, and an `autofd`-using implementation, where the code could for example be much more succinct with `autofd` while matching the performance of the JAX-native implementation.\n\nIn addition the draft has a number of minor typos, the addressing of which would improve the legibility greatly. For example:\n- Page 1, last paragraph: \"Base\" -> Based\n- Page 2, last line: \"maps arbitrary\" -> map arbitrarily"
            },
            "questions": {
                "value": "A number of questions arose while reading through the paper:\n\n- Did you perform an analysis of the computational properties of AutoFD, and most specifically the way JAX compiles AutoFD code if it is not being mapped to XLA-ops?\n- Is there an implicit trade-off computationally or conceptually to the chosen representation as an infinite-dimensional array?\n- Why did you choose a purely Python-based implementation of AutoFD, as compared to a version of AutoFD acting directly on the JAXPR?\n- For the _Efficiency of Implementation_, have you considered tracing your implementation with perfetto\n- How do you anticipate an inversion operator in JAX to be feasible? Would it be possible to implement such operator, if you were writing the operators at the level of the JAXPR?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1974/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1974/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1974/Reviewer_Mbhg"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1974/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699126173456,
        "cdate": 1699126173456,
        "tmdate": 1700740435052,
        "mdate": 1700740435052,
        "license": "CC BY 4.0",
        "version": 2
    }
]