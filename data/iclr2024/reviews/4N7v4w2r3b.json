[
    {
        "id": "8VctBG98zc",
        "forum": "4N7v4w2r3b",
        "replyto": "4N7v4w2r3b",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8708/Reviewer_8v7i"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8708/Reviewer_8v7i"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a comprehensive study on the robustness of neural network proxies, specifically in the text domain, under various optimization pressures. The authors introduce \"ProxyBench,\" a benchmark designed to investigate the robustness of these proxies, and conduct extensive experiments to uncover properties of the proxy gaming problem and highlight issues with current proxy reward models used in fine-tuning large language models. The paper emphasizes the importance of robust proxies in machine learning applications across diverse domains and sheds light on the challenges posed by proxy gaming, where proxies may fail to accurately represent the true objective under optimization pressure."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. **Address a Crucial Issue**: The paper targets the significant issue of proxy gaming in machine learning, attempting to bring attention to how proxies may fail to accurately represent true objectives under optimization pressure. Addressing this problem is crucial for the advancement of reliable machine learning applications.\n\n2. **Introduction of a Benchmark**: The authors introduce \"ProxyBench,\" aiming to provide a framework for evaluating the robustness of neural network proxies. This benchmark has the potential to standardize evaluations and comparisons of different models."
            },
            "weaknesses": {
                "value": "1. **Enhanced Clarity Needed**: The paper is rich in information and analysis, yet there is a noticeable need for enhanced clarity and brevity. The articulation of the problem statement and the methodology is somewhat unclear, making it challenging for readers to fully grasp the proposed works.\n\n2. **Constrained to Text Domain**: The benchmarks and experiments conducted in the paper are exclusively centered around the text domain. This singular focus could potentially limit the broader applicability of the findings and the relevance of the benchmark across diverse domains within the realm of machine learning.\n\n3. **Ambiguous Contributions**: From my perspective, the authors presuppose that the proxy will undergo optimization pressure, and they proceed to evaluate the proxy under this specific condition, using it as a benchmark. However, they fall short of providing a compelling rationale for this assumption. Additionally, the utility of the benchmarks provided remains unclear. On the validation front, the benchmark's credibility is questionable as it is solely based on the performance of one DeBERTa-v3 model, which is insufficient for establishing validity across various model types.\n\n4. **Lack of novelty**: The optimization methods employed in the paper, such as BoN, RL, and white box optimization, are pre-existing techniques. The authors do not offer a justification for their selection of these particular methods for the benchmark, nor do they delve into an analysis comparing the optimization processes of these different methods. This leaves a gap in the paper\u2019s contribution to the field, as it does not provide novel insights or methodologies."
            },
            "questions": {
                "value": "1. Why do we need an adversarially robust proxy?\n2. What are the contributions of the current work?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8708/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698829219384,
        "cdate": 1698829219384,
        "tmdate": 1699637092170,
        "mdate": 1699637092170,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "monGGKIn1v",
        "forum": "4N7v4w2r3b",
        "replyto": "4N7v4w2r3b",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8708/Reviewer_tXHZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8708/Reviewer_tXHZ"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces ProxyBench, a benchmark tool for assessing the robustness of neural network proxies in the text domain against optimization pressures. It reveals limitations in the effectiveness of current robustness enhancement techniques, like adversarial training, and exposes significant issues with existing proxy reward models. The experiments conducted with ProxyBench indicate a need for new methods to ensure proxy robustness, setting a foundation for future research in this area."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The strengths of the work is listed below:\n1. The introduction of ProxyBench is a substantial contribution, as it provides a practical and comprehensive benchmark for assessing the robustness of neural network proxies against optimization pressure, especially in the text domain.\n2. The paper conducted extensive experimental studies, and the insights from the statistics are very meaningful.\n3. This work uncovers the relationship between proxy model size and its robustness, contributing to the understanding of how different factors like model parameters, training data, and training steps affect proxy stability.\n4. The introduction of metrics for evaluating proxy reward model robustness and detailed analysis on the performance of baseline methods are practical.\n5. The exploration of different optimization strategies, including best-of-n, reinforcement learning, and white-box optimization, indicates a deep understanding of the landscape and provides a nuanced perspective on potential vulnerabilities."
            },
            "weaknesses": {
                "value": "1. Assumption of Gold Model Accuracy (Section 4.1): The paper assumes that the ensemble of functions trained on human preferences accurately reflects the true objective. However, there's an inherent assumption that the datasets used encompass all aspects of \"helpful and harmless\" which might not be comprehensive. There is also a risk that the ensemble could inherit systemic biases or errors present in the training datasets.\n2. Training the proxy model on a fixed dataset without further interaction with the gold model may not reflect the iterative nature of learning and adapting to new data.\n3. There is a missing relevant work \"TextGrad: Advancing Robustness Evaluation in NLP by Gradient-Driven Optimization, ICLR2023\".\n4. The paper notes a significant correlation of gamed examples across different proxy models. This suggests potential inherent weaknesses or biases in the training process or data. However, the paper does not provide a detailed analysis of the underlying causes of this correlation or strategies to mitigate it.\n5. The appearance of phase shifts in reward curves during RLHF optimization highlights that current methods may not capture the complexity of policy learning dynamics. The paper's methods might be missing other, subtler forms of proxy gaming."
            },
            "questions": {
                "value": "I do not have other questions. Please refer to the weakness colum above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8708/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699157313290,
        "cdate": 1699157313290,
        "tmdate": 1699637092032,
        "mdate": 1699637092032,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4X9r8Wg99a",
        "forum": "4N7v4w2r3b",
        "replyto": "4N7v4w2r3b",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8708/Reviewer_2dFK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8708/Reviewer_2dFK"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces ProxyBench, a benchmark for investigating the robustness of neural network proxies under various sources of optimization pressure. Through extensive experiments, the authors uncover previously unknown properties of the proxy gaming problem and highlight serious issues with proxy reward models currently used to fine-tune or monitor large language models. They also find that common robustness enhancement techniques such as adversarial training provide limited gains for proxy robustness, and suggest new methods that may be required."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* This paper studies the proxy gaming problem in RLHF, which is an interesting and important problem.\n* This paper conducted extensive experiments and revealed many previously unknown phenomena in proxy gaming\n* The writing is overall clear and easy to follow"
            },
            "weaknesses": {
                "value": "*  This paper mainly targets a benchmark for measuring proxy robustness. However, I believe most of the setup/data are borrowed from the existing works, in particular, [1], which weakens the originality.\n*  This paper did introduce a novel metric to measure the severity of proxy gaming, namely the difference between the first derivatives of the proxy and golden reward curves as the optimization proceeds (RMSE). However, the authors didn't highlight the importance and necessity of such a metric. The major motivation from what I can see is the phenomenon that proxy gaming can still happen at a subset of the examples even if the expectation of the proxy reward aligns with the gold reward. But it is rather hard to see why the proposed metric can reflect the severity of this problem.\n* Further, I am not sure why the aforementioned phenomenon, which is the major discovery of this paper, is important itself. I believe using the expected target across the data examples as a metric is rather a standard practice for probably any learning problem. Randomness always exists in the dataset and considering, for example, a worst-case target as a metric, is probably impractical.\n* Nevertheless, such a phenomenon can indeed be important if the subset of examples experiencing serious proxy gaming, is relatively consistent across different proxy models. Because this points to some fundamental behavior changes of the policy model, which may lead to arbitrarily worse golden reward if the data distribution for evaluation is skewed towards the favor of the policy model. To clarify this, the authors did mention that this phenomenon may not be random at the end of Section 5.1, and that the policy model may exploit the flaw of the proxy in Section 5.4. However, there is no quantitative result mentioned, especially for the former, making it hard to judge whether they are indeed the case.\n\n* Finally, I believe this is an interesting paper and has great potential. But it may not be well-organized in its current form. Personally, I would prefer pitching it as a comprehensive analysis of the proxy gaming problem, rather than as a benchmark. But certainly the former would require more extensive empirical results for more soundness.\n\n* [Minor point] The related work is a little bit lacking and makes it quite difficult for people to catch up, as I believe the focused problem is brand new. The authors should at least introduce RLHF before reward modeling and proxy gaming.\n\n\n[1] Scaling laws for reward model overoptimization. Gao et al., 2022."
            },
            "questions": {
                "value": "Please address the weaknesses mentioned above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8708/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699184075183,
        "cdate": 1699184075183,
        "tmdate": 1699637091873,
        "mdate": 1699637091873,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WxPVKPBT3X",
        "forum": "4N7v4w2r3b",
        "replyto": "4N7v4w2r3b",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8708/Reviewer_k5g4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8708/Reviewer_k5g4"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the proxy gaming phenomenon when we use a proxy model to learn policies. By introducing two evaluation metrics, final gold reward and RMS divergence, the authors obtain several empirical observations via benchmarking various training algorithms. In the end of the paper, the authors also provide several insights into designing methods to mitigate proxy gaming."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proxy model is broadly used in policy training, especially in the case when evaluation is costly. It is necessary and interesting to study the phenomenon of proxy gaming, especially in the context of LLMs. In addition, the authors also provide several hints on how to mitigate proxy gaming in this context."
            },
            "weaknesses": {
                "value": "1. My major concern is the evaluation metric, since almost all conclusions of Section 5.2 are based on this metric, it is necessary to elaborate further why you chose this metric and the mathematical formulation of P' and G'. From my point of view, let $\\\\\\{e_i\\\\\\}_{i = 1}^T$ represent the difference of each instance between the proxy reward and gold reward, we can use the loss $(\\frac{1}{T} \\sum\\_{i = 1}^T e\\_i^\\gamma)^{1 / \\gamma}$ with a hyper-parameter $\\gamma$ adjusting how much we should emphasis the instance-wise gap. \n\n2. The title of this paper is a bit misleading. First, all the investigations are conducted in the text domain. Second, I think the key part of the paper is to benchmark the proxy gaming phenomenon under various settings. In addition to adversarial optimization (WB), BoN and reinforcement learning methods are also studied."
            },
            "questions": {
                "value": "My major concerns are pointed out in the \"weakness\" part, I welcome the authors to address my concerns. I will do a re-evaluation after receiving the authors' feedback. In addition, we have the following questions or confusion.\n\n1. The gold objective here is also a learning-based model constructed from a few dataset. Have the authors evaluated how \"gold\" these models are? There is still a gap between the RLHF practice and the gold model used for analysis in this work. I am not sure how big this gap is.\n\n2. Any intuition of the claim \"This might be explained by a larger policy providing a stronger attack and finding more ways to exploit the proxy model.\" in the first part of Section 5.2?\n\n3. In classification or regression problems, adversarial training may hurt the performance on the clean input, does the adversarial training technique introduced in Section 5.5 hurt the performance when the proxy model is not adversarial?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8708/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699257587897,
        "cdate": 1699257587897,
        "tmdate": 1699637091720,
        "mdate": 1699637091720,
        "license": "CC BY 4.0",
        "version": 2
    }
]