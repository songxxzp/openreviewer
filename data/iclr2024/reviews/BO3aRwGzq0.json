[
    {
        "id": "q117bS8EBL",
        "forum": "BO3aRwGzq0",
        "replyto": "BO3aRwGzq0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5100/Reviewer_nS4z"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5100/Reviewer_nS4z"
        ],
        "content": {
            "summary": {
                "value": "The authors propose an approach for protecting FL neural network layers that leak more private information. This approach is motivated by an observation (Mo et al.; 2021) that is there is a layer in neural networks that leaks more private information than other layers.\n\nIn order to compensating accuracy, adaptive gradient descent is used. The evaluation results show that the proposed idea reduces the membership inference attack success rate with good model accuracy."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The idea seems to be interesting.\n+ The authors conducted extensive experiments for comprehensive analysis."
            },
            "weaknesses": {
                "value": "- I have privacy concerns for other layers.\n- The motivation seems interesting, but it is from an unpublished paper.\n- There is no discussion of similar related works."
            },
            "questions": {
                "value": "1. The authors aim to protect a specific layer in FL models. What are the privacy risks contained in other layers? I think this protection is insecure.\n2. The motivation is from an unpublished paper. Are there any similar papers from reputable conferences/journals? I have great concerns about the reliable analysis of the motivation. Although we should care about both published and unpublished papers, I think it would be better for authors to find more support for their conclusive motivation.\n3. the authors criticize differential privacy (DP) and cryptography in the related works. The authors said they are different and novel. However, given my understanding, the proposed idea is less secure than DP and cryptography-based works. I do not think the comparison is fair.\n4. The authors missed the discussion of similar related works in the same research direction.\n5. The author claims better model accuracy. I suspect it is from using adaptive gradient descent. If so, the improved utility is not from the newly-designed protection, i.e., not novel. Where is the high utility from?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N.A."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5100/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5100/Reviewer_nS4z",
                    "ICLR.cc/2024/Conference/Submission5100/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5100/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697977124948,
        "cdate": 1697977124948,
        "tmdate": 1700719944198,
        "mdate": 1700719944198,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0jPqVyCcrZ",
        "forum": "BO3aRwGzq0",
        "replyto": "BO3aRwGzq0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5100/Reviewer_AcVa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5100/Reviewer_AcVa"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes DINAR, a method designed to enhance the privacy of Federated Learning (FL) systems, specifically guarding against membership inference attacks. DINAR employs a straightforward yet efficient fine-grained approach, focusing on protecting the most vulnerable model layer in terms of privacy. This approach ensures effective and non-intrusive privacy protection in FL. Additionally, DINAR addresses potential accuracy losses in the protected model by leveraging adaptive gradient descent, thus optimizing model utility."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper proposes a fine-grained defense against MIAs. The authors investigate the impact of MIAs on different layers and provide empirical insights about perturbing some specific layers rather than the whole model.\n\n2. The authors conduct extensive experiments on multiple datasets."
            },
            "weaknesses": {
                "value": "1. The authors do not provide a theoretical privacy guarantee against MIAs. Without the theoretical guarantee, people cannot analyze the effectiveness and generalization of the proposed privacy defense method. \n\n2. It is unclear how the server or the clients derive the sensitive layer, e.g., layer p. Do all the clients share the same sensitive layer even under non-IID settings? If the clients have different sensitive layers, how does Dinar solve the divergence problem of local model updates since the clients perturb different layers?\n\n3. It is unclear which MIA methods Dinar is evaluated against in the paper. The author should claim the attack methods more explicitly.\n\n4. It might be a good idea to shrink the scope of the title to \"privacy against MIA\" rather than general privacy-preserving FL."
            },
            "questions": {
                "value": "Please see the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5100/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5100/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5100/Reviewer_AcVa"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5100/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698609427255,
        "cdate": 1698609427255,
        "tmdate": 1700582196692,
        "mdate": 1700582196692,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "B6JlbzquG0",
        "forum": "BO3aRwGzq0",
        "replyto": "BO3aRwGzq0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5100/Reviewer_uPn1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5100/Reviewer_uPn1"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a privacy-preserving method for FL, called  DINAR. Specifically, the paper presents an empirical analysis of how much each layer of a neural network leaks membership privacy information and identifies the most privacy-sensitive layer. Then, it proposes a fine-grained approach that obfuscates the most privacy-sensitive layer of the model, before sending it to the server for aggregation, and restores it at the client-side for personalization. It also adopts adaptive gradient descent for local training to improve the utility of the protected model. Finally, it evaluates DINAR with six datasets and four neural networks, and compares it with three FL privacy protection mechanisms. It shows that DINAR achieves effective privacy protection, without hurting model accuracy or inducing computational overhead."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Originality: The proposed DINAR is new in its approach to obfuscating the most privacy-sensitive layer of the model before sending it to the server for aggregation, and restoring it at the client-side for personalization. \n- Quality: The quality of the paper is good in its thorough empirical analysis of layer-wise privacy characterization and obfuscation.  \n- Clarity: The paper is well-written and clear in its presentation. The methodology is explained in detail, making it easy for readers to understand.\n- Significance: By identifying and protecting the most privacy-sensitive layer of a model, DINAR can potentially help to advance FL techniques that balance privacy protection with model accuracy."
            },
            "weaknesses": {
                "value": "Novelty \n-  It would be helpful if the authors could provide a clear and explicit comparison between the proposed DINAR method and the layer-wise privacy characterization studied in FL by Mo et al. (2021). Discussing the differences, similarities, and potential advantages of DINAR over Mo et al. will enhance the reader's understanding of the novel contributions of the present study. \n\nRelated Work\n- The literature review on DP & FL seems outdated, with references primarily from 2019 and 2020. Recent advancements have significantly improved the utility of DPFL algorithms. Therefore, it is recommended that the authors conduct a comprehensive review of the latest DP & FL algorithms,  such as [1,2,3,4].\n\nClarifications: \n- Generalization Gap of the layers: \u201cThe generalization gap of the penultimate layer is notably higher than the generalization gap of the other layer\u201d This conclusion about the \u201cpenultimate layer\u201d could be specific to certain model architectures used in Figure 1.  Can the authors clarify the extent to which this claim holds true across diverse model architectures?\n\n- Adaptive Gradient Descent vs. Adam: Could the authors shed more light on why adaptive gradient descent exhibits superior convergence behavior compared to Adam?   \u201cGiven the high-dimensional nature of optimization problems in neural networks, this technique dynamically adjusts the learning rate for each dimension in an iterative manner.\u201d  this statement holds not only for adaptive gradient descent but also for Adam. \n\n\nBaselines:\n- Comparison to DP-based Techniques:  My concern is that the comparison to DP-based techniques in Section 4.4 Figure 4 might be unfair, as the specific $\\epsilon$ used for DP methods is either undisclosed or excessively large. According to Section 5.1,  $\\epsilon$ might be set to 2.2 for all DP-related experiments throughout the paper.  Note that DP, by definition, safeguards data privacy against membership inference attacks. The suboptimal empirical privacy performance of DP methods in Figure 4 could potentially be attributed to the utilization of a large privacy budget. It would be more convincing if the authors could evaluate MIA against DP methods under a small privacy budget. \n- Comparing the proposed method with recent DP & FL techniques, which have state-of-the-art  DP utility,  would strengthen the submission, which helps to highlight the advantages and unique contributions of the proposed method. \n\nExperiments:\n  - It is claimed that \u201cleveraging adaptive gradient descent and, thus, further maximizing model utility\u201d. However, there are no ablation studies or analyses verifying the effectiveness of adaptive gradient descent, compared to other optimization methods such as Adam and SGD. \n\n  - The experimental setup appears to be overly simplistic, with only 5 clients considered for all datasets. This is contrary to the typical cross-silo and cross-device settings, which usually involve a significantly larger number of clients. \n  - Additionally, partitioning the entire dataset among just 5 clients might result in each client possessing a sufficient quantity of data, thereby ensuring the local model is well-trained and potentially leads to memorization or overfitting phenomena for specific layers. It might be helpful to explore how the effectiveness of the proposed method under MIA attacks might be influenced by varying the number of clients, the size of local data, or the number of local training epochs.\n  - Some experiment details are not provided.  What is the number of local epochs for each FL client? How many FL rounds are trained for all methods? \n  - Model Architecture: \u201cwe consider the CelebA dataset with a model containing eight convolutional layers.\u201d Does this mean that the authors trained a classification model exclusively using convolutional layers? There should be at least one fully connected layer to predict the class. \n\nLack of theoretical guarantee: \n- While the proposed method is insightful, it is primarily based on heuristics and lacks privacy guarantees. Consequently, It is possible that advanced membership inference attacks could compromise the proposed method, whereas DP, with a small $\\epsilon$, is guaranteed to provide privacy preservation.\n\nTypos:\n-  There is a missing period in the  caption of Figure 1.\n\nReference:\n- [1] Projected federated averaging with heterogeneous differential privacy. VLDB 2021. \n- [2] On privacy and personalization in cross-silo federated learning. NeurIPS 2022.\n- [3] Make Landscape Flatter in Differentially Private Federated Learning. CVPR 2023.\n- [4]  PRIVATEFL: Accurate, Differentially Private Federated Learning via Personalized Data Transformation. USENIX 2023."
            },
            "questions": {
                "value": "Please see the questions in \"Weaknesses\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5100/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5100/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5100/Reviewer_uPn1"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5100/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698813111769,
        "cdate": 1698813111769,
        "tmdate": 1699636501404,
        "mdate": 1699636501404,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "32nEMxOsUi",
        "forum": "BO3aRwGzq0",
        "replyto": "BO3aRwGzq0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5100/Reviewer_Y9fY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5100/Reviewer_Y9fY"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes DINAR, a privacy-preserving FL method to defend against Membership inference attacks in FL by specifically hiding FL neural network layers that leak more private information than other layers from the FL server in the aggregation step of FL. In order to compensate for any potential loss in the accuracy of the protected model, DINAR combines the proposed approach with adaptive gradient descent."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed approach is simple and easy to apply in real-world scenarios of FL settings.\n- The proposed method can reduce the attack success rate of membership inference attacks while maintaining high model performance.\n- Extensive empirical experiments are conducted for analytical insights and evaluations."
            },
            "weaknesses": {
                "value": "- The privacy risk of Membership Inference Attacks is on the data-point level. Therefore, different data points will correlate to different neurons in different layers. As a result, the proposed method obfuscates a few layers and can not provide privacy protection for all data points in the clients' datasets.\n- No guarantee is given for privacy protections.\n- The considered attacks do not have high ASR in no defense models (e.g., 58% AUC in CelebA). Therefore, the defensive ability of the proposed method is degraded since it might be able to defend against weak attacks but not for stronger ones."
            },
            "questions": {
                "value": "1/ What is the attack success rate of the MIA on the vulnerable data points, i.e., the data points whose memberships are easy to infer?\n2/ How does the proposed method deal with non-i.i.d problems in FL? Specifically, when the data is non-i.i.d, obfuscating multiple layers will incur a distribution shift between the client's local data and the data from other clients. \n3/ In Figure 6, why the model with no protection can have lower model performance compared to the model trained by the proposed method?\n4. The size of the images is very small and hard to read. I suggest the authors make it more straightforward for the audiences."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5100/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698822834326,
        "cdate": 1698822834326,
        "tmdate": 1699636501266,
        "mdate": 1699636501266,
        "license": "CC BY 4.0",
        "version": 2
    }
]