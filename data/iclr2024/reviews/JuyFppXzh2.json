[
    {
        "id": "WWythtt6L4",
        "forum": "JuyFppXzh2",
        "replyto": "JuyFppXzh2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9159/Reviewer_pzXY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9159/Reviewer_pzXY"
        ],
        "content": {
            "summary": {
                "value": "Gandalf, a novel approach which makes use of a label correlation graph to leverage label features as additional data points to supplement the training distribution. Their approach can be applied in a plug-and-play manner with several existing methodologies, leading to a 30% performance improvement. The authors focus on the short-text setting where they exploit the symmetry between inputs and labels to obtain improved learning of label correlations. They propose an approach leverages the innate symmetry of short-text XMC along the LCG to construct valid data-points."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The approach can be applied in a plug-and-play manner with several existing methodologies - the results of which have been demonstrated in Table 2. \n* They have used publicly available benchmark datasets to evaluate their results.\n* Gandalf shows relatively large improvement of 30% over 5 state-of-the-art algorithms across 4 benchmark datasets."
            },
            "weaknesses": {
                "value": "* The authors focus on short text. While it is widely used across the industry, it will be good to demonstrate why their approach is better for short text when compared to other approaches. In a sense what makes the approach more suited for short-text? At the same time, would the approach work on large text as well?\n\n* It will be great if the authors can elaborate on the \"Symmetric nature of short-text XMC\". An example to illustrate the symmetric form would strengthen the reasoning for utilizing the symmetry. Especially when the paper in a way hinges on the utilizing the symmetry along with the label correlation graph; LCGs have been used in other approaches.\nAs a result, the novelty seems limited. \n\n*Along the same lines, it will greatly help if the authors can detail how the approach can handle the sparse instance-to-label mapping present in the datasets.  \n\n\nA minor issue: I believe there are a few missing references in the paper and the supplemental material."
            },
            "questions": {
                "value": "Addressed in the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9159/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698821304046,
        "cdate": 1698821304046,
        "tmdate": 1699637152895,
        "mdate": 1699637152895,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6Cj5ySQMT6",
        "forum": "JuyFppXzh2",
        "replyto": "JuyFppXzh2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9159/Reviewer_tePQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9159/Reviewer_tePQ"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a data augmentation method called Gandalf which leverage label correlation as additional data points against short-text extreme multi-label text classification problem. The presented experiment results show Gandalf is able to improve the performance on other extreme classifiers on several benchmark datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1)\tThe proposed data augumentation idea is relatively simple and work effectively on several benchmark datasets.\n\n2)\tThe empirical studies are relatively abundant."
            },
            "weaknesses": {
                "value": "1) The underlying idea of the main method a little bit lacks novelty and seems an extension of the existing work likes ECLARE.\n\n2) The method does not contribute to the real-world settings as most XMC methods choose to make partial experiments on long-text benchmark datasets. Besides, the method seems to increases the overhead of training datasets which may cause limitations."
            },
            "questions": {
                "value": "1)\tWhy classical XMC problems like AttetionXML, SiameseXML++ are not experimented with Gandalf?\n\n2)\tIt seems that the proposed Gandalf does not give competitative performance on PSP metrics compared to existing methods. Do you think Gandalf is an effective method dealing with the tail labels in XMC problem?\n\n3)\tCan Gandalf work on long-text XMC datasets?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9159/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698823182740,
        "cdate": 1698823182740,
        "tmdate": 1699637152770,
        "mdate": 1699637152770,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "adD28lkoss",
        "forum": "JuyFppXzh2",
        "replyto": "JuyFppXzh2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9159/Reviewer_Wc5R"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9159/Reviewer_Wc5R"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes Gandalf, which augments the training dataset in extreme classification by using label features as documents, with their corresponding \u201clabel mapping\u201d being constructed using a label-label graph. Such a setup allows most existing extreme classifiers to now leverage label features for improved generalization, without any changes to the training pipeline and no added inference cost. Gandalf shows significant improvement in both Precision and Propensity-scored Precision metrics over four commonly used extreme classification datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The proposed methodology is architecture-agnostic thereby resulting in easy and widespread adoption.\n- Consistently improved performance for a variety of extreme classifiers, especially for tail labels."
            },
            "weaknesses": {
                "value": "- The training time will be significantly increased since the new number of training points will be number of documents + number of labels. And in typical extreme classification setups, the number of labels can be much greater than the number of available documents.\n- The approach assumes (1) label features exist in the same input space as documents; and (2) the extreme classifier is NOT a two-tower approach, and embeds the labels and documents in the same space."
            },
            "questions": {
                "value": "- What if you use graphs other than the random-walk graph in ECLARE? For example, the co-occurrence graph?\n- Why should the performance of classifiers that already use label-features (e.g., ECLARE, DECAF) improve with Gandalf?\n- To combat the increased training cost, it would be interesting to understand the sample efficiency of Gandalf. To be more specific, how much performance is improved when augmenting, e.g., {0, 25, 50, 75, 100}% of random labels to the training data?\n- Missing citations in multiple places in the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9159/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9159/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9159/Reviewer_Wc5R"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9159/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698926610740,
        "cdate": 1698926610740,
        "tmdate": 1699637152641,
        "mdate": 1699637152641,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3lWGRa4ztm",
        "forum": "JuyFppXzh2",
        "replyto": "JuyFppXzh2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9159/Reviewer_S9V4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9159/Reviewer_S9V4"
        ],
        "content": {
            "summary": {
                "value": "This paper studies Extreme Multi-label Text Classification (XMC) problems, which assigns a short text input sample with a subset of most relevant labels from millions of label choices. The principal difficulty in XMC is managing the vast array of possible classes. Building upon existing research, this work incorporates the textual features of labels into the classifier's training process. Especially, given that input samples often share common tokens with the labels they're associated with, this task becomes correlating short text inputs with related sets of text.  For example, the input sample \u201c2022 French presidential election\u201d could be associated with \u201cApril 2022 events in France\u201d,  \u201c2022 French presidential election\u201d, \u201c2022 elections in France\u201d, \u201cPresidential elections in France.\u201d\n\nWhile previous research has explored various methods for aligning input and label texts, this paper proposes a straightforward technique for data augmentation, illustrated in Figure 1. It enhances the original N*L training data matrix with an additional L*L matrix, which captures the interrelationships between the L labels. The results from experiments suggest that this enrichment with the L*L matrix enables established XMC classifiers to attain better accuracy in classification tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1)\tThe data augmentation concept introduced in this paper is refreshingly straightforward, offering an intuitive strategy to expand the training dataset.\n2)\tEmpirical assessments indicate that incorporating this additional data into the training process proves beneficial."
            },
            "weaknesses": {
                "value": "1.\tThe augmented dataset introduced is considerably large, e.g., potentially consisting of a large matrix in size of millions by millions. The training time will be significantly increased.  While it's true that this does not affect the inference time, it substantially extends the duration of the training phase due to the increased volume of data.\n2.\tThe two-tower model \u201cNGAME + classifier\u201d yields the highest performance on the Amazon datasets. Even with the introduction of additional data, the base algorithms do not surpass the efficacy of the two-tower. \n3.\tSome reference citations are missing: Zhang et al., 2021a; ?; Lu et al., 2022),  ANCE (?)"
            },
            "questions": {
                "value": "1)\tWhat is the computation cost for training the base algorithms when taking the additional L*L matrix? including the process of obtaining the L*L matrix.  It would be beneficial for the paper to detail the expected impact on the training duration.\n2)\tWhy there is no evaluation of two-tower models on the two wiki-dataset?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9159/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699195530715,
        "cdate": 1699195530715,
        "tmdate": 1699637152535,
        "mdate": 1699637152535,
        "license": "CC BY 4.0",
        "version": 2
    }
]