[
    {
        "id": "TrlnGo96pB",
        "forum": "FJlIwGqPdL",
        "replyto": "FJlIwGqPdL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission163/Reviewer_XaQc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission163/Reviewer_XaQc"
        ],
        "content": {
            "summary": {
                "value": "This paper mainly discusses the robustness of conformal prediction (CP) against adversarial examples. The authors first present their discovery of two weaknesses of the existing CP against adversarial examples. From experiments, the authors empirically demonstrate that the existing CP methods have much larger prediction set sizes to cover correct predictions over adversarial examples. Also, while existing adversarial training methods improved top-1 accuracy, they also increased prediction set sizes. Then, the authors propose a new training method (AT-UR) to improve the robustness of existing CP methods. This method consists of two components: *entropy minimization* and *beta importance weighting*. Additionally, the authors present a theoretical finding that justifies beta importance weighting."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper contains many different findings: empirical discovery about CP, experimental verification of the proposed method, and one theoretical statement on beta importance weighting.\n2. The paper writing is clear enough to understand those findings. The findings are supported with visualizations that make the findings easier to understand.\n3. The experiments use four different datasets and demonstrate that the findings generalize over different datasets."
            },
            "weaknesses": {
                "value": "1. There is only one attack method used in this paper, i.e., PGD. This could be good enough to show the problem of the existing CP methods. However, when showing the robustness improvement, it would be better to include other powerful attack methods, e.g., CW, DeepFool, etc."
            },
            "questions": {
                "value": "1. It looks like the improvements on the CIFAR datasets are smaller than the improvements on the Caltech256 and the CUB200 datasets. Is this specifically related to the number of classes in those datasets? If so, would it be better to normalize the improvements by the number of classes?\n2. Minor comments\n    - Section 4 and Section 5 are relatively short compared to other sections. Maybe you can merge those sections into one section (that summarizes the discovery regarding CP against adversarial examples) with two subsections."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission163/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698594506003,
        "cdate": 1698594506003,
        "tmdate": 1699635941879,
        "mdate": 1699635941879,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "i6u9bJOAhx",
        "forum": "FJlIwGqPdL",
        "replyto": "FJlIwGqPdL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission163/Reviewer_k8ir"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission163/Reviewer_k8ir"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the uncertainty quantification provided by the conformal prediction framework under adversarial attacks. The conformal framework constructs prediction sets (sets of classes/labels) with high-probability coverage guarantees. One desires to obtain such high-probability prediction sets with small set sizes. Consequently, this paper contributes the following:\n1. It experimentally demonstrates that adversarial training is required to achieve small prediction sets.\n2. It experimentally demonstrates that the prediction sets constructed under newer adversarial training variants are larger than those for the vanilla version (even though they improve top-1 accuracies).\n3. It experimentally demonstrates that two factors correlate with the prediction set size: (i) the entropy of the predicted class probabilities and (ii) the predicted rank of the ground-truth class. This paper proposes to reduce the prediction entropy and the predicted rank of the ground-truth class to reduce the prediction set sizes; the former by adding an entropy regularization term and the latter via importance weighting w.r.t. a fixed beta distribution. This method empirically reduces the prediction set sizes. The paper also provides theoretical results showing that beta importance weighting improves the generalization of the trained model."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The paper is motivated by improving the performance of conformal prediction under adversarial attacks, an important research area for practical deployment.\n2. The writing structure is good, with empirical findings driving the direction of the paper.\n3. The proposed method is simple to implement."
            },
            "weaknesses": {
                "value": "[Details included in the Questions section]\n\n1. Novelty and contributions - The paper argues its contributions to be as highlighted in the Summary section. However, some of the empirical findings are not so novel.\n2. Related work - Comparisons with related works like Gendler et al. (2021) and Ghosh et al. (2023) are insufficient.\n3. The proof for Theorem 1 seems incorrect."
            },
            "questions": {
                "value": "1. Novelty and contributions\n    1. The fact that a model trained without adversarial training does not produce small conformal prediction sets on adversarial examples is not surprising. Since such a model has poor performance (often random or worse), this trend is expected; the prediction set size depends on the quality of the underlying model [Vovk et al. (2005), Shafer and Vovk (2008)].\n    2. Similarly, the correlation between the prediction set size with the prediction entropy and the predicted rank of the ground-truth class is not surprising. It is a consequence of the non-conformity function (or the set function) used; the correlation is apparent from the function definition. Instead, for example, if one were to use the 0-1 loss that outputs 0 if the top-1 prediction is correct and 1 otherwise, I believe the prediction set size would correlate with the top-1 accuracy (which this paper argues is not true).\n\n2. Related work\n    1. Gendler et al. (2021) and Ghosh et al. (2023) are highly related to this paper; they propose conformal algorithms to do well on adversarial examples. While this paper highlights the comparisons with Gendler et al. (2021) in Section 1, the more recent work of Ghosh et al. (2023) is not. Did the authors compare against their proposed method?\n    2. Did the authors compare the proposed beta importance weighting method against that of Einbinder et al. (2022)?\n    3. I believe the subsection \"Adversarial Robustness\" (Section 2) contains an incorrect citation. Was it meant to be Gendler et al. (2021) instead of Salman et al. (2020)?\n\n3. Proof for Theorem 1 - I encourage the authors to revisit the following.\n    1. The proof expands the gamma function as $\\Gamma ( n ) = ( n - 1 ) !$, which is true when $n$ is a positive integer, not a real value.\n    2. When $A$ and $B$ are combined, the bound $c \\leq K^{- \\alpha}$ is used. This is not satisfied when setting $c = \\max \\\\{ K^{- \\alpha} , \\cdot \\\\}$.\n    3. How is the bound $A \\leq c K^{- c}$ obtained?\n    4. How is the last inequality $\\sum_{k = 1}^{K} p_{\\text{Beta}} ( k / K ; a - c , b ) / K^{2} \\leq 1$ obtained?\n    5. It seems that the inequality $a > c$ is used throughout, but is not assumed.\n    6. Typos\n        1. $\\hat{r} ( x , y )$ should be replaced with $r ( x , y )$.\n        2. The indicator function should be $r ( x , y ) = k$ instead of $\\hat{r} ( x , y ) = 1$.\n\n4. The proposed method\n    1. Theorem 1 is not empirically supported (cf. Table 3).\n    2. What is the evidence to show that the majority of data lies in the promising region (stated in Section 6.2)?\n    3. What is done to reduce the importance weight when $r_{i} = 1$ or $\\hat{r}_{i} = 1 / K$ (since this is not part of the promising region)?\n    4. The paper should explicitly mention how the predicted ranks are normalized, i.e., define $\\hat{r} ( x , y ) = r ( x , y ) / K$.\n    5. Fig. 5 - The histograms look indistinguishable. Can the entire x-axis be included to highlight the difference, if there is one?\n    6. Fig. 4\n        1. How is the promising region determined in this illustration?\n        2. Is the ratio reported not the fraction of points in that region? The caption and the text supporting this figure are confusing.\n\n5. Experiments\n    1. What is the reason for only looking at $l_{\\infty}$ deviation adversarial examples? RSCP handles $l_{2}$ deviations; how do the experimental results differ?\n    2. What is the pre-trained model used for? Are the adversarial examples not constructed based on the model at hand?\n    3. APS is claimed to be more stable than RAPS in Section 7. What is this stability concerning?\n    4. MART is used for the initial experiments but not the main ones. Is there a reason for that? How does the proposed method perform with MART?\n    5. The generalizations for which version of the proposed method to use in Section 7.2 are done for datasets. However, it seems to be dependent on the adversarial training method used.\n\n6. Preliminaries\n    1. Prediction set size and conformal prediction inefficiency are used synonymously. However, the latter is not defined.\n    2. The paper includes empirical risk minimization but does not discuss its assumptions. The generalization error is bound under the i.i.d. assumption.\n    3. What are adversarial examples? What is the PGD attack? What is adversarial training? The paper should provide these details.\n    4. The conformal prediction framework is not explained well. Additionally, the paper does not discuss its assumptions and statistical guarantees in detail.\n    5. $y_{i j}$ in Eq. 1 is not defined.\n\n7. Section 4 does not show that adversarial training is indispensable (as mentioned in the last paragraph). It shows that standard non-adversarial training methods lead to large conformal prediction sets. Additionally, saying that the non-adversarially trained models are \"completely broken\" is incorrect. What the paper might want to emphasize is that the prediction sets are large, making them less informative.\n\n8. It is worth mentioning in the main paper that the top-1 accuracy decreases when using the proposed method.\n\n9. It is also worth including details of the experimental setup in Sections 4-6 or pointing to where they are in the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission163/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698790268127,
        "cdate": 1698790268127,
        "tmdate": 1699635941781,
        "mdate": 1699635941781,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gOSGMgrKdI",
        "forum": "FJlIwGqPdL",
        "replyto": "FJlIwGqPdL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission163/Reviewer_GTvN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission163/Reviewer_GTvN"
        ],
        "content": {
            "summary": {
                "value": "The authors considered the problem of adversarial training to output probability prediction (for a classifier) that leads to a more efficient conformal prediction interval using APS. To achieve this goal, the authors proposed to include two additional loss terms: (1) the entropy minimization loss which encourages outputting uncalibrated prediction with more certainty, and (2) beta-weighting loss that up weights samples that in the moderate difficulty regime of classification (e.g., top probability does not correspond to the true label but not so far away)."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Training a (robust) classifier that aims for a small prediction interval is an interesting problem."
            },
            "weaknesses": {
                "value": "I feel that the efficacy why including the two additional terms can help the prediction interval construction and their robustness are not fully explored."
            },
            "questions": {
                "value": "1.  Is the gain related to the entropy minimization loss tied with the APS framework, which, due to the additional randomization, favors prediction with low entropy? Do you still observe the improvement of using the entropy minimization loss using other conformal prediction constructions? For example, using Sadinle et all with average-coverage/per-class coverage.\n\n2. How does the choice of beta distribution shape influence the results? Are the results sensitive to the beta-weight parameters?\n\n3. How do different attack budgets influence the results?\n\n4. How do different training budgets and step-sizes influence the results?\n\n5. The assumption on how pk distributed in Theorem 1 seems very stringent. In addition, even if I accept the assumption (which the authors certainly need to justify), the proof also needs to be discussed in more detail and I am not completely convinced currently. For example, the first term in Lemma 1 seems to be dropped in Theorem 1's proof, but isn't it the case that the first term will change as you change the weights and will be higher for the beta-weighted problem?\n\n6. Some minor issues:\n6.a: what is d(x,y) in the proof of Theorem 1?\n6.b: the summation in d2(P||P/w) should be r=k instead of r =1.\n6.c: I need to go to later sections in order to understand Table 1 in the preliminary results, some brief explanations about the evaluation metrics will be helpful.\n....\n\n[1]Mauricio Sadinle, Jing Lei, and Larry Wasserman. Least ambiguous set-valued classifiers with bounded error levels. Journal of the American Statistical Association, 114(525):223\u2013234, 2019."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission163/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698855456152,
        "cdate": 1698855456152,
        "tmdate": 1699635941695,
        "mdate": 1699635941695,
        "license": "CC BY 4.0",
        "version": 2
    }
]