[
    {
        "id": "QyZnr6oty4",
        "forum": "hkQOYyUChL",
        "replyto": "hkQOYyUChL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8876/Reviewer_LEME"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8876/Reviewer_LEME"
        ],
        "content": {
            "summary": {
                "value": "This research focuses on the safety implications of LLMs trained on third-party custom finetuning data. The study finds that LLMs can learn unsafe content but tend to forget it when subsequently trained on safer data. To address this, the \"ForgetFilter\" algorithm is introduced, which effectively filters out unsafe data during finetuning, resulting in significantly reduced assimilation of harmful content by LLMs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The problem addressed in this paper holds significant importance as it focuses on ensuring the generation of safe responses by LLMs.\n* The overall narrative presented is quite reasonable, highlighting the LLM's tendency to forget conflicting content when subjected to safety fine-tuning. Additionally, the observation that larger models exhibit improved ability to forget unsafe content during the reviewing stage adds an intriguing aspect.\n* The proposed FF algorithm demonstrates sound reasoning and showcases strong empirical performance, further bolstering its credibility."
            },
            "weaknesses": {
                "value": "* I have a concern regarding the paper's consideration of the first fine-tuning stage involving noisy and unsafe content. Considering that fine-tuning datasets are typically small and of high quality, I find this setting to be somewhat artificial. In my opinion, a more interesting and realistic issue arises from the existence of unsafe content during the pre-training stage of LLMs [1]. I kindly suggest that the authors explore whether the ForgetFilter (FF) method can effectively filter out unsafe content from the pre-training dataset, as this would provide valuable insights to the field.\n\n* The reason why unsafe content is forgotten remains unexplained.  Providing an empirical or theoretical explanation for this phenomenon would greatly enhance the paper and warrant a higher score. Additionally, it is crucial for the authors to investigate the conditions under which unsafe content can be forgotten, enabling readers to understand when to effectively apply the proposed ForgetFilter (FF) method. \n\n\n[1] Pretraining Language Models with Human Preferences"
            },
            "questions": {
                "value": "The presentation can benefit from a diagram to show the procedure of the 2-stage fine-tuning in the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8876/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697945895140,
        "cdate": 1697945895140,
        "tmdate": 1699637117427,
        "mdate": 1699637117427,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GCiQ1M0N04",
        "forum": "hkQOYyUChL",
        "replyto": "hkQOYyUChL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8876/Reviewer_DzUN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8876/Reviewer_DzUN"
        ],
        "content": {
            "summary": {
                "value": "This paper claims that the unsafe data points are more likely to be forgotten during finetuning. They study this effect under different scales of Large Language Models. They design a data filtration method to remove unsafe datapoints from the original finetuning dataset."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. I found the collection of datasets well done and expressive enough to conduct rigorous experiments. Moreover, the use of different model sizes was indeed a good addition to understanding the effects of scale on the phenomena studied in this work. \n2. The motivation of this paper is strong since producing safer models is indeed an important issue.\n\nOverall, I vote to reject this paper. This paper does not demonstrate any new behavior that is not already apparent from existing literature. The experiments also are not setup correctly to prove the original claim that unsafe points are more easily forgotten. If the authors sufficiently answer all of my questions, I am willing to increase my scores."
            },
            "weaknesses": {
                "value": "1. In the abstract, it is claimed that models tend to forget unsafe data points when finetuned on safe data points. Isn't this true of any attribute? Training on data points that include only waterbirds will decrease the performance of a model on landbirds for image vision tasks for example. This forgetting phenomenon when shifting domains like this is well-observed in the literature in my opinion. My most grave concern with this paper is that the forgetting of unsafe examples is only done during safety review. However, when further finetuning on safe examples, it is completely expected that the forgetting on safe examples would be less, the forgetting on random points would be slightly higher, and the forgetting on unsafe points would be more. This seems like it has everything to do with the distributions from which the points rely and not that there is something fundamental about unsafe data that is more easily forgettable which is what this paper seems to claim. For example, if one did an Unsafe Review by finetuning on unsafe data points, I expect the exact opposite trend to hold. You mention evidence to support this point in that using safe examples from a different domain causes forgetting over all points, safe or unsafe, from the original domain. Thus, it is not the safety of a data point affecting its rate of forgetting, just the similarity to the data points seen during the review. If this is the case, then the claim that unsafe data points are more easily forgettable than safe data points has not been corroborated. You have only proved that finetuning on safe points from a domain causes the model to produce answers similar to these safe points on the same domain, which is not surprising at all and such ideas exist already in the literature. \n2. As a slight writing suggestion, I would make it such that the list of contributions on page 2 had more details. Currently, they are very spare and do not convey much information besides barebones ideas. Adding more details here would greatly improve readability.\n3. This paper proposes the ForgetFilter method as a way of filtering the finetuning dataset of unsafe examples to produce safer models. However, in the Related Works section, there is no mention of different filtering algorithms. This is a large miss in my opinion. It is difficult to understand where such a filtering technique stands in the context of the large existing body of work of data filtration for safety. I would add a brief description of existing filtration methods and why this method improves on these methods. \n4. The Related Works also does not mention the connection between memory and safety that has long been analyzed in the literature. For example, the work \"Does Learning Require Memorization? A Short Tale about a Long Tail\" discussed how private points on the tails of the distribution get memorized and influence the weights of the models more than other data points. This connection should be mentioned here as it is completely possible that unsafe points may belong to tails of the data distribution and the effects examined in this paper can be explained by this previous work.\n5. I found the ordering and structure of Section 3.1 not readable. I would rearrange this section in chronological ordering to better understand the data first before discussing what a safety review is. \n6. I found several spelling and grammatical errors in this paper. While not critical to my score, a scientific paper should be more presentable. For example, on page 3, you misspelled promote as promot. Another example is forgetting the comma after default on page 3.  Fixing these will improve the presentation of the work significantly. \n7. The metric you are reporting is the average rate for a set of data points. You are not reporting the expected value. The expected value is a specific term and you report the empirical average, which is not the same value. Please change this to be correct.\n8. The main contribution of the ForgetFilter method is a way of filtering the original finetuning dataset such that the resulting model produces safer outputs. It does this by removing points predicted to be unsafe. Therefore, for the experiments, it should be tested if using a forgetting rate to predict the safety of a datapoint in the finetuning dataset is a strong method for predicting the safety of a datapoint. Thus, the baselines should be simpler methods or existing methods of filtering the data set. However, Safety Replay and Moral Self-Correction are neither of these. Thus, it is unclear whether Forget-Filter is really improving over anything since the comparison is not fair. There are many ways of filtering datasets for safety. These methods must be used as a baseline first and foremost to truly understand the strength of ForgetFilter."
            },
            "questions": {
                "value": "1. Why are the ground truth responses in the BBQ dataset modified to a stereotypical choice?\n2. Why were only the learning rate and the batch size changed from the default hyperparameters of every model? Did the results seen depend on this hyperparameter selection or was this done purely for computational reasons? If it is the latter, please make sure to note this in the main text."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8876/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8876/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8876/Reviewer_DzUN"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8876/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698799805159,
        "cdate": 1698799805159,
        "tmdate": 1699637117312,
        "mdate": 1699637117312,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cDrUEMMV0X",
        "forum": "hkQOYyUChL",
        "replyto": "hkQOYyUChL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8876/Reviewer_pR1P"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8876/Reviewer_pR1P"
        ],
        "content": {
            "summary": {
                "value": "This paper presents an empirical study that unsafe examples are more likely to be forgotten during fine-tuning and that the ability emerges with larger-scale LMs. The authors utilize this empirical finding and propose a novel approach, ForgetFilter, to identify harmful training example which reduces harm in learned models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The analysis is quite extensive. The authors performed study over multiple model architectures, ranging from smaller ones to larger ones, and evaluated various types of unsafe examples (bias, toxicity, harmfulness) and observe consistent findings. \n- The proposed approach, ForgetFilter, is simple and effective. I appreciate author's evaluation of long-term safety where the proposed approach clearly outperforms alternative methods."
            },
            "weaknesses": {
                "value": "- I believe the authors should discuss related works that filter training data / perform data selection based on learning dynamics like frequency of forgetting. \n\nSome papers:\n\n[1] Maini et al. Characterizing Datapoints via Second-Split Forgetting, NeurIPS 2022\n\n[2] Swayamdipta et al. Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics. EMNLP 2020\n\n- I wonder whether in practice \"safe examples\" are always readily available for a fine-tuning task. For example, if I download a random dataset D from the web without having a safe dataset of the same task as D, can I still apply ForgetFilter? Is this setup practical?"
            },
            "questions": {
                "value": "- We see from Figure 2 that up to 80% of harmful examples will be forgotten in the end. But what is the proportion of harmful examples among all the forgotten examples?\n\nI am asking the question above because I am a bit surprised to see applying ForgetFiltering improves downstream performance as well in Table 2. I thought ForgetFiltering may also remove training examples that are not harmful mistakenly. Can you explain side effects that is happening on downstream performance?\n\n- See the \"weakness\" part for my question about applying ForgetFilter when no safe dataset is available."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8876/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698826080695,
        "cdate": 1698826080695,
        "tmdate": 1699637117182,
        "mdate": 1699637117182,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "llylpPoOUb",
        "forum": "hkQOYyUChL",
        "replyto": "hkQOYyUChL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8876/Reviewer_qeKd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8876/Reviewer_qeKd"
        ],
        "content": {
            "summary": {
                "value": "The authors first analyze if the language models can forget unsafe content on fine-tuning. For this the authors first fine-tune a pre-trained language model on a noisy dataset with both safe and unsafe content and then fine-tune this model on safe data and analyze if the model is able to forget the classification of the unsafe data points in the first fine-tuning. Authors name the second fine-tuning stage as safety review. The author utilize different datasets to study the effect of presence of bias, toxicity and harmfulness, and conclude that during safety review the model can easily forget the unsafe content. Motivated by this, the authors propose a forget filter algorithm, which analyzes which samples are forgotten first during the safety review. The ones which are forgotten first are unsafe samples and therefore can be removed from the noisy dataset to make it safe. In the considered setup, the authors demonstrate that such a method could be used to filter out the data and make the models safer."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "* The authors have attempted to make models safer and better aligned. This is an important question in the safety community.\n\n* The experiments attempted by the authors are interesting"
            },
            "weaknesses": {
                "value": "Major Weaknesses:\n\n* The authors have only presented the case where there is presence of limited amount of safe and unsafe data. In real worlds the models are trained on internet scale corpus where the data is present from different domains and distributions. It is likely that on safety review the model would forget the out of distribution samples from the safety review finetuning dataset rather than explicitly forgetting the unsafe datapoints. Since the authors have limited their dataset to limited amount of safe and unsafe data points, it might give an illusion that the model can actually unlearn the unsafe datapoints. Therefore a more rigorous evaluation where the authors can also consider different sources and domains of safe and unsafe data, should be done.The scalability of the proposed method to filter the features from the pre-training dataset is not clear. \n\nMinor weaknesses:\n\n* The authors should try to perform some attacks like jailbreaking on the models fine tuned on the filtered dataset. If the unsafe content can be filtered from the dataset, then the model should not be jailbroken easily.\n\n* A recent work [1] also uses the concept of second fine-tuning in order to filter out noisy samples. The idea of the proposed approach seems to be very similar to this. It would be nice, it the authors could look into this work [1].\n\n[1] Maini, Pratyush et al. \u201cCharacterizing Datapoints via Second-Split Forgetting.\u201d ArXiv abs/2210.15031"
            },
            "questions": {
                "value": "I would request the authors to kindly address the comments in the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8876/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699053548009,
        "cdate": 1699053548009,
        "tmdate": 1699637117043,
        "mdate": 1699637117043,
        "license": "CC BY 4.0",
        "version": 2
    }
]