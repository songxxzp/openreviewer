[
    {
        "id": "WZgHO4PFTI",
        "forum": "xibcBSuuq0",
        "replyto": "xibcBSuuq0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9370/Reviewer_4uBa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9370/Reviewer_4uBa"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the problem of trembling hands in multi-agent systems, namely the negative effect that exploration has on the coordination between agents. This effect is particularly evident when $\\espilon$-greedy policies are used as template policies. This work proposes a method to compute a template policy to be followed, instead of greedy policies, and offers some empirical evidence that the proposed method can be competitive on some experimental settings."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The trembling hands problem plays a central role in multi-agent coordination and the idea to follow specifically designed policies instead of more standard policies might lead to original results."
            },
            "weaknesses": {
                "value": "Unfortunately, the limitations of this work are extensive and I believe structural. The exposition is the main factor concerning this feedback, both from the point of view of the rhetorics and for the clarification of the original contributions. Here is a (non-exhaustive) list of points:\n- In the abstract it is claimed that [you] \" find that $\\epsilon$- greedy policies can be deemed...\", it is unclear how and why this was not already known. In its second part unclear. How do you compute such policies? What do you mean by \"plan an existing optimal policy\"? The description of what was done is unclear to me, and how this was done is absent.\n- The related works section addresses the background rather than the related works, and the background is insufficient in the exposition to provide tools to understand what will be done later. Trembling Hands Nash Equilibria are never defined, for example. This leads to the fact that in the proposed method, it was unclear to me what portions of the whole regime are proper contributions of the work and what are not. \n- The Theoretical Analysis is absent, meaning that in the way it is done is mostly unclear what it should suggest. \n- The Experimental Evaluation suggests some cases of competitiveness but does not compare the methods from a computational point of view, which I believe would help understand the pros and cons of the proposed method. Finally, it was not clear to me how the hyper-optimization of the Sota algorithms used as baselines was done, both in the standard case and in the SDD-augmented case.\n- A scientific analysis of the limitations would be needed.\n\nFinally, some English phrasing is wrong and some typos are present (for example there should be an $\\epsilon$ at the 9th line of the first page I believe)"
            },
            "questions": {
                "value": "Unfortunately, the limitations seem extensive, and I believe a refactoring of the work is needed, I hope the comments suggest the portions of the work to be addressed, but I am open to further provide insights and discuss."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9370/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9370/Reviewer_4uBa",
                    "ICLR.cc/2024/Conference/Submission9370/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9370/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698596523366,
        "cdate": 1698596523366,
        "tmdate": 1700682941355,
        "mdate": 1700682941355,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zDGIRGC8oM",
        "forum": "xibcBSuuq0",
        "replyto": "xibcBSuuq0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9370/Reviewer_3kTw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9370/Reviewer_3kTw"
        ],
        "content": {
            "summary": {
                "value": "In order to alleviate the Trembling Hands Nash Equilibrium solution caused by the $\\varepsilon$-greedy method in multi-agent reinforcement learning, this paper proposes a Stable Prefix Policy (SPP). SPP can rebalance the exploration and exploitation process when the policy of agents is close to the optimal policy during the training process. The specific method is to implement a Monte-Carlo Trajectory Tree (MCT$^2$) to preserve the structure of previous trajectories, which can plan the existing optimal trajectory template. When agents follow this template during rollouts, the target value is assembled with other target values with the same trajectories. When the agents drop out from the template, the $\\varepsilon$-greedy method is activated afterward. SPP can be applied to any value decomposition framework, and experimental results in SMAC and MPE show that it can improve the performance of the basic algorithm."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper introduces the concept of the trembling hands into cooperative multi-agent reinforcement learning, which is reasonable and novel. The two didactic tasks in the introduction section fully demonstrate that the Trembling Hand Perfect Nash Equilibrium does exist in multi-agent tasks, which provides sufficient reasons for the proposal of the Stable Prefix Policy.\n2. This paper implements MCT$^2$, which can plan an existing optimal trajectory (EOT) based on the trajectories in the replay buffer. SPP calculates the target value for TD update by comparing the actual trajectory of the agent with EOT, which is indeed a very novel approach.\n3. Key resources (proofs, code, and replay videos) are available, and sufficient details are described such that an expert should be able to reproduce the main results.\n4. The experimental results are thoroughly analyzed. For example, The dropout time step ratio in Figure 7 illustrates the working mechanism of SPP and is intuitive."
            },
            "weaknesses": {
                "value": "1. The proposed method is based on the premise that agents should be capable of finding a policy toward success from historical interactions. In other words, SPP relies heavily on the performance of the underlying algorithm.\n2. The trembling hands is a concept in multi-agent games, but this paper only provides solutions in cooperative scenarios (Dec-POMDP problems). At the same time, SPP is only applied to value decomposition methods.\n3. MCT$^2$ introduces more hyperparameters, which increases the difficulty and workload of hyperparameter tuning.\n4. The proposed method was only evaluated on SMAC (the description of the experimental results in MPE is skimpy and unconvincing). SMAC is a popular multi-agent experimental platform but has been pointed out to have many shortcomings [1]. More and more researchers in the MARL community advocate conducting experiments in multiple different domains to evaluate the proposed algorithm comprehensively [2].\n\n**Reference**\n\n[1] Ellis, Benjamin et al. SMACv2: An Improved Benchmark for Cooperative Multi-Agent Reinforcement Learning. 2022.\n\n[2] Gorsane, R. et al. Towards a Standardised Performance Evaluation Protocol for Cooperative MARL. 2022."
            },
            "questions": {
                "value": "1. What is the value of the hyperparameter $t_{inter}$? How does its value affect the performance?\n2. The target value $y_t$ in vanilla QMIX is $y_t = r_t+\\gamma\\max_{a^{t+1}}Q_{tot}(s^{t+1}, a^{t+1} )$, which is related to $s^{t+1}$. Why is $y^t$ still related to $s^t$ in Eq. (3)?\n3. Is there any theoretical basis to prove that $Q^t_{assem}$ is more accurate than the original $Q_{tot}$?\n4. I think that in some scenarios, the SPP variant may be more likely to fall into a local optimal solution. Suppose that in such a scenario, agents can easily access the state corresponding to the suboptimal solution, while the state corresponding to the global optimal solution is in the opposite direction and relatively difficult to access (for example, further away from the initial position of the agents). The SPP variant may directly give up early exploration and find it difficult to converge to the global optimal solution. Of course, the above issue can be alleviated by adjusting $c_{ucb}$, but this requires sufficient prior knowledge."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9370/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9370/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9370/Reviewer_3kTw"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9370/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698651891023,
        "cdate": 1698651891023,
        "tmdate": 1700549348040,
        "mdate": 1700549348040,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zwpI0TjvXj",
        "forum": "xibcBSuuq0",
        "replyto": "xibcBSuuq0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9370/Reviewer_RkkR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9370/Reviewer_RkkR"
        ],
        "content": {
            "summary": {
                "value": "In order to balance between exploration and exploitation during the training process, the authors encourage the policy to follow the optimal trajectory as planned by a Monte-Carlo Trajectory Tree (MCT\u00b2). The MCT\u00b2 is built upon historical trajectories, wherein states are organized into clusters via KMeans clustering. Within the MCT\u00b2 framework, state values within the same cluster node are concurrently updated. The authors leverage PUCB values to find the optimal path across these clusters. During the rollout, when the actual state (cluster) diverges from the predicted state (cluster), the policy adopts an \u03b5-greedy approach to facilitate exploration.\nExperiments conducted within the SMAC benchmark show that the proposed method accelerates training and can be integrated into various MARL algorithms, including QMIX, QPLEX, and OW_QMIX."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors innovatively apply Monte-Carlo Tree structure into MARL context, leading to increased training speed. The proposed method may be applied to various existing MARL algorithms, thereby potentially contributes to the field of MARL research."
            },
            "weaknesses": {
                "value": "The experiment results do not conclusively demonstrate the effectiveness of the proposed method. In Figure 8, the performance of the proposed policy closely mirrors that of the original QMIX implementation. I would suggest the authors to test on more challenging MARL benchmarks, though those benchmarks often require more exploration, which may pose challenges for the proposed method.\n\nAlso, many MARL algorithms already suffer from a lack of exploration. The proposed method, in its pursuit of faster convergence, makes the additional trade-off of further diminishing exploration in favor of exploitation. This strategy necessitates careful consideration due to the potential consequences it may have on the algorithm's overall effectiveness."
            },
            "questions": {
                "value": "- In Section 6, the authors claim that the proposed method can be applied to the critic training in Actor-Critic MARL alrogithms. Can you briefly describe how to implement the proposed method in, say, MAPPO? And what is the performance improvement when applying to MAPPO?\n- In the matrix game presented in Section 1, should the $epsilon$ for player 1 be 0.1?\n- Can the proposed method be applied to scenarios with continuous action spaces?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9370/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699609932967,
        "cdate": 1699609932967,
        "tmdate": 1699637178595,
        "mdate": 1699637178595,
        "license": "CC BY 4.0",
        "version": 2
    }
]