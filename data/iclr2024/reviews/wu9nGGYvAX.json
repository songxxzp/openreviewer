[
    {
        "id": "ExoTQhmfKn",
        "forum": "wu9nGGYvAX",
        "replyto": "wu9nGGYvAX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8652/Reviewer_Vcjk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8652/Reviewer_Vcjk"
        ],
        "content": {
            "summary": {
                "value": "This work looks at whether deep neural networks can learn generalizable same-different relations. The authors extend prior work by testing different architectures like ViT and training schemes like CLIP. In addition, they test a variety of datasets (including one containing natural objects) and evaluate out of distribution generalization. They show strong in and out of distribution same-different generalization using a CLIP pre-trained ViT. Finally, the authors assess color and texture bias, showing that CLIP pre-training and fine-tuning on shape-centric datasets reduces color and texture bias."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is well written. The authors clearly state how their work addresses limitations of prior studies on same-different visual-relations. \n\nThe authors contributed a number of new evaluations. These include: testing out-of-distribution generalization of same-different relations, testing ViT models, testing CLIP pre-training, and evaluating both abstract and natural object datasets.  \n\nThe finding showing strong same-different relations using a CLIP pre-trained ViT is novel and interesting to me. I appreciate how the authors carefully varied parameters such as architecture and dataset to test their hypotheses. In addition, the analyses of texture and color bias provided new insights into the factors that contribute to learning generalizable same-different relations. The authors did a good job of motivating the analysis by citing prior work."
            },
            "weaknesses": {
                "value": "Generally, the idea of testing same-different relations in deep neural networks is not new. The authors make an interesting contribution in studying out of distribution generalization, but the work is not especially novel.\n\nAlong those lines, I appreciate that the authors tested models beyond the usual ImageNet trained CNN, but they only extend the study to a ViT and CLIP pre-training. I think the work could have been strengthened by testing more widely. For example, self-supervised pre-training could have been examined or even models that have been shown to align well with human visual processing in other areas (such as top-performers on metrics like BrainScore).\n\nRegarding the datasets, I agree that the authors extend prior work by testing more natural objects in addition to the standard shape datasets. However, I would argue that these datasets are still highly unnatural. They do not contain the regularities and context contained in natural scenes. I think the study could have been enriched by using even more natural stimuli, such as full images, but I recognize that the same-different task is harder to set-up in that scenario."
            },
            "questions": {
                "value": "I am curious about why the authors chose to evaluate CLIP pre-training over other methods. I am wondering why you did not test more pre-training methods and if there was something particularly interesting to you about CLIP.\n\nCould the authors explain more about their rationale in choosing the datasets they evaluated? In particular, I am curious to hear your thoughts on the point I made about using more natural scene images.\n\nFinally, could the authors explain the significance of their study to a broader representation learning audience? My main hesitation for strongly accepting this paper is that the findings seem incremental and niche to a subset of cognitive science.\n\nI generally liked the paper and would be open to raising my score if questions and weaknesses are addressed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8652/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8652/Reviewer_Vcjk",
                    "ICLR.cc/2024/Conference/Submission8652/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8652/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698175598229,
        "cdate": 1698175598229,
        "tmdate": 1700711789708,
        "mdate": 1700711789708,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5MpfOB6zpY",
        "forum": "wu9nGGYvAX",
        "replyto": "wu9nGGYvAX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8652/Reviewer_VFaW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8652/Reviewer_VFaW"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a list of experiments to examine the generalizability of two kinds of deep neural networks, i.e., ResNet and ViT, and whether they are pretrained by CLIP, in classifying same-different visual relations onto four datasets. The conclusion is that ViT pretrained on CLIP can learn a pixel-level same-different relation that is generaliable to out-of-domain datasets. The experiments also find fine-tuning the model by abstract shapes will introduce more OOD generalization for the mentioned same-different relation prediction."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. In my opinion, understanding the ability to learn generalizable same-different visual relations is important, especially if one would like a DNN to also perform basic logical operations in addition to instance-level perceptions. This paper gives a well-organized experiment-based summary that attempts to answer how and why recent DNN architectures and pretraining datasets can perform generalizable same-different visual relation classification. \n2. The paper is well-written and easy to follow. A comprehensive list of experiments are conducted to support its conclusions."
            },
            "weaknesses": {
                "value": "I am afraid that the definition of same-different visual relation should be beyond comparing objects at a pixel level. Thus, the observations and analyses may be of limited insights. The reasons are explained below:\n- In the human-visual system, two objects that are considered the same are usually based on some specific semantic and/or attribute similarities, rather than counting how many pixels that exactly have the same values. It means the dataset/task should define same-different visual relations by more criteria, such as geometry, texture, color, categories, identities, and etc. Moreover, the dataset should consider more visual distortions in real scenarios, such as cluttered background, color jittering, slight/moderate shape distortion, or allowing overlapping between objects. Sec. 4.2 tries to dissociate color, texture and shape, but more test scenarios can be included.\n- Defining the same-different visual relations at the pixel level actually gives quite a strong clue that can be captured by DNNs. Possibly it is the reason why the predication results are almost saturated (even can be achieved 100% accuracy if fine-tuned) by different pretrained models, especially when the texture similarity is involved (i.e., SHA and NAT datasets). Therefore, some conclusions drawn from these experiments may not be sound enough. For example, CLIP pretrained ViT models can achieve 100% in-distribution and nearly 100% out-of-distribution test accuracy is not a universal conclusion, but depending on the carefully designed testing datasets. The conclusion that fine-tuning abstract shape leads to a more generalizable predication may come from that all the four datasets (SQU, ALPH, SHA, NAT) can classify the same-different relations by shape consensus. If evaluating OOD generalization onto texture-based  relations, the observations may be quite different."
            },
            "questions": {
                "value": "In the paper weaknesses, I have mentioned several questions that should be addressed in the rebuttal. Here is one more question:\n- Sec. 3.2: As what I have mentioned, the OOD generalization in this paper may come from how well the shape is extracted. The closeness of stimuli may not be a reliable cue to better extraction of shapes, and thus is not well correlated with OOD generalization. The authors apply another dataset containing patches with random noise to prove that closeness of stimuli is not a perfect correlate of OOD generalization. But classifying this dataset relies more on texture similarity, thus it is reasonable that models fine-tuned on this dataset exhibit weaker generalization. I am interested in another dataset combining SQU with noises filling in each object. Possibly this dataset has more close stimuli, but also a higher OOD generalization."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8652/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698411162303,
        "cdate": 1698411162303,
        "tmdate": 1699637084042,
        "mdate": 1699637084042,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QrsZ8npHdC",
        "forum": "wu9nGGYvAX",
        "replyto": "wu9nGGYvAX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8652/Reviewer_LAfe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8652/Reviewer_LAfe"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on learning a same-different relation from images and analyzes 2 networks (ResNet and ViT) and 3 different pre-training strategies (none/from-scratch, pre-trained on ImageNet, pre-trained on CLIP). Extensive experiments show that CLIP-pretrained ViT models can generalize same-different relations while prior works need strong inductive biases (e.g., separately processing two objects in the image) when only ResNet models are studies."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is clearly written.\n2. The authors present their empirical results in a sound way. For example, in section 3.2, they provide additional experiments fine-tuning on random noise to illustrate that \"closeness\" of stimuli is not a perfect correlate of OOD generalization."
            },
            "weaknesses": {
                "value": "1. It is hard to tell how much the paper can contribute to the community. In my opinion, this work is an empirical study. For this category of works, the criteria are usually a) how many new observations are found, how surprising they are, and how useful they are for future works; b) whether the study is systematic and convincing; c) whether a new perspective is proposed or a new methodology is used to study the problem. I think this paper focuses more on a) and b). I am not sure how novel and important the findings introduced in this paper are, since the authors study a network architecture (ViT) which are actually already widely used in VLM models and tasks that require relation understanding like VQA. **I suggest the authors should clarify the generality and importance of their findings if only same-different relation is studied, or try tasks beyond same-different relation.**"
            },
            "questions": {
                "value": "1. Figure 5 shows that when randomly initialized ViT is trained on *Masked Shapes*, it generalizes to all datasets (including in-dist one) poorly. According to the authors' claim \"fine-tuning on abstract shapes that lack texture or color provides the strongest out-of-distribution generalization\", if trained on *Masked Shapes*, the model should generalize the best, which is in contrast to results.\n2. In Appendix B.1, \"Because training datasets are constructed by sampling random objects, the exact objects used be\ntween the original, grayscale, and masked datasets are not the same\" is a little confusing to me. Given the description above this claim, it seems that the authors can convert the original datasets to grayscale or masked versions with the same objects.\n3. As ViT takes image patches as inputs, it natively \"segments\" objects in the image. Have the authors tried to change object sizes in the image or experiment with other transformer architectures like Swin-Transformer?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8652/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698543410317,
        "cdate": 1698543410317,
        "tmdate": 1699637083908,
        "mdate": 1699637083908,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RVVmOlYrQS",
        "forum": "wu9nGGYvAX",
        "replyto": "wu9nGGYvAX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8652/Reviewer_GpvX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8652/Reviewer_GpvX"
        ],
        "content": {
            "summary": {
                "value": "This work investigates the learning and generalization of some existing architectures on the same-different relation image data."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The work attempts to better understand same-different challenges for current architectures. \nThe work can provide useful conclusions for the same-different task."
            },
            "weaknesses": {
                "value": "The novelty and contributions of this work are limited. The work fine-tuned some pretrained existing architectures to improve the same-different recognition performance. The work is also very incremental in relation with the work:\n Guillermo Puebla and Jeffrey S Bowers. Can deep convolutional neural networks support relational reasoning in the same-different task? Journal of Vision, 22(10):11\u201311, 2022. \nI think the contributions of this work are not enough for publication at such strong venue. In my opinion this work is more appropriate as a workshop paper."
            },
            "questions": {
                "value": "See above my concerns"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8652/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698766406446,
        "cdate": 1698766406446,
        "tmdate": 1699637083788,
        "mdate": 1699637083788,
        "license": "CC BY 4.0",
        "version": 2
    }
]