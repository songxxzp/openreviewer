[
    {
        "id": "GOnS9xnwxP",
        "forum": "70PPJo3DwI",
        "replyto": "70PPJo3DwI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4505/Reviewer_RF1L"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4505/Reviewer_RF1L"
        ],
        "content": {
            "summary": {
                "value": "The paper considers the problem of out-of-federation generalization in the context of federated learning. The goal is to train a model able to generalize well to unseen clients during training. In order to achieve this goal, the paper proposes Topology-aware Federated Learning (TFL), a framework combining two ideas: robust/agnostic federated learning (Deng et al., 2020; Mohri et al., 2019), and federated multi-task learning (Smith et al., 2017; Vanhaesebrouck et al., 2017). \n\nTFL considers an optimization problem over the model parameters and importance of each client, as well as the client topology capturing the clients' relationship. In order to solve this optimization problem, TFL acts iteratively: at each iteration, the  model parameters and importance of each client are optimized for a fixed topology, then the topology is updated for fixed model parameters and importance vector.\n\nThe paper curates two out-of-federation benchmarks using real-world healthcare data, and empirically evaluates the performance of TFL on these and standard benchmarks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper curates two out-of-federation benchmarks using real-world healthcare data. Both datasets could be beneficial in future work. \n- The numerical experiments are extensive and covers many aspects of the proposed learning framework."
            },
            "weaknesses": {
                "value": "- The technical novelty of the paper is limited. The paper simply combines the ideas from  robust/agnostic federated learning (Deng et al., 2020; Mohri et al., 2019), and federated multi-task learning (Smith et al., 2017; Vanhaesebrouck et al., 2017).\n- The notation and the theoretical results are not rigorous: \n    - The definition of $\\Theta$ is not consistent when moving from (2) to (3). In (2), we optimize the parameters of one global model, while in (3), we optimizer the individual parameters of each client. \n    - The function $F$ as defined in (6) ought to be iteration-dependent since $\\mathbf{p}$ has the potential to vary from one iteration to another.\n    - The function $F$ is defined in two different manners in (3) and (6). The function $F$ as defined in (3) should depend on $W$, while the function $F$ defined in (6) should depend on $\\mathbf{p}$.\n    - In light of the previous point, it is unclear what is the statement of Theorem 1. \n    - I am uncertain about how to interpret Theorem 2.\n- Other minor issues: \n    - In the abstract and introduction, the paper conveys an initial emphasis on healthcare data. However, I believe it would be more effective if the paper avoids an exclusive focus on healthcare data, considering that the proposed approach holds broader applicability. Instead, I recommend the authors highlight healthcare as one potential use case rather than positioning it as the sole focus of the paper.\n    - In Figure 2, it is unclear why FedProx need more communication in comparison with FedAvg. \n    - In the opening of Section 2, the paper asserts that the empirical risk is equal to the population risk, a statement generally incorrect. I believe the authors intended to convey that the population risk is approximately equal to the empirical risk."
            },
            "questions": {
                "value": "I am of the opinion that the current theoretical results are not entirely accurate. Should the authors fail to refute my assertion, I recommend considering the removal of these theoretical results from the paper. However, if the authors are able to either demonstrate the correctness or agree to exclude the theoretical results, I am open to revising my evaluation positively."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4505/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4505/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4505/Reviewer_RF1L"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4505/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698770068741,
        "cdate": 1698770068741,
        "tmdate": 1700640402037,
        "mdate": 1700640402037,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AEVz8E39rA",
        "forum": "70PPJo3DwI",
        "replyto": "70PPJo3DwI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4505/Reviewer_L1rQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4505/Reviewer_L1rQ"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors proposed a new optimization framework for solving out-of-federation (OOF) problems in federated learning. In particular, they proposed to alternatively optimize a client graph topology and minimize the overall weighted loss whose weights closely dependent on the graph topology. In this case, the new optimization framework can leverage the influential clients and also the \u201coutliers\u201d. They conducted comprehensive numerical experiments to compare the proposed framework with several existing baselines on both real-world datasets and some FL benchmark datasets. The proposed framework has marginal improvements over existing baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Overall, the paper is clear and easy to understand. The proposed framework seems novel. The authors conducted many empirical experiments to demonstrate the performance of the proposed framework."
            },
            "weaknesses": {
                "value": "1. As the authors has mentioned, solving the client topology learning problem requires $O(N^2)$, which does not scale well when $N$ is large. Although the authors provide an alternative solution: clustering based method, no experiments has conducted under such scenarios.\n2. In the existing literature, there are a few papers have discussed how to measure the similarity between two clients\u2019 local distribution, for example, using prototype model. How's the method used in this paper compared with those ones?\n\n> Tan, Y., Long, G., Liu, L., Zhou, T., Lu, Q., Jiang, J., & Zhang, C. (2021). FedProto: Federated Prototype Learning across Heterogeneous Clients. AAAI Conference on Artificial Intelligence.\n\n3. I wonder if the proposed framework still works when there is/are an/some adversarial client(s) presented in the FL system. Sometimes the adversarial behavior may occur due to connectivity issues."
            },
            "questions": {
                "value": "1. Page 4, line 4, \u201cIn the objective function, the first term follows the same spirit of Equation 3\u2026\u201d, a typo? Is this Equation 2?\n2. Figure 4, do clients use the same training algorithm and same training hyperparameters? If the clients uses different algorithm or training hyperparameters, I doubt their model parameters will be similar even if they have similar local data distribution.\n3. In Equation 4, why using cosine similarity and $\\ell_0$ distance? In figure 4, it seems that other differentiable  distances also follow the same trend as cosine similarity."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4505/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4505/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4505/Reviewer_L1rQ"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4505/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698775373276,
        "cdate": 1698775373276,
        "tmdate": 1699636426663,
        "mdate": 1699636426663,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kI41D09F40",
        "forum": "70PPJo3DwI",
        "replyto": "70PPJo3DwI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4505/Reviewer_Q4J6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4505/Reviewer_Q4J6"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the OOF generalization problem in federated learning, i.e., whether a trained global model can generalize to new clients that do not participate in FL training. The author propose a method to construct a client similarity graph, and emphasize those \u201cinfluential\u201d clients in the graph. The algorithm is empirically shown to be effective and outperforms a line of federated DG baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is well-written and easy to follow. The algorithm is clear. \n2. The experiments are extensive and verify the superior performance of the proposed algorithm."
            },
            "weaknesses": {
                "value": "1. A detailed explanation of the motivation of the algorithm will be beneficial. For example, what is the motivation behind emphasizing influential client? Is it because the OOF clients\u2019 distribution are more likely to be similar to these clients? If so, why is that the case? If not, why up-weighting these clients? \n2. The author claim that solving Equation 4 with $l_0$ can be NP-hard. However, it seems to be wrong. In this objective function, the optimization for each $w_{k, l}$ is purely disentangled, since $\\\\|W\\\\|\\_0 = \\sum_{k, l} 1\\\\{w\\_{k, l} > 0\\\\}$. And the solution is just very similar to the proposed method, if $\\|W\\|_0$ is weighted by epsilon. This does not hurt the soundness of the method."
            },
            "questions": {
                "value": "1. How the wall-clock time is calculated in Figure 2? Usually, wall-clock time is influence by both computation and communication cost, and their weights depend on the bandwidth, delay, device, \u2026 I believe number of communication rounds or total # bits transmitted could be a better metric for communication efficiency. \n\nMinor: page 4 line 4: Equation 3 -> Equation 2"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4505/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4505/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4505/Reviewer_Q4J6"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4505/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698813819226,
        "cdate": 1698813819226,
        "tmdate": 1699636426539,
        "mdate": 1699636426539,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lnjNigYAE4",
        "forum": "70PPJo3DwI",
        "replyto": "70PPJo3DwI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4505/Reviewer_Nx6V"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4505/Reviewer_Nx6V"
        ],
        "content": {
            "summary": {
                "value": "This work tackles the problem of out-of-generalization in FL where the trained model from conventional FL performs poorly for clients outside of the current federation with different distributions. The work proposes to leverage client topology where the relationships across the clients are learned with a weight matrix as we do for graphs. The relationships are learned with pair-wise similarity with only the last few layers of the clients' models. The authors claim that this is communication efficient. The authors include experimental results on a variety of datasets and compare the performance with baselines such as FedAvg, FedProx, DRFA."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The work investigates a relevant problem in FL where clients with local data coming from data distributions different from which the global model is trained on suffer from bad performance.\n\n- The work builds upon previous work on graph centrality to propose client centrality and topology for improving previous DRFL methods.\n\n- The work provides experimental validation on extensive number of datasets and different baselines."
            },
            "weaknesses": {
                "value": "- A main concern I have is regarding the part where we have to learn the client topology through the similarity measures in eq. (3). This requires training over all of the clients' models (even if it is some of the last layers) which can incur large computation overhead for cross-device FL scenarios where the number of clients can easily range to millions of clients. While the authors argue that models can be freely shared among clients and this can address privacy issues, I am unsure why this can ensure privacy. I think it will do the opposite. \n\n- Another concern I have is regarding eq.(5) where the authors try to figure out the influential clients that represent the distribution of out-of-federation clients, along with the worst distribution clients. Wouldn't this lead to potential bias to the distribution of the influential clients? Let us assume there is a setting where there are mainly three groups of clients with different distributions within the out-of-federation group. Wouldn't this lead to the algorithm biasing the model towards one of the distribution and not performing well for the other two groups?\n\n- Lastly, regarding the experimental results, the authors argue that the method is communication efficient since the achieved targeted performance occurs in an earlier communication round for the proposed method compared to other methods. However, I wonder if this is actually the case for the actually communicated number of parameters (for example in Figure7)?\n\nOverall due to these concerns I am leaning towards rejection, but I look forward to the discussions with other reviewers and authors."
            },
            "questions": {
                "value": "See weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4505/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698986083790,
        "cdate": 1698986083790,
        "tmdate": 1699636426433,
        "mdate": 1699636426433,
        "license": "CC BY 4.0",
        "version": 2
    }
]