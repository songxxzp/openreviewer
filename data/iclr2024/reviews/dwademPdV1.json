[
    {
        "id": "WaTYAUKq5C",
        "forum": "dwademPdV1",
        "replyto": "dwademPdV1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3936/Reviewer_AAoD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3936/Reviewer_AAoD"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to identify the causes of a model\u2019s unfairness by leveraging the method of influence function. Instead of only detecting the training samples that induce unfairness directly and passively, the article takes a step further and tries actively to replace the \u201chigh-influence/biased\u201d samples with new counterfactual samples generated by designing an algorithm. This solution is interesting and intuitive. In addition, the framework proposed in this paper is general and has the potential to be applied to various new scenarios."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. this paper is well-written; \n2. the topic is both intriguing and of practical significance.\n3. the main idea is intuitive and easy to understand; \n4. the algorithm design is reasonable."
            },
            "weaknesses": {
                "value": "1. There are no theoretical guarantees about the methods used to generate counterfactual samples."
            },
            "questions": {
                "value": "I do not have any major concerns about this work in technical details or presentation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3936/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3936/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3936/Reviewer_AAoD"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3936/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698749150377,
        "cdate": 1698749150377,
        "tmdate": 1699636354265,
        "mdate": 1699636354265,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "shC8wZgiTD",
        "forum": "dwademPdV1",
        "replyto": "dwademPdV1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3936/Reviewer_rxAc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3936/Reviewer_rxAc"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces tools from the influence function literature to quantify algorithmic fairness."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "See below for contextual discussion of strengths."
            },
            "weaknesses": {
                "value": "This paper uses influence functions to quantify fairness. The topic is one of the paper's strengths; although influence functions and counterfactual fairness models are well-established, the connection between the two is (to my knowledge) less developed. At least for me, for a paper combining two well-worn topics, it is important that the paper does so in an exceptionally clear or general way. The paper has potential for doing so but may be somewhat lacking in its current framing (at least according to my current understanding of the paper and contextual literature). For example, much of the discussion seems to assume a relatively limited class of ML problems (e.g., binary classification). Also, the relationship between ''Concepts'' and ''Fairness'' could (in my view) use a clearer elaboration. ''Transforming the sample based on certain concepts of training data'' seems far more general (this could concern any number of modifications). This framing somewhat complicates (at least this readers') analysis of the paper. I would consider revisions that streamline some of this. The introduction of the override operator also wasn't particularly obvious to me (e.g., ''override() operator counterfactually sets the value of the concept variable to a different c0.'' sounds like the do() operation, although I see an attempt to distinguish). \n\nOverall, I see promise in the paper, but (at least in my opinion), certain limitations exist in terms of clarity. Also, the methods don't seem to have a comparison benchmark (even a naive one) which would help readers understand the contribution over any existing approaches in this context. \n\nA few more focused comments. \n\n(1) The paper uses the term ''fairness'' throughout. It would be helpful to provide an explicit definition early on. At times, it seems like the term is more qualitative (e.g., ''Influence Functions for Fairness''; ''They can all contribute to unfairness''). At other times, the term seems to be in some sense quantitative (i.e., ''fairness can improve''; ''fairness becomes worse''). Although the meaning may be clear to readers embedded in the fair ML literature, I think it would help to fix ideas early on for readers. Having said that, what constitutes ''fair'' (in at least some of its meanings) is, of course, subject to social values. \n\n(2) It could help to discuss some of the challenges of computing the Hessian vector product (e.g., in high dimensions) and to emphasize why bypassing the Hessian calculation via the HVP can be useful. Would the influence function approach break down, e.g., when the parameter number ranges in the billions? More generally, based on the paper's existing framing, I had a hard time understanding some of its limitations, which are not particularly emphasized in the paper (the ''Conclusions and Limitations'' section is 10 lines long.)"
            },
            "questions": {
                "value": "(1) Could the authors clarify what is meant by, ''we show that the influence function often identifies the data from the majority group and recommends them to be changed to the minority group''? Presumably, this refers to re-weighting of the training data, but it reads almost like the direct changing/manipulation of observed data values. \n\n(2) Do all of the performance metrics have definitions in the paper? (I don't see one, for example, for the ''Equality of Odds'' metric). If the metrics could be compiled into a table with equations, this reader may have a better understanding of them and the results."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No major ethnics concerns."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3936/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3936/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3936/Reviewer_rxAc"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3936/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698803299385,
        "cdate": 1698803299385,
        "tmdate": 1700748549735,
        "mdate": 1700748549735,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iPbUFpNkGU",
        "forum": "dwademPdV1",
        "replyto": "dwademPdV1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3936/Reviewer_BzqW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3936/Reviewer_BzqW"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces CIF (concept influence for fairness) as a framework to identify potential fairness problems in training data when looking at 1) sensitive attributes 2) features and 3) labels by overriding concepts in each case. The authors provide details on how to change concepts, and then how to define the influence of those concept changes. The authors provide experimental results."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper provides a framework that is very flexible to changing technologies."
            },
            "weaknesses": {
                "value": "This paper seems incremental in its approach. For example, the authors state that the effectiveness of their solution depends on finding a proper transform() which is their work's focus. The then proposed transforms in \"generating counterfactual samples\" use techniques not novel to this paper and pulled from a number of different sources. Although this framework is generally useful, I don't believe it provides enough novelty for this conference."
            },
            "questions": {
                "value": "Could the authors provide more details on the \"overriding X\" experiment w.r.t each of the datasets used. What kind of features where chosen in these cases?\n\nCould the authors please comment on novelty from above?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3936/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3936/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3936/Reviewer_BzqW"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3936/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698806397651,
        "cdate": 1698806397651,
        "tmdate": 1699636354085,
        "mdate": 1699636354085,
        "license": "CC BY 4.0",
        "version": 2
    }
]