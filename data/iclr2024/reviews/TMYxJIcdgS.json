[
    {
        "id": "OR0kjxeKS2",
        "forum": "TMYxJIcdgS",
        "replyto": "TMYxJIcdgS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4388/Reviewer_fH12"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4388/Reviewer_fH12"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the differences between ImageNet and the created LAIONet dataset out of LAION. Through three carefully designed experiments, the authors claim that the information bottleneck explains why ImageNet is less diverse than LAIONet."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The findings about information bottleneck in the paper are very interesting and insightful for future data curation efforts."
            },
            "weaknesses": {
                "value": "1. The abstract states the \"long-held intuition\" that ImageNet images are \"stereotypical, unnatural, and overly simple representations\". I don't find enough references in Section 1.2 and any other sections.\n2. One important difference between ImageNet and LAION is their data sources - the former is from Flickr and the latter is from CommonCrawl. The two data sources should definitely exhibit different levels of data distribution and diversity. The reviewer think this should also be taken into consideration for analysis.\n  - Both DataComp and LAION come from CommonCrawl. In DataComp [1], the images come from various data sources more than Flickr (Fig 13 in [1]).\n\n[1] Gadre, Samir Yitzhak, et al. \"DataComp: In search of the next generation of multimodal datasets.\" arXiv preprint arXiv:2304.14108 (2023)."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4388/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4388/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4388/Reviewer_fH12"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4388/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697942055059,
        "cdate": 1697942055059,
        "tmdate": 1700630782418,
        "mdate": 1700630782418,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pjFsxVOSqp",
        "forum": "TMYxJIcdgS",
        "replyto": "TMYxJIcdgS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4388/Reviewer_fkPT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4388/Reviewer_fkPT"
        ],
        "content": {
            "summary": {
                "value": "- The paper proposes LAIONet, an ImageNet-like dataset created from LAION-400M\n- The dataset is created by filtering out instances with an image-text CLIP similarity of 0.3. Next, the images are selected based on the ImageNet category synset occurence + a high similariy with the text and the synset definition.\n- The paper then analyzes LAIONet and finds that it is distinctly unlike ImageNet -- the intra-class similarity is lower, and the accuracy of ImageNet trained models drops by 5-12% on LAIONet.\n- The paper then shows that the difference is because ImageNet relied on the image content for the selection process, and that relying on just the text captions creates an information bottleneck which mitigates the selection bias"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper looks into the data creation process and how to mitigate biases in the process, which is important for the community\n- The paper is easy to read and understand, all the experiments are explained very clearly"
            },
            "weaknesses": {
                "value": "- The paper claims in Section 1.1 \"Choosing an image reveals nothing more about the image than what can be learned from its textual representation. This powerful conditional independence property limits how much selection can bias the distribution of the image. In contrast, in the case of ImageNet (Figure 2b), there is a link from the image to the selection decision.\". This isn't accurate -- choosing an image gives more information than the text representation which is used for LAIONet selection, namely the CLIP image-text similarity, with a high threshold of 0.3. LAIONet doesn't remove image content from the selection criteria, it just uses a CLIP model to do the image-text based selection instead of humans. There is a lot of focus on LAIONet only relying on texts to get closer to the \"true\" distribution and avoiding bias, whereas LAIONet is getting closer in distribution a dataset of concepts which CLIP recognizes and getting biased towards CLIP's understanding of concepts. It is possible though that CLIP has a different and lower bias than human annotators though but there is no discussion of this. \n- The paper claims that ImageNet uses the image content for selection heavily, and for LAIONet there is an information bottleneck. It claims in Section 1.1 that \"Selecting on the basis of the text caption, therefore, retains much of the entropy present in the image distribution\" -- while this statement would be true theoretically for a noise-free dataset, the paper never touches upon or even considers the fact that LAION is a noisy dataset. Using a noisy dataset will produce a higher entropy and diverse dataset on account of mislabeled images as well. The noise also affects the performance of models -- it is not clear how much does the performance of models drop on LAIONet just because the images are mislabeled? The paper has a fundamental flaw that it simply considers one dimension, diversity, and creates LAIONet to be more diverse, without ever considered the label noise dimension -- diversity and noise are inversely correlated.\n- The paper then mentions in Section 2 \"We found CLIP zero-shot top 1 accuracy to only differ by 2% across datasets. Hence, at least from the CLIP view, LAIONet images are not harder to classify. ...\". This discussion also has a flaw -- CLIP was used to filter the images to begin with, so there is an inherent bias here where the test set was created from the same model which is being evaluated.\n- The section about \"A WEAKER IMAGE-TO-SELECTION LINK MAKES IMAGENET MORE LIKE LAIONET\" also comptely ignores noise and just mentions \"weaker image-to-selection link\", wherein lower MTurk selection frequency results in a distribution closer to LAIONet. There is again a confounding factor at play, which is the noise in labels -- if the MTurk selection frequency is lower, it means that the likelihood of mislabeling is higher. \n- There is a discussion on figuring out whether images were used for selection for the creation of ImageNet (section 4.2, section 4.3). \"These observations reject the hypothesis that the graphs of Figure 2 have the same structure and show a potential leak from the image to the selection.\" -- a leak suggests this was unintentional, whereas it is known ImageNet was created by looking at the images' content. I am not sure what the point / contribution of this discussion is? \n- Also, section 4.3 creates a subset which is not like ImageNet, but also not like LAIONet, this is a third setting where the image isn't used at all since this section doesn't use CLIP based filtering."
            },
            "questions": {
                "value": "- The paper has two limitations which need to be addressed --\n  - There is no discussion of noise at all in the datasets and the paper just talks about diversity. At a bare minimum, all analyses should have shown and compared the prevalence of noise in ImageNet and LAIONet. Only then can any conclusions be drawn which are made in the paper regarding image-to-selection link and / or diversity\n  - The paper ignores the contribution of CLIP thresholding on the creation of LAIONet -- this creates a very strong link to the image content as well in the creation of LAIONet, and also adds a different bias from CLIP. A threshold of 0.3 is very high, and this thresholding is directly connected to the noise and diversity of LAIONet but there isn't any discussion around this either.\n- I am not sure what is the value add of testing the hypothesis whether ImageNet data collection used image content or not when it is known that the image content was used already?\n- The paper also mentions that models perform worse in more frequent classes, but the analysis is only show on LAIONet -- this is a surprising result, given that frequent classes will be seen more often during training, and models are expected to perform on infrequent classes. Does this only happen on LAIONet or does it happen on other datasets as well, specifically ImageNet? It could also be that frequent classes have a different label noise rate on LAIONet?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4388/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4388/Reviewer_fkPT",
                    "ICLR.cc/2024/Conference/Submission4388/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4388/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698715601705,
        "cdate": 1698715601705,
        "tmdate": 1700674047930,
        "mdate": 1700674047930,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "10aPepdvyg",
        "forum": "TMYxJIcdgS",
        "replyto": "TMYxJIcdgS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4388/Reviewer_j7LU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4388/Reviewer_j7LU"
        ],
        "content": {
            "summary": {
                "value": "This paper conducts a comparative analysis between the predominant ImageNet dataset in the computer vision field and the recently widely-used LAION dataset. By analyzing their data collection processes, the intrinsic differences between ImageNet and LAION datasets are highlighted. Heuristically, guidelines for selecting data instances based on information bottlenecks are provided."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Analyzing mainstream datasets helps deepen researchers' understanding of the data. At the same time, it aids the community in designing future datasets with minimal human-induced bias, which in turn helps enhance the generalization performance of models.\n\n- This paper is logically structured, and the conclusions regarding the differences between the ImageNet and LAION datasets are comprehensive. Starting from the inconsistent dataset filtering processes, it further analyzes the differences in intra-class similarity between the two. This leads to the conclusion that the image diversity in the two datasets is inconsistent.\n\n- This paper offers a wealth of visual analysis, which is very helpful in understanding the main conclusions."
            },
            "weaknesses": {
                "value": "- This paper still lacks a central objective. Although a series of analyses point out the differences between ImageNet and LAIONet, both Figure1 and Figure5 seem to indicate that model performance on ImageNet and LAIONet is positively correlated. This suggests that LAIONet doesn't offer additional indicative value for model performance analysis, which is typically the most important for classification datasets.\n\n- Additionally, the ImageNet dataset and the LAION dataset were created at different times and for different purposes. The former emerged before deep learning became mainstream, aiming to provide a broad object-centric benchmark. In contrast, the latter was prepared for the pre-training of current large-scale models. Given that the paper suggests it can provide guidance for the construction of new datasets, and considering that the current processing methods for the LAION dataset (as well as similar datasets like COYO, mC4, etc.) are already being adopted, what specific new recommendations are included?\n\n- Considering the different collection times of the two datasets as mentioned above, is the gap in intra-class similarity related to the distributional shift of internet data? Also, given that ImageNet-1K was derived from ImageNet-22K, would an analysis of ImageNet-22K be more meaningful?"
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4388/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699248855790,
        "cdate": 1699248855790,
        "tmdate": 1699636411922,
        "mdate": 1699636411922,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qKCYuLxsQ2",
        "forum": "TMYxJIcdgS",
        "replyto": "TMYxJIcdgS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4388/Reviewer_UY2S"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4388/Reviewer_UY2S"
        ],
        "content": {
            "summary": {
                "value": "This paper analyzes the difference between ImageNet and the version of LAION dataset recreated with ImageNet classes. The main finding is that, the image selection of the creation process of ImageNet depends partially on images themselves except for text descriptions, leading to smaller intra-class variances and easier tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The viewpoint of connecting and comparing older and newer datasets is interesting. \n- The writing is generally clear and easy to follow."
            },
            "weaknesses": {
                "value": "- The only conclusion of this paper is that ImageNet is more of an easy dataset than LAION because the images are curated dependent on image similarities, which makes images of each class less diverse and has smaller intra-class variances. This conclusion is unsurprising since ImageNet is curated very carefully to exclude outlier examples. \n- I do not see much value of the findings. Visual datasets should not be curated only using text descriptions, which leads to a higher probability of getting wrong images inside the dataset. Thus the findings do not reveal a drawback of ImageNet curation process. On the other hand, the datasets nowadays, like LAION, are mostly not curated using names of classes, while the conclusion of this paper only supports curation using the names of classes, and thus has limited values.\n- This paper does not reveal anything related to the different curation processes of Imagenet and LAION, one for image classification and another for vision-text pretraining, but instead create another ImageNet-like dataset from LAION. Thus the title of this paper is inappropriate."
            },
            "questions": {
                "value": "- Why not pretrain models on both datasets and compare the differences to support your conclusion?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4388/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4388/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4388/Reviewer_UY2S"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4388/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699443513208,
        "cdate": 1699443513208,
        "tmdate": 1699636411853,
        "mdate": 1699636411853,
        "license": "CC BY 4.0",
        "version": 2
    }
]