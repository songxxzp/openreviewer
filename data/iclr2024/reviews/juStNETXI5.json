[
    {
        "id": "VdQ05iTGyW",
        "forum": "juStNETXI5",
        "replyto": "juStNETXI5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5406/Reviewer_B8Me"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5406/Reviewer_B8Me"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors presented TinyStyleWizard, which is a framework for generating specialized dataset for complex style transfers, and then fine-tune small language models on the datasets. The authors also devised some novel evaluation metrics for the tasks. The authors compared their fine-tuned models against baselines on their newly generated datasets, and achieved good results. The authors also performed additional analysis on the relationships between performance and training data volume, as well as generation diversity."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The authors presented a new benchmarks that includes several new datasets for complex style transfers. These datasets could be beneficial for future research in text style transfers.\n\n2. The research focused on enabling small language models to perform text style transfer tasks, which is really interesting because even though large language models can easily perform the same tasks, small language models are much cheaper and less constrained to finetune and use, and how we can utilize large language models to \"teach\" small language models remains challenging."
            },
            "weaknesses": {
                "value": "The paper claims to study \"the potential of small language models in complex style transfer\", but it did not present a clear definition of \"small language model\" or \"complex style transfer\". It is unclear how small the model needs to be to be considered a \"small LM\". It is also unclear which transfers can be considered \"complex\". I believe the authors need to provide a concrete definition for both terms (such as a clear size boundary for \"small LM\" and a clear linguistic definition of \"complex style transfer\"). The paper currently uses both terms vaguely, which makes it hard to accurately assess the significance of the contribution, as there already exists many parallel corpuses of style transfer that could be considered \"complex\", such as formality transfer (GYAFC[1]) or composition of multiple fine-grained text style transfer (StylePTB[2]). The paper could also benefit from evaluating TinyStyleWizard models on these existing and well-defined parallel corpuses to show the generalizability of the proposed methods.\n\nThere is no details on how the TinyStyleWizard models are fine-tuned from their pre-trained state.\n\nIn addition, there are a few serious issues in the presentation of the paper that significantly increases difficulty of reading, including:\n\n(1) The in-text citation style seems to be wrong, i.e. there are no parentheses around each in-text citation, which makes the text much harder to read, especially in the introduction and related work sections where there are a lot of citations.\n\n(2) The Tables are very difficult to comprehend and often there is no clear takeaway messages. See Questions for details.\n\n(3) The ordering of the tables are quite strange. Table 3 is referred in later context than Table 4,5,6. Table 6 appears in the paper above Tables 4 and 5.\n\nReferences:\n[1] https://aclanthology.org/N18-1012/\n[2] https://arxiv.org/abs/2104.05196"
            },
            "questions": {
                "value": "1. I found most of the tables in the paper difficult to understand.\n\n(a) For Table 1, it is unclear to me what the takeaway message should be. Is it used to show the quality of the corpus you collected? Or is it showing that your metric is better? In either case, it is hard to draw conclusions from the bolded numbers in the table.\n\n(b) For Table 4,5,6, why are the baselines different in these settings?\n\n(c) For Table 3, which TinyWizard model is \"ours\"? (i.e. which row from Table 4 corresponds to \"ours\" in Table 3?) You should also bold the best numbers from Table 3, since you bolded best numbers from all other tables.\n\n2. Why do we want \"diversity\" (sec 3.5.4) in style transfer tasks? If the style is precisely defined and the semantic preservation is perfect, then all results should be similar and a \"diverse\" result would either mean poor style accuracy in some transfers or poor semantic preservation in some transfers."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5406/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697492811614,
        "cdate": 1697492811614,
        "tmdate": 1699636548167,
        "mdate": 1699636548167,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4c5dOzeF87",
        "forum": "juStNETXI5",
        "replyto": "juStNETXI5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5406/Reviewer_Yb3s"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5406/Reviewer_Yb3s"
        ],
        "content": {
            "summary": {
                "value": "The authors propose approaches using \u201csmall\u201d language models to address \u201ccomplex\u201d style transfer, i.e. addressing style transfer in the context of more complex styles associated with unique personalities and long form narratives. They propose a chatGPT based evaluation, which addressing weaknesses with existing automatic evaluations (e.g. SBERT), and also use chatGPT as a data augmentation tool to create parallel datasets for fine-tuning. The demonstrate that their fine-tuned approaches operate on par with chatGPT across several different evaluations on two datasets. They also conduct studies on the amount of data and quality of finetuning data, in line with prior work, they find that quality is crucial."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors cover a significant amount of related work, and compare their approaches to a good set of benchmarks (though they do not compare consistently across datasets). They also conduct good studies on data quality and generation diversity. The research questions are well outlined and motivated in the introduction, however there are significant weaknesses in addressing them (see below)."
            },
            "weaknesses": {
                "value": "* Multiple instances of improper grammar (ex. Sentence 2 of section 2 \u201c...it generates\u201d) and unusual wording. Difficult to understand at times with grammar issues.\n  * \u201cunleash the capability of these models\u201d\n  * \u201cSmall language models specialized for the magician capabilities of style transfer\u201d\n  * 3.5.3 title typo \u201cHow small can language models take effects\u201d. The first word in this paragraph should also be capitalized. \n* Re data generation: figure 2 and section 2.1 indicate chatGPT is used in conjunction with a instruction based prompt to generate synthetic data, but there are no details on what exactly the prompting strategy is, or what any of the results looks like\n* Details lacking on chatGPT based evaluation metric - no description of how this actually works. This is especially important as the authors highlight the proposed metric as one of their 3 novel contributions. In Table 1, chatGPT is reported but there is no corresponding description of how it is used, the same table also lists \u2018fine-tuned\u2019 but doesn\u2019t not indicate what is fine-tuned.\n\nMinor issues\n* In section 2.3, \u201cHowever, our analysis has identified..\u201d it would be nice to have a forward reference to the experiment here\n* Table 1 can be moved to a later page, closer to when it\u2019s referenced\n* Table 3: There is no metric for the values in the table, not sure what is being reported\n* Consider adding https://arxiv.org/pdf/2010.05700.pdf to related work, as there are other ways to getting around a lack of parallel corpora."
            },
            "questions": {
                "value": "* In the human evaluations (section 3.4) are you certain that the evaluators are not conflating strong fluency for style matching? Especially for style associated with a particular character, it may be hard to judge which is best (if they aren\u2019t familiar with the characters). This may be an explanation for why chatGPT performs well.\n\n* Why aren\u2019t the same baselines used in Tables 4 and 5?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5406/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698792461557,
        "cdate": 1698792461557,
        "tmdate": 1699636548066,
        "mdate": 1699636548066,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vWxA3yadSL",
        "forum": "juStNETXI5",
        "replyto": "juStNETXI5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5406/Reviewer_KX34"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5406/Reviewer_KX34"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a technique for complex style transfer using the following steps:\n- Choose a style\n- Create a small, high quality training dataset (~O(10k)) of that style using an LLM\n- Use that dataset to finetune a much smaller model which is ultimately used.\nThey also propose an evaluation to rank the quality of the generated examples to choose which to use for finetuning, and to evaluate style transfer across a few dimensions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- It's great to see approaches like this which recognize that LLMs are not tractable in many situations. Additionally, it's interesting to see how LLMs can be leveraged for small, impactful parts of the pipeline (e.g., creating a small and high quality dataset)\n- The paper is well-written and straightforward; I enjoyed the descriptions of complex style transfer being \"magician capabilities\" :)"
            },
            "weaknesses": {
                "value": "- While the end-to-end system is an interesting use case, I'm not sure this is particularly novel overall. We know that LLMs can create small, high quality datasets, and that those datasets can be used to train smaller models.\n- Using a different model's embedding space to calculate BERT-score doesn't seem significant enough to include as a contribution\n- Nit: use \\citep vs \\citet for clarity (https://www.reddit.com/r/LaTeX/comments/5g9kn1/whats_the_difference_between_cite_citep_and_citep/)\n- \"...challenge the conventional wisdom that balancing style transfer accuracy and semantic preservation is difficult\" this seems like a strong claim. In my mind, this tradeoff has not been an issue for LLM-based style transfer methods."
            },
            "questions": {
                "value": "- Do you have the prompts that you used for the style transfer accuracy classifiers? \n- Also for the style transfer itself (in Table 6)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5406/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698802189512,
        "cdate": 1698802189512,
        "tmdate": 1699636547972,
        "mdate": 1699636547972,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6wiCbicURi",
        "forum": "juStNETXI5",
        "replyto": "juStNETXI5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5406/Reviewer_Lzy3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5406/Reviewer_Lzy3"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a simple but effective method of text style transfer (TST), which first generates a parallel corpus of the target style with few-shot prompts, and then train a seq2seq model over the parallel corpus. The experiments confirm the effectiveness of the method in several benchmarks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "**Clarity and Significance**\n\n- The paper is well-written and easy to follow. The improvement in small LM TST performance is significant, although such gain should be attributed to the generated parallel datasets.\n- The method is relatively easy and effective."
            },
            "weaknesses": {
                "value": "**Evaluation**\n\nMy main concern about this work is the evaluation: afaik there are several other strong TST baselines that also leverage parallel datasets (either synthetic or mined from unsupervised ground truth) such as TSF-DelRetGen [1], IMaT [2], STRAP [3], LaMer [4] which are missed in the current work. IMaT proposed to use translation to generate parallel/matching datasets, STRAP framed the TST task as a paraphrase generation tasks, and LaMer mined roughly parallel data from unparallel TST datasets, which I believe all have similar ideas as yours. Another baseline is [5], which finds directly asking LLMs about target style text with few-shot is enough to achieve SotA performance --- if that is the case, what's the benefit of distilling the data into a small TST model (and maybe less generalized with limited performance)?\n\nThe human evaluation is crucially ignored, which I believe is the most important metric we should care for TST tasks (as pointed out in [3], automated metrics such as BLEU/ROUGE can be easily gamed).\n\nIn general, the evaluation is not very convincing to me as the missing strong baselines and human evaluation.\n\n\n\n\n[1] Transforming Delete, Retrieve, Generate Approach for Controlled Text Style Transfer\n\n[2] IMaT: Unsupervised Text Attribute Transfer via Iterative Matching and Translation\n\n[3] Reformulating Unsupervised Style Transfer as Paraphrase Generation\n\n[4] Non-Parallel Text Style Transfer with Self-Parallel Supervision\n\n[5] A Recipe for Arbitrary Text Style Transfer with Large Language Models"
            },
            "questions": {
                "value": "Could you discuss the difference/contribution of your method compared with the above-mentioned work in the revised version?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5406/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698872251422,
        "cdate": 1698872251422,
        "tmdate": 1699636547884,
        "mdate": 1699636547884,
        "license": "CC BY 4.0",
        "version": 2
    }
]