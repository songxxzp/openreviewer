[
    {
        "id": "CLBeaGJtuI",
        "forum": "PKsTHJXn4d",
        "replyto": "PKsTHJXn4d",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8529/Reviewer_ccmh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8529/Reviewer_ccmh"
        ],
        "content": {
            "summary": {
                "value": "This work presents an approach by which a large language model (LLM) can generate natural language explanations of agent behavior based only on observations of the state and the agent's actions. The method involves first learning a decision tree representation of the agent's behavior and then feeding traces through this tree to an LLM, which provides an explanation of the user's behavior. Multiple approaches to scrutinize the quality of the generated explanations are used including determine the rate of hallucinations, determining whether the LLM understand the agent's behavior well enough to predict subsequent actions, and user studies that seek to determine whether a user prefers explanations generated via the proposed approach. Notably, the experiments also allow users to interrogate the LLM for clarification or more detail about the provided explanations, a unique and potentially powerful capability in this domain."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper is very well written and clear. The introduction is strong and clearly communicates the core ideas in the paper. The motivation behind the problem under scrutiny is also clear. There were only a few points in reading that I did not follow what was going on.\n\nThe overall execution of the methodology also seems sound, though there are some general comments I have about the depth of the experiments. The idea of having an explanation system that an end user can interact with is a good point, and a clear argument in favor of this approach and a unique capability in this domain."
            },
            "weaknesses": {
                "value": "There are a few issues with the paper that I believe should be addressed and questions about the broad applicability of the proposed approach.\n\n**The approach seems somewhat limited in scope, requiring a considerable amount of work from a designer and a description of the world model.**\nThe authors claim that the proposed approach can \"generate natural language explanations for an agent's behavior based only on observations of states and actions.\" While this is technically true, there are significant additional requirements for the proposed approach to succeed. First, the agent's policy must be summarized into a decision tree, requiring a potentially lengthy training phase and access to the environment. In addition, the LLM is provided a comprehensive description of the world model, which it seems to need in order to generate reasonable explanations. The authors should comment on this constraint and address the scenario in which one might use this pipeline, since it seems to need access to low level knowledge about the environment (in the form of a model) such that other forms of explanation are more appropriate.\n\n**Being able to summarize behaviors into a decision tree.**\nBehaviors need to lend themselves to fairly simple summarization, not just for the LLM, but also because the requirement that a behavior tree could be used to represent the robot's behavior is a fairly stringent constraint. In many ways it seems that the ability of the system to be summarized in terms of a behavior tree is the more difficult of the two tasks. However, there is no way to know in advance if the behavior tree is a good proxy for the agent's behavior. Moreover, there are no experiments in the work to judge how accurate the behavior trees are.\n\nThere are only three policies being explored, each of which has a fairly simple high-level description. The lack of a more complex policy for which the system must generate explanations is notable, as it is unclear how effectively the system would be able to describe more sophisticated behaviors.\n\n**On the subject of 'optimality'**\nThere are multiple places in which \"optimal behavior\" is mentioned or discussed, yet there is not a clear reward function. Presumably, the goal is to reveal and assist all of the victims as quickly as possible, and this seems to also be what the participants expect as well, since this is likely why they view the \"exploratory\" behavior as suboptimal. Some clarification on this point would be helpful.\n\n\n**Concerns about the user studies**\nFinally, the user study results are somewhat unconvincing to me and I worry that the format of the study are fundamentally flawed (I may be wrong here! I welcome comments by the authors). The purpose of explanations is to help a (non-expert) human end-user to understand why an agent behaved a certain way. The user study instead asks which explanation is preferred between two examples; this is potentially problematic since there is no way to determine if the user actually understands the agent's behavior or if they merely prefer the prose of one explanation over another. ChatGPT is well known to make high quality prose and can produce statements that are linguistically coherent and convincing even if the statements themselves are false in some way.\n\nIt is for this reason that I am concerned by the conclusion that hallucinations do not seem to impact user preference: this result may suggest that the user is indeed not understanding the agent's behavior, preferring only the quality of the prose in the explanations over their substance.\n\nInstead, the user studies would have been much more effective if the user were evaluated on their ability to demonstrate a comprehension of the agent's behavior, for example asking the user to predict the agent's next action (as the LLM was asked in the earlier studies). This could be compared against the performance of a user if they were given the same high-level description of the agent's policy readers of this paper are provided: e.g., \"explore\" vs \"exploit\" policies.\n\nRelatedly, it is concerning that the rate of hallucinations is so high, with ~30% hallucinations for the proposed BR (Path) approach.\n\n\n**Conclusion**\nOverall, I think the paper presents some interesting ideas, though I am unsure if its exploration of some of the relevant weaknesses or issues associated with the broad applicability of the approach is deep enough at this time.\n\n\n**Minor comments**\n- Sec. 4: I believe that the \"Explore\" and \"Exploit\" policy descriptions are flipped. As written, the \"Explore\" policy targets immediately clearing rubble.\n- I do not believe that an example of the \"Template\" response is included, yet is important for understanding the user studies. The authors should provide an example of the \"Template\" explanation.\n- The connection to MARL (multi-agent reinforcement learning) is not very clear. The authors mention that the explanations are being applied to a MARL task, yet for all of the example scenarios, there does not appear to be any synergy between the two robots in the domains, as they generally seem to have policies that are independent of one another. I recommend that the authors provide additional justification about how the experiments show the capacity of the system to explain a multi-agent system or remove the mention of the MARL scenario.\n- While the colors in Figure 4 are useful at a glance, numbers should also be included. If the authors could add small-text numbers inside each of the corresponding boxes (keeping the colored background for each square), it would be helpful for understanding."
            },
            "questions": {
                "value": "Here I summarize the most important questions from my longer discussion above:\n\n- The authors should comment on this constraint and address the scenario in which one might use this pipeline, since it seems to need access to low level knowledge about the environment (in the form of a model) such that other forms of explanation are more appropriate.\n- Can the authors comment on the accuracy of the decision tree? Are there any experiments that seek to judge the rate with which the decision tree?\n- The authors should comment on the utility of the user study and address potential issues with questions that seek only to judge preference between different forms of explanations. Notably, they should comment further on the lack of any impact of the high rate of hallucination on user preference."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8529/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698779648126,
        "cdate": 1698779648126,
        "tmdate": 1699637066833,
        "mdate": 1699637066833,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hCAv18l9Lm",
        "forum": "PKsTHJXn4d",
        "replyto": "PKsTHJXn4d",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8529/Reviewer_Pb35"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8529/Reviewer_Pb35"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a method for generating natural language explanations of an agent's behavior using large language models (LLMs) with a decision-tree representation of the agent's decision-making process. Given the ability to simulate a black-box agent policy, the method first distills a decision tree policy that is a surrogate of the original policy. To generate an explanation for why an agent took a particular action $a$ in state $s$, this surrogate policy is applied to the current state $s$, and a corresponding decision path (which the authors call a \"behavior representation\") is extracted from the decision tree. This decision path is then used to prompt LLM, which generates a human interpretable English representation of why the agent made that particular decision.\n\nThe authors demonstrate this method in the context of a search-and-rescue gridworld environment with partial observability. They distill decision-tree policies that approximate several different types of agent behavior, e.g. a policy that first explores all unexplored rooms before rescuing victims. They then apply their method to extract decision paths for observed state-action pairs, and to generate natural language explanations from those decision paths. To evaluate these explanations, the authors manually annotate the accuracy of the explanations with respect to several criteria (e.g. whether it accurately describes the agent's goal, or the agent's next action), and compare it several baselines, finding that explanations generated from decision paths are more accurate and less hallucinatory than explanations generated by baseline methods (e.g. when no decision path is provided to the LLM). The authors also perform a user study, finding that users prefer the explanations generated according to their method compared to the baselines, and that human-written explanations are not significantly preferred by users compared to the explanations generated by their method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper attempts to develop a model-agnostic method for generating natural language explanations of agent behavior, relying on interpretable policy distillation and LLMs to generate plausible reasoning traces and corresponding explanations for why an agent took a particular action in a particular state. Insofar as the choice of model architecture cannot be controlled, a model-agnostic approach to explanation generation is potentially widely useful and applicable. In addition, the proposed method limits the tendency of LLMs to confabulate false or misleading explanations by providing the LLM with a reasoning trace from a decision tree surrogate of the original policy, improving the chances that the generated explanation is actually faithful to the original policy. Compared to directly providing the trace of the decision tree as an explanation, LLM generated explanations are more interpretable, while also being interactive, avoiding the pitfalls of traditional approaches to explainable and interpretable AI. Empirically, these benefits are borne out by the higher accuracy metrics of the proposed method compared to baselines, and the favorable user study."
            },
            "weaknesses": {
                "value": "While the idea behind this paper is interesting, several aspects of the paper raise questions about (i) how model-agnostic the approach actually is (ii) how well the results will generalize to other domains and policies, and (iii) how explanation quality is evaluated.\n\nRegarding the model agnostic nature of the approach, it seems to me that a major requirement for the proposed method to work is that the representation of the environment itself is interpretable. Otherwise, there's no obvious way that the decision tree policy, and corresponding decision path, can be translated into an interpretable natural language template for the LLM to consume. While this requirement is satisfied by the gridworld environment evaluated by the authors, it is not true for a lot of other models and agent policies, which may directly consume pixels or low-level state representations as input. As such, one of the promised benefits of the proposed method --- that all it requires is simulation access to some policy --- is not really delivered upon. Rather, this method can only be applied if the underlying state representation is sufficiently interpretable. While that is a reasonable assumption to make, it should be clearly stated in the framing of the paper. And if such a framing is taken, then the method should ideally also be compared against XAI approaches that do leverage the assumption of interpretable state representations, such as plan recognition and goal recognition algorithms. \n\nAs for the whether the results will generalize to other environments and policies, one worry I have about the current set of experiments is that they seem highly tailored to generating explanations for either explorative policies or exploitative policies in the search and rescue setting. While the natural language explanations shown in the paper seem intuitively plausible, there's a much larger space of policies that purely exploratory or exploitative policies, and it's not obvious to me that either humans or LLMs would succeed in coming up with compelling natural language explanations of those policies. Even looking at the example decision paths / behavior representations shown in the paper, it seems to me like a wide variety of goals, strategies, and intentions could be consistent with those decision paths, and I worry that the fact that LLMs are generating the \"right\" explanations is just an artifact of the few-shot examples combined with the design of the test set. For example, would the LLMs still generate the \"right\" explanations if the test policies happened to be exploratory in the top half of the grid, and exploitative in the bottom half of the grid? Or if the true policies applied some mixture of exploration and exploitation, e.g. via Thompson sampling?\n\nAll of this is tied to questions I have about what counts as \"good\" explanation of an agent's behavior, and how that can be fairly evaluated. At least when it comes to explaining *human* behavior, we typically default to explaining others' actions in terms of mental states like goals, beliefs, intentions, etc, as part of our theory-of-mind. The authors seem to implicitly have something like this view in mind, given that they manually annotated the LLM-generated explanations for accuracy in terms of goals, intents, strategies, and other supposed mental states. However, it's not clear to me that this is the right evaluation metric for explanations about *artificial* agents, given that black-box policies could act very different from humans, and may not have anything like goals or intents at all! (In fact, a decision-tree policy is a purely reactive policy, and doesn't involve any goal-directed planning -- so there's a certain sense in which it's a metaphysical error to impute a goal to a reactive agent, unless the reactive policy was trained to imitate some goal-directed policy computed via value iteration.)\n\nTo avoid this anthropomorphic bias in explanation generation, a different standard for a \"good\" explanation is that good explanations should support both accurate prediction, but also accurate intervention and counterfactual reasoning. For example, one reason why we might attribute mental states like goals and beliefs to other humans is because they are useful abstractions that we can plan to intervene upon (see [1] for an exposition of this idea). In context of explaining the behavior of artificial agents, this could be evaluated by considering the accuracy of using the explanations for future action prediction (which the authors do), but also by seeing whether they support useful interventions. I think adding such evaluations would be a helpful improvement to the current paper. Otherwise, I worry that the current evaluation methodology is overfit to the particular policies considered by the authors, which admit relatively simple anthropomorphic explanations involving exploratory or exploitative goals.\n\n[1] Ho, M. K., Saxe, R., & Cushman, F. (2022). Planning with theory of mind. Trends in Cognitive Sciences, 26(11), 959-971."
            },
            "questions": {
                "value": "- How model-agnostic is this approach, given that it seems to rely on interpretable state representations?\n\n- Can we expect this method to generalize to wider a range of policies, or to a richer range of environments with much more features?\n\n- How were the generated explanations hand-annotated, and what steps were taken to make sure that the evaluation/coding criteria weren't overfit to the choice of test policies?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8529/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8529/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8529/Reviewer_Pb35"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8529/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698788730766,
        "cdate": 1698788730766,
        "tmdate": 1699637066718,
        "mdate": 1699637066718,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8JFZapXQ0Q",
        "forum": "PKsTHJXn4d",
        "replyto": "PKsTHJXn4d",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8529/Reviewer_NiyW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8529/Reviewer_NiyW"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the author proposed an model agnostic approach to generate natural language explanations for an agent's behavior based only on observations of states and actions, To be specific, it's implemented through a 3 step approach: first distill the agent's policy into a decision tree.  Second generate a behavior representation from the decision tree. Third query an LLM for an explanation given the behavior representation.  The proposed method is evaluated in a multi-agent search-and-rescue environment and showed the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper proposed a novel framework to generate natural language explanations of an agent's behavior given only observations of states and actions. By make such assumption, the proposed method is model agnostic for both the agent's policy model and LLM, which makes it pretty independent."
            },
            "weaknesses": {
                "value": "The proposed method is supposed to be a general framework, however, in the paper, it's only verified in one domain with limited data. It's better to do more experiments on more data to prove the effectiveness of the proposed method."
            },
            "questions": {
                "value": "1. For the experiment setup section, can you explain more on the metrics, for example, what's the difference between Category and Goal?\n2. Can you provide more details on the decision tree based distillation, such as what features used? \n3. For the distilled decision tree, how does the its performance compared with original policy model? how will this performance gap influence the generated explanation?\n4. As proposed in the conclusion, this method is limited  by using only non-dense inputs, however, for real world application, many of them will include dense inputs like continuous features, this makes the proposed method limited."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8529/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698800711570,
        "cdate": 1698800711570,
        "tmdate": 1699637066521,
        "mdate": 1699637066521,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ve7c8Bhhmi",
        "forum": "PKsTHJXn4d",
        "replyto": "PKsTHJXn4d",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8529/Reviewer_5qZT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8529/Reviewer_5qZT"
        ],
        "content": {
            "summary": {
                "value": "Authors propose a framework to explain the behavior of an agent. The framework consists of three phases, where in phase 1, the agent's policy is approximated by a decision tree. In the next phase, authors represent the decision paths of the decision tree as behavior representations. The last phase takes these behavior representations and feeds them to a language model using in-context learning, to produce a natural language explanation/output. Evaluation of the framework is done both quantitatively and qualitatively. In the quantitative evaluation, the quality of the behavior representation is assessed based on the reasoning performance and the future behavior prediction of the language model. Authors have conducted a human-subject study to evaluate the explanation quality. Further evaluations were reported on the hallucinations of the language model. \n\nWhile the proposed framework presents an interesting methodology that can produce natural explanations, there are some major drawbacks and weaknesses which I detail below."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "-The method is model agnostic, and can be used with any agent described by an MDP. \n-The framework allows for interactive explanations, which enables the user to clear any doubts using back and forth dialog.\n-Framework can be quickly adapted to existing agents, with limited computing resources and implementation overhead. \n-Evaluation is done both quantitatively and qualitatively, which I see as a strength in an interpretability method."
            },
            "weaknesses": {
                "value": "There are major drawbacks of the method detailed below.\n\n-The validity and the correctness of the behavior explanations are assessed by the language model using feature behavior inference,  instead of the human user (as the intended end user). This is generally assessed by action prediction (where we ask the human participant to predict the future behavior of the agent, given some previous explanations). Refer to Metrics for XAI of Hoffman et.al [1].\n-There are several surrogate models that approximate the reasoning of the base agent, which can affect the faithfulness of the explanations. Ideally the faithfulness of the explanations should be more thoroughly evaluated.\n-There are few details given of the agent, the decision tree, environment and the language model used. Authors have used one type of agent, in one environment and used one language model, which raises further questions of the methods generalizability and scalability to complex agents and larger environments. \n- The quality of the explanation depends heavily on the task description, and this might be harder to craft in more complex environments.\n- The human-subject experiments are only used to evaluate the preferences of the participants. Explanations can be highly preferred but be misleading (i.e. unfaithful to the underlying agent).\n\n[1] https://arxiv.org/abs/1812.04608\n\n\n---- after the rebuttal ----\n\nI acknowledge the rebuttal of the authors. Some of my major concerns still remain."
            },
            "questions": {
                "value": "- Can this framework handle domains with continuous actions? If this is the case, what are the changes that the framework needs?\n- The selection of the language model can have a major impact on the explanations, have authors tested the framework with any other language models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8529/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8529/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8529/Reviewer_5qZT"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8529/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698847214593,
        "cdate": 1698847214593,
        "tmdate": 1700696317930,
        "mdate": 1700696317930,
        "license": "CC BY 4.0",
        "version": 2
    }
]