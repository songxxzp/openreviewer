[
    {
        "id": "bNLCt2r9yQ",
        "forum": "AgCz44ebFe",
        "replyto": "AgCz44ebFe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5067/Reviewer_QKiJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5067/Reviewer_QKiJ"
        ],
        "content": {
            "summary": {
                "value": "In this study, the authors leverage the phenomenon of forgetting to tackle a specific challenge: continual learning under noisy labels. They posit that samples affected by noisy labels are more prone to being forgotten, a characteristic that can be exploited to sift clean samples from noisey datasets. To achieve this, they introduce Alternate Experience Replay (AER), a mechanism that select potential clean samples in the current task while concurrently replaying samples from previous tasks. The efficacy of their proposed method is assessed using both synthetic and real-world datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. Combining continual learning with label noise is interesting, although this setting is controversial.\n2. The paper is well-structured and easy to follow."
            },
            "weaknesses": {
                "value": "1. The techniques proposed in the study, specifically 'forgetting' in the realm of learning with noisy labels and 'replay' within continual learning, aren't novel in their individual contexts. The mere combination of these two established methods doesn't inherently bring novelty to the field.\n2. The authors advocate the use of forgetting as a selection criterion, yet its effectiveness may be limited to specific types of label noise. It appears inadequate for more complex noise categories, for instance, pairflip-45 and instance label noise, let alone for real-world datasets. A more extensive experimental analysis is essential to thoroughly investigate the 'forgetting' phenomenon's applicability and limitations in various noise scenarios.\n3. The experiments are confined to small datasets, with only 10 classes from WebVision utilized, which casts doubt on the scalability of the proposed methods. For a dataset of 50k, joint learning seems more appropriate, and the unimpressive outcomes in Table 1 underscore the proposed methods' shortcomings. Similarly, the results in Table 2 are too inconclusive for any solid verification. The authors should report the results of other label noise methods such as DivideMix on Joint."
            },
            "questions": {
                "value": "1. Why are the results of CIFAR-10 with asymmetric label noise missing in Table 1?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5067/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698037905146,
        "cdate": 1698037905146,
        "tmdate": 1699636496828,
        "mdate": 1699636496828,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XcboAY7BpG",
        "forum": "AgCz44ebFe",
        "replyto": "AgCz44ebFe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5067/Reviewer_Jj1M"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5067/Reviewer_Jj1M"
        ],
        "content": {
            "summary": {
                "value": "This paper tries to solve the continual learning problem under noisy labels by proposing Alternate Experience Replay (AER). The author found that the loss of clean data and noisy data have different trends that can help the model find which data has noisy labels. From this perspective, the author proposed Asymmetric Balanced Sampling to improve the AER performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The proposed method is well-motivated and easy to understand.\n+ The experiments demonstrate the effectiveness of this method."
            },
            "weaknesses": {
                "value": "+ I believe the alternate replay is an interesting design, but the novelty of the proposed method is still limited. The strategies used in this paper are simple and very common techniques, such as selecting clean samples based on loss thresholds and MixMatch. It seems like a direct combination of existing technologies, with no essential innovation in methodology, especially for continual learning.\n+ It is difficult to adapt to more complex noise distribution by selecting samples directly based on the loss threshold. More advanced methods should be considered [1, 2] instead of still adjusting the threshold manually, which is not helpful for either CL or noisy label problems.\n+ PuriDivER is designed to handle online CL and blurry tasks with noisy labels. Therefore, it may not be suitable for offline CL. However, both online CL and blurry tasks are more challenging and realistic settings, and I\u2019m curious how well the proposed method AER works under these settings as well. In particular, how to adapt AER to online CL where the distribution changes rapidly?\n+ The comparison methods cannot demonstrate the real effectiveness of AER and ABS. Since few methods focus on offline CL with noisy labels, the authors should conduct more detailed comparative experiments. For example, using the same ABS strategy, compare the performance of AER and DividMix to verify the excellence of AER. In addition, using the same AER strategy, the performance of ABS and PuriDivER sampling methods should also be compared.\n   ```\n   [1] Meta label correction for noisy label learning. AAAI-2021.\n   [2] Learning from noisy labels with decoupled meta label purifier. CVPR-2023.\n   ```"
            },
            "questions": {
                "value": "Please refer to the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5067/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698675833048,
        "cdate": 1698675833048,
        "tmdate": 1699636496744,
        "mdate": 1699636496744,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5eUU8r0m0R",
        "forum": "AgCz44ebFe",
        "replyto": "AgCz44ebFe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5067/Reviewer_aFAe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5067/Reviewer_aFAe"
        ],
        "content": {
            "summary": {
                "value": "This paper explores the problem of continual learning under noisy labels. The authors mainly designed Alternative Experience Replay and Asymmetric Balanced Sampling to address the problem. At the same time, the authors also adopted consolidation and MixMatch to strengthen their algorithm. In the experiment, the authors validated their algorithms on 4 datasets and showed state-of-the-art results. They also performed a thorough analysis of the algorithm."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors perform a thorough analysis both in exploring the problem and in the effect of the algorithm. Each component of the algorithm is well motivated. The intuition of the algorithm is well presented."
            },
            "weaknesses": {
                "value": "1. The  main concern is about the performance of the algorithm. \n- The authors conducted experiments on four datasets, three of which were synthesized, and one contained natural noise (WebVision). The discussions and improvements primarily focused on the synthesized datasets. Regarding the WebVision dataset, it remains unclear why the authors selected only 10 classes out of the available 1000 classes, and the criteria for selecting these 10 classes are also not clear. According to Table 2, the AER + ABS approach did not outperform PuriDivER. The proposed method marginally outperformed PuriDivER only with the implementation of consolidation and MixMatch, which were adopted from other papers. It is of great concern that the algorithm can only function effectively in the synthetic scenario.\n- Ablation study of AER and ABS is needed. \n- Is there explanation of PuriDivEr's consolidation? \n- It is not clear whether the experiments are in offline continual learning or online continual learning. The single or multiple training iterations is decided by the offline/online setting, and not something that an algorithm could choose. (section 4.3)\n- It would be beneficial if the authors also compared it to PuriDivER when applying the algorithm to DER++.\n2. Some motivation and details of the algorithm are not clear. \n- In Figure 1, the authors aim to demonstrate the change in loss of noisy data with and without replay to highlight that noisy data tends to exhibit greater forgetting. However, it remains unclear in the figure which task the noisy data originates from. Moreover, the depiction of forgetting would be clearer by comparing the accuracy (loss) of the same data from different tasks, rather than comparing the same task with different data or comparing the same data at different stages.\n- It is not clear if Eq(1) can represent the optimization goal. If I understand correctly, Eq(1) means finding a model to fit the noisy distribution, even when given a wrong label. The loss function yields a smaller value when the model also predicts the wrong label. However, the algorithm seems to eliminate the influence of the noisy label and encourages the model to predict the correct label.\n- In Eq(4), is the score function $s(x)$ for task $t$? In this case, why would the buffer contain the current task data?\n- In the motivation of ABS, why $\\mathcal{L} \\geq 0$ cause $s(x)$ favour samples from the current task?\n- In the ABS part, what is $p_{curr}$ and $p_{past}$?\n3. There are also some writing issues, mainly related to logic, that make the paper harder to read at some important points.\n- The logic is weak in the motivation part of the abstract -- noise scenarios (caused by limited annotation time) renders the CL with buffer vulnerable\n- It is wired to mention \"adaption is faster\" (page 2 line 3) when explain CL is vulnerable under noise label. \n- In section 3.4, line 2, \"the backbone had to be trained on a stream of noisy data\". This is exactly the setting. This sentence is confusing."
            },
            "questions": {
                "value": "See the weaknesses part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5067/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5067/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5067/Reviewer_aFAe"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5067/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698779253796,
        "cdate": 1698779253796,
        "tmdate": 1700740720101,
        "mdate": 1700740720101,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "b5h44LpDbp",
        "forum": "AgCz44ebFe",
        "replyto": "AgCz44ebFe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5067/Reviewer_2XeP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5067/Reviewer_2XeP"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on continual learning with noisy labels and mitigating the forgetting of knowledge of past learning tasks. To achieve the two targets, this paper proposes to apply the experience replay method and proposes a new sample selection strategy called Asymmetric Balanced Sampling (ABS) for replay. ABS aims to select clean samples with lower loss from the current task and select the most informative samples with higher loss from the past tasks. This paper claims that the clean samples can increase the purity of buffer and the informative samples can increase the diversity to better mitigate forgetting. To evaluate the effectiveness of applying the replay with ABS on the tasks of continual learning with noisy labels, this paper set up experiments on 4 different datasets with different noisy ratios. The results prove the methods can achieve better performances on all classification tasks compared with other CL methods with different noisy label learning approaches.  Additional experiments also present the ABS can outperform other sample methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper focuses on the problem of continual learning with noisy labels, which is vital in real scenarios.\n2. This paper applies experience replay into noisy label problems and proposes novel sampling methods for the replay.\n3. The experiments are comprehensive, including four different types of datasets\nand two different noise injection processes. This paper compares the \nproposed methods with other different works of continual learning and noisy \nlabel learning. The improvement of the proposed method is significant."
            },
            "weaknesses": {
                "value": "1. The presentation of the part of the method requires more details. For example, what does the task D_t = (X_t, Y_t) mean? Does it mean a dataset or distribution? And what does D_t \u2229 M mean in eq.(4)?\n2. The experiments only provide an average accuracy for all the tasks and do not prove the proposed method can reduce the forgetting of past tasks directly."
            },
            "questions": {
                "value": "a) How to guarantee the stored data in the buffer does not contain a noisy label when selecting the data with high losses.\nb) How does the size of the buffer impact the performance?\nc) How does the proposed method perform if the order of sequence is shuffled?\nd) When the length of a sequence is increased, more informative data is added and how to preserve the information from the first several tasks with a smaller size of data in the buffer?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5067/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698835451185,
        "cdate": 1698835451185,
        "tmdate": 1699636496576,
        "mdate": 1699636496576,
        "license": "CC BY 4.0",
        "version": 2
    }
]