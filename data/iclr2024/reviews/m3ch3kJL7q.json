[
    {
        "id": "fbTpOPtXWw",
        "forum": "m3ch3kJL7q",
        "replyto": "m3ch3kJL7q",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1658/Reviewer_NuL6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1658/Reviewer_NuL6"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to learn sentence-level prompts for supervised composed image retrieval task, to handle complex changes in CIR task such as modifications involving multiple objects. The sentence-level prompts are generated from query image and relative caption, to yield precise descriptions of specific elements in the query image that are described in the relative caption. Experimental results demonstrate that the proposed method achieves better results on two public CIR benchmarks including FashionIQ and CIRR."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. It is reasonable to generate sentence-level prompts depending on both reference image and relative caption to enrich the expressivity. \n2. Moreover, the experiments are solid since the authors conduct a thorough comparison with previous methods. \n3. The paper is well-written and the idea is easy to follow."
            },
            "weaknesses": {
                "value": "1.\tin my comprehension, the sentence-level prompts are actually latent vectors output from the MLP layer, so it is hard to make sure the prompts work as expected as demonstrated in Figure 1(c), i.e., decoupling the multiple objects and attributes of query image, and correctly integrating the process of object removal or attribute modification. \n2.\tIt is difficult to understand the pi\u2019 in prompt alignment loss. Whether each reference image has an auxiliary text prompt? As a result, the Figure 2(a) involves two training stages (ITC loss to optimize p and prompt alignment loss to optimize pi\u2019)? Furthermore, during the optimization of pi\u2019, the text encoder is frozen, so the image encoder learns to align with the frozen text encoder; while in optimization of pi, the text encoder is not frozen, so the image encoder learns to align with the updated text encoder. I find it hard to understand how the prompt alignment loss works and it seems very tricky to achieve a good performance."
            },
            "questions": {
                "value": "The same as weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1658/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1658/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1658/Reviewer_NuL6"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1658/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698718627136,
        "cdate": 1698718627136,
        "tmdate": 1700727780508,
        "mdate": 1700727780508,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GJMEeeEzNb",
        "forum": "m3ch3kJL7q",
        "replyto": "m3ch3kJL7q",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1658/Reviewer_a9pf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1658/Reviewer_a9pf"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a sentence prompt generation based approach to composed image retrieval or CIR. They train their system to learn the generation of such sentence prompts for known target images (in the training set) through two innovative loss functions. Then at inference time their system essentially generates a sentence prompt that along with the original user query enables the user to retrieve the target image with greater accuracy since the sentence prompt has a much more fine-grained description of the target image."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Comprehensive literature survey and good motivation of the problem.\n2. Sound approach based on innovative loss functions.\n3. Good results that exceed the state of the art."
            },
            "weaknesses": {
                "value": "1. The overall innovation could be seen as modest. However, I am open to being convinced otherwise."
            },
            "questions": {
                "value": "1. How does your approach do across domains? Is it able to adapt to domain shifts in other words?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1658/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698784151770,
        "cdate": 1698784151770,
        "tmdate": 1699636093811,
        "mdate": 1699636093811,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CmXQpIuKbA",
        "forum": "m3ch3kJL7q",
        "replyto": "m3ch3kJL7q",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1658/Reviewer_fBxs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1658/Reviewer_fBxs"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an novel approach for the Composed Image Retrieval (CIR) problem. The idea is to generate a sentence (\"text prompt\") to describe both the input image and the relative caption and it for searching the image database using standard text-to-image retrieval methods. The method is evaluated against 10+ baseline methods against two datasets ( CIRR, Fashion - IQ). The results provide significant top-k recall gains over all the baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The proposed method is technically sound and simple. \n- The paper is easy to follow, experiments are thorough along with ablations (such as prompt length, weight in the loss function etc). \n- Provides SOTA results against 10+ baselines on two public datasets. \n- To be open-sourced."
            },
            "weaknesses": {
                "value": "- Limited novelty: A recent paper (https://arxiv.org/pdf/2310.09291.pdf) with quite similar methodology and motivations except for a nuanced difference: training-free vs learned sentence level prompts. There is a need for contextualizing these methods together, ideally under the same evaluation framework so that we understand the value of learned sentence level prompts proposed by this paper. \n- Experimental setup: CIRR dataset experiments uses a random split of the training dataset as the test set for evaluations. The results should be reported in the official (hidden) test set instead. Otherwise reported numbers are not comparable to other papers published in this area."
            },
            "questions": {
                "value": "Q1: Could the database image-caption pairs be enriched with the proposed sentence generation method and used for refining the search?\nQ2: How easy to extend the proposed method for addressing other problem domains or modalities?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1658/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1658/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1658/Reviewer_fBxs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1658/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698908157111,
        "cdate": 1698908157111,
        "tmdate": 1700699695622,
        "mdate": 1700699695622,
        "license": "CC BY 4.0",
        "version": 2
    }
]