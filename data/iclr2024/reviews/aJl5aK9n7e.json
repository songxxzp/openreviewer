[
    {
        "id": "0ypSwceaWB",
        "forum": "aJl5aK9n7e",
        "replyto": "aJl5aK9n7e",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6089/Reviewer_SSQZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6089/Reviewer_SSQZ"
        ],
        "content": {
            "summary": {
                "value": "The paper theoretically investigates the generalizability of Graph Transformers (GTs) by studying the convergence of shallow GT model on a semi-supervised node classification setup where node labels are determined by discriminative patterns in node features and majority voting within core neighborhoods. Interesting findings include 1) larger fraction of discriminative nodes and smaller confusion ratio improves sample complexity, 2) self-attention helps GTs outperform graph convolutional networks (GCNs) by essentially reducing the number of training samples and iterations needed to achieve zero generalization error, and 3) learnable positional encoding in GTs improves generalizability by promoting core neighborhoods. Experiments on synthetic and real-world data verify these theoretical insights."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- [S1] **Theoretical novelty and contribution is clear.** The paper is the first to propose a framework for analyzing the generalizability of graph Transformers, and the analysis using the semi-supervised binary node classification setup is very interesting.\n- [S2] **Empirical results well support the theoretical claims.**"
            },
            "weaknesses": {
                "value": "- [W1] **It is unclear whether the theoretical claims can be generalized to real-world scenarios and datasets given the restrictive assumption made under the presented results.**\n  - In particular, Assumption 1 states that node labels follow a majority vote of discriminative patterns in core neighborhood, which is somewhat similar to network homophily. To verify this assumption on real-world datasets, Appendix E.1 provides additional analysis on the Cora network, which is known to be homophilic [A]. Thus, the question remains: \"does the assumption hold for heterophilic networks as well?\" Performing similar analyses on additional networks such as Actor and PascalVOC-SP-1G used in Section 5.2 would provide further support towards verifying this assumption.\n  - Similarly, the data model assumes that the data is balanced where discriminative and non-discriminative nodes are distributed uniformly across the graph. Is this also a reasonable assumption to make in various real-world networks?\n\n- [W2] **The proof sketch in Subsection 4.6 is not so intuitive and hard to follow.**\n  - Considering that the technical contribution of the paper is mostly theoretical, it would be great if the proof sketch stated what each lemma is arguing for intuitively, a rough picture of how each is proven, and how the Lemmas altogether culminate towards Theorem 4.1. The current proof sketch seems to be missing a significant portion that connects the lemmas to the main theorem, which makes the claims presented above somewhat unconvincing on their own.\n  - Several questions that could be clarified in the proof sketch are: \"Why do we use a two-phase-mini-batch SGD where we first solely train $W_O$?\" and \"How to the gradient updates of the weights $W_O$, $W_Q$, $W_K$, $W_V$, and $b$ roughly lead to the generalization guarantees in Theorems 4.1-4.3?\"\n\n[A] Pei et al., Geom-GCN: Geometric Graph Convolutional Networks. (ICLR 2020)"
            },
            "questions": {
                "value": "- [Q1] In Figure 8, why was the \"Uniform sampling\" method that samples nodes across the whole graph for feature aggregation chosen as a baseline? I don't think I fully understand what this baseline is hoping to convey.\n- [Q2] Also in Figure 8, where in the plot do we find that SPD-based PE \"correctly reflects the homophily, heterophily, and long-range dependency of these three datasets\"? Is it from where the performance peak of PE-based sampling is located along the x-axis?\n\nThere are a number of typos that could use some additional proofreading:\n- Page 4: \"We assme $\\kappa\\geq 4\\tau$\" -> \"We assume $\\kappa\\geq 4\\tau$\"\n- Figure 9 (x-axis title): \"Fraction of labeld nodes\" -> \"labeled nodes\"\n- The legend on the Left plot of Figure 9 is different from the other two.\n- Middle plot of Figure 9 has an additional y-axis."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6089/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6089/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6089/Reviewer_SSQZ"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6089/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698665868905,
        "cdate": 1698665868905,
        "tmdate": 1700715682498,
        "mdate": 1700715682498,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "41tYPtAp1E",
        "forum": "aJl5aK9n7e",
        "replyto": "aJl5aK9n7e",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6089/Reviewer_vpyX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6089/Reviewer_vpyX"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the learning and generalization of a shallow graph transformer for semi-supervised node classification with a self-attention layer with relative positional encoding and a two-layer perception. The sample complexity to achieve a zero-generalization error by SGD is provided, which suggests some key factors for improving generalization. Some interesting points for enhancing generalization are also revealed, supported by experimental results on synthetic and real-world benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper is well-motivated and makes a first attempt of studying generalization of graph transformers trained by SGD. Some interesting points have been raised such as the use of graph sampling for attaining better generalization performance.\n\nThe paper is well-written and easy to follow. The novelty and contributions are well-organized. This paper develops a novel and extendable feature learning framework for analysing optimization and generalization of graph transformers; this could be of interest to the community.\n\nIn addition to the theorems, the remarks provide some interesting comments/insights that would be interesting to the community."
            },
            "weaknesses": {
                "value": "It is unclear how the generalization obtained for GCNs differs from the existing ones using e.g., Rademacher complexity, algorithmic stability, and/or PAC-Bayesian. Comparisons or discussions should have been provided to make this clearer."
            },
            "questions": {
                "value": "1.\tGiven that graph transformers can be represented as GNNs, is it possible to employ existing generalization analysis of GNNs to investigate graph transformer? Why is the proposed analytical framework more suitable for graph transformer compared with the existing ones?\n2.\tIs the proposed analytical framework applicable to node classifications for heterophily graphs? How about other graph tasks such as graph classification?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6089/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698678993929,
        "cdate": 1698678993929,
        "tmdate": 1699636656452,
        "mdate": 1699636656452,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "p4QgLA6C1H",
        "forum": "aJl5aK9n7e",
        "replyto": "aJl5aK9n7e",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6089/Reviewer_j2s8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6089/Reviewer_j2s8"
        ],
        "content": {
            "summary": {
                "value": "This article conducts an in-depth theoretical research on the application of shallow Graph Transformers in semi-supervised node classification, yielding several intriguing conclusions. In particular, the author provides a well-reasoned justification for the improvement in generalization observed in shallow Graph Transformers. This enhancement is attributed to several key factors:  larger proportion of discriminative nodes, a more decisive consensus among these nodes, a smaller percentage of incorrect labels, and reduced errors in both the initial model and node patterns. Additionally, the article theoretically investigates the mechanisms through which self-attention leads to superior performance of Graph Transformers compared to Graph Convolutional Networks (GCNs). It also explores how positional embedding contributes to improved generalization by acting upon the core neighborhood."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This article has a rigorous theoretical derivation and proof. Considering that this is an article that examines the factors affecting GT performance from a theoretical perspective, this rigour is commendable.\n2. This article is valuable from a theoretical perspective in its analysis of the mechanisms by which discriminative nodes influence the ability of GTs to generalize."
            },
            "weaknesses": {
                "value": "1. In the introduction, the article states that \"where nodes with discriminative patterns in some so-called core neighborhoods determine the labels by a majority vote, while non-discriminative nodes do not affect the labels.\" This sentence is a bit confusing for me. What are nodes with discriminative patterns, and how are so-called core neighborhoods defined, since a larger fraction of discriminative nodes improves generalization performance is an important contribution of this paper, perhaps a more detailed explanation or a citation is needed here.\n\n2. It seems to be common knowledge that a smaller fraction of erroneous labels, and smaller errors in the initial model mentioned in the paper affect performance, and there doesn't seem to be a strong incentive to adopt a theory to analyze the mechanism in depth.\n\n3. The article focuses on the factors affecting the performance of the shallow Graph Transformer on the semi-supervised node classification task, and it is unclear whether the conclusions can be generalized to more complex frameworks, or to other downstream tasks."
            },
            "questions": {
                "value": "Please see the questions in the Weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6089/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6089/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6089/Reviewer_j2s8"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6089/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698836548464,
        "cdate": 1698836548464,
        "tmdate": 1699636656358,
        "mdate": 1699636656358,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UTW9T09HvJ",
        "forum": "aJl5aK9n7e",
        "replyto": "aJl5aK9n7e",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6089/Reviewer_Eng5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6089/Reviewer_Eng5"
        ],
        "content": {
            "summary": {
                "value": "This paper studies generalization capability of a graph transformer (a shallow GCN with a single-head self-attention layer) and present the relevant generalization results in terms of training sample complexity and several restrictive assumptions."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "Studying generalization of graph neural networks with attention is important and interesting."
            },
            "weaknesses": {
                "value": "My major concerns are about the restrictive assumptions and lack of comparisons about generalization on transudative/semi-supervised graph neural networks.\n\nFirst of all, the assumptions are rather restrictive compared to existing theoretical results in transductive/semi-supervised learning. \n\nFor example, it is assumed that the dataset is balanced, i.e., the gap between the numbers of positive and negative labels is at most $O(\\sqrt{N})$. I can hardly believe this will always happen for useful practical scenarios, and existing theoretical results for transductive learning, such as [1-2] (just name a few), do not require such assumption.\n\nAlso, Assumption 1 states that every node admits label consistent with majority voting, which is highly restrictive. It is pretty common in practice, such as noisy graph data with noisy labels, that such assumption may not hold.\n\nThe survey on generalization results about transudative/semi-supervised learning using graph neural networks is very sketchy, and a much more detailed comparison to this literature including [1-2] is indeed necessary. In particular, one would be curious how the obtained results in this paper can be compared to the sharp generalization bounds using local Rademacher complexity in [1]. \n\nMoreover, I encourage the authors to fix the typos and improve the presentation in the next submission. The current submission has too many unnecessarily marked texts, and most formulas are not punctuated.\n\n[1] Localized complexities for transductive learning, COLT 2014.\n\n[2] Learning Theory Can (Sometimes) Explain Generalisation in Graph Neural Networks, NeurIPS 2021."
            },
            "questions": {
                "value": "See weakness.\n\n**My feedback after reading the authors' response**\n\nI thank the author for their response for my comments. However, several key issues still remain unsolved. In particular, I mentioned that with noisy graph-structured data which is common in practice, the noisy node labels can easily be inconsistent with majority voting so all the theoretical results in this paper do not hold. The authors' argument using clean graph data does not apply to the noisy graph data concerned here. Also, Table 6 does not show that all the nodes satisfy the winning majority vote, while Assumption 1 requires all the nodes to satisfy the winning majority vote. I consider this as a very serious issue. In addition, the authors have a profound misunderstanding about the previous generalization works including [1-2, a]: these works not only give gap between the population risk and empirical risk, but also exhibit models, such as kernel methods as the training model where the training loss can be reduced to arbitrarily small value under mild conditions. As a result, I respectively disagree the authors' claim that other strong theoretical results are not addressing the same problem as that in this paper.\n\nDue to such concern and the highly restrictive assumption 1 which requires that the label of every node is consistent with majority voting, I decrease my score to 1. I encourage the authors to perform a deeper survey about existing research in transudative/semi-supervised learning.\n\nMore comment: after another proofreading of the paper, there are other unjustified assumptions, such as $\\kappa> 4 \\tau $ (under the title of Sec. 4.2), and it is not clear if it is reasonable or when it can hold. After verifying the proof of the paper, it turns out that the main reason for the authors to claim zero generalization error (in Theorem 4.1-4.3) is due to the highly restrictive Assumption 1: indeed, if every node admits a label by majority voting, then it is not surprising one can enjoy 100\\% test accuracy. However, again as shown in Table 6, there are still a fraction (relatively small but nonzero) of nodes not satisfying $\\Delta_n(z_m)>0$ required in Assumption 1. \n\n[a] Transductive Rademacher Complexity and its Applications. El-Yaniv et al. Journal of Artificial Intelligence Research 2009."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6089/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6089/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6089/Reviewer_Eng5"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6089/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1700702597875,
        "cdate": 1700702597875,
        "tmdate": 1701016495976,
        "mdate": 1701016495976,
        "license": "CC BY 4.0",
        "version": 2
    }
]