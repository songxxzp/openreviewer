[
    {
        "id": "cO2za11VXg",
        "forum": "ey3GhWXQ97",
        "replyto": "ey3GhWXQ97",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5492/Reviewer_ycxM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5492/Reviewer_ycxM"
        ],
        "content": {
            "summary": {
                "value": "This paper studies how many rounds $K$ of adaptive queries $n$ are required to learn MDPs with linear function approximation in $d$ dimensions, with 1 round corresponding to offline/batch setting, and $K = n$ corresponding to an online (or more accurately, generative access) setting. \n\nThey provide a lower bound and show that any algorithm which makes poly(d) total queries must use at least $\\Omega(\\log \\log d)$ rounds. In particular, the number of rounds for sample efficient algorithms must depend on the dimension. The proof leverages some recent results in subspace covering (Soleymani & Mahdavifar, '21)."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The paper is well written and easy to digest. The results / contributions are easy to pick out, and the proof sketch clearly captures the high-level intuition. The paper also makes clear the relationship to prior / related work.\n- The paper asks an important and interesting question about adaptivity, and the contribution is solid."
            },
            "weaknesses": {
                "value": "- In the later version, it would be nice to have diagrams illustrating the hyperspherical cap / sector as well as the policy class considered in the lower bound. \n- I still lack some intuition for how the policy class is constructed, and why it is hard to learn.\n- Perhaps some discussion on whether the lower bound can be improved, i.e., what sort of technical results are needed in order for the current construction to strengthen the lower bound.\n\nMinor typo: on page 21, the definition of $W$ needs to have \\exp. Also, where is $g(\\gamma)$ defined? I see it used in Lemma C.5, but I wasn't sure if it was properly defined anywhere."
            },
            "questions": {
                "value": "1. It wasn't obvious to me why finite horizon linear MDPs can be solved with number of deployments independent of $d$. More generally, is the gap really between finite horizon vs infinite horizon, or between linear MDPs and your $Q^\\pi$ realizability assumptions (4.1 and 4.2)?\n2. Do you expect to be able to improve the lower bound significantly? Perhaps, to log d or even some polynomial in d?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5492/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5492/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5492/Reviewer_ycxM"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5492/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698719693772,
        "cdate": 1698719693772,
        "tmdate": 1699636561159,
        "mdate": 1699636561159,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "A9gqxxccq8",
        "forum": "ey3GhWXQ97",
        "replyto": "ey3GhWXQ97",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5492/Reviewer_XsoY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5492/Reviewer_XsoY"
        ],
        "content": {
            "summary": {
                "value": "This paper studies sample efficiency in offline reinforcement learning with linear function approximation. In detail, it shows a sample efficiency separation gap between the adaptive setting (where the data is obtained in multiple batches, and the data obtained later is selected based on the information of the reward or transition induced by previous data) and the non-adaptive setting (where the data can only be obtained based on the linear feature itself, not on the unknown environment). The authors claimed that when the number of batches is less than $\\log\\log d$ where $d$ is the dimension of the linear feature, for any reasonable learner, there always exists a hard environment can not be learned by it efficiently."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The presentation of this paper is clear. \n\nThe theoretical proof is technically sound."
            },
            "weaknesses": {
                "value": "The importance of the proposed problem setting is unclear to me. Overall the problem setting tries to suggest that if number of adaptivity is too small (less than $\\log\\log d$), then the $d$ dimension feature space can not be fully spanned by given features induced by the data obtained so far, and the solution $\\theta$ which admits the value function (either optimal value function or value function associated with the target policy need to be evaluated) is not unique, which gives a large value gap when we consider the worst state-action pair we want to evaluate. However, it seems that such a separation gap becomes not important if we admit\n1.  A regret guarantee instead of a worst-case evaluation error guarantee (as defined in Def 3.1). For the regret guarantee, since we only care about the value gap over \\bf{existing} state-action pairs (due to the definition of regret) rather than the state-action pair that achieves the maximum which we may never faced before (again, due to Def 3.1), the nonuniqueness of $\\theta$ will not affect the regret as it affects the policy evaluation error. Is it true that such a lower bound does not hold if we slightly revise the definition of Def 3.1 from $P(\\sup_{a \\in \\mathcal{A}}(\\cdots))$ to $P(\\mathbb{E}_{a \\sim \\mu}(\\cdots))$ where $\\mu$ is some distribution over $\\mathcal{A}$?\n2. Any limitation for the value gap (e.g., the sub optimality gap defined in [1]) or the size of action space $|\\mathcal{A}$, or some coverage assumption which assumes that the obtained data can cover the target policy. To me it seems that if any of the above assumptions hold, then the separation gap does not hold either. Meanwhile, since these assumptions are also considered as mild assumptions (which hold in many settings), the proposed result in this paper is less interesting. \n\n[1] He, Jiafan, Dongruo Zhou, and Quanquan Gu. \"Logarithmic regret for reinforcement learning with linear function approximation.\" International Conference on Machine Learning. PMLR, 2021.\n\nMeanwhile, the problem setting and the hard instance construction are also very similar to that in Zanette 2021, which dwarfs the technique contribution in this work. More comparison between the technique difficulty faced in this work and that in Zanette 2021 are welcomed."
            },
            "questions": {
                "value": "See Weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5492/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698783571172,
        "cdate": 1698783571172,
        "tmdate": 1699636561029,
        "mdate": 1699636561029,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mUxTyMrLcJ",
        "forum": "ey3GhWXQ97",
        "replyto": "ey3GhWXQ97",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5492/Reviewer_7s3L"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5492/Reviewer_7s3L"
        ],
        "content": {
            "summary": {
                "value": "This paper studies multi-batch reinforcement learning, where the learner is allowed to query the oracle multiple times and adaptively adjust the policies. The goal is to evaluate a policy/identify the best policy. For this problem, the authors show that to achieve sample efficiency, the number of batches K has to grow at least on the order of loglog d."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "Significance of the contribution: This paper provides a novel lower bound for the number of batches needed to achieve sample efficiency for the multi-batch reinforcement learning problem, which is an excellent contribution. This lower bound shows that this number increases with loglogd, so a constant number is not enough, which is a very meaningful result. It also indicates that, adaptivity is necessary for achieving sample efficiency, and the dependence on d is not negligible. Finally, this result is also inspiring and leaves many questions open, such as whether this lower bound is tight, or how to obtain matching lower bound. \n\nNovelty of the method: The lower bound is constructed based on extending the proof techniques in Zanette (2021), but with novel tools such as subspace packing with chordal distance.\n\nPresentation: this paper is in general well-written and easy to read. I in particular appreciate the proof sketch section, which helps the reader understand the general proof idea."
            },
            "weaknesses": {
                "value": "As mentioned before, the paper provides a lower bound for K, but it is not clear whether it is tight.  loglogd is also sometime  considered as a constant is some works, since lnln 10^9 is only approximately 3. \n\nI have some questions on some details of the paper, which is listed in the next section."
            },
            "questions": {
                "value": "Questions:\n\nSection 4: \u201cwe assume gamma\\geq sqrt{3/4}\u201d. Can the authors provide some justification/context for this assumption? It is purely an assumption to make the analysis work?     \n\n\nThe proof of Theorem 4.4 (Appendix D) and that of Theorem 4.5 (Appendix E) seem very similar, in particular, some subsections, such as D.4.1 and E.4.1, seem to be exactly the same. So I was wondering what is the relationship between the two proof? Is one proof just an simple extension of another? \n\nEither way, I suggest the authors avoid copy-past proof so that the proof can be shorter. \n\n\nFor, Theorems 4.4 and 4.5, does the conclusion only holds for a particular pair of epsilon, delta, e.g., 1, 1/2, or the a more general conclusion can be easily obtained?\n\nI don\u2019t quite understand the second paragraph of Section 4, partly because it is a short paragraph with lots of information. In particular, the authors mentioned the established a *matching* lower bound for some problem while the upper bound is d. How is this lower bound relates to the main result?\n\nAt the beginning of Section 5, the authors mentioned that their proof extends that in Zanette (2021). However this paper is not mentioned at all in the rest parts of this Section. It would be great if the authors can compare the idea here with Zanette (2021) in the proof sketch section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5492/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5492/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5492/Reviewer_7s3L"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5492/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698956700702,
        "cdate": 1698956700702,
        "tmdate": 1699636560942,
        "mdate": 1699636560942,
        "license": "CC BY 4.0",
        "version": 2
    }
]