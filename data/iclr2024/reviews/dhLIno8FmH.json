[
    {
        "id": "kwFq7dVqbd",
        "forum": "dhLIno8FmH",
        "replyto": "dhLIno8FmH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2544/Reviewer_pm39"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2544/Reviewer_pm39"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a self-supervised framework: Natural Image Contrast EEG (NICE) to decode image from EEG"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Clarity: \nDescribes the proposed NICE model with very clear logic and diagrams"
            },
            "weaknesses": {
                "value": "Motivation: \nThe authors' main MOTIVATION consists in solving the following two problems (the problems mentioned in the second paragraph of the introduction are not sure whether they are motivations, so they are placed in the Question section): 1) Existing work has predominantly relied on supervised learning, often dealing with limited data from a few categories (introduction, first line of the third paragraph); 2) Limitations of the EEG feature extractor: convolution in the temporal and spatial dimensions respectively (the second sentence from the bottom of the third paragraph of introduction)\nW1:Indeed most of the existing EEG tasks (emotion recognition, motor imagery) are supervised learning, but Decoding tasks (such as EEG2speech, EEG2Image) are different from traditional EEG tasks. The essence of the task is cross-modal conversion rather than Classification, self-supervised learning is a very common method in decoding tasks. Comparisons with supervised learning for traditional tasks are therefore unreasonable.\nW2: Many existing EEG feature extraction methods focus on the spatial-temporal relationship of EEG, such as methods based on spatial-temporal graph, or spatial-temporal attention, or spatial-temporal convolution that is very similar to TSconv in NICE (Such as TSception). However, these very common methods are not mentioned in this paper, so this motivation is unreasonable.\n\nExperiment\uff1a\nW1: The main experiment only included one baseline, but there has been a lot of work on EEG vision decoding, such as Mind-Vis in CVPR. It is difficult to demonstrate the superiority of NICE by only comparing it with one baseline on a not commonly used dataset.\nW2: There is no analysis of why NICE achieves better results than BraVL.\nW3: In the Encoder comparison experiment, the baseline compared with TSConv did not include methods used for EEG feature extraction in the past five years."
            },
            "questions": {
                "value": "Q1: In the second paragraph of the Introduction, the author mentioned four problems with existing work. Are these problems the motivation of this paper? If so, why are some issues not mentioned or solved in the follow-up; if not, why are the issues that have not been addressed and paid attention to mentioned in the paper?\nQ2: If so, for P3: \u201coverlooking the inferior temporal cortex plays a necessary role in object recognition (fifth line)\u201d, the inferior temporal cortex is located at the bottom of the frontal lobe, and EEG mainly records signals from the surface layer, so how does NICE pay attention to it from EEG and how the signals from the inferior temporal cortex can help NICE perform Vision Decoding?\nQ3: For P4: \"focuses on the peak of pairwise decoding about 110 ms after stimulus onset. (fourth line from the bottom)\", the paper does not mention whether NICE's focus on the temporal dimension of the EEG signal is biologically interpretable, but P4 points out that the problem with the current model is that the focus of specific EEG time intervals is unreasonable"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2544/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698391768321,
        "cdate": 1698391768321,
        "tmdate": 1699636190906,
        "mdate": 1699636190906,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SrnG0qHTSz",
        "forum": "dhLIno8FmH",
        "replyto": "dhLIno8FmH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2544/Reviewer_9bdz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2544/Reviewer_9bdz"
        ],
        "content": {
            "summary": {
                "value": "The work presents a method for the task of EEG-based object recognition, using a recent dataset collected with the Rapid Serial Visual Presentation (RSVP) paradigm. The authors propose a self-supervised framework where an image and an EEG encoder network are used to extract visual and EEG features. During training, the method maximizes the similarity between positive pairs of samples (i.e. when the signals of the EEG sample of a pair are obtained from the image sample of the pair), and minimizes the similarity for negative pairs. After training, the authors perform zero-shot EEG decoding, by matching EEG features to visual features from image categories that were not present during training. Experiments are performed in one dataset, using various network architectures for EEG and image feature extraction, presenting results that are superior to the previous state-of-the-art and demonstrating the importance of EEG electrode areas, EEG frequency bands, and the temporal window timings of the input EEG signals.\n\nThe contributions are:\n1) an architecture for EEG feature extraction, named \"TSConv\", along with two plug-and-play attention modules that can be incorporated in the architecture\n2) the training methodology of the proposed method, called \"NICE\" (Natural Image Contrast EEG)\n3) the investigation of natural image information from EEG signals"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "There is some originality behind the work presented in the manuscript. The authors work on a recent RSVP dataset of EEG & visual data (Gifford et al., 2022), being among the first works that conduct experiments on the task of EEG-based object recognition using this dataset and presenting superior results compared to the previous state-of-the-art (Du et al., 2023). The contrastive loss employed during training, is different from contrastive losses that have been used in other works for EEG-based object recognition (e.g. in (Palazzo et al., 2021)).\n\nGifford et al., \"A large and rich EEG dataset for modeling human visual object recognition\", NeuroImage 2022\nDu et al., \"Decoding Visual Neural Representations by Multimodal Learning of Brain-Visual-Linguistic Features\", TPAMI 2023\nPalazzo et al., \"Decoding Brain Representations by Multimodal Learning of Neural Activity and Visual Features\", TPAMI 2021\n\nThe authors are aware of the challenges on the studied task, presenting a clear summary of previous works relevant to EEG-based object recognition. The presentation of the results provides clear insights for several factors that are involved (e.g. intra-subject versus cross-subject testing in Table 2, network architectures in Table 3, ablation studies on the attention modules in Table 2, electrode areas, EEG timings and EEG frequency bands in Figure 2)."
            },
            "weaknesses": {
                "value": "The proposed architecture for EEG feature extraction, TSConv (described in Table 1 of the manuscript), is highly similar to that of the ShallowNet (as described in (Schirrmeister et al., 2017)) architecture, which the authors include as a baseline method. Specifically, the ShallowNet architecture originally consists of a temporal and a spatial convolutional layer, followed by average pooling and a fully-connected classification layer. Leaving aside the differences on the employed activation functions between TSConv and ShallowNet, one could say that the difference lies only on the order of placement for the average pooling layer. Given that the results reported in Table 3 are quite different for these two architectures (i.e. ShallowNet is the worst performing architecture and TSConv is the best performing architecture), what could be the interpretation?\nSchirrmeister et al., \"Deep learning with convolutional neural networks for EEG decoding and visualization\", Human Brain Mapping 2017\n\nRegarding the two plug-and-play spatial modules, the authors state that they are \"instrumental in preserving the spatial characteristics of EEG channels\" (page 3, Section 3.1). Considering the self-attention (SA) module and looking at Equation (1), one can notice the spatial filtering nature of the proposed self-attention. However the authors do not provide any insight on the spatial characteristics of the signals obtained through Equation (1), i.e. are the signals more or less correlated after this operation? The authors also mention that, following this transformation of the SA module, \"Residual connection is employed by integrating the input and output of the SA module\", but it is not clear how much does this integrating procedure affect the spatial correlations between the EEG channels.\n\nThe graph attention mechanism proposed by the authors (page 5, Section 3.3.2) is similar to that of (Li et al., 2023).It would be interesting if the authors could explain the differences.\nLi et al., \"STGATE: Spatial-temporal graph attention network with a transformer encoder for EEG-based emotion recognition\", Frontiers in Human Neuroscience, 2023\n\nRegarding the intuition behind the proposed training methodology, the authors state that \"The self-supervised strategy allows us to learn inherent patterns from EEG signals without labels, rather than directly separate different classes with supervised learning.\" (page 4, Section 3.2.3). This argument is not well-supported, as image embeddings extracted from pretrained models like ResNet, CLIP and ViT may in fact contain class-related information (Van Gansbeke et al., 2020).\n\"SCAN: Learning to classify images without labels\", Van Gansbeke et al., ECCV 2020\n\nOther comments:\n\nThere are some typos, grammar and syntax issues throughout the manuscript, e.g.:\n- \"We device\" (page 2)\n- \"has remained a challenging.\" (page 2)\n- \"as the widely use way\" (page 3\n- \"representation similarity anaylasis\" (page 8)\n- \"categoring\" (page 8)\n- In Table 1, while s_1 is not presented in any layer's description, it is then used in the descriptions of output dimensions. I think that this is a mistake and that s_2 should be used instead.\n\nThis particular phrase is a bit vague and informal, the process could be stated more clearly: \"we look for a few images that belong to the image concepts\" (page 3)"
            },
            "questions": {
                "value": "Important information about the experiments is missing from the manuscript. The number of trainable parameters for each EEG encoding architecture is not reported. Moreover, the dimensionality of the EEG features extracted from each EEG encoding architecture, exactly before being mapped to the dimensionality of the image features, should also be reported. This would allow the reader to obtain a better understanding on the details behind each architecture's performance. For example, it could be the case (among other reasons) that some EEG encoding architectures have better performance because of a larger EEG feature dimensionality (hence an EEG-to-image feature mapping layer of larger learning capacity).\n\nThe results presented in Figure 3 are not sufficiently explained. It is unclear to what do rows and columns correspond, i.e. which dimension corresponds to EEG features and which corresponds to image features. It would also be helpful to get an idea of the similarities between the visual features from (some of the) concepts of the training set and the visual features from concepts of the test set.\n\nIn Figure 2, sub-figure (D) is not sufficiently explained, i.e. the exact method that was used for obtaining the time-frequency map is not mentioned, the quantity that is depicted is not stated and it is unclear to what does the range [-1, 0] correspond."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2544/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2544/Reviewer_9bdz",
                    "ICLR.cc/2024/Conference/Submission2544/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2544/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698602050936,
        "cdate": 1698602050936,
        "tmdate": 1700925105847,
        "mdate": 1700925105847,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0Z75jpBYiZ",
        "forum": "dhLIno8FmH",
        "replyto": "dhLIno8FmH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2544/Reviewer_JYif"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2544/Reviewer_JYif"
        ],
        "content": {
            "summary": {
                "value": "The authors present a deep learning pipeline to retrieve the images seen by participants whose brain activity was collected using EEG. The pipeline combines a pretrained frozen image encoder (CLIP, ViT or ResNet) and an EEG encoder (a parameter efficient ConvNet optionally prepended with a spatial attention layer) trained with a contrastive loss and evaluated in a retrieval scenario. Results favorably compare to an existing baseline and to existing neural network architectures. An analysis of the decoding performance and of the biological plausibility of the results highlights spatial, temporal and spectral patterns underlying the image decoding task."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Originality: The proposed approach combining a frozen image encoder and a trained brain encoder has previously been used on fMRI, however it appears its application to EEG data is novel.\n\nQuality: The manuscript is overall of good quality. The experiments adequately answer the core research question of the paper and provide supporting evidence for the physiological plausibility of the decoded patterns (i.e. through the complementary analyses of the decoding performance, spatial/temporal/spectral patterns and impact of number of stimulus repetitions).\n\nClarity: The manuscript is clearly written, and most figures and results are presented clearly.\n\nSignificance: This study goes beyond the existing image decoding results from EEG in which a limited number of classes were considered. It also is an important building block toward better image decoding from EEG, following in the footsteps of previous studies in fMRI."
            },
            "weaknesses": {
                "value": "- Some of the secondary claims do not appear to be adequately supported by the presented results. For instance, the claims on gamma-based decoding (see Q3 below) and on the use of occipital information in the graph attention layer (Q4). \n\n- Based on previous image decoding work in fMRI and EEG, a logical continuation of the proposed contrastive learning pipeline is to then pass predicted latents to a generative image model (see Q5). I believe the presented image decoding performance results (along with the analyses of Sections 4.5-4.8) are interesting in themselves, however as a result the scope of the paper is narrower than existing studies on the topic. I believe the authors should discuss this in their manuscript."
            },
            "questions": {
                "value": "Q1. In Section 3.1, the authors write \u201cThese images are processed and averaged to obtain one template for each concept.\u201d It is not clear to me how these averaged images are used in the experiments - was this in the semantic similarity analysis? Moreover, were the images averaged in pixel space or in latent space?\n\nQ2. In Figure 2A: What is plotted on the topomaps? I assume it is the average EEG amplitude of each channel across the 100 ms window. If so, how was the data normalized (since the colorbar is from -1 to 1)? Also, in Figure 2B: What do the shaded areas represent? Is it variability across subjects?\n\nQ3. In Section 4.5: \u201cHigh-frequency responses could be observed from electrodes on the temporal cortex.\u201d How is that measured? I find it hard to conclude from visual analysis of Figure 2D. Along with the chance-level performance of the gamma band (Figure 2E) I don\u2019t see compelling evidence for the following claim: \u201cThese results align with established principles, coarsely indicating that the bottom-up feedforward is carried by the synchronization of theta and gamma bands.\u201d \n\nQ4. In Figure 4C and in Suppl. Figure 1: If I understand the topomaps correctly it looks like graph attention actually mostly ignored occipital electrodes for most subjects (as opposed to self-attention for which the occipitally-located pattern is very clear). It would be interesting to look at the average attention weights predicted by both attention modules instead of using GradCAM for this kind of analysis. More generally, given the two formulations (self-attention and graph attention) end up predicting C weights, directly comparing their respective attention weights might give more insight into what drives their different performance. Finally, a central difference between the two approaches appears to be the use of a residual connection after the attention module - it would be interesting to evaluate whether that explains (part of) their differences.\n\nQ5. Existing image decoding studies, e.g. in fMRI (Ozcelik & VanRullen, 2023) but also in EEG (Palazzo et al., 2020), have extended a similar approach up to image generation by feeding predicted image latents to a generative image model. Have the authors attempted this step?\n\nOzcelik, Furkan, and Rufin VanRullen. \"Natural scene reconstruction from fMRI signals using generative latent diffusion.\" Scientific Reports 13.1 (2023): 15666.\nPalazzo, Simone, et al. \"Decoding brain representations by multimodal learning of neural activity and visual features.\" IEEE Transactions on Pattern Analysis and Machine Intelligence 43.11 (2020): 3833-3849.\n\nQ6. The description of the analysis of Section A2 is a bit confusing. The comparison appears to be made between (1) using pretrained frozen image encoders and (2) using randomly initialized image encoders which are trained along the EEG encoders. The description doesn\u2019t make it clear that the difference in computational resources between both approaches comes from the frozen/unfrozen dimension, not from the pretrained/non-pretrained dimension.\n\nQ7. Algorithm 1 describes the CLIP loss (Radford et al., 2021). I think this should be introduced and cited as such, rather than through the lens of InfoNCE.\n\nRadford, Alec, et al. \"Learning transferable visual models from natural language supervision.\" International conference on machine learning. PMLR, 2021."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2544/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698686765172,
        "cdate": 1698686765172,
        "tmdate": 1699636190750,
        "mdate": 1699636190750,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "H66CzSuMNB",
        "forum": "dhLIno8FmH",
        "replyto": "dhLIno8FmH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2544/Reviewer_ixpa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2544/Reviewer_ixpa"
        ],
        "content": {
            "summary": {
                "value": "The manuscript aims to match images to the EEG signals they evoke using a contrastive loss on the encodings of an EEG model and a vision model. The models are evaluated on zero-shot-discriminating 200 images, that were shown 80 times each, and the corresponding 80 EEG signals are averaged for each image.  The manuscript introduces an EEG model specific for this task and results show it to reach improved decoding performance over existing commonly used EEG models. Further, the manuscript finds that semantically similar images of the same category (like animal, food) have more similar learned EEG representations compared to images of different categories. Temporal analysis shows 100 to 600ms to yield the highest decoding accuracy. Frequency analysis show delta theta and beta bands contain information for reasonable decoding accuracies. The manuscript also performs a variety of spatial analyses."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The manuscript is well-written \n* method is straightforward to understand\n* a lot of detailed analysis of the results on different levels\n* helpful figures"
            },
            "weaknesses": {
                "value": "Some details I found not easy to understand, see below"
            },
            "questions": {
                "value": "I am not sure I understood the graph attention method details, I found the notation a bit confusing, does aT mean the result fo applying the feedforward layer is transposed? Also it would be nice to mention if this is a unique way of doing graph attention or a well-established way, then would be good to provide references for it. \n\nFor the GradCam Analysis, I am not sure I understood exactly which gradients of which layer do you use and with regard to what? These details may be helpful to write more clearly.\n\nA very recent work tackling similar problems is https://arxiv.org/abs/2310.19812. Due to its recency, I do not expect any comparison, but a brief mention of this work as concurrent work would be great.\n\nFont in Figure3 b could be bigger\nFigure 4 Colored stars could be bigger\n\n\nTypo p. 8: anaylasis"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2544/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699195223004,
        "cdate": 1699195223004,
        "tmdate": 1699636190693,
        "mdate": 1699636190693,
        "license": "CC BY 4.0",
        "version": 2
    }
]