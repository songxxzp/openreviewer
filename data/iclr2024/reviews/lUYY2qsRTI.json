[
    {
        "id": "N3esIAtYKJ",
        "forum": "lUYY2qsRTI",
        "replyto": "lUYY2qsRTI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5633/Reviewer_Lkaz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5633/Reviewer_Lkaz"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the problem of nonidentifiable confounding in RL. In this regard, a new notion of so-called delphic uncertainty is introduced in addition to aleatoric and epistemic uncertainties. An offline RL algorithm is proposed that penalizes taking actions with high delphic uncertainty. The performance is reported on both synthetic and real data."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The presentation is excellent with adequate illustrations that helped my understanding.\n- The new notion of uncertainty is insightful and important in the real application of RL.\n- The algorithm and employed strategies sound reasonable to me."
            },
            "weaknesses": {
                "value": "I generally enjoyed reading this paper, but there are a few things that I wish were discussed in more depth or clarified:\n1. I'm a little confused about how should I decide whether a world is compatible or not. Apparently, I can start with any confounder space dimensionality, prior $p(z)$ and model architectures and then estimate the parameters using ELBO, and then I have a compatible world? How far I can go here or what should I see to say a world is not compatible here?\n2. The behavior policy is in general a context-aware policy. So, I'd expect enforcing similarity to the behavior policy might result in some sort of context awareness. Is this the case? For instance, in the illustrative bandit example, an optimal context-independent policy only explores $a_0$ and $a_2$, which in World 2 is very different from the behavioral policy and seems to be suboptimal compared to a uniform policy. So, if I get it right, the similarity to behavioral policy is encouraged in this setting with unobserved confounders.\n3. As a similar question to the previous question, don't we expect avoiding actions $(s,a)$ with high delphic similarity to result in a policy more similar to the behavioral policy? It seems to be the opposite: page 8 \"... we also studied the discrepancy of our trained policy with that in the data. Particularly, we compared the actions taken by our policy and the policy in the data and found that ... our policy was significantly different.\"\n4. Could you elaborate on how counterfactual $Q_w^\\pi$ is estimated using importance sampling in Section 5.1? Also, I'm not sure where in appendix C is referred to.\n5. I'm not sure how to think about a reasonable $\\Gamma$. Isn't Figure 7 concerning?\n\nRecommendations:\nI do not see the name of real data or the details explicitly mentioned in the main text. But I can see, as you have cited, Raghu et al. have pioneered using RL in the setting of sepsis treatment using a publicly available MIMIC dataset. If this is not your data, I recommend reporting performance on MIMIC for further reproducibility. Also, looking at the recent citations of Raghu et al., it seems CQL might not be the best baseline here. For instance, Shirali, Schubert, and Alaa have included medical guidelines as potential contexts for RL formulation, with better performance and higher similarity to behavioral policy. You may want to consider more recent works as a baseline or use their observations in favor of context awareness."
            },
            "questions": {
                "value": "Please refer to the Weaknesses. I'm happy to update my scores after hearing your thoughts."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5633/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5633/Reviewer_Lkaz",
                    "ICLR.cc/2024/Conference/Submission5633/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5633/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698617076836,
        "cdate": 1698617076836,
        "tmdate": 1700519226466,
        "mdate": 1700519226466,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NBlfr4LHx0",
        "forum": "lUYY2qsRTI",
        "replyto": "lUYY2qsRTI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5633/Reviewer_qrjW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5633/Reviewer_qrjW"
        ],
        "content": {
            "summary": {
                "value": "The paper uses the contextual Markov Decision Process to model unobserved confounders in offline reinforcement learning. First, the authors define the class of contextual MDP that are compatible with the dataset. Then, based on a variance decomposition formula, the authors introduce the delphic uncertainty. Delphic uncertainty means the variance of policy performance across all compatible worlds. Then based on the delphic uncertainty term, the authors propose a penalty term for offline RL."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The method is applied to two real/semi-real-world medical datasets, which is very nice."
            },
            "weaknesses": {
                "value": "The probability setup of the paper is a bit unclear to me."
            },
            "questions": {
                "value": "1. On page 4, what does the notation $P_w \\mapsto \\Delta W$ mean? Do you mean $ P_w \\in \\Delta W$?\n\n2. Right after Def 4.1, the author proposes to model the Q function as a random element \"value model $Q_{\\theta w}$ is defined by some stochastic model\". Where is this randomness coming from? Isn't the value function of a policy just a deterministic function?\n\n3. Related to the previous remark, the meaning of Theorem 4.2 is unclear to me. Could the authors detail the probability setup for this theorem? From my understanding, there is a measure over the space of compatible worlds (by the notation $E_w$), and then there is a measure over Q-value functions (by the notation $E_{\\theta_w}$). What is the relationship between these measures?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5633/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698705496401,
        "cdate": 1698705496401,
        "tmdate": 1699636584916,
        "mdate": 1699636584916,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "weVMUVdZVb",
        "forum": "lUYY2qsRTI",
        "replyto": "lUYY2qsRTI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5633/Reviewer_x1vz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5633/Reviewer_x1vz"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the problem of offline learning an optimal independent policy. To achieve this goal, the authors propose the Delphic Offline RL algorithm that:\n1. identifies the compatible world model for confounded MDP and learn the world-dependent value function $Q_w^\\pi$;\n2. incorporates pessimistic policy optimization using the estimated delphic uncertainty;\nThe experimental results show that Delphic ORL method achieves better performance under large confounding strength."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is well motivated and the author proposed an interesting idea to incorporate the delphic uncertainty in pessimistic offline policy optimization. The writing is generally clear and easy to follow."
            },
            "weaknesses": {
                "value": "1. It seems that the estimator $\\mathbb{Var}_w (Q^\u03c0_w (s, a))$ is not an unbiased estimator for the delphic uncertainty as the other two forms of uncertainty can still enter the estimation (noting that $Q_w^\\pi$ is not the conditional expectation given by Theorem 4.2). I didn't see the author making effort to justify this point. \n\n2. Is it necessary to evaluate the delphic uncertainty on a state-action level rather than evaluating the same thing for the total reward of the whole trajectory under  policy $\\pi$? I'm not convinced of the soundness of the method here, as there might be some correlations between different state-action pairs in the Q function."
            },
            "questions": {
                "value": "It is assumed that the offline policy is known. What if $\\pi_b$ is unavailable so that the importance sampling method has biased in estimating the value function? Is it possible to incorporate other unbiased estimation method in confounded POMDP setting like (Shi. et al, 2022)?\n\n\n### Reference:\n\nShi, Chengchun, et al. \"A minimax learning approach to off-policy evaluation in confounded partially observable markov decision processes.\" International Conference on Machine Learning. PMLR, 2022."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5633/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5633/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5633/Reviewer_x1vz"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5633/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698732815802,
        "cdate": 1698732815802,
        "tmdate": 1700625972443,
        "mdate": 1700625972443,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "i32Fx1PUjq",
        "forum": "lUYY2qsRTI",
        "replyto": "lUYY2qsRTI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5633/Reviewer_3wVu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5633/Reviewer_3wVu"
        ],
        "content": {
            "summary": {
                "value": "* The authors address the unobserved confounding problem in offline reinforcement learning with pessimism over possible \"worlds\" (confounder values) compatible with the observation (distribution of trajectory).\n* They define a new type of uncertainty \"Delphic uncertainty\" as the variance of Q value over the compatible (thus unidentifiable) worlds with theoretical decomposition with other types of uncertainties.\n* Simulated evaluation and evaluation by experts clearly indicated that their method outperformed existing methods that do not address the Delphic uncertainty such as CQL and BC when strongly confounded."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "* The unobserved confounding is a major issue in offline reinforcement learning.\n* They investigate a minimal problem setting (contextual MDP) to reproduce it and propose a simple and intuitive method that models the uncertainty related to the confounding.\n* The theory that decomposes the variance into several types of uncertainties motivates the approach well.\n* Empirical evidence including evaluation by experts clearly supports their claim."
            },
            "weaknesses": {
                "value": "1. Baselines and environments tested are relatively limited (see also Question 1 and 2).\n1. Not being a major concern, it would be more intuitively superior if an end-to-end formulation was possible, as in the CQL. The proposed method is divided into a step of learning multiple possible worlds and a step of pessimism using them."
            },
            "questions": {
                "value": "1. Intuitively, it seems that estimating $z$ from the trajectory and using it as $\\pi(a|s,z)$ as in POMDP methods would improve performance for later steps $t$, but is such an extension possible? Also, is the proposed method still superior when such a POMDP method is used as a baseline? I'm wondering if the online identification of the world is possible within an episode through such a formulation.\n1. The existing approaches for a similar setting are discussed (e.g. using partial identification) but not compared. Isn't it possible to compare them?\n1. Is the $\\max$ taken w.r.t. not only $z,z'$ but also $s,a$? If not, how $\\Gamma(s,a)$ is summarized for an environment?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5633/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5633/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5633/Reviewer_3wVu"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5633/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698837594632,
        "cdate": 1698837594632,
        "tmdate": 1699636584735,
        "mdate": 1699636584735,
        "license": "CC BY 4.0",
        "version": 2
    }
]