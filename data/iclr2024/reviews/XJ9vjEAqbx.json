[
    {
        "id": "lPWkDYDF0F",
        "forum": "XJ9vjEAqbx",
        "replyto": "XJ9vjEAqbx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2912/Reviewer_NF3x"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2912/Reviewer_NF3x"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the problem of training a classifier to be robust to adversarial attacks wherein an adversary can arbitrarily perturb a test data point $X$ a neighboring data point $X+\\eta$ within a small ball of radius $\\epsilon$. A popular way to train such a classifier is Adversarial Training (AT) with a min-max objective where the attacker solves the inner maximization of the loss function and the defender solves the outer minimization of the worst case loss. The paper identifies that solving the inner maximization by replacing the 0-1 misclassification with a surrogate loss can lead to suboptimal adversarial perturbations, and subsequently, the defenders trained on such weak attacks result in suboptimal robust accuracy. Building upon this observation, the paper proposes to solve the inner problem of finding an adversarial perturbation by maximizing the margin gap of the classifier. Through experiments, the paper demonstrates that performing AT with the proposed attack leads to competent robust accuracies. Through experiments, it is also shown that the proposed change to AT also prevents robust overfitting phenomenon."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper correctly identifies the problem of using improper surrogates for the inner maximization in AT. \n- The paper hypothesizes that robust overfitting (RO) may be due to this problem of using improper surrogates for solving inner maximization in AT. Experimentally, it shows that RO can be eliminated by fixing this issue. This, I think, is surprising and worth exploring more from a theoretical viewpoint. \n- The proposed modification to AT is simple, principled, and free of heuristics."
            },
            "weaknesses": {
                "value": "**Misleading presentation:**\n\nI don't think the paper substantiates the claim made in the title, \"AT should be cast as a non-zero sum game\". The proposed \"non-zero sum\" formulation of AT in equations (16)-(17) is simply a tractactable reformulation of the standard zero-sum formulation in (11)-(12). The change is only in how the inner maximization is solved i.e., how the worst-case adversarial perturbation is found. \n\nIn Section 2.3 the paper claims that the tradeoff between robustness and accuracy as studied in many papers (in particular, Tsipras et. al. ICLR 2019) is a pitfall of the zero-sum formulation. However, the paper does not discuss how their supposed non-zero sum formulation resolves / ameliorates this trade-off either in theory or through experiments. There is also some misrepresentation of the prior work. For example, the robustness-accuracy tradeoff shown in Tsipras et. al. holds for the 0-1 robust loss. Clearly, the tradeoff is not caused by the use of improper surrogate loss.\n\nAnother claim is that the non-zero sum formulation eliminates robust overfitting. However, there is again no discussion on how the proposed non-zero sum formulation helps with this. \n\nOverall, the paper repeatedly stresses on the non-zero sum property of their new AT formulation, and how it overcomes the pitfalls of the standard zero-sum formulation. But the main contribution of the paper is a tractable reformulation of the same old zero-sum game, by addressing the problems that occur through the use of surrogate losses. \n\nThere are other instances of misleading presentation in addition to the above. For instance, section 3.1 is titled \"decoupling adversarial attacks and defenses\". I don't think the proposed formulation (16)-(17) decouples the attacks and defenses any more than the standard formulation. The outer minimization still needs solution to the inner maximization. \n\n**Missing prior work:**\n\nWhile the main contributions of the paper (i.e. the reformulation of AT and the algo to solve the reformulated objective) stem from the core issue of using surrogate losses for AT, there is a surprising lack of any discussion on the many works that study surrogate losses for AT. See for instance Bao et. al. COLT 2020, Awasthi et. al. Neurips 2021 and the references therein. \n\nAnother surprising omission is any discussion on consistency. I quote from the paper, \"Crucially, the inequality in (2) guarantees that the problem in (3) provides a solution that decreases the classification error (Bartlett et al., 2006), which, as discussed above, is the primary goal in supervised classification.\" This quote misses the main point of Bartlett et al., 2006. The solution to the surrogate loss minimization guarantees a solution to the 0-1 error minimization *if* the surrogate loss is \"calibrated\" or \"consistent\". While calibration and consistency are equivalent notions in standard classification (as shown in Bartlett et al., 2006), they are two separate things when it comes to adversarial classification. See the discussion in Munier et. al. Neurips 2022, and more recently Frank and Niles-Weed Neurips 2023."
            },
            "questions": {
                "value": "- I would really appreciate your comments on both the main weaknesses that I listed above. \n- It would be good to include a toy example where the proposed inner maximization retrieves the optimal adversarial perturbation while the standard inner max with surrogate loss does not do so. \n- How effective is BETA attack with a single PGD step, compared to FGSM?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2912/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698832217808,
        "cdate": 1698832217808,
        "tmdate": 1699636234778,
        "mdate": 1699636234778,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SUpjEH9Rs0",
        "forum": "XJ9vjEAqbx",
        "replyto": "XJ9vjEAqbx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2912/Reviewer_hcTf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2912/Reviewer_hcTf"
        ],
        "content": {
            "summary": {
                "value": "The authors propose to use different surrogate loss for attacker and defender in adversarial training and formulate it as a non-zero-sum game and a bilevel optimization problem. A new adversarial training algorithm is proposed (BETA-AT), which is an extension of the TRADES method at high level. The authors claim that the algorithm eliminates robust overfitting and achieves state-of-the-art robustness. Numerical results on the CIFAR-10 dataset are reported."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The authors discuss why traditional adversarial training suffers from the zero-sum-game formula with a shared objective function in details. Also, the numerical results show that the proposed method achieves equal or better performance than some existing methods."
            },
            "weaknesses": {
                "value": "1. I feel the contribution of this work is somewhat limited in the sense that the core idea of the algorithm is not very new. At high level, the proposed BETA-AT method is an extension of TRADES: in TRADES training, the attacker aims to maximize $\\max_{j\\neq f_\\theta(X)}M_\\theta(X+\\eta, f_\\theta(X))_j$, and BETA gives a try with each incorrect class. I am wondering if it is the usual case that the BETA-AT gives the same attack as from TRADES, given the classifier is correct?\n\n2. I expect more results for the experiments. Specifically, please consider the followings:\n\na) How does the training curve (as similar to Figure 1) of TRADES look like? Does it suffer robust overfitting in your setting?\n\nb) Does BETA-AT take significantly longer time to train? Can you provide numerical results comparing the training time of the methods considered in Table 1?"
            },
            "questions": {
                "value": "Please consider my questions in the previous section. I do not have additional questions in this section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2912/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2912/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2912/Reviewer_hcTf"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2912/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698900971175,
        "cdate": 1698900971175,
        "tmdate": 1700633158034,
        "mdate": 1700633158034,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wUGmRTKYSe",
        "forum": "XJ9vjEAqbx",
        "replyto": "XJ9vjEAqbx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2912/Reviewer_XrbP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2912/Reviewer_XrbP"
        ],
        "content": {
            "summary": {
                "value": "The paper attempts to show the standard zero-sum formulation of the adversarial training methods could lead to suboptimal results in practice. To resolve the issue, the authors propose a non-zero-game formulation of adversarial training problems where the classifier is trained using adversarial perturbations designed by optimizing a different margin-based loss function. This proposal results in the BETA-AT algorithm (Algorithm 2 in the paper). The numerical results in section 5 suggest improved results by applying the proposed BETA-AT method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1- The paper discusses the potential drawbacks of the zero-sum game approach to adversarial training which I find interesting.\n\n2- The paper numerically shows that the BETA-AT algorithm could improve the robustness of the neural nets and alleviate the robust overfitting issue."
            },
            "weaknesses": {
                "value": "1- While I find the paper's discussion on the drawbacks of zero-sum game formulation of adversarial training very interesting, I think the paper's proposal of considering a non-zero-sum game formulation for AT is not novel. The non-zero-sum game formulation of AT in Section 3.2 can be viewed as \"applying an effective attack algorithm to generate adversarial examples for training the neural network\". In that sense, the paper's main point could be viewed as \"one should use effective attack algorithms to find adversarial examples in adversarial training\". While this idea makes sense, some known variants of AT, e.g. adversarial training with examples generated by DeepFool attack, follow the same non-zero-sum game strategy: we do not optimize the adversarial perturbation by fully maximizing the classification loss and instead use a more effective attack algorithm.\n\n2- Based on the point I discussed above, it seems that the paper's main contribution is to propose an adversarial attack scheme using the margin-based loss function (equation 13) and then use this attack scheme for adversarial training. While the proposed cost function makes sense, the paper does not demonstrate that the margin-based function possesses a desired property that sets it apart from other attack schemes. I think the paper would be much stronger if it theoretically showed what is unique about the specific margin-based attack scheme. Does this attack scheme help with the optimization or generalization of the AT method? At this point, it is not clear how the authors theoretically justify the choice of cost function (13) over other adversarial attack algorithms like DeepFool.\n\n3- The paper's claim that the BETA-AT method helps with the robust overfitting issue is not well justified. The authors mention that \"AT algorithms which seek to solve (8) are ineffective in that they do not optimize the worst-case classification error. For this reason, it should not be surprising that robust overfitting occurs.\" However, the overfitted AT models usually achieve 100% training accuracy under any norm-bounded adversarial attack so they perform perfectly against any norm-bounded perturbation to training data. Does not their perfect training performance show that they have been trained against strong enough adversarial attacks on training data?\n\nI look forward to the authors' responses to the above questions and comments to assign my final score."
            },
            "questions": {
                "value": "1- What is the theoretical justification for applying the margin-based attack in (13) for designing adversarial examples over other adversarial attack methods? Does this attack scheme perform more effectively than other attack methods such as PGD or DeepFool? What is its fooling rate vs. the baseline attack methods when applied to standard image datasets?  \n\n2- (Weakness 3 above) The overfitted AT models usually achieve 100% training accuracy under any norm-bounded adversarial attack so they perform perfectly against any norm-bounded perturbation to training data. Does not their perfect training performance show that they have been trained against strong enough adversarial attacks on training data?\n\n3- Figiure 1 shows the training and test accuracy of the networks until epock 130~140. Does the same generalization comparison remain valid if the training continues for more epcohs (like 200 epochs)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2912/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699086857026,
        "cdate": 1699086857026,
        "tmdate": 1699636234645,
        "mdate": 1699636234645,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "50MdplYHcX",
        "forum": "XJ9vjEAqbx",
        "replyto": "XJ9vjEAqbx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2912/Reviewer_DH4f"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2912/Reviewer_DH4f"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a robust training (AT) algorithm for deep neural networks. Different from most existing two-player zero-sum AT algorithms in which the attacker and defender simultaneously optimized the same objective, the proposed AT algorithm is formulated in a novel non-zero-sum fashion, where the attacker and defender optimized separate objectives. This separated scheme prevents the proposed AT algorithm from robust overfitting."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed method resolves the issue of robust overfitting that is faced in existing zero-sum AT algorithms.\n2. The paper is very well-written. The presentation is clear and easy to understand.\n3. This paper points out the fundamental limitation in the popular zero-sum AT methods and provides a new non-zero-sum AT method, which would be an interesting future research direction for robust training."
            },
            "weaknesses": {
                "value": "1. The proposed AT algorithm has higher time complexity compared to zero-sum AT algorithms such as PGD, FGSM and TRADES."
            },
            "questions": {
                "value": "1. In Table 1. Why does BETA-AT$^{20}$ have lower test accuracy than BETA-AT$^{10}$ for clean data?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2912/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699253206466,
        "cdate": 1699253206466,
        "tmdate": 1699636234550,
        "mdate": 1699636234550,
        "license": "CC BY 4.0",
        "version": 2
    }
]