[
    {
        "id": "D7II3bm0HG",
        "forum": "zmJDzPh1Dm",
        "replyto": "zmJDzPh1Dm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7114/Reviewer_bzG1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7114/Reviewer_bzG1"
        ],
        "content": {
            "summary": {
                "value": "This paper answers the question \"do we need to normalize the soft prompts in VLMs?\" by (1) uncovering a phenomenon called the low-norm effect and (2) proposing a new method named normalizing the soft-prompt vectors of vision-language models (Nemesis) to normalize soft-prompt vectors in VLMs. The contributions include, (1) new soft-prompt vector normalization method for VLMs (normalizing soft prompts during soft-prompt tuning), (2) better results when evaluated by domain generalization settings for VLMs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(1) new soft-prompt vector normalization method for VLMs, which can be incorporated into any soft-prompt based methods;\n(2) better results when evaluated by domain generalization settings for VLMs."
            },
            "weaknesses": {
                "value": "1. prefer to learn more details of how you decide the length of soft prompt vectors, e.g., why 4 and 16, will there be more ranges to be investigated basing on the specificl tasks for VLMs?\n2. prefer to learn more investigations of combining Nemesis with existing PEFT algorithms to see if the results can be further improved or not so that other researchers can better leverage your method to their existing frameworks."
            },
            "questions": {
                "value": "1. could there be a combination of between soft-prompt tuning and hard-prompt tuning? (hard = explicitly use some predefined words/phrases as part of the prompts);\n2. any idea of further combining existing PEFT (prompt tuning, prefix tuning, LoRA...) with your Nemesis method?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7114/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7114/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7114/Reviewer_bzG1"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7114/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698453250577,
        "cdate": 1698453250577,
        "tmdate": 1699636841025,
        "mdate": 1699636841025,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jmBGXmUuc2",
        "forum": "zmJDzPh1Dm",
        "replyto": "zmJDzPh1Dm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7114/Reviewer_hcSQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7114/Reviewer_hcSQ"
        ],
        "content": {
            "summary": {
                "value": "The paper, at its core, explores the significant yet uncharted territory around the impact of norms of soft-prompt vectors on the performance of vision-language models (VLMs), like CLIP. The authors have brought to light a unique phenomenon termed the \"Low-Norm Effect\", highlighting how reducing norms of specific learned prompts can sometimes boost the performance of VLMs. The effect seems to be more prevalent in certain datasets like Imagenet, OxfordPets, and Food101 as compared to others. Interestingly, the Low-Norm Effect appears to have a stronger presence when there's limited training data, hinting at potential issues with soft-prompt methods under data constraints.\n\nTo harness this Low-Norm Effect, the paper proposes a method named \"Nemesis\". This approach introduces two techniques \u2013 Position Equality Normalization (PEN) loss and the more refined Position Awareness Normalization (PAN) loss. While the PEN loss aims to normalize the norms of all prompt vectors, the PAN loss is more discerning, identifying positions that might induce the Low-Norm Effect before selectively normalizing them. The authors suggest that this method can notably enhance VLM performance without incurring significant computational costs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper pioneers a systematic investigation into the role of soft-prompt vector norms in VLMs, addressing a previously unexplored research question.\n\n2. The proposed Nemesis method, with its innovative PEN and PAN losses, offers a potential solution to the Low-Norm Effect, showing promise for improving VLM performance.\n\n3. Extensive corruption experiments shed light on the Low-Norm Effect's impact, providing valuable insights for future soft-prompt tuning endeavors."
            },
            "weaknesses": {
                "value": "1. $\\beta$ can be either 0 or 1, corresponding to two variants of the proposed Nemesis method. However, there is no ablation study on the selection of $\\beta$, nor is there an exploration of the potential impact of setting $\\beta$ with decimal values to assign weights to the two methods.\n\n2. The paper introduces a pre-inference step before each training batch to identify positions inducing the Low-Norm Effect. Such a step could introduce computational overhead, especially with larger datasets or when rapid training iterations are required. The study hasn\u2019t provided a detailed analysis of the computational cost or time implications this might have in different scenarios.\n\n3. The Position Equality Normalization (PEN) loss applies equal weight to the norms of soft prompts at all positions. While the paper does acknowledge that normalizing prompt vectors at positions unaffected by the Low-Norm Effect may not yield performance improvement, the inherent assumption of the universality of the Low-Norm Effect across positions may not hold true for all datasets or real-world scenarios. The approach could benefit from a more dynamic, adaptive mechanism.\n\n4. The paper utilizes the RESCALE operation with a specific rescaling factor, \u03c4, described as a positive real number less than 1. However, there\u2019s no mention of how the value of \u03c4 is determined, if it's consistent across datasets, or its sensitivity. The choice of \u03c4 could have implications on the effectiveness of the Nemesis method, and without clear insight into its selection, there\u2019s potential variability in results."
            },
            "questions": {
                "value": "Given the significance of the parameter $\\beta$ in differentiating between the two variants of the Nemesis method, why was an ablation study not conducted to evaluate its impact? Additionally, have you considered exploring decimal values for $\\beta$ to potentially strike a balance between the effects of the PEN and PAN losses?\n\nHow does the proposed Nemesis method compare with other soft-prompt tuning methods in terms of computational efficiency and scalability, especially in larger datasets or more complex tasks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7114/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7114/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7114/Reviewer_hcSQ"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7114/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698676246781,
        "cdate": 1698676246781,
        "tmdate": 1700437796947,
        "mdate": 1700437796947,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "w5zyKrQ9Qf",
        "forum": "zmJDzPh1Dm",
        "replyto": "zmJDzPh1Dm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7114/Reviewer_KnC5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7114/Reviewer_KnC5"
        ],
        "content": {
            "summary": {
                "value": "The paper discussed the influence of soft-prompt to VLM, introduced REPLACE and RESCALE corruption affecting VLM, and proposed two normalization loss improving the performance of soft-prompt. The authors conducted a lot of experiments to confirm the effectiveness of method."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1\u3001The paper is the first study to discuss the influence of soft-prompt toward VLM.\n2\u3001The paper conducted REPLACE and RESCALE to discuss the normalization of soft-prompt, and proposed Nemesis including two normalization losses to improve the effectiveness of soft-prompt.\n3\u3001The paper has conducted a lot of experiments to prove the effectiveness of the method."
            },
            "weaknesses": {
                "value": "1\u3001The writing of some parts of the paper are not clear enough. It is recommended that the authors check. For example, there is a discrepancy between formula 4 and the symbol definition in the previous paragraph.\n2\u3001The two types of losses proposed in the paper lack a correlation with practical significance, suggesting authors discuss why the two forms of normalization affect soft prompt.\n3\u3001The paper lacks discussion on the applicable scenarios of two normalization losses."
            },
            "questions": {
                "value": "1\u3001The paper proposes two normalization methods, while only testing the effects of PEN and PAN on the experimental results respectively. Why cannot both types of losses be used simultaneously? If there is a contradiction between the two losses, it is recommended that the authors discuss the differences. If the two losses are similar, can the two losses be unified? If the two losses gain from different perspective, should relevant experiments be provided?\n2\u3001Can author discuss application circumstance of two normalization methods? In practical applications, what kind of normalization loss should we choose for what situation? Suggest the authors to discuss."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7114/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699194938030,
        "cdate": 1699194938030,
        "tmdate": 1699636840782,
        "mdate": 1699636840782,
        "license": "CC BY 4.0",
        "version": 2
    }
]