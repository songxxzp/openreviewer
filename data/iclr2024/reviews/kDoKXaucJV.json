[
    {
        "id": "a9FLrw4g5w",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6812/Reviewer_ETXH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6812/Reviewer_ETXH"
        ],
        "forum": "kDoKXaucJV",
        "replyto": "kDoKXaucJV",
        "content": {
            "summary": {
                "value": "The paper investigates the effectiveness of sparse coding-based network architectures as a defense against model inversion attacks (MIAs). More specifically, the approach uses sparse-coded layers in the beginning of a network to control and limit the amount of private information contained in those layers' output features. As a consequence, black-box model inversion attacks in a split-network setting should no longer be able to reconstruct the original (private) input features based on the intermediate outputs of the model. When compared to existing defense strategies (adding Gaussian noise to intermediate activations and augmenting training data with GAN-generated images), the proposed defense outperforms those strategies on the MNIST and Fashion MNIST datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Applying sparse coding-based networks to limit the information contained in the features of later layers in a network is an interesting and novel research direction, not only in the model inversion setting. I think the same approach might even be used in other privacy settings, e.g., membership inference attacks. I do not expect the paper to investigate these settings, just want to highlight possible extensions.\n- The results on MNIST and Fashion MNIST state a clear improvement above existing methods and promise better training-time efficiency. This is also underlined by the qualitative samples depicted in the paper.\n- The approach is well motivated, and the paper is overall well written."
            },
            "weaknesses": {
                "value": "- The evaluation is rather limited since it only conducts experiments on MNIST and Fashion MNIST, both datasets which are easy to fit by a network due to the overall low sample variance. Finding meaningful shared features in the sparse code layers is rather easy for the model. Also, the samples contain no private information at all. The evaluation should also contain more complex dataset evaluations, e.g., the common CelebA dataset, to prove that the approach is also usable within more complex tasks. Also, repeating the experiments with different seeds to provide a standard deviation of the results would make the evaluation more reliable.\n- The overall evaluation setting seems a bit strange. I understand the split-network setting and that reconstructing inputs given only the intermediate activations indeed can be a privacy breach. But why should the adversary have access to the activations of the training samples? I think a more realistic evaluation should consider unseen (test) samples and then try to reconstruct those given the intermediate activations.\n- Moreover, I think the approach should also be evaluated on common MIAs that utilize GANs to reconstruct training samples based only on the network's weights, e.g., [1, 2, 3]. Otherwise, the defense mechanisms should be positioned only for split-network (and federated learning) settings. Also, the approach should be compared to related information bottleneck defense approaches [4,5].\n- I think the overall technical contribution is rather low since the approach seems to be simply re-using the sparse coding layer framework of Rozell et al. (2008) and demonstrating that such networks can also act as a defense against MIAs. I still think the direction of the paper is interesting but the technical novelty seems limited.\n- The related work is comprehensive but mixes up different model inversion settings and approaches. For example, the [1] proposes MIAs that try to reconstruct features from specific classes by optimizing the latent vectors of a GAN. It uses the target model for guidance (and there exist much more works in this line of research, e.g., [2,3]). This is a completely different setting from the one investigated by the paper, which uses the intermediate activations of training samples to train a decoder-like model. I think a clearer separation between different types of MIAs would make the related work part stronger. Also, mixing works investigating the memorization of training samples in LLMs with vision-based inversion attacks might be confusing to the reader.\n\nSmall remarks:\n- Table captions should be above the table (Table 1)\n- The space after Table 2 should be increased (and manipulating the spaces might even run counter to the official guideline!)\n\n[1] Zhang et al., The Secret Revealer: Generative Model-Inversion Attacks Against Deep Neural Networks, CVPR 2020\n[2] Chen et al., Knowledge-Enriched Distributional Model Inversion Attacks, ICCV 2021\n[3] Struppek et al., Plug & Play Attacks: Towards Robust and Flexible Model Inversion Attacks, ICML 2022\n[4] Wang et al., Improving robustness to model inversion attacks via mutual information regularization, AAAI 2021\n[5] Peng et al., Bilateral Dependency Optimization: Defending Against Model-inversion Attacks, KDD 2022"
            },
            "questions": {
                "value": "- How much longer does it take to train a network using the Sparse-Guard architecture compared to a model without it?\n- Why is the FID metric valid to evaluate the privacy leakage? Generally, we are interested in how well a single sample can be reconstructed and less about recovering the overall feature distribution."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6812/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6812/Reviewer_ETXH",
                    "ICLR.cc/2024/Conference/Submission6812/Senior_Area_Chairs"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Note: Not an ethical concern, just want to point out that I'm not a first time reviewer for ICLR (the checkbox cannot be unchecked by me)"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6812/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697448034240,
        "cdate": 1697448034240,
        "tmdate": 1700464165587,
        "mdate": 1700464165587,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mGxCgZGZ0r",
        "forum": "kDoKXaucJV",
        "replyto": "kDoKXaucJV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6812/Reviewer_jF3v"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6812/Reviewer_jF3v"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes SPARSE-GUARD, a neural network architecture that leverages sparse coding to defend against model inversion attacks. It inserts sparse coding layers between dense layers which help remove unnecessary private information about the training data. Through extensive experiments on MNIST and Fashion MNIST datasets, the paper shows SPARSE-GUARD provides superior defense compared to state-of-the-art techniques like data augmentation, noise injection and standard sparse coding, while maintaining high classification accuracy."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The key strengths are the novel approach of using sparse coding for privacy protection, and code is available for reproducibility."
            },
            "weaknesses": {
                "value": "1. The attacks used in this study do not represent state-of-the-art techniques [1, 2, 3], and the baseline defense methods employed also fall short of the current state-of-the-art [4].\n2. The study relies solely on synthetic datasets like MNIST and FMNIST, lacking the inclusion of real-world datasets, such as facial recognition data, which could enhance the practical relevance of the findings.\n\n[1] Knowledge-Enriched Distributional Model Inversion Attacks, Chen et al., ICCV 2021\n\n[2] Plug & Play Attacks: Towards Robust and Flexible Model Inversion Attacks, Struppek et al., ICML 2022\n\n[3] Re-Thinking Model Inversion Attacks Against Deep Neural Networks, Nguyen et al., CVPR 2023\n\n[4] Bilateral Dependency Optimization: Defending Against Model-inversion Attacks, Peng et al. KDD 2022"
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6812/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698569967586,
        "cdate": 1698569967586,
        "tmdate": 1699636787725,
        "mdate": 1699636787725,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AGswPnTwg2",
        "forum": "kDoKXaucJV",
        "replyto": "kDoKXaucJV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6812/Reviewer_kXvF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6812/Reviewer_kXvF"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a novel architecture (Sparse-Guard) for defense against black-box model inversion attacks. It is demonstrated to be superior against state-of-the-art data augmentation and noise-injection-based defenses."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper, overall, is well-written and organized. The idea of interweaving sparse coding layers as a means of model-inversion attack is a novelty yet to be explored. Empirical analyses have also been provided to understand the mechanism behind the Sparse-Guard defense through UMAP 2D projections of output. Having openly accessible codebase is also a plus"
            },
            "weaknesses": {
                "value": "The paper does not do a good job at the exposition of how sparse coding is implemented. This is especially important as the implementation here seems to be *convolutional* sparse coding and differs from traditional sparse coding where matrix multiplication rather than a convolution is applied. e.g. (Bristow, Hilton, Anders Eriksson, and Simon Lucey. \"Fast convolutional sparse coding.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2013.)\n\nRozell et al. 2008 were cited for the update rule. However, in that paper, the update rule was not given for convolutional sparse coding. Either a more explicit derivation for the update rule can be given or a different citation would be relevant."
            },
            "questions": {
                "value": "The sentence \"The learned spatiotemporal representation closest to input image X is represented by this sparse presentation R_X\" is confusing. \nWhy is it a spatiotemporal representation? Where is the temporal element, all of the inputs are static images. Should 'sparse presentation' also be sparse representation?\n\nMultiple claims in the paper is made about sparse coding \u201cremoving unnecessary private information\u201d. This claim is not really supported by any study. In fact, the empirical study concluded that the effect of sparse coding layers is an \"unclustering effect\". How the conclusion of jettisoning unnecessary information is unclear. What is considered unnecessary information in the first place? In fact, it would be interesting to see if any other algorithm that produces the same unclustering effect will provide a similar effectiveness in defense against model inversion attacks."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6812/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698807876812,
        "cdate": 1698807876812,
        "tmdate": 1699636787612,
        "mdate": 1699636787612,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6SodVaEJKS",
        "forum": "kDoKXaucJV",
        "replyto": "kDoKXaucJV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6812/Reviewer_CjY9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6812/Reviewer_CjY9"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to defend against model inversion attacks by inducing sparse coding into DNNs. The key design is an alternating sparse coded and dense layers that discards private information. Experiments show effective defenses on MNIST and Fashion MNIST."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The method maintains great privacy with little training computation overhead and accuracy loss\n2. A cluster-ready PyTorch codebase is provided for future study\n3. The paper is well motivated and easy to follow"
            },
            "weaknesses": {
                "value": "The major drawback is that the experiments are only conducted on simple, low-resolution datasets. I do not think the results in small datasets convincingly validate the effectiveness of the proposed method. There exist lots of model inversion attacks that are capable of extracting high-resolution data, from CIFAR-10, CelebA, to ImageNet. Since high-resolution images are much more valuable as training data, it is the high-resolution model inversion attacks that post private threats. And an effective defense would be significant in that case.\n\n[1] MIRROR: Model Inversion for Deep Learning Network with High Fidelity\n\n[2] Plug & Play Attacks: Towards Robust and Flexible Model Inversion Attacks\n\n[3] Re-thinking Model Inversion Attacks Against Deep Neural Networks\n\n[4] Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures"
            },
            "questions": {
                "value": "Response to rebuttal: Thanks for the strong rebuttal with great efforts! I raised my score to 5 based on experiments on CIFAR10 and Plug-and-play advantage."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6812/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6812/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6812/Reviewer_CjY9"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6812/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698819695082,
        "cdate": 1698819695082,
        "tmdate": 1700766857428,
        "mdate": 1700766857428,
        "license": "CC BY 4.0",
        "version": 2
    }
]