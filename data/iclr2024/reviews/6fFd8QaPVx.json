[
    {
        "id": "0pYfMLPbmY",
        "forum": "6fFd8QaPVx",
        "replyto": "6fFd8QaPVx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4273/Reviewer_HiaQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4273/Reviewer_HiaQ"
        ],
        "content": {
            "summary": {
                "value": "This paper studied binarized convolutional neural network and proposed to replace nxn 2D BNN by two 1D BNN (nx1 row-wise BNN and 1xn column-wise BNN). Further, the paper designed two basic BNN blocks, 1-D binarized convolutional layer in Figure 1 and downsampling 1-D binarized convolutional layer in Figure 2. Adjustment of activation distribution are analyzed in Section 3.3. By combining above two basic blocks and adjustment of activation distribution, the paper built the architecture based on ResNet18 for image classification tasks.The experimental results validate the effectiveness of the proposed binarized convolutional neural network."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper provided detailed  analysis of proposed binarized convolutional network network from both theoretical and experimental perspectives.\n2. The designed architectures achieved SOTA performances on CIFAR10 and imagenet. \n3. The author's writing is very good, and the entire paper is relatively easy to understand.\n4. simple algorithm, easy to follow."
            },
            "weaknesses": {
                "value": "1. from engineering viewpoints, the proposed basic blocks seems obviously due to 1\uff09the work of binarizing nx1 and 1xn convolutional neural network existed, and 2) adjustment of activation distribution also existed. The combination of them is not novel enough for top conferences. Thus, the contribution of this method is not very important.\n2. The reference format in this paper can be improved, e.g. \n - \"Reactnet: Towards precise binary neural network with generalized activation functions\" is an ECCV paper, \n - \"Recu: Reviving the dead weights in binary neural networks\" is an ICCV paper.\n3. In provided tables, some columns are left aligned, while others are center aligned. It is best to use a consistent format"
            },
            "questions": {
                "value": "see weaknesses above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4273/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4273/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4273/Reviewer_HiaQ"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4273/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697767859464,
        "cdate": 1697767859464,
        "tmdate": 1699636394870,
        "mdate": 1699636394870,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "t3xJrARyzQ",
        "forum": "6fFd8QaPVx",
        "replyto": "6fFd8QaPVx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4273/Reviewer_6xWT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4273/Reviewer_6xWT"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new binary neural network called OneBNet, which mainly replaces NxN 2D binarized convolution by Nx1 row-wise and 1xN column-wise 1D binarized convolutions. The proposed model shows strong performance on ImageNet, i.e., ResNet18-based model obtains 63.9% Top-1 accuracy by training from scratch and 68.4% Top-1 accuracy by applying teacher-student training."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ The proposed method is easy to follow.\n+ Latency on Raspberry Pi is reported, which is beneficial for the BNN community.\n+ The proposed model shows strong performance on ImageNet, i.e., ResNet18-based model obtains 63.9% Top-1 accuracy by training from scratch and 68.4% Top-1 accuracy by applying teacher-student training."
            },
            "weaknesses": {
                "value": "-\tIt is not new to replaces NxN 2D binarized convolution by Nx1 and 1xN 1D binarized convolutions. For example, SqueezeNext (CVPR\u201918 workshop) decompose the KxK convolutions into two separable convolutions of size 1xK and Kx1. Although this paper focuses on binary neural network, the novelty of using such strategy for BNNs is still quite unclear.\n-\tCould the proposed method also suitable for object detection tasks using binary neural network?\n-\tSeveral recent methods reported in Table 2 are not included in Table 3."
            },
            "questions": {
                "value": "See the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4273/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698659166464,
        "cdate": 1698659166464,
        "tmdate": 1699636394797,
        "mdate": 1699636394797,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BqYl8M9ssS",
        "forum": "6fFd8QaPVx",
        "replyto": "6fFd8QaPVx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4273/Reviewer_w6Pw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4273/Reviewer_w6Pw"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to decompose a 2D convolution into two 1D convolutions on a Binarized neural network model to improve inference speed and model accuracy on edge devices. On the basis of the previous Binarized ResNet, the 3x3 convolutional kernel was changed to two sets of 1x3 and 3x1 convolutional kernels, achieving higher accuracy."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "This paper is easy to read and understand. The method in this paper is relatively simple, clear, and easy to reproduce. The experimental data in the paper indicates that the improved model outperforms the previous Binarized neural network model in terms of speed and accuracy."
            },
            "weaknesses": {
                "value": "1. The contribution and innovation of this paper are insufficient. The decomposition of 2D convolutions into two 1D convolutions used in this article is not a new idea, but a widely studied method. Although its combination with Binarized neural networks may make it more effective, it is easy to consider or attempt.\n\n2. The generalizability of this method has not been verified. The author's experiment only trained and tested the smaller ResNet model, and the dataset only included CIFAR10 and ImageNet.\n\n3. The basic theory of the method in this paper is not sound enough. Binary quantization and 2D convolutional decomposition are methods that sacrifice accuracy for less computational complexity. Why can a combination of the two achieve better accuracy? Substantive improvements may come from element wise calculations and learnable bias in more layers after decomposition, but this is not without cost, as FLOPs cannot accurately reflect the additional hardware overhead this brings. The explanation of Figure 3 also lacks quantitative data support.\n\n4. This method lacks a determination method for parameter selection. The paper mentions that not all convolutional layers of blocks are suitable for such transformations. In Table 1, several selection combinations are attempted, and the best ones are selected for subsequent comparison. This will bring difficulties to practical applications. If the model structure is different and there are more blocks with different channel numbers, it will not be suitable for such selection.\n\n5. The comparison of experiments lacks fairness and universality. Compared to other methods in Table 3, they are all different binary quantization methods and will not significantly change the model structure. This paper essentially changes the structure of the model orthogonal to previous work. Unless compared with other structural optimizations, such changes are unfair."
            },
            "questions": {
                "value": "In experimental environments, it said \"Like other BCNN models, the first convolutional and last fully-connected layers adopted FP32\nweights and activations.\" Is the latency data measured End2End? If not, are non binarized layers becoming performance bottlenecks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4273/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4273/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4273/Reviewer_w6Pw"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4273/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698672249585,
        "cdate": 1698672249585,
        "tmdate": 1699636394723,
        "mdate": 1699636394723,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zJk7fqsTFn",
        "forum": "6fFd8QaPVx",
        "replyto": "6fFd8QaPVx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4273/Reviewer_38Cc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4273/Reviewer_38Cc"
        ],
        "content": {
            "summary": {
                "value": "The work proposes decomposing 2-D binarized convolution into two 1-D convolutions in different directions to reduce model complexity. Then, the paper investigates the settings where such replacement can be beneficial, followed by experimental verification of the architecture performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This work identified the proper settings where replacing 2D convolutions with 1D convolutions can be beneficial. The model performance shown in the paper is promising."
            },
            "weaknesses": {
                "value": "Overall the paper should improve on the presentation befoer it is ready for publication. At current state, I am unsure about many technical details. Please also see below."
            },
            "questions": {
                "value": "P3: section 3.1 The section is very difficult to follow, please rewrite this.\n\nP4: \"However, it shrinks the receptive fields\", I am not sure whether this is true, please verify.\n\nFig. 2. It is not clear how the results from 1D convolution are summed together with ouput of the 1x1 FP32 convolution, given they are of different shape.  \n\nP5: not sure about \" It removes the negative part with \u03b2\" \n\nP5: Not sure why there should be both  \u03b6 and \u03b1\n\nFigure 5: please change Fig. (a) to (a)\n\nTable 1 is very confusing, please clear it up.\n\nDistillation is quite a standard approach for improving model performance. Therefore, I suggest the authors move the results from the distillation experiment to the appendix. \n\nI suggest adding a model performance comparison with a similar OP level. \n\nReActNet presented results of much higher performance (probably higher complexity); I suggest including an experiment that compares those results."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4273/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698740124136,
        "cdate": 1698740124136,
        "tmdate": 1699636394624,
        "mdate": 1699636394624,
        "license": "CC BY 4.0",
        "version": 2
    }
]