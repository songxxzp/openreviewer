[
    {
        "id": "fOLtonJKJh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6930/Reviewer_q1QA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6930/Reviewer_q1QA"
        ],
        "forum": "3GDKJSQnW2",
        "replyto": "3GDKJSQnW2",
        "content": {
            "summary": {
                "value": "This paper targets video editing, especially for the non-rigid, motion editing in the video. Two novel techniques are proposed.\n\n(1) prompt pivoting tuning using a masked target prompt\n\n(2) Spatial-temporal focusing which fuse the cross-attention map of pivotal tuning and editing\n\nThis method is compared with tune-a-video, text2video-zero in many quantitative and qualitative metrics"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Provide many quantitative results in Figures 4,7 of the main paper, Table 1,2, Figure 11 of the appendix"
            },
            "weaknesses": {
                "value": "My largest concern is how the results if viewed in a dense video format like mp4 or gif. For results in sparse discrete sequences in Figures, 6, 7, and 8, the difference in two consecutive is quite large.\n\nAnother point is the \u201cdynamic motion\u201d in the paper can be better defined. Currently, the concept of \u201cdynamic motion\u201d is closer to the domain of \u201cnew pose\u201d, including \u201cjumps\u201d, \u201cdance\u201d, and \u201cjump\u201d. If it is a pose editing problem, a fair starting point can be a pose-driven controlnet or a text-to-pose model. If the paper targets a larger open-domain \u201cmotion\u201d, like camera motion, fluid, and liquid motion. The better starting point is the motion prior in a pre-trained video model, because fundamentally, pre trained stable diffusion does not have knowledge about open-domain \u201cmotion\u201d."
            },
            "questions": {
                "value": "Hope the video or long sequence results (including mp4, gif or PNG frames) can be further provided for evaluation.\n\nConsidering the time limit of rebuttal, I understand evaluation on open-domain motion can be difficult and that is not expected."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6930/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6930/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6930/Reviewer_q1QA"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6930/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697420893924,
        "cdate": 1697420893924,
        "tmdate": 1699636807827,
        "mdate": 1699636807827,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GQNOvhyvCo",
        "forum": "3GDKJSQnW2",
        "replyto": "3GDKJSQnW2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6930/Reviewer_9TJK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6930/Reviewer_9TJK"
        ],
        "content": {
            "summary": {
                "value": "The paper proposed a Pivotal Dynamic Editing (PDEdit) framework for text-based video editing task.  PDEdit allows non-rigid edits to a general video using the target text. PDEdit is versatile and effective for editing various types of videos. Extensive experimental results reveal the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. A versatile video editing method that allows non-rigid editing is proposed.\n\n2. The motivation is clear, and experimental results reveal the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1. The concept of \"non-rigid edit\" is difficult to understand, existing methods (e.g., Tune-A-Video) can edit the input video so that the motion is inconsistent with the original video, isn't this non-rigid editing?\n\n2. The proposed PDEdit involves many hyperparameters to achieve good results, and requires additional Charades-STA dataset, which makes the practical application process full of challenges.\n\n3. Since the prompt corresponding to the original video is not used, if the target prompt has a low correlation with the original video, will this cause the first tuning stage to fail?\n\n4. Some \"rigid edit\" methods such as Gen-1, VideoComposer and Control-A-Video, which incorporate structure guidance from input videos to generate videos, should also be discussed in the related work.\n\n5. Lack of comparison with existing open-source editing methods such as Pix2video and Video-p2p.\n\n6. Authors should include edited sample videos in supplementary materials or on anonymous websites for easy comparison."
            },
            "questions": {
                "value": "See Weaknesses for more details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6930/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698396638796,
        "cdate": 1698396638796,
        "tmdate": 1699636807697,
        "mdate": 1699636807697,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZNE4rWgkb0",
        "forum": "3GDKJSQnW2",
        "replyto": "3GDKJSQnW2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6930/Reviewer_UHtg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6930/Reviewer_UHtg"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on spatial-temporal non-rigid video editing, a relatively underexplored area in video editing tasks. The proposed method, Pivotal Dynamic Editing (PDEdit), distinguishes itself by not requiring original video captions but editing videos based on suggested prompt pivoting. The method demonstrates a degree of generality."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Unlike previous video editing approaches, which mainly focus on simple edits like appearance and style, this paper pioneers motion editing, showing a forward shift in research focus.\n- The method proposed here is innovative, deviating significantly from existing pipelines.\n- The writing is clear and relatively easy to follow."
            },
            "weaknesses": {
                "value": "- The visual results presented in the paper are subpar. While the difficulty of the task is understood, the demonstrated demos are unsatisfactory in their current state.\n- The experimental section is weak, lacking explicit numerical comparisons in the main text. The experiments are limited to only 20 videos, which is a small sample size. The comparison metrics lack persuasiveness.\n- Although supplementary materials include editing demos, the videos appear to lack continuity, and there seems to be a limitation in supporting video editing across different resolutions."
            },
            "questions": {
                "value": "See above. \n- I am uncertain about how many frames of video the author's method supports editing. It appears to be limited to very short 8-frame videos, which have limited practical significance.\n- Stacking images as a presentation method is not ideal; I would prefer actual demos in GIF or video format.\n- Despite these shortcomings, I recognize the contribution of the authors to this underexplored area. Their method might inspire future research. However, at the current stage, I can only give a borderline score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "no"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6930/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698410953172,
        "cdate": 1698410953172,
        "tmdate": 1699636807549,
        "mdate": 1699636807549,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rc1J7LiRY4",
        "forum": "3GDKJSQnW2",
        "replyto": "3GDKJSQnW2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6930/Reviewer_1N5p"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6930/Reviewer_1N5p"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a method for text-based video dynamic editing, which involves making non-rigid spatial-temporal alterations.  They propose pivotal prompt tuning for the system to tune a video with target text prompt and temporal dynamic editing to apply motion changes in spatial and temporal domain."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper proposes a novel approach to text-based video dynamic editing\n2. The video frames shown in the paper demonstrate edited motion similar to the text-prompt inputs."
            },
            "weaknesses": {
                "value": "1. The absence of accompanying videos in a video editing paper makes it challenging to assess the temporal consistency of the edited frames.\n2. The quality of the editing falls short of expectations, with significant color discrepancies and noticeable object alterations post-editing. It seems like the unedited area is not preserved after editing for the applications outside of style transfer.\n3. The quantitative evaluation should also evaluate how this editing method preserves the unedited area. For example, include PSNR, SSIM or LPIPS for the unedited background area for motion editing."
            },
            "questions": {
                "value": "1. In distributional pivoting, do you need to tune the parameter for s* for different videos?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Video motion editing could be abused for illegal behavior that should be concerned."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6930/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6930/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6930/Reviewer_1N5p"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6930/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698876362596,
        "cdate": 1698876362596,
        "tmdate": 1699636807423,
        "mdate": 1699636807423,
        "license": "CC BY 4.0",
        "version": 2
    }
]