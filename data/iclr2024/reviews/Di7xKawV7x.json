[
    {
        "id": "mnB3b1xVdw",
        "forum": "Di7xKawV7x",
        "replyto": "Di7xKawV7x",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2207/Reviewer_qwYJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2207/Reviewer_qwYJ"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces on model compression method, which is with low complexity, compared to the typical NAS. \nThe experiment shows that the 50% reduction is only with 1% performance loss. Besides, the author introduces its procedure of the math derivation. \n\nBut, such method is not novel enough, and similar idea (optimize the mask) has been widely discussed. \nSecondly, model compression and quantization should be widely experimented in diverse models and modules."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The experiment shows that the 50% reduction is only with 1% performance loss. Besides, the author introduces its procedure of the math derivation. \nThe paper is with a well-written, and clearly present the contents."
            },
            "weaknesses": {
                "value": "But, such method is not novel enough, and similar idea (optimize the mask) has been widely discussed. \nSecondly, model compression and quantization should be widely experimented in diverse models and modules, e.g., conv, transformer, linear."
            },
            "questions": {
                "value": "One question that arises is: what is the relationship between model compression and transfer learning? Do they occur in the same pipeline? For instance, is model compression done within the transfer learning process? Or is model compression performed first, followed by transfer learning to enhance performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2207/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2207/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2207/Reviewer_qwYJ"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2207/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698762415138,
        "cdate": 1698762415138,
        "tmdate": 1700734347042,
        "mdate": 1700734347042,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gr8rIkjyxE",
        "forum": "Di7xKawV7x",
        "replyto": "Di7xKawV7x",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2207/Reviewer_Hf33"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2207/Reviewer_Hf33"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose an end-to-end framework for model compression. Specifically, the FLOPs is regarded as the target of optimization via l1/l2 latency surrogate. The experiments on MobileNetV3 and BERT show that the proposed method can achieve a cheaper architecture with almost the same amount of time as a single model training run."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper propose an end-to-end model compression method via optimizing the FLOPs.\n\n2. The l1/l2 regularized surrogate is employed to optimize for the latency of the compact neural networks.\n\n3. Experimental results on language and vision tasks demonstrate that the proposed method can find a more compact architecture."
            },
            "weaknesses": {
                "value": "1. The authors claim that the proposed method can be used with pruning, low-rank factorization, quantization etc. However, the quantization is not verified on the experiments.\n\n2. The performance on different devices can be various, so I wonder is the proposed method still effective on other device?"
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2207/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698827039260,
        "cdate": 1698827039260,
        "tmdate": 1699636154836,
        "mdate": 1699636154836,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "saTeAU8zWy",
        "forum": "Di7xKawV7x",
        "replyto": "Di7xKawV7x",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2207/Reviewer_mcQZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2207/Reviewer_mcQZ"
        ],
        "content": {
            "summary": {
                "value": "This paper presents an end-to-end compression technique to compress deep models using a $l_1/l_2$ regularizer. The algorithm is versatile and fast and can be applied to different compression techniques, including pruning, low-rand factorization, and quantization. The authors build extensive experiments on various tasks, including BERT compression on GLUE fine-tuning tasks and MobileNetV3 compression on ImageNet-1K."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The writing is clear and easy to understand.\n- The proposed technique can be used with popular compression methods such as pruning, low-rank factorization, and quantization.\n- The authors built experiments on various domains, including the pre-training and transfer learning tasks on CV and NLP benchmarks, like BERT compression on GLUE fine-tuning tasks and MobileNetV3 compression on ImageNet-1K."
            },
            "weaknesses": {
                "value": "- The main concern is the limited novelty of the work. The idea of making the FLOPs constraint differentiable is commonly used in many pruning works, such as using auxiliary masks. The positivity constraints are crucial for pruning models. However, as mentioned in the work, similar ideas have been studied in existing works.\n- It is recommended to evaluate the performance of the proposed method multiple times, as it is claimed to be more stable than other methods. \n- Direct comparison with advanced compression methods is also encouraged, and it would be better to show the training cost comparison since the proposed method is claimed to be fast."
            },
            "questions": {
                "value": "Please refer to the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2207/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2207/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2207/Reviewer_mcQZ"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2207/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699100278959,
        "cdate": 1699100278959,
        "tmdate": 1699636154777,
        "mdate": 1699636154777,
        "license": "CC BY 4.0",
        "version": 2
    }
]