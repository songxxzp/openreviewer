[
    {
        "id": "cs3fKTCUdZ",
        "forum": "hgDDyoWQt3",
        "replyto": "hgDDyoWQt3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8190/Reviewer_Z7Er"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8190/Reviewer_Z7Er"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to use Large Scale Language Model to identify infeasible pairs for open world compositional zero shot learning (ow-czsl). Speicifically, the authors propose In-context Learning to ensure the feasibility of the composition, which utilize a few examples of true feasible pairs, allowing the LLMs to learn from these instances and better understand what constitutes feasibility within the dataset. The experiments demonstrate that the proposed method improves over the existing methods that identify compositions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.The paper is well-written and easy to follow.\n2.According to the experiments, FLM achieves noteworthy improvement in performance."
            },
            "weaknesses": {
                "value": "1.Time costs need to be taken into account. As is known to all, the open-word setting will produce a large number of virtual compositions, which will bring a huge amount of calculation to the model (the proposed model process all possible pairs once and predict the score).\n2.According to the paper, the In-context Learning seems not to be trained in the process, which means that the performance relies on the LLMs. However, there existing some objects and states that are totally unknown to LLMs. In this situation, the proposed model cannot transfer the knowledge to the unknown compositions.\n3.The proposed method relies much on the quality of LLMs, and the transferability of the model is not reflected in the paper."
            },
            "questions": {
                "value": "NA"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8190/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8190/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8190/Reviewer_Z7Er"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8190/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698632889725,
        "cdate": 1698632889725,
        "tmdate": 1699637015941,
        "mdate": 1699637015941,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VK7PeSRalx",
        "forum": "hgDDyoWQt3",
        "replyto": "hgDDyoWQt3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8190/Reviewer_niw2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8190/Reviewer_niw2"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel feasibility calibration scheme for open-world compositional zero-shot learning. The key ideas leverage large language models as the intermediate agency for feasibility decisions. The authors conducted extensive experiments to show that the proposed scheme can improve compositional zero-shot recognition. The authors also include ablation experiments to address effects of underlying LLMs and prompts."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "To the best of the reviewer\u2019s knowledge, this method proposed in this paper is novel. This paper is clearly motivated and the intuition behind the proposed methods are also very clear. The idea of using LLMs for solving feasibility conflicts is simple yet quite effective. The authors also show that as an orthogonal component to existing compositional zero shot learning methods, LLM-guided feasibility calibration can clearly boost the performance for most of the scenarios."
            },
            "weaknesses": {
                "value": "Despite the work\u2019s obvious merit, the idea itself is very simple. Within the ablations, it would be helpful if the authors are to thoroughly examine more variants of prompts since LLMs output can vary a lot. The performance variations under such scenarios would be very informative to the community."
            },
            "questions": {
                "value": "Despite the method being effective as the authors have shown, the idea of using a single LLM for decisions may suffer from the common issues of hallucinations. I wonder if the authors have tapped into potential solutions to get around this issue."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8190/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8190/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8190/Reviewer_niw2"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8190/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698769998240,
        "cdate": 1698769998240,
        "tmdate": 1699637015828,
        "mdate": 1699637015828,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Mb6pFxVPVl",
        "forum": "hgDDyoWQt3",
        "replyto": "hgDDyoWQt3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8190/Reviewer_655z"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8190/Reviewer_655z"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to determine the feasibility of state-object (s, o) combinations (i.e., while the phrase\u201chot fire\u201d is feasible, the phrase\u201cwet fire\u201d is not feasible). This paper proposed to use large language models (LLM), such as Vicuna and ChatGPT to classify whether the phrase is feasible or not. This is implemented by entering ``Does a/an [THE PHRASE] exist in the real world?\u201d as the input to the LLM and the probability of LLM answering \u201cyes\u201d is the feasibility score. The author also tries to show several feasible phrases to the LLM to help the LLM decide whether [THE PHRASE] is feasible or not."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. Figure 1 is well designed and helps the reader to understand the content.\n2. The proposed method is simple and easy to understand."
            },
            "weaknesses": {
                "value": "1. The main concern of this work is its contribution. The paper basically uses the existing LLM to determine the feasibility of a state-object combination. This only shows that the existing LLM is able to determine the feasibility of a state-object combination, but what is the author\u2019s contribution throughout the process? \n2. Since different threshold will affect the binary classification performance, wouldn\u2019t a metric like ROC curve suits the tasks better?\n3. For Figure 2, it seems that both green and red block only show the feasible (s,o) pairs. The author is suggested to show some infeasible (s,o) pairs and the model prediction on those infeasible (s,o) pairs.\n4. It is challenging to tell whether GloVe or the proposed FLM separates the feasible and infeasible better by only looking at the figures. The author is suggested to show some numerical results to support the claim.\n5. In the evaluation metric, the author mentioned that the calibration bias is varied. Does it mean that different calibration bias is used for different metric?\n6. Typo: such \u201cat\u201d ChatGPT"
            },
            "questions": {
                "value": "1. For the rebuttal, the author is suggested to highlight the contribution of this work. After reading the submission, this paper is more like showing the observation that existing LLM already did a great job on identifying infeasible (s,o) pairs. There is no modification of the LLM, and no loss function is proposed. \n2. Some clarification problem in the weakness section needs to be addressed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8190/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8190/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8190/Reviewer_655z"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8190/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698817422754,
        "cdate": 1698817422754,
        "tmdate": 1699637015710,
        "mdate": 1699637015710,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aQBQ35PlOu",
        "forum": "hgDDyoWQt3",
        "replyto": "hgDDyoWQt3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8190/Reviewer_iHMU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8190/Reviewer_iHMU"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose a novel approach that leverages large language models (LLMs) to predict the feasibility of the state-object pair for the open-world compositional zero-shot learning (OW-CZSL). A prompt is designed to query the feasibility score by leveraging the autoregressive nature of LLMs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "S1: The studied problem about open-world compositional zero-shot learning is significant important and can apply to the real-world scene.\n\nS2: The large-language models are used to reduce the gap between machines and humans.\n\nS3: Extensive experiments on many prompt variants and six LLMs shows the best performence."
            },
            "weaknesses": {
                "value": "W1: Is this the first paper to solve the CZSL problem by using  the LLMs? If yes, I am curious about the motivation or some motivation experiments to demonstrate the effectiveness of LLMs? If no, I tend to see some differents compared with other published related works.\n\nW2: This method in this paper is not novel and performance improvement depends entirely on the language model. If the language model introduces biases, such as racial discrimination, during training, will this also affect downstream tasks?\n\nW3: Does a more powerful language model perform best in this paper?\n\nIn my opinion, simply introducing a language model to solve downstream tasks does not reach the upper limit of ICLR acceptance."
            },
            "questions": {
                "value": "See Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8190/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698904133371,
        "cdate": 1698904133371,
        "tmdate": 1699637015595,
        "mdate": 1699637015595,
        "license": "CC BY 4.0",
        "version": 2
    }
]