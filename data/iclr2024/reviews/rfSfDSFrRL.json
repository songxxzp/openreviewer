[
    {
        "id": "kskuEVbvmM",
        "forum": "rfSfDSFrRL",
        "replyto": "rfSfDSFrRL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5288/Reviewer_1u2u"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5288/Reviewer_1u2u"
        ],
        "content": {
            "summary": {
                "value": "This work analyzes recent developments in linear gated RNN/SSMs in the context of linear attention. The work shows how to construct a set of parameters in gated RNNs that can exactly implement linear self-attention. The paper also shows how LSTMs can be constructed in this way as well, but GRUs cannot. Synthetic experiments are performed that show the gated RNNs can learn the attention construction in a student-teacher setup. Experiments are then performed that show gated RNNs can find the linear attention solution when trained on an in-context learning linear regression task."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Overall the paper provides an interesting analysis of the connection between gated linear RNNs and linear self-attention.\n\n- The paper makes a nice connection that shows how linear self-attention can be exactly implemented within the weights of a gated RNN (if a quadratic increase in parameters is used). The investigation into LSTMs and GRUs is also interesting.\n\n- The experiments flow nicely from showing it is possible for the gated RNNs to learn the linear attention solution in a teacher-student setup, to then showing that when trained from scratch they can also learn the solution in the linear regression task. The additional experiments related to overparameterization and nonlinearities and identification are also interesting."
            },
            "weaknesses": {
                "value": "- Figure 1 is helpful, but the paper would benefit from also formalizing the construction in equations, either in the main paper in Section 3.1 or in the Appendix. I found myself having to stare at Figure 1 and the description in Section 3.1 longer than probably necessary, whereas I think a bit of math (in particular with dimensions clearly defined) along with the figure and description would make this much easier to see.\n\n- The experiments are demonstrative, but very toy, and have a lack of diversity. This is mostly ok for this type of paper, but it is unclear how well the results generalize. Perhaps analyzing and experimenting with additional tasks could be helpful. An additional toy task that might have been interesting is the associative recall/inductive head tasks from https://arxiv.org/pdf/2302.10866.pdf, https://arxiv.org/pdf/2212.14052.pdf, https://arxiv.org/abs/2209.11895. In particular, the H3 work also proposes a construction of how softmax attention can solve these tasks. Given that these tasks are of great interest to those studying language modeling with linear RNNs/SSMs, connecting with this prior work might broaden the audience of this work.\n\n- More discussion and analysis around some of the results would strengthen the paper. \n   - In particular, the compression result from Figure 3.B where the gated RNNs can solve the linear regression task with a size smaller than the theoretical construction size. Are there other tasks where this is not the case? E.g. perhaps the associative recall task from the point above? More analysis and experimentation around this point would strengthen the paper\n  - While potentially more difficult, I would have also appreciated more discussion, analysis, experiments around the GRU results presented in Figure 4.B, since it does so well despite not reflecting the linear attention solution. Again, perhaps an additional experiment might be insightful."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5288/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5288/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5288/Reviewer_1u2u"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5288/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698623430043,
        "cdate": 1698623430043,
        "tmdate": 1700534710480,
        "mdate": 1700534710480,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AdCt06VghU",
        "forum": "rfSfDSFrRL",
        "replyto": "rfSfDSFrRL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5288/Reviewer_pjc5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5288/Reviewer_pjc5"
        ],
        "content": {
            "summary": {
                "value": "This paper provides a construction proof to demonstrate that gated linear recurrent units can learn linear autoregressive self-attention exactly.  The first experimental results (section 4) show that the theoretical result holds in practice: a GLRU network trained as the student to a linear self-attention network learns to imitate its teacher with vanishingly small error.  The second experimental result is more interesting: it shows that, when LARSA and GLRU are taught using exactly the same in-context linear regression data, they take exactly the same gradient updates."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The title of the original Transformer paper (Attention is All You Need) suggested that the Transformer is nothing more or less than a more pathlength-efficient implementation of the same set of functions that an RNN can learn.  The exact nature of the near-equivalence between Transformers and RNNs has been harder to describe than that simple first title suggested.  This paper's experimental results on the gradient update for the in-context linear regression problem are a demonstration of the closest link between Transformers and GLRUs that I have seen yet."
            },
            "weaknesses": {
                "value": "My enthusiasm is tempered by the rather extreme limitations placed on both the Transformers and the GLRUs in this paper.  Linear self-attention is far less powerful than softmax self-attention, and as demonstrated in this paper, linear gated recurrence is less powerful than nonlinear gated recurrence, so a proof of equivalence between them, while of some theoretical interest, doesn't seem to be of very high impact."
            },
            "questions": {
                "value": "Is there any reason to believe that the demonstrated equivalence would continue to hold for neural nets that include nonlinearities?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5288/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5288/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5288/Reviewer_pjc5"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5288/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698760663927,
        "cdate": 1698760663927,
        "tmdate": 1699636529217,
        "mdate": 1699636529217,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DNUcz0NeHA",
        "forum": "rfSfDSFrRL",
        "replyto": "rfSfDSFrRL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5288/Reviewer_imQt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5288/Reviewer_imQt"
        ],
        "content": {
            "summary": {
                "value": "The paper takes a theoretical and empirical study to relate RNNs and attention models. The authors first show a theoretical construction that simulates a single linear attention head using a gated RNN. The idea behind the construction is simple, gated RNNs accumulate key-value matrix products at each time step and use an output gated unit to compute the output using the accumulated products and the queries at each step. However, such a construction requires $O(d^4)$ parameters to simulate a $3d^2$ parameter linear attention.\n\nInterestingly, in multiple numerical experiments to mimic linear attention, the authors still observe that such over-parametrization in gated RNNs is necessary to simulate linear attention. The authors conduct multiple structural probing experiments on trained gated RNNs to find the simulation of their construction. Furthermore, they show that existing RNN-based architectures fail to properly mimic linear attention. The authors end with interesting in-context experiments on linear regression and showcase differences in mechanisms of different RNN-based architectures."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The main strength of the paper lies in its clinical approach to connecting RNNs and attention models, which is an important question to understand for architecture design. It is an interesting approach to have a theoretical construction to understand the importance of gates in RNN models. Furthermore, the role of over-parametrization for such models has been pointed out by their theoretical construction and empirical experiments. \n\nIn addition, the in-context experiments on linear regression provide two significant observations for any future work to follow, (a) gated RNNs can simulate one step GD with even fewer neurons, and (b) other sequence-to-sequence models can perform the same task but without necessarily mimicking the behavior of one-layer attention. Thus, I believe this paper opens up interesting questions for mechanistic interpretability."
            },
            "weaknesses": {
                "value": "The main weakness of this paper lies in its slightly difficult presentation of experimental details. Here, I point out some of the difficulties that I faced when reading this paper. I additionally pose a few questions that I believe might strengthen the authors' claims.\n\n(a) There are many experimental statements whose details aren't clear from the current version.\n\n1. \"First, we observe that only perfect memory neurons ($\\lambda = 1$) and perfect forget neurons ($\\lambda = 0$) influence the network output.\" \n \nIn Figure 2, \" Only recurrent neurons with perfect memory (\u03bb = 1, dark blue) or no memory at all (\u03bb = 0, light grey) influence the output, consistently with the theory.\" \n\nHow do the authors verify this? Is this related to the pruning experiment that the authors conduct later, where they remove the neurons with any other $\\lambda$ values?\n\n2. In Figure 2, \"The block structure almost perfectly matches the one of our construction\". I don't understand the block structure that the authors refer to.\n\n3. Again in Figure 2, the statement \"For each output coordinate  ... which can be generated in a way that is coherent with the structure of our construction\" is extremely difficult to parse. \n\n4. In Table 2, what do the terms $x_i^j y _1$ for different $i, j$ even mean? Notations would help readers parse the results of the probing experiments.\n\n(b)  The experiments conducted in sections 2 and 5 are with a fixed dimension. How does the loss behavior change with different parameter counts at different dimensions? Such a plot can give an empirical dependence on the order of parameters necessary with dimension.\n\n\n(c) The linear regression experiments show that with sparsity in the key-value matrix, the gated RNN models can simulate more efficiently than the theoretical construction. It would be interesting to conduct similar experiments in section 2, where the authors impose low-rank/sparse constraints on the key-value matrix product and observe the empirical behavior of loss with different parameter counts.\n\n\nOverall, I believe this paper will be an interesting read to the community. The current paper presentation is difficult to parse at different experimental details. Hence, I would like to interact with the authors during the rebuttal period with the questions that I posed above."
            },
            "questions": {
                "value": "Please see my questions in the previous section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5288/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698853856416,
        "cdate": 1698853856416,
        "tmdate": 1699636529127,
        "mdate": 1699636529127,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ALb88CCyPh",
        "forum": "rfSfDSFrRL",
        "replyto": "rfSfDSFrRL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5288/Reviewer_2JoC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5288/Reviewer_2JoC"
        ],
        "content": {
            "summary": {
                "value": "The authors present a construction of a gated RNN that implements self-attention (linear) and provides a conceivable path towards RNNs that can learn self-attention. The construction relies on GLUs with a simplified rule for describing input and output gating. The authors conduct several experiments demonstrating activated neurons in the RNN correspond to scores in that would be expected in the construction. They also demonstrate parity with a linear self-attention mechanism. The authors then study features of these networks, in particular with linear regression and gradient descent, observing the impact of nonlinearity and sparsity. This work provides a theoretical foundation with which to study other approximations of self-attention."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The construction is novel and draws a clear connection with the special case of linear self-attention.\n- The explanation and construction of gated recurrent networks is clear, and the correspondence with self-attention is transparent and intuitively explained, i.e. in Figure 1.\n- The idea can guide development of attention implementations with other architectures which may have implications for efficiency. Given a general foundation, future work can use similar styles of constructions to proceed."
            },
            "weaknesses": {
                "value": "- Overall, the thrust of the contribution of the paper needs to be much more clearly articulated.\n  - Why is this particular construction good?\n  - What is the methodology that is general enough here to use for future constructions?\n  - How, explicitly, does the authors' approach pave the way for future contributions?\n  - Why do the learned ideas (e.g. linear regression) strengthen the thrust of the paper.\n\nIf these ideas can be articulated more clearly in a response here and in the manuscript, I would likely change my score.\n\nRegarding presentation:\n- Worth noting that citations in the PDF version of the paper don't appear linked to citations (for me)\n- Worth mentioning that GLUs in their initial construction from Dauphin et al were actually used in gated convolutional models, which resembled RNNs in their hierarchy, but were different\n- While the regularization task presented in the manuscript is valuable, not having a sequence learning task holds back some of the strength of the empirical results."
            },
            "questions": {
                "value": "- Section 3.2 discusses the invertibility of the value matrix per the number of hidden neurons the RNN needs to store KVs. Under which conditions is this matrix invertible?\n- In Section 4.1, how do the number of activated neurons in the construction correspond to activated attention weights? Is this correspondence clear?\n- In Section 4.2, the authors describe overparameterization insofar as twice as many neurons are needed to replicate the behavior of self-attention with the RNN construction. What might the effect of regularization be here, implicit or otherwise?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5288/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5288/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5288/Reviewer_2JoC"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5288/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699516272555,
        "cdate": 1699516272555,
        "tmdate": 1699636529015,
        "mdate": 1699636529015,
        "license": "CC BY 4.0",
        "version": 2
    }
]