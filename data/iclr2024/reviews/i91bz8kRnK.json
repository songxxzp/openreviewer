[
    {
        "id": "OhRuWbEDX0",
        "forum": "i91bz8kRnK",
        "replyto": "i91bz8kRnK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7047/Reviewer_eZk5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7047/Reviewer_eZk5"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to build an open-vocabulary keypoint detector, using text prompts to localize arbitrary keypoints of any species. It is achieved by associating text prompt with relevant keypoint features, by mapping the textual features and detected keypoint heatmaps into a shared semantic space.\nIn order to adapt to species of different number of keypoints, all keypoint categories are clustered by kmeans into groups according to their textual embeddings.\nThe experiments on MP100 shows SoTA performance using a zero-shot fashion, even compared with few-shot keypoint detection."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "With the proposed clustered keypoint categories (keypoint domain), the model can generalize to different species and various keypoint categories."
            },
            "weaknesses": {
                "value": "1. The keypoints that lack semantic information are not suitable for language-driven keypoint detector, such as keypoints on clothes and furniture. On the other hand, those keypoints are becoming increasingly important these days, espeicially for clothes.\n2. Related to 1), the detector seems to only work for very sparse keypoints, lacking the ability to upscale the number of keypoints. For instance, the classical 68 facial landmarks."
            },
            "questions": {
                "value": "1. Since \"animal species\" are needed in the text prompt, how does this model generalize to very rare species (rare in CLIP)?\n2. How is the performance compared with the semi-supervised methods, which also only needs 1 or 5 shots [A][B]? As they can deal with denser keypoints, I would recommend to at least add a discussion in the paper.\n\n[A] Few-shot Geometry-Aware Keypoint Localization (CVPR2023)\n\n[B] 3FabRec: Fast Few-shot Face alignment by Reconstruction (CVPR2020)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7047/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697600815841,
        "cdate": 1697600815841,
        "tmdate": 1699636828620,
        "mdate": 1699636828620,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AN7Kqhv5YN",
        "forum": "i91bz8kRnK",
        "replyto": "i91bz8kRnK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7047/Reviewer_As86"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7047/Reviewer_As86"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a novel open-vocabulary keypoint detection framework, especially for recognizing animal keypoints. The problem and the motivation are clearly addressed and described, and experiments look promising. I agree that the open-vocabulary setting for the animal / human keypoint detection is promising and can contribute insights to the community."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper formulates novel frameworks for the OVKD task, including a baseline framework and an augmented one. Both frameworks show promising performance on different OVKT settings. In particular, the augmented version outperforms the baseline framework by a great margin. The studied problem is also an interesting topic in the community, and the motivation is well-written."
            },
            "weaknesses": {
                "value": "Although the introduction and motivation part are well-written, I find that the quality of the method description and experiments is not as good as I expected. In general, I have many confusions after reading the method descriptions and also have questions about the experiments. Please read the following section for specific questions. Overall, I think the unclear method makes the quality of this paper quite questionable."
            },
            "questions": {
                "value": "1. In Figure 2, I found that there is a 'Zero-shot' term on the top right. However, I cannot understand which part is zero-shot. In the paper, I found that the baseline method is still trained to deliver keypoint detection, except that the text_encoder is fixed. If the authors mean that the zero-shot can be carried on novel categories, it is quite misleading to put \"zero-shot\" on the overall baseline framework. \n2. For the augmented framework, there are even more confusions. For example, the authors state that K-means is used to cluster the keypoint categories. However, I cannot understand: What are the actual data or features to be clustered? Why they should be clustered? Why K-means? Why not use other clustering algorithms? \n3. In addition to question 2, I found a term called \"binary domain distribution matrix\". Still, I am confused about: What is binary domain distribution matrix? What's its formal definition? What does it represent? Why do we need binarized matrix? \n4. Furthermore, why do we need to reorder channels for H'? What problem does the reordering solve? \n5. In the experiment, I have the following question: The new MP-78 split is not clear: which species are used for training and which new species are used for testing? Will there possible be data leaking? \n6. In Table 3, I found that VKLA improves a lot, while, according to my understanding, the so-called VKLA is just some extra attention modules that for sure can certainly improve performance. What if we stack more VKLS modules, does it further improve performance? If so, the contribution of this paper is quite questionable. Correct me if my understanding is wrong. \n7. I have a question about the experiment design. Talking about OVKD, I suppose the proposed method should be working in different settings, such as pre-train keypoint knowledge only on human keypoints, and test on animal keypoints. In theory, I think the proposed should still be working. It would better to discuss this.\n8. There are some unclear presentations (or typos?). In Figure 4, bottom 2 raws, what is 'keypint category'? Shouldn't that be some specific keypoint names?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7047/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698368022179,
        "cdate": 1698368022179,
        "tmdate": 1699636828508,
        "mdate": 1699636828508,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dqO3CUL95s",
        "forum": "i91bz8kRnK",
        "replyto": "i91bz8kRnK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7047/Reviewer_r55N"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7047/Reviewer_r55N"
        ],
        "content": {
            "summary": {
                "value": "The paper defines a new Open-Vocabulary Keypoint Detection (OVKD) task to locate arbitrary keypoints of any species. The authors propose a KDSM method to utilize visual and textual information for this task. In particular, KDSM contains a Vision-keypoint relational awareness module for visual and language information interaction, and a domain distribution matching module for semantic alignment and generalization. Experiments show the effectiveness of the method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The defined OVKD task is important for the development of the keypoint detection task. It will be beneficial for more real-world applications and research.\n2. The proposed domain distribution matrix matching module can process arbitrary keypoints and species input flexibly.\n3. Experiments show that the proposed method outperforms the baseline by a large margin."
            },
            "weaknesses": {
                "value": "1. Unclear experimental settings: \nIn the definition of keypoint categories, will keypoints that are structurally consistent be categorized as a single keypoint category? For example, the front paw of a panda, a human's hand, and the forehooves of a horse. Will there be keypoints that are structurally consistent in both the training and test sets (such as the above examples)? If the keypoint categories in the training and test sets are structurally exclusive, (for instance, the train set lacks facial keypoints while the model is tested on facial keypoints; the train set lacks forelimbs keypoints while the model is tested on forelimb keypoints), how will the model perform?\n\n2. The motivation should be further clarified:\nIn Section 3.3, it is suggested to provide more explanations to clarify why \u201cThe baseline framework struggle with generalization...performance\u201d and why the proposed modules can address the problem.\n\n3. Visualizations of the domain distribution matrix will help to understand the proposed method better. In addition, please provide some matched examples. For instance, given a prompt in the test set, please provide the training prompts falling in the same bin (in O groups). This can give us a more comprehensive understanding of your approach."
            },
            "questions": {
                "value": "See the weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7047/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698628711526,
        "cdate": 1698628711526,
        "tmdate": 1699636828397,
        "mdate": 1699636828397,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VtGJgtow12",
        "forum": "i91bz8kRnK",
        "replyto": "i91bz8kRnK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7047/Reviewer_yVmq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7047/Reviewer_yVmq"
        ],
        "content": {
            "summary": {
                "value": "The work introduces the task of Language-Driven Open-Vocabulary Keypoint Detection (OVKD) for animal body and face keypoint localization. The existing methods for keypoint detection are limited to specific keypoints and species, requiring manual annotation or support images. OVKD aims to overcome these limitations by using text prompts to detect arbitrary keypoints of any species. The proposed framework, called Open-Vocabulary Keypoint Detection with Semantic-feature Matching (KDSM), leverages the relationship between text and vision using language models and domain distribution matrix matching. Extensive experiments show that KDSM outperforms the baseline framework and achieves impressive results in OVKD."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "-\tThe authors introduce a new open-vocabulary task for animal keypoint detection. \n\n-\tThe proposed framework improves the generalization capability across different species and keypoint categories, making it applicable to previously unseen keypoints and species.\n\n-\tKDSM integrates domain distribution matrix matching and some special designs to reinforce the relationship between language and vision."
            },
            "weaknesses": {
                "value": "-\tThis work lacks a comparison of complexities. The author should compare their approach with previous few-shot methods about the network parameters and inference time. Since this work employs CLIP, there might be a potential for a higher number of parameters which might lead to an unfair comparison. \n\n-\tRegarding CLAMP, although the author mentions that CLAMP uses the same keypoint categories, the technical methods between the two works exhibit minimal differences. Furthermore, the authors do not compare their results with CLAMP on the dataset.\n\n-\tWhat will the result be if both language and few-shot images are applied for training/inference simultaneously? Relying solely on language can be imprecise for species that have not been observed previously, making it challenging to generalize. \n\n-\tThe authors only conduct experiments on the MP-78 dataset. Results on a single dataset can hardly be compelling."
            },
            "questions": {
                "value": "-\tThe comparison of complexity.\n-\tComparisons with CLAMP.\n-\tThe results of using both language and few-shot images.\n-\tMore experiments on other datasets."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7047/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698686833371,
        "cdate": 1698686833371,
        "tmdate": 1699636828292,
        "mdate": 1699636828292,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HEedqkDiRZ",
        "forum": "i91bz8kRnK",
        "replyto": "i91bz8kRnK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7047/Reviewer_R5x4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7047/Reviewer_R5x4"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes the open-vocabulary keypoint detection (OVKD) with the goal of detecting arbitrary keypoints of arbitray species from query image given text prompts. There is a baseline OVKD model and another advanced OVKD model proposed. Both models use CLIP as text encoder  while the ResNet or ViT as vision encoder. Moreover, the open-vocabulary Keypoint Detection with Semantic-feature Matching (KDSM) is proposed to leverage the benefit of language models, exploit the relationship between text and vision, and employ domain distribution matrix matching to enhance performance. The experiments are performed in a subset of MP100 and the results show the effectiveness of proposed OVKD model."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The task of OVKD is proposed to detect arbitrary keypoints.\n\n2. Keypoint Detection with Semantic-feature Matching (KDSM) is proposed, which improves performance.\n\n3. The experiments show the effectiveness of proposed method."
            },
            "weaknesses": {
                "value": "1. From the abstract, contributions, and conclusion, all appear \"some special designs\", what are these special designs?\n\n2. When we check the comparison methods, it seems there is no method called FS-ULUS in paper [1]. Is there something wrong or just modifying their methods? It would be better to use the model name indicated in paper, for example, FSKD.\n\n    [1] Few-shot keypoint detection with uncertainty learning for unseen species @ CVPR'22\n\n3. Since there already exists an open-vocabulary keypoint detection work like CLAMP [2], what is the performance when comparing your method to CLAMP on Animal pose dataset in the setting of five-leave-one-out problem? Namely training on four species while testing on the left one species. The results of PCK @ 0.1 should be presented in paper for comparisons.\n\n    [2] CLAMP+Prompt-based Contrastive Learning for Connecting Language and Animal Pose @ CVPR'23\n\n4. The dimension of text feature (K=100, C=64) is only 64, which makes me doubt whether the model leverages the language prior or not.\n\n5. KDSM essentially is matching and retrival. The global matching matrix will cause K-k invalid text prompts, which would waste computation.\n\n6. Why not use CLIP's vision encoder, while using the ResNet50 pre-trained on imagenet. If the text & vision model do not match, their multi-modal embeddings are not aligned anymore. How can it claim that \"leverage the language model's knowledge\"?\n\n7. PCK@0.2 may be too high to cause over-confident results.\n\n8. What is the meaning of \"query the names of datasets that do not have semantic naming\"? How do you use chatgpt to query? Just manually query?\n\n9. Moreover, I still doubt that zero-shot yields better performance than few-shot model, if images are not seen at training phase.\n\n10. The split of seen and unseen keypoint categories should disclose for fair future comparisons. However, in appendix, one still cannot find the concrete splits."
            },
            "questions": {
                "value": "."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7047/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7047/Reviewer_R5x4",
                    "ICLR.cc/2024/Conference/Submission7047/Senior_Area_Chairs"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7047/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698750485800,
        "cdate": 1698750485800,
        "tmdate": 1700314869914,
        "mdate": 1700314869914,
        "license": "CC BY 4.0",
        "version": 2
    }
]