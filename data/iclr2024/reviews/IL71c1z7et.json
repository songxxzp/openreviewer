[
    {
        "id": "3kHqRuFQ5Y",
        "forum": "IL71c1z7et",
        "replyto": "IL71c1z7et",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4480/Reviewer_DxkE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4480/Reviewer_DxkE"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a method to merge policies learnt by a fleet of robots trained for different skills. The proposed FLEET-MERGE algorithm which allows for distributed learning of recurrent neural networks through a merging strategy that accounts for permutation invariance of neurons in layers. The authors also introduce a new robot manipulation benchmark for fleet policy learning, which they use to evaluate their method."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1) The experimental results for policy merging are strong for merging RNN policies trained for different tasks. The proposed method outperforms naive averaging and other similar baselines. Distributed learning is becoming increasingly important for robotics as deployment of fleets of robots for data collection is more feasible.\n\n2) The introduced benchmark FLEET-TOOLS is a useful contribution to the robot manipulation community, for easy collection of expert trajectories useful for training policies with imitation learning.\n\n3) The algorithms and experiments are explained clearly, with good presentation."
            },
            "weaknesses": {
                "value": "1) The proposed algorithm was only demonstrated for behavior cloning, which is not what we generally consider as the setup where we benefit from distributed learning. If we have a large amount of static expert data, it is not too difficult to just merge the datasets and train a policy on the joint dataset. When we consider fleet policy learning, it is much more useful to consider a reinforcement learning setup where a collection of robots are collecting data for various tasks, and we wish to collectively use these data sources to learn optimal policies. But this setup is much more complicated due to non-stationarity and other challenges with online RL. It would be much more interesting to see if these merging strategies could work in an online RL setting.\n\n2) The novelty of the proposed method is not very clear. The algorithm used by the authors is based heavily on Pena et al. [1], as such it is hard to see what the main contribution is, apart from the FLEET-TOOLS benchmark. Is it the application of the algorithm for training RNNs, and the experimental demonstration of its usefulness in fleet robot learning?"
            },
            "questions": {
                "value": "1) Could the authors motivate distributed policy learning for behavior cloning? If static expert datasets are already present, it does not seem useful to train separate policies and merge them. Does this approach work in the online reinforcement learning setting?\n\n2) Could the authors summarize the main novel contributions of the FLEET-MERGE algorithm, especially compared to the work from Pena et al. [1]? I would be content if the contributions are mainly experimental validation for robot learning, but would appreciate the clarity since the exact contribution is difficult for me to parse.\n\nIt is possible I did not understand the contributions properly, so I would be willing to raise the score if the authors can provide a strong response to my questions. But as of now I am leaning slightly towards rejection since behavioral cloning does not seem to be an appropriate task to motivate fleet learning, and the novelty of the FLEET-MERGE algorithm is not clear.\n\n[1] Re-basin via implicit Sinkhorn differentiation, arxiv.org/abs/2212.12042"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4480/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4480/Reviewer_DxkE",
                    "ICLR.cc/2024/Conference/Submission4480/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4480/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698626903241,
        "cdate": 1698626903241,
        "tmdate": 1700543680162,
        "mdate": 1700543680162,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QYgCpkfHnm",
        "forum": "IL71c1z7et",
        "replyto": "IL71c1z7et",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4480/Reviewer_2Z2Z"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4480/Reviewer_2Z2Z"
        ],
        "content": {
            "summary": {
                "value": "This paper studies policy merge, which seeks to merge policies that are trained on different datasets and tasks into a single set of weights, while preserving the learned skills of the original policies. In particular, taking account into the sequential and dynamic nature of robot policy learning, this paper focuses on merging RNN-parameterized policies. The proposed method, Fleet-Merge, outperforms prior model merging methods by over 50%, accounting for not only the permutation symmetries in RNN policies but also enabling multiple rounds of merging between each training update as well as merging multiple models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper is very novel in both its problem setting. Policy merging, to the best of my knowledge, has not been explicitly studied before. This paper will have impact in the robot learning community, where large-scale datasets as well as data/model sharing across institutions are becoming increasingly prevalent. However, most such effort has primarily focused on consolidating all data to train a single large model (e.g., the recent RT-X effort), but this paper offers a fresh perspective that opens up the possibility of different institutions sharing their pre-trained policies, which are much more manageable.\n\nThe proposed method, at a technical level, is largely derived from a prior work (Pena et al., 2022). That said, the paper does introduce several additional improvements that are particular fitting to the policy learning setting. For one, Fleet-Merge is designed to be able to merge multiple models and do so in multiple-round fashion, both of which are important features to have in sequential decision making fashion; in contrast, prior works in the model merging literature is primarily considering the simplified case of merging two models. Because of these changes, Fleet-Merge does diverge from prior works with several modifications such as grounding the model merging in each iteration to the current average of all models, reminiscent of common practices in federated learning. Therefore, the approach, though extensions of prior works, is well motivated."
            },
            "weaknesses": {
                "value": "The main weaknesses of the paper is in its presentation and the strength of the experimental results.\n\nFirst, the presentation of the paper can be improved. In Figure 3 and 4, several charts are hard to see because the legends block them.  The experiment section can be better structured by having subsections for each of the evaluation setting instead of each benchmark; right now, all evaluation settings are presented upfront without concrete contextualization and results that interleave between them. The new benchmark Fleet-Tools is not well motivated and the section on \"Keypoint transformation & trajectory optimization\" within Section 4 is too dense while not directly related to the algorithmic components of the paper. I suggest the authors better motivate why Fleet-Tools is particularly well-suited for studying the policy merging problem. \n\nThe performance improvement of Fleet-Merge compared to simple baselines is moderate at best. Fleet-Merge does not seem to be substantially better than Git-Rebasin or FedAvg on the Metaworld and Fleet-Tool benchmark.  Considering that either baseline is missing key components in Fleet-Merge, it is worth asking the question whether Fleet-Merge is truly necessary given its added algorithmic complexity as well as computational cost. Furthermore, while the paper emphasizes the multi-task aspect of policy merging, most of the experiments are conducted in the single-task setting. Interestingly, in the multi-task setting, simple averaging appears to be very competitive. \n\nOverall, I think this paper is promising and I am willing to improve my score if the authors could resolve my reservations about the practical utility of the proposed method as well as more clearly present the paper."
            },
            "questions": {
                "value": "Several suggestions and questions are already included in the Weaknesses section. Here are some additional questions/suggestions:\n\n1. The definition of Loss Barrier should be included somewhere early. In Section 5, the concept of \"performance barrier\" is introduced assuming readers' familiarity with the equivalent concept of loss barrier in the supervised learning model merging literature."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4480/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698719659256,
        "cdate": 1698719659256,
        "tmdate": 1699636423702,
        "mdate": 1699636423702,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TDi87BRxEa",
        "forum": "IL71c1z7et",
        "replyto": "IL71c1z7et",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4480/Reviewer_FoVN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4480/Reviewer_FoVN"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a method, called Fleet-Merge, for merging neural network policies trained on different tasks into one neural network policy. When a large fleet of robots learn a diverse set of skills on diverse tasks and it is not possible to collect a centralized dataset of all environmental interactions, how can we learn singular policies that capture all the training done by individual robots? A common tool to merge policies in federated learning is to simply average the weights/biases of all the neural networks into one policy. However, this leads to a drop in performance because the internal representation space of these policies may be significantly different. This paper introduces a method for merging the policies using the permutation invariance property of RNNs. By applying a permutation matrix to each layer of weights and biases, the paper aligns the internal representation space of the RNNs and is able to perform better merging. The authors test on a series of domains including linear control tasks and realistic robotic control tasks. They also introduce a tool use benchmark called Fleet-Tool, a task suite for robotic tool usage (like hammering, screwing, cutting,...)"
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "This is a very interesting paper and is well written. The problem that they investigate is well supported and they propose (to my knowledge) a novel algorithm that allows us to merge different neural networks by aligning their internal representation spaces. They do extensive experiments in simulation and the real-world and show that policies from diverse tasks can be merged together in a smart way.\n\nThey leverage known properties of RNNs (i.e.  internal permutation invariance and time-invariance) and find transformation matrices to match the parameters of different RNNs together. This is an interesting idea and very applicable to the problem that they are solving."
            },
            "weaknesses": {
                "value": "While the method proposed is clear to me, I do not understand how the merged policy can determine what task to solve during test time. \n\nFurther, in the results section I did not see how well the policies perform on a single task. In figure 4, it is clear that FleetMerge performs better than Federated Averaging, however I am curious to see how much of a decrease in performance is incurred by merging the policies at all."
            },
            "questions": {
                "value": "1. At test time, how does the merged policy know what task to solve? Could there be an ambiguity on what the task specification is?\n2. What would the performance be of a policy trained on all the data collected during the fleet learning? I am curious to know what the upper bound on multi-task training is.\n3. When choosing the first permutation matrix, does it matter which RNN  policy you align the rest of the policies? I am unclear on how this initialized permutation matrix is chosen/calcualted"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4480/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4480/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4480/Reviewer_FoVN"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4480/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698858848920,
        "cdate": 1698858848920,
        "tmdate": 1699636423634,
        "mdate": 1699636423634,
        "license": "CC BY 4.0",
        "version": 2
    }
]