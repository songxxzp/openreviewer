[
    {
        "id": "4BHKZuTDls",
        "forum": "Buvbx3xRdu",
        "replyto": "Buvbx3xRdu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1494/Reviewer_B82Y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1494/Reviewer_B82Y"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a face clustering algorithm for videos that involves  a self supervised learning process to adapt a generic face ID model to the target video and also a clustering algorithm based on the adapted model. In addition, a new benchmark is proposed for the task of video face clustering. However, while the method of joint face representation adpation and clustering for video has been used in several prior works, such as in the related paper(Sharma et al. (2019), Zhang et al.(2016b)) mentioned by the authors, its novelty is not clearly established. On the other hand, the motivation for proposing the benchmark is not sufficiently explained."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Quite detailed experiments and good performance compared to current methods.\n2. Automatic face clustering is important and practical for many video editing application but lacks sufficient attention in the research community. One of the reasons is the lack of good benchmarks. This work makes a contribution towards addressing this."
            },
            "weaknesses": {
                "value": "1. Compared to related works that use joint face ID model adaptation and clustering, the uniqueness of this work is not stated clearly. Is the performance increase comes from better face ID model adpatation or just the following clustering method?  If better face ID model, is it because of the teacher-student branch method used in this work or other perspectives?\n\n2. Why we need a new benchmark? The existing ones not challenging enough for practical use? Why? Any quantitative/qualitative comparison among the porposed one and existing ones? The statement should be put clearly in the paper."
            },
            "questions": {
                "value": "Please refer to the section of weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1494/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698764150373,
        "cdate": 1698764150373,
        "tmdate": 1699636078342,
        "mdate": 1699636078342,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zZtyv5ZnXl",
        "forum": "Buvbx3xRdu",
        "replyto": "Buvbx3xRdu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1494/Reviewer_wXAX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1494/Reviewer_wXAX"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a self-supervised algorithm for clustering face tracks in a video using a generic face ID model. A coarse track matching method is used to extract positive tracks for fine-tuning the face ID model. The fine-tuned model's embedding space is used to evaluate the similarity between face tracks, and an adaptive thresholding mechanism is used for the final clustering step. To evaluate the proposed model, a movie dataset is curated, and the results demonstrate its state-of-the-art performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The annotation effort for the movie dataset is substantial.\n- The method is novel in its elimination of the need for manual selection of a clustering threshold.\n- The pre-processing step includes cutting-edge building blocks such as RetinaFace for face detection and SER-FIQ for face quality assessment.\n- The noticeable efforts to replicate baselines in PyTorch are commendable.\n- Limitations and future works are discussed."
            },
            "weaknesses": {
                "value": "Missing important details regarding fine-tuning: it is unclear how the data is divided into train/validation sets during the model fine-tuning stage. The self-distillation fine-tuning step proposed requires multiple tracks from different temporal steps to ensure adequate appearance variations. However, the number of tracks required in a video and how it impacts the model is not discussed.\n\nThe authors claim that the positive track pair construction step is independent of any ground truth labels or temporal motion track constraints. This statement is confusing as it contradicts the requirement of ground truth labels.\n\nUnfair comparison with baselines due to advanced pre-processing: the pre-processing techniques used in this paper rely on advanced algorithms such as PySceneDetect (2022), RetinaFace (2020) and SER-FIQ (2020). In contrast, most of the baselines in Table 1 were established before 2020. There is no investigation into how these advanced pre-processing steps affect the clustering performance of the proposed method or the baselines. The observed performance improvement could be attributed to more accurate scene detection or face detection rather than the proposed method itself.\n\nData, training code and baseline re-implementation code are not promised to be open-sourced for reproducibility."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The release of the proposed MovieFaceCluster dataset might breach the copyright of these movies."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1494/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1494/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1494/Reviewer_wXAX"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1494/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698772503648,
        "cdate": 1698772503648,
        "tmdate": 1699636078228,
        "mdate": 1699636078228,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HnTZ7BpJMj",
        "forum": "Buvbx3xRdu",
        "replyto": "Buvbx3xRdu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1494/Reviewer_RNxJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1494/Reviewer_RNxJ"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a self-supervised video face clustering approach that adapts to challenging variations in facial pose, lighting, and expressions by fine-tuning a generic face ID model to learn robust facial embeddings progressively. Furthermore, it introduces a parameter-free clustering algorithm that automatically clusters facial features based on the fine-tuned model embeddings without the need for user-defined thresholds or initial cluster numbers. In addition, a movie face clustering benchmark dataset MovieFaceCluster is provided to better evaluate the performance of video face clustering algorithms in real-world scenarios."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well-motivated and easy to follow.\n- The proposed method is reasonable and feasible.\n- Compared to the prior work and baseline, the proposed method achieves competitive results."
            },
            "weaknesses": {
                "value": "- The paper does not provide enough information about the proposed dataset, particularly in terms of presenting its uniqueness and advantages. There is no visualizations or statistical analyses to demonstrate distinctions from existing datasets. Furthermore, the description of dataset annotations is unclear. It remains uncertain whether the term \"Varying Parameter\" mentioned in Figure 1 is associated with dataset annotations.\n- The authors did not compare their dataset with existing movie person identification (PI) datasets, such as MovieNet [1], which includes annotations suitable for PI tasks.\n- While the paper claims not to require pre-defined parameters, fixed values are still set in the quality assessment and coarse track matching modules to generate adaptive thresholds. However, these fixed values lack empirical or theoretical support.\n- Regarding the t-SNE embedding visual comparison on the MovieFaceCluster dataset in Figure 5, the visualizations generated by the proposed method seem to closely approximate the ground truth, which exhibits some anomalous clusters. Authors may explore the limitations of proposed method and present failure cases so as to provide more in-depth insights into video face clustering.\n- Most of the methods used for comparison are outdated, with only one introduced within the last three years.\n- Please ensure the consistency of citation formats. For instance, there is an inconsistency in the citations of Table 1 in Section 4.1, where both 'Table 1' and 'Tab. 1' are used.\n\nRef:\n[1] Huang, Qingqiu, et al. \"MovieNet: A Holistic Dataset for Movie Understanding.\" European Conference on Computer Vision. 2020."
            },
            "questions": {
                "value": "See the above weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1494/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698834234768,
        "cdate": 1698834234768,
        "tmdate": 1699636078110,
        "mdate": 1699636078110,
        "license": "CC BY 4.0",
        "version": 2
    }
]