[
    {
        "id": "HAFLpvjlqq",
        "forum": "9Wy6pLNQcG",
        "replyto": "9Wy6pLNQcG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2923/Reviewer_3vj6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2923/Reviewer_3vj6"
        ],
        "content": {
            "summary": {
                "value": "The paper aims to combine SAM and CLIP to improve region-level visual understanding. Specifically, the proposed RegionSpot method freezes the whole SAM and CLIP models, and adds new layers to them to let the position-aware tokens from SAM interact with the image-level features from CLIP, thus leading to region-level semantic tokens. After training the new layers on Object365, OpenImages, and V3D, the method shows decent performance on LVIS open-world object recognition."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The motivation of combining the capabilities of SAM and CLIP is clear and makes sense.\n- The method is simple and easy to implement, as most of the parameters are frozen and only several new layers are trained."
            },
            "weaknesses": {
                "value": "- The claim of 6.5% and 14.8% improvements over GLIP makes no sense. The comparison is not fair. The paper only compares with GLIP-Tiny, which is way smaller than the proposed RegionSpot. GLIP-Tiny uses Swin-Tiny which has 29M parameters, while RegionSpot-BB has at least 160M parameters. According to the GLIP paper, GLIP-L achieves 26.9 AP on LVIS, which is better than the best RegionSpot-BL's 23.7 AP.\n- Evaluation is limited. The paper only tests on open-world LVIS recognition. It would be more convincing to do a more comprehensive evaluation.\n- It is not appropriate to claim a zero-shot recognition on LVIS, as RegionSpot is trained on the combination of Object365, OpenImages, and V3D, which shares a lot of common categories, objects, and scenes with LVIS.\n- Please check the reference carefully. For example, \" However, the use of ROIAlign (Ren et al., 2015) for region feature extraction...\". The ROIAlign is not proposed in Ren et al., 2015."
            },
            "questions": {
                "value": "- The paper only experiments with different CLIP models, i.e., CLIP-base and large. What about using larger SAM models, e.g., RegionSpot-LL? \n- More details can be reported, e.g., the number of new parameters."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2923/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698634964990,
        "cdate": 1698634964990,
        "tmdate": 1699636236111,
        "mdate": 1699636236111,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AzarjLvsFa",
        "forum": "9Wy6pLNQcG",
        "replyto": "9Wy6pLNQcG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2923/Reviewer_Tjjg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2923/Reviewer_Tjjg"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a open-world region recognition architecture, named RegionSpot, designed to integrate position-aware localization knowledge from SAM and CLIP."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The method performs open world object detection through pre-trained vision foundation model SAM and CLIP. SAM is a good foundation model with excellent performance. This work sucessfully adopts SAM into object detection (requiring object recognition). This idea is novel and good. Introducing vision foundation models into new tasks is an important thing, I think.\n2. The experimental results are good."
            },
            "weaknesses": {
                "value": "Most of the problems come from the experiment part.\n1. This method still requires existinig region proposal generation models, like ground-truth, RPN, GLIP, which makes this method extremely limited. From this view, this method is even not complete, since you cannot find such a region understanding task in reality. In addition, the pre-extracted regions are also meaningless, since SAM can also perform the region proposal generation task. Therefore, the main experimental setting is meaningless and unreasonable.\n2. There are also many alternatives to perform the so-called region understanding task with SAM and CLIP. For example, SAM can directly extract region proposals and CLIP (or RegionCLIP) can predict the category tags of them. We can also add some projection layers in this way, freeze most parameters and finetuning a small part of parameters for efficient training. The author should perform experiments to compare with baselines like this. Otherwise, the effect of the position-aware localization cannot be seen.\n3. The experiment in Table 1 is also unfair. RegionCLIP simply performs image-text pre-training, without object detection finetuning. However, RegionSpot performs something about detection training. Therefore, the author should finetuning RegionCLIP on the same detection datasets for a fair comparison.\n4. The author should also compare with some more recent methods, like GLIP v2, Grounding DINO and so on."
            },
            "questions": {
                "value": "Most of the problems come from the experiment section. The author should provide more additional results to make the paper accepted,"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2923/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2923/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2923/Reviewer_Tjjg"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2923/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698752028698,
        "cdate": 1698752028698,
        "tmdate": 1699636236004,
        "mdate": 1699636236004,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Jxkw9EpiMp",
        "forum": "9Wy6pLNQcG",
        "replyto": "9Wy6pLNQcG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2923/Reviewer_UsjU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2923/Reviewer_UsjU"
        ],
        "content": {
            "summary": {
                "value": "Authors propose RegionSpot, an open-vocabulary detection and instance segmentation approach that leverages frozen foundation models. Specifically, given some set of candidate bounding box proposals, authors use SAM for class-agnostic localization and CLIP features for classification. Importantly, authors only train a small projection and attention module to combine the location queries from SAM with the semantic key/value pairs from CLIP. Authors evaluate their method on LVIS and find that their method beats prior work including RegionCLIP and GLIP."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Simple Approach. Authors propose a simple way of combining two popular off-the-shelf foundational models for vocabulary detection and segmentation that effectively leverages the foundational pre-training of each model.\n- Excellent Training and Data Efficiency. Due to the small size of the RegionSpot attention module, authors can train on 3M data points in 22 hours. (Table 2 is notable)\n- Clear Explanation. Authors present their work in a generally coherent manner."
            },
            "weaknesses": {
                "value": "- Limited Baseline Comparisons. Despite significant prior work in OVOD [1,2,3], authors primarily only compare with RegionCLIP and GLIP. In reality, prior work [1] significantly outperforms RegionSpot.\n- Unfair Comparisons. Since authors show that pre-training data scale significantly contributes to model performance, comparing RegionSpot, a technically very similar method trained on much less data is unfair. Instead, it would make more sense to evaluate RegionSpot trained on only CC3M. \n- Limited by Quality of Boxes. RegionSpot is always limited by the bounding box proposals provided as input to the system. As authors show in Table 1, the type of proposals has a significant impact on model performance. It would be interesting to evaluate how the impact of using proposals from one of the more performant open-vocabulary models than GLIP (e.g. GroundingDINO).\n\nReferences \n\n[1] Scaling Open-Vocabulary Object Detection. Minderer et. al. ArXiv. \n\n[2] Multi-Modal Classifiers for Open-Vocabulary Object Detection. Kaul et. al. ICML 2023\n\n[3] https://github.com/witnessai/Awesome-Open-Vocabulary-Object-Detection"
            },
            "questions": {
                "value": "- How to Deal with False Positive Boxes? Since all regions are classified into one of K categories, how are false positive proposals addressed?\n-  Can this method be most improved by better classification (e.g. CLIP) or localization (e.g. SAM)? What are the typical error modes?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2923/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2923/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2923/Reviewer_UsjU"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2923/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698840022164,
        "cdate": 1698840022164,
        "tmdate": 1699636235936,
        "mdate": 1699636235936,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6EcSF9EumM",
        "forum": "9Wy6pLNQcG",
        "replyto": "9Wy6pLNQcG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2923/Reviewer_PEMU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2923/Reviewer_PEMU"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on the challenging task of region recognition, particularly, open-world object detection. The authors present a new region recognition framework, combining a vision foundation model, i.e., SAM, and a vision-language foundation model, i.e., CLIP. In this framework, localization knowledge and semantic conception are integrated to promote each other. Experimental results and analyses conducted on the LVIS detection dataset demonstrate its effectiveness and generalization."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "**Originality**: The paper proposes a lightweight knowledge integration module to unleash the ability of both SAM and CLIP models for open-world object detection tasks.\n\n**Quality**: The paper provides a thorough experimental evaluation of **RegionSpot** on a challenging object detection dataset, i.e., LVIS. The authors also conduct various ablation studies to analyze the impact of different components of **RegionSpot**, such as the scale of training data, different representations from the CLIP model, selection of position-aware tokens, etc. \n\n**Clarity**: The paper also provides sufficient background information and related work to situate the contribution of **RegionSpot** in the context of existing literature on region understanding and zero-shot recognition."
            },
            "weaknesses": {
                "value": "**Major Issues**:\n\n**Insufficient novelty and contribution**: The newly proposed **RegionSpot** framework lacks justification for its design. The pipeline of fine-tuning a lightweight module while frozen SAM and CLIP models seems natural and basic. Additionally, only conducting experiments on object detection tasks is not convening.\n\n**Insufficient results for experiments**: Although the authors claim that \"This implementation effectively facilitates the fusion of semantic and location information in a manner that is amenable to learning and yields substantive efficacy.\", they provide no experimental results. Also, the motivation is not clear. For example, why do the authors serve the localization feature as the role of `query`? what if the ViL feature assumes as the `query`? \n\n**Minor Issues**:\n\n**Excessive statement**: The authors claim that \"Our model's flexible architecture allowed us to seamlessly replace one-hot labels with class name strings.\". This may be overemphasizing their contribution.\n\n**Grammar and minor errors**:\n- In section **Position-aware tokens selection in SAM**, a grammatical error is present in the sentence: \"Surprisingly, although it can outperform GLIP (i.e., 17.2 vs. 18.6).\"\n- In section **Prompt enginerring**, there is a discrepancy in the reported increase: \"an increase of 1.4 AP\" \u2014 should this be 1.6 AP? Additionally, clarification is needed regarding whether the method in the last line of Table 5 indeed represents the baseline with both prompts."
            },
            "questions": {
                "value": "1. My major concern is the contributions of combining SAM and CLIP for object detection tasks.\n\n2. The authors should discuss the limitations and potential negative societal impact in the Conclusion.\n\n3. Please also refer to Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2923/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699601351964,
        "cdate": 1699601351964,
        "tmdate": 1699636235874,
        "mdate": 1699636235874,
        "license": "CC BY 4.0",
        "version": 2
    }
]