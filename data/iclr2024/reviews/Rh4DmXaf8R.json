[
    {
        "id": "mAYuK5E8oG",
        "forum": "Rh4DmXaf8R",
        "replyto": "Rh4DmXaf8R",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8377/Reviewer_pXQz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8377/Reviewer_pXQz"
        ],
        "content": {
            "summary": {
                "value": "The paper analyzes multistep prediction models for model-based reinforcement learning. \nTraining models for multiple steps can yield better performance. However, as every timestep step prediction is required for planning,  different loss compositions for training single-step models with multistep losses are studied. \nThe paper reports that an exponential weighting of loss terms, where longer time horizons have a lower weight, performs better, both in short as well as long-term predictions. The empirical evaluation is based on the cart-pendulum system."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- topic is important\n- analysis and proposed loss weightings make sense\n- analysis and methodology in principle good"
            },
            "weaknesses": {
                "value": "- literally, no conclusion can be drawn from a single environment (with two levels of noise)\n- the paper is empirical, so in order to make a robust statement the method needs to be evaluated on a large range of tasks\n- the exponential weighting requires another hyperparameter that needs to be tuned to the horizon (and likely the task)\n- presentation of results can be improved"
            },
            "questions": {
                "value": "- Sec 4.2 + Fig 1: it is hard to believe that the model with h=10 makes no difference in the random case, but h=50 suddenly makes a big difference. \n    - The inset does not really help, as all lines coincide\n- Sec 4.3 + Fig 2: I am confused. Here $h=2$ ($\\alpha=0$)$ improves performance whereas in Fig 1 it did not?!\n    - Also, I would suggest plotting these results with respect to h=1. So h=1 is a flat line at 0. It would be potentially easier to see differences. \n    - I think the long-term behavior (h>50) might not be so helpful, as such low $R^2$ values render the models useless anyway\n\n- Sec 5.2: I would expect that when I need a model for h-step horizon planning, I would train it with an h-step loss (and then also only evaluate the prediction power up to h-steps. In the plots, there is mostly no visible difference after those steps. But how important is a difference in $R^2$ close to 1? Your Fig 5 suggests small differences can be actually quite important when planning. \n- Fig 4: for h=4 and h=10, the pink lines seems to be missing\n- Also for the non-noisy case, the learned weighting seems very competitive, which is really interesting.\n\n- Sec 5.3:\nI see where little robust results here. for h=10 it seems to be mostly worse and with high variance.\nFig 5: Do I understand correctly that e.g. the red markers are all the same model trained with n=10 loss but used only for planning with h=1, 5, 10... ?\n\nComments: \n- I suggest in a future submission to run the same method on a variety of tasks going beyond just low-dimensional tasks and the DM-control suite. Following up on the learned alphas might be very fruitful and general. \n- testing the strength of the models with a short horizon model-based planning method might also be useful"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8377/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698605022505,
        "cdate": 1698605022505,
        "tmdate": 1699637042369,
        "mdate": 1699637042369,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MTLVBt4yWl",
        "forum": "Rh4DmXaf8R",
        "replyto": "Rh4DmXaf8R",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8377/Reviewer_huS2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8377/Reviewer_huS2"
        ],
        "content": {
            "summary": {
                "value": "This papers studies how the accuracy of a model used for model-based reinforcement learning depends on the prediction horizon it has been to trained to predict at. It proposes different heuristics to weight the prediction horizons into a unified loss, and studies on CartPole how the prediction performance is related to the expected return collected by the resulting agent after RL training."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "Despite the severe limitations of the paper, the following are positive points about its perspective on model-based reinforcement learning:\n- The problem of finding better loss functions for training models of the dynamics, considering the final use that the reinforcement learning algorithm will make of these models, is important and relevant to the community\n- I find the approach based on weighting different prediction horizons in a different way to be promising."
            },
            "weaknesses": {
                "value": "Unfortunately, I believe that the current iteration of the paper lacks a sufficient level of rigor for the contribution to be ready for publication:\n- Despite the paper says this is a limitation, I believe the fact that the study is only conducted using a single, extremely simple, environment reduces the scope of the paper to be so small to be irrelevant. I encourage the authors to consider a larger suite of benchmarks, (e.g., MuJoCo, Brax, Myriad, MinAtar, Atari), picking the one that best suites their computational constraints.\n- The results on the performance of RL algorithms are not particularly meaningful or significative (e.g., looking at Figure 4 and 5, the performance of all approaches seems to be the same): this might actually be related to the fact that more complex or even just diverse environments might be required to have a better understanding of how dynamics accuracy is related to expected return collected by an agent.\n- Empirical comparison with similar approaches that are mentioned in the related work is missing. For instance, it is unclear why one should use a weighted loss instead of learning a different model for each time horizon."
            },
            "questions": {
                "value": "- What are the results in other environments?\n- What are the results of comparison with other baselines?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8377/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698768563249,
        "cdate": 1698768563249,
        "tmdate": 1699637042252,
        "mdate": 1699637042252,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jvTTuthVgx",
        "forum": "Rh4DmXaf8R",
        "replyto": "Rh4DmXaf8R",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8377/Reviewer_yoeH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8377/Reviewer_yoeH"
        ],
        "content": {
            "summary": {
                "value": "This paper tackles the compounding of model prediction errors\nin model-based reinforcement learning for long prediction horizons.\nThey approach this by rolling out the prediction from a model\nfor multiple time-steps, computing the error for each time-step,\nweighting the losses, and backpropagating the loss through the\nfull rollout trajectory. They tested several weighting strategies,\nsuch as a uniform weighting, exponentially decaying weighting,\nweighting to normalize the loss magnitudes.\n\nThe work performed experiments on the cartpole task in both a batch RL\nsetting (where they learn the model from data, then optimize the\npolicy from this model), and in an iterated RL setting, where the\npolicy is deployed in the environment to gather more data, iterating\nmultiple times to improve the performance.  In the batch RL setting,\nthe results were not statistically significant, while in the iterated\nRL setting there was no improvement over a regular 1-step model."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The use of the R2 score for evaluation of the prediction accuracy\nwas nice, as it provides an interpretable metric.\n- The literature review and discussion were OK."
            },
            "weaknesses": {
                "value": "- The experimental results are not substantial. The method is only\ndemonstrated on cart-pole, and there is no statistically significant\nimprovement. I am not convinced the method works effectively.\n\n- In some of the datasets, the data is generated from a fixed policy,\nand the one-step model is used to predict the state at time step $t+h$,\nby sequentially applying the actions $a_t, a_{t+1}, a_{t+2}$, etc. that\nwere applied in the rollout. In practice, the actions may also be\ncorrelated with the state transitions, so making predictions in such\na feedforward manner may lead to inaccurate predictions. It may be\nbetter to consider both the cases when applying the actions in a\nfeedforward manner, as well as the case when the actions are computed\nfrom the policy based on the predicted states, as these may be different.\nA simple exmaple to see the difference is when the environment is noisy,\nand a feedback controller is applied in the system. The feedback controller\nwould control the system to eliminate the noise, and keep the system stable.\nBut if the model is rolled out in a feedforward manner, the applied\nactions are unrelated to the noise, and the prediction may not be stable."
            },
            "questions": {
                "value": "Much more substantial experiments (in many more tasks) would be\nnecessary to change my opinion. This would be a large change to the\ncurrent manuscript, which I think would be too large and would require\na resubmission. Also, I am not confident that the method will provide\nan improvement.\n\nHow do the computational costs for the different horizon training\nmethods compare?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8377/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698796108061,
        "cdate": 1698796108061,
        "tmdate": 1699637042131,
        "mdate": 1699637042131,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Ab3ewc4D7t",
        "forum": "Rh4DmXaf8R",
        "replyto": "Rh4DmXaf8R",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8377/Reviewer_XNaE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8377/Reviewer_XNaE"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes to use a multi-step loss to train a dynamics model. Instead of training on the standard 1-step loss, the paper proposes to use the n-step loss that recursively backpropagates through each model update. The learned dynamics model is then evaluated via batch (aka offline) and iterated reinforcement learning on the cartpole model."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper is nicely written and very easy to follow."
            },
            "weaknesses": {
                "value": "The paper has two severe weaknesses, first the proposed approach has been evaluated multiple times and second the experimental evaluation is very limited. \n\n1) Multi-step Losses:\nIf I understand the proposed multistep loss correctly, this multistep loss has been proposed and utilized very often. For example, see the references [1-4] and there are many more. I am quite certain that one could even go back to the older system identification literature that talks about the multi-step loss for linear system identification. [4] even provides an ablation study across multiple horizons and systems. Interestingly, the results of [4] paint a different picture that going longer in the horizon is not necessarily better in terms of reward and quite frequently even worse (see Figure 3). The evaluation metrics as well as the RL algorithm are different in [4], [4] uses a CEM-like planner using the learned model instead of an actor-critic, but the discrepancy requires further investigation (see point 2).   \n\nCould the authors please precisely elaborate on how their multi-step loss is different from the previous works, except for the different weighting approach? \n\n2) Experimental Evaluation:\nThe paper only evaluates using the cartpole swing-up task which is quite limited and does not include discrete contact events. Therefore, the paper would need to evaluate on much more and more complex dynamical system. Again [4] shows quite nicely that the results/conclusions from a cartpole do not transfer to a more complex system. [4] shows that for the cart pole, the longer horizon is always better in terms of prediction error and obtained reward, the conclusion cannot be generalized to more complex systems, where longer horizons start to perform worse. \n\nTo make more claims about the multi-step loss the paper needs more evaluation of more complex systems that involve contacts.\n\n[1] Hafner et. al. (2015). Learning latent dynamics for planning from pixels\n[2] Abbeel et. al. (2005) Learning vehicular dynamics, with application to modeling helicopters\n[3] Venkatraman et. al. (2015) Improving multi-step prediction of learned time series models. \n[4] Lutter et. al. (2021). Learning Dynamics Models for Model Predictive Agents"
            },
            "questions": {
                "value": "see above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8377/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698800993566,
        "cdate": 1698800993566,
        "tmdate": 1699637041918,
        "mdate": 1699637041918,
        "license": "CC BY 4.0",
        "version": 2
    }
]