[
    {
        "id": "XREIQKJ63m",
        "forum": "MK7TEe7SJ3",
        "replyto": "MK7TEe7SJ3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5087/Reviewer_yHrZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5087/Reviewer_yHrZ"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a spatial-temporal attention MixFormer framework for visual object tracking, with experimental results affirming its effectiveness."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The USTAM approach is crafted as an end-to-end VOT network, integrating spatial and temporal attentions."
            },
            "weaknesses": {
                "value": "The proposed approach is an incremental improvement of MixFormer tracker. Many of the components in the paper such as MAM block,\nasymmetric attention, loss function have already been proposed in MixFormer."
            },
            "questions": {
                "value": "1. It would be beneficial to allocate a dedicated section to MAM, considering it serves as the primary building block for this approach. Distinguishing between the author's specific contributions and those stemming from MAM can be challenging otherwise.\n2. Rearranging the reference section in order of the last name of the first author would enhance searchability.\n3. The dimensions of G_f^i nxn don't seem to align. G_f^i represents the attention map between the search area and mixed, which is the combination of both the search and target areas.\n4. Figure 1 appears too small to discern the letters effectively.\n5. In equation (6), given that g_i,j is the attention map in equation (5), it follows that the sum of each row of G_f_t should be 1 after applying softmax. This operation essentially subtracts a constant value. Could you elaborate on the rationale behind this step?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5087/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698674842715,
        "cdate": 1698674842715,
        "tmdate": 1699636499556,
        "mdate": 1699636499556,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DJQSW8qfUV",
        "forum": "MK7TEe7SJ3",
        "replyto": "MK7TEe7SJ3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5087/Reviewer_ciuV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5087/Reviewer_ciuV"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to track the target object using spatial and temporal attention-based Transformer networks. This paper points out that existing works fail to find the appropriate balance between effective feature extraction and the incorporation of attention modules. They also lack explicit modeling of the relationship between spatial and temporal information. The experiments are conducted based on three widely SOT datasets.\n\nthe issues of this work are that:\n\nthe idea of incorporating attention mechanisms into the Transformer networks for tracking is not new;\nthe speed of this tracker is about 30-40+ FPS, which is not fast compared with other SOTA trackers, such as OSTrack;\nConsidering the limited novelties and regular tracking efficiency, I tend to reject this paper."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "This paper proposes to track the target object using spatial and temporal attention-based Transformer networks. This paper points out that existing works fail to find the appropriate balance between effective feature extraction and the incorporation of attention modules. They also lack explicit modeling of the relationship between spatial and temporal information. The experiments are conducted based on three widely SOT datasets."
            },
            "weaknesses": {
                "value": "the issues of this work are that:\n\nthe idea of incorporating attention mechanisms into the Transformer networks for tracking is not new;\nthe speed of this tracker is about 30-40+ FPS, which is not fast compared with other SOTA trackers, such as OSTrack;\nConsidering the limited novelties and regular tracking efficiency, I tend to reject this paper."
            },
            "questions": {
                "value": "1. re-organization of the novelties proposed in this work; \n2. showing the real advantages of this work;"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5087/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698763537901,
        "cdate": 1698763537901,
        "tmdate": 1699636499471,
        "mdate": 1699636499471,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gsgRwkbiwu",
        "forum": "MK7TEe7SJ3",
        "replyto": "MK7TEe7SJ3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5087/Reviewer_DDtH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5087/Reviewer_DDtH"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a unified spatio-temporal attention mixformer framework for video object tracking (VOT). Specifically, they\u2019re two main contributions stated by the authors: 1) a simple yet effective unified pipeline is proposed for feature extraction, target information integration, and localization estimation within the framework of a ViT network; 2) a spatio-temporal attention module is introduced to more effectively distinguish the target from the complicated background. Experimental results on several popular VOT benchmarks show the proposed approach performs favorably against SOTA trackers."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The idea seems to be somewhat effective, which can observe some performance improvements on the main VOT benchmarks (e.g., LaSOT and TrackingNet).\n- The paper is well organized, which is easy to follow.\n- Sufficient related works are discussed in Sec. 2."
            },
            "weaknesses": {
                "value": "- The statement for \u2018We present a simple but effective unified VOT pipeline for feature extraction, target information integration, and localization estimation within the framework of a ViT network\u2019 is not really true. This unified framework has already been proposed in previous one-stage trackers, e.g., OSTrack, including all the feature extraction, target interaction and localization in the same ViT framework.\n- The contribution in this paper is somewhat incremental. It seems that the proposed framework is still similar to the MixFormer framework, although it uses a ViT-based architecture and considering the previous target state by using the temporal attention module.\n- The usage of the temporal attention module is a bit similar to use the Cosine Window (e.g., also used in OSTrack), which also makes the tracker object moves smoothly in consecutive frames. In this paper, the authors make it in a learnable way by using the attention map in the previous frame. But one unsolved problem is about the reliability of the previous target state. If the previous prediction is noisy, the effectiveness of the proposed approach is also questionable.\n- Missing some essential details and unfair comparison. It is not clear whether the proposed tracker use the pre-trained models. e.g., OSTrack and DropTrack. In Table 2, the authors compare with the OSTrack-384 only trained on GOT-10k training split, while the proposed approach additionally  uses more training data, which is not fair. From Table 3, it seems that the proposed USTAM-B-384 trained on GOT-10K is inferior to OSTrack-384. What\u2019s the reason? Does the compared two approaches use the same pre-trained model?"
            },
            "questions": {
                "value": "- The statement for \u2018We present a simple but effective unified VOT pipeline for feature extraction, target information integration, and localization estimation within the framework of a ViT network\u2019 is not really true. This unified framework has already been proposed in previous one-stage trackers, e.g., OSTrack, including all the feature extraction, target interaction and localization in the same ViT framework.\n- The contribution in this paper is somewhat incremental. It seems that the proposed framework is still similar to the MixFormer framework, although it uses a ViT-based architecture and considering the previous target state by using the temporal attention module.\n- The usage of the temporal attention module is a bit similar to use the Cosine Window (e.g., also used in OSTrack), which also makes the tracker object moves smoothly in consecutive frames. In this paper, the authors make it in a learnable way by using the attention map in the previous frame. But one unsolved problem is about the reliability of the previous target state. If the previous prediction is noisy, the effectiveness of the proposed approach is also questionable.\n- Missing some essential details and unfair comparison. It is not clear whether the proposed tracker use the pre-trained models. e.g., OSTrack and DropTrack. In Table 2, the authors compare with the OSTrack-384 only trained on GOT-10k training split, while the proposed approach additionally  uses more training data, which is not fair. From Table 3, it seems that the proposed USTAM-B-384 trained on GOT-10K is inferior to OSTrack-384. What\u2019s the reason? Does the compared two approaches use the same pre-trained model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5087/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699517405314,
        "cdate": 1699517405314,
        "tmdate": 1699636499361,
        "mdate": 1699636499361,
        "license": "CC BY 4.0",
        "version": 2
    }
]