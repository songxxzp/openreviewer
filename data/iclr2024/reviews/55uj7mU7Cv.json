[
    {
        "id": "MBEbyaRJmW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8129/Reviewer_rKLC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8129/Reviewer_rKLC"
        ],
        "forum": "55uj7mU7Cv",
        "replyto": "55uj7mU7Cv",
        "content": {
            "summary": {
                "value": "The paper tackles common failures in CycleGAN and variants where the desired translation functions are not successfully identified and the methods produce content-misaligned translations. \n\nThis limitation is claimed to be related to the presence of multiple translation functions (MPA). The authors introduce an MPA elimination theory and suggest a modified learning approach in which the cross-domain distributions are matched over auxiliary variable-induced subsets of the domains (e.g. translation between real human faces to cartoonized figures is conditioned on hair color and gender). \n\nQuantitative and qualitative evaluation on several geometrically-unaligned pairs of datasets (Rotated MNIST, Rotated Edges-2-Shoes and CelebA to Bitmoji) are presented to support the theoretical claims."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The studied problem of matching the distributions of unaligned image domains is of great interest in the image-to-image translation line of work. The development of theoretical frameworks as the one introduced in this paper can shed light on the limitations of such unsupervised approaches and lead to more robust translation methods."
            },
            "weaknesses": {
                "value": "1. The experimental study is not conducted in the most relevant setting in my opinion. As the proposed method relies on several auxiliary variables (e.g. hair color in human faces or the digit class in the MNIST experiment), I believe the baselines should represent methods in weakly-supervised image-to-image translation [1]. Comparison against unsupervised methods is unfair.\n\n2. The authors claim the auxiliary variables can be queried from available foundation models as CLIP. This idea is already explored in [1], could the authors please provide any experimental benchmark including CLIP-based annotations against [1]?\n\n3. There are some other works relating the failures in geometrically-unaligned image domains to architectural inductive biases [2]. Moreover, methods as [2] present translations between domains with some degree of geometry variation without access to additional labels in the form proposed in this paper. Could the authors provide a comparison to [2] on the celebA-to-bitmoji?\n\n[1] Gabbay et al. \u201cAn Image is Worth More Than a Thousand Words: Towards Disentanglement in the Wild\u201d. In NeurIPS, 2021.\n\n[2] Gabbay et al. \u201cScaling-up Disentanglement for Image Translation\u201d. In ICCV, 2021."
            },
            "questions": {
                "value": "1. I find the qualitative results quite limited. For example, In Fig. 8, the translation from human faces to bitmoji does not preserve the facial expression. Considering that the gender and hair color is provided to the model, and the facial expression is not preserved, what other properties should the reader focus on to verify the validity of the translation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8129/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8129/Reviewer_rKLC",
                    "ICLR.cc/2024/Conference/Submission8129/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8129/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697302210434,
        "cdate": 1697302210434,
        "tmdate": 1700237510523,
        "mdate": 1700237510523,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "A0W8cwdIMw",
        "forum": "55uj7mU7Cv",
        "replyto": "55uj7mU7Cv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8129/Reviewer_siTh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8129/Reviewer_siTh"
        ],
        "content": {
            "summary": {
                "value": "The paper studies Unsupervised domain translation (UDT),\ne.g. to learn to generate cartoon sketches from ID photos\nwithout supervision. The authors study why CycleGANs fail to\nlearn the desired UDT function; previous work has suggested that\nthe reason is the existence of automorphisms of the generative\ndistributions; they corroborate this suggestion with a theoretical\nargument and then propose a second theoretical argument that\nprevents existence of such automorphisms if one introduces conditioning\non auxiliary variables. They then show the effectiveness of the proposed\nautomorphisms elimination approach on a few benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is well written and easy to read.\n\n2. The suggested elimination idea is well-motivated and simple\n  to implement.\n\n3. While *Theorem 1* operates under idealized assumptions, *Theorem 2*\n  makes an attempt to show that the author's proposal is robust\n  under more realistic circumstances.\n\n4. The UDT tasks they experiment on seem challenging enough to be interesting."
            },
            "weaknesses": {
                "value": "The abstract sounds very specialistic to me. I think the paper might be of interest to a broader audience, but some readers unfamiliar with the jargon might be put off by the abstract."
            },
            "questions": {
                "value": "My initial rating inclines towards acceptance. A limitation of my review is that I have not a direct experience with the baselines, so I cannot assess if the chosen baselines were too easy to beat.\n\n**Questions**:\n1. In assumption 1 how realistic are the invertibility assumptions?\n  Are there weakened versions, e.g. in a probabilistic sense?\n2. Regarding Proposition 1 and the MNIST example in Figure 1, it seems\n  that for MNIST the support of $P_x$ would not be path-connected, with one path-component\n  for each digit. Then Proposition 1 would not apply directly. Can you formulate a case of Proposition 1 that would apply to this case?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8129/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8129/Reviewer_siTh",
                    "ICLR.cc/2024/Conference/Submission8129/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8129/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698409414569,
        "cdate": 1698409414569,
        "tmdate": 1700230273290,
        "mdate": 1700230273290,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GwxeHlvI75",
        "forum": "55uj7mU7Cv",
        "replyto": "55uj7mU7Cv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8129/Reviewer_bENZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8129/Reviewer_bENZ"
        ],
        "content": {
            "summary": {
                "value": "This paper seeks to address the issue of content misalignment in unsupervised domain translation. The authors pinpoint the presence of \"measure preserving automorphism\" (MPA) as the primary culprit and present a theoretically-founded method to neutralize it. Their method was validated on datasets like Edges to Rotated Shoes, yielding high-quality samples that maintained content integrity."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. It is innovative to introduce auxiliary variables for tackling the MPA issue. I'd like to offer more insights on this approach. Essentially, **supervised domain translation can be seen as a specific instance of their method.** By choosing a specific auxiliary variable, we can tailor each conditional distribution $p(x|u=u_i)$ to hold precisely one sample, $x_i$, with a probability of 1, whereas all other samples in the space have a zero probability. Similarly, we manipulate each conditional distribution $p(y|u=u_i)$ to include only one sample, $y_i$, also with a probability of 1. These corresponding sample pairs, $(x_i, y_i)$, are essentially the supervised pairs for domain translation. By adjusting loss function 7 (i.e., the distance metric of cycle loss and the balance parameter $\\lambda$), this approach could replicate any supervised domain translation methods. The paper's impact would be significantly enhanced if the authors included this observation.\n\n2. Overall, the structure of the proof is clear and easy to follow.\n3. The experiments verified that this method could generate content-preserved samples with high quality, which corroborates their theory."
            },
            "weaknesses": {
                "value": "Overall, the structure of the theory is clear. However, there are several mistakes that should be corrected:\n1. The MPA of the PDF of a gaussian distribution $N(\\mu, \\sigma)$ should be $h(x) = 2\\mu - x$, rather than $h(x) = \\mu - x$.\n2. Within the \"Notation\" section of the introduction, \"A\" ought to be a subset of \"Y\", not \"X\".\n\nHonestly, it is impractical to check every detail of the proof. The author should ensure the proof's rigor and review it meticulously.\n\nAdditional suggestion: Assumption 1 is confusing. I think it refers to the existence of the content-preserved mapping $f^*$ and $y^*$. Please make it clearer."
            },
            "questions": {
                "value": "In this study, it appears that only one auxiliary variable is used to diversify the distribution. What would happen if we used several variables? For example, we're considering not just the distributions $p(x|u=u_i)$ and $p(y|u=u_i)$, but also $p(x|v=v_j)$ and $p(y|v=v_j)$. Intuitively, utilizing one auxiliary variable is akin to \"slicing\" the original distribution in one way, while employing multiple variables is like trying different ways to make the \"cuts\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8129/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8129/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8129/Reviewer_bENZ"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8129/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698808465190,
        "cdate": 1698808465190,
        "tmdate": 1699637006892,
        "mdate": 1699637006892,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qVNEvRAwLl",
        "forum": "55uj7mU7Cv",
        "replyto": "55uj7mU7Cv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8129/Reviewer_4LhS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8129/Reviewer_4LhS"
        ],
        "content": {
            "summary": {
                "value": "The paper aims to propose an unsupervised image translation framework that ensures identifiability of underlying generator maps. It achieves the same by relying on auxiliary variables. Promising empirical results showcase the potential of the method on benchmark image datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The writing of the paper is good with detailed exposition of the problem. It also includes detailed notes on related literature. The paper produces promising qualitative and quantitative results based on experiments. It also gives ample ablation and suggestions on the architecture and parameters involved. The brief declaration of limitations is appreciated."
            },
            "weaknesses": {
                "value": "The theory under quite strong assumptions tends to be straightforward and does not fully complement what the paper set out to achieve. It revolves around a particular model and due to certain vague notions becomes somewhat vacuous. The empirical results give the paper strength which the theory fails to support. In my opinion, the experiments should be prioritized."
            },
            "questions": {
                "value": "1. My first concern is regarding the strong assumption that continuous functions $f^*$  and  $g^*$ exist under arbitrary input data on both domains. It is quite challenging to ensure that the optimal transport map between distributions has any regularity (e.g. Lipschitz continuity). Also, in most cases proving so requires the support of the base distribution to have restrictions in terms of convexity. In real data, the same hardly follows. Perhaps sacrificing generality for the sake of accuracy would be better for the theory.\n\n2. The discussion on the notion of \"content\" seems vague. Is the content of an image and its rotated counterpart the same? Is there any generalization of it for general group actions?\n\nIs there any way of justifying Assumption 1, even if with examples? \n\nIt seems to me that homeomorphic spaces will have the same \"content\", whereas the assumption claims the converse. Am I right in saying that?\n\n3. Given there exist non-unique members in the kernel (i.e. multiple solutions bringing about zero loss), is Definition 1 even meaningful in a non-parametric setup, where there is no inherent identifier?\n\n4. The entire theory revolves around the CycleGAN loss in particular. This does not complement the initial impression of unsupervised domain translation in general. The proposed loss function ($7$) is also a modified CycleGAN setup. Also, what are the \"any criterion\" in Fact 1?\n\nDoes the discriminator play any role in ensuring identifiability? This seems crucial as the resultant translation map would be a result of a stable discriminator.\n\n5. [Section 3] Can this at all be called unsupervised given that pseudo or weak labels ($u$) are used? Also, what is meant by \"sufficiently different\" $P_{x|u_i}$ and $P_{x|u_j}$?\n\nShouldn't the difference between distributions $P_{x|u(A,B)}[A]$ and $P_{x|u(A,B)}[B]$ be based on a divergence measure and not inequality ($\\neq$)? \n\nThere remain some typographical/grammatical errors in the manuscript (e.g. see the Section Identifiability Characterization)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8129/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699397063436,
        "cdate": 1699397063436,
        "tmdate": 1699637006637,
        "mdate": 1699637006637,
        "license": "CC BY 4.0",
        "version": 2
    }
]