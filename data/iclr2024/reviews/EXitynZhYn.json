[
    {
        "id": "6GqEGkxzzS",
        "forum": "EXitynZhYn",
        "replyto": "EXitynZhYn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1243/Reviewer_GTBt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1243/Reviewer_GTBt"
        ],
        "content": {
            "summary": {
                "value": "The work proposes a more principled evaluation of vision-language models by taking advantage of the semantic hierarchy of textual labels. The core problem the work addresses is that the exact match scoring systems used by VQA benchmarks can be ambiguous, and penalize models for essentially correct answers. To fix this, the authors crop images so to the object of interest, allow coarse answers, and ask follow up questions to clarify the fine-grained answer. Finally, they also measure performance with multiple metrics, such as BertScore and GPT-4 based evaluation."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "While a substantial body of work exists on designing better benchmarks for VQA, the idea of using the semantic hierarchy to ask follow up questions is original. VQA is hard to evaluate, yet is the most fundamental task in vision-language (other than perhaps, image-text matching). One of the biggest problems in VQA is that questions can be underspecified or ambiguous and there can be multiple correct answers to questions. \n\nThe method presented here is a neat way to deal with these ambiguities and inconsistencies in the evaluation process. \n\nAnother useful contribution of the paper is the empirical evaluation of vision-language models on different aspects of vision-language, such as attribute recognition and coarse vs fine-grained recognition. This is useful. \n\nFinally, Table 7 is also very useful, especially because it helps resolve questions about the appropriateness of different metrics for open-ended VQA scoring."
            },
            "weaknesses": {
                "value": "There are no substantial weaknesses. \n\nA figure showing the differences between a VQA question + label and examples from the proposed datasets would be useful. Fig. 2 would be easier to read if it said \"attribute\", \"object\" etc instead of OVAD, COCO."
            },
            "questions": {
                "value": "I have no questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1243/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698714265899,
        "cdate": 1698714265899,
        "tmdate": 1699636050856,
        "mdate": 1699636050856,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dsQASn1WLm",
        "forum": "EXitynZhYn",
        "replyto": "EXitynZhYn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1243/Reviewer_3kX8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1243/Reviewer_3kX8"
        ],
        "content": {
            "summary": {
                "value": "Evaluating text-generative vision-language models remains an intricate undertaking due to the inherent limitations of current Visual Question Answering (VQA) benchmarks. Motivated by the need for a more comprehensive assessment mechanism, this paper aims to address the ambiguity and specificity issues rampant in open-ended VQA (oVQA) tasks. These challenges emerge, in part, from the diverse ways natural language can express similar ideas and the constraints of current evaluation metrics that often favor shorter responses. To navigate these obstacles, this research introduces a novel VQA benchmark, enriched with subbenchmarks focusing on objects, actions, and attributes. It leverages a follow-up procedure, drawing from existing classification benchmarks, to generate contextually apt questions and expects models to provide answers with the requisite level of detail. Additionally, the study explores various metrics from the VQA and NLP spheres to find an optimal evaluation criterion, validated via human judgment. This holistic approach paves the way for an intricate analysis of leading vision-language models, unearthing their specific strengths and vulnerabilities."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Innovative Evaluation Methodologies: The research addresses the inadequacies of existing Visual Question Answering (VQA) benchmarks by proposing a new VQA benchmark. This benchmark, derived from well-established visual classification datasets, facilitates a more detailed evaluation of text-generative vision-language models and their comparative performance against discriminative vision-language models.\n\n2. Addressing Coarse Answer Challenges: Recognizing the challenges in evaluating coarse answers in fine-grained tasks, the research introduces a method using the semantic hierarchy of labels. By doing so, it can automatically generate pertinent follow-up questions about the true category, pushing for more accurate and detailed model responses.\n\n3. Enhanced Ambiguity Management: To better handle the inherent ambiguities in open-ended visual questions, a unique follow-up procedure is proposed. By adapting classification benchmarks for oVQA, the model is first provided with an apt visual context, and then, based on its initial response, a further clarifying question is posed using concept hierarchies. This ensures answers with the desired detail and precision.\n\n4. Comprehensive Metric Evaluation: The research undertakes a rigorous examination of various metrics from both the VQA and NLP domains, emphasizing those that treat paraphrasing and synonyms as valid answers. The eventual metric is grounded in a human evaluation study, ensuring that it aligns closely with human judgments and provides a reliable measure for assessing model performances."
            },
            "weaknesses": {
                "value": "The paper omits certain statistical details regarding the oVQA dataset, such as the distribution of question/answer lengths, the number of entity class labels, and the formats/types of the questions. Given the dataset's general and expansive nature, there is a concern that, despite the introduction of subbenchmarks, it might introduce new challenges that could affect the quality of evaluations, such as issues related to imbalance."
            },
            "questions": {
                "value": "Please also refer to the previous section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1243/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1243/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1243/Reviewer_3kX8"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1243/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698785006422,
        "cdate": 1698785006422,
        "tmdate": 1699636050779,
        "mdate": 1699636050779,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wFTGhjKpB3",
        "forum": "EXitynZhYn",
        "replyto": "EXitynZhYn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1243/Reviewer_bBwP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1243/Reviewer_bBwP"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a dataset to granularly evaluate the text-generative vision-language models. The authors base the evaluation benchmark on classification datasets where the labels semantic hierarchy are present. The semantic hierarchy is used as the source of generating the follow-up questions."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(1) propose a new topic for evaluating VQA models that assess how well the VQA models can classify or recognize fine-grained objects, activity, and attributes\n\n(2) A intuitive solution to constructing a benchmark that contains the class semantic hierarchy based on classification model.\n\n(3) comprehensive evaluation on lots of open sourced VQA systems."
            },
            "weaknesses": {
                "value": "(1) I believe assessing how granular a VQA system is in good and necessary, however, when evaluating, the current benchmark put additional constrains on the image space, cropping the image to certain objects and using imagenet which is object centric. These constrains largely limited the scope of VQA that is supposed to work on natural use case, for example, natural image QA (VQA v2, vizwiz, etc). \n\n(2) The Cropping activity operations seems very risky the action needs a few frames to evaluate? Like sit down and stand up?"
            },
            "questions": {
                "value": "Please comment on the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1243/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1243/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1243/Reviewer_bBwP"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1243/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698892847825,
        "cdate": 1698892847825,
        "tmdate": 1699636050721,
        "mdate": 1699636050721,
        "license": "CC BY 4.0",
        "version": 2
    }
]