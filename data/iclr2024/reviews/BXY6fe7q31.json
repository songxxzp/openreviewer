[
    {
        "id": "GR9OjOEYbR",
        "forum": "BXY6fe7q31",
        "replyto": "BXY6fe7q31",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission192/Reviewer_kFPK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission192/Reviewer_kFPK"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to address the issue that the image-captioning based training objective often leads the visual prompt generators (VPGs) to neglect visual details. It proposes a VPG-C(omplete) module to complete the missing details and a synthetic discriminative training strategy to train VPG-C without the need for supervised instructions. The experiments are conducted on the proposed DEMON benchmark, the MME, and OwlEval benchmarks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well-written and easy to follow.\n- The idea of completing the missing details for visual content is reasonable. The method of synthetic discriminative training is interesting and straightforward for training the VPG-C without the need for supervised instructions.\n- The proposed DEMON benchmark encompasses a wide range of tasks spanning multiple categories, offering the potential for the evaluation of future research efforts."
            },
            "weaknesses": {
                "value": "- The effectiveness of the proposed VPG-C has not been fully validated. On the proposed DEMON benchmark, the improvement of VPG-C compared to InstructBLIP is quite limited, which does not align with the expectation that completed details would significantly enhance VPG. The improvement on the MME benchmark is also not significant.\n- The experimental evaluations on many common benchmarks are missing, such as the evaluation protocols of InstructBLIP (\u201ddivide the 26 datasets into 13 held-in datasets and 13 held-out datasets\u201d including NoCaps, Flickr30K, GQA, VSR, IconQA, TextVQA, Visdial, HM, VizWiz, SciQA Image, MSVD QA, MSRVT QA, iVQA), the Mini-GPT4 dataset, the LLaVA-Instruct-150K benchmark, and the MIMIC-IT dataset. Evaluating on the same benchmarks as other MLLMs is important for a comprehensive and fair comparison."
            },
            "questions": {
                "value": "The primary concerns are related to experimental evaluation, and the rating would improve if the experimental evaluation were comprehensive."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission192/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission192/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission192/Reviewer_kFPK"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission192/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698633247836,
        "cdate": 1698633247836,
        "tmdate": 1700493683367,
        "mdate": 1700493683367,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rQnmwWT5XG",
        "forum": "BXY6fe7q31",
        "replyto": "BXY6fe7q31",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission192/Reviewer_gX7w"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission192/Reviewer_gX7w"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a new training method for Q-Former/Resampler, aiming to provide richer visual representations for lora-based Multimodal Language Models (MLLMs). In addition, the paper proposes a synthetic training dataset for training VPG-C. After training, VPG-C achieves surprising results on the benchmark proposed in this paper and other open-source benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The authors are the first (at least to my knowledge) to propose using the latent features of the intermediate layers of the llm as guidance for the q-former, providing directed detail supplementation for the LLM.\n\n2. The provided dataset/training method may inspire future research.\n\n3. The provided benchmark can better diagnose the capabilities of MLLMs."
            },
            "weaknesses": {
                "value": "1. The paper does not mention the setting for instruction tuning. My understanding is that after using the synthetic discriminative training strategy, the model automatically acquires the ability to follow instructions without needing an instruction tuning phase.\n\n2. The ablation in the paper validates the effectiveness of several proposed modules. But can VPG-C be applied to a finetuning setting, such as llava/minigpt4?\n\n3. In multimodal dialogues, does the model need to update guidance multiple times when providing multiple answers? In other words, for each new additional question, does the model need to run the whole model to obtaion all hidden states and can't use the existing qkv cache?"
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission192/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698769563918,
        "cdate": 1698769563918,
        "tmdate": 1699635945107,
        "mdate": 1699635945107,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EQePdHSTEY",
        "forum": "BXY6fe7q31",
        "replyto": "BXY6fe7q31",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission192/Reviewer_cxqK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission192/Reviewer_cxqK"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the instruction tuning in multimodal language models. In particular, it tries to improve the bottleneck of visual prompt generator (VPG), aka the visual feature converter which converts a generic visual embedding into LLM-interpretable inputs. It hypothesizes that the bottleneck comes from the lacking of attention to details in VPG. To address this, the paper proposed two components: (1) a VGP-C architecture which additionally generate features for intermediate LLM layers with attention to LLM intermediate features, (2) a synthetic data generation procedure to generate training data to teach VPG to attend to details. In addition to these, the paper also introduced a new evaluation benchmark."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper proposed two methods for multimodal instruction following, the VPGC architecture and the data synthesis technique. Both of them are novel and inspiring.\n2. Plenty of ablation studies are provided to support the effectiveness of the method. Comprehensive experiments also demonstrate the superiority of the method compared to existing models.\n3. The paper also introduced a benchmark for future research."
            },
            "weaknesses": {
                "value": "1. The method contains several steps and is thus quite complicated. It may be hard to reproduce the whole framework in different settings and code bases."
            },
            "questions": {
                "value": "NA."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission192/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698987262259,
        "cdate": 1698987262259,
        "tmdate": 1699635945041,
        "mdate": 1699635945041,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ysMNhimnqn",
        "forum": "BXY6fe7q31",
        "replyto": "BXY6fe7q31",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission192/Reviewer_HGkc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission192/Reviewer_HGkc"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to improve reasoning capabilities of Multi-modal Large Language Models (MLLMs) for demonstrative instructions. The authors highlight that most of MLLMs have been over-optimized on the image-captioning objective which has led to the use of visual features that could only describe the captions while neglecting its focus on minor yet discriminative features important for fine-grained reasoning.  \n\nFirstly, this work proposes VPG-C which is a lightweight adaption on top of VPG which aims to reuse the information from the intermediate LLM layer and is used to modulate the VPG features through guidance. The modified VPG features are integrated into LLM intermediate layer which effectively improves the fine-grained reasoning performance.\n\nSecondly, to train the VPG-module, this work proposes a automatic way to generate synthetic data used to improve fine-grained discriminative performance of MLLMs. \n\nLastly, DEMON benchmark is proposed to evaluate the demonstrative instruction understanding of the proposed technique and other MLLMs.\n\nThe proposed approach is fairly motivated with analysis and ablation studies."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Strengths:\n\n1) This paper identifies and aims to address a crucial limitation of lack of reasoning capabilities of Multi-modal Large Language Models (MLLMs) for demonstrative instructions. Improving MLLMs for demonstrative instructions will pave more rapid growth for building human-friendly AI assistants.\n\n2) The proposed VPG-C design is fairly motivated and it is compute friendly.\n\n3) The idea of generating synthetic data with automatic pipeline to improve fine-grained discriminative capabilities of MLLMs is encouraging.\n\n4) The authors have proposed a suitable benchmark demon which would enable more systematic developments in improving MLLMs."
            },
            "weaknesses": {
                "value": "I could not observe any significant weaknesses. However I have a concern regarding the inference compute efficiency of the proposed approach. \n\n1) As the model is reusing its intermediate features via a feedback loop system, this will significantly increase the training and testing time and might not be batch friendly during inference. How does the throughput of this technique compares against previous methods?"
            },
            "questions": {
                "value": "Please refer to weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission192/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission192/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission192/Reviewer_HGkc"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission192/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699589638145,
        "cdate": 1699589638145,
        "tmdate": 1699635944982,
        "mdate": 1699635944982,
        "license": "CC BY 4.0",
        "version": 2
    }
]