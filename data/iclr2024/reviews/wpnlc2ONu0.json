[
    {
        "id": "awQ6KH0Dlp",
        "forum": "wpnlc2ONu0",
        "replyto": "wpnlc2ONu0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission557/Reviewer_p6P6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission557/Reviewer_p6P6"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes the Excitation-Inhibition Mechanism-assisted Hybrid Learning (EIHL) algorithm for training spiking neural networks. The algorithm combines the global learning and local learning together, and achieves better results than individual learning rules, as demonstrated through the experiments."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The method is inspired by biological mechanisms. Researchers in the SNN community should be encouraged to seek inspiration from neuroscience."
            },
            "weaknesses": {
                "value": "1.The authors do not provide a clear and comprehensive description of their method in section 4. The review raises several concerns, outlined as follows:\n\n (i). What does the notation $x$ mean? Is it a global parameter for the entire network? How does it \"gradually increase\" during the STDP period and \"decay by itself\" during the STBP period?\n\n (ii). What is the precise formula for the operation $thresh(\\cdot)$? It appears that $b$ remains unchanged during the STDP period based on Alg. 1. Does this imply that $thresh(b)$ remains constant in each STDP period?\n\n (iii). Could the authors provide further elaboration on Eq. 7? Why is gradient descent necessary for updating the weight in the STDP period? How is dL/dx computed, given that L does not seem to be differentiable with respect to x? Also, why do x and W share the same dimension? The reviewer thinks that it should be a scalar. Additionally, why is the \"contraction factor\" $a$ needed?\n\n (iv). Regarding alg. 1, how is the \"current sparsity\" calculated? Why is \"Curr S >= Pre S\" required in the algorithm?\n\n\n2.The contribution of this work is not very clear at the current stage. In the reviewer's understanding, this work aims at improving the current hybrid learning algorithms which \"have some theoretical and practical shortcomings and need further improvement\". However, there is no theoretical (mathematical) results in the paper and the biological plausibility of the proposed hybrid method is not inadequately clarified. Furthermore, the practical preformance is relatively unsatisfactory: \n\n(i). The improvement based on the global learning rule STBP is minimal. In essence, STBP represents a special case of the proposed EIHL with specifically chosen hyperparameters. Consequently, EIHL can consistently yield slightly superior results to STBP through randomness and careful hyperparameter tuning.\n\n(ii). The performance notably lags behind the latest research. Especially, the SOTA results of DVS_CIFAR10 is 20% better than the proposed method.\n\n(iii). There is no comparison between this work and other hybrid methods regarding accuracy and biological plausibility.\n\n(iv). No experiments on large-scale datasets.\n\nIn summary, the motivation and contribution of the paper remain ambiguous. The reviewer perceives this work as merely a combination of two exsiting methods without convincing reasons. \n\n3.This paper is not well-written. Several sentences lack coherence, making the overall presentation disjointed. The presentation of equations is arbitrary and non-standard. The resolution of Fig. 1 is low."
            },
            "questions": {
                "value": "Could the authors provide a more comprehensive explanation of the excitation-inhibition mechanism and how it is used in this work? Although the authors keep mentioning it, the reviewer cannot understand how excitatory and inhibitory synapses are handled differently and how they are balanced."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission557/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission557/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission557/Reviewer_p6P6"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission557/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698391270407,
        "cdate": 1698391270407,
        "tmdate": 1700735235815,
        "mdate": 1700735235815,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kVMENvJBbF",
        "forum": "wpnlc2ONu0",
        "replyto": "wpnlc2ONu0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission557/Reviewer_K674"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission557/Reviewer_K674"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a hybrid learning method that uses the neural excitation and inhibition mechanism to assist local learning and global learning (called EIHL), by simulating the biological neural excitation and inhibition mechanism to adjust the network connection state, thus integrating local learning and global learning. The experimental results also show that this method has advantages in accuracy and sparsity compared to separate local learning and global learning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper is inspired by the biological neural excitation-inhibition mechanism and proposes a new hybrid learning method for SNN, which is more brain-like than previous methods and has originality. Moreover, this paper has some practical significance from both the biological perspective and the accuracy and sparsity of the experimental results. Furthermore, the whole paper is logically coherent and fluent, and the language is concise and clear."
            },
            "weaknesses": {
                "value": "The Fig.1 in this paper that explains the EIHL algorithm is too simple and not detailed enough, only showing the connection processing between the convolutional layer and the IF neuron layer. This figure could consider adding another layer to show the specific operation of the algorithm more finely."
            },
            "questions": {
                "value": "Is the EIHL method only applied to the convolutional layer? If not, I hope it can be reflected in the figure. I suggest updating and optimizing Fig.1."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission557/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission557/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission557/Reviewer_K674"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission557/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698455695348,
        "cdate": 1698455695348,
        "tmdate": 1700697208428,
        "mdate": 1700697208428,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8uwyFPWoCP",
        "forum": "wpnlc2ONu0",
        "replyto": "wpnlc2ONu0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission557/Reviewer_UpZe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission557/Reviewer_UpZe"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an Excitation-Inhibition Mechanism assisted hybrid Learning (EIHL) algorithm. Inspired by the biological neural excitation-inhibition mechanism, it achieves adaptive adjustment of spiking neural network connectivity, and automatically alternates between global and local learning according to the growth or decay of synaptic strength which depends on the excitation-inhibition mechanism. It also conducts three experiments to demonstrate that this method has higher accuracy than global learning, and higher sparsity than local learning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper proposes a new hybrid method of local and global learning, which is regulated by the biological neural excitation-inhibition mechanism. The paper also argues that the excitation-inhibition mechanism leads to sparse results for neural networks, which is an innovative perspective. Moreover, the language and logic of the introduction and method description are clear and smooth. Finally, the paper presents EIHL as a new point in the field of SNN training methods, and also provides new insights for ANN training methods, which has some significance."
            },
            "weaknesses": {
                "value": "The paper conducted three experiments to verify the performance advantages of the method, but the details of the third experiment are less described. The third experiment should specify which layer or the whole network is randomly pruned at different levels, and also explain the specific operation of random pruning."
            },
            "questions": {
                "value": "I would like to ask the author, is the random pruning at different levels applied to the whole network or to a specific layer? And will the synapses that are randomly cut off at the beginning be restored in the later learning or remain disconnected? Maybe it should be explained in the third experiment."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission557/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699069678273,
        "cdate": 1699069678273,
        "tmdate": 1699635982753,
        "mdate": 1699635982753,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ibz9oC318r",
        "forum": "wpnlc2ONu0",
        "replyto": "wpnlc2ONu0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission557/Reviewer_iXWZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission557/Reviewer_iXWZ"
        ],
        "content": {
            "summary": {
                "value": "The authors proposed an Excitation-Inhibition Mechanism-assisted Hybrid Learning (EIHL) algorithm for training Spiking Neural Networks (SNNs), a learning algorithm that hybrid local learning rule and global learning rule. \nExperiments on CIFAR10/100 and DVS-CIFAR10 showed that EIHL outperforms other methods in terms of accuracy and sparsity."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Fusion of Global and Local Learning Rules: According to the authors, the integration of both global and local learning paradigms could potentially pave the way for attaining enhanced performance and energy efficiency in neural networks.\n\n2. Performance on CIFAR Benchmark: The authors successfully demonstrated an improvement in performance on the CIFAR dataset when compared to the traditional backpropagation technique."
            },
            "weaknesses": {
                "value": "1. The authors should elucidate their contributions more explicitly in order to provide a comprehensive understanding of the research.\n\n2. In the context of hybrid learning, 'STDP' process employs a contraction curve to facilitate Long-Term Depression. Nevertheless, the authors have not adequately expounded upon the association between LTD and STDP, and the proposed method do not have a dependence of the spike timing. It's not clear why excitation should be like STBP and depression should be like STDP.\n\n3. The accuracy of references should be ensured. For example, the paper states, \"Spike-Timing Dependent Plasticity (STDP) was proposed based on these rules by Caporale & Dan (2008).\" However, the discovery of STDP predates 2008. Caporale & Dan (2008) is a review paper."
            },
            "questions": {
                "value": "1. In the context of the hybrid learning rule, what is the significance of excitatory and inhibitory synapses, given that STDP and STBP do not appear to rely on the distinction between these synapse types? Furthermore, it seems that excitatory and inhibitory synapses are not typically delineated in deep spiking neural networks.\n\n2. Weight pruning is a technique employed in deep learning to increase network sparsity by eliminating the smallest weights. Please elucidate the distinctions between the 'STDP' process in EIHL and weight pruning techniques in deep learning.\n\n3. Kindly review the terminology and references utilized in the manuscript to ensure a more precise and coherent presentation of the research study."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission557/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission557/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission557/Reviewer_iXWZ"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission557/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699543956904,
        "cdate": 1699543956904,
        "tmdate": 1699635982688,
        "mdate": 1699635982688,
        "license": "CC BY 4.0",
        "version": 2
    }
]