[
    {
        "id": "GjJ4G3QHVw",
        "forum": "oEF7qExD9F",
        "replyto": "oEF7qExD9F",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission586/Reviewer_4AC7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission586/Reviewer_4AC7"
        ],
        "content": {
            "summary": {
                "value": "Transformer models are highly accurate but complex and not suited for sequential processing, making them ill-suited for edge devices. RNNs are more suitable for these devices due to their lower complexity and sequential processing capabilities, but they often lag in performance compared to Transformer models. The paper proposes LMUFormer, a model that augments Legendre Memory Units (LMU) with convolutional patch embedding and convolutional channel mixers. This results in a fully-sequential recurrent model that approaches the performance of Transformer models while retaining the ability to process data sequentially.  A spiking version of LMUFormer is also introduced, which brings the benefits of states within the patch embedding and channel mixer modules, further reducing computing complexity.\nThe contribution lies in \n1. New Model Architecture: Introduction of LMUFormer, a novel architecture that combines LMUs with convolutional patch embedding and channel mixers, aiming to provide a balance between high performance and low complexity.\n\n2. Performance: LMUFormer demonstrates impressive performance, particularly on the Speech Commands V2 dataset, showing comparable results to state-of-the-art transformer-based models but with significantly fewer parameters and lower computational complexity.\n\n3. Spiking Version: Presentation of a spiking version of LMUFormer, establishing a new state-of-the-art in low-complexity Spiking Neural Network (SNN) variants with an accuracy of 96.12%, and demonstrating the ability to process real-time data efficiently.\n\n4. Efficiency in Real-Time Processing: The model shows proficiency in real-time data processing, achieving a 32.03% reduction in sequence length with minimal performance decline, highlighting its suitability for streaming applications at the edge."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "**Originality:**\n- Innovative Model Design: The LMUFormer introduces a unique combination of Legendre Memory Units (LMU) with convolutional patch embedding and channel mixers, creating a novel architecture that stands out in the realm of spiking models and RNNs.\n- Spiking Model Integration: The integration of a spiking version of the LMUFormer adds a layer of originality, as it brings the benefits of states within the patch embedding and channel mixer modules, while also aiming to reduce computing complexity.\n\n**Quality:**\n- Robust Performance: The LMUFormer demonstrates robust performance, especially highlighted by its results on the Speech Commands V2 dataset, where it shows comparable performance to state-of-the-art transformer-based models but with a significant reduction in parameters and computational complexity.\n- Comprehensive Evaluation: The paper includes a thorough evaluation of the architectures on multiple sequence datasets, providing a solid basis for the claims made about the model\u2019s performance and efficiency.\n\n**Clarity:**\n- Well-Structured: The paper is well-structured, with clear sections that logically flow from one to the next, making it easy for readers to follow the development of ideas and understand the proposed model.\n- Detailed Explanations: The authors provide detailed explanations of the LMUFormer architecture, the spiking version of the model, and the motivations behind their design choices, contributing to the overall clarity of the paper.\n\n**Significance:**\n- Addressing Resource Constraints: The LMUFormer addresses a significant challenge in the field of edge computing and streaming applications, where devices are heavily resource-constrained. By providing a model that combines high performance with low complexity and sequential processing capabilities, the paper makes a meaningful contribution to this area."
            },
            "weaknesses": {
                "value": "**Addressing Potential Biases:**\nModel Limitations: The paper could be improved by providing a more balanced view, including a discussion of potential limitations or scenarios where the LMUFormer might not perform as well. This would help readers develop a more nuanced understanding of the model\u2019s applicability.\n\n**Enhancing Reproducibility:**\nImplementation Details: Providing more implementation details, including hyperparameters and training procedures, would enhance the reproducibility of the results, contributing to the paper\u2019s overall quality.\n\n**Real-Time Processing Analysis:** \nGiven the focus on streaming applications and real-time data processing, a more detailed analysis of the model\u2019s performance in real-time scenarios, including potential latency issues and how they are addressed, would be valuable.\n\n**Software-Hardware codesign**\nIt would be nice to see a hardware simulation for SNN to learn all aspect of the new model, engergy, latency, throughput and any overhead, etc."
            },
            "questions": {
                "value": "- citation for S4 models?\n\n- Could you add \"how to get 32.03%\" in section 5.5? I assume it is (128-87)/128? similarly to 70x and 140x to help reading"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission586/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698739714677,
        "cdate": 1698739714677,
        "tmdate": 1699635986057,
        "mdate": 1699635986057,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6tFvnxJ2JU",
        "forum": "oEF7qExD9F",
        "replyto": "oEF7qExD9F",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission586/Reviewer_FTnd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission586/Reviewer_FTnd"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a LMU based model that also has a spiking variant. The goal is to explore the accuracy gap between transformer based model with LMU based model with more complex sub-modules."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* This paper is mostly easy to follow and well organized.\n* Related background and works are discussed and introduced extensively.\n* The intention of exploring this performance gap in function complexity feels intuitive and interesting.\n* Experimental are conducted extensively amongst different models and tasks and comparing both accuracy and computational cost, results also look mostly promising."
            },
            "weaknesses": {
                "value": "* In task 2, there is still some accuracy gap between the proposed method and some transformer based methods. Is there any way to compare pLMU on the same task as well as resource usage? It feels the narrative is that accuracy wise on relative complex tasks LMUformer is on par with transformer based method while out-performing pLMU. Besides rMNIST task, it is difficult to tell how much performance gain that the proposed method would offer while (potentially) introducing extra hardware cost."
            },
            "questions": {
                "value": "* Does the proposed method suit SNN better and more nature? What about the training cost and time?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission586/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698799021765,
        "cdate": 1698799021765,
        "tmdate": 1699635985984,
        "mdate": 1699635985984,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hBxkiVMo3R",
        "forum": "oEF7qExD9F",
        "replyto": "oEF7qExD9F",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission586/Reviewer_2bBb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission586/Reviewer_2bBb"
        ],
        "content": {
            "summary": {
                "value": "This work combines Legendre Memory Units (LMU)  with convolutional patch embedding and convolutional channel mixer in its network design to improve accuracy. It also introduces a spiking version to further improve its inference efficiency. On speech recognition tasks, their results show a significant reduction in both model size and FLOPs compared to the SoTA AST model. It also achieves higher accuracy compared to other low-complexity SNN designs. It also shows it can achieve competitive accuracy with only two-thirds of the seq length due to its sequential processing capability."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The addition of the convolutional patch embedding (w/ 1d conv) and convolutional channel mixer to LMU is well explained and justified.\n- The model can be trained in parallel and has the streaming capability during inference. It does not need to wait until the entire input sequence is available, which can reduce the compute complexity and memory requirement.\n-  This transformer design achieves competitive accuracy on a wide range of tasks compared to its counterparts with low compute complexity. It also demonstrates better classification accuracy in long token tasks compared to other linear-complexity attention designs."
            },
            "weaknesses": {
                "value": "- The results could be made stronger if the model size and compute complexity comparison can be added to every performance comparison table (Table 1,2,4).\n- Ops represented in the paper for SNN can be a very optimistic proxy for latency performance. It is worth mentioning that SNNs might not be easily accelerated on off-the-shelf hardware like CPUs and GPUs. It might require specialized hardware to demstronate its advantage in latency and energy reduction."
            },
            "questions": {
                "value": "1. Is there a reason why Am(t) is changed to Am[t-1] from eqn(1) to eqn(2) for discretization?\n2. From eqn (4), it seems the LMU module is composed of mainly matrix-vector operations, which is similar to RNN. Is it correct that we can also parallelize it across the time dimension? can you elaborate on how it can be solved in non iterative way to enable parallelized training? \n3. I'm curious to know how its measured training and inference time compares to that of other linear-time transformers like Linformer on the off-the-shelf hardware."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission586/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission586/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission586/Reviewer_2bBb"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission586/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698822537692,
        "cdate": 1698822537692,
        "tmdate": 1699635985896,
        "mdate": 1699635985896,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1RmUmEoGQd",
        "forum": "oEF7qExD9F",
        "replyto": "oEF7qExD9F",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission586/Reviewer_KEVY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission586/Reviewer_KEVY"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a sequential network that exploits Legendre Memory Units (LMU) module as its temporal computational core. It has been shown that the proposed network, which is called LMUFormer, can achieve a similar performance to transformers with less number of parameters and operations."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1) The accuracy performance of the proposed work is decent and the memory/operation reduction is significant.\n\n2) The paper is also well-written and easy to read and understand."
            },
            "weaknesses": {
                "value": "1) The contribution of this work is rather limited as it relies on a previously proposed module (i.e., LMU). It is not also clear why the proposed network yields a better accuracy.\n\n2) My main concern is the performance of such a network when pre-trained. The main advantage of transformers come from its pre-training stage which allows the model to perform well on downstream tasks during fine-tuning. To learn and store those pre-training data, transformers contains numerous parameters. As such, I am not supersized by the results reported in this paper since the transformer used for the comparison was not pre-trained and consequently it is expected to see a better performance from LMUFormer. What would be the performance of LMUFormer on well-known benchmarks such as GLUE? Can LMUFormer be pre-trained?\n\n3) Lack of theoretical analysis and reasoning on the superior performance of LMUFormer."
            },
            "questions": {
                "value": "See my concerns listed as weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission586/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698859639514,
        "cdate": 1698859639514,
        "tmdate": 1699635985827,
        "mdate": 1699635985827,
        "license": "CC BY 4.0",
        "version": 2
    }
]