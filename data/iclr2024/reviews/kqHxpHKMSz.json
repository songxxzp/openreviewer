[
    {
        "id": "G6TsyarP4M",
        "forum": "kqHxpHKMSz",
        "replyto": "kqHxpHKMSz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1652/Reviewer_kBUt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1652/Reviewer_kBUt"
        ],
        "content": {
            "summary": {
                "value": "Drawing from this insight (ie., 2D detection in a single-view (camera plane) often has a stronger ability to generalize than multi-camera 3D object detection), this paper leverages the 2D view prior to better construct the consistency between cross domains. The proposed method achieves excellent results in both DG and UDA benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed method is novel, and can not bring about any inference latency cost.\n- The performance improvements of all sub-modules are significant.\n- The paper is well-organized and well-written."
            },
            "weaknesses": {
                "value": "- It is better to analyze that the 2D feature is more suitable to deal with the DA or UDA problems, ie., some statistical analysis or specific documentary evidence.\n- In Table 1, are the settings of BEVDepth and PC-BEV aligned?\n- Many papers have shown that adding a 2D detection prediction task for the MC3D-Det series detectors will significantly boost the performance. I'm worried about how much of your current rise is coming from the 2D prediction of the detector. So you should scrupulously add an ablation to show the effect of 2D prediction (even other sub-modules) for source domain only. \n- You should discuss the relative works of consistent learning and extra 2D prediction, i.e., [a,b] etc.\n[a] Probabilistic and Geometric Depth: Detecting Objects in Perspective.\n[b] Towards 3D Object Detection with 2D Supervision."
            },
            "questions": {
                "value": "Please see the Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1652/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1652/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1652/Reviewer_kBUt"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1652/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697962980500,
        "cdate": 1697962980500,
        "tmdate": 1699636093700,
        "mdate": 1699636093700,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "abrJneKIN8",
        "forum": "kqHxpHKMSz",
        "replyto": "kqHxpHKMSz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1652/Reviewer_tA1k"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1652/Reviewer_tA1k"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a method (MC3D-Det) to solve domain shift problem in multi-view 3D object detection. The proposed method aims to tackle this problem by aligning 3D detection with 2D detection results to ensure accurate detections. The framework, grounded in perspective debiasing, enables the learning of features that are resilient to changes in domain. It renders diverse view maps from bird's eye view features and corrects the perspective bias of these maps. Experimental results prove its efficiency in both Domain Generalization (DG) and Unsupervised Domain Adaptation (UDA)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "*  Experiments illustrate that the proposed approach outperforms previous approaches \uff08DG-BEV\uff09 on nuScenes dataset. \n*  The paper is well written, and comprehensive component analysis."
            },
            "weaknesses": {
                "value": "* This article corrects the model's bias through the consistency of 2D detection and 3D detection. I am quite curious whether the 2D render is necessary. Is it possible to project the 3D box into a 2D plane and supervision only applied to the 2D bounding box. There are many papers 3D consistency supervision on monocular 3D detection. From this perspective, the novelty of the model is insufficient.\n\n* How to evaluate the quality of the 2D branch of the render, at first I thought the model render the rgb image, but after carefully reading the paper, I found it was mainly about the heatmap. However, from Figure 3 (c), it does not show the quality of the rendered heatmap very well. So, how do we validate the motivation well. \n\n* The proposed method has limitations as it has not been validated on sparse-query methods. In the past year, sparse-query methods like Sparse4Dv2, SparseBEV have shown great performance and speed advantages. Without an explicit BEV, would the domain shift problem still be as significant?"
            },
            "questions": {
                "value": "*  The 3D consistency supervision of bounding boxes is need to validate the motivation. \n*  More experiments about sparse-query methods are needed to prove the effectiveness of the method."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1652/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698748583322,
        "cdate": 1698748583322,
        "tmdate": 1699636093589,
        "mdate": 1699636093589,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bRcysMlCYE",
        "forum": "kqHxpHKMSz",
        "replyto": "kqHxpHKMSz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1652/Reviewer_HYKF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1652/Reviewer_HYKF"
        ],
        "content": {
            "summary": {
                "value": "The manuscript introduces a method designed to enhance feature learning in a way that is robust to changes in domain, leveraging an approach centered around perspective debiasing. It substantiates its efficacy through experimental findings in the contexts of Domain Generalization and Unsupervised Domain Adaptation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Firstly, the proposed framework is innovative and can be smoothly incorporated into existing 3D detection techniques. \nSecondly, the breadth of the experiments conducted is comprehensive, effectively illustrating the framework's robustness and effectiveness."
            },
            "weaknesses": {
                "value": "Figure 1 presents some ambiguity. It is not immediately clear how this figure is intended to convey the robustness of the perspective view to domain shifts."
            },
            "questions": {
                "value": "An inquiry for further clarification: How would methods that do not employ perspective view transformations, such as DETR3D, fare in performance across varying domains?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1652/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1652/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1652/Reviewer_HYKF"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1652/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699459919175,
        "cdate": 1699459919175,
        "tmdate": 1699636093531,
        "mdate": 1699636093531,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "euK9L3kV9g",
        "forum": "kqHxpHKMSz",
        "replyto": "kqHxpHKMSz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1652/Reviewer_seTP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1652/Reviewer_seTP"
        ],
        "content": {
            "summary": {
                "value": "The paper considers the task of 3D object detection from multiple cameras. Existing methods exhibit poor generalization due to overfitting to specific viewpoints and environments. This paper proposes to re-render heatmaps between 2D views using an implicit 3D representation to learn features that are independent of the perspective and context. The paper demonstrates strong quantitative results on the task of domain generalization as well as the newly proposed task of unsupervised domain adaptation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The task considered is highly important. 3D object detection should generalize across different camera setups and environments\n* Paper demonstrates strong quantitative results. Detections look qualitatively more accurate than existing baselines."
            },
            "weaknesses": {
                "value": "* The paper has clarity issues that make it difficult to understand. It is not clear what the perspective bias comes from in the first place, and how re-rendering different viewpoints addresses the issues of poor generalization. It is also not clear how the perspective debiasing works\n* Figure 3, the only figure in the paper that explains the perspective debiasing, is uninformative. It\u2019s not clear how the re-rendering has improved the heatmaps."
            },
            "questions": {
                "value": "* How are the features warped between images?\n* Which components of the model need to be retrained, and which ones are lifted off-the-shelf from existing BEV methods? Are they finetuned?\n* How does the network learn the depth and heights given new target domains without 3D data?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1652/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1652/Reviewer_seTP",
                    "ICLR.cc/2024/Conference/Submission1652/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1652/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699501722800,
        "cdate": 1699501722800,
        "tmdate": 1700595778694,
        "mdate": 1700595778694,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Zu801fBoiK",
        "forum": "kqHxpHKMSz",
        "replyto": "kqHxpHKMSz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1652/Reviewer_S4As"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1652/Reviewer_S4As"
        ],
        "content": {
            "summary": {
                "value": "- The paper focuses on the problem of multi-view 3D object detection for autonomous driving conditions.\n- The main focus is when there is a domain gap between training (source) and testing (target_ conditions.\n- The authors evaluate two scenarios: a) domain generalization, i.e., no target images available, and b) unsupervised domain adaptation, i.e., target images available (no labels, of course).\n- Key Idea: Sec. 3.2 shows that the existing methods overfit to camera intrinsics and extrinsics of the train images - perspective bias of the model. The proposed method MC3D-Det modifies the BEVDepth pipeline using perspective debiasing to fix this.\n- What is perspective debiasing? In the source domain, perturb the existing camera views and render the \"view maps\" from the novel camera views. These random camera positions and angles help to avoid overfitting. \n- Decreasing the domain gap? If you have unlabelled target images, MC3D-Det uses an offshelf single view 2D detector to predict 3D bboxes and use it to rectify the BEV features using consistency loss.\n- Evaluation Datasets: nuScenes (real), Lyft (real), DeepAccident (synthetic). Metrics are standard 3D bbox detection metrics.\n- Baselines: BEVDepth (the base method used by MC3D-Det), DG-BEV. Other domain adapation baselines like Pseudo labeling, Oracle etc.\n- Table 1. shows that the proposed method improved mAP on the target domain by about 2-4% over the baseline."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The main idea is presented clearly, and the proposed approach is intuitive and easy to understand. The technical details are all laid out in Sec. 4 and the supplementary. The technical contributions made by MC3D-Det are novel.\n- The perspective bias of the model, Eq. 2, is derived theoretically in Sec. 3 and supplementary using limited assumptions and first principles. This stands at the core of the motivation of the proposed approach and is, therefore, an important step.\n- Substantial evaluations are done on multiple datasets (Lyft, nuScenes, DeepAccident), especially cross-domain evaluations, including pseudo labeling and oracle training. The method is compared against multiple relevant baselines.\n- The ablative studies in Table 2 and Table 3 are informative and highlight the importance of source and target domain debiasing and the plug-and-play capabilities of the MC3D-Det with existing methods.\n- Main results show gains of the proposed method for domain generalization and unsupervised domain adaptation."
            },
            "weaknesses": {
                "value": "- Missing details on BEVDepth architecture used on Table. 1: The performance of the baseline BEVDepth reported on the source domain nuScenes (I am assuming this is val set, the argument also holds for the test set) in Table. 1 of 32.6 mAP is significantly less than the published results of BEVDepth (refer Table. 7) of 41.8 mAP, R101-DCN architecture. Bigger backbones with the scale of data like nuScenes exhibit better generalization; it is worth while investigating if the performance gain of the proposed MC3D-Det also holds when using BEVDepth at its full capacity. Especially, since BEVDepth is the base method for MC3D-Det.\n\nQuick minor comment on similar lines on DG-BEV. The reproduced results for DG-BEV in Table. 1 are much worse than the published results of DG-BEV (Table. 1). For Lyft -> nuScenes, for domain generalization, DG-BEV (published) == MC3D-Det > DG-BEV (reproduced). However, the code for DG-BEV is not available, so I would side with the authors here.\n\n- Computational overhead compared to baselines: Proposed method adds an overhead to the baseline method. It would be helpful to quantify the additional parameters and GLOPs used compared to the methods evaluated in Table. 3. \n\n- The focus on the car category: Please mention the categories used for evaluation in Table. 1. Correct me if I am wrong, but all the evaluations only consider the vehicle and car category (Lyft <-> nuScenes) - a rigid object with easy-to-learn 3D geometry prior, allowing for a consistent rendering with a perturbed camera. Do the results also hold for categories like the pedestrian, cyclists or less represented rigid classes like truck, construction vehicle, bus, and trailer? Using Waymo -> nuScenes protocol here would be more informative.\n\n- Camera perturbations used: The qualitative results do not show the rendered views from the perturbed cameras. Since perspective overfitting is an issue, we should augment the view to avoid biasing, but still, we want to stay within the camera extrinsics distributions of the target domain. How is this balance achieved? What is the magnitude of translation and rotation perturbations used for perspective debiasing in MC3D-Det? How does the performance change when the perturbations increase from the anchor positions? \n\nMinor:\n- Sec. 1. However, without taining data -> However, without training data \n- Please increase text font in Figure 2. Mention the intermediate feature in the figure using notation established.\n- Sec 4.1, C, X, Y, Z are not defined.\n- Eq 6: D_vitural -> D_virtual\n- Sec. 4.3. there is not 3D labeled -> there are no 3D labels\n- Sec 5.1. It demonstrate that -> It demonstrates that\n- Sec 5.3. 2D Detetctor -> 2D Detector\n- Table 2, caption: 2D Detetctor -> 2D Detector\n- Fig. 3: The bounding boxes and the image is extremely small. The image should be visible without zooming in.\n- Sec. 6: or 2D pre-trained 2D detectors -> or pre-trained 2D detectors"
            },
            "questions": {
                "value": "As above.\n- Fair baseline comparison to BEVDepth.\n- Parameter overhead.\n- Generalization to other categories.\n- Information on Camera perturbations."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1652/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699551880603,
        "cdate": 1699551880603,
        "tmdate": 1699636093384,
        "mdate": 1699636093384,
        "license": "CC BY 4.0",
        "version": 2
    }
]