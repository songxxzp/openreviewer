[
    {
        "id": "ejHpKRy1iz",
        "forum": "vmlboYfvpf",
        "replyto": "vmlboYfvpf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5933/Reviewer_LxAL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5933/Reviewer_LxAL"
        ],
        "content": {
            "summary": {
                "value": "This paper tackles the domain of LLM calibration. The authors propose a framework that improves the performance of LLMs in an unsupervised manner. Furthermore, they provide both theoretical analysis and experimental validation of the proposed approach."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed method is unsupervised in nature which makes it more usable when compared to supervised methods of calibration. \n2. The authors provide theoretical analysis of the proposed method, which strengthens their case.\n3. The experimental results are promising."
            },
            "weaknesses": {
                "value": "1. The presentation of this paper can be improved a lot. There are many formatting issues, missing text and grammatical errors. The flow of the paper can also be improved significantly."
            },
            "questions": {
                "value": "1. In the discussion section the authors mention that the since the proposed methods do not have specific requirements for the loss functions of components and the datasets used for training, it is less intrusive. Can you elaborate a bit more on this? How are you quantifying intrusiveness here?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5933/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698712627535,
        "cdate": 1698712627535,
        "tmdate": 1699636631686,
        "mdate": 1699636631686,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cjAGqlFeVX",
        "forum": "vmlboYfvpf",
        "replyto": "vmlboYfvpf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5933/Reviewer_F18K"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5933/Reviewer_F18K"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces Generative Adversarial Learning (GAL), a framework for calibrating question answering (QA) systems without traditional supervision, such as using a held out validation set as part of the calibration process. GAL involves a multi-agent game among a generator (the original pretrained QA model), an inspector, and a processor. The inspector evaluates the generator and generates additional questions to ask the generator. Finally, the processor updates the weights of the generator."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The technique appears to be novel. \n- The authors perform a reasonably extensive evaluation and include several performance metrics. \n- The IGS score appears to be correlated with accuracy, although I would strongly encourage the authors to make this clear much earlier on in the paper (unless I missed this, which is possible)."
            },
            "weaknesses": {
                "value": "- The paper lacks crucial details about how the algorithm operates. In particular, it is not clear to me how the processor model works. \n- Claiming that convolutional kernels are interpretable seems suspect (see 4.3). What is the end objective of interpreting this techniques? Does it help in some concrete way to solve a downstream QA problem, where other techniques fail? \n- The authors should include an empirical evaluation of the computational costs incurred by the method. In light of this being missing from the paper, the technique does not appear to be lightweight and non-invasive as the authors claim."
            },
            "questions": {
                "value": "- The authors refer to the method as non-intrusive -- what exactly does this mean, and how does this differ from existing unsupervised calibration techniques? \n- It is not clear to me how the processor model, which is parameterized as a convolutional kernal, works as an optimizer or what the rationale is behind this design choice. \n- It is not clear what the confidence matrix refers to. Is this a standard object? Please elaborate on this. \n- With regard to the generator model: \"It does not need any modifications to its structure or loss function, which is still cross-entropy.\" I'm not following why the generator still uses the cross entropy loss. Is it not the case that its weights are updated using the processor model? Where do gradients come into play? I assume that supervision signal comes from the inspector?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5933/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698717166364,
        "cdate": 1698717166364,
        "tmdate": 1699636631581,
        "mdate": 1699636631581,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LGZId4iK0d",
        "forum": "vmlboYfvpf",
        "replyto": "vmlboYfvpf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5933/Reviewer_9Xja"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5933/Reviewer_9Xja"
        ],
        "content": {
            "summary": {
                "value": "Authors study the problem of adversarial learning in the unsupervised setting which is a relevant problem. They propose an inter-connected framework having three components a) generator, b) processor and c) inspector modules.  Finally, a Proximal Policy Optimization based RL loss objective function is proposed. Authors conduct benchmarking w.r.t recently proposed techniques and ablation studies. The generator is a standard pre-trained LLM, the processor is a weighted convolution model and inspector is the recursive bayesian approximator. Although, authors have claims on interpretability in this paper, strong empirical results supporting the same seem missing."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1) This paper to the best of my knowledge does present a framework having a generator, processor and inspector modules for adversarial learning. While the individual components don't seem novel, their application together does come across as something useful especially in the unsupervised paradigm.\n\n2) The individual components are technically sound. Authors also benchmark w.r.t recent techniques"
            },
            "weaknesses": {
                "value": "1) Theoretical analysis in main paper seems under developed and not sure how its useful.\n2) Paper touches upon several aspects such as interpretability and adversarial learning without exploring them completely. The paper does come across as being very broad in its claims lacking sufficient evidence supporting all claims made.\n3) Ablation studies only a pointer is provided to a table without sharing key insights in the section ( I do see the table caption which by itself does not suffice). I believe ablations are very important for such an inter connected framework with multiple components to validate usefulness of individual components and parameter sensitivity. Sizes of generator models considered, etc are also good variations to explore.\n4) Another key weakness I feel is that the writing style does not make the technical contributions very clear, for example for an adversarial learning paper a threat model section is definitely needed explaining the goals, knowledge and tools for both defender and adversary. Paper definitely can go through a thorough editing to surface these contributions better. \n\nMinor comment\nPaper has several typos and needs proper editing before submission."
            },
            "questions": {
                "value": "Most of my questions are mentioned in the weakness section above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5933/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699458300053,
        "cdate": 1699458300053,
        "tmdate": 1699636631491,
        "mdate": 1699636631491,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CJC0AHykbc",
        "forum": "vmlboYfvpf",
        "replyto": "vmlboYfvpf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5933/Reviewer_fsZm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5933/Reviewer_fsZm"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel unsupervised learning paradigm called \"Generalized Adversarial Learning\"(GAL) to improve the calibration of large-scale question-answering (QA) models. GAL views adversarial learning as a multi-agent game process, and consists of three components: the generator, the processor, and the inspector. The processor is a module that adjusts the model weights using a convolutional kernel prediction method, while the inspector updates a confidence matrix and a guidance vector. The inspector and the processor cooperate to update the generator's (i.e., the pre-trained LLM) weights in an unsupervised manner."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The experiments are conducted on several well-known and widely-used LLMs."
            },
            "weaknesses": {
                "value": "1. The motivation and design principle of the proposed method are not well illustrated.\n2. The figures of the paper are not clearly presented.\n3. The writings could be greatly improved."
            },
            "questions": {
                "value": "1. The design principle of the GAL model is not clear. The overall model pipeline of GAL is not clearly described. The model structure is not clearly presented. Why weight convolution model is used for processor?\n2. The figures of the paper are not clearly presented. Figure 1 does not clearly presented the model structure. Figure 2 does not show the technical design and contribution of the proposed module. Figure 3 is poorly presented.\n3. There are many grammar mistakes and typos. Great efforts should made on improving the writings."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5933/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5933/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5933/Reviewer_fsZm"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5933/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699528011520,
        "cdate": 1699528011520,
        "tmdate": 1699636631385,
        "mdate": 1699636631385,
        "license": "CC BY 4.0",
        "version": 2
    }
]