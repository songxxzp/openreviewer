[
    {
        "id": "MzDh7uJCEU",
        "forum": "1FWDEIGm33",
        "replyto": "1FWDEIGm33",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9417/Reviewer_Fkgw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9417/Reviewer_Fkgw"
        ],
        "content": {
            "summary": {
                "value": "In general discourse surrounding the rise of LLMs, it is common to ascribe individual characteristics to LLMs. This paper challenges this tendency suggesting that it is more evident to view LLMs as a superposition of perspectives instead of as individuals. The paper provides empirical demonstrations to make this point. In particular, the experiments included show that LLM responses are context dependent in ways that differ from humans. The paper calls into question the use of psychological questionnaires to examine LLMs. The main contribution is the introduction of \"perspective controllability\" and an empirical demonstration to probe whether LLMs are robust to perspective shift effects and how different LLMs compare in terms of their perspective controllability."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(1) The experiments seem thorough and the paper states that reproducibility and transparency has been a priority."
            },
            "weaknesses": {
                "value": "(1) The framing of the paper needs significant improvement. The argument seems to go something like this: LLMs tend to be context-dependent. Humans tend to be stable across contexts. Therefore, LLMs should not be assumed to be human-like. Probing an LLM with questions derived from a field that assumes a human subject is flawed. Making general conclusions from results based on these questions is also flawed. LLM as a superposition not an individual is proposed is a new metaphor. This new metaphor motivated the study of perspective change in LLMs, which is the focus of this paper.\n\nNotice there are multiple jumps in this line of argument. First, the fact that LLMs are context-dependent needs to be reconnected to the point about LLMs being seen as individuals. You do not need to provide evidence that LLMs are not human-like to make this point. Second, the paper briefly argues that probing an LLM with questions derived from psychology is problematic but this point is not properly fleshed out or supported directly by results. Third, the point that general scientific conclusions are therefore problematic has not been properly made. Fourth, the reference to quantum mechanics is an interesting inspiration for said metaphor but is not a sound analogy in that language models do not operate in the quantum regime. Further, this inspiration is not necessary to make the argument laid out in this paper. Fifth, the main final point which is that studying perspective change in LLMs to study induction techniques is disconnected from the rest of these points and could stand as an interesting topic in itself.\n\n(2) Conclusions are overstated. The paper states \"we will see that discarding the old metaphor may question the interpretation of recent studies aiming at characterizing the values, personality traits, social skills or moral values of LLMs using tools developed to measure attributes of human psychology\". The current status of the argument has not led to this conclusion directly. The paper needs to reconnect and build out a cohesive careful argument in order to support this claim.\n\n(3) Exposition could be greatly improved throughout for clarity and precision. For instance, the related works section is written as part of the argument that recent work uses \"LLM as an individual\" metaphor, which should be discussed as such. The paper states \"There has been a lot of research studying large language models using tools from psychology...\" before the paper has fully developed the argument for what it means to view \"LLM as an individual\". It is more standard to use the related works section to contextualize this work in reference to existing literature not necessarily to support the content of your argument. Further, the paper states \"All these works aim to make general conclusions about LLMs\nbehavior, personality, or abilities, but they do not explore how personality traits expressed through\nbehaviour can change in unexpected ways over diverse unrelated contexts.\" which seems to say that the difference is in the focus on changes due to unrelated contexts. It would have been clearer to simply state that this work is related to other studies of personality traits but diverges in its focus on context-based shifts in performance. But for some reason, the section is written in a way that requires the reader to parse this out. \"At first glance, these might seem like examples of the unexpected perspective\nshift effect, however these effects are both common in humans, and their effect on the perspective\nchange is intuitive.\" This sentence is unclear and way too dense. And again, not exactly positioned properly in related work section if the function is to be part of the overall argument of the paper. The section continues with \"The second part of our paper studies how models\u2019 values and personality expression can be controlled, i.e., the expected perspective shifts due to context changes.\" This marks a shift in tone where the section is now describing the paper instead of the related work. Again, a sign of expository improvement needed.\n\n(4) The main focus is not clearly defined. The first sentence in the methods section states, \"This paper aims at uncovering the existence of unexpected perspective shift effects i.e. how context can impact the values and personality traits expressed by LLMs in unwanted, unexpected ways\". This is the definition provided. It is unfortunately unclear."
            },
            "questions": {
                "value": "(1) What is the technical definition of \"unexpected perspective shift effects\"?\n\n(2) How do you distinguish between expected and unexpected? Expected by whom?\n\n(3) What is the technical definition of \"perspective controlability\"?\n\n(4) What is the theoretical basis for equation (1) ?\n\n(5) How does this compare to other measures of predictive inconsistency? Why is \"context\" so specifically interesting in this paper?\n\n(6) How do you define induced perspective? Can you offer theoretical analysis to support your measure?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9417/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698688639435,
        "cdate": 1698688639435,
        "tmdate": 1699637186829,
        "mdate": 1699637186829,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pfCqf1kmPM",
        "forum": "1FWDEIGm33",
        "replyto": "1FWDEIGm33",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9417/Reviewer_7GoP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9417/Reviewer_7GoP"
        ],
        "content": {
            "summary": {
                "value": "The authors make the case that LLMs show a sort of superposition of cultural perspectives, since their outputs, as measured by standard tests widely change according to the input. The authors consider value and personality tests in order to measure different \"cultural perspectives\" in LLMs. One of the goals of the authors is to measure the consistency of the outputs of the LLMs in an experimentally sound way. The evaluation is carried on several LLMs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The paper is indeed timely, there is a lot of interesting ideas to explore around LLMs and this is indeed a good example of an interesting paper in the area."
            },
            "weaknesses": {
                "value": "- The study that the authors carried out is indeed interesting, but unfortunately, it seems to me that the actual assessment of the results is somehow \"hyped\": at the end these are probabilistic models highly dependent on the prompt and it is somehow expected that they exhibit a variety of personal values, cultural values, personality traits. The authors highlight the fact they observe \"unexpected perspective shift effects\". However, in my opinion, it would be more surprising to see consistency. \n- It is very difficult to understand which inputs led to a change of perspectives. In my opinion, this is a key problem of the paper since small variations might have a significant effects on the outputs. Also, for this reason, it is very difficult to judge the actual consistency of the outputs of the LLMs in the experiments carried out by the authors.\n- Superposition is a wrong term in my opinion given the probabilistic nature of LLMs. In fact, even the same input might lead to different outputs.\n- The term controllability appears to me inappropriate, since the authors are not measuring actual \"controllability\" of the outputs in my opinion.\n- The selection and the analysis of the application of the induction methods are not completely clear, especially with respect to the underlying research hypotheses at the basis of the study design."
            },
            "questions": {
                "value": "- What is the exact definition of cultural perspective you consider in the paper? What is the relation between cultural perspective and personal values?\n- Which kind of inputs did you use for measuring the change of perspectives? (the supplementary material does not consider sufficient material for reproducibility in my opinion).\n- It seems that the authors report the fact that LLMs are not \"coherent\" as the key finding of their paper. Indeed, it is always good to see measurement studies, but the reviewer wonders if this can be considered as something unexpected. After all, the models are trained on a variety of sources. Were the authors expecting a different result? \n- Do you have any data about the influence of the training datasets on the experimental results that you showed in this paper?\n- Can you discuss Formula (1) in details? How do you analyze the outputs of the LLMs? How do you calculate the mean in this formula?\n- It would be good to know the reasoning beyond the selection of the term \"controllability\". This appears an unusual choice for the phenomena you study in this paper.\n- Can you please discuss the effects of the induction methods in relation to their effects on the outputs of the LLMs?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9417/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698878911095,
        "cdate": 1698878911095,
        "tmdate": 1699637186722,
        "mdate": 1699637186722,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PMFn1oXv5c",
        "forum": "1FWDEIGm33",
        "replyto": "1FWDEIGm33",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9417/Reviewer_P79N"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9417/Reviewer_P79N"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the ability of language models to answer psychological questionnaires. Past research has used these tests designed for humans to try to probe LLMs. The study tests model robustness in answering questionnaires under different contexts or conditions (e.g. writing code, prefixing with a random wikipedia page) which are unrelated to the question in the questionnaire and observe there are significant changes in responses. Further, the paper introduces the notion of perspective controllability and aims to test which models can be guided to answer questions in a certain way.\n\nThe conclusions are that LLMs are unreliable in answering trait questions, with unrelated perturbations leading to different results and that most models are not controllable, albeit some models exhibit some degree of controllability."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Sound methods for statistical analysis of results.\n\nCreative approaches to test robustness of models.\n\nAdequately challenges the assumption of 'LLMs as individuals' for measuring traits."
            },
            "weaknesses": {
                "value": "Primarily, I think the model needs to have more robust results to understand better what and which types of models behave in different ways. Namely:\n\nThe first experiment (Section 4.1) is only performed using a single model (ChatGPT).\n\nThe models can be better selected for experimentation to facilitate understanding the machanisms that lead to consistent or inconsistent results in Table 1. I think the key comparison directions could be along these axes: base model, models from the same series and different size, base vs. chat. vs. instruct vs. RLHF.\n\nI think the experiments lead into another metaphor than 'superposition of cultural perspectives'. For a perspective to hold, it would have to be consistent across inputs i.e. to produce consistent results when conditioned in the same way. The results show that the conditioning changes results in unexpected and inconsistent ways. Hence, my conclusion from these experiments would be that LLMs lack awareness or knowledge of a perspective.\n\nIn general, I consider using questionnaires about traits is a bit tricky or ill posed in this context. The questionnaires for traits are usually built as a proxy for behaviors e.g. 'make friends easily' loads on the intra/extraversion scale; so it would be perhaps more suitable (and robust?) to have these framed as test on a behavior e.g. at a party where you don't know anyone and some one is sitting also alone, do you approach to strike up a conversation with them? (yes - more likely extravert, no - more likely intravert).\n\nAnother aspect worth mentioning is that in addition to the test-retest validity which is brought up in the paper as stability over different ways of providing context before asking the questionnaire question, one could also measure the variance inside each questionnaire, as multiple questions load on the same factor and the variance across these should also be low by design (i.e. people would respond to questions about extraversion similarly)."
            },
            "questions": {
                "value": "NA"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9417/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9417/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9417/Reviewer_P79N"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9417/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698994512911,
        "cdate": 1698994512911,
        "tmdate": 1699637186602,
        "mdate": 1699637186602,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wXp9GL22N6",
        "forum": "1FWDEIGm33",
        "replyto": "1FWDEIGm33",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9417/Reviewer_B9Pb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9417/Reviewer_B9Pb"
        ],
        "content": {
            "summary": {
                "value": "This paper challenges the view of large language models (LLMs) as individuals and proposes a new metaphor: \"LLMs as superpositions of perspectives\". The authors conducted experiments that demonstrate unexpected perspective shifts in personal values, cultural values, and personality traits. LLMs changed their responses depending on contexts, and even context variations not related to the target topics led to significant changes in the values and personality traits they expressed. The authors also compared four different perspective induction methods (prompts) to assess whether they could control the models' perspectives (perspective controllability)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper studies large language models (LLMs), which is a hot topic in the current society.\n- The paper challenges some existing views on LLMs trying to understand them better, giving some warnings of the potential danger of the existing views.\n- The paper explores if the \"perspectives\" could be controlled, by suggesting four induction methods."
            },
            "weaknesses": {
                "value": "I struggled to understand the importance of this problem, even after reading the paper. It is unclear what the implications and potential applications of this work are. The paper confirms that LLMs do not give consistent responses, and that LLMs are not like humans, as shown in Experiments and discussed in Discussion. However, it is not clear what the paper suggests (besides proposing a new metaphor) and why this is critical."
            },
            "questions": {
                "value": "1. Could you elaborate on the definition of a perspective in this paper? \"A perspective is conceptualized\nas a context from which a model is required to simulate a behavior\". \n2. it is not clear what the paper suggests (besides proposing a new metaphor) and why this is critical."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9417/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698996162716,
        "cdate": 1698996162716,
        "tmdate": 1699637186492,
        "mdate": 1699637186492,
        "license": "CC BY 4.0",
        "version": 2
    }
]