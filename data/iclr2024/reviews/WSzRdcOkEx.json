[
    {
        "id": "kmJ138abEY",
        "forum": "WSzRdcOkEx",
        "replyto": "WSzRdcOkEx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3797/Reviewer_fKuC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3797/Reviewer_fKuC"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a data-independent metric called GREAT Score for evaluating adversarial robustness. To be specific, GREAT Score is calculated as the mean of the gaps in the likelihood of model prediction between the ground-truth class and the most likely class other than the ground-truth class achieved by the samples generated from a generative model. The empirical results seem to validate the effectiveness of GREAT Score in evaluating robustness."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The GREAT Score is data-independent, which is applicable to privacy-sensitive black-box evaluation. \n\n2. The GREAT Score is less sensitive to data points compared to AutoAttack."
            },
            "weaknesses": {
                "value": "1. The definition of robustness in Eq. (1) and Eq.(4) seems to be confusing and possibly wrong. In Eq. (1), $\\Delta_{\\min}$ is defined as the minimal perturbation of a sample-label pair causing the change of the top-1 class prediction. Then, I understand $g(x)$ is a lower bound of $\\Delta_{\\min}$. However, in Eq. (3), $g(x)$ is defined as the gap between two probabilities. Therefore, I am confused about how the gap between two probabilities measures the minimal perturbation.\n\n2. The rank of the model in terms of adversarial robustness is unclear and confusing. As shown in Table 1, the rank of a model based on GREAT Score could be different from the rank based on AutoAttack.  Besides, the rank of a model based on GREAT Score could also be different from the rank based on the calibrated GREAT Score.  Therefore, the research would be confused about what is the genuine rank of a model in terms of its robustness.\n\n3. How can you guarantee the gap between the two probabilities achieved by the generated data is achieved in the worst case? As far as I can see, the authors directly used the generated samples from the generative models, which are supposed to be natural samples instead of adversarial samples.  \n\n4. The table and figure should be resized to be larger, which makes it easier to read."
            },
            "questions": {
                "value": "Please refer to my comments in \u201cWeaknesses\u201d."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3797/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698399063571,
        "cdate": 1698399063571,
        "tmdate": 1699636337102,
        "mdate": 1699636337102,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tfw36rloRR",
        "forum": "WSzRdcOkEx",
        "replyto": "WSzRdcOkEx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3797/Reviewer_npik"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3797/Reviewer_npik"
        ],
        "content": {
            "summary": {
                "value": "To solve the problems of lack of proper global adversarial robustness evaluation, limitation to white-box settings and computational inefficiency, this paper proposes GREAT Score for global robustness evaluation of adversarial perturbation using generative models. The authors have introduced and compared the proposed method, but the novelty of this paper is limited. Here are some of my suggestions for improving the quality of this paper."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is well writen, with better typesetting, the research content is also of certain practical value."
            },
            "weaknesses": {
                "value": "(1) There are obvious spelling and coincidence errors in the paper.\n(2) In the background and related works section, it is suggested to simplify the introduction of generative models and add the description of global robustness evaluation rather than Appendix 6.3.\n(3)  It is recommended to draw flow charts of the algorithm proposed by the author and other algorithms to highlight the innovation of the algorithm.\n(4)  It is recommended to give a more detailed time complexity calculation.\n(5)  In the experimental results section, the layout of the experimental results graphs and tables is confusing, and it is recommended to rearrange them.\n(6)  Appendix chapters need to be arranged according to the order in which they appear in the main text.\n(7)  Some of the examples in the article need to be replaced to adhere to the previous arguments."
            },
            "questions": {
                "value": "(1)  For other competitive methods, it would be better to use all the data in the dataset to measure robustness rather than the strategy proposed by the author?\n(2)  Does it still make sense to use a lower bound if a true global robustness evaluation can be obtained with high computational efficiency ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3797/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698805139136,
        "cdate": 1698805139136,
        "tmdate": 1699636336997,
        "mdate": 1699636336997,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lgqQsnjDKp",
        "forum": "WSzRdcOkEx",
        "replyto": "WSzRdcOkEx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3797/Reviewer_UUBS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3797/Reviewer_UUBS"
        ],
        "content": {
            "summary": {
                "value": "The authors propose GREAT score, a privacy measure that may be used to quickly evaluate the robustness of black-box models.  Towards this end, GREAT score utilizes generative models (e.g., GANs and DDPMs) to generate (potentially label-conditioned) samples which are fed into the black-box model of interest.  In this manner, the black-models outputs (over the input classes) are gathered and used in the calculation of GREAT score.  The GREAT score itself is interesting; it leverages the overlapping input-noise characteristics utilized by both GANs and DDPMs (i.e., zero-mean isotropic Gaussian noise) to calculate a certified lower-bound on the true global robustness (wrt to the underlying generative model used).  GREAT score may thus be used either as a standalone auditing strategy for black-box models, or in conjunction with computationally intensive benchmarks which directly test robustness via adversarial attacks (e.g., RobustBench or specific attacks, such as AutoAttack, FGSM, PGD, etc.).  One limitation of the presented work is GREAT score's theoretical guarantees only apply to L2-based attacks, the effects of which on other widely-used distance metrics (L0 and L_{\\inf}) requires understanding through further follow-up work."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The overall writing and derivation of the GREAT score itself are easy to follow and intuitive.  Furthermore, the authors do a good job of extensively testing GREAT score, empirically contrasting its results to RobustBench, and demonstrating its use across a suite of face recognition APIs.  While the theoretical results are interesting, exhaustive experiments are impressive and convincing of the utility of this score for measuring robustness of black-box models."
            },
            "weaknesses": {
                "value": "Given the extensive research concerning robustness wrt L0/L2/L_{\\inf} based adversarial attacks, GREAT score's limitation to L2-based attacks is a major weakness.   As only L2-based attacks are considered in the paper, either GREAT score must be extended to cover L0/L_{\\inf} based attacks (which, the authors note, is unlikely under the current derivation) or follow up work must explore how effective GREAT score is at measuring robustness for L0 and L_{\\inf} based attacks.\n\nThe papers description of some relevant work is either terse or lacking.  For instance:\n- \"using the Rebuffi_extra model (Rebuffi et al., 2021)\" <- please give a short inline description of this model\n\n- \"successful adversarial perturbations (an upper bound on the minimal perturbation of each sample) returned by any norm-minimization adversarial attack method such as the CW attack (Carlini & Wagner, 2017)\" <- Please give a brief description of the CW attack\n\n- In Table 1, please add a column showing which generative model is being evaluated.\n\nFurthermore, GREAT score's role as a lower bound for CW distortion is noted throughout the paper.  However, this is not entirely clear given the current version of the paper.  Please explain that the CW (L2) attack solves an optimization objective which results in low L2 distortion.  Also, it is important to note why GREAT score serves as a lower bound for the CW attack distortion; Equation 3 is equivalent to the non-L2 term of the CW (L2) attack, with c = sqrt{ \\pi / 2} (in practice, CW determines c via a grid search).  Thus, assuming c >= sqrt{\\pi /2), GREAT score trivially lower bounds the CW L2 attack."
            },
            "questions": {
                "value": "- \"Moreover, as a byproduct of using generative models, our adversarial robustness evaluation procedure is executed with only synthetically generated data instead of real data, which is particularly appealing to privacy-aware robustness assessment schemes, e.g., remote robustness evaluation or auditing by a third party with restricted access to data and model.\" <- Please note that this is not a panacea, i.e., see:  \n-Carlini, Nicolas, et al. \"Extracting training data from diffusion models.\" 32nd USENIX Security Symposium (USENIX Security 23). 2023.\n-Duan, Jinhao, et al. \"Are diffusion models vulnerable to membership inference attacks?.\" arXiv preprint arXiv:2302.01316 (2023).\n\n- \"In Figure 2, we compare the cumulative robust accuracy (RA)\" <- RA is used before its definition on page 7; please define on the first use of acronym RA\n\n- \"using the Rebuffi_extra model (Rebuffi et al., 2021)\" <- please give a short inline description of this model\n\n- \"DMs reverse the forward process and implement a sampling process from Gaussian noises to reconstruct the true\nsamples\" <- Please mention that this is done by solving a stochastic differential equation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3797/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698822214776,
        "cdate": 1698822214776,
        "tmdate": 1699636336915,
        "mdate": 1699636336915,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3r3jTOkWQv",
        "forum": "WSzRdcOkEx",
        "replyto": "WSzRdcOkEx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3797/Reviewer_kzkL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3797/Reviewer_kzkL"
        ],
        "content": {
            "summary": {
                "value": "The paper is focused on an important problem which is \"global robustness\" that aims to evaluate the robustness of classifiers with respect to the underlying, unknown data distribution. This work can be classified in the family of robustness evaluation algorithms. The authors propose GREAT Score for evaluating the global robustness of classifiers by leveraging generative models. Specifically, the GREAT Score utilizes a generative model to approximate the data distribution and then calculates a certified lower bound on the minimal adversarial perturbation level, averaged over the sampling distribution of the generative model. This allows estimating the distribution-wide robustness without needing the true data distribution."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ This problem setting is important. The overwhelming majority of work in the space of robustness evaluation has considered aggregated local robustness statistics over test samples. However, one might argue that these test samples might be biased and not able to represent the true data distribution. The global robustness of models w.r.t. the entire data space is still under exploring. In this sense, the problem the authors seek to address is relevant and will likely continue to grow in relevance.\n+ The paper is very well-written and organised. The main insights are well explained, and it is easy to follow. The use of generative models is well-motivated."
            },
            "weaknesses": {
                "value": "- The authors utilise the generative models to estimate models' global robustness and try to provide statistical guarantees which is claimed as one of the main theorems. My biggest concern is from this point. Since generative models are not perfect, there exists a notable gap between the generative and true data distribution. We are still worrying about the error bound induced by the difference between the underlying data distribution and generative data distribution.\u00a0 The paper would greatly benefit from including probabilistic guarantees or error bounds on the estimated global robustness concerning the true data distribution, as opposed to solely with respect to the generative data distribution. Such bounds would significantly enhance the appeal and credibility of the results.\n- Table 1 illustrates the comparison between (Calibrated) GREAT Score v.s. minimal distortion found by CW attack on CIFAR-10. However, the table does not explicitly show the advantages of GREAT score compared to other metrics. For example, the CW distortion score between Rebuffi_extra and Gowal_extra is pretty large, but the GREAT score only differs 0.003. \n- Is there any other options besides generative models or GANs, which can approximate the unknown data distribution?\n- Figure 1 and 2 should be larger. Similarly, it is hard to get a close look at Table 1 and 2. The figures and tables on Page 7 and 8 are not well-constructed."
            },
            "questions": {
                "value": "Pls see the Section Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3797/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699080620680,
        "cdate": 1699080620680,
        "tmdate": 1699636336810,
        "mdate": 1699636336810,
        "license": "CC BY 4.0",
        "version": 2
    }
]