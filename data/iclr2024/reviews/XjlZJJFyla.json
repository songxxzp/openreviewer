[
    {
        "id": "0SCJVv5xFg",
        "forum": "XjlZJJFyla",
        "replyto": "XjlZJJFyla",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4535/Reviewer_zJm8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4535/Reviewer_zJm8"
        ],
        "content": {
            "summary": {
                "value": "he paper presents a novel approach to prompt tuning in Vision and Language Models (VLM), where a distribution over prompts is learned in a per-class fashion. For a target class, the prompt embeddings are drawn from a latent distribution parameterized by a small learnable MLP, similar to (Derakhshani et al., 2022). However, contrary to (Derakhshani et al., 2022), the prompts are learned for each image class, and they are computed from patch embeddings rather than from the holistic image. In addition to the extended distribution over the prompts, the paper proposes an optimal-transport loss to align the image and text features. A broad set of experiments are conducted to showcase the benefits of the proposed approach w.r.t. CoOp and CoCoOp."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is technically sound and motivated. On the one side, extending the pool of prompts is appealing to improve generalization to new classes. On the other hand, the idea of using optimal transport to align the image and text probabilities sounds novel to me, which brings a new optimization technique to the domain of vision and language pre-training with good results.\n\nThe experiments follow the standard protocols providing superior performance to CoOp and CoCoOp, showcasing the importance of having a proper set of prompts from which one can sample in task-specific manner.\n\nThe paper is well documented, and while the writing can be improved (see below), the narrative is easy to follow and understand. The authors provide code with their submission that hopefully will be made publicly available for reproducibility."
            },
            "weaknesses": {
                "value": "While acknowledging the novelty of extending the sampling pool of prompts to be patch-specific and class-specific, I wonder to which extend such novelty is just marginal w.r.t. the framework proposed by Derakshani et al. In my opinion, this extension is rather marginal and while the authors have the merit to be the first to apply such extension, the technical contribution in this sense is small to me.\n\nThe use of the optimal transport is in general well motivated, but I am not sure if the results shown in Table 1 and Figure 7 (a) are a bit worrying in the sense that adding such optimization loss results in many cases detrimental. While novel, it is worth questioning whether its contribution is significant.\n\nI wonder why the comparisons against state of the art works dismiss ProDA (Lu et al 2022) and Derakshhani et al. 2022).\n\nThe writing needs to be improved, while the narrative is well threaded, I believe the paper can benefit from proof-reading.\n\nI might have missed this point but a question I have is to which extend having per-class prompts is beneficial and how this is applied to the new classes. A proper description of such scenario for inference would be desirable."
            },
            "questions": {
                "value": "All my concerns are addressed above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4535/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698683229870,
        "cdate": 1698683229870,
        "tmdate": 1699636431019,
        "mdate": 1699636431019,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Rd50PtXiRn",
        "forum": "XjlZJJFyla",
        "replyto": "XjlZJJFyla",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4535/Reviewer_EsJ6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4535/Reviewer_EsJ6"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a method to improve the prompt engineering problem. They generate stochastic prompts via a random sampled inputs and a learnable sef-attention generator. Then aligns the text embeddings with the image embeddings using bidirectional distance. The model in being trained by optimizing the ELBO. The experimental results and ablation studies showing the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is well-written and easy to understand\n- The idea of generating label-specific stochastic prompts is novel\n- Results on multiple tasks validates the effectiveness of the proposed method"
            },
            "weaknesses": {
                "value": "- I'm thinking about the motivations of generating stochastic prompts. On one hand, there are different ways to describe a given class (e.g. \"a dog that chews bones\", \"puppies are good friends of people\"). On the other hand, we can always combine multiple prompts into one single, long prompt. Is it possible that \"stochastic\" method works just because it provides more input variance in the training, therefore reduces the overfitting? (Especially finetuning data is limited)"
            },
            "questions": {
                "value": "- In figure 7 (b), I see not all randomly generated prompt correlates to the class label well (e.g. visualizations of the dog image). Are they any randomness in inference? How about the variances?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4535/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698837343236,
        "cdate": 1698837343236,
        "tmdate": 1699636430934,
        "mdate": 1699636430934,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "I6UbRmLrP7",
        "forum": "XjlZJJFyla",
        "replyto": "XjlZJJFyla",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4535/Reviewer_H1KY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4535/Reviewer_H1KY"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a Bayesian probabilistic approach to prompt tuning. In this method, label-specific stochastic prompts are generated hierarchically. This involves sampling a latent vector from an underlying distribution and utilizing a lightweight generative model. Additionally, a regularization technique is introduced to minimize the statistical distance between visual patches and linguistic prompts."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "[$\\textbf{Good Presentation}$] The paper is well-written and easy to follow.\n\n[$\\textbf{Effectiveness}$] Experiments have shown that the proposed method outperforms baseline methods."
            },
            "weaknesses": {
                "value": "[$\\textbf{Lack of Novelty}$] The concept of incorporating Bayesian neural networks for prompt learning was previously presented in \"Improving Zero-Shot Generalization for CLIP with Synthesized Prompts, ICCV23.\" This diminishes the novelty of Bayesian prompt tuning. It would be nice to differ these two works. Also, the regularization seems to be a simple utilization of conditional transport.\n\n[$\\textbf{Missed Baselines and Weak Performance}$] Some SOTA methods are missed in experiments, e.g., \u201cImproving Zero-Shot Generalization for CLIP with Synthesized Prompts, ICCV23.\u201d, \u201cSelf-regulating Prompts: Foundational Model Adaptation without Forgetting, ICCV23\u201d and \u201cMaPLe: Multi-modal Prompt Learning, CVPR23\u201d. It is not adequate to simply compare an old baseline CoCoOp only. More importantly, comparing with these SOTA methods, the performance of the proposed methods is much worse.\n\n[$\\textbf{Some Suggestions}$] The ablation study should be conducted on ImageNet as it would nice to see the effectiveness of the proposed on the most challenging dataset."
            },
            "questions": {
                "value": "Please refer to the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4535/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4535/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4535/Reviewer_H1KY"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4535/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699503627523,
        "cdate": 1699503627523,
        "tmdate": 1699636430819,
        "mdate": 1699636430819,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LuSo6Tu9Q7",
        "forum": "XjlZJJFyla",
        "replyto": "XjlZJJFyla",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4535/Reviewer_19zz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4535/Reviewer_19zz"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to hierarchically generate the label-specific stochastic prompts using generative modules from sampled noisy latent vector. Then, a conditional transport framework is employed to establish a relationship between visual patches and textual prompts. Several experimens are performed in few-shot, transfer learning, domain generalization, and base-2-new manners using ViT-B/16 and RN50 as the backbones."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The idea of using the noisy latent vector combined with deterministic mapping to generate diverse prompts for alleviating the overfitting issue in vision language prompt learning is meaningful.\n2. The paper is well-organized and easy to follow."
            },
            "weaknesses": {
                "value": "1. In Fig.3, the reported PLOT using ViT-B/16 is run by this submission. However, this result is much different from the one reported by PLOT on github (https://github.com/CHENGY12/PLOT/tree/main/plot-pp). According to these results, PLOT achieves better few-shot performance when using the ViT-B/16 as the visual backbone. \n2. The PLOT base to new experiment using ViT-B/16 reproduced in this paper also lacks credibility, considering that the performance of the proposed method and PLOT are comparable when similar experiments using RN50 as the visual backbone are performed.\n3. In my view, the primary reference for this paper is PLOT, and therefore it needs to be compared to PLOT as exhaustively as possible. However, this paper lacks some important comparisons. For example, PLOT mainly employs RN50 as the visual backbone, although this paper has added experiments in few-shot and base-2-new manners using RN50 as the backbones, the domain generalization experiments using RN50 are missing. \n4. I appreciate the proposed Stochastic Prompts Generation, however, the conditional transport seems not meaningful. In the main text, the author only claims that OT needs two stages. However, the first stage of OT is not time-costly in the CoOp-related experiments. So, what is the main contribution of using CT instead of OT?\n5. The paper lacks an ablation study on the proposed conditional transport and optimal transport (OT). We need to compare experiments using SPG and CT with experiments using SPG and OT to determine whether the proposed CT is meaningful.\n6. In PLOT, the number of prompts is set to 4. However, this paper only uses C for the number in Eq. (4) without stating the exact value in the experimental details, which may result in unfair comparisons. I also find that the Moter Carlo sampling number is set to 20 as the default setting.  Does this Moter Carlo sampling number correspond to the number of PLOT prompts? If yes, this is unfair, please conduct fair experiments and explain the reason. \n7. The learnable parameters shown in Table C.9 indicate that the proposed approach uses much more parmas compared with CoCoOp and PLOT. I would like to know the composition of these parameters and whether the additional parameters rather than the suggested method are the reason for the performance improvement.\n8. Multi-modal approaches such as CoPrompt[1], MaPLE[2], and VioLET[3] can achieve much better base-to-new performacne using ViT-B/16 as the visual backbone. I understand that the proposed approach only tunes the language branch, however, I wonder whether the proposed approach can further improve the multi-modal approaches.\n9. From the ablation studies in Table.1, P-Prompt shows better performance compared to B-Prompt, while in Figure.7(a), B-Prompt exhibits better few-shot performance. These results indicate that the proposed CT is useful for generalization while SPG accounts for better supervised performance. This may not be intuitive, as the proposed Bayesian approach is more capable of introducing uncertainty thereby enhancing generalization performance and reducing overfitting (also mentioned in Sec. 2.2).\n\n[1] Roy, Shuvendu, and Ali Etemad. \"Consistency-guided Prompt Learning for Vision-Language Models.\" arXiv preprint arXiv:2306.01195 (2023).\n\n[2] Khattak, Muhammad Uzair, et al. \"Maple: Multi-modal prompt learning.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n[3] Wang Y, Liu Y, Zhang X, et al. VioLET: Vision-Language Efficient Tuning with Collaborative Multi-modal Gradients[C]//Proceedings of the 31st ACM International Conference on Multimedia. 2023: 4595-4605."
            },
            "questions": {
                "value": "The proposed method claims to be generalizable and can solve the overfitting problem well. Then I am curious, when we increase the number of training epochs to 50 or even 200 in base-to-new experiments, does it affect the generalization performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4535/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699595852365,
        "cdate": 1699595852365,
        "tmdate": 1699636430743,
        "mdate": 1699636430743,
        "license": "CC BY 4.0",
        "version": 2
    }
]