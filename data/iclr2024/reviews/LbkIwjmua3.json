[
    {
        "id": "m3P9aBTKfE",
        "forum": "LbkIwjmua3",
        "replyto": "LbkIwjmua3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3295/Reviewer_DTs6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3295/Reviewer_DTs6"
        ],
        "content": {
            "summary": {
                "value": "This paper utilize the Sharing Differential Evolution algorithm to come up with multiple signle-pixel adversarial attacks at diverse locations. The attack region reveals the vulnerable region in the input image."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The idea of utlizing single-pixel adversarial attack to highlight vulnerable region is interesting\n2. The paper is overall well written and easy to follow\n3. The paper make interesting observation on how adversarial training affect vulnerable region, and how teh region changes with different source/target classes"
            },
            "weaknesses": {
                "value": "My major concern of the paper is the lack of technical contribution. As an attack, the proposed method is not effectively leading to high success rate, and is not well bounded (e.g. constraining the maximum number of pixels to be perturbed etc.) While as a visualization/explaination method, it is not well motivated/explained how the identification of vulnerable region can help better understand or improve the model. This hinders the significance of the paper."
            },
            "questions": {
                "value": "See weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3295/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3295/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3295/Reviewer_DTs6"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3295/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698452835872,
        "cdate": 1698452835872,
        "tmdate": 1700627720380,
        "mdate": 1700627720380,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "C033mqw9NC",
        "forum": "LbkIwjmua3",
        "replyto": "LbkIwjmua3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3295/Reviewer_rfSA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3295/Reviewer_rfSA"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a novel approach to uncover vulnerable regions using\none-pixel perturbations located at various positions. Extensive experiments\ninvolving various network architectures and adversarial training models have\nbeen conducted, demonstrating that the proposed algorithm can indeed discover\ndiverse adversarial perturbations. Additionally, a large number of well-designed\nexperiments are conducted to study the properties of discovered regions, which\nprovide some interesting insights to deep models."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "**1.** It is quite an interesting work. This paper provides a new perspective on\nunderstanding the weaknesses of DNNs with diversely located perturbations. Specifically, this work focuses on pinpointing vulnerable regions to\nsingle-pixel perturbations, distinguishing itself from interpretable methods\nemphasizing significant areas influencing the network\u2019s final output. This\napproach facilitates the identification of specific vulnerable image areas\ndeserving more attention.\n\n**2.** The paper conducts comprehensive experiments involving a range of network architectures and adversarial training techniques, clearly showcasing\nhow this method aids in assessing model vulnerability and contributes to\nimproving model interpretability. Experiment results demonstrate that\nthe proposed approach can effectively generate diverse adversarial perturbations to form vulnerable regions.\n\n**3.** Beyond vulnerability detection, the paper also conducts a series of well-designed experiments to study the properties of the discovered regions,\ne.g., how adversarial training influences the position of such vulnerable\nregions. Overall, this approach leads to valuable insights into the behavior\nof deep models, contributing to a deeper understanding of DNNs."
            },
            "weaknesses": {
                "value": "**1.** The proposed approach exhibits limitations when applied to high-resolution\nimages. Even though the author increased the population size to 800 for\nhigh-resolution images, this still only represents approximately 1.5% of the\ntotal images. However, it requires up to 80,000 queries to deep models.\n\n**2.**  It might be better if the author could briefly introduce adversarial training\nin related works section.\n\n**3.** While the authors provide details on acquiring adversarial-trained models,\ninformation on obtaining standard-trained models is lacking."
            },
            "questions": {
                "value": "This work provides a novel way to understand the weaknesses of DNNs, and\nincludes some well-designed, intriguing experiments. I have the following questions:\n\n**1.** The experimental results reveal limitations when applied to high-resolution\nimages. On average, only around 20 diverse adversarial examples are found\nfor what the author refers to as vulnerable images. This number is relatively small. Is it possible to further adapt the method to high-resolution\nimages?\n\n**2.** While understanding vulnerabilities is crucial, the paper may benefit from\ndiscussing potential real-world scenarios where such vulnerabilities can be\nexploited and their implications"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3295/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3295/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3295/Reviewer_rfSA"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3295/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698646666886,
        "cdate": 1698646666886,
        "tmdate": 1699636278410,
        "mdate": 1699636278410,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oaVQ4fO8w8",
        "forum": "LbkIwjmua3",
        "replyto": "LbkIwjmua3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3295/Reviewer_4smo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3295/Reviewer_4smo"
        ],
        "content": {
            "summary": {
                "value": "A one-pixel adversarial attack is an alteration to a single pixel that changes the network's prediction on an image.  If you take an image and repeatedly existing run one-pixel attacks, most of these runs will change the same pixel or small set of pixels.  In this paper, the authors are interested in identifying a large set of pixels that _each_ can be modified in such a way that fully changes the network's prediction.  To that end, the paper proposes an evolutionary algorithm that takes as input an image and a network, and returns a _set of distinct_ one pixel attacks that all succeed.  Often, these pixels are spatially clustered together, so the authors refer to this set of pixels as a \"vulnerable region.\"  In section 4.4 authors argue that finding 'vulnerable regions' of an image can be one way to interpret a neural network.  (It seems to sometimes highlight very different regions than GradCAM.) The authors apply their method for finding vulnerable regions on both CIFAR-10 and ImageNet for both untargeted and targeted perturbations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Originality: the paper is original, as I'm not aware of other methods for simultaneously finding distinct sets of one-pixel adversarial attacks\n\nQuality: the paper is of reasonable quality\n\nClarity: most of the paper was clear, but I thought the abstract was confusing: \"Traditional norm-based adversarial example generation algorithms, due to their lack of spatial constraints, often distribute adversarial perturbations throughout images, making it hard to identify these specific vulnerable regions.\" If I understand correctly, the proposed method does not have spatial constraints that encourage the found pixels to be close together.\n\nSignificance: I think the significance of the paper is a little unclear -- see below"
            },
            "weaknesses": {
                "value": "1.  It's not clear to me what is the motivation for finding these vulnerable regions.  What will we do with them?  If the goal is explainability, why is this method conceptually better than other alternatives?\n\n2.  As is hinted at in the paper, there is a conceptual problem with the idea of using groups of one-pixel adversarial attacks as a kind of interpretability method: as images get higher and higher resolution, the influence of each individual pixel get smaller, so one-pixel attacks get harder to find.  Ideally, an interpretability method shouldn't fall apart as the resolution gets higher."
            },
            "questions": {
                "value": "How would the authors respond to the two weaknesses listed above?>"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3295/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698692874181,
        "cdate": 1698692874181,
        "tmdate": 1699636278333,
        "mdate": 1699636278333,
        "license": "CC BY 4.0",
        "version": 2
    }
]