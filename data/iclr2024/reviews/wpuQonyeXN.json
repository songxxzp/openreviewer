[
    {
        "id": "dFhh2y0cz8",
        "forum": "wpuQonyeXN",
        "replyto": "wpuQonyeXN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1872/Reviewer_AteR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1872/Reviewer_AteR"
        ],
        "content": {
            "summary": {
                "value": "This work studies quantum reinforcement learning, where quantum means that the classical reward and state transition feedback is replaced by quantum pure states (see Eq. (3.1) (3.2)). This paper studies both the general MDP and linear MDP, and it shows that they can achieve logarithmic regret performance, which breaks the classical square-root regret lower bound."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The authors did a good job of presenting this work and comparing it to known literature."
            },
            "weaknesses": {
                "value": "- Lack of novelty. I am familiar with the work of Wan et al 2022 for quantum multi-armed bandits and quantum linear bandits. As the main theoretical tools for multi-armed bandits and linear bandits are considerably similar to RL and linear RL respectively, this paper can be regarded as an extension from Wan et al 2022 to quantum RL. Although the author pointed out one new challenge in Remark 5.1, I did not see enough novel contributions in this work.\n\n\n\n---\nZongqi Wan, Zhijie Zhang, Tongyang Li, Jialin Zhang, and Xiaoming Sun. Quantum multi-armed\nbandits and stochastic linear bandits enjoy logarithmic regrets. In To Appear in the Proceedings\nof the 37th AAAI Conference on Artificial Intelligence, 2022. arXiv:2205.14988"
            },
            "questions": {
                "value": "Although leaning toward a negative evaluation of this work for its lack of contribution, I think this quantum RL topic is interesting and would suggest that the authors look into challenging issues around this topic, e.g., regret lower bounds for quantum RL which is not studied in quantum bandits in Wan et al 2022. \n\nIf the authors think there are other nontrivial challenges (except for Remark 5.1) in this work than in Wan et al 2022, please take the chance of rebuttal to explain."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1872/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698133234840,
        "cdate": 1698133234840,
        "tmdate": 1699636117834,
        "mdate": 1699636117834,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rbY7Te3Aaq",
        "forum": "wpuQonyeXN",
        "replyto": "wpuQonyeXN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1872/Reviewer_pp1B"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1872/Reviewer_pp1B"
        ],
        "content": {
            "summary": {
                "value": "This paper studies quantum RL, which provides sample complexity for both tabular MDP and linear mixture MDP, based on several quantum estimation oracles. Compared with previous literature, this paper provides an online exploration method for quantum RL."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This paper is well written and easy to follow\n- Study quantum RL is novel in the literature, with limited prior works\n- The proposed online exploration paradigm is more practical than previous work."
            },
            "weaknesses": {
                "value": "- The discussion on sample complexity is not enough. For example, it would be better to discuss why the $\\sqrt{T}$ factor is removed. Is that because of Lemma 3.1 and Lemma 3.2 such that the previous $\\epsilon^{-2}$ sample complexity can be reduced to $\\epsilon^{-1}$ sample complexity so that the exploration can be more aggressive? \n- Besides the previous comment, I'm also looking for discussions about the lower bounds (or at least some conjectures). For example, if the dependency on $d, H$ within the lower bounds still match (Zhou et al., 2021) or not?"
            },
            "questions": {
                "value": "Besides my concern about the weakness, I'm concerned about the cost of translating a classical RL task into a quantum-accessible RL task. Here are my questions\n- Can one directly covert the observation in classical RL to a quantum-accessible RL? (e.g., changing the Atari games to quantum). If the quantum RL can be used in classical RL tasks, then how would the current $\\log T$ bound break the classical $\\sqrt{T}$ regret bound?\n- If the current algorithm can only be used in quantum-accessible RL, and we cannot convert a classical RL task into quantum, then how will this algorithm contribute to real-world RL tasks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1872/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698619512720,
        "cdate": 1698619512720,
        "tmdate": 1699636117749,
        "mdate": 1699636117749,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "55wCdm6gvh",
        "forum": "wpuQonyeXN",
        "replyto": "wpuQonyeXN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1872/Reviewer_KqvL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1872/Reviewer_KqvL"
        ],
        "content": {
            "summary": {
                "value": "This work studies a quantum RL problem, where the objective is to explore the episodic MDP with quantum access and learn the optimal policy while minimizing the regret over $T$ episodes. The authors propose the Quantum UCRL and Quantum UCRL-VTR algorithms for tabular MDP and linear mixture MDP settings respectively. Their analysis of the algorithms gives $O(\\text{poly}(\\log T))$ regret upper bounds."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This work is one of the pioneering effort in studying quantum reinforcement learning with theoretical guarantees.\n- This work incorporates quantum *multi-dimensional/multivariate* estimation subroutines into UCRL-based algorithms. The insights in such incorporations may of interest to the emerging quantum machine learning community."
            },
            "weaknesses": {
                "value": "- Quantum regret lower bound is not discussed in the paper \n- The presentation is not clear -- some notations are used without clearly defined or explained\n- A closely related work is missing from the literature review: Ganguly, Bhargav, et al. \"Quantum Computing Provides Exponential Regret Improvement in Episodic Reinforcement Learning.\" arXiv preprint arXiv:2302.08617 (2023).\n- This work does not have any empirical study of the proposed algorithms"
            },
            "questions": {
                "value": "- Could the authors comment on why the binary oracle is not considered in the tabular setting? What is the main difficulty in generalizing the result of Lemma 3.1 to binary oracle?\n- Is the regret lower bounds of the studied \"quantum\" exploration problems known? If not, could the authors comment on the difficulties of getting such lower bounds? \n\nThe above two points may be worth mentioning in a future direction section/paragraph.\n\n- The introduction/description of the Quantum UCRL algorithm is not clear enough. Specifically, I could not find the $\\bar{\\varphi}_{h+1}, \\mathcal{D}_h(s^k_h, a^k_h)$ notations appeared in Algorithm 4 being defined anywhere. \n- If $\\bar{\\varphi}_{h+1}$ is as defined at the end of subsection 3.2, then it is a quantum state in superposition. How could Algorithm 4 line 9 update the counter according to the superposition? Please correct me if I missed anything.\n- Why does Quantum UCRL divides the episodes into T/H phases while Quantum UCRL-VTR divides into K phases? How should the practitioners set the parameter K for Quantum UCRL-VTR?\n\nI would love to see Algorithm 4 be presented in the main paper for the sake of clarity if the page limit allows.\n\n- (minor wording issue)  The term \"quantum state\" is somehow ambiguous as the term \"state\" has its special meaning in RL problem."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1872/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1872/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1872/Reviewer_KqvL"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1872/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698758373264,
        "cdate": 1698758373264,
        "tmdate": 1699636117677,
        "mdate": 1699636117677,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tO6nnkRKkj",
        "forum": "wpuQonyeXN",
        "replyto": "wpuQonyeXN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1872/Reviewer_XG1J"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1872/Reviewer_XG1J"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the online exploration problem in reinforcement learning. Specifically, two RL settings are considered: tabular Markov decision processes (MDPs) and linear mixture MDPs; and the goal is to learn the policy that minimizes regret. To achieve this, the authors propose two algorithms that adapt existing RL algorithms by using tools from quantum computing to get performance gain."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "**The following are the strengths of the paper:**\n1. Adapting recent tools from quantum computing to improve the performance of RL algorithms is a challenging and interesting contribution.\n\n2. The authors consider two RL settings -- tabular MDPs and linear mixture MDPs; and propose algorithms (Quantum UCRL and Quantum UCRL-VTR) with logarithmic regret (in terms of episodes) for both problems due to quantum speedup."
            },
            "weaknesses": {
                "value": "**The following are the key weaknesses of the paper:**\n1. Motivating examples: It is unclear if the assumptions (access to quantum oracles and their inverse, quantum state) made in the paper are practical or not. Adding a few motivating examples where these assumptions (will) hold will make the contribution even more significant.\n\n2. The doubling trick to design lazy-updating algorithms with quantum estimators is already used in existing work (e.g., Wan et al., 2022), so saying this is a  novel technique proposed in the paper is an overclaim (Last paragraph on Page 2). However, I agree adapting this idea to MDPs is not that straightforward.\n\n3. Since the learner does not observe the next state, it is unclear how the number of quantum samples ($n_h(s, a)$) is tracked by the learner. It is important as tracking $n_h(s, a)$ is needed to update the estimate of the transition kernel. Overall, adding a detailed explanation of how quantum computing tools are used will make it easier to understand the contributions."
            },
            "questions": {
                "value": "Please address the above weaknesses. I have a few more questions/comments:\n1. Page 6, paragraph before 'Lazy updating via doubling trick': Is there any connection between phase length (H) and episode horizon (H)?\n\n2. The quantum oracle for reward function is not used as it is assumed to be known for the problems considered in the paper. Is this right?\n\nMinor comment:\n If possible, authors can add a few experiments using the Python library QisKit. It will make the paper stronger.\n\nI am open to changing my score based on the authors' responses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Not applicable"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1872/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1872/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1872/Reviewer_XG1J"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1872/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1700626200451,
        "cdate": 1700626200451,
        "tmdate": 1700629875106,
        "mdate": 1700629875106,
        "license": "CC BY 4.0",
        "version": 2
    }
]