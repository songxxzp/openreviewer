[
    {
        "id": "arlHw1qi0C",
        "forum": "PaOuEBMvTG",
        "replyto": "PaOuEBMvTG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4979/Reviewer_gfC6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4979/Reviewer_gfC6"
        ],
        "content": {
            "summary": {
                "value": "This paper indicates that contrastive learning for single object centric images has achieved impressive performance and suffered inferior performance on multiple objects. To this end, this paper proposes a method, i.e., Multiple Object Stitching (MOS), to refine the unsupervised representation for multi-object images. Particularly, they construct the multi-object images by stitching the single object centric ones, where the objects in the synthesized multi-object images are predetermined. In the experiments, the proposed method is evaluated on multiple datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "Applying contrastive learning to multiple object scenarios is meaningful."
            },
            "weaknesses": {
                "value": "1. To the best of my knowledge, applying contrastive learning to multiple object scenarios has been explored by many works. There exist multiple works that aim to utilize contrastive loss for object detection and semantic segmentation. However, in the Introduction Section, the authors do not introduce these works. I recommend the authors modify their paper carefully, introduce these works, and make corresponding contrasts.\n\n2. To address the multiple object contrastive learning, the authors propose a stitching strategy. However, this strategy is somewhat simple. Meanwhile, I am not clear on how to use the proposed strategy to obtain object-related content.\n\n3. In Method Section, the authors should add a discussion section to discuss the advantages and disadvantages of the proposed method.  Using more stitched images may increase computational costs. The authors should give more interpretations. Finally, the authors should show some training curves, which is helpful for further understanding the proposed method. Meanwhile, the authors should show some T-SNE results to indicate the superiorities of the proposed method."
            },
            "questions": {
                "value": "The authors should evaluate the proposed method on object detection and semantic segmentation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4979/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697533519641,
        "cdate": 1697533519641,
        "tmdate": 1699636485458,
        "mdate": 1699636485458,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AnMP571gP2",
        "forum": "PaOuEBMvTG",
        "replyto": "PaOuEBMvTG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4979/Reviewer_AZJE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4979/Reviewer_AZJE"
        ],
        "content": {
            "summary": {
                "value": "Contrastive learning suffers from the issue of semantic inconsistency caused by random cropping. This paper circumvents this problem by exclusively utilizing single-object images and their synthetic combinations, creating a 2x2 grid of a single image. This synthetic image serves as a pseudo-multi-object image, where the location of the positive object is known, allowing it to be trained similarly to a multi-object image. This technique enhances the learned representation, as evaluated in image classification and object detection benchmarks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Self-supervised learning, particularly learning from multi-object images, is an important problem.\n- The performance improvement is quite significant, both in image classification and object detection."
            },
            "weaknesses": {
                "value": "**Movitation is not new**\n\nThe issue of semantic inconsistency in contrastive learning has been discussed in several prior works [1-4].\nThese works are not properly cited, which may lead readers to overestimate the contribution of this paper.\nThe efforts of prior work and the contributions of this paper should be clarified in the second paragraph of the introduction.\n\n[1] CASTing Your Model: Learning to Localize Improves Self-Supervised Representations. CVPR'21.\\\n[2] Unsupervised Object-Level Representation Learning from Scene Images. NeurIPS'21.\\\n[3] Object-aware Contrastive Learning for Debiased Scene Representation. NeurIPS'21. \\\n[4] Object-aware Cropping for Self-Supervised Learning. TMLR'22.\n\n\n---\n**Not addressing the raised issue**\n\nThis paper does not address the issue that has been raised: the problem of random cropping in multi-object images.\nInstead, the paper solely utilizes single-object images and their combinations, while ignoring multi-object images.\n\nThis raises two problems:\n- The paper can be considered a general augmentation strategy for contrastive learning, rather than specialized for multi-object images. This necessitates a proper revision of the storyline, as well as the baselines to compare with.\n- Restricting the training images to single-object images significantly limits the applicability of the method to the majority of unlabeled images containing multiple objects. Thus, the proposed method should be compared with vanilla contrastive learning trained on a larger superset of images that also includes multi-object images.\n\n\n---\n**Method is not new**\n\nCombining multiple images for data augmentation has been studied in multiple prior works [5-6].\nFurthermore, similar augmentation strategies have also been applied in the context of contrastive learning [7-8].\nThe paper should properly cite these prior works and make comparisons with them, particularly the most relevant MosRep [8].\n\n[5] RICAP: Random Image Cropping and Patching Data Augmentation for Deep CNNs. ACML'18.\\\n[6] CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features. ICCV'19.\\\n[7] i-Mix: A Domain-Agnostic Strategy for Contrastive Representation Learning. ICLR'21.\\\n[8] Mosaic Representation Learning for Self-supervised Visual Pre-training. ICLR'23.\n\n\n---\n**Method section could be revised**\n\nThe current method section is difficult to understand.\nSince the overall method is relatively simple, the authors should consider a more effective way to convey the message:\n- Super/subscripts are overused. Consider simplifying the notations.\n- Equations are overused. Consider moving less critical ones (e.g., straightforward definitions like Eq. (1)) into the inline text and using the equation mode exclusively for important formulas (e.g., the definitions of m2s/m2m/s2s losses) to emphasize them.\n- For Fig. 2, consider making the caption self-contained, possibly using natural language without math notations that are defined in the main text below."
            },
            "questions": {
                "value": "1. Compare this method with prior works such as i-Mix and MosRep.\n2. This method can also be applied to multi-object images?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4979/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4979/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4979/Reviewer_AZJE"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4979/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698284219972,
        "cdate": 1698284219972,
        "tmdate": 1700506835086,
        "mdate": 1700506835086,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jmn0hGDmiY",
        "forum": "PaOuEBMvTG",
        "replyto": "PaOuEBMvTG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4979/Reviewer_Kxdm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4979/Reviewer_Kxdm"
        ],
        "content": {
            "summary": {
                "value": "The research paper introduces a novel method for unsupervised multi-object representation learning in computer vision. It addresses two significant challenges in this field: the semantics inconsistency between views and the difficulty in establishing accurate object correspondences. The proposed method tackles these challenges by synthesizing single-object images into multi-object images. This unique approach is demonstrated to be highly effective in experimental evaluations, excelling in image classification and outperforming existing methods in object detection and instance segmentation. The research opens up new possibilities for unsupervised learning in various modalities."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "-\tThe paper presents an innovative approach to unsupervised multi-object representation learning, which is an increasingly important area in computer vision.\n-\tThe method's technique for multi-object image stitching through data augmentation, scaling, and tensor operations is efficient and leads to improved representation learning."
            },
            "weaknesses": {
                "value": "-\tWhile the method excels in multi-object representations, it may not have been extensively tested in scenarios with highly dynamic or cluttered objects."
            },
            "questions": {
                "value": "-\tIn table 3, Selfpatch is trained for only 200 epochs and is compared against this work which is trained for 300 epochs? What is the rationale behind this comparison?\n-\tHave you tested the method in scenarios with highly dynamic or cluttered objects or occluded objects?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4979/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4979/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4979/Reviewer_Kxdm"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4979/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698739956332,
        "cdate": 1698739956332,
        "tmdate": 1699636485284,
        "mdate": 1699636485284,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "E5V5q0yDSC",
        "forum": "PaOuEBMvTG",
        "replyto": "PaOuEBMvTG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4979/Reviewer_R2AJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4979/Reviewer_R2AJ"
        ],
        "content": {
            "summary": {
                "value": "Most existing algorithms for self-supervised learning use single object-centric images for contrastive learning. The paper claims they failed to perform well on images with multiple objects. To resolve the problem, the paper proposes to stitch multiple object-centric images to become multi-object images for contrastive learning. More specifically, the learning objective contains singe-to-single, single-to-multi, and multi-to-multi contrastive pairs. The paper shows improved performance on multiple benchmarks including ImageNet-1K, CIFAR, and COCO."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper achieves state-of-the-art results on multiple benchmarks. \n1. The idea is simple but effective."
            },
            "weaknesses": {
                "value": "1. The motivation is unclear. The paper claims that contrastive learning for single object-centric images \"suffer inferior performance on\nthe widespread images with multiple objects\", but it doesn't provide enough evidence to support the claim. For example, ImageNet-1K and CIFAR are recognition problems with single objects, why do we need to stitch multiple together? I understand that, for example, even in ImageNet-1K a lot of times images do contain multiple objects. If this is the case, I suggest the paper show the number of objects vs classification performance by applying some existing detectors, which can prove that the performance of existing algorithms degrades on multi-object images. However, somehow it is counter-intuitive because the proposed method may strengthen the minor objects in the images which distracts the recognition of the main object.\n1. Similar to the above, detection and segmentation tasks should be the main and right benchmarks to explain the proposed algorithm because they definitely contain multiple objects in a scene. However, the paper still needs to provide more analysis to support the main assumption about multi-objects. For example, it will be very convincing if the paper shows the proposed algorithm can already achieve a better class-agnostic recall of objects on the attention map on COCO before even fine-tuning on it. In addition, I think it's better if it can show the ablation in table 4 with the detection and segmentation task. \n1. To me the idea is simple and I totally understand the proposed algorithm, but I think section 3 is overcomplex with math expression. To me math expression is to help explanation not to make it harder to read. One problem might be there are so many notations. I believe there is a simpler way to make it clearer. For example, stitching itself might not be that important. More important is how to learn from stiched images, e.g. proposed three losses. I hope I can see more underlying reasons for learning objectives instead of just showing the formula. For example, I am not quite convinced by the claim of L_s2s, \"to alleviate the domain gap between synthesized multi-object images and natural images\". Isn't it just the same as the existing algorithms without stitching?\n1. The paper uses ViT as the backbone. I am curious about the performance with CNN because to my understanding this assumption should not be related to the architecture."
            },
            "questions": {
                "value": "1. In table 4, what is the main difference between s2s and existing unsupervised algorithms?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4979/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698816567558,
        "cdate": 1698816567558,
        "tmdate": 1699636485174,
        "mdate": 1699636485174,
        "license": "CC BY 4.0",
        "version": 2
    }
]