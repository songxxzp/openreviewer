[
    {
        "id": "0iHiyf2V4r",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3309/Reviewer_2hLo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3309/Reviewer_2hLo"
        ],
        "forum": "QRWrvzRU4w",
        "replyto": "QRWrvzRU4w",
        "content": {
            "summary": {
                "value": "The paper proposes a method to convert rate-encoded spiking neural network into an equivalent OneSpike model with only one timestep. Authors use a parallel spike generation (PSG) method and develop a OneSpike framework. The paper claims that this method can achieve ultra-low latency, high accuracy, and hardware feasibility for SNNs. The paper compares OneSpike with various state-of-the-art SNNs and BNNs, and shows that OneSpike achieves the highest accuracy (81.92% on ImageNet) over other ANN-SNN conversion methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "Authors evaluation the OneSpike method on ImageNet with RepVGG-L2pse architecture and achieve an 81.92% accuracy."
            },
            "weaknesses": {
                "value": "Compared to IF neuron, OneSpike neuron use different group to generate spike output corresponding to different timesteps in IF neuron. Thus, the claim of one timestep neuron is not true.\n\nOneSpike model is mathematically equivalent to an activation quantized model. Compare to the widely used, GPU friendly network quantization technique, I don't think OneSpike has any advantage."
            },
            "questions": {
                "value": "Please discuss the concerns addressed in weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3309/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697458482126,
        "cdate": 1697458482126,
        "tmdate": 1699636280199,
        "mdate": 1699636280199,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uSvizz2vA5",
        "forum": "QRWrvzRU4w",
        "replyto": "QRWrvzRU4w",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3309/Reviewer_t2PL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3309/Reviewer_t2PL"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a parallel spike generation (PSG) method that generates all spikes for a network layer within a single timestep."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The authors think that they have achieved superior results on complex datasets under low time latency."
            },
            "weaknesses": {
                "value": "1. **I think the concept of OneSpike proposed in this paper is actually a gimmick.** As shown in Fig.1, the authors split the same parameters of each layer into $g_l$ groups, in fact speculating whether neurons will fire a spike at $i$-th step ($i=1,...,g_l$) under the condition that the input current in each step is completely the same (i.e. the current is uniformly distributed). Subsequently, they obtain an accurate spike sequence $s_1,...,s_g$ under the condition of uniform input current, then continue to calculate the new average input current $x^{l+1}$ after passing through the next-layer weights $W^l$. Note that in this process $s_1,...,s_g$ are respectively calculated with $W^l$ and the overall number of operations is the same as the number of operations in the previous works that emitted spikes for $g_l$-steps. **That is to say, the overhead of OneSpike mentioned by the authors is actually equivalent to the cost of the previous researchers' $g_l$ time-steps.**\n\n2. Eq.10 involves multiplication and modulus operations, which were usually not allowed in previous SNN related works.\n\n3. The reason why authors can achieve an accuracy >80% is not merely because their algorithm is superior to previous works, but because of the advantages of RepVGG network structure itself. Previous works mainly used VGG-16 and ResNet-34, which is obviously difficult to achieve an accuracy of >75% on ImageNet.\n\nOverall, I think the contribution of this paper is actually very limited. If we switch the order of the weight matrix $W^l$ and summation operations ($\\sum$) in Figure 1, in fact, the operation mechanism of the entire network is completely equivalent to QCFS ANN [1], which is an ANN with quantized activation output.\n\n[1] Tong Bu, Wei Fang, Jianhao Ding, PengLin Dai, Zhaofei Yu, and Tiejun Huang. Optimal ann-snn conversion for high-accuracy and ultra-low-latency spiking neural networks. ICLR 2022."
            },
            "questions": {
                "value": "See Weakness Section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3309/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3309/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3309/Reviewer_t2PL"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3309/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697797938573,
        "cdate": 1697797938573,
        "tmdate": 1699636280121,
        "mdate": 1699636280121,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PYbgQkX1GI",
        "forum": "QRWrvzRU4w",
        "replyto": "QRWrvzRU4w",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3309/Reviewer_nUrX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3309/Reviewer_nUrX"
        ],
        "content": {
            "summary": {
                "value": "The paper represents an interesting step in the efforts to make SNNs live up to the promise of lower energy consumption than there ANN counterparts. The paper proposes an ANN-to-SNN conversion, or more specifically a conversion from N-step SNNs to 1-step SNNs that preserves accuracy. The results on ImageNet getting over 80% accuracy is really strong as this is a much higher accuracy than previous SNN papers. However, in this reviewer's perspective there are several questions unanswered which makes the true benefits of the approach unclear."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The resulting accuracy improvement on ImageNet is impressive."
            },
            "weaknesses": {
                "value": "The weaknesses of the paper are related to an incomplete energy model and analysis. It does not fully consider the cost of memory access nor the cost of handling the sparsity (compared to ANNs).  Comparisons to state of the art SNNs are focused on accuracy and not energy."
            },
            "questions": {
                "value": "The paper talks proposes to in parallel create different spike groups changing the traditional IF model significantly. However, this opens the question of proper comparisons. For example, there are many non-multiplier-based implementations of ANNs that should also be considered when doing comparisons. In particular, the fact that their approach involves a module of a power of 2, made me think that their approach must be similar to decomposing the weights of an ANN bit-wise. However, I understand that the power of 2 for each group and each layer is fixed. I had wondered if you considered varying theta for different groups. \n\nMore generally, I think it would be good for the paper to better explain the SNN -> ANN conversion step. Your abstraction mentions this but in your algorithm, you focus on converting a N-step SNN to a 1-step SNN. Also, in figure 1, it seems you are using W for both weights and a dimension of the input feature map. This is confusing. Can you clarify?\n\nI find the analysis of energy consumption based on FLOPs somewhat limiting. In many neuromophic designs the dominant energy consumption is the weight and membrane potential lookup. Can you include an estimate of the memory access cost in your designs and comparisons? There are a number of energy models of SNNs (see e.g., https://arxiv.org/pdf/2309.03388.pdf) that include means of capturing the memory cost of SNNs that would make the results far more reliable.  In particular, my concern is that most of the membrane potentials need to be updated despite the sparsity of activations and this should be captured.\n\nSecondly, I think the paper should at least have a discussion of  the cost of supporting the SNN bit-level sparsity (compared to ANNs that do not do typically have or need to handle this granularity of sparsity).  For example, looking up a 1-bit activation is not 8 times less energy than looking up a 8-bit activation because much of the energy is associated with address decoding.  In designs that are spike centric (like Loihi) the cost of memory lookups and routing data can overshadow the cost of add vs mulitply (which is why they support graded spikes). Numerous hardware designs have been proposed to better manage weight and activation sparsity but they come at a cost. This should be recognized when proposing advanced SNN algorithms.\n\nI also wondered if your constraint on the ANN quantization has an impact on accuracy. This does not seem to be addressed.  It was called \"near-lossless\" but not quantified (from what I can see).  Can you clarify?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3309/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3309/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3309/Reviewer_nUrX"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3309/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698441924623,
        "cdate": 1698441924623,
        "tmdate": 1699636280048,
        "mdate": 1699636280048,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0p57vYVQqU",
        "forum": "QRWrvzRU4w",
        "replyto": "QRWrvzRU4w",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3309/Reviewer_yNko"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3309/Reviewer_yNko"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a method to convert a classical analog neural network into a spiking neural network. The results are compared with other state-of-the-art methods and show good performance on the imagenet challenge."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is clear and well written. The results are interesting and show a significant performance improvement over state-of-the-art methods."
            },
            "weaknesses": {
                "value": "The parallel spiking generation method could be understood as some kind of quantization of analog numbers in a dyadic format, and this point should be more clearly stated.  As such, such a method seems relatively similar to methods that use quantization in analog networks. In particular, even if this method seems original, the parallelism with existing methods needs to be strengthened. In particular the claim that \"To the best of our knowledge, this study is the first to explore converting multi-timestep SNNs into equivalent single-timestep ones\" should be circonstantied. On the other hand, do you see any analogy between this mechanism and processes that might take place in biological neural networks? It seems, for example, that predictive methods will use residuals, and that these can themselves be quantified, and so on... but to my knowledge, there are no papers exploring this interesting direction of research."
            },
            "questions": {
                "value": "The method presented in this paper works well on static images. Do you think this method could be extended to dynamic images, such as videos? Do you think this method could be extended to recurrent neural networks?\n\n\nMinor:\n- \"a PyTorch toolkit called OpCounter(Lyk), \" fix reference"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3309/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698659730953,
        "cdate": 1698659730953,
        "tmdate": 1699636279978,
        "mdate": 1699636279978,
        "license": "CC BY 4.0",
        "version": 2
    }
]