[
    {
        "id": "8ITeKH7WCY",
        "forum": "wk77w7DG1N",
        "replyto": "wk77w7DG1N",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6094/Reviewer_Zcqt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6094/Reviewer_Zcqt"
        ],
        "content": {
            "summary": {
                "value": "Authors propose a three-step algorithm for detecting and improving the consistency between a reference piece of text and a generated piece of text. The algorithm begins by segmenting the input texts, then using an LLM to generate a justification statement for each generated sentence. In the next step, the justifications are used as input to another LLM to produce numerical predictions, to be aggregated to output a single score. The final step is to use a third LLM to re-write the generated sentences if they are predicted to be inconsistent.\n\nThe method is evaluated in two paraphrase detection and two summarization datasets, and shows a lot of improvement over BERTScore, BARTScore, and a few other metrics."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Very well-written paper, both in terms of writing and also in terms of presentation.\\\nVery good results.\\\nThe research task is real, and has real world applications."
            },
            "weaknesses": {
                "value": "My argument is very simple: What if all the improvement that the authors are getting is coming from using GPT3.5/4 ? I am saying, it is not fair to compare GPT3.5/4 to BERT, or even to GPT3.\\\nAuthors describe a long list of techniques and methods to justify their model. What if all of them are just distraction, and the main reason that their method works is because of good outputs generated by GPT3.5/4 ?\\\nIn fact the authors report an experiment that supports my argument. Tables 2-4 compare the results of their method when GPT3.5 is replaced with GPT4. The improvements in most cases are substantial. Which means a lot of work is carried by the underlying LLM.\\\nIn my opinion, all the experiments should be repeated with identical underlying LM or LLM to be able to claim that the method really works (please just do not say that because a method is called BERTScore, then we should only use BERT vectors in it, and for example BLOOM vectors cannot be used in it).\n\nMy second argument is to refute the authors argument about the robustness of their method against hallucination. On Page 2, the authors state that their model does not rely on LLMs, they also repeatedly claim that existing methods are prone to hallucination---multiple times in the intro section and in the other sections.\\\n But they are using LLMs through out their algorithm! I don\u2019t know how they cannot see that! Plus all the three steps of their algorithm can be easily subject to hallucination as well. For example in the first step the LLM can easily generate hallucinated justification. Why are the authors so certain that this cannot happen?! Not clear to me."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6094/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698445364770,
        "cdate": 1698445364770,
        "tmdate": 1699636657371,
        "mdate": 1699636657371,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UYDjYm4VI3",
        "forum": "wk77w7DG1N",
        "replyto": "wk77w7DG1N",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6094/Reviewer_9wcP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6094/Reviewer_9wcP"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a sentence-granularity evaluation strategy based on LLM.\nThe evaluation is decomposed into pre-sentence assessments, which are then combined into a single score.\nThe experiments show that sentence-level evaluation is more consistent than token-level (e.g., BertScore) or passage-level assessment (e.g., GPT-Score) for paraphrasing and summarization tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1 Assessing the quality of candidate answers is best done using fine-grained signals such as sentence-level granularity.\n\n2 The proposed method achieves higher consistency with human annotations in tasks of paraphrasing identification and summarization, compared to a wide range of baseline methods."
            },
            "weaknesses": {
                "value": "1 The proposed method has a limited scope and is not easily generalizable to broader instruction-following tasks. In the experiment, the method required hand-crafted prompts for each specific task, which can be overwhelming and even infeasible when evaluating a wide spectrum of tasks. Other LLM-based evaluators can handle this common scenario more effectively.\n\n2 The experiments lack human evaluations beyond the instance level, leaving unclear the overall accuracy of sentence-level judgments.\n\n3 The per-sentence assessment protocol is used to identify and fix inconsistencies, but it may be prone to LLM's overconfidence -- it prefers its predictions even if it is wrong.\n\n4 The evaluation focuses solely on consistency, but other criteria like factuality and coherence are also important for text generation. It's unclear why only consistency was considered."
            },
            "questions": {
                "value": "1 Why it is stated in the Intro that DCE \u201cdoes not rely on LLM\u201c, considering Chatgpt/GPT4 is required for most components of the method?\n\n2 How is the reason-assisted improver (RAI) compared to self-refine?\n\n3 G-Eval does not use references during evaluation, so comparing the proposed method to G-Eval may not be fair since they use different evaluation approaches."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6094/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698747032512,
        "cdate": 1698747032512,
        "tmdate": 1699636657180,
        "mdate": 1699636657180,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "12ln8caGjA",
        "forum": "wk77w7DG1N",
        "replyto": "wk77w7DG1N",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6094/Reviewer_2NAw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6094/Reviewer_2NAw"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates consistency evaluation by employing a Large Language Model (LLM) as an evaluation agent. The authors introduce a \"divide-and-conquer\" prompting technique to break down both candidate and reference documents into individual sentences. This approach facilitates comparison at the sentence level initially, before combining these sentence-level results to produce a final evaluation. The proposed methodology is tested on SUMMeval consistency and QAGS benchmarks. Results indicate an improvement in correlation with human evaluators, specifically on the consistency criterion."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper introduces an efficient \"divide-and-conquer\" evaluation prompt designed to enhance the assessment of the consistency criterion through an LLM agent. \n- The manuscript is both well-structured and easy to understand."
            },
            "weaknesses": {
                "value": "- Novelty and Contribution:\n  - The approach of SMART [1], which suggests using sentences as the basic units for text evaluation, seems similar to the divide-and-conquer technique introduced in this paper. Specifically, methods like SMART-Bleurt and SMART-CHRF have shown strong agreement with human evaluations. Given that, the novelty of the divide-and-conquer strategy that focuses on sentence-level evaluation might not be very original when considering what SMART has already introduced. This makes me question the overall unique contribution of the present work.\n\n[1] SMART: Sentences as Basic Units for Text Evaluation\n\n- Evaluation Fairness:\n  - The authors use a \"reference\" document to help with consistency evaluation. However, from the prompts shown in Appendix A.2, it's unclear what \"reference\" means. In some cases, it seems to mean the \"true answer\" (like a gold standard in common situations), while in others, it refers to the \"article\" (or source document in common contexts). Could the authors clarify what they mean by \"reference\"?\n\n  - G-EVAL is described as a method that doesn't rely on a reference (reference-free method), where \"reference\" seems to mean a gold reference written by the human expert. If this study's prompts use the \"gold reference\" or the \"source\", it would be helpful if the authors could specify whether the proposed divide-and-conquer technique is based on a reference or not, and if it uses a source or not.\n\n  - Additionally, the lack of comparisons with other consistency methods like SMART, NLI, and Bleurt makes me question the thoroughness of the paper.\n\nUpdate after author feedback:\n\nThe authors have addressed my queries regarding evaluation fairness and have conducted additional experiments comparing with the \"consistency method.\" Given the constraints of the rebuttal period and the quality of open source code, they were able to partially reproduce the results from the SMART paper. This effort has largely resolved my concerns about evaluation fairness.\n\nConsequently, I am increasing my overall recommendation score to 6 (weak accept).\n\nHowever, I maintain that the novelty of the \"divide and conquer\" approach is somewhat limited, as the SMART paper has already proposed using \"sentence as evaluation unit.\" Additionally, the paper could benefit from revisions for methodological clarity."
            },
            "questions": {
                "value": "Given that the reference and candidate documents might have varying numbers of sentences, how do you handle sentence-level comparisons? Specifically, do you employ any sentence matching techniques, and if so, how are they implemented?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6094/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6094/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6094/Reviewer_2NAw"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6094/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698783518325,
        "cdate": 1698783518325,
        "tmdate": 1700444441478,
        "mdate": 1700444441478,
        "license": "CC BY 4.0",
        "version": 2
    }
]