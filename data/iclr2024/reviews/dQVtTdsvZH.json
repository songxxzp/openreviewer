[
    {
        "id": "vxUXOGiRWA",
        "forum": "dQVtTdsvZH",
        "replyto": "dQVtTdsvZH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3782/Reviewer_BXkg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3782/Reviewer_BXkg"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a video diffusion model that can be trained efficiently via content-motion disentanglement. An autoencoder is trained to learn common visual content and low-dimensional video motion. Then, a pretrained image diffusion model is fine-tuned to generate content frames, and the motion patents are generated with a lightweight diffusion model. This method can generate a video faster while achieving better FVD in WebVid-10M."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tCDM consumes much lower computation resources to inference.\n2.\tThe method presented in this paper is cool and has the potential to improve the video generation research community, especially the idea of decomposing a video to motion and content. \n3.\tCDM achieves good CLIPSIM metric in MSR-VTT and WebVid-10M.\n4.\tThe GPU memory usage and speed evaluations are extensive."
            },
            "weaknesses": {
                "value": "This is a technically solid paper. I appreciate the design of the autoencoder design in CDM and believe the video generation community can learn from it. But it suffers from the following weaknesses:\n1.\tExperiment comparison setting: According to the supplementary video, CDM\u2019s generation has considerably slow motion or low motion amplitude. However, LVDM and ModelScope can produce diverse and large motions. Therefore, the FVD comparison setting is not fair. We need other experiments to show CDM is better. For example, LVDM is trained on WebVid-10M with frame stride 8 (i.e. sample 16 keyframes in a 128-frame video clip), please re-train CDM to match the setting of LVDM.\n2.\tEvaluation of autoencoder: Please conduct a quantitative evaluation of the performance of the CDM autoencoder. You can use a series of quantitative metrics. For example, SSIM, PSNR, etc. It would be great if you also showed how good stable-diffusion KLVAE can do on video reconstruction.\n3.\tTraining cost: To train CDM, one needs to train an autoencoder, a content diffusion, and a motion diffusion model. As a result, the training cost of CDM is very close to that of training ModelScope or LVDM. According to the supplementary material, training the autoencoder consumes 8 A100 for a week. After that, content diffusion and motion diffusion models can be trained. And they use 96 A100 for days to train on WebVid-10M. The training cost of CDM is not considerably lower than LVDM and ModelScope.\n4.\tVisual definition problem: CDM produces 512x1024 resolution videos. However, the supplementary is a 9M mp4 file, and is very hard to tell the visual definition of the samples. It would be great if the author provided evaluations on the visual definition of CDM.\n\nThis paper is generally well-written. But when I parse through the literature, I find some statements that are somewhat unproper. See below:\n1.\tIn the abstract, the author says CDM is the first that can directly utilize a pretrained image diffusion model. But, to the best of my knowledge, Video LDM, LVDM, and ModelScope are also utilizing pretrained text-to-image models. \n2.\tPlease number your equations. It will be very helpful for readers to understand your math formulas.\n3.\tThe training time for the content frame diffusion generator is not shown in the supplementary material."
            },
            "questions": {
                "value": "Please see the above questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3782/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698631123237,
        "cdate": 1698631123237,
        "tmdate": 1699636334980,
        "mdate": 1699636334980,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "seg2mss1uW",
        "forum": "dQVtTdsvZH",
        "replyto": "dQVtTdsvZH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3782/Reviewer_cm4G"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3782/Reviewer_cm4G"
        ],
        "content": {
            "summary": {
                "value": "This paper presents the content-motion latent diffusion model (CMD) for video generation. The CMD model consists of an autoencoder and two diffusion models learned in two stages. In the first stage, the autoencoder is trained to map an input video into a lower-dimensional latent representation. This latent representation consists of a single \u201ccontent frame\u201d (a weighted sum of the input frames) and a latent vector encoding motion information. In the second stage, CMD fine-tunes a pre-trained image diffusion model to learn the distribution of the content frames and additionally trains another diffusion model to generate the latent motion representation conditioned on a given content frame. CMD is compared to several baselines on popular video generation benchmarks such as UCF-101 and WebVid-10M and achieves a comparable or better performance (measured by metrics such as FVD and CLIPSIM) while maintaining lower memory and computational costs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed CMD model achieves a comparable or better performance (measured by metrics such as FVD and CLIPSIM) while maintaining lower memory and computational costs against several baselines.\n\n- CMD can utilize a pre-trained image diffusion model which can save training costs. \n\n- With minor exceptions (addressed in the questions section), the paper is written clearly. \n\n- To the best of my knowledge, the CMD model is novel and would be of interest to the ICLR community."
            },
            "weaknesses": {
                "value": "- Since the focus is on relatively short videos (16 frames), it would be interesting to establish whether CMD\u2019s downstream performance is still favorable as the number of frames increases.\n\n- The presented results can be even stronger if confidence intervals are provided."
            },
            "questions": {
                "value": "- Would it be feasible to include confidence intervals for results in Tables 1-3? \n\n- Could the authors provide more intuition behind the 2D-projection-based encoding for the motion latent?\n\n- In Figure 5, wouldn\u2019t the comparison between different models\u2019 computational & memory consumption be represented more accurately if there is a single bar for the CMD model with three stacked parts (autoencoder, content frame DM, motion latent DM)? For example, in the Memory panel, it would show that CMD has higher memory consumption than LVDM. \n\n- Additionally, how were the baselines (ModelScope, LVDM) chosen in Figure 5? \n\n- Are the authors planning to release an implementation of their proposed framework?\n\nTypos:\n- Abstract: \u201ccan directly utilizes a pretrained\u201d -> \u201ccan directly utilize a pretrained\u201d."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3782/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698778522948,
        "cdate": 1698778522948,
        "tmdate": 1699636334909,
        "mdate": 1699636334909,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sOT7axT8CJ",
        "forum": "dQVtTdsvZH",
        "replyto": "dQVtTdsvZH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3782/Reviewer_jHjr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3782/Reviewer_jHjr"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to decompose a  video clip into a content frame and a motion vector, both of which are then generated by two latent diffusion models accordingly. The proposed method largely accelerates text-to-video generation while maintaining high generation qualtiy."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is well-written and easy to follow. The proposed idea is illustrated clearly.\n\n2. The idea of training an auto-encoder to decompose the video into content & motion is novel and dose help accerlate the generation of videos.\n\n3. The experiments are adequate to illustrate the superiority of the proposed method."
            },
            "weaknesses": {
                "value": "1. Constrained by computational resources, the auto-encoder can only handle a limited number of frames at one time, which means that the auto-encoder can only encode motions within a limited interval. The reduced computational cost does not help us to explore the generation of longer videos.\n\n2. The content code is represented by the weighted sum of original frames, which may limit the representing ability of the content code. For example, the key components of a video may appear in different frames, (e.g. one person in frame 1 and another person in frame 2), in which case, it is difficult to maintain the information all key components in the content code."
            },
            "questions": {
                "value": "What is the resolutoin of the generated videos in Table 1, 2 and 3?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3782/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698818473994,
        "cdate": 1698818473994,
        "tmdate": 1699636334819,
        "mdate": 1699636334819,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9Z1RBMoQRn",
        "forum": "dQVtTdsvZH",
        "replyto": "dQVtTdsvZH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3782/Reviewer_cbVn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3782/Reviewer_cbVn"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes to generate videos by decomposing the latent representation into content-related features and motion features. The proposed method consists of two blocks: decomposed autoencoder and diffusion models for motion sampling. The proposed method generates plausible videos."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1) The paper is well-written and easy to follow.\n\n2) The proposed method achieves state-of-the-art performance in video generation.\n\n3) It makes sense to decompose the video generation into two parts, the content generation and the motion part."
            },
            "weaknesses": {
                "value": "1) The idea of decomposing the video generation into two factors has been proposed in previous methods [a, b, c].\n\n[a] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, Bo Dai. AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning\n\n[b] Yuming Jiang, Shuai Yang, Tong Liang Koh, Wayne Wu, Chen Change Loy, Ziwei Liu. Text2Performer: Text-Driven Human Video Generation\n\n[c] Haomiao Ni, Changhao Shi, Kai Li, Sharon X. Huang, Martin Renqiang Min. Conditional Image-to-Video Generation with Latent Flow Diffusion Models.\n\n2) In the training of autoencoder, how to ensure the model learns meaningful motion representation, rather than directly copying the information containing content information from the inputs?\n\n3) The content frame is an average of the latent representations of multiple frames, and this would lead to the generated frames being blurry, especially for the first frame. From the results of Fig. 10 and Fig. 11, the first frames are kindly of blurry."
            },
            "questions": {
                "value": "Please see my concerns in the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3782/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699169080948,
        "cdate": 1699169080948,
        "tmdate": 1699636334680,
        "mdate": 1699636334680,
        "license": "CC BY 4.0",
        "version": 2
    }
]