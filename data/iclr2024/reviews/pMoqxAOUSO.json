[
    {
        "id": "7IIYImoodr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3303/Reviewer_C4KL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3303/Reviewer_C4KL"
        ],
        "forum": "pMoqxAOUSO",
        "replyto": "pMoqxAOUSO",
        "content": {
            "summary": {
                "value": "This paper proposes a new 4D point cloud sequence understanding method. The major techniques are 4D tokenization, token updating and token merging. The proposed approach demonstrated strong performance in HOI4D action segmentation and Semantic KITTI online semantic segmentation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I'll update my scores after the rebuttal.\n\n1. The proposed method demonstrated strong performance on two benchmarks. Moreover, their experimental results show that the proposed module can be used in various SOTA 4D backbones, which is very beneficial.\n\n2. The proposed modules are somewhat novel and may inspire other works. I think the geometry and motion factorization is novel. I feel it is novel because it is not direct to see how this is implemented. I think the token operation modules might be insightful for future work.\n\n3.  The writing is clear, and various analyses support this method."
            },
            "weaknesses": {
                "value": "1. I'm not familiar with this sort of work in action segmentation. Probably, some strong baselines are missing. Also, ICLR is probably not the best venue for this vision work. Also, the plug-and-play strategy may not be particularly novel in this area. I know PTv2 very well, but I am not sure whether this approach should compare to other methods for Semantic KITTI tasks, such as SphereFormer or other works. It seems that mIoU in Table 2 is pretty low (<55%). \n\n2. Probably, this model is time-consuming because of the complex reasoning procedure.\n\n3. Some parts of this submission are confusing to me:\n\n(1) In the introduction, the authors say, \"Querying a dynamic reconstruction for historical spatial-temporal contexts would be much more efficient compared with directly querying the raw 4D data.\" But I do not see this directly. From my experience, raw 4D data is fast to index and insert because of the regular 4D grid structures. More evidence is needed to support this fact. The authors may use some references, diagrams, or experiments to illustrate this. Also, this sentence connects poorly with other sentences.\n\n(2) In the introduction, how the 4D sequence is factorized is unclear, but this is crucial in this method, I believe. Figure 1 is not that clear as well. The point cloud is messy, and the diagram is not informative.\n\n(3) It is weird to say, \"We propose a new **diagram** that serves as a **plug-and-play** framework\" because a new diagram means a novel framework. Still, the plug-and-play module means a new module to put into existing frameworks. It's unsure whether this can be called a framework-level contribution.\n\n(4) The problem statement part seems confusing. First, authors should provide mathematical symbols, which can lay the basis for their subsequent model description. Second, I'm unsure what \"comparing online perception to offline perception\" means. It seems that this is a minor technical detail. And why this is important is not explained. Furthermore, I cannot connect the challenges listed here to the introduction. It seems that the challenges introduced here are not the same as the introduction. Furthermore, too many problems are raised in this paper and the reader's attention is easily scattered."
            },
            "questions": {
                "value": "1. What is the number of parameters used in this model? How is the speed?\n\n2. Can the proposed method generalize to other 3D scene representations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3303/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697265277078,
        "cdate": 1697265277078,
        "tmdate": 1699636279477,
        "mdate": 1699636279477,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iwzFMgHRNC",
        "forum": "pMoqxAOUSO",
        "replyto": "pMoqxAOUSO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3303/Reviewer_KTef"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3303/Reviewer_KTef"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces NSM4D, a plug-and-play strategy designed to enhance existing 4D point cloud sequence perception backbones, particularly in their ability to understand long sequences with flexible length. The approach factorizes a 4D sequence into geometry and motion through implicit dynamic 3D reconstruction. The 4D tokenization module aggregates geometry and motion features from the current observation, with the token updating module using scene flow to align historical information with the current one. The token merging module manages new tokens and maintains a reasonable token size across different sequence lengths."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is well-written and easily comprehensible.\n2. The motivation behind the model design is reasonable, with tokenization offering a scalable approach for handling variable sequence lengths and efficient querying.\n3. The plug-and-play design is effectively evaluated for certain tasks."
            },
            "weaknesses": {
                "value": "The major concern of the paper is the absence of a long-term task evaluation. This absence makes it challenging to ascertain the method's effectiveness in handling long-term sequence information, which is a crucial aspect of the paper's proposed approach. The evaluation primarily focuses on tasks that require short-term temporal information, leaving a significant gap in assessing the method's claims regarding its long-term sequence information retention and query capabilities. \n\nPlease see questions for more details."
            },
            "questions": {
                "value": "1. The depiction of the pipeline in Figure 1 is somewhat ambiguous and needs clarification for a more precise understanding of the processes involved.\n2. While the method claims to handle long history sequences effectively, the evaluated tasks seem to focus on relatively short-term temporal information rather than requiring long-term information querying. It would be valuable to include a long-term task evaluation for a more comprehensive assessment.\n3. An efficiency analysis is required to determine the additional time cost, FLOPs, and weights introduced by the proposed method.\n4. The method relies on motion estimation from scene flow. Assessing the accuracy of this motion estimation is crucial. For instance, if ground truth motion is provided, how much improvement could be gained?\n5. A discussion of the limitations is needed to identify the current design's bottlenecks and areas for improvement.\n6. Additional visualizations of task performance evaluations would enhance the paper's comprehensibility.\n7. In Figure 1 supplementation, the term \"Synthia4D\" is mentioned but not explained or introduced in the paper; further clarification is needed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3303/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3303/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3303/Reviewer_KTef"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3303/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698517347732,
        "cdate": 1698517347732,
        "tmdate": 1699636279400,
        "mdate": 1699636279400,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bPGsvauk81",
        "forum": "pMoqxAOUSO",
        "replyto": "pMoqxAOUSO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3303/Reviewer_XSLL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3303/Reviewer_XSLL"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces NSM4D, a new approach for understanding 4D point cloud sequences online. \nNSM4D uses a neural scene model to factorize geometry and motion information, dynamically updating itself with new observations. \nIt improves over the baselines on online perception capabilities for both indoor and outdoor scenarios and shows robustness to sensor noise."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "A straightforward component can be integrated into existing 4D frameworks.\n\nAlso, It shows an improvement over the baselines in terms of real-time perception for various scenarios and exhibits resilience to sensor disturbances."
            },
            "weaknesses": {
                "value": "1. The experiments results are a bit of confusion to me, and may be not sound or solid. The pptr segmentation results on HOI4D doesn't match the nubmers on pptr paper (pptr achieves 68.54 miou in the original paper with 3 frames, yet it only achieves 42.73 in Table3). Similarly, P4Transformer is also way off. There isn\u2019t clear explanation provided for these discrepancies (except the flip-stitch, which unlikely to cause this big difference). To evaluate the effectiveness of the proposed methods, it\u2019s crucial to compare performance with the baselines from the original papers. If there are changes in the settings or evaluation methods, these should be clearly stated and justified. Please correct me if I\u2019m wrong, as I have not previously worked on HOI4D.       \n4. The speed of the method is a concern. Given that RAFT3D need to applied to obtain the flow of past frames, I anticipate that the process would be quite slow. This is a critical issue for a method termed \u201conline\u201d scene understanding, which typically necessitates high efficiency.\n2. The novelty of the work is somewhat questionable. The concept of \u201cdecoupling geometry and motion\u201d doesn\u2019t resonate with me as it seems to be just a feature representation with explicit location.\n3. The writing is a bit wordy and not that clear. e.g. the beginning of Section 4 (NEURAL SCENE MODEL) is almost just a repeat of the introduction."
            },
            "questions": {
                "value": "Please provide clarification on the experiment numbers mentioned in the weakness section.\n\n It would be greatly appreciated if you could use the settings of the baselines (e.g., the settings mentioned in table 2 of the pptr paper) for a clearer comparison. Additionally, it would be helpful to have a more precise comparison with ptv2 on semantickitti. Ideally, please consider submitting your results to the test server."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3303/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3303/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3303/Reviewer_XSLL"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3303/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698639022357,
        "cdate": 1698639022357,
        "tmdate": 1699636279306,
        "mdate": 1699636279306,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sLKd2zuv07",
        "forum": "pMoqxAOUSO",
        "replyto": "pMoqxAOUSO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3303/Reviewer_Usdf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3303/Reviewer_Usdf"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an online 4D feature extractor that takes a 3D pointcloud sequence as an input and output set of tokens for each frame in an online fashion. The model needs to take scene flow predictions as input and then explicitly track points and aggregate features. The author claimed that by combining their model with current 3D perception backbones, they can achieve state-of-the-art segmentation results on HOI4D and SemanticKITTI."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The versatility of its design, making it adaptable to most state-of-the-art 3D backbones, is one strength."
            },
            "weaknesses": {
                "value": "- The presentation of this paper needs to be improved. \n    - This paper contains lots of cluttered sentences. Lots of the sentences in this paper are long and sometimes meaningless, making readers hard to follow. It would be beneficial to rewrite lots of it more concisely. For example, the first paragraph in section 4: \"Our goal is to devise a methodology that is both effective and efficient for sequentially aggregating long-term historical information for precise 4D online perception. NSM4D aims to achieve this by introducing a generic paradigm design that enables seamless integration with the existing 4D networks, empowering them with the capability of efficient and effective online 4D perception.\"    This paragraph can be rewritten in a clearer and more concise way: We devise a method to aggregate 3D pointcloud sequences online called NSM4D. NSM4D can integrate with existing 4D networks seamlessly for online 4D perception.\n    - Figures are hard to understand.\n        - Figure 1 is referenced in Section-1, but it is hard to understand without finishing reading Section-4. Moreover, there isn't too much useful information in Figure-1.\n        - Figure-2 has lots of information, but it is still hard to understand every component. Also, it would be beneficial to add (a), (b), (c) under three parts of Figure-2.\n- Experimental evaluation is not solid. There is a problem in Table 2. I checked the SemanticKiTTI leaderboard, https://paperswithcode.com/sota/3d-semantic-segmentation-on-semantickitti, and Table 2, 3 of paper SPVCNN. The number of baseline methods need to be corrected. For example, the mIoU of SPVCNN is above 60% in their table-2,3. And PTv2 achieved around 70% mIoU according to the leaderboard. However, this paper only reports mIoU of 49.70% and 51.13% for SPVCNN and PTv2, respectively. Also, experiments on more datasets would be beneficial."
            },
            "questions": {
                "value": "- There are some missing details about the method.\n    - How many points are sampled at every frame. \n    - Why use uniform sampling, rather than furthest points sampling, or stratified sampling? \n    - Size and computational cost of the model.\n- This paper uses the word: \"efficient\" to describe their methods several times. It would be beneficial to provide some analysis and comparisons on running speed or computational cost to justify this."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3303/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698797987915,
        "cdate": 1698797987915,
        "tmdate": 1699636279231,
        "mdate": 1699636279231,
        "license": "CC BY 4.0",
        "version": 2
    }
]