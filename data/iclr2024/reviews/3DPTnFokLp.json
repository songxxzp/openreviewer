[
    {
        "id": "zbVFmXVcTX",
        "forum": "3DPTnFokLp",
        "replyto": "3DPTnFokLp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5452/Reviewer_8MxM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5452/Reviewer_8MxM"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a codebase to implement and compare multimodal VAEs in a unified setting. In addition they propose the CdSprites+ dataset, a multimodal variant of dSprites, where images are paired with matching textual descriptions, and a difficulty level can be selected among five different ones."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The paper tackles the relevant problem of evaluation of multimodal VAEs\n- The idea of providing a multimodal benchmarking dataset where one can vary the difficulty level is interesting.\n- The paper is clear and well-structured."
            },
            "weaknesses": {
                "value": "In spite of the fact that coming up with better benchmarks for multimodal VAEs is an important problem, I do not think that the relevant aspects of this problem are tackled in this paper. And the proposed benchmark does not bring novelty in the landscape of multimodal datasets used for evaluation. In particular\n- PolyMNIST [1] is already a quite successful synthetic benchmark for multimodal VAEs [1,2,3,4]. As a more challenging dataset, Translated PolyMNIST [1], was proposed. Note that the same idea to augment the difficulty of the dataset (making the shared object smaller and shuffle its position in the image) was used in this work. Hence the idea cannot be labelled as novel, and probably it would be necessary to cite [1] in this context. \n- The other peculiarity in designing the CdSprites+ dataset is having matching captions as an additional modality. However, the recently proposed Multimodal3DIdent [5][6] is based on a very similar idea, and provides a more realistic setting compared to CdSprites+. In particular, the authors provide the code in a toolbox to generated images such that different versions of the dataset (more and less challenging) can be obtained. \nIn summary, I think the dataset proposed in this paper does not bring sufficient novel ideas to the community, and the evaluation metrics used (e.g. using pre-trained classifiers for coherence, and FID scores for generative quality) largely build on what is adopted in previous works [7,1,2,3,4].\nI think a more interesting side to investigate about benchmarking multimodal VAEs is the need for realistic datasets, where an objective quantitative evaluation of multimodal VAEs is still possible. For instance, general generative coherence is hard to objectively evaluate in real-world scenarios as e.g. CUB Image-Captions [7, 3, 4]. \n\nIn addition to this, I believe there are other weaknesses in the paper that should be addressed \n-Third paragraph in 2.1 seems outdated. A recent work that compares multimodal VAEs comprehensively is [4], where the authors show that their proposed model well-performs when jointly looking at all criteria (generative quality and generative coherence are the main focus, but also latent factorisation and synergy seem to achieved when looking at the results in the Appendix). Note this work is not cited in the paper.\n- In spite the aim of the authors is to provide a toolbox to systematically evaluate existing multimodal VAEs, recent relevant work is not implemented [2,4].\n- When stating \"However, these images are too complex to be generated by the state-of-the-art models (as proven by Daunhawer et al. (2022)) and the authors thus only use their features and perform the nearest-neighbour lookup to match them with an actual image (Shi et al., 2019).\". This is not exact as [4] successfully tackle this setting when generating in pixel space.  \n- Generally, the paper in various parts builds on the assumptions that so far multimodal VAEs have been evaluated inconsistently, while especially more recently certain benchmarks such as PolyMNIST, as well as metrics such as generative coherence with pre-trained classifiers and generative quality via FID scores, are fairly standard and well-established [1,2,3,4].\n\n[1] Thomas M. Sutter, Imant Daunhawer, and Julia E Vogt. Generalized multimodal ELBO. In International Conference on Learning Representations, 2021. [2] HyeongJoo Hwang, Geon-Hyeong Kim, Seunghoon Hong, and Kee-Eung Kim. Multi-view representation learning via total correlation objective. In Advances in Neural Information Processing Systems, 2021. [3] Imant Daunhawer, Thomas M. Sutter, Kieran Chin-Cheong, Emanuele Palumbo, and Julia E Vogt. On the limitations of multimodal VAEs. In International Conference on Learning Representations, 2022. [4] Emanuele Palumbo, Imant Daunhawer, and Julia E Vogt. MMVAE+: Enhancing the generative quality of multimodal VAEs without compromises. In International Conference on Learning Representations, 2023. [5] Imant Daunhawer, Alice Bizeul, Emanuele Palumbo, Alexander Marx, Julia E Vogt. Identifiability Results for Multimodal Contrastive Learning. In International Conference on Learning Representations, 2023. [6]  Alice Bizeul, Imant Daunhawer, Emanuele Palumbo, Bernhard Sch\u00f6lkopf, Alexander Marx, Julia E Vogt. 3DIdentBox: A Toolbox for Identifiability Benchmarking. CLeaR, 2023. [7] Yuge Shi, N. Siddharth, Brooks Paige, and Philip Torr. Variational mixture-of-experts autoencoders for multi-modal deep generative models. In Advances in Neural Information Processing Systems, 2019."
            },
            "questions": {
                "value": "As a suggestion, as stated above as well, when tackling the problem of enhancing the current state of multimodal VAEs evaluation, I'd encourage the authors to focus more on realistic datasets to objectively evaluate multimodal VAEs, or in coming up with new quantitative proxies for performance criteria that are challenging to evaluate in realistic settings (e.g. semantic coherence)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5452/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697899872855,
        "cdate": 1697899872855,
        "tmdate": 1699636554983,
        "mdate": 1699636554983,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "b7mK70j8IC",
        "forum": "3DPTnFokLp",
        "replyto": "3DPTnFokLp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5452/Reviewer_naTF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5452/Reviewer_naTF"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel benchmark and dataset for evaluating multimodal VAEs. The evaluation consists of two parts: a dataset-dependent part and a dataset-independent part.\nIn the dataset-independent part, the reconstruction loss (negative log-likelihoods), KL-divergence, and 2d-visualization of latent space embeddings are reported. In the dataset-dependent part, the evaluation focuses on label-based metrics, e.g. latent representation classification accuracy and the coherence of generated samples. 4 different multimodal VAE methods are evaluated.\nThe proposed dataset is an adapted bimodal version of the well-known dSprites dataset. It offers 5 different levels of increasing complexity of both image and text data."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- given the promise of multimodal learning and the ubiquity of multimodal data, establishing benchmarks and benchmarking datasets for multimodal VAEs is important and necessary\n- introducing a novel dataset where there are multiple levels of complexity is important for scaling up multimodal VAEs to more difficult tasks and datasets"
            },
            "weaknesses": {
                "value": "- dataset: although I like the dataset with its different complexity levels, I am not sure whether a bimodal dataset is able to fully capture the difficulties of multimodal data. In Sutter et al., 2021, the differences in performance between MVAE, MMVAE, and MoPoE depending on the size of the input subset have been shown. In addition, more than two modalities allow for more complicated relationships between modalities than just information is either shared between all modalities or modality-specific.\n- Limited related work: for a benchmark paper, I found the related work section to be a bit short and only a subsection of multimodal VAEs to be discussed. First, there are two (rather) recent review papers on multimodal learning (Liang et al., \"Foundations and recent trends in multimodal machine learning: Principles, challenges, and open questions\", 2022 [link to review paper](https://arxiv.org/abs/2209.03430), Suzuki and Matsuo, \"A survey of multimodal deep generative models\", 2022 [link to review paper](https://www.tandfonline.com/doi/full/10.1080/01691864.2022.2035253).\nMore specifically, there are more works on multimodal VAEs that are similar to the ones discussed:\n    - Sutter et al., \"Multimodal Generative Learning utilizing the Jensen-Shannon divergence\", 2020\n    - Daunhawer et al., \"Self-supervised Disentanglement of Modality-Specific and Shared Factors Improves Multimodal Generative Models\", 2020\n    - Palumbo et al., \"MMVAE+: Enhancing the Generative Quality of Multimodal VAEs without Compromises\", 2023\n    - I do not expect that all these methods and models have to be included. However, I would expect them to be listed in the related work section. In addition, it would be good to include the selection criteria for the models/methods implemented in the benchmark.\n- for me, to understand the usefulness and especially usability of the proposed benchmark, a diagram of the code structure or some other way to visualize the implemented coding structure would be very helpful. There is no evidence or explanation that tells me what makes it easy to include a novel dataset or method in the benchmark. Anything that supports this would be helpful in my opinion."
            },
            "questions": {
                "value": "- I wonder whether single best performances for different metrics are the best way to evaluate these models. I would like to have your opinion about reporting metrics for a range of hyperparameters. E.g. the latent representation performance for multiple beta values (e.g. beta 0 [0.1, 0, 1.0, ...])? Or for the latent space dimension. Wouldn't this provide a more thorough picture of a method's performance?\n- does it make sense to quantify the coherence of images based on the ratio of correct features? Is this metric able to grasp the semantic coherence?\n- image and text are some of the most prevalent modalities. Given the multitude of already existing image-text datasets, as mentioned in the proposed paper, would it not be interesting for a multimodal benchmark to include different modalities and more than just two? I would like to hear your opinion on that."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5452/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698447649051,
        "cdate": 1698447649051,
        "tmdate": 1699636554892,
        "mdate": 1699636554892,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EyOYMRSOhQ",
        "forum": "3DPTnFokLp",
        "replyto": "3DPTnFokLp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5452/Reviewer_2Pvg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5452/Reviewer_2Pvg"
        ],
        "content": {
            "summary": {
                "value": "The author's present a toolkit for benchmarking multimodal VAEs, a topic where exhaustive and objective model comparison is hard. The tools enables adding new datasets and multimodal VAEs in a modular way"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "As someone that has worked in the topic, I was aware of the lack of libraries that allow for easy comparison of multimodal VAEs. This paper brings tools to solve this."
            },
            "weaknesses": {
                "value": "A single dataset is included, which limits impact. \n\nHow Image quality generation is measured? Could this be adapted to other multimodal generative models?"
            },
            "questions": {
                "value": "See above comments."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5452/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698758155923,
        "cdate": 1698758155923,
        "tmdate": 1699636554764,
        "mdate": 1699636554764,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3VMR55eHHe",
        "forum": "3DPTnFokLp",
        "replyto": "3DPTnFokLp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5452/Reviewer_EZ4B"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5452/Reviewer_EZ4B"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a bimodal benchmark dataset for multimodal VAE study, called Captioned disentangled Sprites (CdSprites). The original Sprites dataset comprises images of objects with different shapes, positions, colors, etc. This proposed new dataset adds captions to the images as the second modality. The tested multimodal VAEs are expected to model both the text and image modality, and are expected to be capable of reconstructing the image and text. Authors chose four multimodal VAEs, MMVAE, MVAE, MoPoE, DMVAE to validate their respective performance on this new dataset, in both prediction and reconstruction power. The new dataset has 5 different levels of difficulty, and an automated visualization tool is provided to analyze models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper provides a new dataset which contains both image and text modalities, as well as 5 difficulty levels, facilitating the multimodal learning field.\n2. The authors take into account 4 state-of-the-art multimodal VAE for evaluation and compared the new dataset with multiple other commonly used datasets.\n3. Multiple metrics including prediction, likelihood and reconstruction are incorporated into evaluation."
            },
            "weaknesses": {
                "value": "1. In Figure 1, regarding the captions, it's a bit vague to tell if the square or heart is big or small unless the model has a sense of the whole dataset.\n2. If you use MMVAE, MVAE, MoPoE, DMVAE as 4 representative models for testing, you should elaborate on how are models designed and whey they are representative among all the multimodal VAEs, as well as their distinctions.\n3. In section 2.2, while capturing the modalities of image and single class label may be oversimplified, recent works have explored bimodality of feature and multi-label tags [1], which is more challenging while facilitating the test of the alignment in the latent space.\n4. While your title says it's multimodal, essentially this paper investigates the bimodal scenario. If you incorporate more modalities, the dataset would be more interesting.\n5. While reconstruction and likelihood could be good indicators, for VAE we are more intrigued by the generation power, especially on the text part. I can see some latent traversal results on the image recon from the appendix, but would there be any similar studies on the text side?\n\n\n[1] Bai, J., et al. Gaussian mixture variational autoencoder with contrastive learning for multi-label classification. In ICML 2022."
            },
            "questions": {
                "value": "1. To some extent, your level 1-5 is like different granularities regarding the disentangled representation learning. Could you further elaborate on the connections and distinctions between multi-modalities and disentangled representation learning?\n2. I can see the cross-sampling qualitative results are less satisfactory. Have you tried to use better text and image encoder before feeding into VAE?\n3. It seems that your dataset is somewhat related to some Vision Question Answering (VQA) tasks, which also require capturing both modalities?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5452/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699168028860,
        "cdate": 1699168028860,
        "tmdate": 1699636554668,
        "mdate": 1699636554668,
        "license": "CC BY 4.0",
        "version": 2
    }
]