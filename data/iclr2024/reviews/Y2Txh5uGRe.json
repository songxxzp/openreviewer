[
    {
        "id": "VKcqAiizQM",
        "forum": "Y2Txh5uGRe",
        "replyto": "Y2Txh5uGRe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8626/Reviewer_qy29"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8626/Reviewer_qy29"
        ],
        "content": {
            "summary": {
                "value": "The paper presents an approach called Text2Data, which utilizes unlabeled data via an unsupervised diffusion model, then optimizes control to prevent forgetting. It efficiently utilizes both labeled and unlabeled data and employs a learning objective based on constraint optimization to prevent catastrophic forgetting during fine-tuning. The constrained optimization objective is based on lexicographic optimization. It minimizes the negative log-likelihood on labeled data, subject to the unsupervised loss on unlabeled data being less than some threshold. Experimental results show that Text2Data surpasses other benchmark methods in terms of generation quality and controllability."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Proposed method has two main stages:\n1. Learn the data distribution from unlabeled data through an unsupervised diffusion model.\n2. Finetune the model on limited labeled data using a novel constrained optimization objective. This aims to achieve controllability while preventing catastrophic forgetting of the distribution learned in stage 1.\nThe lexicographic approach involves first optimizing L2 as the primary objective without any constraints. \nThen L'1 is optimized as a secondary objective by adding the constraint L'1 <= \u03be, where \u03be is determined by the minimum value of L'1 from the first optimization.  Thus it can balance labeled data fitting and remembering unlabeled data distribution."
            },
            "weaknesses": {
                "value": "1.There is no comparison to other techniques like data augmentation or transfer learning for low-resource text-to-data generation. Comparisons would be useful to know if Text2Data outperforms them.\n2.The threshold \u03be for the constrained optimization is set heuristically based on the minimum value of unsupervised loss L1. \n3.Catastrophic forgetting is identified as an issue, but not experimentally verified by comparing with unconstrained finetuning. This would further prove the need for constrained optimization."
            },
            "questions": {
                "value": "My mainly technical concern on this paper is mainly on how to come up with thresholds for constrained optimization.\n\nAnother point is that the use of text annotation is perplexing. Given that prompts are either obtained from existing datasets or synthesized using templates and advanced language models like GPT-3, one might question: What is the fundamental difference between this approach and data augmentation (or using a classifier guidance)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8626/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698522815542,
        "cdate": 1698522815542,
        "tmdate": 1699637078987,
        "mdate": 1699637078987,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FznISbuHZE",
        "forum": "Y2Txh5uGRe",
        "replyto": "Y2Txh5uGRe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8626/Reviewer_m4kw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8626/Reviewer_m4kw"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a framework for data generation with controlling signal (e.g., text) where the controlling signal data is limited. To solve the data-scale challenge, the paper proposes to firstly train an unconditional generative model and then finetune the unconditional model with the limited paired dataset with controlling signal. To prevent the model from catastrophic forgetting, the paper proposes a constraint objective to keep the finetuned parameter space close to that of pretrained parameter."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The challenge addressed in this paper is meaningful. The paper is well-written and easy to read. The motivation of data-scale challenge and the solution is convincing.\n2. The proposed method is verified on three different low-resource tasks to demonstrate the effectiveness."
            },
            "weaknesses": {
                "value": "1. As for the method, the proposed method is relevant to the classifier-free guidance, which both involves a conditional model and an unconditional model (and the two models share the parameters). The difference is that the proposed method: finetune the unconditional model while adding constraint for catastrophic forgetting and the classifier-free guidance: guidance the denoising direction (in diffusion model) of conditional model with the help of unconditional model. I think the discussion of why the proposed method is better than the classifier-free guidance is necessary.\n\n2. As for the related work, the paper should discuss the related work about transfer learning, where catastrophic forgetting is an important topic. Since one of the contributions of the paper is preventing catastrophic forgetting. I think the paper should compare the proposed method with other works about catastrophic forgetting.\n\n3. As for the experiments, I understand the proposed method is proposed for low-resource condition. However, the proposed method should be verified on popular tasks such as image generation (maybe the author can show that the proposed method can achieve a high performance with only 5% data compared with the conventional image generation methods). Currently the 3 task in paper is not quite convincing. And the time series task even has no previous published baselines (the baseline model is also proposed by the author)."
            },
            "questions": {
                "value": "1. How the constraint in Eq. 6 is achieved? Whether it is applied with an auxiliary loss function? If so, which kinds of loss function?\n\n2. What is the text description in  time series task? Can you give some examples? I am also curious that why the time series task (seems) does not conduct prediction task? Or can you give a more detailed description about the time series task?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8626/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698638594568,
        "cdate": 1698638594568,
        "tmdate": 1699637078877,
        "mdate": 1699637078877,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mfW9ryPfV6",
        "forum": "Y2Txh5uGRe",
        "replyto": "Y2Txh5uGRe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8626/Reviewer_Yf2d"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8626/Reviewer_Yf2d"
        ],
        "content": {
            "summary": {
                "value": "This work studies data generation conditioned on textual control under low-resource scenarios. The key idea is to conduct large-scale pre-training to learn the data marginal distribution; Then solving the conditioned generation while mitigating the catastrophic forgetting issue."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The authors tackle the catastrophic forgetting issue of the pre-training then fine-tuning strategy via solving a constrained optimization problem. The authors provide a textual description on how they come to the objective function, from equation 4 to 5 and then to 6; A proof is proved to show that the relaxation from 5 to 6 is fine given some assumptions.\n\n\n\nThe proposed baselines and the evaluation metrics all make sense. The authors manage to show that their constrained optimization can provide benefits. Actually, the proposed method shows benefits in the evaluations the authors have conducted."
            },
            "weaknesses": {
                "value": "Details/Clarity are missing:\n\n\u2014 The authors claim that it is \u201cusually easy to obtain by either manual collection of simulation\u201d to get the unlabeled data. What\u2019s the estimated manual data collecting effort and procedure? How do the authors design a system to do simulation? If data synthesis is based on an existing generative model, to train such a high-quality generative model already requires a lot of data.\n\n\u2014 The authors claim they use a cross-attention layer, but some implementation details are missing.\n\n\u2014 What\u2019s the pre-training data? Do the authors use the same dataset of the downstream task for pre-training? By reading the 2nd paragraph of page 6, here\u2019s my understanding: The authors identified N datasets with textual description. The authors create the synthesized \u2018low-resource\u2019 scenario by sampling varying proportions of the dataset. Pre-training is done on the full dataset for each concrete problem. \n\nSome more questions/comments:\n-- The authors should have a baseline that trained on the 100% of the labeled data.\n\n-- Please include proportions that are larger than 40 as reference.\n\n-- What if the model is not in-domain pre-trained? For example, a generative model really pre-trained on large-scale data, but does not use the downstream dataset.\n\n-- The authors aim to solve the problem under a low-resource scenario. However, the proposed methods do not always show benefits when using 2%, 4% and 6% data, according to Table one and two."
            },
            "questions": {
                "value": "Questions are posted in Weaknesses section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8626/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8626/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8626/Reviewer_Yf2d"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8626/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699200518167,
        "cdate": 1699200518167,
        "tmdate": 1700969800018,
        "mdate": 1700969800018,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hO1xqSh6qO",
        "forum": "Y2Txh5uGRe",
        "replyto": "Y2Txh5uGRe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8626/Reviewer_L6cR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8626/Reviewer_L6cR"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a model for performing semi-supervised learning that first trains a model to maximise marginal likelihood\n and then fine tunes the same on labelled data by enforcing a constraint that the marginal likelihood of the fine-tuned model should not deviate much from the starting point. This is modelled as a constraint during optimisation.\n\nThe authors introduce a relaxation for the constraint and prove that under the relaxed constraint, the set of \\theta that minimises the empirical loss contains the true \\theta (the one that minimises the true loss)"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The primary contribution of this paper is the constrained-optimisation objective  and the generalisation bound on the constrained optimisation problem. Since learning theory is not my area of expertise, it is difficult for me to evaluate the generalization bound contribution in terms of correctness and novelty."
            },
            "weaknesses": {
                "value": "The approach of optimising marginal likelihood on unlabelled data and conditional likelihood on labelled data is very common in semi-supervised learning literature [1]. The primary difference is that here the authors have also included the marginal likelihood on unlabelled data as a constraint. This would have been interesting if the authors had discussed how that constraint is used during training batch-by-batch. Unfortunately the algorithm for training is missing from the paper.\n\nThere are lots of other details missing from the paper as well. For instance, how is $\\epsilon_\\theta$ modelled? How can it be a function of x at one point and x and c at the other point. Perhaps the authors have assumed that a lot of stuff to be obvious.\n\nThere are no novel insights from the paper. It is not clear why a constrained optimisation objective is superior over other forms of semi-supervised learning. This weakens the paper a lot.\n\n[1] Semi-Supervised Learning with Deep Generative Models\" Authors: Diederik P. Kingma, Danilo J. Rezende, Shakir Mohamed, Max Welling"
            },
            "questions": {
                "value": "Included in weaknesses section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8626/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699604725783,
        "cdate": 1699604725783,
        "tmdate": 1699637078612,
        "mdate": 1699637078612,
        "license": "CC BY 4.0",
        "version": 2
    }
]