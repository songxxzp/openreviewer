[
    {
        "id": "UFeagfVPje",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7760/Reviewer_hiUg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7760/Reviewer_hiUg"
        ],
        "forum": "5TlHjMVrNG",
        "replyto": "5TlHjMVrNG",
        "content": {
            "summary": {
                "value": "This paper introduces a set of 18 new attacks for evaluating unforeseen robustness and propose a metric UAR2 which uses these attacks for measuring the performance of existing models.  The authors then evaluate existing models using this UAR2 metric to determine the impact of adversarial training and other defenses for the task of unforeseen robustness, impact of data augmentation, and impact pretraining and regularization."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I think that the direction of unforeseen robustness is an interesting and important problem in adversarial robustness which currently has not gained much attention.  I think that the proposed benchmark is a useful tool for evaluating current models, understanding how different training choices impact unforeseen robustness, and can motivate more research in the field.  I also find the presentation to be clear overall and am impressed by the large scope of evaluations presented across the paper and Appendix."
            },
            "weaknesses": {
                "value": "- I think that the paper can reference the work [1] which also looks at the problem of benchmarking unforeseen robustness and performs several ablations on the impact of factors like using synthetic data during adversarial training on unforeseen robustness.  This work differs from the MultiRobustBench paper as the MultiRobustBench paper focuses mainly on robustness for CIFAR-10 due to computational constraints and does not propose new attacks.\n- There are some results that are referenced in the main paper (mainly those which compare existing defenses against unforeseen attacks and the MNG approach) which are in the Appendix.  If possible, it would be good to move them into the main paper since I think these evaluations of adversarial training approaches are quite important.\n- It would also be interesting to also have a few more evaluations of existing defenses tailored to unions of lp attack robustness outside of the MNG approach (ie. the approach in [2] or [3] might be able to be used with ImageNet, or evaluating approaches in [4] or [5] with CIFAR-10) in order to have a better understanding of how well these approaches for multiattack robustness generalize to unforeseen attacks\n\n[1] Dai, S., Mahloujifar, S., Xiang, C., Sehwag, V., Chen, P. &amp; Mittal, P.. (2023). MultiRobustBench: Benchmarking Robustness Against Multiple Attacks. **Proceedings of the 40th International Conference on Machine Learning**, in **Proceedings of Machine Learning Research** 202:6760-6785 Available from https://proceedings.mlr.press/v202/dai23c.html.\n[2] Sriramanan, G., Gor, M., & Feizi, S. (2022). Toward Efficient Robust Training against Union of $\\ell_p $ Threat Models. **Advances in Neural Information Processing Systems**, 35, 25870-25882.\n[3] Croce, F., & Hein, M. (2022, June). Adversarial Robustness against Multiple and Single $ l_p $-Threat Models via Quick Fine-Tuning of Robust Classifiers. In International Conference on Machine Learning (pp. 4436-4454). PMLR.\n[4] Maini, P., Wong, E., & Kolter, Z. (2020, November). Adversarial robustness against the union of multiple perturbation models. In International Conference on Machine Learning (pp. 6640-6650). PMLR.\n[5] Tramer, Florian, and Dan Boneh. \"Adversarial training and robustness for multiple perturbations.\" Advances in neural information processing systems 32 (2019)."
            },
            "questions": {
                "value": "How different are the different forms of robustness tested across the benchmark?  For example, does robustness against something like glitch also give some robustness against JPEG attacks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7760/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7760/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7760/Reviewer_hiUg"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7760/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697213911434,
        "cdate": 1697213911434,
        "tmdate": 1700706552319,
        "mdate": 1700706552319,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QnEQbmjPTH",
        "forum": "5TlHjMVrNG",
        "replyto": "5TlHjMVrNG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7760/Reviewer_PXCR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7760/Reviewer_PXCR"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a benchmark to evaluate robustness against unforeseen adversarial attacks. To model unforeseen attacks, this benchmark includes 19 non-Lp adversarial example generation methods. These attacks are designed to be differentiable so the attacker can leverage gradient-based optimizers like PGD. Evaluations provide insights into several research questions, such as how Lp robustness measures relate to unforeseen robustness, what techniques can improve unforeseen robustness, and how classic progress on CV partially improved the unforeseen robustness."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "### Originality\n\n* The proposed benchmark and its attacks are mostly novel.\n* The evaluation provides several interesting insights into the new notion of adversarial robustness.\n\n### Quality\n\n* Evaluations are sound.\n* Methodology is sound, it is good to see that non-Lp attacks can optimize worst-case perturbation where the Lp constraint is on some internal parameters.\n\n### Clarity\n\n* The presentation is clear and easy to follow.\n\n### Significance\n\n* Studying robustness against unforeseen attacks is important and necessary, given that most focus is on Lp-norm robustness."
            },
            "weaknesses": {
                "value": "### Originality\n\nN/A\n\n### Quality\n\n**Q1: Unclear role of the proposed benchmark in the defense's development.**\n\nThis benchmark studies the problem of \"unforeseen\" attacks by labeling a set of new attacks as \"unforeseen,\" which has a nature limitation in the sense that once these attacks are evaluated, they are no longer \"unseen.\" From the current presentation, this benchmark should be evaluated once and dropped right away -- the defense should not explicitly optimize toward these attacks. For example, if a defense optimizes on some of these attacks, would they obtain higher robustness against the other remaining attacks? Would this action be treated as cheating by training on the test set?\n\nTo make the benchmark more useful, I feel it important to develop a validation/test split from these attacks. That is, the progress should be made to optimize against one subset of attacks, and evaluate their robustness against another subset of attacks. Only in this case one can really claim that they are improving the \"unforeseen robustness\" with this benchmark. This paper includes some evaluation by adversarial training with these attacks, and the performance indeed improves. However, this is obvious and cannot provide any confidence to even newer attacks, as from this paper's own argument, those newer attacks are now unseen compared with this benchmark's now-seen attacks. In this case, if the validation/test concept were applied, I assume that the paper would discover (again) that adversarial training does not provide unforeseen robustness, even if the model was trained with these \"unforeseen\" attacks.\n\n### Clarity\n\n* Typos like nineteen vs eighteen, how many attacks indeed?\n\n### Significance\n\n**Q2: Does this benchmark really model the robustness against unforeseen attacks by including 19 attacks?**\n\nThe importance of studying robustness against unforeseen attacks is that test-time attacks may not be observed at the training time. While this benchmark includes a wide range of empirical attacks, it is unclear if the measured robustness against these 19 now-seen attacks would generalize to robustness against other unforeseen attacks outside these 19 attacks. Yet, this is the whole point that matters in understanding unforeseen robustness. I wonder if a defense would always be more robust to \"unforeseen\" attacks if they demonstrate higher robustness against these 19 attacks. On the other hand, for the purpose of demonstrating non-robustness, if a defense observes low robustness against attack A, would this imply that the defense is not robust against attack B?\n\n**Q3: The attacks are manually selected, which limits the benchmark's impact.**\n\nAs far as I understand, these attacks are manually constructed without any underlying systematic concept. As a result, the benchmark's outcome only provides \"some\" sense of unforeseen robustness against a fixed set of \"manually selected\" attacks that are interpreted as an \"unforeseen\" test set. The measured robustness cannot provide a confident claim that the defense will also have true robustness against unforeseen attacks. Note that I am not criticizing these attacks (their differentiable nature made a good case for different worst-case perturbations), but this manual design limits the significance of this benchmark.\n\nIt is suggested to systematize or even automate the attack's choice so that they can suggest some generalization. For example, these attacks are generally perturbations along different semantic directions on the image manifold. Right now such directions are manually chosen, is it possible to systematize such directions (even if they do not have very semantic interpretations) so they would exhibit some generalization to other directions?"
            },
            "questions": {
                "value": "I appreciate the good design of attacks and frameworks, but I am concerned about the significance of this work, as the benchmark cannot rigorously make a true claim for \"robustness against unforeseen attacks\" (Q2) due to the lack of validation/test split (Q1) or a systematic design that functions similarly (Q3)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7760/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7760/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7760/Reviewer_PXCR"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7760/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698557984936,
        "cdate": 1698557984936,
        "tmdate": 1699636947987,
        "mdate": 1699636947987,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Yg5jbtObSE",
        "forum": "5TlHjMVrNG",
        "replyto": "5TlHjMVrNG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7760/Reviewer_aca9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7760/Reviewer_aca9"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the gap between L-p perturbation attacks and real adversarial attacks in the image classification setting. They provide 18 novel types of attacks (19 including a previously known) to generate a more realistic adversarial dataset based on gradient methods. . \n\nWhile the attack can have full access to gradients, and model weights, the defense does not have access to train-time examples to do adversarial training. Each of the attack types is a differentiable function (not known to the defense) of the image and hyperparameters (\\delta) which is then constrained to a L_p bounded by \\epsilon. This is then optimized by PGD to find the adversarial hyper-parameter \\delta_adv."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* Proposed a benchmark of plausible adversarial attacks through differential functions where intensity is bounded by hyperparameter norm of mixing the perturbations\n* Conducted evaluation across multiple image classification models to show that proposed robustness differs from L-p and L-2 robustness\n* Preliminary investigation into using L2 and Lp, and generative adversarial training for improving robustness gives mixed results (comparison between L2 and L_p cannot be conclusive due to varying results as we vary \\epsilon)\n* Non-optimized UA2 closer to average robustness; whereas optimized UA2 matches L-p robustness empirical results\n* UA2 improves with data augmentation, perceptual adversarial training, and regularization"
            },
            "weaknesses": {
                "value": "* Paper has many typos which limits its readability, and proofreading would make it significantly more accessible.\n* The diversity of the choice of the proposed operators is mentioned in writing, but not quantified. Perceptually several of the proposed perturbations are indistinguishable from each other (e.g. Mix, JPEG, etc), and hence justifying each addition requires some analysis of the minimal patterns required to assess the coverage of unforeseen robustness.\n* Comparison with existing ImageNet perturbation datasets when used for adversarial training is lacking. Although usage of the dataset for evaluation-only is understood, a benchmark against existing adversarial datasets ImageNet-V2 (Recht et al., 2019), ImageNet-R (Hendrycks et al., 2021a), ImageNet-Sketch (Wang et al., 2019), and ObjectNet (Barbu et al., 2019) is missing."
            },
            "questions": {
                "value": "* Justification for using the term \u201cunforeseen\u201d needs to be made in the text. (something like functional or otherwise would be better suited)\n* Not sure if this is true: \u201chave precisely defined perturbation sets which are not dependent on the solutions found to a relaxed constrained optimization problem\u201d. Eqn 4 is still a relaxed constrained optimization problem."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7760/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698695321678,
        "cdate": 1698695321678,
        "tmdate": 1699636947865,
        "mdate": 1699636947865,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CuIviBTKJS",
        "forum": "5TlHjMVrNG",
        "replyto": "5TlHjMVrNG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7760/Reviewer_Sze7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7760/Reviewer_Sze7"
        ],
        "content": {
            "summary": {
                "value": "This work introduces a new benchmarking suite called ImageNet-UA. This benchmark introduces multiple different distortions to the ImageNet dataset to evaluate unforeseen adversarial accuracy. Using this new benchmark, the authors introduce their UA2 metric, which represents the mean accuracy across a core set of their attacks. They then conduct a variety of experiments to assess how UA2 changes across models and a variety of adversarial training settings."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "**Originality** - Introduces a new benchmarking suite that includes attacks/distortions not present in current other robustness evaluation tools (assuming the authors plan to release this to the public)\n\n**Quality** - Wide range of experiments, includes multiple datasets in the evaluation\n\n**Clarity** - Images of the attack examples provide a clear picture of the outcome of the applied methods, introduction and abstract is well written"
            },
            "weaknesses": {
                "value": "Below is a summary of my concerns. Specific details and suggestions can be found in the Questions section.\n\n* Problem and goals are unclear/not well motivated\n* Unclear how this work is substantially different from prior work, specifically the ImageNet-C benchmark\n* Incomplete evaluation: comparison to existing benchmarks is not present, experiments done on a subset of the benchmark\n* Insights are based on UA2, the use of which lacked justification"
            },
            "questions": {
                "value": "**Goals, definitions, and relation to prior work**\n\nThe descriptions of the goals and motivation for this work could benefit from additional attention, specifically in how these differ from previous work. The goals and motivations that were highlighted in the abstract and intro (in italics), and my specific comments/confusions on each of them in the context of prior work are as follows: \n1. _Current adversarial examples are not realistic because they use $\\ell_p$ norms_ - While I agree that $\\ell_p$ norms are a poor proxy for human perception, this fell flat to me because (a) the attacks introduced here do constrain to $\\ell_p$ norms and (b) the ImageNet-C [1] benchmark already evaluates model robustness outside of $\\ell_p$ norms. \n2. _We need benchmarks against \"unforeseen\" adversaries_ - While the definition for what constitutes as an \"unforeseen\" adversary (rather than a \"seen\" adversary) wasn't entirely clear, it was implied that evaluation attacks should go beyond the attacks used during adversarial training and is stated that this is contrary to prior work, citing [3]. However, it is already common practice to evaluate against other attacks and to adaptive attacks that have knowledge of the defense strategy. Additionally, the evaluation in [3] includes attacks beyond PGD, so this isn't a representative example. \n3. _We need benchmarks that represent worst-case adversaries_ - It is unclear why existing adversarial robustness benchmarks like AutoAttack [2] don't already represent worst-case adversaries, given that the same level of model access is assumed here and it is not demonstrated that this benchmark is more performant than other benchmarks (more details on this point later). \n\n\n**Evaluation/methodology**\n* Comparison with ImageNet-C are missing. Given that it seems like the corruptions in this methodology were based on ImageNet-C, it should be included. Only evaluating the UA2 of the introduced framework doesn't provide adequate support for using this benchmark over other benchmarks.\n* I struggled with understanding how this benchmark is significantly different from the ImageNet-C benchmark (because of the likeness in corruptions) or from an adversarial robustness benchmark like AutoAttack (because of the likeness in optimization over the perturbation and $\\ell_p$ bounding). What is the motivation behind combining the corruptions and optimization techniques? \n* What is the purpose of performing the evaluation only on the \"core attacks\"? Specifically, it would be helpful to clarify (a) why would you want to leave out more than half of the total attacks in the framework, (b) why these attacks were selected and (c) how the core attacks are still capable of representing the performance of the entire benchmark\n* The use of UA2 requires additional justification, why is the average across attacks useful (particularly if this is meant to be an ensemble method) and why isn't the individual accuracy of the attacks reported in tables 3-7?\n\n**Presentation**\n* Figure 3 does not give a clear depiction of how the attack works. Given that the perturbations are based on the corruptions, it's unclear what exactly is being optimized. Some of the arrows are confusing (e.g., images feeding into images) and it's not mentioned what the different arrow types represent. Why is the size of the perturbation not the same as the size of the input? What is $project^1_\\epsilon$?\n* It's not clear what conclusions are supposed to be drawn from Table 1, given that there doesn't seem to be any trend between PGD accuracy and UA2. What do the norms next to the model names represent and how are they different from the norm in the column?\n* Table 2 seems to bold the model with the highest accuracy for each attack, but given that the performance of the attacks is being assessed, it seems like it would make more sense to bold the attack with the lowest accuracy for each model.\n* Awkward language in some places (e.g., table 2 caption says \"we plot a range of models on the pareto frontier on imagenet-UA\")\n\nTypos/errors:\n- Figures 1 and 2 not referenced in text\n- Tables 3 and 4 not referenced in text\n- Figures and Tables are referenced out of order\n- make us of -> make use of\n- out suite of attacks -> our suite of attacks\n- in line which -> in line with\n- which novel -> which are novel\n- make use use -> make use of"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7760/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698786332600,
        "cdate": 1698786332600,
        "tmdate": 1699636947657,
        "mdate": 1699636947657,
        "license": "CC BY 4.0",
        "version": 2
    }
]