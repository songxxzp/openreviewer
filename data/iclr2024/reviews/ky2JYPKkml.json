[
    {
        "id": "JtRARMT2vN",
        "forum": "ky2JYPKkml",
        "replyto": "ky2JYPKkml",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6383/Reviewer_knsN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6383/Reviewer_knsN"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new approach for multi-modality learning based on a domain-agnostic concept space, i.e. a concept space that reflects the real-world relations between the different concepts and that aims to be transparent and interpretable. The concept space learning approach follows a box embedding approach motivated by the preservation through geometric structures of relations between the concepts. In addition, the paper proposes domain-specific projection models to map each domain-specific input to a box representation in the concept space. Multimodality is tackled through cross-modality joint training. The proposed approach is applied and evaluated through two downstream tasks: zero-shot image-text matching and zero-shot visual question answering."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The main strengths of the paper are the following :\n\n+ The idea of learning a kind of universal concept space is very interesting and the use of box embedding approaches is also a very interesting idea towards this aim in particular regarding more explainability and transparency.\n+ The decoupling of concept space and domain-specific projection models is also an important idea that enables extension to various several modalities.\n+ The approach is evaluated on two downstream tasks and results in good performance."
            },
            "weaknesses": {
                "value": "I have several comments on the paper :\n\n+ First, a major weakness is the lack of a clear and rigorous definition of the main notions of the paper. For instance: what is the difference between a domain and a modality ? What is a concept and how are they defined? \n+ Since the aim of the paper is to learn a kind of universal representation, some important references are missing such as the works of Li et al in the few shot domain (see for instance [these papers](https://github.com/VICO-UoE/URL)) or others references on the notion and evaluation of universal representations. Moreover, since one of the targeted objectives is also to gain transparency and interpretability, I think that positioning of the contribution compared to the concept-based approaches in the XAI field is also missing (see for instance [this paper](https://arxiv.org/abs/2205.15612)).\n+ Some technical choices are not motivated and justified. For example, why a softplus function in equation 1? Why a KL divergence? Moreover, is not clear how the proposed concept space learning approach is novel compared to the line of works on box embedding (see for instance the references used [here](https://www.iesl.cs.umass.edu/box-embeddings/main/index.html). Q is not clearly defined in Equation 3. Globally, the formalization lacks some rigors. In the cross modality joint training why not weighted loss terms ?\n+ No state-of-the art comparison is given for the zero-shot image-text matching and more globally the experimental study lacks details. \n+ The paper provides no evaluation of the interpretable part of the approach. Evaluating interpretability is difficult but this point should be more discussed in the paper."
            },
            "questions": {
                "value": "+ One strong hypothesis of the proposed work is to have annotated data in order to replicate real-world concept entailment probabilities. This is a big constraint and this kind of annotated data is often non unavailable. How to update the approach in order to tackle this issue? Moreover, what king of relations between concepts are targeted in the proposed approach? (semantic ones, hierarchical ones). Since using geometric-based embedding is a way to preserve semantic or structural relations, this point should be more discussed in the paper and in the evaluation.\n+ In the experimental work, what is the effect of the size of the concept domain space ? More globally what are the implicit or explicit constraints on the concepts in this concept space ?\n+ What about the disagreement between modality in the proposed approach ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6383/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698742352359,
        "cdate": 1698742352359,
        "tmdate": 1699636706673,
        "mdate": 1699636706673,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EUXnKBYXk5",
        "forum": "ky2JYPKkml",
        "replyto": "ky2JYPKkml",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6383/Reviewer_xec7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6383/Reviewer_xec7"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a multimodal learning framework aimed at enhancing the explainability of AI systems with concept space. The framework incorporates a domain-agnostic concept space intended for transparency and interpretability, along with a suite of domain-specific projection models for processing various modalities and aligning them with the concept space. The framework's efficacy was tested in a zero-shot learning scenario on tasks such as Image-Text Matching and Visual Question Answering."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The idea of using concept space to enable explainable multimodal learning is new and makes sense. As many existing approaches aim at aligning features from two modalities in the same feature space, such space might be replaced by a modality-agnostic concept space that is explainable.\n\nThere is an ablation study to demonstrate that the learned concept space contains useful information for downstream tasks."
            },
            "weaknesses": {
                "value": "Although the idea of bringing concept space into multimodal learning is new, some important details are missing and the experiment setup and results seem unclear. Please see the questions section below. Overall I believe the idea has novelty however the method and experiment section needs improvement to be more convincing."
            },
            "questions": {
                "value": "In section 3.1, more clarification could be beneficial. Such as how the collection of concept labels was obtained? How are the true entailment probabilities obtained? How are these two related? Does P(c1,c2) simply represent the union of P(c1) and P(c2)? Why would Eq.2 lead to learning a shortcut of unbounded boxes? Why negative samples would prevent that?\n\nIn the experiment section, since pre-training involves text modality, and the sentence is constructed to describe the image, I wonder if those sentences are constructed from the ground truth labels including information like color, and shape. If that's true, then basically the text modality would have all the information needed for question answering and it is not a visual reasoning task anymore. And maybe that is why the pertaining accuracy is very high to 99.8% as the answers are already encoded.\n\nFor zero-shot experiments, it is also unclear as zero-shot usually indicates that we use a pre-trained model and evaluate a new task/ new distribution. If it is pre-training on the training set and evaluating on the testing set, it is a more classic setting instead of zero-shot. The meaning of \"no fine-tuning\" on the results seems a bit confusing.\n\nThere is no need to use an entire table for dataset statistics, one line of words should convey the information."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6383/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6383/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6383/Reviewer_xec7"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6383/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699123470450,
        "cdate": 1699123470450,
        "tmdate": 1699636706505,
        "mdate": 1699636706505,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kWdgLNOJDM",
        "forum": "ky2JYPKkml",
        "replyto": "ky2JYPKkml",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6383/Reviewer_L6aq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6383/Reviewer_L6aq"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a multimodal learning framework that abstracts concepts into a high-dimensional space and treats each concept as a box in the space. Each concept is assumed to have an implicit meaning learned from the training data, and projections of other modalities into this space make the representations of those modalities interpretable. The concept space is based on prior research on geometric embedding space and is optimized to reflect real-world relationships between concepts. Users can query this concept space to gain insights into the model\u2019s decision-making process."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper proposes to define a geometric concept embedding space using hypercuboids which, to my knowledge, has not been explored before. The proposed method simplifies querying the latent space to interpret model predictions. The idea of modelling concept relations as conditional probabilities is also underexplored in literature."
            },
            "weaknesses": {
                "value": "While the method itself may be novel, I find the motivation, experimentation, and overall writing to be lacking.\n\n**Formulation:**\nThe paper states in the introduction that the \u201cconcept space is optimized to reflect real-world relations between concepts via entailment probabilities.\u201c However, in the formulation, the paper only consider the probabilities where concepts appear together in the training data. This may be unrealistic especially from an OOD generalization perspective. For example, if the pair \u201c(blue, sphere)\u201d is missing from the training set, it will be assigned a zero probability during inference. The paper states that: \u201cThe learning of this concept space is achieved by replicating real-world concept entailment probabilities as observed in training data.\u201d This could imply that  the model may not generalize well to unseen data, specifically unseen concept pairs, thus making the method not suitable for real-world settings, where concepts that are not seen together in training data may occur together.\n\n**Motivation:**\nWhile using geometric latent spaces to embed concepts may be novel, the paper's motivation behind doing so is unclear. The same results can be achieved using models such as CLIP or FLAVA that align visual features and text embeddings in a shared latent space with minimal changes to the pipeline. The latent space can also be made interpretable through a nearest-neighbour search with respect to concept embeddings in the shared latent space. The paper does not show any ablations of using such pre-aligned models instead of training in a geometric latent space. Some of the other design choices (eg. use of SoftPlus) are also not provided in the paper.\n\n**Experiments:**\n* The paper compares the proposed method with older methods that do not capture state-of-the-art in VQA on CLEVR. The baselines may have to be more comprehensive and contemporary. One additional thought - a baseline I would have liked to see is using the pre-trained CLIP model placed as-is into the VQA pipeline. The alignment of vision-language pairs for pre-training is similar to CLIP pre-training. \n* Did the validation set contain concept pairs that did not occur in the training set? How many? Did the model perform better than random guessing on such validation samples?\n\n**Scalability:**\nThe loss function in Eqn 3 scales quadratically with the number of concepts. For real-world datasets like ImageNet, this may not be scalable. \n\n**Writing:**\n* The core methodology described in the paper is spread out and makes it hard to get the big picture without reading it multiple times. I would appreciate an overview diagram or a summary of the overall method, where the reader can get the big picture of what the work attempts to do.\n* The difference between \u201cdomain\u201d and \u201cmodality\u201d is unclear to me in the text. Are these terms used interchangeably, or do they have separate meanings in the paper?\n* The descriptions of tasks also lack clarity. For example, in VQA tasks, the paper states that the system needs to generate a natural language answer such as \u201cYes\u201d. How is this accomplished? Is a pre-trained LLM used to achieve this? To my knowledge, the set of natural language answers in such VQA tasks is provided as a Multiple-Choice Question with the model being expected to select the best option. \n* The paper states that it uses a neuro-symbolic-inspired approach in the VQA task. This was not mentioned previously in the paper, and no neuro-symbolic baselines have been provided.\n* The meaning of some symbols has not been specified and is left to the reader to interpret (eg. Section 3.1.2: f_A: A -> C; what does \u2018A\u2019 mean here?)\n* The paper also contains grammatical errors in various places (eg. Introduction: \u201chas also drew criticism\u201d; Program generator: \u201cis freezed\u201d etc.)"
            },
            "questions": {
                "value": "1.\tWhat is the main motivation behind using geometric latent spaces? The same results can be obtained using grounded latent vectors from models like CLIP. Concept-specific latent spaces can be trained in the manner described without any geometric considerations using frameworks like SimCLR.\n2.\tThe paper states training of separate modalities as a weakness in prior work and as a motivation for the proposed work. How is this different from training the domain-specific models in the proposed approach?\n3.\tHow does the framework function if existing VL-alignment methods such as CLIP are used?\n4.\tWhat is the motivation behind using SoftPlus as the smoothing function? Can any other smoothing functions be used, or is there some specific property of SoftPlus that the authors wish to exploit?\n5.\tHow can the model scale to larger datasets with thousands or millions of concepts and handle cases where plausible concept pairs are not seen in training data?\n6.\tWhy does the paper suddenly use $\\omega_\\Delta$ instead of $\\omega_{max}$ for implementation? How does this ensure \u201cvalid lower and upper boundaries\u201d?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6383/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699252614157,
        "cdate": 1699252614157,
        "tmdate": 1699636706378,
        "mdate": 1699636706378,
        "license": "CC BY 4.0",
        "version": 2
    }
]