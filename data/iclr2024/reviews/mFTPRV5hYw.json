[
    {
        "id": "7Alfn60AHn",
        "forum": "mFTPRV5hYw",
        "replyto": "mFTPRV5hYw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2078/Reviewer_KUtq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2078/Reviewer_KUtq"
        ],
        "content": {
            "summary": {
                "value": "The paper emphasizes the importance of assessing privacy risks in mobility data-based ML models. The authors propose a threat model for evaluating privacy risks and provide a comprehensive privacy risk assessment for such models. They also suggest a privacy-preserving solution for point-of-interest recommendation models to mitigate privacy risks. Their contributions include designing four different attacks, such as common location extraction (LOCEXTRACT), training trajectory extraction (TRAJEXTRACT), location-level membership inference attack (LOCMIA), and trajectory-level membership inference attack (TRAJMIA), developing a privacy-preserving training method that protects against data extraction and membership inference attacks aimed at point-of-interest recommendation models. Overall, the authors identify potential privacy risks in mobility data-based machine learning models and propose solutions to address these risks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1) The paper presents a privacy attack suite that is specifically designed for POI recommendation models. This suite consists of data extraction and membership inference attacks. By conducting experiments with real-world mobility datasets, the authors demonstrate the vulnerability of current POI recommendation models to these attacks.\n\n2) The paper investigates the effectiveness of existing defense mechanisms, such as L2 regularization and differential privacy, against the proposed attacks. However, it concludes that these mechanisms have limitations in providing comprehensive protection.\n\n3) The impact of training and attack parameters on attack performance is analyzed. The effect of training epochs on information leakage and the influence of query timestamps on data extraction attack performance are discussed."
            },
            "weaknesses": {
                "value": "1) The study only evaluates the privacy risks of POI recommendation models in a controlled setting. They didn't provide some insights into how these models might perform in real-world scenarios, where more complex factors are at play.\n\n2) The author did not provide insights on how these risks compare to privacy risks associated with ride-sharing or food-delivery apps."
            },
            "questions": {
                "value": "1) According to the paper, no definitive defense mechanism can protect against all the proposed attacks simultaneously. Can you please explain why this is the case? Additionally, could you provide insights into the challenges that must be addressed to develop more effective defenses against such attacks?\n\n2) How do the proposed attacks and defense mechanisms compare to existing methods in the literature?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2078/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2078/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2078/Reviewer_KUtq"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2078/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698403692507,
        "cdate": 1698403692507,
        "tmdate": 1699636139915,
        "mdate": 1699636139915,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GBlFI9uejT",
        "forum": "mFTPRV5hYw",
        "replyto": "mFTPRV5hYw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2078/Reviewer_sUA6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2078/Reviewer_sUA6"
        ],
        "content": {
            "summary": {
                "value": "This paper offers a privacy attack suite (including data extraction and membership inference attacks) on point-of-interest recommendation models, tailored specifically for mobility data. Experiments are performed on three models trained on two distinct datasets. This suite could in principle be used as a privacy auditing tool."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is very clearly presented and the thoroughness of experimental results is commendable to the point where I am left with view remaining questions. \n\nThe paper demonstrates clear privacy vulnerabilities in POI recommendation systems that should inform future defenses and auditing."
            },
            "weaknesses": {
                "value": "It would be valuable to understand how attack vulnerability changes with sample size. For very large datasets, where there are generally a larger number of unique users to all locations, does the attack success decline? This theory seems to be somewhat supported by Fig 4. \n\nMore detailed descriptions of datasets size and dimensionality would be valuable to understand whether the emulate real-world production systems. \n\nThe paper could benefit from explicitly contextualizing how attack performance compares to attack performance for other type of data/models (e.g. text & image are referenced)."
            },
            "questions": {
                "value": "How do we know if the utility privacy trade-off inherent or a limitation of existing DP algorithms? You note the privacy-utility trade-off does not strictly hold in your experiments but can you show that practically acceptable utility and privacy can both be achieved?\n\nCould you explain why having more total check-ins seems to help protect a user against a MIA? This seems surprising in the context of differential privacy where worst-case privacy loss degrades with sensitivity. \n\nHave you explored the feasability of DP synthetic data for this type of application?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2078/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2078/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2078/Reviewer_sUA6"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2078/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698437021321,
        "cdate": 1698437021321,
        "tmdate": 1699636139844,
        "mdate": 1699636139844,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OMo9Lm3H5G",
        "forum": "mFTPRV5hYw",
        "replyto": "mFTPRV5hYw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2078/Reviewer_1v9T"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2078/Reviewer_1v9T"
        ],
        "content": {
            "summary": {
                "value": "This paper evaluates the privacy risks of POI recommendation models by introducing an attack suite and conducts extensive experiments to demonstrate the effectiveness of these attacks. Additionally, it analyzes which types of mobility data are vulnerable to the proposed attacks and further adapts two mainstream defense mechanisms to the task of POI recommendation."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "S1. The scenario of this paper, POI recommendation, has real-world applications.\n\nS2. The paper proposed several attack methods.\n\nS3. Extensive experiments are conducted on two real datasets."
            },
            "weaknesses": {
                "value": "W1. The motivation of this work is not convincing enough. \n\nW2. The definition of sensitive information is unclear.\n\nW3. Experiments are inadequate and some insights are not surprising.\n\nW4. There are some typos in this paper. For example, at line 16 in algorithm 4,   $f_out$ should be $f_\\theta$."
            },
            "questions": {
                "value": "Q1. The definition of sensitive information and privacy guarantee should be formally defined and well justified. Then, it may become meaningful to conduct adversary attacks.   \nQ2. There have been quite a few studies on protecting spatial/location/trajectory privacy. However, most of them was not reviewed/evaluated by this paper. Thus, it was uncertain whether existing privacy preservation mechanisms could help on the mentioned limitation of POI recommendation.    \nQ3. Take private spatial data publish as an example. Based on GDPR, a LBS platform can only collect user\u2019s check-in data that has been well protected (e.g., by differential privacy). Under this practical setting, deriving the platform\u2019s data will not leak the sensitive information of users, which makes the attacker model proposed in this work less meaningful.    \nQ4. I am also curious: if the input data has been well protected by existing privacy mechanism and then trained by POI recommendation model, is there any sensitive information leakage?    \nQ5. Since privacy preserving learning has been well studied in recent years, it is sometimes possible to extend existing POI recommendation models with privacy guarantee (e.g., by adding differential privacy noise in the gradients). Does this fact significantly change the main insight?   \nQ6. In Appendix E, the epsilon setting of DP-SGD is a little large. Please provide more justifications.    \nQ7. Both datasets are a little outdated and relatively smaller-scale than the current LBS platform. However, the major insights are strongly related to the data sparsity. Maybe, it would be better to conduct experiments on large-scale datasets.    \nQ8. Why does the curve in Figure 5(b) first rise and then drop when the number of POI increases?    \nQ9. It is mentioned that k is usually 1, 5, and 10 when using top-k to measure accuracy in page 3. However, only 1, 3, and 5 were tested in the experiment as shown in Figure 1. What is the rationale of this experimental setting?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2078/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2078/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2078/Reviewer_1v9T"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2078/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699346231924,
        "cdate": 1699346231924,
        "tmdate": 1700708499220,
        "mdate": 1700708499220,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dpc3b9dHzy",
        "forum": "mFTPRV5hYw",
        "replyto": "mFTPRV5hYw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2078/Reviewer_iRwY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2078/Reviewer_iRwY"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes data extraction and membership inference attacks to POI recommendations involving location data. The experiments are conducted in two datasets and the empirical results show the effectiveness of the proposed attacks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The research problem is important.\n2. The paper analyzes the factors in the data that affect the attack performance.\n3. The proposed attacks for data extraction and membership inference attacks are simple yet effective."
            },
            "weaknesses": {
                "value": "1. The appropriate baselines are missing. Can the existing data extraction or membership inference attacks be applied to the POI recommendation models, e.g., [1]?\n2. There is no qualitative result analysis on the data extraction attacks. It would be better if the authors could conduct these analyses on the data extraction attacks.\n3. The threat models assume that the adversaries are capable of accessing the confidence scores, which makes them impractical. In practice, the model owner only releases the final result to the users. In this case, whether the proposed attacks are still effective is unknown.\n\n\n\n\n[1] R. Shokri, M. Stronati, C. Song and V. Shmatikov, \"Membership Inference Attacks Against Machine Learning Models,\" 2017 IEEE Symposium on Security and Privacy (SP), San Jose, CA, USA, 2017, pp. 3-18, doi: 10.1109/SP.2017.41."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2078/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699393743190,
        "cdate": 1699393743190,
        "tmdate": 1699636139724,
        "mdate": 1699636139724,
        "license": "CC BY 4.0",
        "version": 2
    }
]