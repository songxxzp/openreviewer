[
    {
        "id": "F6yIlSPotf",
        "forum": "Feiz5HtCD0",
        "replyto": "Feiz5HtCD0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3642/Reviewer_vExG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3642/Reviewer_vExG"
        ],
        "content": {
            "summary": {
                "value": "The paper describes extensive work on the evaluation of language models for content diversity and homogenization. The work defines homogenization as the degree of similarity between the produced content of users co-writing with LLMs measured using overlap metrics such as Rouge-L and BERTScore; while diversity as the degree of linguistic variation of the text measured by n-grams and/or type token ratio (TTR). The authors cite the main motivation of the study being to assess the quantifiable degree of which LLMs reduce content diversity and homogenization in a co-writing scenario. The author/s then conduct a controlled experiment to gather data in 3 setups across several thematic prompts: writing essays w/ LLM assistance (solo), using GPT as is (GPT-3), and using a GPT model finetuned for instruction-following (InstructGPT). From the collected data, the author/s perform several experiments geared towards answering the research questions on (a) how users often engage with the model, (b) homogenization between essays from the three setups, and (c) differences in diversity between the two generative models used in the co-writing scenario. In terms of homogenization, the results of the paper show that using instruction-tuned GPT models like InstructGPT produces much more similar content compared to using these models (obviously) and using regular GPT-3 models without the instruction-tuning. In terms of content diversity, using InstructGPT does have some tendency to reuse word combinations, as evidenced through the n-gram analysis by the authors, and shows a considerable reduction in key-point and lexical (word-based) diversity. Overall, the paper paints a picture of an empirical evidence of how using these models in a co-writing setup affects the quality of texts we co-produce with them."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is very well written and structured, and the narrative flows smoothly. I had no problem understanding the picture the author/s are constructing, starting with the problem definition, motivation, methodology, and the discussion of findings. I believe anyone, even a non-technical audience from interdisciplinary fields, will fully understand and appreciate the results of the paper in one sitting.\n\nAs highlighted, one of the most important contributions of the paper is the dataset built from the controlled experiment from the three different groups (solo, using GPT3, using InstructGPT). I find value in the availability of this dataset for public use. \n\nI find the quantitative and controlled experiment with how LLMs affect the diversity of texts, especially in a co-writing setting explored in the study, to be extremely useful and timely. It is a common intuition and suspicion in the education community that LLMs do reduce creativity when used for content-generation tasks such as writing essays, but this study fully supports and grounds that intuition with succinct and convincing experiments."
            },
            "weaknesses": {
                "value": "One thing that feels quite lacking and limited in the paper is that the metric for diversity only focuses on word-based evaluation through n-grams and keypoint analysis. As a reader, I was expecting a more expansive set of linguistic features to be explored to really shed light on the diversity of generated texts from (Instruct)GPT models. This includes syntax or parse tree-based differences and some perspectives even on information theory by measuring the suprisingness (entropy) of human vs machine-generated texts. Well-known previous works in text generation such as (https://arxiv.org/abs/1904.09751) have shown that human-written texts are naturally surprising\u2013which can be a prerequisite for its texts being diverse\u2013therefore exploring this form of feature with the co-writing setup done by the author/s is necessary. \n\nThere is a dedicated section for the author/s\u2019 inferences as to why the use of InstructGPT produces less diverse content, but it doesn\u2019t supply the expectations of the reader. I\u2019m quite confused with this takeaway \u201cThe reduction in diversity in collaborative writing is attributed more to text contributed directly by the InstructGPT model.\u201d This is an obvious case. Upon reading Section 6, I was expecting more of a deeper discussion as to why InstructGPT reduced diversity more than models not trained for following instructions, such as a regular GPT-3, which can be traced back to some analysis about the quality and characteristics of data used for training these models. In order to level the playing field and to compare these two models equally, you may need to extensively compare their generated content to even more fine-grained linguistic analysis beyond word-based patterns such as n-grams or type-token ratio.\n\nThe paper makes heavy use of the Co-Author system by Lee et al (2022), and it seems in the paper that the authors of the system have already done a similar experiment on co-writing with LLMs, especially on the lens of creativity, vocabulary, and grammaticality. The author/s of the submitted paper have only extended this through the addition of homogenization and diversity analysis. I\u2019d like to see more clear and explicit differentiation of the work compared to what analysis has been in the Co-Author paper so that it does not seem to be incremental. I invite the authors to discuss this."
            },
            "questions": {
                "value": "1. Once the essay has been completed, were the users required to reread and revise the essay for another round? This may negatively bias the result with the n-gram pattern analysis as sometimes users may freely switch words for more fitting synonyms in phrases while the GPT-generated remains constant.\n\n2. Are the writers recruited classified as native speakers of English? There are multiple claims in literature (https://arxiv.org/abs/2304.02819, https://hai.stanford.edu/news/ai-detectors-biased-against-non-native-english-writers, https://arxiv.org/abs/2306.04723) stating that there are similarities with writing styles of non-native speakers of English with machine-generated texts. Recruiting non-native English speakers in the mix may introduce some bias.\n\n3. Why is the model-suggested text not required for groups using GPT models? Does this mean that there are possibilities that some passages in those two groups (users using GPT and InstructGPT) have fully human-written text since they are not required to use the model generations anyway?\n\n4. For writers that are in the group using LLMs, is there a checker to check whether their outputs are a certain percentage of human and LLM-generated content? If a writer fully made use of the LLM generations and then only contributed one to two sentences, would this be considered a qualified entry for analysis? I understand that the experiment is on co-writing but it would be pointless then if more than 80% of the submitted text passage has been written by GPT.\n\n5. Is there a possible method that you can integrate to quantify the usable words forming the overall vocabulary for each language model? I believe this would greatly support the claim that using LLMs may reduce diversity and creative in writing. In addition, what does the literature say about the quantifiable reduction of the cost of instruction-tuning LLMs to the degradation of its diversity? The papers made a mention of this but only at a surface-level detail without going deeper into its discussion."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3642/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3642/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3642/Reviewer_vExG"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3642/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698240208380,
        "cdate": 1698240208380,
        "tmdate": 1700585704738,
        "mdate": 1700585704738,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QQ9EYJlTru",
        "forum": "Feiz5HtCD0",
        "replyto": "Feiz5HtCD0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3642/Reviewer_qs9Z"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3642/Reviewer_qs9Z"
        ],
        "content": {
            "summary": {
                "value": "The authors measure the diversity of outputs in LLM-assisted essay writing. They compare three essay setups: written solely by a user, written by a user + gpt3, and written by a user + instruct-gpt. They find that essays written using instruct-gpt are significantly less diverse than the others. Disaggregating the text by LLM-authored or user-authored shows that this lack of diversity comes from the model itself (ie, the user is *not* more repetitive when using instruct-gpt). They also present a novel diversity metric (by first summarizing the essay into \"key points\") to measure the diversity of the essay on a structural level."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- It sometimes feels like there's a mismatch between how the academic world studies and evaluates LLMs, and how LLMs are actually used in the real world. This paper bridges that gap, and directly asks important questions that are relevant to today's users of ChatGPT.\n- It's interesting that you use the LLMs as part of the process in generating the key points .\n- The diversity metric of \"key points\" that the authors present is useful and interesting-- as LLMs become ubiquitous, it's clear that measures of text diversity/similarity/etc are often insufficient, so this type of work can have a significant impact."
            },
            "weaknesses": {
                "value": "- If we know that instruct-gpt generates lower-diversity text, then I think I am missing a bit of why this is a surprising result overall.\n- I'm not convinced that these results can be extrapolated to other LLM-based writing tools. When designing a good tool, questions like \"is it limiting what the writer wants to express\" are some of the things you'd first think about addressing. In fact, other work has found that having the LLM can be helpful for getting users out of their normal writing patterns [(e.g.)](https://wordcraft-writers-workshop.appspot.com/learn). But then again, ChatGPT is so ubiquitous that maybe it's enough to just evaluate it alone.\n- Having a precise character number on the text written by the model vs the user can be misleading. For example, what if the model generates a sentence, and the user fully rewrites it but keeps the underlying meaning? (having worked on similar tools, this seems to happen frequently)\n- Nit: Figure 4a-- it's confusing that the \"InstructGPT\" and \"GPT3\" color key is horizontal and lines up with the vertical center division line (I thought maybe GPT3 was the label for the left half or something). Maybe just put them on top of each other like 4b?\n- It would be good to compare to essays written entirely with instruct-gpt and GPT3\n- Figure 4b is generally kind of confusing, especially the x axis. Can you just simplify and say \"average probability for top 10 ngrams\" or something? (This is just a suggestion-- I'm not sure it's actually better. But the current version could use more explanation)."
            },
            "questions": {
                "value": "- If the key points are generated with GPT3.5, aren't they inherently more likely to be similar to other text generated with GPT3 or ChatGPT than with human text? If so, won't that skew the results?\n- (More questions in \"weaknesses\")"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3642/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698277162309,
        "cdate": 1698277162309,
        "tmdate": 1699636320085,
        "mdate": 1699636320085,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LbBRgako64",
        "forum": "Feiz5HtCD0",
        "replyto": "Feiz5HtCD0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3642/Reviewer_ttyL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3642/Reviewer_ttyL"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates whether writing assisted by large language models (LLMs) leads to reduced diversity in the composed text. It uses a within-subject design (but see my comment about the confusion below) that includes three groups of participants that wrote essays without using an LLM, a group that used a base LLM (GPT3) to assist writing, and a group that used an LLM tuned with human feedback (InstructGPT) to assist writing. The main finding of the paper is that writing with InstructGPT leads to significantly more homogeneous context and less diverse n-gram (n>1) distribution and semantic key points (as determined by prompting gpt-3.5-turbo) than writing without an LLM and writing with the GPT3 base LLM. The authors further attributed the increased homogeneity and reduced diversity to the portions of text contributed by the LLM (instead of those portions contributed by the human)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "S1. Taking an empirical approach to answer the interesting and important question about how humans interact with LLMs when composing text. This is a first investigation in this topic area to my knowledge.\nS2. Defining a number of metrics to quantify the diversity and homogeneity of text written under different human-LLM collaboration settings, encompassing both the lexical aspect (n-grams) and semantic ones (key points extracted by gpt-3.5 and BertScore based similarity)."
            },
            "weaknesses": {
                "value": "W1. There are some confusion / lack of clarity about the experiment design. While Appendix Sections A.1 and A.3 clearly indicate that the authors used a within-subject design wherein each participant wrote text under all three settings, the text on p. 1 (\"control group\", \"LLM treatment group\", and \"feedback-tuned LLM treatment group\") seems to indicate that the study was based on a between-subject design, where nonoverlapping sets of participants were tested under the three different settings. This confusion is further acerbated by the fact that the authors used independent-samples t-test for statistical analysis throughout this manuscript, which indicates this is a between-subject design. If the study was really based on a within-subject design, as Sections A.1 and A.3 indicate, then the statistics should be redone with repeated-measures ANOVA and paired t-tests, which may alter the significance test results of this paper. \nW2. While reading this manuscript, I can't help but feel some duplication between Sections 4 and 5, which focuses on \"homogeneity\" and \"diversity\", respectively. Although these two terms usually refer to slightly different aspects of a collection of text, the authors never tried to define them and how they are related to and different from each other in this paper. So in the end, the reader has to read and understand that \"homogeneity\" is operationally defined as the average of a semantic distance metric (e.g., BertScore) among pairs of the composed text in the corpus, while diversity refers to both lexical and semantic aspects, and are calculated in a different way (# of unique lexical or semantic units). \nW3. The user interaction paradigm is limited to generating and adopting continuations with the LLM (whether base or feedback-tuned), as described throughout the paper and in Section A.3. However, anecdotally we know that users of LLM and associated chatbots (e.g., ChatGPT and Bard) often prompt the models to write or rewrite text by using their own prompt templates or instructions. The flexibility is higher in such prompting-based interactions than in the more stereotypical continuation paradigm. The authors did not mention such instruction- and prompting-based interaction paradigms, which are arguably more prevalent and useful compared to simple continuation.\nW4. To evaluate the text composed by the study participants, the authors used only homogeneity and diversity metrics. They didn't report more basic metrics of the composed texts, which IMO should also be reported. These include the length, the perplexity (e.g., as determined by GPT3), and writing quality (e.g., grammaticality and other human rater-based writing quality) metrics. These basic metrics are not only important for ensuring comprehensiveness of the result reporting, but also useful for ruling out the possibility that differences in homogeneity and diversity are somehow due to the confounding factors due to differences in those basic aspects of writing."
            },
            "questions": {
                "value": "Q1. See my comments in W1 above. Please clarify whether the study design was between-subject or within-subject. Please ensure that the proper statistical analysis is used accordingly.\nQ2. Were the base LLM (davinci) and feedback-tuned LLM (text-da-vinci-003) based on the same model architecture and the same parameter count to make it a fair comparison between the texts composed with them?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3642/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698773368528,
        "cdate": 1698773368528,
        "tmdate": 1699636320008,
        "mdate": 1699636320008,
        "license": "CC BY 4.0",
        "version": 2
    }
]