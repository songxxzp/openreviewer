[
    {
        "id": "Cn7ZbpI7pK",
        "forum": "UnstiBOfnv",
        "replyto": "UnstiBOfnv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1180/Reviewer_X9cm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1180/Reviewer_X9cm"
        ],
        "content": {
            "summary": {
                "value": "This paper studies weaknesses with using LLMs as proxies for human judges. To do so, the authors take 40 questions, then have GPT-4 generate responses that satisfy different personas (e.g., an advanced English learner versus an intermediate English learner), then modify the responses when GPT-4 does not satisfy the specified criteria. The authors then use an ELO analysis and finds that writing with \u201cseveral factual errors\u201d outperforms \u201ccorrect + short\u201d, while finding that order impacts judge\u2019s decisions, and humans tend to be inconsistent. To evaluate better, the authors introduce MERS, which evaluates responses ion terms of \u201cAccuracy\u201d, \u201chelpfulness\u201d and \u201cLanguage\u201d, and find that the ratings of these subcategories more closely align with the ways the different promptings of GPT-4 can be ranked."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The work studies an important problem: assessing the reliability of ELO style evaluation, that is increasingly adopted\n* The decomposition trick (i.e., evaluating multiple attributes instead of a single preference) seems general and useful."
            },
            "weaknesses": {
                "value": "* The study is small scale: it only analyzes forty questions, which makes it hard to make systematic conclusions.\n* The study only considers three different attributes to evaluate, and only a few types of generated answers. It\u2019s possible these three attributes are useful for identifying problems with these types of answers, but not others\n* The characteristics rely on GPT-4 faithfully following the prompt, which doesn\u2019t work, so the authors manually edit the text. This adds a human bias to the study, which could impact the results."
            },
            "questions": {
                "value": "Do you expect the results to change with a larger study, or with different attributes (e.g., beyond \"helpfulness\") that you measure?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1180/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1180/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1180/Reviewer_X9cm"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1180/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698815852387,
        "cdate": 1698815852387,
        "tmdate": 1699636044211,
        "mdate": 1699636044211,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hlRNHIBMwO",
        "forum": "UnstiBOfnv",
        "replyto": "UnstiBOfnv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1180/Reviewer_fkGz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1180/Reviewer_fkGz"
        ],
        "content": {
            "summary": {
                "value": "The paper investigates evaluation of LLM-generated answers by human experts, crowd evaluators as well as LLMs.\nThe GPT-4 model is used to generate the answers to 40 questions, and different instructions/prompts were used to generate different types of outputs (with major factual errors, with minor factual errors, with grammar errors).\nGPT-4 is also used for evaluation, but one more LLM (Claude-1) is also used in order to avoid potential biases of the model towards its own output.\n\nThe outputs are then ranked pairwise and Elo rating (designed for chess) is computed from all rankings.\nIn addition, apart from using a single overall criterion for ranking, using three different aspects separately is presented, too: accuracy, helpfulness and language.\n\nThe main findings are that in all evaluations longer answers are preferred, that crowd evalutors often disregard factual errors, that human annotators often disregard grammar and spelling errors."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Evaluating LLM outputs is important, so it is great to see exploring the evaluation process by different evaluators. \nSince LLMs are more and more used for evaluating NLP tasks, it is also important to analyse and understand their capabilities and flaws.\n \nConsidering different quality aspects is great, too."
            },
            "weaknesses": {
                "value": "Several important parts are not clearly presented or defined.\nPutting related work at the end also makes the reading harder -- for example, it was not clear until the end if Elo rating is a widely used approach.\n Also, it is not clear whether  the mentioned criteria are usually used for evaluating question answering, or it is something proposed in this work. It seems that evaluating on different critera separately is new.\n\nIt is not clear why only GPT-4 is used for the evaluation on different criteria, and not the other LLM (Claude-1) -- this is important, especially because of the bias toward own outputs (mentioned by authors, too).\n\nSome terms should be revised for the sake of clarity (e.g. replace \"games\" by \"pairwise comparisons\", \"answer order\" by \"order of outputs\", etc.  \n\nMore details can be found in \"Questions\"."
            },
            "questions": {
                "value": "Introduction\n\nIn this study, we systematically generate a set of responses\n=> not clear if evaluation responses were generated, or texts to be evaluated\n\n\nwe observe hesitancy among human \n=> what is \"hesitancy\" in this context? \n\n while LLMs lean toward the first answer and expert annotators prefer the second answer\n=> what are \"the first answer\" and \"the second answer\"?\n\nOverall, the entire paragraph starting with \"In the course of our investigation (Table 1 and Section 3)\" is much more suitable for Discussion than for Introduction. \n\n\nIt is not clear what are the widely used evaluation methods and criteria: for example, is calculating Elo rating from pairwise rankings widely used or only in some publications?  \n\n\nSection 2\n\nSection 2.2 \"answer generation\" is out of place within the section about the evaluation, it should be  a separate section (Section 2, also with corpus statistics), and then Section 3 only evaluation.\n\nIs the set of questions publicly available? If yes, on which link? \n\nWhy not explicitly ask for spelling errors and grammatical erros, instead of giving learning level and supposing the type of errors? \n\nanswer ordering\n=> does this refer to order of outputs of different mdoels (set-ups)? \nthen it would be better to call it \"order of generated outputs\"\n\nSection 2.3\nThe guidelines for the annotators are very important and should be part of the main sections, not only in Appendix.\n\nSection 2.4\nwe sample 200 games out of a total pool of 5280 games\nannotate 10 games\n=> which games? \n\nThis approach enables us to accurately evaluate the effectiveness and reliability of each annotation method.\n=> Why/How? \n\nSection 2.5\nstudies relies => studies rely\n\nwhich may not be appropriate for our work as our answers are refined by humans after being generated by GPT-4\n=> why refining by humans can be inappropriate? \nusing the same LLM for generating outputs and for evaluating them can indeed be inappropriate\n\nThe prompt assesses the answers based on their helpfulness, relevance, accuracy, and level of detail\n=> were the same instructions/guidelines given to human annotators?\nThe guidelines for the annotators should be mentioned/explained in the previous sub-sections. \n\nfootnote 2) it seems that \"games\" refer to pairwise comparisons? \nthe term \"games\" is confusing, it should be replaced by \"comparisons\" \n\n\nSection 3\n\nAs anticipated, the standard correct model attains the highest Elo score across all human\njudges. Nevertheless, intriguing disparities emerge when comparing the Elo results from human\njudges to those generated by GPT-4 and Claude-1.\n=> Not clear: the standard correct model also has highest scores according to GPT-4 and Claude-1? What are the intriguing disparities? \n\n\nFigure 2: \nWhat are \"Assistant 1\" and \"Assistant 2\"? \nthere is no explanation in the table caption\n\n leading to Elo scores that remain relatively close to the initial\nvalue of 1000 + \n\"by assigning a high number of ties\" \n\nsomething like this should be added in order to make the statements fully clear\n\nWhy is this necessarily a disadvantage?\nEspecially if the outputs are similar in the important aspects? \n\n\"the order of answers\" is the order of generated outputs, right? \nWhat is \"Assistant 1\" and \"Assistant 2\"? \nWhat is exactly the order of the answers? Which is the first output and which is the second?\nThey should be presented at the same time in order to be ranked? \nIs the first on the left and the second on the right?\nOr the first is on above the second? \nOr something else? \n\nthose model => those models\n\nparticularly when they appear convincing => what \"convincing\" means exactly? \nWhat is the difference between human failures of fact-checking and LLMs' failures of fact-checking?\nOr they are of the same nature?  \nWhy there is no figure with percentages? (similar to Figure 1 and Figure 2)\n\nWhat values were used for calculating Kappa coefficient?\nBecause the evaluation consists of rankings, not of assigning numbers.\nWhy not for example Kendal's Tau? \n\n\nSection 4\n\nthe current widely used evaluation approach => what is that exactly?\nCalculating Elo rating from pairwise rankings?\nGuidelines/instructions for ranking (what to pay attention to:  accuracy, level of details, relevance, language, helpfulness\n\n\nRecent advancements in MT evaluation also advocate breaking down the evaluation into multiple aspects\n=> this is error annotation, not ranking\n\nMQM is error annotation framework, not really related to different overall quality aspects\nin this submission, there is absolutely no error annotation, only pairwise ranking\nan appropriate analogy to MT could be separate ranking according to adequacy, fluency, readability\n\nTable 4\nWhy Claude-1 is not used for this experiment (evaluating separately according to three aspects)? \n\n\nSection 5\nit is crucial to supplement human opinions with other evaluation methods => are \"other evaluation methods\" using LLMs for evaluation? \n\n\nRelated work should follow Introduction, not be placed almost at the end of the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1180/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1180/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1180/Reviewer_fkGz"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1180/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699649409658,
        "cdate": 1699649409658,
        "tmdate": 1699649409658,
        "mdate": 1699649409658,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "nlRf2bTSHI",
        "forum": "UnstiBOfnv",
        "replyto": "UnstiBOfnv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1180/Reviewer_2ems"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1180/Reviewer_2ems"
        ],
        "content": {
            "summary": {
                "value": "The paper is focusing on the important open problem of evaluation in natural language generation. Using a dataset consisting of 40 questions of intentionally flawed machine generated (GPT4) answers, the authors employ both human evaluation (crowd-sourced and expert) and LLM-based evaluation focusing on three aspects of the generated texts: i) language proficiency, ii) factual accuracy, and iii) response length. Their analysis reveals biases in the evaluation process: human hesitancy vs LLMs certainty in determining answer quality, humans do not thoroughly fact-check answers vs LLMs displaying some degree of fact-checking ability, and both human and LLM judges favor longer texts. Moreover, factually incorrect models are preferred over those that generate grammatically incorrect or short responses. Informed by these findings, the authors recommend the evaluation of texts from multiple perspectives as opposed to a single metric only, and propose a rating system that assesses the quality of machine-generated texts accounting for i) language, ii) accuracy, and iii) helpfulness criteria."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The topic of the paper is very relevant and timely since evaluation is an open problem in the literature. The community needs a thorough understanding of current evaluation metrics and informed by their limitations,  better evaluation metrics. The paper makes a step in this direction."
            },
            "weaknesses": {
                "value": "Their evaluation setup is limited and includes dataset of only 40 questions. The sample size is small and insufficient for drawing convincing conclusions. The authors also do not discuss the importance of the categories of these questions (\u201cgeneric\u201d, \u201cknowledge\u201d, \u201ccommon sense\u201d, and \u201ccounterfactual\u201d) after leaving out other categories. \n\n\nThe expert evaluation section (2.4) discusses games, but these games are not introduced/presented ahead of this section: \u201cwe sample 200 games out of a total pool of 5280 games, which is statistically sufficient to yield meaningful in- sights into the model\u2019s performance\u201d. \n\n\u201cBy incorporating multiple LLM judges, we can establish a more comprehensive and unbiased assessment of the generated answers in our study.\u201d - Generally agree with this statement, however in the experiments only GPT-4 and Claude (Antrophic) are used, how is this in line with your claims? Only two LLM-based models is not reflective of the goals of your statement. In addition, Claude is not even mentioned in the Introduction or anytime before Section 2.5, leaving the reader the impression you are only using GPT4.\n\nThe findings of the user study conducted by the authors are unsurprising. The length of the text is a well-known confounder in the evaluation of generated texts. Similarly, texts which contain grammatical errors can be quickly identified as having poor quality. Moreover, it is unsurprising that humans tend to be biased towards factually incorrect answers that look coherent and are grammatically correct (especially true if they lack in-domain knowledge). Besides, the authors make misleading statements when claiming LLMs reach consensus when in fact GPT-4 and Claude only reach moderate agreement, and that LLMs display some degree of fact checking abilities - hard to quantify this statement, since the lack of factuality is an open problem with current models. Similarly, the authors claim \u201cthe expert annotators outperform the crowd-sourced annotators in the evaluation, despite not being entirely error-free themselves.\u201d - this is a well known fact in the literature, however it is presented as if it were a novel insight which is very misleading! \n\n\u201cexpert annotators indeed prioritize factual accuracy during evaluation.\u201d -  of course, since they have in-domain knowledge. However, there is no discussion highlighting the relevance of the selected experts vs the selection of the specific domains the authors choose to focus on in this work.\n\n\u201cRegarding the \u201cHelpfulness\u201d dimension, both expert annotators and GPT-4 consistently consider longer answers to be more helpful.\u201d - of course, since they contain more information! Nevertheless, the authors only make vague assumptions and fail to quantify the degree of factuality to support their statements. \n\n\nThe setup is not consistent: it evaluates i) language proficiency, ii) factual accuracy, and iii) response length, and then proposes a rating system that accounts for  i) language, ii) accuracy, and iii) helpfulness. Helpfulness criteria is not clearly motivated, and its inclusion is inconsistent with the initial evaluation criteria. Besides, the paper does not clearly describe how this aspect is being evaluated, to whom it is helpful and in which contexts.\n\n\n\nThere is no clear motivation in the paper on why the authors chose to focus on these particular three aspects ( language proficiency, factual accuracy, and response length) of machine-generated texts. There are so many aspects of the generated texts that can be considered, and this paper fails to make a convincing argument on the selection of their evaluation criteria. While the authors acknowledge in the Discussion section that this may not be the perfect setup, they fail to motivate their experimental choice. \n\nThe paper fails to discuss limitations of human evaluation, as well as limitations of LLM-based evaluation with respect to findings already well-established in the literature. Their statements in the Discussion section are vague and unsupported by evidence. \n\nThe Related Work section is not very informative for the topic of the paper, particularly the paragraph focused on enumerating existing LLMs is not discussing evaluation at all (which is the main focus of the current paper). In addition, the authors fail to cite and refer to important bodies of work in the literature focused on both human and machine evaluation. I recommend a complete rewrite of this section and instead of presenting generalities, discuss papers that have addressed the same topic and are particularly relevant to your current work. \n\nIn general, the experiment setup decisions made in this paper seem rather arbitrary and not well documented / supported by the arguments presented. The authors fail to discuss and include significant papers focused on the evaluation of natural language generation (NLG) models, and do not place their work in the context of recent literature on NLG evaluation."
            },
            "questions": {
                "value": "This sentence is confusing: \u201cwe limit each annotator to provide no more than 20 annotations and enforce a strict maximum of 50 annotations per person.\u201d - Does an annotator judge 20 or 50 questions?\n\nHow do you evaluate helpfulness?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1180/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1180/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1180/Reviewer_2ems"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1180/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699683015942,
        "cdate": 1699683015942,
        "tmdate": 1699683015942,
        "mdate": 1699683015942,
        "license": "CC BY 4.0",
        "version": 2
    }
]