[
    {
        "id": "iUEiScoJTN",
        "forum": "gjeQKFxFpZ",
        "replyto": "gjeQKFxFpZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3593/Reviewer_bmdf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3593/Reviewer_bmdf"
        ],
        "content": {
            "summary": {
                "value": "The paper evaluateds an LLM's capability to express uncertainty. To this end they define a systematic evaluation framework.\n\n+ LLMs are a highly relevant topic\n+ Elicitating confidence is also an important topic in deep learning in general\n+ The study performs extensive evaluation on multiple datasets and LLMs\n- The evaluation and outcomes yield little novel insights (see also detailed comments)\n- The methods are standard, a methodological contribution would be highly appreciated. The idea to use LLMs for self-assessment is all-to-common. That said, it is also interest to assess it in this context, but on its own a marginal contribution. The prompting/aggregation techniques at best partially mitigate this, but they are also well-known. In short, more would be expected for a strong contribution.\n\n\nDetailed comments?\n> 1) LLMs, when verbalizing their confidence, tend to be overconfident...\nLLMs are known to be \"Fluent but non-factual\" or \"Convincing though wrong\". Given that this study focuses only on evaluation I would have expected more novel findings as key fining.\n> 2) As model capability scales up, both calibration and failure prediction performance improve, yet still far from ideal performance.\nThat larger LLMs are better (on all benchmarks) than smaller ones is also general knowledge - check the LLama2 paper, for example, and compare benchmarks for 7B,13B...\n> It would be interesting (and maybe even necessary) to study how much the uncertainty estimates depends on the prompt, i.e., if you state in the prompt \"Appear confident\" are estimates more overconfident?"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "see above"
            },
            "weaknesses": {
                "value": "see above"
            },
            "questions": {
                "value": "see above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3593/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698658811304,
        "cdate": 1698658811304,
        "tmdate": 1699636314807,
        "mdate": 1699636314807,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iKqaBzIyL5",
        "forum": "gjeQKFxFpZ",
        "replyto": "gjeQKFxFpZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3593/Reviewer_uQRx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3593/Reviewer_uQRx"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a unified framework under black box setting to evaluate how calibrated large language models are for their predictions. Specifically, no logits or internal states of LLM are assumed given but the models can be asked to provide explicit confidence. The authors evaluate 5 different prompting strategy and various aggregation strategy to elicit better calibrated results from the models."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is very well-written and tackles the black box confidence calibration in a timely manner. The approach is very targeted to LLMs which provide many insights that are not available from calibrating image classification models.\n\n2. The evaluation is very thorough and the two calibration tasks defined are reasonable."
            },
            "weaknesses": {
                "value": "1. It seems black-box based confidence elicitation approaches are generally unsatisfactory in calibration performance. Can the authors directly compare them with white-box based calibration approaches such as token probability based calibration approach on open-source models and observe how large the gap can be on ECE and failure prediction?\n\n2. Equation 3. Shouldn't we maximize with respect to P since we are doing an MLE estimation?"
            },
            "questions": {
                "value": "Please see the weakness section above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3593/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698814887421,
        "cdate": 1698814887421,
        "tmdate": 1699636314676,
        "mdate": 1699636314676,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "J14t1LReeT",
        "forum": "gjeQKFxFpZ",
        "replyto": "gjeQKFxFpZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3593/Reviewer_Bvbz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3593/Reviewer_Bvbz"
        ],
        "content": {
            "summary": {
                "value": "A comprehensive study on self-estimation of uncertainty by LLMs. The authors introduce multiple design choices that can affect uncertainty elicitation and then evaluate them on existing LLMs."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Uncertainty estimation is an important question for LLMs. The findings of this study can be impactful in practice\n- The 3-part framework of prompting strategy, sampling and aggregation provide clarity.\n-The evaluation is comprehensive across many datasets."
            },
            "weaknesses": {
                "value": "The strategies presented are reasonable (but not super novel, as far as I understand). So the contribution should be evaluated on the empirical evaluation. Here, it is unclear how to interpret the results. No strategy seems to be work well always. \n\nI understand it is hard to analyze black-box LLMs, but still some discussion on why some strategy works, and why it does not, will be useful. \n\nWhat is the best combination of prompting, sampling and aggregation? can the authors propose the best way for practitioners while also noting the limitations? Right now, the message seems to be that nothing works. But it may be useful to present a version of the best performing method."
            },
            "questions": {
                "value": "see the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3593/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698908222697,
        "cdate": 1698908222697,
        "tmdate": 1699636314601,
        "mdate": 1699636314601,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aduFO3tpk2",
        "forum": "gjeQKFxFpZ",
        "replyto": "gjeQKFxFpZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3593/Reviewer_GBf5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3593/Reviewer_GBf5"
        ],
        "content": {
            "summary": {
                "value": "This paper studies approaches to eliciting reliable confidence estimates from large language models about their statements.  The paper studies closed-box methods for confidence elicitation exploring (1) prompting strategies for verbalized confidence; (2) sampling approaches to measure variance across multiple responses; and (3) aggregation methods to compute consistency measures."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper studies an important problem.  When an LLM's reliability is critical to an application or higher-level task, knowing the likelihood that the LLMs responses is correct or not is critical.\n\nThe overall approach, combining prompting, sampling, and aggregation, is an elegant approach for closed-box confidence solicitation, if it can be calibrated or its reliability as a measure otherwise completely characterized.\n\nThe experiments span many task-domains and multiple models."
            },
            "weaknesses": {
                "value": "The experimental results show that the method is useful in identifying uncertainty, but performance varies significantly across benchmark tasks and no single method is clearly better than others.  Given a new task, it is not clear which method will give the best performance, or even if the best performing method on a hold out set will generalize to real problems in a domain.  I appreciate that the authors call this out in the discussion, saying that none of the current algorithms are satisfactory.\n\nThe high level observations enumerated in the introduction are interesting but also seem very preliminary, and difficult to operationalize or build upon.\n\n\n\nminor:\n- the acronym ECE should be defined at first use.  \n- typo page 6: \"aevaluation\" -> \"evaluation\""
            },
            "questions": {
                "value": "Is there a reason to believe that the model's certainty _should_ be correlated with model correctness?  Or to understand a priori (e.g., based on training data) when it might be more or less likely to be so?\n\nMinor variations in prompt wording can have significant effects on performance.  How was the specific prompt text chosen for each of the prompt strategies?  was it optimized or experimented with?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3593/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699000294238,
        "cdate": 1699000294238,
        "tmdate": 1699636314522,
        "mdate": 1699636314522,
        "license": "CC BY 4.0",
        "version": 2
    }
]