[
    {
        "id": "GHCPs7Vnlv",
        "forum": "7Scc7Nl7lg",
        "replyto": "7Scc7Nl7lg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6104/Reviewer_HcSy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6104/Reviewer_HcSy"
        ],
        "content": {
            "summary": {
                "value": "The authors set out to map multimodal networks trained on text and vision to predict intracortical recordings from the brain using sEEG for epilepsy during movie watching. They find clear instances of sites encoding both language and text at the temporoparietal junction."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This is an informative and well-done study of multimodality measured through intracortical recordings. The data is unique and abundant and the evaluation was done for a large number of models. A lot of attention was put into the controls. As someone who specializes in the field of task-driven neural networks vs brains, I can appreciate that this is well-executed and will surely find a receptive audience for neuroscientists."
            },
            "weaknesses": {
                "value": "I don't think this is an appropriate venue for the paper. There's no clear methodological advance in ML that would be of broad interest to the ICLR community: it needs to be read by neuroscientists, not ML people. I looked at the neuroscience papers at ICLR in the last 2 years and found only one that would classify as investigating task-driven neural networks in brains in the style of Yamins, DiCarlo, Kriegeskorte, etc., and that paper showed a clear methodological contribution (https://openreview.net/forum?id=Tp7kI90Htd). The authors should look at where this type of neuroAI work is typically published, e.g. NeurIPS, SVRHM, cosyne, PNAS, Nature Comms, etc.\n\nEdit Nov 23rd: I have no real qualms about what's presented in this paper; in response to my original comments, the authors have argued that every once in a while, a task-driven neural network paper in this style is published at ICLR. Thus, my original questions about the suitability of this work for ICLR notwithstanding (and I will note, consistent with reviewer JsYg), I have increased my score to a 6."
            },
            "questions": {
                "value": "-"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "-"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6104/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6104/Reviewer_HcSy",
                    "ICLR.cc/2024/Conference/Submission6104/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6104/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698164416747,
        "cdate": 1698164416747,
        "tmdate": 1700764096456,
        "mdate": 1700764096456,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ucTvwZzyvK",
        "forum": "7Scc7Nl7lg",
        "replyto": "7Scc7Nl7lg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6104/Reviewer_KL3N"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6104/Reviewer_KL3N"
        ],
        "content": {
            "summary": {
                "value": "The current paper described a comparison between language/vision unimodal and language-vision multimodal models in predicting brain activities while the subject is watching movies. The authors trained model features to predict SEEG brain activities using ridge regression. Following the performance of the ridge regression on hold-out set of data, the authors identified the brain areas that best fit by a unimodal or multimodal model. This study shed light on the function of cortical areas."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "Using STOA vision and language models to help interpret the function of the brain is interesting."
            },
            "weaknesses": {
                "value": "Overall, no insights have been generated in the current study, nor have a solid novel methodology.\n\n1.\tMissing references in the related work section (paragraph 1, last two sentences; paragraph 2, 1st sentence). Missing figure references throughout the text. The writing of the paper lacks clarity. \n\n2.\tModel performance seems to be low. Figure 2a suggests that the Pearson correlation between model predictions and the true neuron activity is about 0.1. This means the model explains about 1% of the variance of the data (linear model r2). This is pretty low. Please explain why we should care about a model with limited prediction power. \n\n3.\tThe performance difference between brain regions, or between models is low, compared to the error bar per condition (figure 2a, and in multimodality tests). By the way, it is not clear to me what the error bar stands for. Is the difference between models or between electrodes sufficient for discrimination?\n\n4.\tWhether the model selection is consistent with the known biological function of the brain areas. \n\n5.\tDo all multimodal tests identify the same set of multimodal selection electrodes? Quantitative results should be provided. A standard procedure should be followed to identify brain areas as potential vision-language integration regions.   \n\n6.\tHow the regression model is trained? What is the input and what is the output? For each model with a different feature size, how does the parameter space for the ridge regression model differ between models? Whether the model performance was affected by the number of parameters?"
            },
            "questions": {
                "value": "1.\tClarify the analysis and model comparison criteria. \nFigure 1a suggests that the Pearson correlation coefficient is obtained per time bin per electrode per model. What exactly is in the vector that feeds into the Pearson correlation analysis? \nHow many image-text pairs are in the training, testing, and validation dataset? What is the fraction that achieved above threshold prediction? \nProvide example predictions and the corresponding true brain activities, and provide the Pearson r value for the example. \n2.\tImprove figure resolution, please. \n3.\tAddress all the questions raised in the Weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6104/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698795966279,
        "cdate": 1698795966279,
        "tmdate": 1699636659069,
        "mdate": 1699636659069,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9VUxAF3vWT",
        "forum": "7Scc7Nl7lg",
        "replyto": "7Scc7Nl7lg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6104/Reviewer_gGN5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6104/Reviewer_gGN5"
        ],
        "content": {
            "summary": {
                "value": "There is a multitude of papers training decoders from latent representations of DNN models onto brain activity. The goal is to reconstruct the brain activity, which, if successful, would indicate that there is something brain-like in the representations that are formed by artificial learning models. One cohort of such papers focus on models of vision (and predicting activity of visual areas of the human brain), while another on language models (and predicting the activity of language areas in the brain). In this paper the authors ask what if we take models that work on vision and language simultaneously - would the representations and activations of those models be more predictive of brain activity? And if so - in which regions?\n\nAn answer to this question might help us understand where in the brain are the areas that integrate different sensory modalities together, or at least work on several modalities at the same time.\n\nThe authors then take 14 models and use their representation to predict each electrode's recordings. The idea is that if a multimodal model's representation is significantly more useful for predicting the brain activity, then this model's representations are closer to what is happening in the brain, and thus can be thought of as evidence for that electrode's area being involved in multimodal processing of information.\n\nThe authors do find several such electrodes, but the number of those electrodes is not sufficiently high to draw strong conclusions (at least this was my impression from reading the paper, please correct me if I am wrong).\n\nOverall this work presents and very cool idea, which is well-executed and logically reported, however due to the lack of data (I suspect) there is just not enough ground to claim definitive findings."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Indeed an important and timely question, and, to the best of my knowledge, this work is first to explore this topic. I really like the idea and the research question.\n\nI very much support the emphasis you made on the fact that before any analysis is done we should confirm that there is either a strong difference in signal reconstructive power between trained and untrained networks, or a strong decoding ability -- we need to have a confirmation that the signal is indeed there before we analyse anything related to it."
            },
            "weaknesses": {
                "value": "My overall impression can be summarised as follows: while the first half of the paper is great and explains the idea and motivation really well, creating a rightful sense of expectation of the result, the section on the results somewhat comes short of delivering the findings with a bang. After reading the first half I was excited to read the next pages to find out \"where, indeed, are those areas that integrate vision and language?\" and tingling with an expectation of learning something new about our brains. But then, for some reason, the Results section is very timid and just presents dry numbers for each of the test that were planned. Were the differences in Pearson R not strong enough for the authors to be confident in their findings? What would be the strong result? Is this just the matter of presentation, or the result is too weak to claim some sort of victory and knowledge discovered?\n\n(1) It would be helpful to include a better explanation of what the \"event structures\" are exactly. Maybe a picture.\n\n(2) Page 6, Section 4, second paragraph: This bit of text here is a bit too overloaded with number and while the authors might expect their readers to be careful and try to understand what is the meaning and significance of those numbers being what they are... but a reader is rarely that careful. I would advise to add explanations to this paragraph that explain to the reader what they are supposed to think when they see this or other set of numbers. Tell the reader what they are supposed to with those numbers.\n\n(3) Figure 2: The caption does not explain the figure well. Panels (a) and (b) are not mentioned in the caption. An attempt to explain with \"mid left\" and \"bottom right\" is confusing, perhaps just put the names of the models on the figure plots. The dots on the freesurfer brain surfaces on the right of the figure are not explained at all - what are they? For the colour-blind it is very hard to see the red dot, I recommend using blue.\n\n(4) Page 8, Section 4.2, second test paragraph: It is unclear without further explanations what do the authors themselves make of this result. Was it interesting and/or significant? What did it demonstrate? Was this a strong result or no so much? Expected or unexpected? All in terms of the main research question of the paper.\n\n(5) Same as (5) applies to paragraphs on tests three, four and five. Actually one also. Currently these paragraphs are basically just a table of results but written out in words. A table with results and numbers is good, but we also need an interpretation and analysis of these results. Are they strong / interesting? Is there a scientific discovery here? What is it? How strong is the evidence? Were the R differences significant?\n\n(6) Page 9, first paragraph: \"we find that the largest contiguous cluster of these electrodes is found in and around the temporoparietal junction\" -- it would help a lot to see those electrodes on a picture of a brain! Not only in a way how it was presented on Figure 4, but actually each electrode plotted as a dot so that the reader could also discover this by actually seeing that \"yep, indeed, here are the electodes that are more predictive in multimodal regime and indeed they cluster around superior temporal region\". Figure 4, in my opinion, falls short of presenting this finding and sweeps the results under average-colored areas, raising questions why the plot was individual electrodes was not shown. Something like Figure 8 (supplementary G), but please use different colors for \"unimodal\" or \"multimodel across vision and language\" as the current selection of colors blurs together\n\n(7) In your own estimation, are those singular electrodes shown on Figure 8 as \"multimodal across vision and language\" provide sufficient evidence to multimodal processing in those areas? Or are the numbers too few to provide strong support for this claim? The yellow dots seem to be quite scattered, and we also don't know, for example, is the lack of them in visual areas explained by the fact that no electrodes in those areas were multimodal, or is this just because there were not electrodes implanted there?\n\n(8) It would help to evaluate the strength of the funding if we would be able to see a comparison (maybe a distribution plot) of multimodal over language-only / vision-only -- this would allow us to see not only where and how much of those electrodes exist, but also _how different_ their predictive power is. The averaged numbers you provide in section 4 are just averages and are just one number, hiding the true distribution we would be interested to see."
            },
            "questions": {
                "value": "(1) The implantation sites of sEEG electrodes in your dataset were clinically motivated. To what extent did they cover the areas you were interested in? Both whether all of the areas of interest were covered, and also among the the areas that were covered - was the coverage sufficient for you analysis in your estimation and why?\n\n(2) Are there multimodal models combining vision, text and audio? The data that you have contains all 3 modalities, what was/is the main obstacle to identifying tri-modal predictive regions in the brain? Is it the lack of appropriate DNNs or something else?\n\n(3) In your experiments were the subjects able to hear the audio track of the movie?\n\n(4) For your literature review here is another work comparing vision to DNN specifically on sEEG data from a 100+ subjects https://www.nature.com/articles/s42003-018-0110-y\n\n(5) How do you deal with the fact that the data comes from different subjects? Does inter/intra-subject considerations enter your analysis at all or you just consider each LFP electrode on its own regardless of the subject it came from?\n\n(6) Could you perhaps use fMRI data instead? You would lose temporal and frequency resolution, but for the level of analysis at which you are working these are not too relevant and demonstrating higher predictability of BOLD signal would be equally impressive and informative. More importantly it would allow you to capture the whole brain and there should be more datasets available for fMRI that for sEEG."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6104/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698890312872,
        "cdate": 1698890312872,
        "tmdate": 1699636658964,
        "mdate": 1699636658964,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zegMs2LTYY",
        "forum": "7Scc7Nl7lg",
        "replyto": "7Scc7Nl7lg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6104/Reviewer_9iyA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6104/Reviewer_9iyA"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to identify neural sites where multimodal integration is occuring in the brain. To achieve that, authors evaluate in which regions  multimodal (vision + language) models are better than unimodal models in predicting neural recordings (SEEG). \n\nUsing this method, the authors identify 141 out of 1090 total sites where multimodal integration is happening."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The methodology to identify multimodal sites is well described and quite comprehensive (trained vs. random, multimodal vs. unimodal, SLIP combo vs SLIP-SimCLR). The release of code will enable future work investigating similar questions with other modalities or other type of brain recordings\n2. The paper is easy to follow. This is due to clear writing and presentation of methods and results. \n3. Statistical tests and confidence intervals.  \n4. Multimodality test results on section 4.2. I appreciate 5 tests of multimodality reported in the results and how each test  filters out possible confounds."
            },
            "weaknesses": {
                "value": "1. One test that can also be included is to randomly input one of the modalities in a multimodal model and then compare with predictions using actual multimodal inputs. This test can reveal the importance of multimodal information avoiding the confounds due to architecture, parameters, training set etc. \n2. Did the authors perform SLIP-CLIP vs SLIP-SimCLR vs SLIP-Combo comparison? Because SLIP-CLIP is also multimodal I am curious what was the motivation for only showing SLIP-CLIP vs Combo results in Figure 4\n3. It is not clear to me why language alignment and vision alignment event structures leads to difference in results. I would like to read author\u2019s explanation on why results depend on how event structures and if this is a limitation of this approach."
            },
            "questions": {
                "value": "1. Is the dataset also part of this paper or is it from an already published paper? If yes then this is an additional contribution of this paper and should be emphasized.\n2. Will this dataset be publicly released? ( if not released already )"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6104/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698918250134,
        "cdate": 1698918250134,
        "tmdate": 1699636658867,
        "mdate": 1699636658867,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kOdmGYipr4",
        "forum": "7Scc7Nl7lg",
        "replyto": "7Scc7Nl7lg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6104/Reviewer_JsYq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6104/Reviewer_JsYq"
        ],
        "content": {
            "summary": {
                "value": "I have reviewed this work previously and the current version does not address my previous major concerns so I will repeat my points in this review in hopes that the authors can address them this time.\n\nThis work investigates the ability of multi-modal neural networks to align with multi-modal ECoG brain recordings, acquired while 7 epileptic children were watching movies. The authors aim to use a contrast between multi-modal and uni-modal models to reveal which locations in the brain relate to integration from multiple modalities. A large number of models are tested (7 multi-modal and 5 uni-modal). This work finds that about 13% of the tested neural sites are better predicted by multi-modal models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Using multi-modal brain recordings from movies\n- Using ECoG for high spatial and temporal precision\n- Checking many models (12, 2 models in depth)\n- An additional analysis that goes more in depth because, as the authors realize, a difference in brain predictivity in a direct comparison between a multimodal and a unimodal model can be due to the many possible differences between the models"
            },
            "weaknesses": {
                "value": "1. The biggest weakness is the central claim that the proposed analyses of multi-modal models can localize vision-language **integration** in the brain. Can the authors please define what they mean by vision-language integration? Even on the model side, it is an open question to what degree multi-modal models actually integrate information from multiple modalities as opposed to increasing the alignment between individual modalities (Liang et al. 2022 https://arxiv.org/pdf/2209.03430.pdf). It is not clear whether the results that are observed are due to vision-language integration or whether they are due to improved representations of the language-only or vision-only modality. For example, in Fig 4, all regions that are identified as multimodal (e.g. marked with a green star), are canonical language regions. How can the authors disentangle the effect of integration of modalities vs the effect of improving language-only information in the model? For instance, let's do a thought experiment and apply the authors' methods to study different layers of a language-only encoder: I predict that these results will be similar to what has been shown before which is that some layers in the language model predict exactly the same regions they call multimodal substantially better than other layers (see Jain and Huth, 2018 NeurIPS; Toneva and Wehbe, 2019 NeurIPS for some of the earlier work showing this). That clearly is not due to multimodality though because the input is only language.\n\n2. The presentation can be much improved: there is very little discussion of what the different models that are used are and how they are trained. This is key to understand the contributions of this work. I suggest the authors include a table of all models and model variations used with clearly marked information about what modality was used to train the model and what modality is used to evaluate the model (e.g. even if the authors are using only a vision encoder at inference time, if the vision encoder was jointly trained with a language encoder, this should be noted as this may make a difference in the representations)\n3. The work is still not positioned well in the current literature on multi-modal modeling of brain recordings, and it\u2019s not clear what the novelty here is for people who are unfamiliar with this area. The authors should discuss the work of Oota et al. 2022 COLING and Wang et al. 2022 bioRxiv https://www.biorxiv.org/content/10.1101/2022.09.27.508760v1. \n4. The contribution to an ML audience is not very clear. I believe this work will be better suited to be evaluated by neuroscientists, since the claimed contributions are on the neuroscience side, and will also be more appreciated at a neuroscience venue."
            },
            "questions": {
                "value": "See Weaknesses above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6104/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699009183914,
        "cdate": 1699009183914,
        "tmdate": 1699636658767,
        "mdate": 1699636658767,
        "license": "CC BY 4.0",
        "version": 2
    }
]