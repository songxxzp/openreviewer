[
    {
        "id": "uZiiRXdCzU",
        "forum": "2JF8mJRJ7M",
        "replyto": "2JF8mJRJ7M",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6807/Reviewer_Q7od"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6807/Reviewer_Q7od"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a method named LIPSUM-FT for zero-shot robust fine-tuning. Specifically, the authors, by observing the relationship between accuracy and energy gap under distribution shift, propose enhancing the accuracy of fine-tuned pre-trained models under distribution shifts by reducing the energy gap. The experimental results demonstrate that the method proposed in this paper can effectively improve the performance of CLIP during zero-shot fine-tuning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The authors explain the performance degradation of fine-tuned pre-trained models under distribution shifts from the perspective of energy models.\n\n2. The proposed LIPSUM-FT effectively enhances the robustness of fine-tuned pre-training.\n\n3. Ablation studies indicate that LIPSUM-FT is insensitive to random token lengths and token quantities."
            },
            "weaknesses": {
                "value": "1. Utilizing energy models to explain the fine-tuning of pre-trained models seems not to be essential. As per my understanding, the objective of the method in this paper as well as related methods ([1,2,3], etc.) is to reduce the difference in features extracted by the models before and after fine-tuning.\n\n2. The authors claim that the text used is randomly generated, but it appears from the code in the supplementary material that tokens are sampled from the openai_imagenet_template. According to CAR-FT, using all templates as text input also yields good performance. What then is the significance of random token sampling in this scenario?\n\n3. It is suggested that the authors provide a brief introduction to energy models in the related work section.\nIn Figure 1, it is not mentioned which points different learning rates in the left graph and different steps in the right graph correspond to.\n\n[1] Context-aware robust fine-tuning. \n\n[2] Fine-tuning can cripple your foundation model; preserving features may be the solution.\n\n[3] Robust fine-tuning of zero-shot models."
            },
            "questions": {
                "value": "The authors claim that the text tokens are randomly generated. What are the specific rules for generation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6807/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6807/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6807/Reviewer_Q7od"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6807/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698603712986,
        "cdate": 1698603712986,
        "tmdate": 1699770471299,
        "mdate": 1699770471299,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Q6cDrKfqNb",
        "forum": "2JF8mJRJ7M",
        "replyto": "2JF8mJRJ7M",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6807/Reviewer_26HN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6807/Reviewer_26HN"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on the problem of robust fine-tuning a large-scale vision-language pre-trained model, which is expected to obtain enhanced downstream performance while maintaining its accuracy across distribution shifts on zero-shot tasks. By investigating the behavior under the perspectives of feature distortion theory and joint energy-based models, the authors propose a robust fine-tuning algorithm Lipsum-FT. Experimental results on DomainNet and ImageNet proves effectiveness of their proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper focuses on the problem of how to maintain the performance of a zero-shot model on distribution shift data while improving its performance on reference data during fine-tuning, which is an important and valuable topic.\n2. The proposed method is easy to realize since it simply introduces an extra regularization term.\n3. The idea of utilize the language model to construct regularization on the vision model is interesting.\n4. The English writing is good and I do not find obvious grammar errors or typos."
            },
            "weaknesses": {
                "value": "1. I am not sure whether the novelty of this paper can be regarded as significant. It just introduces a regularization item to make the vision model keep the original inner products with features generated by the language model after fine-tuning.\n2. The illustration organization of this paper is not clear enough. Therefore, even though the key idea is not so complicated, I find it difficult to understand the viewpoint quickly."
            },
            "questions": {
                "value": "1. As the main contribution of this paper is to utilize the regularization term in (7) to minimize the energy gap in (6), please explain why the energy gap is chosen for improving the robustness of zero-shot model fine-tuning. Why is it defined to be the squared difference of two inner products as in (6)?\n2. Could the authors give more details about how the tokens $\\mathbf{t}$ are generated? The authors just assert that they are generated randomly from the vocabulary. I also want to know what types of texts are used for constructing such regularization and to what extent it covers the common used semantic information.\n3. How much extra computation are introduced by such a regularization term?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "I have no ethics concerns about this paper."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6807/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6807/Reviewer_26HN",
                    "ICLR.cc/2024/Conference/Submission6807/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6807/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698754685299,
        "cdate": 1698754685299,
        "tmdate": 1700446435830,
        "mdate": 1700446435830,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Er2xAgqeXX",
        "forum": "2JF8mJRJ7M",
        "replyto": "2JF8mJRJ7M",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6807/Reviewer_2e7P"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6807/Reviewer_2e7P"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a fine-tuning model, Lipsum-FT, by utilizing the language modeling aspect of the vision-language pre-trained models for zero-shot classification."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper proposed a fine-tuning model, Lipsum-FT, by utilizing the language modeling aspect of the vision-language pre-trained models for zero-shot classification. Lipsum-FT as an incremental algorithm enhances the robustness of zero-shot models by combining language and visual information."
            },
            "weaknesses": {
                "value": "The motivation is unclear. The authors mention a problem with cross-distribution shifts, but Lipsum-FT is a fine-tuning model."
            },
            "questions": {
                "value": "What means of four distribution shifts DomainNet-{P, C, I, S}. Figure 1 fails to represent the distribution shifts.\n\nThere are too many contents that need to be referred to Appendix. Please reduce some important Appendix contents and put them into the text.\n\nIt looks like capturing correlations between images and each language by calculating their inner product vectors in Equation 8. The meaning of Equation 8 wants to express is to match all the language information one by one or to match the current m-th information? If it matches M language information, how much does the algorithm complexity increase? What is the significance of matching with alonely m-th?\n\nLack of complexity analysis on the Lipsum-FT algorithm and its impact on the experiments."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6807/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6807/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6807/Reviewer_2e7P"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6807/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698899389307,
        "cdate": 1698899389307,
        "tmdate": 1699636786893,
        "mdate": 1699636786893,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NLhgk4EMVM",
        "forum": "2JF8mJRJ7M",
        "replyto": "2JF8mJRJ7M",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6807/Reviewer_MuzV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6807/Reviewer_MuzV"
        ],
        "content": {
            "summary": {
                "value": "The paper points out that fine-tuning zero-shot model like CLIP can improve downstream performance, however, the accuracy of the fine-tuned model falls short of the original zero-shot model across distribution shifts. To address the chanllenge of robust fine-tuning, authors first delve into the problem by employing feature distortion theory and joint energy-based models. Subsequently, they introduce a novel robust fine-tuning algorithm called Lipsum-FT. This approach leverages random text guidance as a regularization technique during the fine-tuning process to minimize the change in energy. Authors conduct extensive experiments on two datasets to demonstrate the effectiveness of the proposed approach in addressing distribution shift scenarios."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper investigate the trade-off between the reference and distribution shift data when fine-tuning the zero-shot CLIP model, utilizing feature distortion theory and joint energy-based models as analytical tools.\n2. This paper proposes a simple and effective regularization term based on the correlation between the similarity of vision features and text features derived from the fine-tuned model and original model respectively. Specifically, the text tokens are generated randomly.\n3. The proposed method outperforms the original zero-shot model and exisiting robust fine-tuning methods in both reference and shift domains, demonstrating its superior performance in handling distribution shifts."
            },
            "weaknesses": {
                "value": "1. It would be benefical if authors could include visualizations of $v_{\\theta, \\phi}(x)$ and $v_{\\theta_0,\\phi}$ to provide an inituitive understanding of the distinctions between the original zero-shot model, the fine-tuned model with exisiting methods, and the fine-tuned model with the proposed emthod.\n2. Expanding the experiments to involve various domains as reference data for fine-tuning and using other domains for evaluation would enhance the comprehensiveness of the study. This approach can shed light on the adaptability and robustness of the proposed method in different real-world scenarios."
            },
            "questions": {
                "value": "1. It remains unclear whether there exists a weight for incorporating the regularization term proposed in Eq. (7) into the loss function. In Appendix B.2, authors have discussed existing methods that involve weights related to the regularization term, such as $\\lambda_{L2SP}$ in Eq. (10), $\\lambda_{KD}$ in Eq. (12), and $\\lambda_{CAR-FT}$ in Eq. (12). Authors have also detailed how to select these hyperparameters. However, there seems no mention of how the weight for the proposed method is determined.\n2. The precision of the standard deviation values in Table 2(b) should be improved. The values of NLL are presented accurately to two decimal places, whereas the standard deviation values are limited to only one decimal place, with many of them showing as 0.0. Ensuring consistent precision in reporting these values would enhance the clarity and reliability of the results.\n3. There may be a typo in the gradient descent update rule presented in Eq. (9). It should be as follows: $\\theta_t = \\theta_{t-1} - \\eta \\nabla_\\theta L_{CE}(\\theta)$. It's advisable for the authors to thoroughly review other equations to ensure they are accurately represented.\n4.It would be interesting to know if the proposed method is applicable to various fine-tuning techniques, such as adapters, LoRA, and prompt tuning."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6807/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6807/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6807/Reviewer_MuzV"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6807/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698927926459,
        "cdate": 1698927926459,
        "tmdate": 1700457932753,
        "mdate": 1700457932753,
        "license": "CC BY 4.0",
        "version": 2
    }
]