[
    {
        "id": "OSxzqxHYkh",
        "forum": "MXI8aSgl53",
        "replyto": "MXI8aSgl53",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5960/Reviewer_f3XV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5960/Reviewer_f3XV"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel, robust and accelerated stochastic optimizer based on combining Nesterov's AGD and semi-implicit Gauss-Seidel method to obtain an iterative scheme for the optimization. Convergence of this algorithm in the quadratic case is proved and numerical experiments are demonstrated for logistic regression model, ResNet-20, VGG-11 and Transformers."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "proposed a novel stochastic optimizer by combining Nesterov AGD with Gauss-Seidel semi-implicit method"
            },
            "weaknesses": {
                "value": "(1) it seems the contribution is relatively moderate by just combining Nesterov with Gauss-Seidel\n(2) some of the writings are sloppy. Example: page 4, line 2, two \"either\" appeared"
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5960/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698356216543,
        "cdate": 1698356216543,
        "tmdate": 1699636636080,
        "mdate": 1699636636080,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "j935QU9Swb",
        "forum": "MXI8aSgl53",
        "replyto": "MXI8aSgl53",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5960/Reviewer_13Pv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5960/Reviewer_13Pv"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a stochastic method through applying implicit Gauss-seidel splitting method on the continuous-time ODE mimicing the trajectory of Nesterov\u2019s accelerated method. They prove the convergence of their algorithm for the special case of strongly-convex quadratic functions in Theorem 1. They extended their algorithm to non-convex functions heuristically based on the step-size they found for the special case of strongly convex quadratic functions. Several experiments were conducted to evaluate the performance of their method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1- Connecting theoretical findings with practical applications and implementations.\n\n2- One step toward practical implementations through considering stochastic extension of prior art in [Luo & Chen (2021)].\n\n3- Text is smooth and easy to read."
            },
            "weaknesses": {
                "value": "1- Related work is very imited. The idea of analyzing accelerated methods through their continuous time perspective has been around for quite some time (since [Su, et. al (2014)] or even before that by [Alvarez & Attouch (2001)] and most of the related works mentioned deal with intrepretations of deterministic methods. It makes more sense to focus on works that see stochastic accelerated methods through the lens of ODEs since these are more related to the proposed research.\n\n2- The theoretical analysis is bounded to quadratic case. This is not mentioned accurately in the contribution. Specifically the second main contribution is: **We analyze the properties of the proposed method both theoretically and empirically;**. \n\n3- The introduction is not really an introduction of this work. By just reading the introduction, it is not possible to get an accurate idea of what differences this work has with any other work in \"stochastic optimization algorithms\".\n\n4- \"The Preliminaries\" is a mixture of background, related work and notations. Here, more organization might improve readability. \n\n5- Gaus-Seidel splitting used here is not a novel idea in discretizing ODEs for acceleration as it was previously discussed in [Luo & Chen (2021)].\n\n\nReferences\n\nAlvarez, F., Attouch, H. An Inertial Proximal Method for Maximal Monotone Operators via Discretization of a Nonlinear Oscillator with Damping. Set-Valued Analysis 9, 3\u201311 (2001)."
            },
            "questions": {
                "value": "1- What is $\\mathcal R(\\lambda)$ under (8)? Does it extract the real part of $\\lambda$?\n\n2- Have you tried the predictor-corrector method in [Luo & Chen (2021)] and extend it to the stochastic case (like the way you did for the semi-implicit GS)?\n\n3- [Shi, et al. (2019)] showed that semi-implicit Euler discretization of a \u201chigh-resolution ODE\u201d exactly recovers the NAG algorithm. This is not the case for other ODEs like the one in  [Su, et. al (2014)] or [Luo & Chen (2021)], unless some corrections are made. Do you think applying semi-implicit GS on high-resolution ODEs (combined with stochastic gradients) can lead to an even better algorithm?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5960/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5960/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5960/Reviewer_13Pv"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5960/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698500019297,
        "cdate": 1698500019297,
        "tmdate": 1699636635969,
        "mdate": 1699636635969,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vO8E0ST6db",
        "forum": "MXI8aSgl53",
        "replyto": "MXI8aSgl53",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5960/Reviewer_nCbP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5960/Reviewer_nCbP"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel stochastic optimizer called NAG-GS, which combines elements of Nesterov-like Stochastic Differential Equation (SDE) acceleration and semi-implicit Gauss-Seidel type discretization. The method's convergence and stability are extensively analyzed, particularly in the context of minimizing quadratic functions. The authors determine an optimal learning rate that balances convergence speed and stability by considering various hyperparameters. NAG-GS is shown to be competitive with other state-of-the-art methods like momentum SGD with weight decay and AdamW when applied to various machine learning models, including logistic regression, residual networks, Transformers, and Vision Transformers across different benchmark datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The NAG-GS is derived from an accelerated Stochastic Differential Equation (SDE) using its semi-implicit Gauss-Seidel type discretization, which is interesting.\n\n- The convergence analysis for the quadratic case is comprehensive."
            },
            "weaknesses": {
                "value": "- The discussion of NAG-GS with other similar methods is insufficient. For example, Is NAG-GS faster than Polyak's momentum method for solving quadratic objectives? How does it compare with other variants of NAG, such as Triple momentum method [1, 2], and ITEM [3]? It is not clear what is the key benefit of NAG-GS in its original setting.\n\n- The improvement in the neural network experiments seems marginal. No deviation statistics is provided in the empirical results.\n\n- A minor point: As one of the key features of NAG-GS, the derived optimal learning rate should be mentioned and discussed in the main text.   \n\n\n\n\n[1] Van Scoy, B., Freeman, R. A., & Lynch, K. M. (2017). The fastest known globally convergent first-order method for minimizing strongly convex functions. IEEE Control Systems Letters, 2(1), 49-54.\n\n\n[2] Zhou, K., So, A. M. C., & Cheng, J. (2020). Boosting first-order methods by shifting objective: new schemes with faster worst-case rates. Advances in Neural Information Processing Systems, 33, 15405-15416.\n\n\n[3] Taylor, A., Drori, Y. An optimal gradient method for smooth strongly convex minimization. Math. Program. 199, 557\u2013594 (2023)."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5960/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698944409696,
        "cdate": 1698944409696,
        "tmdate": 1699636635868,
        "mdate": 1699636635868,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "p9u92Q8gkL",
        "forum": "MXI8aSgl53",
        "replyto": "MXI8aSgl53",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5960/Reviewer_MgiM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5960/Reviewer_MgiM"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new accelerated optimization algorithm using gauss-siedel discretization of a randomization version of the recent spectral lifting technique of Luo and Chuo 2021"
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The paper is largely well written with minor polishing still required. The method seems sound from a few empirical and numerical experiments conducted by the authors, and achieves competitive performance."
            },
            "weaknesses": {
                "value": "The main weakness is that the convergence analysis is only for the quadratic case, and the convergence itself as stated in theorem 1 is a weak statement with only asymptotic convergence. I did not go through the entire proof, but i can understand that the analysis for general f (replacing Ax with grad f  for the method) is non-trivial, since the \"lifted\" spectrum needs to be bounded effectively. It seems one also requires prior knowledge of \\mu for the algorithm which is very limiting, and the convergence analysis also is only valid for strongly convex cases.  \n\nHowever, the empirical results are quite strong which warrants that the community should know about this paper.\n\nThere are some typos etc that need to be fixed."
            },
            "questions": {
                "value": "Algorithm 1 requires prior knowledge of \\mu. How did you set up the algorithm practically for non-convex losses ?\n\nMostly minor writing stuff: \n\nBackground: Please explain the definition of A-stable. If you have gone as far to explain the discretization process itself, adding A-stable for reading not familiar with it would benefit from it.\n\nWhat is \\mathcal{R}(\\lambda) below (8) ? \n\n\u201cHowever, this requires to either solve a linear system either.\u201d --> \u201cHowever, this requires to either solve a linear system or.\u201d  \n\nPlease mention what is the baseline SGD-MW before using it in table 1. Is there a reason Adam or a variant of it was considered as a baseline for Resnet20 ?\n\nWhat is the accelerated gradient descent baseline in Figure 1 ?\n\n\u201cBut still, it is expected that an explicit scheme closer to the implicit Euler method will have good stability with a larger step size than the one offered by a forward Euler method. \u201c \u2013 why ?\n\nCan the authors provide some future work directions ? convergence analysis ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5960/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699364838626,
        "cdate": 1699364838626,
        "tmdate": 1699636635771,
        "mdate": 1699636635771,
        "license": "CC BY 4.0",
        "version": 2
    }
]