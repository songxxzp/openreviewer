[
    {
        "id": "meFifnYRr9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5476/Reviewer_ukN9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5476/Reviewer_ukN9"
        ],
        "forum": "KSjPaXtxP8",
        "replyto": "KSjPaXtxP8",
        "content": {
            "summary": {
                "value": "This paper proposes a novel framework that defines memorization within the context of self-supervised learning (SSL). This definition compares the difference in alignment of representations for data points and their augmented views returned by encoders that were trained on these data points and encoders that were not. Empirical analysis on diverse encoder architectures and data sets demonstrate that significant fractions of training data points experience high memorization in SSL, and memorization is essential for encoders to achieve higher generalization performance on different downstream tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The authors propose a novel definition that generalizes memorization effect to self-supervised learning, which can be useful for understanding model generalization\n- This paper is easy to understand"
            },
            "weaknesses": {
                "value": "- The proposed framework lacks enough practical implications. It is not so clear how we should treat samples with higher/lower memorization scores differently. \n- Several claims are subject to challenge, as can be found in Questions part. \n- Empirical results may be further improved, including more different architectures and data sets."
            },
            "questions": {
                "value": "- The definition of (per-example) memorization score is a bit strange. From my perspective, a higher memorization score indicates that sample $x$ has larger impact to model performance. As such, it seems natural that pre-trained models with higher memorization perform well on downstream tasks: if not, it means all training samples do not contribute much to pre-training, and it will be strange that pre-training on such data set produces good models. Some more explanations may be needed on why we need to introduce memorization score here, instead of some other performance metrics. \n- Also, it would be better if the authors can show some connections between other metrics on generalization (examples may be found in [1]) and the proposed memorization score. It would be useful to see how good final performance comes from higher memorization scores beyond simply putting them together. \n- While the authors claimed that their experiments cover different architectures and downstream data sets, the experiments seem a bit restricted from my perspective. For architectures, it would be better if the authors can report i) same architecture (e.g., ResNet) with different depths, widths, etc. and ii) similar number of parameters with different architectures (ViT and ResNet here). That will make the experimental results more comprehensive and we may also gain some insights on how memorization differs across different architectural settings\n- Also, downstream data sets used in current paper (CIFAR-10/100, SVHN, STL-10, ImageNet) are all coarse-grained general classification data sets. The authors may consider also adding some experiments on fine-grained data sets (e.g, Food-101 or Flower102, which are popularly used in literature) and see if their conclusions are changed. \n- Regarding experiments on removing samples for downstream tasks, the performance gap seems not so large. What will happen if we directly try to remove some samples to obtain largest performance drop? Will these samples found have larger memorization scores? Some additional experiments are welcome. \n- I also wonder how it is connected to research on coresets [2], which aims to find a (small) subset of training data that can help obtain models with performances close to full training data. The authors may report something opposite to Figure 3 and Table 3: solely train on samples with high memorization scores, and see how the model works.  \n\nReferences: \n\n[1] Fantastic Generalization Measures and Where to Find Them. ICLR 2020\n\n[2] Deep Learning on a Data Diet: Finding Important Examples Early in Training. NeurIPS 2021"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5476/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5476/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5476/Reviewer_ukN9"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5476/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697187403540,
        "cdate": 1697187403540,
        "tmdate": 1700704410266,
        "mdate": 1700704410266,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aUlVJAb9Wt",
        "forum": "KSjPaXtxP8",
        "replyto": "KSjPaXtxP8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5476/Reviewer_mj5F"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5476/Reviewer_mj5F"
        ],
        "content": {
            "summary": {
                "value": "The authors address an interesting issue in self-supervised learning (SSL): the impact of memorization on both SSL and its effects on downstream tasks. They examine the dissimilarity in the representations alignment between augmented views produced by encoders trained on a specific point and those that were not. This problem statement holds relevance in light of the current privacy concerns surrounding deep learning models"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- It is true that there are not many works addressing the memorization vs. generalization aspect of SSL. This paper aims to address this important issue.\n\n- The proposed method is agnostic to SSL methods and augmentations, making it applicable in various cases.\n\n- It is an insightful study in the right direction. Understanding memorization significantly impacts the generalization of SSL models. The authors have done an excellent job with their extensive empirical analysis."
            },
            "weaknesses": {
                "value": "- There are several works discussing the issue of memorization in self-supervised learning, which haven't received thorough attention in the literature review. While I understand that not all of these works are directly related, a broader discussion of related literature would enhance the paper's readability (e.g., [1, 2]).\n\n- In the machine learning research community, it's generally accepted that there exists a tension between memorization and generalization. However, I would appreciate more clarity and intuition from the authors regarding why memorization leads to improved generalization.\n\n- Although the memorization score is simple, model- and augmentation-agnostic, it lacks intuitive explanations and theoretical background. It would be beneficial for the authors to provide reasons for why this score measures memorization and offer any applicable theoretical support.\n\n- Regarding the statement, \"Our definition compares the difference in alignment of representations for data points and their augmented views returned by both encoders that were trained on these data points and encoders that were not,\" I'd like to seek clarification. It seems that the training sets of g and f overlap. Could you please provide a justification for this design choice? I had expected a distinct encoder with a different training set or random initialization.\n\n- Considering the proposed metric and previous works in supervised learning, it appears that the contributions are more incremental than novel.\n\n[1] Sadrtdinov, Ildus, Nadezhda Chirkova, and Ekaterina Lobacheva. \"On the Memorization Properties of Contrastive Learning.\" arXiv preprint arXiv:2107.10143 (2021).\n\n[2] Bansal, Yamini, Gal Kaplun, and Boaz Barak. \"For self-supervised learning, Rationality implies generalization, provably.\" ICLR 2020."
            },
            "questions": {
                "value": "- \u201cwe consider a data point as having a high level of memorization by an encoder $f$ if its alignment is significantly higher on $f$ than on encoder g that was not trained with the considered data point\u201d \u2013 Why is this the case? Are there any exceptions where this doesn't hold true?\n\n- I understand that the memorization score is relative. However, can one freely choose $g$ as our interest primarily lies in $f$? Have you conducted any ablation studies on the choice of model architectures for $g$ while keeping $f$ constant? Is it necessary for both $g$ and $f$ to share the same architecture?\n\n- What if $g$ is pre-trained on a large but distinct dataset instead of being randomly initialized? Would that result in a reduced memorization score?\"\n\n- The training data is divided into 80%, 10%, and 10%. The last two sets do not overlap between $g$ and $f$. How does memorization change when we vary the overlapping ratios from 80% to 70%, 50%, and 30%?\n\n- \u201cWe formally verify that data points from Sc (Si ) have statistically significantly higher (lower) memorization scores m than those from Ss and Se.\u201d\n\u201cThey support the claim that Sc (Si ) is substantially more (less) memorized than Ss and\nSe\u201d.\nWhy is it the case that Sc is substantially more memorized than Ss? Isn't this because memorization is a relative score, and Ss was used to train both $g$ and $f$?\n\nI find this work quite interesting overall. However, in its current form, it lacks sufficient intuition to answer 'why' questions such as:\n\n- Why does memorization lead to generalization?\n- Why is the proposed metric suitable for measuring memorization?\n- Why were certain design choices made, like training percentages and architecture selections for $g$ and $f$?\n\nThe authors need to provide more comprehensive reasoning for their results. Simply presenting empirical findings is insufficient and can lead to confusion for readers."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5476/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5476/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5476/Reviewer_mj5F"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5476/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698572094736,
        "cdate": 1698572094736,
        "tmdate": 1700642777071,
        "mdate": 1700642777071,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6x2QScBiLF",
        "forum": "KSjPaXtxP8",
        "replyto": "KSjPaXtxP8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5476/Reviewer_tPGb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5476/Reviewer_tPGb"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a way to measure memorization at the representation level, which is applicable to SSL approaches, in contrast to previous work quantifying memorization in supervised learning. The memorization metric is based on measuring differences in alignment between different views of the same input point, between models trained with and without the specific point.\nWith their new measure, the authors investigate the degree of memorization in encoder models using different architectures, trained on different datasets and using different SSL approaches. They find that SSL-trained models exhibit memorization and that the degree of memorization benefits downstream performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Relevance: Understanding memorization is an important problem, which is challenging and underexplored in the SSL domain. The paper makes an important contribution in conceptualizing memorization in this space, as well as proposing a corresponding measure. Further, a representation-level measure of memorization is a valuable tool that could be applied in other interesting ways as well, such as localizing memorization in models.\n- Soundness: The experiments are thorough and the methodology is solid.\n- Presentation: The paper is well written and easy to follow.\n\n### Neutral:\n- Novelty: The findings seem to be similar to those made for supervised learning, i.e. that memorization benefits generalization, as well as atypical points exhibiting more memorization. However, SSL approaches use a different learning paradigm, so it is not a priori clear whether one should expect similar trends to hold. It is therefore interesting to see that similar dynamics hold for SSL models as well."
            },
            "weaknesses": {
                "value": "- The results seem to show that memorization benefits downstream performance. However, *why* this is happening is not quite clear to me. For instance, when removing points with high memorization scores from the training data, does the performance of the model primarily drop on points \"similar\" to the removed ones? If this was the case, memorization here might actually be more of a long-tail generalization phenomenon.\n- In Section 4.4, Eq. (3) you limit the degree of alignment between representations. However, in addition to reducing memorization, this intervention might also degrade the representations in other ways. Therefore, it's not clear to me whether we can conclude that a reduction in memorization is causing a drop in accuracy, or whether both might just be consequences of a degradation in representation quality due to the regularization.\n- There are some smaller clarity issues:\n    1. In Section 4, first paragraph, what is the normalization procedure applied to constrain memorization scores between -1 and 1?\n    2. You say that experiments are repeated over three independent seeds. Does that mean you use different data splits for each seed or just different weight initializations of the models?\n    3. Table 1, what is \"Frac. Mem.\"? Is it the same as \"Avg. Mem.\" defined in the caption?\n    4. In 4.4, do you remove the 500, 1K, etc. datapoints with highest memorization from the set of 25K points, or from the full CIFAR10 training set?\n    5. In 4.4, why do you use cosine similarity here vs l2 distance earlier?\n    6. In 4.5, what does the term \"exploited\" in the context of Deja Vu mean? Would you expect MAE to exhibit higher memorization?"
            },
            "questions": {
                "value": "- Does the metric agree with previous metrics for quantifying memorization in supervised learning? I.e. given a supervised learning model (where you can apply prior supervised learning work), would the metric highlight the same points as memorized as previous metrics for supervised learning?\n- How dependent is the metric on the types of data augmentations used? I.e. if a model was trained with masking, would the metric also be able to quantify memorization under e.g. rotation or noise augmentations?\n- What are atypical datapoints? Is the judgement of typicality just based on visual inspection or are there other indicators as well?\n- The idea behind the metric is that quantifying memorization via alignment differences between different augmentations of the same input point is the common denominator between different SSL approaches (contrastive, non-contrastive, reconstruction-based). Would it be possible to define a \"stronger\" notion of memorization if one were to only consider one family of SSL approaches, e.g. contrastive ones?\n\n### Suggestions:\n- Giving the memorization metric a name would make it easier to refer to it.\n- Given that the proposed metric operates at the representation level, it might be interesting to quantify memorization at intermediate layers in the model, to potentially localize where it is happening."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5476/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5476/Reviewer_tPGb",
                    "ICLR.cc/2024/Conference/Submission5476/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5476/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698685949625,
        "cdate": 1698685949625,
        "tmdate": 1700596506157,
        "mdate": 1700596506157,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "codnOxVTwE",
        "forum": "KSjPaXtxP8",
        "replyto": "KSjPaXtxP8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5476/Reviewer_fWKE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5476/Reviewer_fWKE"
        ],
        "content": {
            "summary": {
                "value": "In this paper the authors propose SSLMem, a framework for defining memorization within Self Supervise Learning (SSL). The authors base their framework analyzing the  difference in alignment of representations for data points and their augmented views returned by encoders. They show an empirical analysis on diverse encoder architectures and datasets, highlighting that significant fractions of training data points experience memorization, and highlight that memorization is essential for encoders to achieve generalization performance on downstream tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "-good presentation and writhing\n\n-easy flow of argumentation\n\n-interesting and valuable insights related to the interplay between memorization and generalization"
            },
            "weaknesses": {
                "value": "-importance and influence of the augmentations, it would have been nice to see does a particular or a set of augmentations plays a role in this empirical evaluation, also instead of the augmentations, how similar data samples play a role it would be interesting to analyze\n\n-regarding the experiment considering differential privacy, only one algorithm was evaluated, I was not able to see other evidence (evaluation using different setups and algorithms) that supports the case about memorization in this context"
            },
            "questions": {
                "value": "Does a particular augmentation or a set of augmentations plays a role in this empirical evaluation?\n\nHow does similar data samples play a role with respect to the memorization vs generalization claims?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5476/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5476/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5476/Reviewer_fWKE"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5476/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1700649721820,
        "cdate": 1700649721820,
        "tmdate": 1700649788381,
        "mdate": 1700649788381,
        "license": "CC BY 4.0",
        "version": 2
    }
]