[
    {
        "id": "QSxTgcHRYm",
        "forum": "liuqDwmbQJ",
        "replyto": "liuqDwmbQJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7649/Reviewer_DXPK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7649/Reviewer_DXPK"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a zero-shot benchmark for linguistic and temporal grounding in video-language models. The evaluation focuses on five aspects: action counting; the recognition of specific actions or action participants; the recognition of action or event subphases; the recognition of rare actions; and distinguishing spatial relations. Experiments show that there is no essential difference between video-language models and image-language models in terms of temporal reasoning abilities."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The motivation of this paper is very important, that is, to establish a fair and reasonable benchmark for linguistic and temporal grounding in video-language models.\n2. The experiments in this paper reveal that there is no essential difference between video-language models and image-language models in terms of temporal reasoning abilities. It provides the direction for the future development of the video-language models.\n3. The paper is well written and easy to follow."
            },
            "weaknesses": {
                "value": "1. The form of the benchmark is still relatively simple, just let models choose the correct answer from two candidate sentences. However, simple two-choice questions are not enough to fully measure the ability of the model.\n\n2. In my opinion, the temporal understanding ability should include having the model locate where an event starts and ends in a video based on the description. More complex temporal understanding requires the model to analyze the events that occur in the video, and infer the actions that may occur in subsequent videos. Therefore, in my opinion, the proposed benchmark does not fully measure the temporal understanding ability of video-language models.\n\n3. With the success of large language models, the latest video-language models, e.g., BLIP2 can output text with variable length and free content. The community may be more concerned about how to properly evaluate these open outputs."
            },
            "questions": {
                "value": "Do you consider doing more analysis on the video itself, such as exploring the sensitivity of the model's temporal understanding ability to the length of the video? What happens to the model if you insert noise frames into the video?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7649/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7649/Reviewer_DXPK",
                    "ICLR.cc/2024/Conference/Submission7649/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7649/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697813302529,
        "cdate": 1697813302529,
        "tmdate": 1700651755765,
        "mdate": 1700651755765,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "U7Uz5Ll6ca",
        "forum": "liuqDwmbQJ",
        "replyto": "liuqDwmbQJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7649/Reviewer_9Dpj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7649/Reviewer_9Dpj"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a zero-shot evaluation benchmark designed to require a strong temporal understanding of video-language models. The proposed benchmark is task-agonistic. They evaluate multiple video-language models and image-language models on the proposed benchmark. They find that video-language models do not have a significant advantage over image-language models, and the input understanding of the model is not robust."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed benchmark is novel and fills a gap in video language model evaluation that tests the zero-shot temporal understanding and reasoning capabilities.\n2. The required capabilities in the proposed benchmark are well-classified."
            },
            "weaknesses": {
                "value": "1. There are some of the most recent VidLMs with good performances missing in the evaluation. E.g. InternVideo [1], mPLUG2 [2], Uniformer v2 [3], etc. The necessity of the proposed dataset needs more evaluation to validate. \n\n[1] InternVideo: General Video Foundation Models via Generative and Discriminative Learning\n[2] mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image and Video\n[3] UniFormerV2: Spatiotemporal Learning by Arming Image ViTs with Video UniFormer"
            },
            "questions": {
                "value": "1. When testing image-language models, which frame from the video is input to the model?\n2. Next-QA [1] also requires the model to perform temporal understanding and reasoning. It seems the main difference between the proposed dataset and Next-QA is the format. Can the author explain the core challenges posed by the proposed dataset?\n\n[1] NExT-QA: Next Phase of Question-Answering to Explaining Temporal Actions"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7649/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7649/Reviewer_9Dpj",
                    "ICLR.cc/2024/Conference/Submission7649/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7649/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697961110206,
        "cdate": 1697961110206,
        "tmdate": 1700683365386,
        "mdate": 1700683365386,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HUjAX7e3vx",
        "forum": "liuqDwmbQJ",
        "replyto": "liuqDwmbQJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7649/Reviewer_cNVC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7649/Reviewer_cNVC"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the problem of evaluating the temporal understanding ability of Video Language Models (VidLMs). It proposes a benchmark called VILMA by constructing \u201cfoil\u201d video captions from existing datasets. Specifically, the foil captions are created by replacing certain phrases in the original captions and then the VidLMs are asked to distinguish between the original and foil captions. The foil captions can be divided into five categories, covering a wide range of temporal understanding abilities. This paper also introduces the **proficiency tests**, which assess the primary abilities required to effectively understand the temporal dynamics (**main tests**). The proficiency tests are designed to examine whether the performance in main tests is robust.\n\nBased on VILMA, a number of VidLMs, image-language models (ILMs) and text-only models are tested. The results show that: (1) existing VidLMs exhibit very poor temporal understanding ability, which is not better than ILMs (even not better than random baseline in particular categories). (2) The performance of VidLMs and ILMs declines significantly when considering the proficiency test, which suggests that they may predict correct answers by chance or by exploiting some spurious features."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The proposed benchmark is novel and valuable, which can provide a more comprehensive evaluation of temporal understanding ability than existing benchmarks.\n* The evaluation results reveal the poor temporal understanding ability of existing VidLMs struggle, which can guide the development of more advanced VidLMs.\n* The paper provides comprehensive details of the proposed benchmark, including the construction process, data distribution and examples."
            },
            "weaknesses": {
                "value": "* The difference between VILMA and existing foiling benchmarks is not adequately described. The reviewer would like to know more details about why VILMA is more comprehensive.\n* The gap between P+T suggests that there exists inherent dataset bias in VILMA, which can be exploited to achieve good performance. This phenomenon seems contradictory to the claim on page 24 that \u201cthe biases are not significantly present\u201d.\n* The evaluation could be more comprehensive by including recent Video Large Language Models (Video LLMs), e.g., VideoChat [1], Otter [2], Video-LLaMA [3].\n\n[1] VideoChat: Chat-Centric Video Understanding.\n[2] Otter: A Multi-Modal Model with In-Context Instruction Tuning.\n[3] Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding."
            },
            "questions": {
                "value": "Please refer to the **Weaknesses**"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7649/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7649/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7649/Reviewer_cNVC"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7649/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698048002976,
        "cdate": 1698048002976,
        "tmdate": 1699636930089,
        "mdate": 1699636930089,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8QzLAbv86l",
        "forum": "liuqDwmbQJ",
        "replyto": "liuqDwmbQJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7649/Reviewer_f3MM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7649/Reviewer_f3MM"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a new suite of benchmarks for video-language models (VidLMs), which requires the VidLMs to distinguish between factual and counterfactual descriptions of the videos. The benchmarks are further divided into a number of tests, including Action Counting, Situation Awareness, Change of State, Rare Actions, and Spatial Relations. Each test has an easy version (proficiency test) and a hard version (main test). \n\nThe paper further evaluates a large number of VidLMs, together with LMs and ILMs. Interestingly, even the blind LMs can achieve significantly higher-than-random accuracy on the Situational Awareness and Spatial Relations tests, which suggests strong linguistic prior. However, the VidLMs often perform worse than the blind LMs on the two tests. Further, the image-only ILMs often outperform the VidLMs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "Evaluations of large VidLMs pose significant challenges. As the paper noted, even if the models achieve good accuracy on the main test, it does not mean it can achieve high scores on the supposedly easier proficiency test. Hence, many test results create misleadingly high performance numbers that lead to the illusion of human-like performance, and contribute to concerns of existential risks. \n\nThis paper presents a solid step in rigorously evaluating VidLMs. The datasets are carefully designed and curated. The AMT protocols seem well thought over. The Action Counting test seems especially challenging."
            },
            "weaknesses": {
                "value": "The Situational Awareness tests and Spatial Relations tests seem to suffer from strong linguistic priors (despite mediocre VidLM performance). The Rare Actions tests are surprisingly easy, with VindLU achieving 88% for the difficult P+T condition. \n\nThe analysis is relatively cursory (even though it's called \"in-depth results\" in the appendices). The number of parameters and amount of training data of the VidLMs should be reported along with the results. Preferably the authors can say a few words about the model architectures. I understand the reader can track these down from the original papers, but doing so would require significant effort given the number of baselines. Having these meta-data would help the reader understand if the model strengths are derived from the model size, the training data, or the architecture design."
            },
            "questions": {
                "value": "Nil"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7649/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698677778101,
        "cdate": 1698677778101,
        "tmdate": 1699636929986,
        "mdate": 1699636929986,
        "license": "CC BY 4.0",
        "version": 2
    }
]