[
    {
        "id": "Kkv15tMq86",
        "forum": "2lDQLiH1W4",
        "replyto": "2lDQLiH1W4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission773/Reviewer_wyA8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission773/Reviewer_wyA8"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a method for generating 3D shapes from text. The paper observes problems in the current score distillation-based optimization method, including slow inference, low diversity, and Janus problems, and proposes methods to solve them. As a first step, it proposed a text-conditioned sparse-view generation model, finetuned from a large-scale diffusion model of text-to-image generation. It is capable of generating high-quality sparse-view images without clustering the background. Second, it reconstructs the 3D shape based on the sparse views it generates. To reconstruct the sparse views, view-conditioned image tokens are encoded. Image tokens are concatenated from four views and fed into a triplane decoder. A NeRF decoder takes the decoded triplane features and reconstructs them into a 3D shape. Using the proposed method, high-quality and diverse 3D shapes can be created in 20 seconds. Using the proposed method, the Janus problem is prevented by maintaining shapes across views."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "This paper is well written, clearly motivated, significantly contributed, and extensively experimented. The strengths I found about this paper include but are not limited to: \n+ It proposes an effective method for resolving the Janus problem in text-to-image optimization-based method for text-to-shape generation. The experiment shows that the generated 3D shapes have better 3D structure and texture consistency across views. \n+ It proposes a lightweight fine-tuning method and conducts extensive experiments for text-to-sparse view image generation. The method leverages the capability of a large-scale text-image generated model and proves that it has the ability to generate images across sparse views with light fine-tuning. I believe this model can not only contribute to text-to-shape generation but also to other domains. \n+ It proposes an effective sparse-view reconstruction method that outperforms other sparse-view reconstruction methods in object-only datasets. \n+ As a feedforward method, it generates 3D shapes efficiently within only 20 seconds."
            },
            "weaknesses": {
                "value": "The paper still has some limitations which I think are not discussed thoroughly: \n\n+ Over-saturated problem. In Figure 4, the paper provides examples that have more photorealistic colors. However, it still suffers from an over-saturated problem to some extent, especially in the examples provided in Figure 5. I think the increment of texture quality majorly resulted from the curated training dataset, which removes cartoonish and low-quality instances, but not a result of improving the texture generate method itself(i.e. improving the rendering method, adding extra photo-realistic losses). If my interpretation is correct, I think this should be stated in the limitation section. \n+ Resolution. As the author stated in the limitation section, generating four sparse view images leads to a degradation of texture quality. It would be better to provide a qualitative experiment by measuring the PSNR/SSIM/LPIPS of single image and multi-view images.\n+ Diversity. In Figure 6, the paper provides examples showing the method is able to generate diverse results. My question is if the method provides more diverse results compared with other optimization-based methods. Will the feed-forward method be helpful in providing more diverse results than the optimization-based method practically? It would be better to provide some examples here. \n+ In A.3 the paper detailed how to use CLIP features to filter out low-quality shapes. While I'm convinced the CLIP feature can filter out shapes with a cartoonish style, I'm not very convinced that the CLIP feature is able to tell apart shape quality. I hope the authors can provide some positive and negative examples here.\n+ Some missing citations. \n 1. Section 2.1 paragraph 1. Missing methods using implicit representation[1-3]. \n 2. Section 2.1 paragraph 2. Missing some diffusion-based generation methods[4]. \n[1] Towards Implicit Text-Guided 3D Shape Generation\n[2] ShapeCrafter: A Recursive Text-Conditioned 3D Shape Generation Model\n[3] CLIP-Forge: Towards Zero-Shot Text-to-Shape Generation\n[4] CLIP-Sculptor: Zero-Shot Generation of High-Fidelity and Diverse Shapes from Natural Language"
            },
            "questions": {
                "value": "+ Object-centric COCO. Considering all of the models are finetuned with the Objaverse-XL dataset, I'm wondering if it is still able to generate some shapes whose distribution is outside the Objaverse-XL dataset. I acknowledge it would be hard to prove, but I'm curious to see if the method is able to generate meaningful shapes in the Object-centric COCO dataset[1]. \n+ View condition. When training the view-conditioned image-to-triplane decoder, are the training shapes canonicalized or not? Let's say we input a set of views V = [v1, v2, v3, v4] and generate a shape A.  Then we multiply all the views with a transformation matrix M and generate a shape B. Will shape A and shape B under the same canonicalized coordinate frame? \n+ Minor writing mistakes. \n 1. Section 2.2. \"unseeen\" -> \"unseen\".\n 2. Section A.4. \"The dimension of each plane is 80 All three....\" -> \"The dimension of each plane is 80. All three....\"\n\n[1] DREAMFUSION: TEXT-TO-3D USING 2D DIFFUSION"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission773/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698778410887,
        "cdate": 1698778410887,
        "tmdate": 1699636004576,
        "mdate": 1699636004576,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AAbU2Wru1U",
        "forum": "2lDQLiH1W4",
        "replyto": "2lDQLiH1W4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission773/Reviewer_eZrX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission773/Reviewer_eZrX"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a framework for text-to-3D generation in a feed-forward manner, without requiring an optimization loop during inference. The approach first generates multi view images from a text prompt and gaussian blob initialization. The multiview images are then fed through a transformer based reconstruction network that generates a triplane, which can then be used for volume rendering novel views. State of the art performance is  demonstrated compared to recent text-to-image baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. **Novelty**: The ideas introduced in this manuscript are reasonably novel. In particular, gaussian blob initialization for multiview generation is a potentially useful trick that can be applied to a variety of text-to-3D or image-to-3D pipelines. \n1. **Paper quality**: The paper is well-written and clearly presented, with attention to detail. The authors have clearly put a lot of effort into making the paper easy to read and understand.\n3. **Related work**: An adequate treatment of related works have been provided to place this work in the context of current literature.\n4. **Reproducibility**: The exact details of the approach, architecture specifics and training details have been provided to aid in the reproducibility of the approach. Furthermore, finetuning datatset information has also been provided in the supplm.\n2. **Comparisons**: The paper provides adequate comparisons to baselines, which is important for demonstrating the effectiveness of the proposed approach. Implementations of Dreamfusion on IF has been used as a strong baseline\n3. **Ablation**: Ablation studies are provided to highlight the need for each of the components introduced. Particularly, the motivation for the gaussian blob initialization and finetuning on different data.\n4. **Approach**: The proposed solution of generating 4 views is interesting and adds to the multiview consistency to some extent. \n5. **Appendix**: The authors provide a clear and detailed appendix section with additional reference to LRM for Image-to-3D reconstruction. A number of uncurated text to image examples are provided."
            },
            "weaknesses": {
                "value": "1. **Need for gaussian blob**: How important is it for the initialization to be a gaussian blob? Can\u2019t the same effect be achieved with a square mask since the primary intent is to localize the generated outputs to a region?\n3. **Image features**: How important are the Dino features? In particular, is there a significant drop in performance with features obtained from other pre-trained networks? Ablation with say VGG or other conv features would be insightful to determine the importance of the choice of features. \n5. **Comparison**: Additional comparison to amortized text-to-3D approaches like ATT3D[1] both in terms of quality and in terms of compute and inference costs will help highlight the contributions of this work. Additionally, most of the comparisons are against volume synthesis methods, how does the quality compare to mesh synthesis methods like Magic3D[2] ?\n6. **Novel view consistency**: It is unclear how multi-view consistent the rendered novel views are. Although table 2 provides comparison of pixel aligned metrics against SparseNeus, the work would greatly benefit by presenting video results of turntables of the rendered objects. This will help with the qualitative evaluation of the multiview consistency of the object.\n7. **Tiled generation vs multichannel**: Although contemporary to this work, motivating the need for tiling the views as opposed to generating them as separate channels as in MVDream[3]. Strict qualitative comparisons are not warranted, but highlighting the advantage of the tiled 4 view representation (particularly, since this reduces the resolution) would be insightful. \n8. **Number of view**: Section 3.1 mentions trade-off of number of views vs quality. Providing some qualitative/ quantitative justification for this (either in the appendix or supplm) would be very helpful.\n9. **Data distribution**: Since stage 2 is only trained on Objaverse-XL renders, is there an issue with the kinds of 3D assets that can be generated? In particular, the generated assets look synthetic and from the distribution of Objaverse instances. \n10. **Choice of Diffusion model**: The tiled approach works well for latent space models like SD and SD-XL due to the inherent high resolution input output. Can this framework also be adapted in pixel space diffusion models like DeepFloyd. Providing some insight for this will be helpful in determining the choice of diffusion model.\n10. **SD vs SDXL**: Although quantitative evaluations are presented, providing some qualitative comparison of assets generated from SD finetuning vs SDXL finetuning would be helpful (in appendix or supplm). \n\n[1] Lorraine et al. ATT3D, ICCV23. \n[2] Lin et al. Magic3D, CVPR23. \n[3] Shi et al. MVDream arxiv23"
            },
            "questions": {
                "value": "1. How important are DINO features?\n2. What is the advantages of tiled generation of 4 views over generating on multiple channels.?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission773/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698820898143,
        "cdate": 1698820898143,
        "tmdate": 1699636004503,
        "mdate": 1699636004503,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wPAKUD6n3F",
        "forum": "2lDQLiH1W4",
        "replyto": "2lDQLiH1W4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission773/Reviewer_dCt6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission773/Reviewer_dCt6"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a 3D distillation method from fine-tuned text-to-2D diffusion models fintuned. The method tackles the diversity and Janus problem in prior methods and achieves significant speedup compared to prior approaches."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* There are several technical components proposed in the method to achieve good visual quality. \n* The speedup compared to prior optimization-based methods is significant."
            },
            "weaknesses": {
                "value": "* In Figure 11, the paper claims to get rid of the Janus problem, but such a claim should be rigorously verified across a large set of text prompts instead of using the selected examples. \n* The paper proposes to use a feedforward transformer for sparse-view reconstruction, and both this model and the fine-tuned Stable-Diffusion model are trained on the Objaverse dataset, which can potentially introduce a large domain gap when applying the model to arbitrary text prompts. A discussion on failure cases related to the domain gap, if there are prominent ones, may help readers better assess its applicability."
            },
            "questions": {
                "value": "* The paper provides qualitative examples suggesting an improved diversity compared to prior methods but lacks a discussion on which technical component in the proposed pipeline contributes to such diversity."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission773/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698844999602,
        "cdate": 1698844999602,
        "tmdate": 1699636004439,
        "mdate": 1699636004439,
        "license": "CC BY 4.0",
        "version": 2
    }
]