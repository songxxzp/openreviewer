[
    {
        "id": "ncouTF0AHL",
        "forum": "wiYV0KDAE6",
        "replyto": "wiYV0KDAE6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5608/Reviewer_Cv2X"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5608/Reviewer_Cv2X"
        ],
        "content": {
            "summary": {
                "value": "The authors introduce a we a novel adaptation to TabDDPM diffusion model, incorporating a transformer (compared to MLP for TabDDPM) and unique masking mechanism to condition the reverse diffusion process. This encoder-decoder structure, allows for introducing columnar embedding and enables data imputation as well as data conditioning. Empirical results seem to support the new model, with better ML utlity at the cost of higher risk of privacy breach."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Well written paper, with clear figures, no grammatical issues, and good flow\n- Empirical results can be directly compared to previous baselines\n- Novelty is clear and well explained\n- Datasets and baselines are appropriate for the evaluation task"
            },
            "weaknesses": {
                "value": "- It would be nice to see a few more plots of the feature distribution rather than a simple distribution difference score\n- Analysis of method on the same 15 datasets as the reference TabDDPM paper would be useful\n- Further ablations / discussions showing the imputation would also add to the paper. For example, why is the performance worse with  TabGenDDPM I vs II?"
            },
            "questions": {
                "value": "- For ML efficiency, the original TabDDPM paper demonstrate multiple examples where the generated data is able to achieve better performance over the baseline. However, this behavior is not seen here?\n- Is it possible to conditionally generate diverse synthetic data by conditioning on an outcome feature? E.g. death event or housing price?\n- Does this model faithfully generate data that captures low-domain clusters / phenotypes in the original data space?\n- Why does the best performance switch for TabGenDDPM I and II in the cardio dataset?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5608/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5608/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5608/Reviewer_Cv2X"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5608/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698717367340,
        "cdate": 1698717367340,
        "tmdate": 1699636578442,
        "mdate": 1699636578442,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tx9kkppEtc",
        "forum": "wiYV0KDAE6",
        "replyto": "wiYV0KDAE6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5608/Reviewer_AvR7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5608/Reviewer_AvR7"
        ],
        "content": {
            "summary": {
                "value": "This paper propose an improvement of TabDDPM model through the addition of tree improvements:\n- 1. The categorical columns are encoded via an Embedding layer instead of a one-hot and the numeric columns are encoded through a linear layer. This allows an uniform encoding of the columns into the same dimension independently of their type.\n- 2. The MLP denoiser of TabDDPM is replaced by a transformer architecture.\n- 3. A BERT-like attention masking system is then used to train the model for dynamic conditioning and missing data imputation\n\nAfter quick introduction and related work sections, a background is given about ddpm and multinomial diffusion algorithms.\nThen the specificities of the model are presented.\nThe experiment section compare two variants of the model against TVAE, CTGAN and vanilla TabDDPM (all with optimized hyper-parameters) on 7 datasets through ML-efficacy and DCR privacy risk. Another statistical similarity metric is proposed as well.\nThe two variants considered are trained with full data (TabGenDDPM I) and with masked data (TabGenDDPM II).\nFor ML-efficacy TabGenDDPM I is shown to outperform the other models on 6 out of the 7 selected datasets. For data imputation TabGenDDPM is also shown to outperform a customized version of TabDDPM.\nOn the other hand, the privacy risk is reported to be slightly higher than with TabDDPM."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The proposed architecture is a natural improvement from TabDDPM and according to the experiments, it seems to really improve the model in term of ML-efficacy\n- The paper is clear and well written with several illustrations\n- The privacy risk is considered"
            },
            "weaknesses": {
                "value": "- The proposed architecture is mostly a derivative work from TabDDPM\n- The proposed diffusion algorithms are a bit outdated now, especially on the discrete side since works like:\nAustin et al. \"Structured Denoising Diffusion Models in Discrete State-Spaces\" NeurIPS 2021, or Campbell et al. \"A Continuous Time Framework for Discrete Denoising Models\" NeurIPS 2022.\nIt is worth noting that \"mask\" systems are also studied in (Austin et al. 2021).\n- No ablation study to validate the separately different changes from TabDDPM (eg. category embedding vs one-hot)\n- No simple \"non-deep\" baseline model (like SMOTE) in the experiment.\n- The code seems not to be open source"
            },
            "questions": {
                "value": "- The hyper-parameter space of TabDDPM seems modified in your experiment (e.g. no batch size 4096 and no learning rate) Why ?\n- With the masking system it is possible to condition on any feature. Why keep a specific treatment for the target value ?\n- The statistical similarity metric is not usual and do not permits an easy comparison with other papers, why not use \"sdmetrics\" library to provide other metrics (notably C2ST detection metrics) ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5608/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698766042381,
        "cdate": 1698766042381,
        "tmdate": 1699636578320,
        "mdate": 1699636578320,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HTtiXFzFI2",
        "forum": "wiYV0KDAE6",
        "replyto": "wiYV0KDAE6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5608/Reviewer_uxeP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5608/Reviewer_uxeP"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a transformer conditioning architecture design on TabDDPM for data imputation and data generation tasks. They conduct experiments on eight datasets under machine learning utility, statistical similarity, and privacy risk."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The experimental comparisons are good. The author conducts TabGenDDPM on eight datasets under three evaluation criteria."
            },
            "weaknesses": {
                "value": "1. The overall contribution of this paper is limited. \n\nAll of the content except the transformer conditioning architecture is already known. The architecture design is heuristic, which has no theoretical guarantees of the performance. Moreover, they build upon Variance Preserving (VP) SDE (e.g., DDPM or TabDDPM in tabular data). The author does not mention wether their method work for Variance Exploding (VE) SDE (e.g, Score-based generative model, StaSy [1] in tabular data).\n[1]: Kim, J., Lee, C.E., & Park, N. STaSy: Score-based Tabular data Synthesis. ICLR 2023.\n\n2. Overclaiming the contribution of transformer conditioning architecture.\n\n* Diffusion model can work on imputation together with generation (conditional generation) without the proposed transformer conditioning architecture. There are well studied in the literature [1,2].\n\n[1]: Tashiro, Y., Song, J., Song, Y., & Ermon, S. CSDI: Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation. NIPS 2021.\n\n[2]: Ouyang, Y., Xie, L., Li, C., & Cheng, G. (2023). MissDiff: Training Diffusion Models on Tabular Data with Missing Values. ArXiv, abs/2307.00467.\n\n3. The effectiveness of the proposed method is not well supported.\n* : The standard evaluation of imputation performance is the mean squared error of imputed value against oracle value instead of the efficiency criterion used in paragraph \"Machine Learning efficiency - Data imputation\". Otherwise, it faces the problem of \"when the generative model needs to fill in the most significant feature or a feature that has a minimal impact on XGBoost output\" mentioned in the paper. If the authors adopt the traditional evaluation on this task, many design in this paragraph will not be needed.\n\n* : To evaluate the performance of TabGenDDPM on imputation task, it should be compared with other imputation methods, e.g., [3,4], rather than only compared with TabDDPM.\n\n[3]: Yoon, J., Jordon, J., & Schaar, M.V. GAIN: Missing Data Imputation using Generative Adversarial Nets. ICML 2018.\n\n[4]: Mattei, P., & Frellsen, J. MIWAE: Deep Generative Modelling and Imputation of Incomplete Data Sets. ICML 2019.\n\n\n* : The author should compare with other diffusion based model on tabular data, e.g., StaSy [1]. Also, some discussion and experimental results of whether transformer conditioning can  be developed on Variance Exploding (VE) SDE.\n\n* : The of illumination the experimental setup should be clarify. Currently, it brings some confusion.\n- The baseline in Figure 3 stands for which method? In my point of view, it is not the methods mentioned in section 5.2.\n- The Table 4 is confusion. In my point of view, three different evaluation criteria have different properties, i.e., the smaller the correlation is, the better the performance is, which is different with privacy risk. Why the authors use Up arrow/Down arrow beside the name of the dataset. It is also not clear why the authors only report the experimental results on six datasets rather than eight datasets in Table 2. \n- It would be helpful to have the performance on each dataset for Table 3 in appendix. \n\n4. Minor\n\nThe paper has many typos, e.g., \n- adding period for the caption of Table 1, 3, 4 and Figure 3; \n- what is the meaning of \"4+2\" and \"2(4+40)\" in Table 1; \n- \"in this situation, the generative model can employ the no-missing values to condition the missing data generation.\" is hard to understand."
            },
            "questions": {
                "value": "Please see Weaknesses Part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5608/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5608/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5608/Reviewer_uxeP"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5608/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698820901417,
        "cdate": 1698820901417,
        "tmdate": 1699636578152,
        "mdate": 1699636578152,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "P1Zijhgi3N",
        "forum": "wiYV0KDAE6",
        "replyto": "wiYV0KDAE6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5608/Reviewer_TceJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5608/Reviewer_TceJ"
        ],
        "content": {
            "summary": {
                "value": "The paper tackles the problem of imputation and generation in a single framework utilizing diffusion model with an additional transformer architecture. Experiments are shown on a widerange of datasets as well as competing models to highlights the benefits of the proposed approach."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Overall:\nThe paper is easy to read and the contribution is simple but effective. The experiments cover a wide range of datasets though not algorithms.\n\nPros:\n\n(i) The paper extends TabDDPM to TabGenDDPM utilizing the transformer architecture which has been wildly succesful in other generative settings. The experiments confirm the benefits of the proposed approach. The additional benefit of covering both imputation and generation in the same framework enables a wide range of usecases in real-world settings.\n(ii) Experiments cover around 10 datasets with varying number of rows and feature sizes and in almost all cases the proposed method is the best and sometimes by a big margin."
            },
            "weaknesses": {
                "value": "Cons:\n\n(a) Some of the other competing methods like AIM, CTAB-GAN+ and others are not compared in the paper. \n(b) The number of features in the datasets are few. HELOC has the highest with only 21 features and it is unclear how this framework performs when the feature set is large."
            },
            "questions": {
                "value": "(1) What is the running time of the proposed approach and how does it compare with the other state-of-the-art algorithms?\n(2) How does it perform when the feature set is large and/or the number of samples is small? \n(3) How does it work in augmentation tasks where the training is a mix of real + synthetic and testing on real?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5608/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699219652599,
        "cdate": 1699219652599,
        "tmdate": 1699636578048,
        "mdate": 1699636578048,
        "license": "CC BY 4.0",
        "version": 2
    }
]