[
    {
        "id": "zb4c79bpiT",
        "forum": "IWpLQfZ8Xg",
        "replyto": "IWpLQfZ8Xg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8242/Reviewer_BhUy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8242/Reviewer_BhUy"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the local sensitivity of dot-product self-attention in Transformers. Though the outputs of all heads is not globally Lipchitz, a weaker condition, i.e., the local sensitivity can be theoretically analyzed by providing a upper bound. Besides, the upper bound is empirically verified and certification on practical models is also given."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Upper bound of local sensitivity analyse is derived\n- Numerical validations are also provided to support the fact that, the upper bound is tight and reasonable"
            },
            "weaknesses": {
                "value": "- The dimension of some matrices are undefined, e.g., $W^O \\in R^{d \\times d}$ and $H \\in R^{n \\times n}$?\n- Solving Eq. (10) requires SVD for the n-by-d matrix. How to ensure the computational efficiency? \n- To bound the second term in Eq. (14), the author uses the triangle inequality to obtain the upper bound at first. However, this can be also obtained with a closed-form solution? This is because the objective function and constraint are both linear."
            },
            "questions": {
                "value": "- Before Proposition 1, the authors mention the robustness under l_2 perturbation. How about using l_\\inf perturbations for robustness when compared to the adversarially chosen l_2 perturbations? In this case, Eq. (5) will be changed to the l_\\inf norm but I\u2019m not sure the used techniques are still valid."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8242/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698580904699,
        "cdate": 1698580904699,
        "tmdate": 1699637024549,
        "mdate": 1699637024549,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PHEu6amj0P",
        "forum": "IWpLQfZ8Xg",
        "replyto": "IWpLQfZ8Xg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8242/Reviewer_VymQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8242/Reviewer_VymQ"
        ],
        "content": {
            "summary": {
                "value": "This paper aim to theoretically analyze the sensitivity of the self attention mechanism. Local perturbations are imposed on the weights, and authors quantify the relationship between the sensitivity between the input, weight matrices, etc. Experiments are done to validate the theory, and insights are provided to achieve more stable self attention structure."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper captures a common problem of the popular Transformer model: self attention mechanism can be sensitive. The work quantifies the sensitivity and provides insight into how to make the self attention structure stable. This topic is important in the performance of Transformer model, which is widely applied in NLP, CV tasks.\n2. I do not have doubt on the theoretical results, as they are clearly derived.\n3. The experiments are closely related with the theory."
            },
            "weaknesses": {
                "value": "1. My main concern is that this work does not provide enough contribution. In Section 4, the gap caused by perturbation is derived. However, these results are not novel, in fact, they are easy to derive. The main idea of Section 4 is just finding a Lipschitz constant to bound the gap when perturbation is added to input. This can be easily done if we take derivative over input X and find an upper bound for the Frobenius norm of the gradient over X. In some other works, the closed form gradients (maybe over $W^Q,W^K$, but similar to gradient over X) are easily derived, e.g, Tian, Yuandong, et al. \"Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer.\" arXiv preprint arXiv:2305.16380 (2023). Thus, I do not think the theory has much contribution.\n2. The theory in Section 4 implies that weight matrices and data with small magnitude is better. However, 'small magnitude' does not mean a self attention mechanism is a good model. Consider an extreme case where all weight matrices are close to 0, then the attention mechanism has poor representation ability. We usually require a model with both expressivity and stability, while in this work, the expressivity is ignored."
            },
            "questions": {
                "value": "1. How to theoretically guarantee that a model can both have good expressivity and stability?\n2. When weights $W^Q,W^K,W^V$ follows some specific distribution, can the sensitivity bound be improved? Or the bound is only related to the magnitude of weights?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8242/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8242/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8242/Reviewer_VymQ"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8242/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698734954955,
        "cdate": 1698734954955,
        "tmdate": 1700708346615,
        "mdate": 1700708346615,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vRnc5Fn47t",
        "forum": "IWpLQfZ8Xg",
        "replyto": "IWpLQfZ8Xg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8242/Reviewer_d9ub"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8242/Reviewer_d9ub"
        ],
        "content": {
            "summary": {
                "value": "This paper provides a fine-grained theoretical analysis on the local sensitivity of self-attention. The primary constrained optimization for this local sensitivity is $\\max_{X': ||X' - X||_F \\leq \\epsilon} || F(X') - F(X) ||_F$ where $F(X)$ is the residual self attention. The authors divide $|| F(X') - F(X) ||_F$ into $\\Delta_1$ and $\\Delta_2$ (equation 7 & 8). For $\\Delta_1$, the authors provide an analytical upper bound (**first contribution** of this paper) as \n\n$$\\xi(x) = || H \\bigotimes I_n + \\sum_{l = 1}^h (P_l(X) \\bigotimes (W_l^V W_l^O)^\\top) ||$$ \n\nFor $\\Delta_2$, the authors first apply triangle inequality to divide equation 13 into the perturbation on self-attention score matrix (equation 17) and $|| X' W_l^V W_l^O||$ (equation 15). **The second contribution** of this paper is on bounding equation 17 as Lemma 2 and equation 19. Putting them all together, we obtain an upper bound for $|| F(X') - F(X) ||_F$.\n\nIn the experiments, the authors first study the $\\Delta_1$ and $\\Delta_2$ values versus the PGD low bound across epsilon values in single and multi (8) head cases. The authors also analyze ViT's certified robust accuracy on CIFAR10 task, and provide a nonzero robustness $\\epsilon \\sim 36/255$ (**third contribution**)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proof organization of this paper is pretty clear and easy to follow in section 4. The authors meticulously described the looseness of each naive bound and strategies to further tighten the bound.\n\nThis local sensitivity analysis would be insightful for both adversarial and general machine learning community.\n\nThe experiments are also conducted on real-world tasks (ViT on CIFAR10), which makes this theoretical analysis practical on understanding the robustness of self-attention."
            },
            "weaknesses": {
                "value": "There are multiple naive bounds described in theory but not evaluated in practice. For example, equation 9 for bounding $\\Delta_1$ and $||W_l^V W_l^O||(||X|| + \\epsilon)$ for bounding $||X' W_l^V W_l^O||$ should also be evaluated in Figure 1 to make the conservatism argument strong.\n\n\nMinor:\n\ntypo in equation 5: $||F(X') - F(x)||$ should be $||F(X') - F(X)||$"
            },
            "questions": {
                "value": "Is it possible to perform another trial of robustness experiments on NLP tasks (text classification, entailment, etc.)? The analysis of this paper is applied to general self-attention and it is definitely great to see a practical evaluation on vision tasks. But it would be even better to see if the same robustness argument is applicable across domains. \n\nIn the network design section, it is mentioned that Theorem 1 would shed light on constraining weight norms for self-attention. It would be nice to see a concrete use case. For example, given a particular quadruple $(W_Q, W_K, W_V, W_O)$ and an input $X$, could we ablate on each weight individually and use the Theorem 1 to predict the local sensitivity?\n\nOverall, this is a good paper, but I believe the evaluation section could be further improved. I would give a weak accept score at this moment, but I am willing to raise my score if my above concerns / questions are addressed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8242/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8242/Reviewer_d9ub",
                    "ICLR.cc/2024/Conference/Submission8242/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8242/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699608212794,
        "cdate": 1699608212794,
        "tmdate": 1700725865153,
        "mdate": 1700725865153,
        "license": "CC BY 4.0",
        "version": 2
    }
]