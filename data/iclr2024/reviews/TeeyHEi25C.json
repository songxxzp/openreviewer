[
    {
        "id": "RL2zoifOmF",
        "forum": "TeeyHEi25C",
        "replyto": "TeeyHEi25C",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7395/Reviewer_ZNxJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7395/Reviewer_ZNxJ"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a paradigm for reinforcement learning that can be considered different from most prior approaches that either learn an auto-regressive transition model or a value function based on temporal difference learning. Instead, the authors develop a method that factories the components of a value function into three parts: The policy, a single step reward model, and a model of the state occupancies conditioned on the policy. Together, the components are able to estimate the value function by sampling states (and actions) from the occupancy model, scoring them with the reward model and training the policy to maximise these scores. The paper offers qualitative examination of the occupancy model on Maze2D data as well as empirical evaluation of the method on offline RL datasets from PyBullet, where it shows promising performances compared to the commonly known offline algorithm CQL. Further, it is shown that the occupancy model offers different perspectives for exploration based on offline data - by adding a reward term that encourages future states that are different from the current one, without the limitation of single-step models, the algorithm appears to perform very well in sparse reward settings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "To the best of my knowledge the paper presents a novel way of doing RL without temporal difference learning or dynamics modelling. Instead, by learning a diffusion based occupancy model of the states visited by the policy, some of the pitfalls of these methods seem to be effectively circumvented (e.g. accumulation of transition errors in model-based methods, usage of low quality data hard in temporal difference learning). The qualitative results look like the occupancy model is generally doing what is expected and the quantitative results show that the new paradigm is actually able to beat some well known algorithms on benchmark datasets for offline RL. Additionally, the occupancy model opens up new ways of performing exploration since it is not limited to a single time step."
            },
            "weaknesses": {
                "value": "In 3.1, the authors address the issue of conditioning the occupancy model on the policy. This appears to me like the hardest thing to do, especially for offline RL since we cannot test whether the model is correct without checking on the real environment. What we have is just a dataset where a behaviour policy (or maybe multiple) has collected interactions - as soon as we move away from replicating the policy(-ies) present in this dataset, we cannot really know the true occupancy and thus value estimation becomes tricky as well.\n\nThe authors propose 2 ways of conditioning on policies, either scalar (by enumerating the set of policies e.g. along the gradient steps) or sequential (\"embed pi using its rollouts in the environment\"). For both, it is a little ambiguous too me as to why they work:\nIn the offline RL comparison on the pybullet datasets, the latter option is chosen and I am wondering what that means, i.e. where do you get rollouts in the environment from the policy that you currently training and that's not the one that collected the dataset? It's not really offline RL then any more if you collect new rollouts, is it?\nSimilarly the scalar way: It is used in the qualitative experiments in maze2D, which makes perfect sense, but using it to embed policies along the improvement path appears like it could go very wrong as well since from one gradient update to the next the behaviour and thus the state-occupancy could change drastically.\nPolicy conditioning seems to me like one of the critical issues here, it seems you have made it work, so please share some more insights how and why.\n\nOne of the main empirical evaluations is done on the offline pybullet datasets. I understand the key contribution is showing that this novel method can work and not necessarily that it is currently the best one, however it seems to me the baselines are not particularly strong. I am surprised to see that CQL often even achieves negative returns, and outperforming BC and data performance especially on random/mixed datasets is also not particularly hard. I believe much value would be added to the manuscript if some more recent / successful offline RL algorithms were used as a comparison. Optimally you would include some model-based ones (e.g. [1,2]) as well as some model-free ones (e.g. [3,4]) since your method lies somewhere in between the two - adding a reward conditioned method that also relies neither on dynamics models nor TD, like RvS [5] could also be interesting. Also, since the main thing DVF is used for is offline RL, mentioning offline RL works like [1-5] in the related works section seems appropriate. Further, [6] could be an addition to the offline pre-training related work section.\n\n[1] Rigter, M., Lacerda, B., & Hawes, N. Rambo-rl: Robust adversarial model-based offline reinforcement learning. NeurIPS 2022.\n\n[2] Swazinna, P., Udluft, S., & Runkler, T. User-Interactive Offline Reinforcement Learning. ICLR 2023.\n\n[3] Fujimoto, S., & Gu, S. S. A minimalist approach to offline reinforcement learning. NeurIPS 2021.\n\n[4] Hansen-Estruch, P., Kostrikov, I., Janner, M., Kuba, J. G., & Levine, S. Idql: Implicit q-learning as an actor-critic method with diffusion policies. Preprint 2023.\n\n[5] Emmons, S., Eysenbach, B., Kostrikov, I., & Levine, S. Rvs: What is essential for offline rl via supervised learning?. ICLR 2022.\n\n[6] Ajay, A., Kumar, A., Agrawal, P., Levine, S., & Nachum, O. Opal: Offline primitive discovery for accelerating offline reinforcement learning. ICLR 2021."
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7395/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7395/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7395/Reviewer_ZNxJ"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7395/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698765578561,
        "cdate": 1698765578561,
        "tmdate": 1699636886161,
        "mdate": 1699636886161,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QnGuWa8YoZ",
        "forum": "TeeyHEi25C",
        "replyto": "TeeyHEi25C",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7395/Reviewer_nnB2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7395/Reviewer_nnB2"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to train a diffusion model for estimating the state occupancy measure $\\rho(s,a)$ as well as a reward model $r(s,a)$ and uses these networks to train a policy $\\pi$ to solve a given task. The authors evaluate their method on a slate of offline RL tasks and show improvement over prior works in offline RL."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "Strengths: \n* proposes a novel application of Diffusion Models to offline RL - instead of training a model for dynamics prediction, train it for state occupancy prediction and use that to compute the reward function without learning a value function directly (DVF)\n* proposes conditioning the diffusion model on the policy embedding which enables it generate future states from unseen policy embeddings\n* method outperforms BC and CQL on d4rl benchmark tasks"
            },
            "weaknesses": {
                "value": "I have serious concerns regarding the position and framing of this paper as well as the experiments. This work is written as if there is little, if any work in applying Diffusion Models in the offline RL/BC setting, citing only Diffuser (Janner et al) while failing to note Diffusion-QL (Wang et al), AdaptDiffuser (Liang et al.), Diffusion Policy (Chi et al) and many more works. The introduction, related works and methods section are all missing this crucially important context to properly understand the contribution. Writing-wise, the methods section is also extremely difficult to follow - there are many typos, notation mistakes and a math error (specifically equation 12 is wrong, there needs to be a term with the dynamics as well). Finally, the authors fail to compare against any Diffusion-based baselines in their work, which would lead the reader to believe that the proposed DVF method is a state-of-the-art method for doing offline RL. As a simple example, see Table 1 in the Diffusion-QL paper - DVF (non-pooled, which is the fair comparison) performs worse than Diffusion-QL in every task. It also appears that even with the curiosity reward added to the offline RL datasets, the Maze2d results (Table 2) are worse than those in Table 1 of the Diffuser paper. \n\nNotes:\n* Figure 1, $s_{t+\\Delta}$ is used multiple times in the leftmost picture - they should have different subscripts to denote they are using different deltas\n* The method description completely skips describing the averaging step that is necessary to get a state occupancy estimate that is not dependent on $\\Delta t$\n* equation 12 is wrong, you need to take the gradient of the expected value with respect to the action as well (the dynamics uses $a_t$)\n* in the abstract, \"A fairly reliable trend in deep reinforcement learning is that performance scales with\nthe number of parameters, provided a complimentary scaling in amount of training\ndata. As the appetite for large models increases, it is imperative to address, sooner\nthan later, the potential problem of running out of high-quality demonstrations.\" - These statements are not entirely correct, perhaps the authors meant a reliable trend in \"supervised learning\"? Also it is not clear where demonstrations have come in when the first sentence discusses reinforcement learning. \n* miscellaneous typos and notation mistakes in the methods section, interspersed throughout, I pointed the most obvious ones above \n\nIn general, I highly recommend the authors re-write the paper for clarity, add proper framing and perspective, improve the methods section considerably and include significantly more comparisons to relevant, SOTA work. In its current form, I do not believe this paper is ready for publication at a venue such as ICLR."
            },
            "questions": {
                "value": "1. In Figure 5, \"Returns are normalized by average per-task data performance.\" What does this mean precisely?\n2. Please evaluate DVF on the full suite of D4RL tasks as done in Diffusion-QL Table 1 so that we can evaluate the complete performance profile of DVF\n3. Please provide concrete discussion of DVF differences/tradeoffs relative to other Diffusion-based offline RL methods\n4. Why was Perceiver I/O used instead of a standard Diffusion U-Net architecture? \n5. Add clarity on which networks are beings trained, their objectives, their inputs and outputs in the methods section. This took a lot of effort to parse from the current methods section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7395/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698784348079,
        "cdate": 1698784348079,
        "tmdate": 1699636886040,
        "mdate": 1699636886040,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TWkNxaaAmr",
        "forum": "TeeyHEi25C",
        "replyto": "TeeyHEi25C",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7395/Reviewer_65QG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7395/Reviewer_65QG"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel method for value function estimation using conditional diffusion models for continuous control tasks. The method learns a generative model of the discounted state occupancy measure from state sequences without reward or action labels, and then uses it to estimate the value function and the optimal action. The paper shows that the method can handle complex robotic tasks, offline reinforcement learning, and exploration from offline data, and outperforms existing baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- It proposes a novel algorithm DVF, for value function estimation using diffusion models without requiring reward or action labels.\n- It demonstrates that DVF can handle complex robotic tasks and outperforms existing baselines in both online and offline settings.\n- It shows how DVF can be used for learning exploration policies from offline datasets, enhancing the efficiency of tabula rasa learning."
            },
            "weaknesses": {
                "value": "See questions."
            },
            "questions": {
                "value": "1. The assuption that the behavior policy $\\mu$ is known is not usual in offline RL. It seems like this paper only utilized the dataset $\\mathcal{D}$. So can this assuption be removed without affecting the result?\n1. There has been many works that apply diffusion models on offline RL [1,2,3,etc.] . Could you please include more baselines that use diffusion models for more convincing experiments? Such works are also worth discussing in related works or other parts of the paper.\n\n[1] Diffusion Policies as an Expressive Policy Class for Offline Reinforcement Learning. https://arxiv.org/abs/2208.06193\n\n[2] Is Conditional Generative Modeling all you need for Decision-Making? https://arxiv.org/abs/2211.15657\n\n[3] IDQL: Implicit Q-Learning as an Actor-Critic Method with Diffusion Policies. https://arxiv.org/abs/2304.10573"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7395/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7395/Reviewer_65QG",
                    "ICLR.cc/2024/Conference/Submission7395/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7395/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698811452235,
        "cdate": 1698811452235,
        "tmdate": 1700562808261,
        "mdate": 1700562808261,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "F9HzE3GnBE",
        "forum": "TeeyHEi25C",
        "replyto": "TeeyHEi25C",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7395/Reviewer_zxfx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7395/Reviewer_zxfx"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method for learning a value function of a policy by training a generative model of the occupation measure given features of the policy. The authors propose to use state samples from the current policy to train a diffusion model, and weight them by the reward in order to predict the value function. Furthermore, they propose to improve the policy with the estimated value function by taking gradients through the reward. The authors show the efficacy of their algorithm on tasks that are mostly in offline RL."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The work presents an interesting idea that hasn't been tried by other previous works, the idea of diffusing an occupation measure is quite interesting. \n2. The work makes good theoretical connections with existing works in RL and the proposed approach.\n3. The incorporation of exploration from data is also quite interesting, as this is rarely considered."
            },
            "weaknesses": {
                "value": "1. The presentation of the work can be improved as the manuscript is a bit hard to understand in its current form. It would be good to dissect and analyze each sentence with a bit more care when rewriting. I will list some of the points here but these are not isolated issues, I think the authors' work could be presented with much more clarity if written more clearly.  \n- In Section 2, $\\Delta t$ suddenly appears without defining, and the readers are left to figure out what it is.\n- The wording of explicit conditioning is also a bit strange in this section, and it requires some domain-expertise to understand what the authors mean by this. The occupation measure is always conditioned on a policy, the choice of whether we implicitly do it or explicitly do it seems like a choice of implementation. Perhaps it's better to say something like \"rather than statistically estimating the occupation measure through Monte Carlo sampling, we choose to directly learn a map that can infer the occupation measure given some features of the policy\"? \n- In Equation 8, $l_{diffusion}$ should explicitly be noted as the function of $\\theta$.  \n- In Section 3, the authors say maximizing $Q(s,a,\\phi(\\pi))$ directly is expensive, but the readers don't have the context to understand this at this point of the manuscript (we don't yet have the details of what parameters are being maximized, and what is being represented by a diffusion model) and $Q(s,a,\\phi(\\pi)$ has never been defined anywhere.\n\n2. The baselines in the empirical results are a bit weak as the only compare to BC and CQL. It would have been more informative to include other approaches in offline RL (e.g.Implicit Q-learning, Trajectory Transformer (TT), Diffuser, Score-Guided Planning (SGP))."
            },
            "questions": {
                "value": "1. The computational aspect of the approach has been relatively not discussed. Is DVF cheaper / more expensive to trained compared to other baselines?\n2. Are there other interesting uses cases of having a generative model for the occupation measure besides estimating the value function?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7395/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7395/Reviewer_zxfx",
                    "ICLR.cc/2024/Conference/Submission7395/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7395/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698815012016,
        "cdate": 1698815012016,
        "tmdate": 1700701347344,
        "mdate": 1700701347344,
        "license": "CC BY 4.0",
        "version": 2
    }
]