[
    {
        "id": "Mc84O9N0ND",
        "forum": "ElykcDu5YK",
        "replyto": "ElykcDu5YK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8829/Reviewer_ZY9Q"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8829/Reviewer_ZY9Q"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a novel meta-learning based approach for Bayesian Optimization of risk measures. Given a set of previous tasks, the authors propose to construct a no-regret query set and they prove that any strategy picking inputs in such sets leads is a no-regret strategy, i.e., it has a sublinear regret. Furthermore, the authors assigns priorities on all the inputs belonging to this NQS $\\mathcal{Q}_t$ at iteration $t$. Those importance weights are computed using the information from previous tasks. They take high values for the most probable local maximizers among the set of previous tasks. Then the authors introduce a tradeoff between exploiting the information from previous tasks and exploring the current task. By applying the approach on Value-At-Risk (and CVAR), the authors show that their algorithm is invariant to scaling and vertical shifting of the blackbox function, and robust towards the presence of harmful previous tasks.\nThe authors show numerical experiments on synthetic data and real-world data."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper contains several key contributions that will be helpful for the future works in BO for risk measures.\nMajor strengths:\n  - the paper is very well written and the motivations are very clear: exploit previous tasks knowledge (achieved using V-UCB) to get better BO for risk measures. The literature review is well detailed and the authors explain clearly advantages and drawbacks of each of them. \n  -  the construction of the no-regret query sets is elegant and the theorem ensuring that any strategy picking input points in such sets is key contribution in this specific research area in my opinion. \n  - the definition of the importance weights exploits well the knowledge from the previous tasks. This also enables to avoid the weight decay to $0$ which is used in the state-of-the-art of meta-BO. The strategy keeps using the knowledge from previous tasks over time. \n  - the authors provide theoretical results ensuring that the presented meta-BO of risk measures maintains invariance properties in scaling and shifting of the blackbox function.\n  - the numerical experiments show that the approach is robust towards the presence of harmful previous tasks (versus V-UCB)."
            },
            "weaknesses": {
                "value": "Although the paper is very clear and provide important results for the community, there are couple of points that are not completely clear to me.\n\nMajor comments/questions:\n   - the distribution of Z is supposed to be known. Is it a parameterized distribution that is fitted in practice?\n   - the point which is unclear to me is the definition of the lacing values. If I understand well, this means that at each iteration the authors pick a value for z (corresponding to the lacing value) and draw an observation $y_t = f(x_t, z) + \\varepsilon$. Is this correct? How are those lacing values computed in practice? I think a paragraph is necessary to explain exactly what is done here. Would it be possible to provide a pseudo-code summarizing the main steps of the algorithm?\n  - it would have been interesting to see an illustration of the set $\\mathcal{Q}_t$ in the numerical experiments and the computed importance weights. And also to see the evolution of this query set over time.  Figure 1 is not easily readable in my opinion.\n  - the authors write that \"we want to maximize the size of the query sets\". I understand this point since it enables to exploit all the previous tasks. However, it seems that there might exist some tasks that do not bring useful information or harmful information. In such case, wouldn't it be more interesting to discard these tasks from the query set? This question is a bit naive, but from a computational perspective, it would make more sense to consider less previous tasks. Is this correct?\n  - Maybe I've missed this point, but could you comment the computational cost of the approach?\n\n\nMinor comments:\n  - titles of subsection 2.2 and Section 3 are the same.\n  - In my opinion, there is a lack of details about the transfer knowledge from previous tasks in the settings. For a reader who is not familiar with meta learning, it would have been helpful to know precisely (in the settings) what is the \"knowledge\" you extract from a previous task. In your case, this is the output of V-UCB. This is explained in the introduction, but this would make the reading easier."
            },
            "questions": {
                "value": "See above.\nIs the code publicly available?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8829/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8829/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8829/Reviewer_ZY9Q"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8829/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698330251516,
        "cdate": 1698330251516,
        "tmdate": 1699637110000,
        "mdate": 1699637110000,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oab7v9Aldt",
        "forum": "ElykcDu5YK",
        "replyto": "ElykcDu5YK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8829/Reviewer_4GcU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8829/Reviewer_4GcU"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a robust meta-BO for risk measures algorithm. It achieves the robustness to harmful previous tasks essentially by constructing the no-regret query set (NQS) that guarantees the convergence of the optimization while using the counting of probably local maximizer in related tasks as the priority within NQS to guide the acquisition. The paper offers theoretical justification for constructing NQS and the regret analysis."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The construction of NQS guarantees the robustness of the meta-BO.\n\n2. The algorithm is generally well-motivated. Since the key concepts are generally constructed on top of confidence intervals for the risk measure, they bear good interpretability."
            },
            "weaknesses": {
                "value": "1. It is unclear why the $\\lambda$ and $\\eta$ are proposed to trade off the goals of NQS explicitly. The algorithm, by default, sets $\\lambda=0$ and $\\eta=1$ as discussed following the theorem 3.7.\n\n2. The priority mechanism in eq (10) is proposed to incorporate information from previous tasks, but it is actually not robust to harmful previous tasks. This is reflected in both the comparison between the results from theorem 3.2 and previous works' regret bounds and the empirical results.\n\n3. Figure 3 demonstrates the effectiveness of the priority mechanism. Yet since the priority function relies on counting, it is unclear whether the proposed method is robust to the increase of the portion of harmful tasks in $\\tau$."
            },
            "questions": {
                "value": "1. It seems that the construction of NQS basically relies on the upper and lower confidence bounds of the risk measure $\\rho_f$ in eq (3). In ordinary BO, the UCB and LCB of the unknown objective function f are the direct equivalence of these confidence bounds for risk measure when the goal is optimized f. Then, an NQS for ordinary meta-BO could be constructed in the same way and could incur a similar regret guarantee. Could the author comment on this extension of NQS in classic meta-BO?\n\n2. The layout is intense and could be more reader-friendly. Could the author reduce the discussion over literature in sections 1 and 2 and leave more room for the essential equations, especially those for the assumptions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8829/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8829/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8829/Reviewer_4GcU"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8829/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698826124262,
        "cdate": 1698826124262,
        "tmdate": 1700719313571,
        "mdate": 1700719313571,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zUqeWBmkvp",
        "forum": "ElykcDu5YK",
        "replyto": "ElykcDu5YK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8829/Reviewer_25FF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8829/Reviewer_25FF"
        ],
        "content": {
            "summary": {
                "value": "This work considers how to leverage prior knowledge when performing Bayesian optimization of risk measures (VaR, CVaR). Prior works in the literature either consider the availability of prior knowledge to warm-start BO for non-risk measures, or optimization of risk measures without using prior knowledge. This work tackles both of these issues simultaneously. The key novelty of the work is the development of a \"no-regret query set\" that defines a set of input query sequences that would result in sub-linear cumulative regret. This set contains those inputs that are likely to be maximizers of the risk measure given the posterior belief about the underlying function and also provides additional information toward the estimation of risk measures. Once such a set is established, prior knowledge can be used to rank/prioritize the query sequences. As any query sequence within this query set would result in sublinear cumulative regret, even if the prior tasks are very different, asymptotic guarantees are still preserved (although finite sample rates deteriorate)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "S1. The idea of a no-regret-query set is novel, to the best of my knowledge.\n\nS2. Authors carefully consider the impact of the quality of knowledge from prior tasks and how those influence the final results."
            },
            "weaknesses": {
                "value": "W1. Empirical results are on extremely toy examples.\n\nW2. Like prior works, this work is also limited in terms of assuming exact knowledge of the noise distribution. Seems like a strong requirement.\n\nW3. Not really a weakness, but my guess is that this paper might have a much bigger audience at other venues (e.g., ICML/AISTATS)."
            },
            "questions": {
                "value": "1. The key contribution seems to be the introduction of the idea of a no-regret-query set (NQS). Maybe I missed it, but it feels like it is more generally applicable than the exact problem being considered in this work. If not, perhaps it would be beneficial to know what makes NQS restricted to the setting of risk measures? Also, seems like various other side-information can be used to prioritize samples within NQS. \n\n2. While I understand that the requirement of noise distribution is similar to assumptions made in the prior work and is not directly related to the proposed method, however, since the paper currently is phrased as a solution for optimizing risk measures, it would be useful for the readers to have the assumption formally stated in Section 2.\n\n3. Can the authors provide some experiments on the scalability of the proposed approach? The toy examples considered all have dimensions less than 5 or 6.\n\n4. I think the introduction can be compressed significantly. Currently, the main contribution of the work starts at the end of page 5.\n\n5.  If \\lambda goes near 0, then \\eta can go near infinity and the regret guarantee becomes vacuous. This results in two important hyper-parameters of the problem: \\eta and \\lambda, that would essentially control exploration-exploitation. While 3.3 gives one recommendation, how sensitive are the practical results to the choice of their values?\n\n6. I do not understand the Lacing value choice of the noise variable, and why is important to obtain the desired result. Given that Theorem 3.2 is the core contribution of the work, I think it would be imperative to explain this assumption in more detail."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8829/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8829/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8829/Reviewer_25FF"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8829/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699874676819,
        "cdate": 1699874676819,
        "tmdate": 1699874676819,
        "mdate": 1699874676819,
        "license": "CC BY 4.0",
        "version": 2
    }
]