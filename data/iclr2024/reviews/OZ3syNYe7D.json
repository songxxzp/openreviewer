[
    {
        "id": "GK3ETvwu9U",
        "forum": "OZ3syNYe7D",
        "replyto": "OZ3syNYe7D",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1453/Reviewer_hXvK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1453/Reviewer_hXvK"
        ],
        "content": {
            "summary": {
                "value": "The authors present PEAR, which first relabels expert demonstrations to obtain a more effective set of subgoals, and then optimizes a hierarchical reinforcement learning (HRL) agent using reinforcement learning and imitation learning on the relabeled expert demonstrations. The authors theoretically and experimentally validate their approach."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The authors present a nice and simple idea that performs well.\n- The authors theoretically validate their relabeling algorithm in the appendix.\n- The authors demonstrate their algorithm's performance on an extensive set of tasks, including on real world robot tasks.\n- The authors qualitatively show the subgoal predictions in Figure 2."
            },
            "weaknesses": {
                "value": "- The learning curve figures are too small and very difficult to see. It would be much appreciated if the authors could fix this.\n- In Algorithm 1 for Lines 5-13 it could improve clarity if the authors included comments on what each line is doing. I found this part a bit difficult to understand.\n- It seems it is necessary to manually set $Q_{\\text{thresh}}$ for each environment."
            },
            "questions": {
                "value": "- I'm curious why PEARL-IRL outperforms PEAR_BC on Maze, Pick Place, Bin, and Hollow, but PEAR-BC outperforms PEARL-IRL on Kitchen. Do the authors have any ideas why this is?\n- Have the authors experimented with different numbers of expert demonstrations? I'm curious how the method would perform with more/less demonstrations.\n- How is $Q_{\\text{thresh}}$ chosen for each environment?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1453/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1453/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1453/Reviewer_hXvK"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1453/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698628535318,
        "cdate": 1698628535318,
        "tmdate": 1699636074314,
        "mdate": 1699636074314,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Vp5zYMeBiL",
        "forum": "OZ3syNYe7D",
        "replyto": "OZ3syNYe7D",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1453/Reviewer_cZN6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1453/Reviewer_cZN6"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces an approach that conducts adaptive re-labeling on a handful of expert demonstrations to address complex long-horizon tasks in hierarchical reinforcement learning. It offers a bound for the method's suboptimality. The proposed method is experimented on simulated tasks and surpasses several hierarchical reinforcement learning benchmarks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper introduces a simple method for generating the subgoal dataset from demonstrations and merges existing solutions for HRL training. By using ower-level policy to adaptively segment expert state-demonstrations into skills, the method provides an appealing feature of replacing the expert annotation. The paper also provides the bounds for the suboptimality of both the higher-level and lower-level policies."
            },
            "weaknesses": {
                "value": "The core idea of generating the subgoal by thresholding an environment-specific value has natural constraints. The method of determining reachability via the low-level Q function concurs the approach in [*]. In optimizing the hierarchical policies, the paper echoes [**] by generating achievable subgoals through adversarial learning on relabeled subgoals in the HRL context. The authors are encouraged to provide a more in-depth discussion about the novelty of their method and its distinction from existing literature. The result figures provided are too small to interpret.\n\n[*] Kreidieh, Abdul Rahman, et al. \"Inter-level cooperation in hierarchical reinforcement learning.\" arXiv preprint arXiv:1912.02368 (2019).\n[*] Wang, et al. \"State-conditioned adversarial subgoal generation.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. No. 8. 2023."
            },
            "questions": {
                "value": "See the above section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1453/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698674273851,
        "cdate": 1698674273851,
        "tmdate": 1699636074217,
        "mdate": 1699636074217,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ydKW9G1AiG",
        "forum": "OZ3syNYe7D",
        "replyto": "OZ3syNYe7D",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1453/Reviewer_uEcd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1453/Reviewer_uEcd"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel joint optimization approach to address non-stationarity in hierarchical reinforcement learning. By comparing the Q-values of the low-level policy with the environment-specific Q-value, adaptive relabeling of expert samples is performed for both high- and low-level policy training. The authors claim that this approach can mitigate the non-stationarity in HRL. Additionally, the authors provide performance difference bounds under the $\\phi_D$ common policy assumption, and simulation as well as real-world experiments demonstrate the comparable performance of PEAR."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "I think it is a new approach, and this paper provides some theoretical explanations. It also includes experiments in real-world environments."
            },
            "weaknesses": {
                "value": "1. **[Deal with the non-stationarity]** I don't agree that this work ameliorates non-stationarity. Since PEAR can access expert samples when training the high-level policy by BC or IRL, the high-level dynamics remain consistent with the dynamics that generate the expert sample trajectories without any change. This is inconsistent with the claim of non-stationarity made by the authors, which raises concerns about the contribution in this regard.\n2. **[Performance limitation from expert]** Since the high-level policy is optimized only through BC or IRL using expert samples and does not participate in the environment exploration process, the performance of expert samples significantly limits the upper-performance limit of PEAR. Therefore, the authors need to analyze the impact of expert performance on the upper-performance limit of PEAR and conduct experiments with expert samples of varying performance.\n\n3. **[How to get $Q_{thresh}$]** When performing adaptive relabeling, the authors require additional environment-specific Q values, which are not available in standard experimental settings and may limit the practicality of PEAR.\n4. Many of the algorithm designs are not well supported:\n   * Why does the additional margin classification objective in low-level policy optimization prevent over-estimation?\n   * In equation (2), why does the low-level policy need the BC regularization objective? In my view, the goal of the low-level policy is to reach sub-goals more effectively, and this regularization is confusing.\n   * Why is the joint value function formulated as a summation of $J$ and $J_{BC}$?\n5. The clarity of the figures in the experiments is poor, especially in Figure 5."
            },
            "questions": {
                "value": "1. In section 3, the authors define the high-level reward, $r_{ex}$. However, the subsequent high-level policy optimization process does not appear to utilize $r_{ex.\" Please clarify the purpose of the high-level reward.\n\n2. I recommend that the authors survey and include some of the most recent works in the related works section.\n\n   [1] Lee S, Kim J, Jang I, et al. DHRL: A Graph-Based Approach for Long-Horizon and Sparse Hierarchical Reinforcement Learning[J]. Advances in Neural Information Processing Systems, 2022, 35: 13668-13678.\n\n   [2] Chane-Sane E, Schmid C, Laptev I. Goal-conditioned reinforcement learning with imagined subgoals[C]//International Conference on Machine Learning. PMLR, 2021: 1430-1440.\n\n   [3] Zhang T, Guo S, Tan T, et al. Generating adjacency-constrained subgoals in hierarchical reinforcement learning[J]. Advances in Neural Information Processing Systems, 2020, 33: 21579-21590.\n\n   [4] C-Learning: Learning to Achieve Goals via Recursive Classification\n\n3. In the experimental setup, since PEAR requires training with expert samples (possibly high-performance expert samples), while other baselines like HAC do not depend on expert data, please clarify the fairness of the experimental comparisons."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1453/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1453/Reviewer_uEcd",
                    "ICLR.cc/2024/Conference/Submission1453/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1453/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698754268858,
        "cdate": 1698754268858,
        "tmdate": 1700714278068,
        "mdate": 1700714278068,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "f0oCf5Aon0",
        "forum": "OZ3syNYe7D",
        "replyto": "OZ3syNYe7D",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1453/Reviewer_XmJk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1453/Reviewer_XmJk"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a combination of hierarchical reinforcement learning (HRL) and imitation learning (IL) for solving complex long-horizon tasks. A small number of expert demonstrations consisting of sequences of states is assumed to be available. The proposed hierarchy consists of a higher level deciding on subgoals and a lower level trying to achieve these subgoals. Target subgoals are extracted from an expert demonstration by plugging states from the trajectory into the low-level Q-function and using its value as a proxy for reachability. The last state which still has a Q-value above a threshold is selected as subgoal and the procedure, referred to as \u201cprimitive enabled adaptive relabeling\u201d, is repeated starting from this state. This results in a sequence of expert subgoals. This relabeling procedure is repeated periodically to take the improved performance of the lower level into account. The authors furthermore provide a bound for the suboptimality of the learned policies. Experiments in simulated and real robotics environments show good performance of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposed method for extraction of expert subgoals from expert demonstrations is well motivated and intuitive. The integration of expert demonstrations into HRL frameworks is furthermore a promising research direction as demonstrated by the good empirical results.\n\nThe paper moreover provides a wealth of empirical results in simulated and real robotics environments. The conducted ablation studies add to an understanding of how to choose the hyperparameters of the method."
            },
            "weaknesses": {
                "value": "Unfortunately, some parts of the paper are difficult to understand or even seem contradictory. For example, in section three it is stated that no actions are part of the demonstrations:\n> Notably, we only assume access to demonstration states and not actions.\n\nHowever, in section 4.2 expert data $(s^f, a^f, s^f_\\text{next})$ is used for imitation learning on the lower level. $a^f$ is explicitly used as an expert action here.\n\nIn Algorithm 1 in line 12 there are furthermore triplets $(s_j, s_w, s_k)$ added to the expert subgoal dataset $D_g^e$ where $k$ runs from the initial to the current goal index. Adding $s_k$ seems to be unnecessary as in section 4.2 only the first two entries in the triplet are actually used for imitation learning. It therefore seems to me that Algorithm 1 could be significantly simplified. In the second to last paragraph of section 4.1 there are furthermore triplets $(s_0^e, s_i^e, a_i)$ sampled from $D_g$ to train the lower level. This is again in contradiction to what was actually added to the dataset $D_g$.\n\nIn section 4.2 BC parameters $\\zeta_L$ and $\\zeta_H$ for the lower and higher parameters are introduced which are distinct from the parameters $\\theta_L$ and $\\theta_H$ of the low- and high-level policies. It is not clear what is meant by that. For example, the BC objective in equation (1) optimizes $\\zeta_H$ but nothing in it actually depends on $\\zeta_H$.\n\nThe theoretical analysis in section 4.3 is unfortunately really difficult to follow. For example, consider the two sentences:\n> Let $\\Pi^H_D$ and $\\Pi^L_D$  be higher and lower level probability distributions which generate datasets $D_H$ and $D_L$ , and $\\pi^H_D$ and $\\pi^L_D$ are policies from datasets $D_H$ and $D_L$ . Although $D_H$ and $D_L$ may represent any datasets, in our discussion, we use them to represent higher and lower level expert demonstration datasets.\n\nSo $\\Pi^H_D$ is a probability distribution which generates a dataset $D_H$ but then $\\pi^H_D$ is a policy from $D_H$. So does $D_H$ consist of policies then? Apparently not because the second sentence says it is representing the expert dataset. So how can $\\pi^H_D$ be from that dataset then? Is it some kind of empirical policy only defined on the data? This is completely unclear at this point.\n\nThere is also the issue of the distribution $\\kappa$. It is not quite clear what it represents but I assume it is either the expert data or the distribution induced by the current policy. This would mean that the factor $\\lVert \\frac{d_c^{\\pi*}}{\\kappa} \\rVert_\\infty$ would almost certainly be infinite or at least ridiculously large which would render the bound completely vacuous.\n\nThere are several claims that PEAR mitigates the non-stationarity issue of HRL (in the introduction and in section 4.1). However, the transitions in the replay buffer of the SAC algorithm used on the higher level are getting outdated when the lower level changes. This is not addressed by PEAR so it seems questionable to me whether the algorithm really mitigates non-stationarity.\n\nWhen it comes to the experiments it is great that many environments have been considered but it seems like the baselines are not suitable for these tasks. The rewards seem to be extremely sparse in the more complex environments which makes it very difficult to make any learning progress without demonstrations. I would therefore suggest to incorporate strong IL baselines into the experiments. It is also not quite clear how the hyperparameters for PEAR and the baselines have been tuned.\n\nIn Figure 5 there are no shaded regions in the second and third row. Does that mean that only one seed was used for these experiments? That might mean the results are too noisy to be able to interpret them properly.\n\nThe overall writing seems unfinished in some parts of the paper. For example, articles are frequently missing or verbs are singular when they should be plural or vice versa. These issues are already present in the first paragraph of the introduction and continue throughout the paper. The plots in figures 3 and 5 and some of the plots in the appendix are furthermore way too small to be legible when printed out.\nThe use of notation is unfortunately not consistent throughout the paper. For example, the threshold for the Q-function during relabeling has been introduced as $Q_\\text{thresh}$ in section 4.1 but in section 5 in the paragraph \u201cAblative analysis\u201d a $D_\\text{thresh}$ makes an appearance. I would assume they refer to the same thing but it is not entirely clear. There are also smaller problems with the notation like $G$ vs $\\mathcal{G}$ for the goal space (which seems to be identical to the state space as states and goals are being subtracted from each other in section 3 but this is not made explicit).\n\nFigure 2 is difficult to interpret as the subgoals appear to be somewhat arbitrary. Perhaps a video would be better suited for demonstrating the improvement of the subgoals over training.\n\nWhile there seems to be enough material for a publication, it is unfortunately not presented in a comprehensible way. Parts of the paper are contradictory and unclear and I therefore cannot recommend it for acceptance in its current form."
            },
            "questions": {
                "value": "* Why is the lower level referred to as a primitive?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1453/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1453/Reviewer_XmJk",
                    "ICLR.cc/2024/Conference/Submission1453/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1453/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698846598598,
        "cdate": 1698846598598,
        "tmdate": 1700676822386,
        "mdate": 1700676822386,
        "license": "CC BY 4.0",
        "version": 2
    }
]