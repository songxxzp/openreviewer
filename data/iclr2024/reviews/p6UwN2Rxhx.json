[
    {
        "id": "mnAc6onFLw",
        "forum": "p6UwN2Rxhx",
        "replyto": "p6UwN2Rxhx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission14/Reviewer_isck"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission14/Reviewer_isck"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a method to prevent unconditional video generation models from encoding temporal location in their outputs. The motivation for this modification is that humans do not rely on temporal location to classify dynamic visual information. The authors show that reducing the bias to encode temporal location often improves generation performance on video quality metrics aligned with human perception. Through their work, they argue that the ability to classify temporal location based on output videos could be used as an evaluation metric for video generation."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors make an interesting observation that temporal location is implicitly encoded in the outputs of unconditional video generation models. The experiment where they show that CNNs struggle to classify temporal information in real videos, but succeed on generated videos is an intriguing contribution and well motivated the preliminary problems statement.  Finally, the proposed method for reducing the bias to encode temporal location appears novel and sound to me."
            },
            "weaknesses": {
                "value": "I found the general argument of the work to be quite confusing. The logic came across as \"humans cannot identify temporal location from one video frame, so it should not be possible to classify video frames from a good quality generative model.\" In general, it is difficult to justify how doing poorly at a task would improve a model. I think it would have been more effective and interesting to analyze *what other features* humans pay attention to when processing spatiotemporal information if not temporal location. \n\nAlthough the authors emphasized that CNNs were inspired by human neuroscience, this was confusing as there was little discussion of how this work contributes to our understanding of CNNs as models of human visual processing. There was also little justification for why CNNs are good models of human spatiotemporal processing. Standard CNNs, in fact, have been shown to often be mis-aligned with human perception when looking at the temporal property perceptual straightness (Toosi \\& Issa 2023, Harrington et al. 2023). The classification experiment, however, was still interesting, and I think it could be made stronger by de-emphasizing the human angle and focusing on the fact that temporal location is much easier to classify in generated videos than real.\n\nFinally, the organization of the paper was a bit odd at times. I do not understand why the related work section came before the discussion. I also think the discussion could be expanded on, especially in thinking about what information humans use in video perception if not temporal location."
            },
            "questions": {
                "value": "As I touched on in the weakness, are the authors trying to make a statement on how human perception relates to CNNs? Or is it more about making classifying temporal location a human-inspired video quality metric? If it is more about the metric, did the authors consider analyzing a wider set of models or even running a human experiment to validate their results?\n\nIn general, I think the work could be strengthened by thinking what other features of human spatiotemporal perceptual could your work give us insight into other than the lack of temporal location encoding? Although I see notable weakness, I think there is a lot of potential in this work and would like to hear more from the authors about what they are hoping to convey about human perception and video generation through their work."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission14/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission14/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission14/Reviewer_isck"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission14/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698094738262,
        "cdate": 1698094738262,
        "tmdate": 1699635925142,
        "mdate": 1699635925142,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yAJdm5mQAb",
        "forum": "p6UwN2Rxhx",
        "replyto": "p6UwN2Rxhx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission14/Reviewer_kbs5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission14/Reviewer_kbs5"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to integrate the Gradient Reversal Layer (GRL) into unconditional video generation, aiming at preventing the encoding of temporal location into each frame's features. This stems from the observation that humans struggle to classify the temporal location of a frame, while CNNs show impressive temporal classification accuracy on generated video frames. The experiments indicate that explicitly training unconditional video generation models to disregard the temporal information in the frames results in reduced temporal classification accuracy, while maintaining comparable or improved Frechet Video Distance (FVD) performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The exploration of the relationship between temporal classification accuracy and the quality of unconditional video generation is novel and intriguing.\n- Preliminary experiment results show the effort to unveil this novel insight, although the experimental design requires refinement.\n- The proposed approach enhances several GAN-based unconditional video generation methods concerning the FVD metric."
            },
            "weaknesses": {
                "value": "1. The preliminary experiment design needs refinement:\n- Experiments should encompass multiple baselines to bolster the claim that current unconsitional video generation methods implicitly encode temporal information.\n- The constructed dataset used for training the temporal classifier appears overly homogenous since the clips are sampled from a single video clip of FaceForensics, Sky\u2013Timelapse, or UCF-101.\n- The labels for certain frames seem less meaningful due to the frame's presence in various clips at different positions, arising from the repeated random sampling during the construction of the temporal classification dataset.\n\n2. Figure-related issues require attention:\n- Figure 2 lacks an introduction of f_{temp} in the caption, and the caption references an ImageNet pre-trained model not presented in the figure.\n- An invalid figure reference in the last paragraph of Section 3 suggests a missing figure in the manuscript.\n\n3. The training quality of the reproduced MoCoGAN-256 appears suboptimal. Tables 2, 3, and 4 reveal an extremely high FVD value for MoCoGAN with 256x256 compared to the other high-resolution video generation baselines.\n\n4. The validity of the negative gradient provided by the temporal classifier during training needs reconsideration. The temporal classifier is trained using frames from different videos, which is different from the practice in the preliminary experiment and the evaluation stage that constrains the training frames to have the same content.\n\n5. The absence of recent diffusion models for video generation (Luo et al., 2023; Yu et al., 2023; Harvey et al., 2022) in the experiments diminishes the contribution of this work.\n\n6. Additional insights on designing architectures that \"do not necessitate classification\" in the discussion section would be beneficial."
            },
            "questions": {
                "value": "- Why not use MoCoGAN-HD and StyleGAN-V in the preliminary experiment? This would avoid reproducing MoCoGAN-256.\n\n- Can you visualize some real and generated video frames used in the preliminary experiment? This could enhance clarity.\n\n- What is the reason for using distinct videos for evaluating temporal accuracy and FVD computation? Why not use the temporal classifier from the training process to evaluate temporal classification accuracy?\n\n- Can you provide qualitative comparisons on UCF101? This would provide additional insights.\n\n- Can you provide some failure cases of video generation with GRL?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission14/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission14/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission14/Reviewer_kbs5"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission14/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698651425859,
        "cdate": 1698651425859,
        "tmdate": 1699635924995,
        "mdate": 1699635924995,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1AGp4AV3b5",
        "forum": "p6UwN2Rxhx",
        "replyto": "p6UwN2Rxhx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission14/Reviewer_LjvM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission14/Reviewer_LjvM"
        ],
        "content": {
            "summary": {
                "value": "This paper demonstrates current unconditional video generation models do not considering the subtle characteristics of real-world video and proposes a simple method using Gradient Reversal Layer (GRL) with lightweight CNN to disregard the implicitly encoded temporal information within each frame. The experiment results show that neglecting implicitly encoded temporal information does not affect generated video quality and can achieve better or comparable FVD score."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper presents a very interesting perspective to estimate the realness of the generated video samples: the temporal locations of frames within random videos. This paper finds CNNs fail to classify the temporal locations from real-world video samples. But CNNs can precisely classify the temporal location of generated video samples. Based on this phenomenon, this paper proposes to use a lightweight CNN to disregard the implicitly encoded temporal information within each frame."
            },
            "weaknesses": {
                "value": "I agree that the videos generated by the model should strive to be as similar as possible to real-world videos in various aspects. However, I have some doubts about your design using CNNs to classify the absolute positions of each frame in a 16-frame video. The positions of video frames should be relative rather than absolute. For example, after sampling many short videos of 16 frames each from a long video, the first frame of one short video may be the last frame of another video. This might make it difficult for CNNs to classify the position of every video frame in real-world datasets. However, for videos generated by video generation models, since they are trained on short videos (e.g., 16 frames) during their training phase, it is easy for the generation model to remember the relative positions between frames. This makes it easier for CNN classifiers to classify the positions of video frames. I believe that more training on real-world datasets may improve their classification performance.\n\nThis paper employs a Gradient Reversal Layer (GRL) to weaken the temporal information in each video frame. The authors use GRL in several places, such as, \"We integrate a Gradient Reversal Layer (GRL) along with an ImageNet pre-trained model,\" \"We adopt an adversarial training technique using GRL with a simple network,\" \"We propose a method consisting of GRL with the temporal classifier.\" These statements may have caused a lot of confusion for readers in understanding GRL. What exactly is GRL, and how does it function within the context of this article?\n\nIn terms of experiments, the authors do not provide a video demo to demonstrate the quality of its visual generation. I think in terms of video generation, the visual quality of the generated video results is far more important than the value of FVD.\n\nIn addition, there are some typos in the article:\nThe proposed method can be simply added to existing video generation methods in a plug-and-play manner. The full framework of the proposed method is shown in Fig. ??. -> In page 5\n\nit is negligible as the difference is only 5%p. -> In page 8"
            },
            "questions": {
                "value": "see weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission14/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698947020576,
        "cdate": 1698947020576,
        "tmdate": 1699635924908,
        "mdate": 1699635924908,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OtAXkvvZEi",
        "forum": "p6UwN2Rxhx",
        "replyto": "p6UwN2Rxhx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission14/Reviewer_xNmb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission14/Reviewer_xNmb"
        ],
        "content": {
            "summary": {
                "value": "This paper tackles video generation problem. The authors find that temporal information are \"secretly\" encoded in videos generated by existing GAN-based video generation methods. To ensure temporal information is not encoded in generated videos, the authors propose to add Gradient Reversal Layer (GRL) to video generation models. Experiments are conducted on three datasets. The proposed method outperforms MoCoGAN and achieves comparable performance with StyleGAN."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Although the presentation of this paper can be improved, it is quite easy to follow the main idea.\n\n2. Comparison with 2 baselines, MoCoGan and StyleGAN are conducted on three different datasets. The proposed method outperforms MoCoGAN and achieves comparable performance with StyleGAN."
            },
            "weaknesses": {
                "value": "1. Lack of comparison with recent methods, e.g., [1, 2, 3]. These mehtods seem to have much lower FVD than the proposed methods on UCF101 dataset.\n\n2. Marginal performance improvement. The proposed method achieves slightly better performance than its baseline, i.e., MoCoGAN, on various datasets. However, the margin is quite small, e.g., FVD of 2539 (MoCoGAN) v.s. FVD of 2360 (proposed method). Such FVD improvement may not be sufficient to convince readers that visual quality of videos generated by the proposed method is better than that of MoCoGAN.\n\n3. I am not able to see why encoding temporal information in generated videos is a major problem that prevents GAN-based methods to generate high quality videos.\n\n4. Minor issues (1) There are \"??\" in this paper. (2) The authors claim that they investigate \"meaning of \u2018realness\u2019 in the video generation models\" in the abstract. However, I am having a hard time finding how the meaning of 'realness' connects to the proposed method.\n\n[1] Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models\n\n[2] Make-A-Video: Text-to-Video Generation without Text-Video Data\n\n[3] CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers"
            },
            "questions": {
                "value": "1. Why encoding temporal information in generated videos is a major problem that prevents GAN-based methods to generate high quality videos?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission14/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission14/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission14/Reviewer_xNmb"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission14/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698991175278,
        "cdate": 1698991175278,
        "tmdate": 1699635924817,
        "mdate": 1699635924817,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7UvZXT3iDx",
        "forum": "p6UwN2Rxhx",
        "replyto": "p6UwN2Rxhx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission14/Reviewer_v3eo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission14/Reviewer_v3eo"
        ],
        "content": {
            "summary": {
                "value": "The paper discusses a phenomenon in unconditional video generation models where each frame seems to inadvertently encode information about its temporal location, which should not be the case since a single frame typically provides limited temporal context. This unintended encoding allows Convolutional Neural Networks (CNNs), which are designed to mimic aspects of human visual processing, to classify the temporal location of a video's frames accurately. To address this issue, the authors propose a new method that involves incorporating a Gradient Reversal Layer (GRL) with a lightweight CNN into existing models. The GRL layer aims to explicitly disregard the temporal information that has been implicitly encoded into frames. The authors' method was tested across various video generation models and datasets and was found to be effective in a plug-and-play fashion. The results indicated that their approach could reduce the undesired temporal information encoding without negatively affecting the Frame Video Distance (FVD) score, a common metric for video generation quality. The research suggests that temporal classification accuracy should be an additional metric to assess the performance of video generation models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper presents an Interesting phenomenon that widely used convolutional neural networks embed temporal information inside the single framework. \n- The paper provides an effective method to tackle the problem it proposes and demonstrates that alleviating this artifact would lead to improved video synthesis quality.\n- As deep fake is widely concerned today, this artifact this paper proposed can be served as a method of detecting the generated videos."
            },
            "weaknesses": {
                "value": "- It is not clear why encoding the temporal information inside the frame would lead to the video quality degradation. Some empirical / theoretical explanation could be useful to provide further insights. \n- The argument that since CNN is inspired from humans, then they should not be able to detect the temporal signal embedded in the generated frames is not necessary. The main point of the paper tries to show that the generated videos have clear temporal information inside a single frame. The author could raise less confusion if framing the CNN detector is a simple quantitative tool to detect the time information embedded inside the video frame. \n- The paper would become more appealing if framing it as a method against fake generated video and find applications in face swapping detection. Adding some experiments in this domain could make the impact broader,"
            },
            "questions": {
                "value": "The authors mentioned about the human study but there is no explicit section to document the details of how to conduct the human study."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission14/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission14/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission14/Reviewer_v3eo"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission14/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699427608750,
        "cdate": 1699427608750,
        "tmdate": 1699635924722,
        "mdate": 1699635924722,
        "license": "CC BY 4.0",
        "version": 2
    }
]