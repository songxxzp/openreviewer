[
    {
        "id": "Dhxdxii12O",
        "forum": "dVq2StlcnY",
        "replyto": "dVq2StlcnY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7185/Reviewer_uKEu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7185/Reviewer_uKEu"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors introduce a self-interpretable graph neural network rooted in the concept of subgraph multilinear extension. The method is elaborated from a perspective of counterfactual fidelity, and the authors put forward both linear and sampling-based graph neural networks (GNNs) for subgraph extraction. The experimental results suggest that the proposed method is effective in achieving its intended goals."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed method is well-supported by theoretical foundations. The authors provide an insightful explanation of the causal subgraph from a fidelity perspective and derive a method that aligns well with this theoretical underpinning.\n\n2. The authors offer a theoretical framework for improving graph interpretability and shed light on why existing methods often fall short in this regard. They also articulate how they aim to meet this criterion and present experimental results that appear to support their proposition."
            },
            "weaknesses": {
                "value": "The authors conduct multiple comparisons between the proposed method and GSAT throughout the paper. However, it appears that the primary distinction lies in the sampling process, with the proposed method incorporating a higher number of samples than previously employed in GSAT. It would be valuable for the authors to emphasize this key differentiator and provide a more detailed discussion regarding the impact of increased sampling on the method's performance and results. This would enable a clearer understanding of the specific advantages of their approach over GSAT."
            },
            "questions": {
                "value": "For figure 2(b) and 2(c), what is the distance metric specifically used? Will there be any difference in the results using different metrics?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7185/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7185/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7185/Reviewer_uKEu"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7185/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698806959155,
        "cdate": 1698806959155,
        "tmdate": 1699636852893,
        "mdate": 1699636852893,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yFH4rsVqw0",
        "forum": "dVq2StlcnY",
        "replyto": "dVq2StlcnY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7185/Reviewer_pmQk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7185/Reviewer_pmQk"
        ],
        "content": {
            "summary": {
                "value": "The paper seeks a theoretical understanding of the representational properties and limitations of interpretable GNNs (XGNNs). The paper identifies the ability to approximate the so-called Subgraph Multilinear Extension as the key distribution for interpretable subgraph learning. Building on this observation, they propose the GMT model which implements such an approximation. The paper experimentally demonstrates the superiority of GMT in interpretation and generalization."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is overall very well-written and the ideas are neatly structured. The authors are able to devise a theoretical framework for characterizing the expressivity of XGNNs, and shows that the existing XGNNs fail to approximate the subgraph multilinear extension function. The proposed XGNN admits a natural design in light of the above deficiency. The experimental protocol is quite thorough and the conducted experiments adequately test the hypothesis."
            },
            "weaknesses": {
                "value": "The prediction performance of the proposed model does not see too much improvement over the state-of-the-art. There is not much discussion over the time complexity of GMT-SAM version, vis-a-vis the GMT-LIN version."
            },
            "questions": {
                "value": "1. Page 4: \"Note that $f_c$ is a GNN defined only for discrete graph-structured inputs (i.e., $\u03b1 \\in [0, 1]^m$)\". Do you mean all Boolean m-length vectors, because otherwise this is not really discrete. \n\n2. Page 4: \"which implicitly assumes the random graph data model (Erdos & Renyi, 1984). Def. 3.1 can also be generalized to other graph models with the corresponding parameterization ... \". Can you please comment on this further if this admits such a straightforward generalization to other models than ER, such SBMs or graphons as you suggested?\n\n3. What is $k$ in Eq. 9? Does it mean an application of k-layers of linear message-passing layers? In continuation, in Eq 11, what is the intuition behind the Hadamard operation between $\\hat{A}$ and $A^{k-1}$? Section 5.1 needs to be written more clearly."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7185/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698834234608,
        "cdate": 1698834234608,
        "tmdate": 1699636852791,
        "mdate": 1699636852791,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3hrBERvHtJ",
        "forum": "dVq2StlcnY",
        "replyto": "dVq2StlcnY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7185/Reviewer_gapk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7185/Reviewer_gapk"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on investigating theoretical foundations for interpretable graph neural networks (XGNN). To study the expressive power of interpretable GNNs, the authors propose a framework that formulates interpretable subgraph learning with the multilinear extension of the subgraph predictivity (subMT). To obtain a more accurate approximation of SubMT, the authors design an XGNN architecture (GMT). The superior empirical results on several graph classification benchmarks support the theoretical findings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. It's interesting to see some works focusing on better understanding the theoretical foundation for the expressive power of XGNN. The motivation to consider the connection between interpretable subgraph learning and multilinear extension is also sound.\n2. There are some empirical results to directly support the theoretical findings/claims, which have some merits. For example, the ablation studies demonstrate a better performance on counterfactual fidelity which supports the claims in Sec 4.2.\n3. The overall empirical performance of the proposed method seems significantly better than those of baselines."
            },
            "weaknesses": {
                "value": "1. For Proposition 3.3, in my understanding, it basically says with > 2-layer linear GNN, Eq (6) cannot approximate SubMT. Do you provide any proof of that? Besides, I am curious if we have any empirical findings regarding this. \n2. For interpretation performance comparison, the authors provide more baseline methods for GIN while PNA only has GSAT. I wonder what the reason is for this. Could you provide more baselines with PNA as the backbone?"
            },
            "questions": {
                "value": "See the above questions in weaknesses. \nMinors:\n1. Table 4 is before Table 2 and 3. It would be better to fix this."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7185/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7185/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7185/Reviewer_gapk"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7185/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699094570374,
        "cdate": 1699094570374,
        "tmdate": 1699636852682,
        "mdate": 1699636852682,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MX0XfCd4yR",
        "forum": "dVq2StlcnY",
        "replyto": "dVq2StlcnY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7185/Reviewer_FFuz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7185/Reviewer_FFuz"
        ],
        "content": {
            "summary": {
                "value": "The authors present a framework for GNN explanation through the len of multilinear extension. To improve the expressiveness they proposed a new method, namely GMT. One variant of GMT is to make sure the GNN is fully linear. Another neural version is to simply re-train a GNN classifier with a frozen subgraph extractor."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper offered some interesting insights into interpretable GNNs and proposed some solutions. The experiments on the proposed methods are extensive and shown pretty good results."
            },
            "weaknesses": {
                "value": "First of all, I find the paper hard to read and follow. The presentation and writing need some good work to make the paper more readable and clear. While the authors seem to had a hard time squeeze the paper below the page limit, they also over cite even by a very conservative standard. The citation list is almost the same length as the paper itself. \nThe experiment set-up is actually quite complicated as detailed in appendix E. The authors also searched extensively for different setups (stated in F2). It is unclear how sensitive the result is, wrt. warm-up strategy and a bunch of sampling strategies in Appendix E2 (page 32)."
            },
            "questions": {
                "value": "Is there any reason or hypothesis that both SubMT (Figure 2) and GMT (Figure 6/7 in Appendix) perform better on Mutag but less well on BA-2Motifs?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7185/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699332114864,
        "cdate": 1699332114864,
        "tmdate": 1699636852573,
        "mdate": 1699636852573,
        "license": "CC BY 4.0",
        "version": 2
    }
]