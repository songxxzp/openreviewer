[
    {
        "id": "yd7sTFVyCt",
        "forum": "x2yFdUSJ4I",
        "replyto": "x2yFdUSJ4I",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission214/Reviewer_S8eU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission214/Reviewer_S8eU"
        ],
        "content": {
            "summary": {
                "value": "The paper extends prior work on diffusion model-based TS with MAB to contextual bandits, and specifically, with a so-called linear diffusion model for linear contextual bandits. The corresponding TS algorithm is derived and analyzed, and numerical results are presented to show advantages over a few baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper writing is clear and easy to follow"
            },
            "weaknesses": {
                "value": "The main contribution of this paper (linear diffusion model for linear contextual bandits) is exactly equivalent to prior works, which makes the novelty and contribution very limited. See below."
            },
            "questions": {
                "value": "It is important to clarify the distinction of linear diffusion models with the \"correct\" equivalent form of HierTS and (1). Indeed, in (4) or (9), with W'sand \\Sigma's as known, it is clear that the multiple layer of linear Gaussian transformation is just equivalent to one layer of linear Gaussian prior, and hence effectively equivalent to HierTS or (1). I notice the authors tried to clarify this point in (17), but in wrong way (which I hope is not on purpose) - instead of keeping the first layer and marginalizing the other latent layers, you should actually marginalizing all layers in the prior. The equivalent to linear bandits with linear Gaussian prior (and ignoring estimation error of the diffusion model) is exactly the reason why you can get a theoretical guarantee). \n\n\n[1] Aouali I, Kveton B, Katariya S. Mixed-effect thompson sampling[C]//International Conference on Artificial Intelligence and Statistics. PMLR, 2023: 2087-2115."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission214/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission214/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission214/Reviewer_S8eU"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission214/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698601266265,
        "cdate": 1698601266265,
        "tmdate": 1700599001108,
        "mdate": 1700599001108,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QU73bdOa5p",
        "forum": "x2yFdUSJ4I",
        "replyto": "x2yFdUSJ4I",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission214/Reviewer_zHzJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission214/Reviewer_zHzJ"
        ],
        "content": {
            "summary": {
                "value": "This paper considers the Bayesian bandit problem where the priors of underlying parameters are generated by a known linear diffusion model. The authors provide a closed-form posterior distribution formula and then apply the Thompson sampling algorithm based on this formula, establishing the corresponding Bayesian regret guarantee."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The motivation is compelling, given the practical success of the diffusion prior in bandit problems.\n2. The paper is well-written with both the problem setting and the results stated clearly.\n3. The results are sound, supported by detailed proofs and simulation results.\n4. The comparsion to previous algorithms TS and HierTS is insightful and reveals benefits on using the refined prior information instead of the marginalized prior."
            },
            "weaknesses": {
                "value": "1. The adoption of the diffusion prior in bandits is interesting and powerful according to previous empirical studies. However, the paper's focus on the linear diffusion prior, essentially a linear Gaussian system, simplifies the problem considerably. Although deriving the exact formula for the posterior mean and covariance, as indicated in equations (5)-(8) of this paper, requires some calculations(see also weakness 2, 3, 4), once these results are established, I don't observe too much additional difficulty compared to prior Thompson sampling papers for linear Gaussian bandits.\n\n2. As pointed out in the first weakness, the linear diffusion considered in this paper is actually a linear Gaussian system (LGS). Several standard results from LGS theory could be directly applied to this paper to simplify the proof. For example:\n\n(2.1). Proposition 2 in Appendix B is straightforward from equations (13.89) and (13.90) in [1] when the parameters $\\Gamma = 0, A = I, z_1 = W_1 \\psi_{\\*,1}, V_0 = \\Sigma_1, \\Sigma = \\sigma^2 I,$ and $C_n = \\mathbf{1}(A_n = i) X_{n}^\\top$ are substituted into equations (13.78)-(13.83) of [1]. Here, $\\Gamma, A, z_1, V_0,C_n$ and $\\Sigma$ are notations from [1], while $W_1, \\psi_{\\*,1}, \\Sigma_1, \\sigma^2, A_n,$ and $X_n$ come from the paper. It's worth noting that although [1] let $C_n \\equiv C$ for the sake of simplicity, generalizing to allow varying $C_n$ is straightforward.\n\n(2.2). Proposition 3 in Appendix B can be directly derived by using equations (2.109) and (2.110) from [1] and making particular choices of prior covariance and design matrices. This also makes Lemma 4 redundant since it only acts as an intermediary result to prove Proposition 3.\n\n\n\n[1] Pattern Recognition and Machine Learning(2006), Christopher M. Bishop."
            },
            "questions": {
                "value": "I have one quesition on the algorithm design: Given the knowledge of {$Q_{t,\\ell}$} and $P_{t,i}$, it appears that the posterior distribution of $\\( \\theta_i \\)$ given $H_{t}$ can be computed analytically using the properties of Gaussian distributions. Why do the authors tend to firstly sample {$\\psi_{t,\\ell}$} instead of directly sampling $\\theta_{t,i}$ from this distribution?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission214/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission214/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission214/Reviewer_zHzJ"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission214/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698657827197,
        "cdate": 1698657827197,
        "tmdate": 1699635946840,
        "mdate": 1699635946840,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "19KQUScnee",
        "forum": "x2yFdUSJ4I",
        "replyto": "x2yFdUSJ4I",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission214/Reviewer_6rz1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission214/Reviewer_6rz1"
        ],
        "content": {
            "summary": {
                "value": "This work extends the previous study on informative prior in contextual bandits to having the prior in the form of a diffusion model, which targets to leverage the prior information to efficiently explore a large action space and make contextual bandit algorithms more practical. The classical Thompson sampling algorithm is extended with more detailed discussions performed in the case of linear diffusion models with (generalized) linear rewards. Theoretical analyses and experimental results are reported to demonstrate the effectiveness and superiority of the proposed design."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This work nicely follows the trends of broader ML developments and incorporates the modern diffusion models into the theoretical study of bandits, which is a valid and meaningful attempt.\n\n- The overall designs and analyses are sound based on my understanding although I have not checked all the proofs. The intuitions are also clear based on the existing works on parameterized priors in contextual bandits."
            },
            "weaknesses": {
                "value": "- Novelty. It would be nice if the authors could better illustrate how this work differs from previous works studying contextual bandits with informative priors, especially the adopted techniques. I have come across some works, e.g., [1, 2]. It seems that the essence of this work is to leverage diffusion models are priors, while diffusion models are one particular kind of graphical model. As extending the design of Thompson sampling is rather straightforward in my view, I would love to hear the authors' comments on the technical challenges in dealing with transformers as priors.\n\n- Additionally, while the target is to handle large action space, the obtained regrets still contain $K$, i.e., the number of arms, unless $\\theta$'s are fixed given $\\psi_{*,1}$. I am wondering whether this meets the design goal of handling large action space.\n\n\n[1]  Aouali et. al 2023, \"Mixed-Effect Thompson Sampling\"\n\n[2] Wan et. al 2022 \"Towards scalable and robust structured bandits: a meta-learning framework\"."
            },
            "questions": {
                "value": "It would be really helpful if the authors could provide additional comments on my unclear points listed in the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission214/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698676995934,
        "cdate": 1698676995934,
        "tmdate": 1699635946709,
        "mdate": 1699635946709,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MxsPIa7bKm",
        "forum": "x2yFdUSJ4I",
        "replyto": "x2yFdUSJ4I",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission214/Reviewer_hKWS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission214/Reviewer_hKWS"
        ],
        "content": {
            "summary": {
                "value": "To capture the correlations between arms, this paper incorporates diffusion models into contextual linear bandits. For this model, the authors propose the diffusion Thompson sampling (dTS) algorithm, utilizing diffusion models as priors. Notably, when the diffusion model is parameterized by linear functions, closed-form expressions can be obtained for the sampling procedure. Both theoretical analyses and empirical experiments confirm the superiority of dTS."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The use of diffusion models to capture correlations between arms in contextual linear bandits is impressive and highly intriguing. Personally, I think this contribution holds significant value for the bandit community.\n2. Theoretical guarantees on Bayes regret are provided.  Section 4.1 compares dTS to other methods, clearly demonstrating its statistical and computational advantages under different scenarios."
            },
            "weaknesses": {
                "value": "1. While overall well-written and easy to follow, there are certain parts where I have specific questions. Please refer to my detailed queries below. \n2. Despite the excellence of the proposed model, the theoretical contribution may be considered somewhat limited. Both the derivation of closed-form posteriors and the analysis of Bayes regret seem somewhat straightforward, given the existing works."
            },
            "questions": {
                "value": "1. Equation (2): At this point, as the specification to the linear regret model has not been made, the Bayes regret should be defined based on the mean of the reward distribution. Furthermore, in the definition of $A_{t, \\*}$, neither the function $r()$ nor $\\Theta_\\*$ has been defined.\n2. In the case that $\\sigma_1=0$ ($\\theta_{\\*, i}$ is deterministic given $\\psi_{*, 1}$), since all the arm vectors are the same, is it true that the regret is always zero?\n3. Section 4.1: The rationale behind LinTS being suboptimal is evident. That is, LinTS needs to learn $K$ independent $d$-dimensional parameters, each with a considerably higher initial covariance $\\Sigma$. However, the grounds for HierTS being suboptimal are not fully elucidated. A mere comparison of upper bounds of regrets is not equitable. More fundamental reasoning is imperative.\n4. In Equation (16), it is beneficial to explicitly state that while $\\theta_{*, i}$ for $i\\in[K]$ follow the same normal distribution $\\mathcal{N}(0, \\Sigma)$, they are not independent."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission214/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission214/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission214/Reviewer_hKWS"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission214/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699093284383,
        "cdate": 1699093284383,
        "tmdate": 1700741123446,
        "mdate": 1700741123446,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "32mXSqid5p",
        "forum": "x2yFdUSJ4I",
        "replyto": "x2yFdUSJ4I",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission214/Reviewer_sz3T"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission214/Reviewer_sz3T"
        ],
        "content": {
            "summary": {
                "value": "This paper studies contextual bandits with large action spaces. With prior specified by a diffusion model, the authors designed diffusion Thompson Sampling (dTS) algorithm. The authors theoretically analyze the performance of dTS, and empirically verify its efficacy.\n\nAfter rebuttal: I have read the rebuttal and I'd like to keep my scores."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The authors provide the first theoretically analysis of Thompson Sampling under linear diffusion models, under certain assumptions. The authors also discuss the benefits from both computational and statistical aspects."
            },
            "weaknesses": {
                "value": "Since this paper is mainly theoretical, can the authors highlight the technical contribution of this paper? More specifically, can authors elaborate on the differences in analysis between the standard Thompson Sampling? It seems that the main difference comes from the sampling of the latent parameters \\psi; but under linear diffusion model, posteriors of \\psi can be easily calculated.\n\nHow does the authors address the problem with large action spaces? It seems that both statistical and computational complexities depends on poly(K), which can be exponentially large for linear bandits. Note that recent work [1, 2, 3] have removed the dependency in K, and thus work for large action spaces.\n\n[1] \"Adapting to Misspecification in Contextual Bandits\" by Foster et al. NeurIPS 2021\n\n[2] \"Upper Counterfactual Confidence Bounds: a New Optimism Principle for Contextual Bandits\" by Xu et al. 2021\n\n[3] \"Contextual Bandits with Large Action Spaces: Made Practical\" by Zhu et al. ICML 2022"
            },
            "questions": {
                "value": "See comments above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission214/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission214/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission214/Reviewer_sz3T"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission214/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699177197048,
        "cdate": 1699177197048,
        "tmdate": 1700777106413,
        "mdate": 1700777106413,
        "license": "CC BY 4.0",
        "version": 2
    }
]