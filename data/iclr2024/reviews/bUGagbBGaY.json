[
    {
        "id": "olreWpljQu",
        "forum": "bUGagbBGaY",
        "replyto": "bUGagbBGaY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2248/Reviewer_LPzE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2248/Reviewer_LPzE"
        ],
        "content": {
            "summary": {
                "value": "This work presents a momentum-accelerated diffusion model for faster training and sampling. Empirical results are reported by applying the proposed FDM into several diffusion models (VP, VE, EDM). Three datasets (CIFAR-10, FFHQ, and AFHQv2) are used for evaluation."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- It is good to evaluate the proposal under different diffusion models.\n\n- Detailed theoretical analysis is provided."
            },
            "weaknesses": {
                "value": "- The idea to connect SGD with forward process of diffusion process is interesting. However, there's a main difference between them: the gradient $g_t$ in SGD has unique relation to $x_t$ but $\\epsilon_t$ is only a random variable, and is independent with $x_t$. They only share the formulation of the formula but has few connection in their meaning. I doubt under this situation, the momentum in the forward process still makes sense.\n\n- 'following EDM, we remove the stochastic gradient noise in Eq. (2)'. Could you point out where EDM has stated this? Discarding stochasticity during forward process seems strange. If there's no stochasticity in forward process, then the equation makes no sense but only $x_T = \\prod_{i=1}^{T} \\alpha_t x_0$ and there's nothing for the model to learn.\n\n- All the three datasets are somewhat small. It is necessary to evaluate FDM on large-scale dataset with higher resolution (like ImageNet 256x256).\n\n- It is better to show more results by applying FDM into state-of-the-art diffusion model (Stable Diffusion)."
            },
            "questions": {
                "value": "Please check the details in Weaknesses section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2248/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698759659488,
        "cdate": 1698759659488,
        "tmdate": 1699636158120,
        "mdate": 1699636158120,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dycCwFEulB",
        "forum": "bUGagbBGaY",
        "replyto": "bUGagbBGaY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2248/Reviewer_ZeuL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2248/Reviewer_ZeuL"
        ],
        "content": {
            "summary": {
                "value": "This work incorporates heavy-ball momentum into the diffusion process of a diffusion model to speed up its training and inference. Specifically,  it first shows that the forward diffusion process can be viewed as an iterative scheme of stochastic gradient descent (SGD) applied to minimize a time-varying quadratic function. Motivated by this, it adds a heavy-ball-type momentum term to the forward diffusion process. To derive a concrete algorithm, it translates the discrete-time forward process into a (deterministic) critically damped ODE (noise term treated as a constant following EDM (Karras et al. [1])). By solving the ODE, it obtains a perturbation kernel, which is incorporated with other diffusion models and used in the reverse sampling process. The numerical experiments demonstrate that the proposed approach can speed up both training and inference given a certain budget.\n\n[1] Elucidating the Design Space of Diffusion-Based Generative Models"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This paper is well-organized and can be easily followed. The idea of introducing momentum into the forward diffusion process seems to be novel and the contributions are made clear. \n\n- The experiments are promising, showing that the integration of the modified forward diffusion process (including momentum) with other models seem to work well, and the improvement in terms of training and inference speed is consistent over the baselines."
            },
            "weaknesses": {
                "value": "- Some claims are not fully supported. Theorem 1 is mainly a convergence result of SGD with momentum on a quadratic function. It is unclear how this rate would imply a faster convergence speed of the modified forward diffusion process (eqn 2) despite some similarities in the iterates. This is further complicated by the fact that the noise term in eqn 2 is treated as a constant in eqn 9. Theorem 2 shows the convergence of the proposed diffusion process to a Gaussian distribution. However, it does not really quantify the rate of convergence, and its implications on the reverse sampling process is largely a heuristic.\n\n- The actual algorithm (and its implementation) differs from the iterates considered in Theorem 2. Thus, the theory does not really capture the training dynamics."
            },
            "questions": {
                "value": "Despite some weakness in the theory justifications. The proposed approach seems to open up some new directions in speeding up diffusion process by drawing ideas from the optimization perspectives."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2248/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698793527393,
        "cdate": 1698793527393,
        "tmdate": 1699636158048,
        "mdate": 1699636158048,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vZxZNBesSH",
        "forum": "bUGagbBGaY",
        "replyto": "bUGagbBGaY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2248/Reviewer_GwvH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2248/Reviewer_GwvH"
        ],
        "content": {
            "summary": {
                "value": "The paper investigate to incorporate the momentum SGD into the diffusion process and propose a method named Fast Diffusion Model (FDM) to speed up diffusion models. Several experiments are conducted, and the results were compared against existing models to validate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The authors not only analyze the proposed method from the theoretical apsect but also validate it empirically. The experimental results seems solid. Also, accelaration of diffusion process is a meaningful problem to investigate.\n2. The paper is well-organized, and it is quite easy for readers to follow."
            },
            "weaknesses": {
                "value": "1. Using momentum to accelerate the optimation seems not a new idea, and the contribution is somewhat limited.\n2. The theoritical analyses seem not very rigorous. See Questions part."
            },
            "questions": {
                "value": "In Theorem 1, the author argues that the momentum SGD is faster by comparing the errors in expectation. However, does the upper bound of SGD tight? It is not safe to draw the conclusion that SGD is slower by simply showing that the upper bound is higher than momentum SGD, epsically when we are not aware whether this upper bound is tight."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2248/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2248/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2248/Reviewer_GwvH"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2248/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698818271414,
        "cdate": 1698818271414,
        "tmdate": 1699636157972,
        "mdate": 1699636157972,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "33cQvELpyQ",
        "forum": "bUGagbBGaY",
        "replyto": "bUGagbBGaY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2248/Reviewer_DPH4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2248/Reviewer_DPH4"
        ],
        "content": {
            "summary": {
                "value": "This paper is trying to reduce the high computational cost of training diffusion models. The authors propose a new method called the Fast Diffusion Models (FDM), which are intuitively similar to doing momentum on SGD in stochastic optimization. FDM significantly reduces the training cost, as well as the sampling cost of DMs, while maintaining or improving their image synthesis performace. Moreover, the FDM framework is general and flexible, can be adapted to several DM frameworks including VP/VE SDE, EDM. The authors verify by experiments that the performance of FDM outperforms the corresponding baseline models under most settings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The algorithm framework presented in this paper is both elegantly designed and robust in terms of performance. In its comparison of the score network of different diffusion models in Table 1, their method can be summarized as modifying the expectation of the perturbation kernel to incorporate a momentum related term $e^{-\\int_0^{t^{\\prime}} \\beta(s) \\mathrm{d} s}\\left(1+\\int_0^{t^{\\prime}} \\beta(s) \\mathrm{d} s\\right)$. The modification is simple, possibly adaptive to more diffusion models. Additionally, it has demonstrated superior performance in benchmark tests."
            },
            "weaknesses": {
                "value": "The idea of aligning diffusion process with stochastic gradient descent, and adopt acceleration techniques in SGD is not entirely new.\n\nIn [1], they first proposed the critically-damped Langevin diffusion, in which they mentioned that \"the velocity in CLD accelerates mixing in the diffusion process, and is equivalent to momentum in gradient descent\", where they refer to [2] for the equivalence.\n\nI understand that removing the velocity term stabilizes training and might result in better performance compared with CLD, but I wonder how to compare the understanding in [2] that critically damped Langevin $=$ gradient descent on probability space, with the viewpoint proposed in this paper.\n\n[1] Dockhorn, Tim, Arash Vahdat, and Karsten Kreis. \"Score-based generative modeling with critically-damped langevin diffusion.\" arXiv preprint arXiv:2112.07068\n\n[2] Ma, Yi-An, et al. \"Is There an Analog of Nesterov Acceleration for MCMC? arXiv e-prints, page.\" arXiv preprint arXiv:1902.00996"
            },
            "questions": {
                "value": "- The formulation of acceleration seems to be closely related to that in the critically damped Langevin diffusion. Specifically, (15) and (19) in this paper, which corresponds to the update equation of $x$ and its velocity, corresponds to (28) in Dockhorn et al. (2021) (with $v_0 = 0$). It seems that the techniques employed in EDM, particularly the conversion of the discrete process into a continuous one, play a significant role in enhancing the efficiency of the proposed method. Can the authors elaborate more on what is most critical in the superiority of their experimental results?\n\n- Why does the mixing speed of the forward diffusion process, transitioning from the data distribution to a Gaussian distribution, relate to the training cost?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2248/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699599589353,
        "cdate": 1699599589353,
        "tmdate": 1699636157904,
        "mdate": 1699636157904,
        "license": "CC BY 4.0",
        "version": 2
    }
]