[
    {
        "id": "XhcKzEb6KT",
        "forum": "1xyar0Ko3E",
        "replyto": "1xyar0Ko3E",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission964/Reviewer_NUL5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission964/Reviewer_NUL5"
        ],
        "content": {
            "summary": {
                "value": "The authors proposed a coreset selection pipeline for quantization-aware training in order to reduce the training cost. The authors introduced the error vector scores to measure the caused loss degradation if one data sample was removed. The authors further proved that knowledge distillation can benefit the coreset selection. The authors conducted both theoretical analysis and experimental results to show the efficacy of the method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The authors provided a new angle in reducing the training cost during quantization-aware training, i.e., using fewer training data. It was well-known that quantization-aware training was often time-consuming in comparison to post-training quantization. Reducing the training cost by using fewer samples is relatively novel and may benefit the following research.\n- The authors provided a relatively solid theoretical proof to bound the caused training error when selecting a core subset of samples. This was especially encouraged."
            },
            "weaknesses": {
                "value": "- It would be great if the authors can provide the training cost comparison between this work and prior work. Since the main target was to reduce the training cost, just listing the ratio of used training samples may not be sufficient to justify the efficiency. I would like to see the real computation cost including core samples selection, KD, SGD training of the paper. I saw Tab.3 compared the training time, but it did not compared with other SOTA QATs.\n- It seemed that this work can not achieve a lossless model in comparison to using the entire training samples. In the caption of Tab.1 and Tab.2, using full training set can almost always get a higher accuracy than using a subset, even if the ratio if 80%. If this was the case, we may not rely on coreset selection during QAT. Although the proposed methods always worked better than random selection, it was still important to show that the methods can reach the same accuracy level as using the full training set in QAT."
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission964/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission964/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission964/Reviewer_NUL5"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission964/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698609929376,
        "cdate": 1698609929376,
        "tmdate": 1699636022210,
        "mdate": 1699636022210,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tdFjugbc2w",
        "forum": "1xyar0Ko3E",
        "replyto": "1xyar0Ko3E",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission964/Reviewer_ALc7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission964/Reviewer_ALc7"
        ],
        "content": {
            "summary": {
                "value": "This submission proposed to improve quantization-aware training (QAT) by using coreset, which contains the importance samples for better training. The coreset is generated by: 1) Estimation of loss brought by specific sample using first order expansion approximation, leading to gradient norm. 2) Disagreement score during distillation with a full-precision counterpart."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. It is interesting to use data selection for QAT impovement."
            },
            "weaknesses": {
                "value": "1. The proposed methods (loss estimation and disagreement score) has no connection with QAT: these two algorithm can be applied to full-precision training without modification. \n2. Gradient norm for sample important estimation is well knowed and widely used."
            },
            "questions": {
                "value": "I don't have questions currently. Please clarify whether the proposed method is related to QAT."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N.A."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission964/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission964/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission964/Reviewer_ALc7"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission964/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698668879494,
        "cdate": 1698668879494,
        "tmdate": 1699636022120,
        "mdate": 1699636022120,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WyKZNqGxgq",
        "forum": "1xyar0Ko3E",
        "replyto": "1xyar0Ko3E",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission964/Reviewer_iCtg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission964/Reviewer_iCtg"
        ],
        "content": {
            "summary": {
                "value": "The paper presents an approach to enhance the efficiency of Quantization-Aware Training (QAT) using coreset selection. The authors examine the redundancy present in the training data of QAT and determine that significance varies among samples. To quantify the importance of each sample for QAT, they develop two novel metrics: Error Vector Score (EVS) and Disagreement Score (DS). These metrics are derived through theoretical analysis of the loss gradient. Subsequently, the authors introduce a groundbreaking approach called Quantization-Aware Adaptive Coreset Selection (ACS) that adaptively chooses samples that are informative and relevant to the current training stages, taking into account the EVS and DS metrics. The proposed ACS has been rigorously tested across various network architectures, datasets, and quantization settings, and the results demonstrate that ACS can improve training efficiency while maintaining the overall performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1.The utilization of coreset selection to enhance the efficiency of quantization-aware training (QAT) constitutes a pioneering method that has not been thoroughly investigated previously, making it an contribution to the field. \n2.The paper is written in a clear and concise manner, featuring a succinct yet comprehensive explanation of the proposed approach and the associated metrics. The authors have also provided a detailed description of the experimental setup, allowing readers to replicate their experiments and compare their results with other methods."
            },
            "weaknesses": {
                "value": "1. There are no advantages from the experimental results in Table 1 and 2. The Top-1 accuracy of 80% selected data is worse than full-data QAT and the relative training time is longer than the full-data training(53.8/0.8=67.25 > 62.3h). \n2. In Table 4, the best strategy for different ratio are various, which make the method not robust and universal."
            },
            "questions": {
                "value": "1.Could you please provide the experimental results (accuracy and training time) without KD?\n2.What's the difference between the proposed EVS and Shapley values? \n3.Do the preconditions of the simplification and approximation meet by the ordinary QAT networks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission964/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698727559634,
        "cdate": 1698727559634,
        "tmdate": 1699636022042,
        "mdate": 1699636022042,
        "license": "CC BY 4.0",
        "version": 2
    }
]