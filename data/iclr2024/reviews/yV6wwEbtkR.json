[
    {
        "id": "fLM5kciq3f",
        "forum": "yV6wwEbtkR",
        "replyto": "yV6wwEbtkR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6436/Reviewer_AG5J"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6436/Reviewer_AG5J"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a new distillation technique which is based on training teacher models so that they are well-suited for conveying information to the student models. Towards that end, the authors introduce a\"conditional mutual information\"(CMI) objective into the training process of the teacher model, whose goal is to improve the teacher's Bayes conditional probability estimates (via its soft-labels) \u2014\u00a0according to recent knowledge-distillation literature, more accurate Bayes conditional probability estimates result in better student's performance.\n\nOverall:\n\n(i) The authors argue that the so-called dark knowledge passed by the teacher to the student is the contextual information of the images which can be quantified via the conditional mutual information.\n(ii) They provide evidence that temperature-scaling in KD increases the teacher's CMI value\n(iii) They provide evidence that show that models with lower CMI values are not good teacher's, even if they're more accurate.\n(iv) They provide experiments on CIFAR-100 and Imagenet datasets showing evidence that their method helps in improving the student's performance, compared to other standard distillation techniques.\n(v) They show that their technique is especially effective in few-shot and zero-shot settings."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This is a well-written paper that presents a novel approach to knowledge distillation. They authors have provided extensive experimental evidence."
            },
            "weaknesses": {
                "value": "\u2014\u00a0The role of the teacher as a \"provider of estimates for the unknown Bayes conditional probability distribution\" is a theory for why distillation works that applies well mainly in the context of multi-class classification, and especially in the case where the input is images. (Indeed, there are other explanations for why knowledge distillation works, as it can be seen as a curriculum learning mechanism, a regularization mechanism etc see e.g. [1])\n\nIn that sense, I feel that the author should either make the above more explicit in the text, i.e., explicitly restrict the scope of their claims to multi-classifcation and images, or provide evidence that their technique gives substantial improvements on binary classification tasks in NLP datasets (but even in vision datasets).\n\n\u2014\u00a0One of the main reasons why knowledge distillation is such a popular technique, is because the teacher can generate pseudo-labels for new, unlabeled examples, increasing the size of the student's dataset. (This is known as semi-supervised distillation, or distillation with unlabeled examples, see e.g. [2, 3]. )  It seems that, in order to apply the current approach, one requires the ground-truth labels and, thus,  one has to give up a big part of the power of knowledge distillation as a technique.)\n\nTo be clear, I still like the paper and I am leaning towards acceptance even if the scope of the paper is more limited, but I think it would be beneficial to the research community if the above comments were addressed.\n\n[1] Understanding and Improving Knowledge Distillation [Tang\u2217, Shivanna, Zhao, Lin, Singh, H.Chi, Jain]\n[2] Big self-supervised models are strong semi-supervised learners [Chen, Kornblith, Swersky, Norouzi, Hinton]\n[3] Weighted Distillation with Unlabeled Examples [Iliopoulos, Kontonis, Baykal, Trinh, Menghani, Vee]"
            },
            "questions": {
                "value": "\u2014\u00a0Does the proposed method and theory works well/applies in NLP datasets/binary classification contexts? \n\u2014\u00a0Is there a way to apply this technique in the context of semi-supervised distillation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6436/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6436/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6436/Reviewer_AG5J"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6436/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698618359604,
        "cdate": 1698618359604,
        "tmdate": 1700420692877,
        "mdate": 1700420692877,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "J4o2wR26FP",
        "forum": "yV6wwEbtkR",
        "replyto": "yV6wwEbtkR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6436/Reviewer_Nadg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6436/Reviewer_Nadg"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a method which aims to train the teacher to optimise the student. This is achieved through maximising the conditional mutual information between input and predicted label, conditioned on the true label. The approach demonstrates improved knowledge distillation on CIFAR100 and Imagenet using varies CNN architectures."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper is very simple to understand and implement, which only a simple regulariser added to the training of the teacher model, which minimises the KL between the predicted probability and the average probability. \n* The results are conclusive and well presented on ImageNet using plenty of architectures. \n* The extension to few and single-shot experiments are nice."
            },
            "weaknesses": {
                "value": "In terms of weaknesses:\n* I'm interested to read more about what the role of the CMI regulariser actually does, is it just decreasing the variance of the predictions? Or leading to a distribution with higher entropy? Does this method work just as well if you add an entropy regulariser?\n* As far as I can tell, the value $T$ is not defined, is this for the softmax?"
            },
            "questions": {
                "value": "* What is the value of $T$? \n* Does the CMI loss just reduce the entropy?\n* If so, is it possible that the same effect can be achieved by simply running this method with temperature scaling? I.e. drop the CMI term?\n* With regards to 6.2. my understanding is that this is using the negative scores during training, so is this really zero-shot classification? Why do you expect this?\n* Did you try varying different classes to drop? \n* In Figure 3, why is the heat map on the terrier not on the body of the animal? Bottom, third from left."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6436/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6436/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6436/Reviewer_Nadg"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6436/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698742739163,
        "cdate": 1698742739163,
        "tmdate": 1700654464213,
        "mdate": 1700654464213,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QeT4oMilyH",
        "forum": "yV6wwEbtkR",
        "replyto": "yV6wwEbtkR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6436/Reviewer_oUDj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6436/Reviewer_oUDj"
        ],
        "content": {
            "summary": {
                "value": "This work  builds upon the insights from the previous study on knowledge distillation [1], which implies that producing a good teacher model \nsimilar to the optimal Bayes class probability $P^{*}_{X}$, is crucial for enhancing the performance of the student model. To convey this message, the authors propose a new training objective for the \"teacher model\" by introducing the empirical estimate of conditional mutual information as a regularizing term (MCMI). \n\nThe authors provide empirical evidence between MCMI and the accuracy of the student model; as the MCMI attains higher values, the the corresponding teach model obtains the highest accuracy. Furthermore, when using the teacher model trained with the MCMI regularizer, the corresponding teacher exhibits improved accuracy in most existing knowledge distillation algorithms. The proposed regularizer leads to improved performance of the student model in zero-shot and few-shot classification tasks  as well.\n\n[1] A Statistical Perspective on Distillation - ICML 21"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "### Simple idea:\n\n> In implementation sense, the idea looks simple and easy to implement this idea; introducing a estimate of the MCMI in Eq (2) is additionally necessary.\n\n### Empirical improvement:\n> It seems that the proposed objective for the teacher model can be integrated with existing knowledge distillation algorithms which mainly focus on the distillation objective in view of \"student\" model. The proposed regularizer for the 'teacher' model seems to be effective in enhancing the performance of the 'student' model trained with existing knowledge distillation algorithms."
            },
            "weaknesses": {
                "value": "### Less elaboration on relationship between conditional mutual information $I(X , \\hat{Y} | Y)$ and optimal bayes classifier $P^{*}_{X}$\n\n> While it is intuitively clear that using the conditional mutual information as the regularizer term can capture the contextual information of $X$ (Image) and provide additional information to a student model, the direct connection between conditional mutual information and the optimal Bayes classifier is less explained. I believe explaining this connection is important because this approach is motivated from the importance of optimal classifier $P^{*}_{X}$."
            },
            "questions": {
                "value": "* Q1.  Could you elaborately explain why minimizing $I(X , \\hat{Y} | Y)$ can make the teacher model $f$ to be more similar to the optimal bayes classifier ? \n\n\n\n* Q2. It seems that the proposed regularizer requires the pre-trained model as the teacher model and apply the further training to the teacher model with the proposed objective of Eq. (14). How do we set the number of iterations further training? Based on my understanding, since we expect this regularizer to make the teacher model contain additional information as well as to be properly certain (not overconfident), setting the number of iterations is important hyperparameters and might significantly affect the performance of student model."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6436/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6436/Reviewer_oUDj",
                    "ICLR.cc/2024/Conference/Submission6436/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6436/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698840782690,
        "cdate": 1698840782690,
        "tmdate": 1700681646671,
        "mdate": 1700681646671,
        "license": "CC BY 4.0",
        "version": 2
    }
]