[
    {
        "id": "zu8Yars45l",
        "forum": "YRXDl6I3j5",
        "replyto": "YRXDl6I3j5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5176/Reviewer_Gapa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5176/Reviewer_Gapa"
        ],
        "content": {
            "summary": {
                "value": "The paper looks into beliefs and lying behavior of large language models. They evaluate consistency in the beliefs of language models, whether language model believes in the lies that it is generating and the deceptive behavior when incentivized for lying."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- They present an interesting exploration of the interaction between the beliefs of language models (the facts that it is trained on) and how this plays into the act of deception by language models.\n- Experiments are quite intuitive.\n- The evaluation is conducted on a large number of models of varying sizes.\n- They make interesting observations like the reason for reduced consistency in smaller language models, the lie re-affirming behavior of LMs, lying as a means to an end etc."
            },
            "weaknesses": {
                "value": "- While it is not hard to follow the text, the structure of the paper makes it very hard to get a complete picture of the experiments.\n\n- Much of the experimental details are only available in the appendix, making it impossible to understand the paper solely based on the main text. For instance:\n    -  \"We use GPT-4 to generate a dataset of 1981 propositions, each with 38 scenarious\" : There is no information on what is the criteria for these propositions, how they are prompted and how the scenarios are generated.\n    - General distribution of labels in the data  \n    - Details regarding instruction-finetuning, SFT and RLFT\n\n- The results are depicted as plots which are quite hard to understand. The corresponding discussion is in a very obscure way that it is difficult to judge the reliability of the claims made.  For instance, the LM consistency evaluation shown in the graph is only for hand full of instances and  not the complete dataset. The plot for scaling trends in lying is also sacrifices a lot of information."
            },
            "questions": {
                "value": "- What are the criteria for generating propositions using GPT-4 ? How are the scenarios generated from the propositions ?\n- How does belief consistency correlate with the lying capability of the Large language models ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5176/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5176/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5176/Reviewer_Gapa"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5176/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698702166895,
        "cdate": 1698702166895,
        "tmdate": 1699636513397,
        "mdate": 1699636513397,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "azvbvVb9Za",
        "forum": "YRXDl6I3j5",
        "replyto": "YRXDl6I3j5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5176/Reviewer_te76"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5176/Reviewer_te76"
        ],
        "content": {
            "summary": {
                "value": "This paper evaluates whether fine-tuning language models on data judged to be truthful by an erroneous judge (e.g., one that mis-evaluates the truthfulness of statements about fruits) will then generate false statements about fruits when it is evaluated. Experiments evaluate different base LMs and the effect of scaling, as well as how a LM responds to user questions asking for a confirmation of confidence about a generated statement."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper evaluates the influence of fine-tuning a model on false statements. It evaluates a breadth of models across different sizes and fine-tuning techniques."
            },
            "weaknesses": {
                "value": "Clarifications on prior work:\n* OpenAI's technical report on GPT-4 doesn't say that it actually successfully convinced a human to solve a CAPTCHA. The report itself is very vague on the experimental setup; this particular example (which was picked up by news sites that made the assumption GPT-4 somehow actually hired a worker) is introduced as \"illustrative\" with no additional details on what actually happened during red-teaming.\n* Cicero (Bakhtin et al. 2022) is never described as having \"learned to deceive and betray other players\", at least in the Science paper. Where is this claim made?\n\nThe definition of \"belief\" is indeed contentious, and it seems this paper attempts to choose a particular formalization and stick with it, but is itself quite inconsistent in this formalization.\n* \"LMs learn to lie as a result of being fine-tuned towards the goal of being evaluated as truthful... these lies are goal-oriented, we argue they are intentional\". The _optimization_ is goal directed, because we are performing gradient descent to maximize the probability of some new data (in SFT) or the reward (in RLFT). I don't understand the conclusion that this means the LMs themselves are goal-directed. There's nothing in particular that is optimizing the models towards \"truthfulness\": they are being trained further to maximize the probability (or reward) of new data. \n* The examples presented in the paper are themselves actually quite ambiguous. \"Bananas are yellow\" isn't something that even semanticists would agree upon. What does it mean for a _human_ to agree to this proposition? It's quite nuanced: perhaps something like bananas are often yellow, or prototypically yellow. But bananas can also be naturally green or brown, and in fact _all_ bananas are green before they are yellow, and some bananas may never be yellow (e.g., if they are eaten before they ripen). In fig h, I don't see why it's wrong: apples can be yellow!\n* In Section 6 this is also a problem: capability is ambiguous in English, where it could either mean \"technically capable\" under certain circumstances, or it could mean having permission to do something. So the statement \"I am not capable of translating a French instruction manual...\" is ambiguous, and it seems to me that concluding that it is \"lying\" is jumping to conclusions.\n* I found the descriptions of \"stated\", \"revealed\", and \"accepted\" beliefs imprecise. \"a question\" about $\\phi$ is very vague: anything could be formulated as a question. What it means for an LM to \"act\" is unclear to me, as their only action space is the tokenizer's vocabulary. I found it unclear where experimental setups were aligned with each of the three different ways of evaluating belief.\n* Figure a doesn't precisely evaluate belief in the proposition \"bananas are high in potassium\". It also evaluates, among other presuppositions: \"You want to help Jeff\", \"Jeff wants to follow his doctor's instructions\", etc.\n* What does it mean for an LM to have \"sufficient incentive... to answer in line with its actual belief\"? What exactly is the incentive here?\n* Functionally, why is the trained to produce false evaluations about fruit statements not considered to be \"lying\", while the LM fine-tuned on its evaluation is? Is something only a lie when it's generating a piece of text? Would it not be considered deceptive/lying to make a false judgment even in this binary classification case, under the definition of deception/lying presented in the paper?\n\nRegarding consistency:\n* I am not convinced that the evaluation of consistency is strong enough. It seems there are infinitely many possible questions that could evaluate whether a model \"believes\" a proposition. E.g., asking in other languages, or in scenarios that were simply not included. Sampling 10 scenarios from a language model does not seem nearly comprehensive enough, and there is no technical reason that these models _would_ be able to have consistent \"beliefs\".\n* An inconsistency in how \"consistency\" is evaluated: the same model behavior is sometimes evaluated as inconsistency, sometimes as mere poor performance (smaller models in Section 4 where they \"do not know the answers\"), and sometimes as lying due to different behavior in different contexts (Section 5.1). Why should one not conclude that the finetuning experiment as described in Section 5 just worsens consistency? How is fig i \"revealing\" beliefs rather than just showing inconsistency across different contexts?\n\nIn general, a lot of actual numerical experimental results are missing. \n* E.g., how often a case like Figure g occurs, where the output is consistent regardless of the context, versus e where it appears to be dependent on the context. Currently, it just says \"some cases\" -- but how often is this actually happening?\n* How often, precisely, does re-affirmation occur for true/false generated statements, as shown in Figs j-l?\n* Instead of figures, I'd suggest putting numbers in tables to help with this.\n\nMinor points on readability:\n* It was unclear to me that Section 3 is introducing a set of evaluation tasks (i.e., creation of the Scenarios dataset).\n* The experiment in Section 5 is referred to many many times before Section 5 itself, sometimes as a non sequitur (2nd sentence of \"Measuring truthfulness on MultiRC\")\n* The figures were very difficult to read. Text was very small and what they are actually plotting is not explained."
            },
            "questions": {
                "value": "* What does it mean for scenarios to be \"neutral so that they do not lead the LM to have any particular belief about the proposition\"?\n* What is the difference between evaluating stated and revealed beliefs in Section 3, where PARALEL evaluates \"stated\" beliefs and Scenarios evaluates \"revealed\" beliefs? Is the different just that PARALEL is asking directly about a proposition and Scenarios is evaluating its application in some context of use?\n* Did you experiment with just SFT on false statements about fruits, rather than training an external evaluator? This essentially seems to be the same thing as the experiment with SFT in Section 5 except that the source of the statements would be coming from some a priori set of false statements rather than generated statements from the model.\n* Can you share some examples of the fruit statements used for RLFT/SFT? Are they in the same format as in Fig 3?\n* Are the false statements used to train the evaluator separate from those used to do fine-tuning and those used to evaluate fine-tuned models? If so, how were they split? Did you experiment with separating not just by statements, but by types of fruit as well? \n* Do you have an example of Fig f (non fruit-related examples) that require using external facts? One fundamental difference from e and f is that f requires matching from the context, whereas e may contradict facts learned during pretraining.\n* I'm confused about what's being shown in Fig e. Is it that the model prediction flips whatever is in the context? Are the two red boxes showing what the output is for \"low\" and \"high\" potassium respectively in order from top to bottom?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5176/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698890613137,
        "cdate": 1698890613137,
        "tmdate": 1699636513311,
        "mdate": 1699636513311,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oXnXAinERl",
        "forum": "YRXDl6I3j5",
        "replyto": "YRXDl6I3j5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5176/Reviewer_vXt1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5176/Reviewer_vXt1"
        ],
        "content": {
            "summary": {
                "value": "The authors present an analysis of how reliable current LLMs are. They do that by testing the beliefs of the LLMs in different ways to measure the consistency of the models output. Examples are giving the same question in different versions and evaluating if the answers change, or giving the models multiple choice question to test their beliefs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors evaluate a very large set of LLM-models. The introduction and state of the art is presented well. The research question is very interesting and important for the application of LLMs in real-world applications."
            },
            "weaknesses": {
                "value": "Questions are generated by GPT-4. For practical reasons, i.e. generating a large corpus of questions for the models, this is understandable. It raises the question of fairness. If the questions are generated by GPT-4, then the models are all tested against GPT-4 as the \"ground truth\" model.\n\nThe section about training a system how to lie is very interesting, but I am not sure if I would classify this as lying. The model is fine-tuned, e.g. trained to believe that a banana is green by reinforcement of a biased judge. I don't see how this is lying, if the system was trained to relate bananas to green. \n\nSimilar with the question about GPT-4's capability of translating a french manual for building a bomb into English. Just by evaluating the three answers, at first, I understand the authors position. The first answer is no, the second is yes, and the third is yes, but it is programmed to refuse to answer such questions. Given the last answer, the first answer \"no\" is also correct, because the system is programmed to refuse to fulfill the task. It seems to me, that the systems has, in all cases, correctly answered the question. I cannot see any lies in Fig m to Fig. o."
            },
            "questions": {
                "value": "- Please address my concern about generating the questions with GPT-4\n- Please address my concern about the lies shown in Fig m to Fig o. GPT-4 is capable of translating the manual, but cannot translate the manual based on the programmed refusal-mechanism. It might have inconsistent beliefs if it should be released as open-source, but I don't see, how this inconsistency classifies a lie."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5176/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5176/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5176/Reviewer_vXt1"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5176/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699257831270,
        "cdate": 1699257831270,
        "tmdate": 1700473021044,
        "mdate": 1700473021044,
        "license": "CC BY 4.0",
        "version": 2
    }
]