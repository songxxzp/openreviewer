[
    {
        "id": "2uzSC29lw2",
        "forum": "N2WchST43h",
        "replyto": "N2WchST43h",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6712/Reviewer_5dfS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6712/Reviewer_5dfS"
        ],
        "content": {
            "summary": {
                "value": "The authors consider fully connected one hidden layer neural networks with shifted ReLU activations and develop an adversarial training algorithm with cost $o(mnd)$ per iteration by applying half-space reporting data structure. To do that, the authors rely on three main techniques:\n1. approximation via pseudo-network sufficiently precise with high probability\n2. using shifted ReLU functions instead of standard ones to attain $o(mnd)$ time complexity per training iteration\n3. use a tree data structure to find the neurons that need updating more quickly (Half-space reporting)\n\nThe algorithm consists of a loop that first calculates the adversarial training set and the indices of the activated neurons for each element of the training set, then doing a gradient update for all active neurons. A series of theorems and lemmata are presented, analyzing the proposed algorithm, which is followed by a time complexity analysis. The reviewer did not check the proofs in the appendix."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Adversarial training algorithms and their analysis is an important problem in the field of adversarial robustness. \n- The assumptions seem reasonable."
            },
            "weaknesses": {
                "value": "- The relevance of the paper remains unclear. Is it theoretically relevant or also practically?\n- The restriction to 1 hidden layer networks is strong. \n- The Paper is hard to read, the reasoning jumps back and forth. The reviewer recommends a thorough rewrite. It would be good if the authors first introduce concepts and then use them. An example is the \"query, insert, remove\" from the data structure in Algorithm 1 (Page 6) that gets introduced in Section 5 (Page 8). Similarly, Theorem 1.1 (Page 2) has a reference to Equation 2 (Page 4). The paper seems to be rushed, also reads that way. \n\nTypos:\n\t- 3.1 the typesetting of $\\ell_p$ \n\t- The inequality in Theorem 4.1 seems to be the wrong way round. \n\t- In theorem 4.3: \"$T = \\Theta(\\epsilon^{-2} K^2)$. in Algorithm\""
            },
            "questions": {
                "value": "- Are the contributions of the paper mainly of theoretical or also practical interest? Can the algorithm be efficiently parallelized? Are efficient vectorized/GPU implementations possible? What are the memory requirements of the algorithm? \n- Page 3: what is the definition for an $\\epsilon$-net?\n- What about deeper networks? On an intuitive level, the reviewer does not see a reason that the complexity should worsen there."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6712/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698747923120,
        "cdate": 1698747923120,
        "tmdate": 1699636771302,
        "mdate": 1699636771302,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "thC8xtsDhA",
        "forum": "N2WchST43h",
        "replyto": "N2WchST43h",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6712/Reviewer_RwpB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6712/Reviewer_RwpB"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an adversarial training algorithm for 2-layer neural networks with ReLU activation that has sublinear o(mnd) per-iteration time complexity, compared to standard \u03a9(mnd). The result mainly relies on the insight that only o(m) neurons will be activated for each input data per iteration."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper presents some interesting result on analyzing per-iteration complexity of adversarial training of a special setting of 2-layer neural networks. Theoretical analyses are solid and insightful.\n2. Complexity of adversarial training is less analyzed compared with standard training, the work would be interesting to some researchers."
            },
            "weaknesses": {
                "value": "1. Just like in many other works on 2 layer neural nets, it is unclear if the insight can generalize to the more common setting of deeper neural nets.\n2. It seems only optimal adversaries are analyzed (e.g., in theorem 4.4), but most cases it is impractical to have an optimal adversary."
            },
            "questions": {
                "value": "Could authors provide some discussions on and thoughts on if the activation sparsity could generalize to deeper nets?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6712/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698801011546,
        "cdate": 1698801011546,
        "tmdate": 1699636771168,
        "mdate": 1699636771168,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gaHxjw1ok4",
        "forum": "N2WchST43h",
        "replyto": "N2WchST43h",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6712/Reviewer_Wuuy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6712/Reviewer_Wuuy"
        ],
        "content": {
            "summary": {
                "value": "The authors propose an adversarial training algorithm designed for two-layer ReLU networks. Combining multiple techniques, they manage to prove that their algorithm runs in time sublinear in the size of the network."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The authors propose an interesting algorithm and manage to show that per iteration cost is sublinear in the size of the network for a two-layer ReLU network. The paper is very clearly written with detailed proofs in the appendix.  \n\n- The use of the techniques such as half-space reporting data structures and shifted ReLU in the design of the algorithm are interesting and instructive."
            },
            "weaknesses": {
                "value": "- The current result seems to be more of a theoretical interest than a practical algorithm. The limitation of two-layer network, well-separation for adversarial loss are both severe limitations in practice. \n\n- There are also some unclear or missing details that need to be addressed in the Questions section below."
            },
            "questions": {
                "value": "- Is there any assumption on the adversarial example oracle A in Algorithm 1? I don't seem be able to find any reference to its complexity analysis in the main paper. Usually adversarial examples are found by iterative projected gradient descent (on the input x) and their runtime should also be taken into account. \n\n- If we take away the adversarial example generation in Algorithm 1 and just use the input x_i as training example, we should then obtain a sublinear training algorithm for a two-layer ReLU network. I am wondering why the authors are focusing on adversarial training, or if there are any parts of the algorithm that are specific to the problem of adversarial examples. The 3 techniques listed in Section 2 are not specific to adversarial training."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6712/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698817909782,
        "cdate": 1698817909782,
        "tmdate": 1699636771060,
        "mdate": 1699636771060,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "eQXU9WAf6G",
        "forum": "N2WchST43h",
        "replyto": "N2WchST43h",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6712/Reviewer_yRgF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6712/Reviewer_yRgF"
        ],
        "content": {
            "summary": {
                "value": "The paper tries to address the computational complexity of adversarial training in neural networks, which is an active research area.\nThe work addresses training efficiency by trading off extra memory for computation and incorporating an additional half-space data structure, efficiently identifying only the active neurons for computation. The key insight is that not all neurons are active for every input data point, enabling the computation of the dot product exclusively with the active neurons. While the method of initially identifying active neurons is not novel, the application of efficient geometric data structures is both new and promising. The research introduces an algorithm designed for this purpose; although it does not include experimental results, it does offer convergence proofs for two-layer networks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The approach is novel where adversarial training that leverages geometric data structures at the expense of extra space. It extends the direction of first identifying the active neurons and then only taking dot product with those neurons. \n\nThe paper is well written and however, without empirical results or illustrative examples, the clarity of the paper's practical implications could be enhanced.  \n\nThe work is significant as it has a potential to further improve the direction of efficient adversarial training of robust neural networks.\n\nI have not conducted an exhaustive review of the convergence proofs; however, upon initial examination, they appear to be correct."
            },
            "weaknesses": {
                "value": "The study lacks experiments, even on toy datasets, to determine whether its findings are applicable in practical scenarios.\n\nFurthermore, it has not been compared with existing works. Although it is primarily theoretical, clarity on how it measures up against established methods is missing.\n\nThe paper does not discuss how reducing training time might impact the robustness performance of the model. This trade-off between efficiency and robustness is crucial for practical use cases.\n\nLikewise, the trade-off between memory usage and efficiency warrants discussion. While the paper could comment on memory requirements, it may be that for only two-layer networks, the difference is minimal. However, it would still be beneficial to understand this aspect, perhaps by employing very wide neural networks to confirm it in a practical scenario.\n\nIt is also unclear whether the proposed methods can be effectively translated into actual GPU computations. If the approach results in underutilized GPUs, it might still be valuable but may not reduce the overall wall time. Some commentary on this feasibility would be constructive."
            },
            "questions": {
                "value": "To translate the theoretical advantages into practical scenarios, the paper should present at least some toy experiments.\n\nIt is important to explore how reducing training time affects the model's robustness performance, as the trade-off between efficiency and robustness is crucial for practical use cases.\n\nThe same consideration should be given to the trade-off between memory usage and efficiency. It would be beneficial for the paper to comment on memory requirements. While the impact on only a two-layer network might be minor, understanding this trade-off is essential, and could be tested using neural networks with extremely high widths in practical scenarios.\n\nIn short, a toy experiment featuring a two-layer network that showcases practical, minimal use cases by comparing memory usage, computational efficiency, and robustness performance would be a valuable addition to the work."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6712/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699198480791,
        "cdate": 1699198480791,
        "tmdate": 1699636770953,
        "mdate": 1699636770953,
        "license": "CC BY 4.0",
        "version": 2
    }
]