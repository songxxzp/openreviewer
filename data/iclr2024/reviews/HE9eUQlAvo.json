[
    {
        "id": "IlPuYhuR4w",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6213/Reviewer_ZqCd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6213/Reviewer_ZqCd"
        ],
        "forum": "HE9eUQlAvo",
        "replyto": "HE9eUQlAvo",
        "content": {
            "summary": {
                "value": "This paper diverges from the mainstream focus on enhancing model architectures and learning algorithms on fixed datasets. Instead, it tackles an essential yet overlooked issue: understanding how a fixed convex learning model (or a convex surrogate for a non-convex model) benefits from data by interpreting the feature space. Specifically, this paper proposes to use influence estimation models to interpret the classifier's performance through the lens of data features. Furthermore, it introduces data selection methods based on influence to enhance model utility, fairness, and robustness. Through extensive experiments on both synthetic and real-world datasets, the effectiveness of the proposed method is validated. Additionally, the method proves effective not only in conventional classification scenarios but also in more challenging situations, such as distribution shifts, fairness poisoning attacks, utility evasion attacks, online learning, and active learning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The research topic is realistic and important. In the era of big data, the analysis of \"more important data points\" is significant. \n- Experimental results are great. In a series of tasks, the proposed method can achieve the best performance."
            },
            "weaknesses": {
                "value": "- The motivation of this paper is not clear and not strong. \n- Technical contributions of the proposed method are limited. \n- Writing is not unsatisfactory. Many times, readers are unable to understand the author\u2019s true intentions. \n\nMore details about the weaknesses can be checked below."
            },
            "questions": {
                "value": "- At the beginning, this paper claims it is related to data valuation,  data influence, and data efficiency. Essentially, this paper studies the problem of \"coreset selection\", which is not a new problem in machine learning. Coreset selection surely is related to the above topics. Therefore, it seems that there is no need to introduce so much redundant content in the main paper. \n- The motivation is not clear. It has been fully studied to use the influence function to analyze the importance of data points. This paper follows this line. However, after checking this paper, I am confused about the proposed method of this paper, as the paper just combines the influence estimation and decision tree. Also, why do we need this tree?\n- This paper uses a lot of space to introduce the previous versions of influence functions (Section 2). However, it is not clear that the difference between previous work and this work mathematically.\n- Could the paper provide more high-level intuitions about the formulas of the overall regression tree prediction and hierarchical shrinkage regularizes?\n- For the method in Section 3.2, what is its time/space complexity?\n- Figure 3 and the illustrations in the appendix are not informative. Could the paper supplement more descriptions for them?\n- Could the paper discuss the difference between this paper and the work [1]?\n\n----\n[1] Shuo Yang et al. Dataset Pruning: Reducing Training Data by Examining Generalization Influence. ICLR 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6213/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6213/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6213/Reviewer_ZqCd"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6213/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697263786738,
        "cdate": 1697263786738,
        "tmdate": 1700448868676,
        "mdate": 1700448868676,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "KeL7EPROT9",
        "forum": "HE9eUQlAvo",
        "replyto": "HE9eUQlAvo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6213/Reviewer_MEKL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6213/Reviewer_MEKL"
        ],
        "content": {
            "summary": {
                "value": "The authors propose an influence-based trimming approach to uncover which samples and features contribute positively/negatively to the specified utility function. The authors perform experiments with various utility functions (fairness, accuracy, etc) on several datasets: adult, German, drugs, and celebA, among others."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- From my understanding, the authors are trying to not only get the best data samples useful for the model but also identify the best features of the data to use. They use regression trees to help in feature selection and use the influence function for the sample selection. \nNeither influence estimators for sample valuation (for different utilities: fairness, accuracy, data poisoning, etc..) nor CART as a feature selector is a new concept, but the combination is a useful endeavor and an interesting perspective. \n\n- Authors carry out several experiments on several datasets and investigate the performance of their method on several applications, and also compare their work with TMC Shapley."
            },
            "weaknesses": {
                "value": "- When I read feature space, I think of d-dimensions where the variables (features) live. The authors' writing was a bit confusing to me because from the abstract to the introduction, I thought their influence estimation-based method was identifying features from the feature space with positive/negative influence on the model utility (accuracy, fairness, robustness, and so on). \nHowever, at the beginning of the background section and throughout the experiment results, the authors focus on only the contribution of the training samples to the utility function. \nI think the authors should be a bit more clear in the writing or presentation.  Although section 3 is fairly written, I would recommend that authors revisit abstract+sections 1-3.\n\n- Since the authors focus on features and samples, it would have been informative to see the difference in selected/excluded features and samples and the consequential contribution to the utility with and without the authors' method. \n\n- Although influences functions are not affected by retraining-related complexity, they have a high incremental complexity due to the computation of the Hessian matrix for each x_{i} valuation, which might worsen (beyond retraining) when n is large. \nAdditionally, using CART as a sub-module further increases model complexity.\nI would have appreciated looking at the code specific to section E.1 in the appendix (I couldn't find it in the shared code base)\n\n\n- Not entirely sure, probably it's the figure, I find the almost constant utility values with random deletion somewhat unrealistic. \nCould the authors also explain Figure 2C?\nThe scale for accuracy on some figures in 2 is not intuitive. Is it possible for authors to adopt similar scales for similar utilities across datasets?\n\n- Experimental results. \n  - Figure 2 Specific questions: I find the almost constant utility values with random deletion somewhat unrealistic.  Could the authors also explain Figure 2C?\n  - Figure 10 in the appendix.  If you're removing low-value samples, I wouldn't expect TMC-Shapley to behave like that, accuracy would increase with the removal of low-value samples.  If you're trimming high-value examples, then this graph would make sense but would mean influence-based trimming is performing poorly.\n  - Instead of TMC-Shapely and random, it would have been more informative to see how the proposed approach compares with other influence estimation-based approaches, including vanilla (without CART) influence estimation.\n  - The scale for accuracy on some figures in 2 is not intuitive. Is it possible for authors to adopt similar scales for similar utilities across datasets?\n\n\n- Minor: \n\n  - While the focus on convex loss is understandable, it might lead to sub-optimal influence value estimation due to the model parameters not being at a stationary point or the model not converging. This might then be a net negative and misleading data value estimation.\n  - It looks like the authors do one utility at a time. Due to often competing utilities,  for example, key features and samples for fairness might not necessarily be the same for accuracy, and in most cases might have a negative influence. It would be interesting to see an interplay of various utilities. \n  - Although authors use several datasets, all of them are binary settings. Value computation increases with classes, so I am curious to know if this is the reason authors only focused on binary settings or if there is another reason behind this design choice.\n  - The authors' paper was 32 pages instead of 9"
            },
            "questions": {
                "value": "While I think the authors propose an interesting perspective, the presentation of the paper needs some improvement. \nI have raised my main concerns in the weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6213/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6213/Reviewer_MEKL",
                    "ICLR.cc/2024/Conference/Submission6213/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6213/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698792152568,
        "cdate": 1698792152568,
        "tmdate": 1700448751486,
        "mdate": 1700448751486,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9NDSAzvXzd",
        "forum": "HE9eUQlAvo",
        "replyto": "HE9eUQlAvo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6213/Reviewer_MiDf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6213/Reviewer_MiDf"
        ],
        "content": {
            "summary": {
                "value": "This paper utilizes influence functions to assess what data samples improve utility (smaller loss), fairness (DP and EOP), and adversarial robustness for a given convex classifier by interpreting which sample features contribute positively or negatively to certain performance metrics, and design a data selection strategy accordingly."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tConsider many aspects of model performance beyond accuracy; especially the fairness.\n2.\tExperiments are thorough and the presentations of the experimental results are sound and clear."
            },
            "weaknesses": {
                "value": "1.\tLimitation on model class: The authors provide a discussion on why the influence function evaluations are limited to convex classifiers, possible remedies, and recently applications to deep neural networks. However, \n2.\tTheoretical analysis: the estimation of the influence function is based on the trees with hierarchical shrinkage regularization. However, there is no analysis on the credibility, time complexity of the proposed Algorithm 1 and Algorithm 2. It seems that these algorithms are not scalable to large-scale datasets. \n3.\tThe utility, fairness and adversarial robustness are important performance metrics for a classifier; however, there is a lack of a unifying story to connect all three and therefore the discussion and experiments may seem distracted\n4.\tFeature explanation is a key aspect in this paper; however, the connection of feature explanation using the influence function with existing explainable AI literature is lacking."
            },
            "questions": {
                "value": "1.\tShould not the influence estimator has the same architecture of the classifier?\n2.\tFor the fairness experiments in Section 5.1, would the authors justify the choice of the fairness intervention baselines?\nFor other questions, please refer to the Weaknesses. I will consider raising the scores if the authors could adequately address my questions in the rebuttal."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6213/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6213/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6213/Reviewer_MiDf"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6213/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698852806363,
        "cdate": 1698852806363,
        "tmdate": 1700670997158,
        "mdate": 1700670997158,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zjahypgdQW",
        "forum": "HE9eUQlAvo",
        "replyto": "HE9eUQlAvo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6213/Reviewer_GBD9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6213/Reviewer_GBD9"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a new approach to enhancing the performance of classification models by interpreting and selecting training data through influence estimation models. The authors aim to improve model utility, fairness, and robustness by identifying data that positively impacts these aspects. Extensive experiments on various datasets demonstrate the effectiveness of their methods, which are also applicable to scenarios like distribution shifts and fairness attacks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. **Important Research Problem**: The authors have targeted an important research problem that focuses on selecting important training data to improve model performance. The research can improve the effectiveness of machine learning models' development that is often overlooked in favor of more complex model architectures or algorithms.\n\n2. **Thorough Experiments**: The authors have conducted thorough experiments to validate their approaches. The use of both synthetic and real-world datasets ensures that the findings are robust and not limited to specific types of data or scenarios. This comprehensive testing framework strengthens the validity of the research conclusions.\n\n3. **Many Applications**: One of the paper's strengths lies in its application to different scenarios. The authors have not only considered conventional classification tasks but have also extended their methodology to address other challenges such as distribution shifts, fairness poisoning attacks, utility evasion attacks, online learning, and active learning. This broad applicability demonstrates the potential impact of the research on various domains and highlights the versatility of the proposed methods."
            },
            "weaknesses": {
                "value": "1. **Scalability Concerns**: The use of tree-based influence estimation models might indeed pose scalability issues. Tree-based models can become computationally expensive as the size of the dataset increases, especially if the influence estimation requires building trees for many subsets of data or for complex feature interactions. This could limit the method's applicability to big data scenarios or require significant computational resources, which may not always be feasible.\n\n2. **Hard to Adopt Data with High-Dimensional Features**: For example, image data presents unique challenges due to its high dimensionality and the spatial relationships between pixels. Influence functions and feature space interpretations that work well for tabular data may not translate directly to image data."
            },
            "questions": {
                "value": "- How do the tree-based influence estimation models proposed by the authors scale with very large datasets, and what are the computational costs associated with these models?\n- Could the authors provide insights into the computational complexity of their influence estimation approach, and are there any strategies they recommend for scaling it to big data applications?\n- How does the tree model handle high-dimensional data, such as images, where feature interactions are more complex?\n- Could the authors elaborate on any modifications or extensions to their approach that might be necessary to apply it effectively to image data or other high-dimensional datasets?\n- The work presented focuses on convex models or convex surrogates for non-convex models. Could the authors discuss the potential limitations of this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6213/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699209689643,
        "cdate": 1699209689643,
        "tmdate": 1699636677234,
        "mdate": 1699636677234,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bT7FqF6G4N",
        "forum": "HE9eUQlAvo",
        "replyto": "HE9eUQlAvo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6213/Reviewer_bDe3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6213/Reviewer_bDe3"
        ],
        "content": {
            "summary": {
                "value": "This paper presents two schemes for 1) identifying the effect of a training point on a function of a model's parameters, and 2) trimming the training set to stem the influence of training instances that have a high negative influence on the test metric of interest. The paper examines the influence of a training point on a test set fairness metric, adversarial robustness, and utility. The first algorithm fits a regression tree using the input and label to the influence function of a metric of interest. The second algorithm then trims the subset of the training set to improve the model. The paper then demonstrates this approach across a variety of metrics including mitigating the effect of unfairness due to distribution shift, adversarial robustness, and the effect of noisy labels in the streaming setting across several datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Overall, this paper provides a comprehensive empirical demonstration of how to improve a performance metric of interest given training samples and model parameters. \n\n- **The breadth of properties**: This paper considers several interesting scenarios ranging from noisy labels, active learning, adversarial robustness to fairness. The comprehensive nature of these settings is quite impressive and commendable.\n- **Compares to adequate baselines**: The paper considers the key baselines that I would've expected and shows improved performance over these baselines.\n- **Compelling results**: I particularly like Figure 2. Over a range of properties and settings, we see that influence-based deletion approach remains quite effective."
            },
            "weaknesses": {
                "value": "I have a number of confusion about this work that I state here and in the questions section. I would be happy to revise my score in light of feedback from the authors.\n\n- **Details of the approach**: The exact procedure of the trimming portion is not quite clear to me. I think the authors miss discussing retraining. I assume that the authors are referring to a model retrained after a subset of examples are deleted? So in Fig. 2, x axis==zero is a model trained on all data points? Then you trim a percent of the training samples and then retrain a model on the new dataset? If yes, is it the original model that is used for deciding which samples to trim or is the model changing? \n\n- **What is the motivation behind the cart regression procedure?**: As it stands it seems the cart procedure takes as input $(x_i, y_i)$, and the predicts some influence score per example? More details could be useful here. Are the samples used in the training of the cart model a subset of the original training set for which the influence was estimated? Since we know that the influence score measures the effect of up(down)weighting the training sample, alone, we also know that the label should not have any effect on predictive quality of the tree. What is the point of then concatenating the label? It seems like the goal here is to estimate the effect of a feature on the performance metric of interest. I take this judging from Figure 1 where the authors plot performance metric vs features that is colored by influence. If the goal is really to determine the effect of a feature on the performance metric of interest, then how to do that is already in section 2.2 of the original Koh and Liang paper. If the goal is not to measure the effect of the feature on the influence score, then I am not sure I understand the point of this section. Another point here is that in the rest of the paper, the trimming-based approach is really what the authors use and not the cart procedure. If this is the case, I don't think we can that as a contribution of this work. I am asking all these questions as a way to better understand the motivation and goal of fitting the tree to predict the estimated influence score.\n\n- **Related Work**: There is some related work that this paper should be aware of. I list them here: Kong et. al., Resolving Training Biases via Influence-based Data Relabeling, Adebayo et. al. Quantifying and mitigating the impact of label errors on model disparity metrics, Richardson et. al. Add-Remove-or-Relabel: Practitioner-Friendly Bias Mitigation via Influential Fairness, (concurrent) Understanding Unfairness via Training Concept Influence, Sattigerri et. al. Fair infinitesimal jackknife: Mitigating the influence of biased training data points without refitting. All of these papers have a trimming and/or relabelling scheme in them. I am not claiming that this work is not novel/important. I think the insights here are quite useful actually, but it would be helpful for the authors to acknowledge these works, and contextualize their contributions in light of these papers.\n\n- **Tabular Data**: I don't see this as an important weakness; however, most of this work is demonstrated on tabular data. It will be tricky to extend the feature analysis portion, as done in Figure 1 for example, to say images or text."
            },
            "questions": {
                "value": "Please see the first two bullet points in the weaknesses section for a list of the questions that I have. Thanks."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6213/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699631710447,
        "cdate": 1699631710447,
        "tmdate": 1699636677121,
        "mdate": 1699636677121,
        "license": "CC BY 4.0",
        "version": 2
    }
]