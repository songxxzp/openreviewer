[
    {
        "id": "tLxi9lbDmM",
        "forum": "s4WWqhD9Mw",
        "replyto": "s4WWqhD9Mw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4585/Reviewer_cHZ2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4585/Reviewer_cHZ2"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an approach for detected spurious correlations in datasets and transfer editing to transfer the interpretable knowledge to a black box model and improve model accuracy on various tasks. The paper discussed how to detect spurious correlations between concepts in the images and class labels using vision language models such as CLIP. It proposes to train 2 white box models based on frozen CLIP model\u2019s autoencoder backbone and trainable MLP layer the weights of which represent the interpretable importance scores of the concepts (similar to TCAV). One of the white-box models contains the spurious concepts and the other one doesn\u2019t. The differences between 2 white box models are then transferred to the black-box model.\nThe authors conduct multiple experiments to show the effectiveness of their approach."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1) The paper addresses an important problem of identifying and fixing spurious correlations in vision models.\n2) It discusses the challenge of entangled concepts and proposes a technique to improve disentanglement using a baseline/neutral concept such as the concept of others.\n3) The paper performs through experimentation for different types of spurious correlations (Co-occurrence, Picture style and class attribute)."
            },
            "weaknesses": {
                "value": "1) The paper has multiple important contributions but they are a bit intertwined. In some cases it sounds that the authors use the term bias when they refer to spurious correlations. It would be good to make the terminology consistent and clear.\n2) Overall I think that it is a bit hard to follow the paper in terms of understanding the full picture. There are multiple models involved and figure 1 attempts to explain it but it is unclear what bias is and what `compare weights with concept vectors` really means.\n3) It is unclear how the human is involved in the guiding of spurious correlation detection and model fixing. It seems that according to the algorithm listing 1, the output of the algorithm is presented to humans but it is unclear how humans guide the process as the title of the paper suggests. \n4) In Figure 4 it is unclear how we decide to incorporate the spurious example C_cat into the white-box. How is human involved in that process ?\n\nMinor\n\neep learning models -> deep learning models"
            },
            "questions": {
                "value": "1) How scalable is the proposed method  ?\n2) Is accuracy the main metric used in evaluation experiments ?\n3) Why are the experimental results mainly focused on showing the advantage for ensemble models ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4585/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698821364418,
        "cdate": 1698821364418,
        "tmdate": 1699636436444,
        "mdate": 1699636436444,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rzdtIFjy1F",
        "forum": "s4WWqhD9Mw",
        "replyto": "s4WWqhD9Mw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4585/Reviewer_xRBd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4585/Reviewer_xRBd"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces Holmex for detecting and mitigating spurious correlations based on concept vectors. In spurious correlation detection, the method is based on CLIP and makes two contributions: (1) subtracting a background concept vector (Section 4.1.2) and (2) proposing a new algorithm for stable detection of spurious correlation. In spurious correlation mitigation, the paper proposes transfer editing to mitigate spurious correlations in a black box model. The experiments are conducted on multiple datasets and tasks to show Holmex\u2019s performance on spurious correlation detection and mitigation."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "* The paper studies an important problem.\n* The code is released for better reproducibility."
            },
            "weaknesses": {
                "value": "## Concerns about the method\n\n### Subtracting background concept vector (Section 4.1.2) \nFirst, I am confused by the motivation of this part of the method. I understand the argument that irrelevant concepts (e.g., cat and airplane) have high cosine similarities. However, I am completely lost for the \u201cmodel editing experiment\u201d where \u201ca linear layer after the concept activation layer\u201d was trained. Such as the model editing experiment was not introduced before and the details are completely left to Appendix A.1, which was not clear to me either. Second, I wonder why not a simple alternative solution would not suffice. Following the equation on the bottom of Page 4, we can compute \n$\nP(y = c \\mid z) = \\frac{\\exp(t_c^\\top z / T) }{\\sum_{c' \\in C} \\exp(t^\\top_{c'} z / T )}\n$\n, where $T$ is temperature in softmax, $c$ is one concept, and $\\mathcal{C}$ is the set of all concepts. You can choose a low temperature to reduce the similarity among different concepts.\n\n### Transfer difference of logits (step 2 in Section 5.1, page 7)\nDifferent models (i.e., white-box and black-box) can have different scales in logits. Although the paper has a discussion of \u201cThe scale of logits\u201d on page 7, my question is still not answered. \n\n## Concerns about the experiments\n\n### Datasets and Metrics\nI appreciate the authors' efforts in doing experiments for three types of biases. However, I don\u2019t think the paper explains the motivation for creating new evaluation settings and metrics. There are many previous benchmarks and evaluation settings for both (1) spurious correlation detection ([1,2] and (Wu et al., 2023)) and (2) bias mitigation benchmarks ([3-6]).\n\n### Comparison Methods\nThe proposed method is only compared with a limited number of methods. For spurious correlation detection, the paper is only compared with the PCBM method and its variants. Why not compare with DISC (Wu et al., 2023), which is also a concept-based method? For spurious correlation mitigation, many methods, especially methods that do not rely on concept vectors [1,2,7-9], are not compared.\n\n\n## References\n\n[1] Sabri Eyuboglu, Maya Varma, Khaled Kamal Saab, Jean-Benoit Delbrouck, Christopher Lee-Messer, Jared Dunnmon, James Zou, and Christopher Re, \u201cDomino: Discovering Systematic Errors with Cross-Modal Embeddings,\u201d in ICLR, 2022.\n\n[2] Gregory Plumb, Nari Johnson, Angel Cabrera, and Ameet Talwalkar, \u201cTowards a More Rigorous Science of Blindspot Discovery in Image Classification Models,\u201d TMLR, 2023.\n\n[3] Nanyang Ye, Kaican Li, Haoyue Bai, Runpeng Yu, Lanqing Hong, Fengwei Zhou, Zhenguo Li, and Jun Zhu, \u201cOoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization,\u201d in CVPR, 2022.\n\n[4] Shiori Sagawa*, Pang Wei Koh*, Tatsunori B. Hashimoto, and Percy Liang, \u201cDistributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Case Generalization,\u201d in ICLR, 2020.\n\n[5] Zhiheng Li, Ivan Evtimov, Albert Gordo, Caner Hazirbas, Tal Hassner, Cristian Canton Ferrer, Chenliang Xu, and Mark Ibrahim, \u201cA Whac-A-Mole Dilemma: Shortcuts Come in Multiples Where Mitigating One Amplifies Others,\u201d in CVPR, 2023.\n\n[6] Robik Shrestha, Kushal Kafle, and Christopher Kanan, \u201cAn Investigation of Critical Issues in Bias Mitigation Techniques,\u201d in WACV, 2022.\n\n[7] Junhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and Jinwoo Shin, \u201cLearning from Failure: Training Debiased Classi\ufb01er from Biased Classi\ufb01er,\u201d in NeurIPS, 2020.\n\n[8] Evan Z. Liu, Behzad Haghgoo, Annie S. Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa, Percy Liang, and Chelsea Finn, \u201cJust Train Twice: Improving Group Robustness without Training Group Information,\u201d in ICML, 2021.\n\n[9] Elliot Creager, Joern-Henrik Jacobsen, and Richard Zemel, \u201cEnvironment Inference for Invariant Learning,\u201d in ICML, 2021."
            },
            "questions": {
                "value": "I expect the authors to address my concerns in the response:\n\n1. Why not use software with temperature to address the problem of irrelevant concepts with high similarity?\n2. Do you assume that white-box and black-box models share a similar logit scale? If so, this approach is not generalizable enough to claim the \u201cblack-box model fixing.\u201d\n3. Why create new evaluation settings with new metrics?\n4. Add experiments to compare with a broader range of methods."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4585/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698822384768,
        "cdate": 1698822384768,
        "tmdate": 1699636436338,
        "mdate": 1699636436338,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OGKayoAPyR",
        "forum": "s4WWqhD9Mw",
        "replyto": "s4WWqhD9Mw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4585/Reviewer_NL8r"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4585/Reviewer_NL8r"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces Holmex, a method designed for human-guided spurious correlation detection and black-box model fixing. It enables humans in the deep model debugging process by addressing two main tasks:\n\nDetecting Spurious Correlations: Holmex uses pre-trained vision-language models to create separable vectors representing high-level and meaningful concepts. It proposes a novel algorithm based on these concept vectors to detect conceptual spurious correlations in training data, and this algorithm is more stable than previous methods.\n\nFixing Biased Black-Box Models: Unlike prior approaches that focus on making biased models interpretable and editable, Holmex is compatible with arbitrary black-box models. It introduces a novel technique called \"transfer editing\" to transfer revisions made in interpretable models to correct spurious correlations in black-box models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The strengths of the paper are as follows:\n\n1. Improved Concept Embeddings: The paper enhances the quality of concept embeddings by reducing the entanglement of raw text embeddings. It achieves this by subtracting a vector of the background word, which is a useful contribution. This improvement is crucial for accurate detection of spurious correlations.\n\n2. Novel Detecting Algorithm: The paper introduces a novel detecting algorithm that is specifically designed to reveal correlations between concepts and labels in a stable manner. This algorithm enhances the reliability and stability of the spurious correlation detection process.\n\n3. Transfer Editing Technique: The paper proposes a transfer editing technique, which is a novel method for transferring revisions made by humans in white-box models to black-box models. This approach enables the fixing of spurious correlations in black-box models, making it a versatile and impactful contribution.\n\nThe paper conducts extensive experiments on multiple datasets with different types of biases, including co-occurrence bias, picture style bias, and class attribute bias. This demonstrates the effectiveness and applicability of the Holmex method across a range of real-world scenarios, which is a significant strength in showcasing its practical utility."
            },
            "weaknesses": {
                "value": "The paper does not cite several works in this domain. Some of the missing citations are:\n\n1. Salient ImageNet: How to detect spurious correlations in deep learning? ICLR 2022.\n2. Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations. ICLR 2023.\n3. Wilds: A benchmark of in-the-wild distribution shifts. PMLR, 2021.\n\nSalient ImageNet provides a scalable methodology for identifying spurious correlations at scale. The paper does not include any comparison with that method, rather no citation is provided. Similarly, the latter paper provides a method for robustifying against spurious correlations. Again, no citation provided. \n\nThis provides a strong evidence that the paper is written without a thorough research of the prior work."
            },
            "questions": {
                "value": "There is no comparison against several of the group robustness methods presented in the prior works. Given the extent of the literature on robustness against spurious correlations, results comparing the accuracy of the proposed method against the baseline are not acceptable."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4585/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698871516793,
        "cdate": 1698871516793,
        "tmdate": 1699636436258,
        "mdate": 1699636436258,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dOfCvMDfUs",
        "forum": "s4WWqhD9Mw",
        "replyto": "s4WWqhD9Mw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4585/Reviewer_G5F3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4585/Reviewer_G5F3"
        ],
        "content": {
            "summary": {
                "value": "This paper presents HOLMEX, a method to identify a model's reliance on spurious correlations, and fix it. The general idea is to construct concept vectors using a large pre-trained model like CLIP, and then surface the correlation between a concept vector and a label to a human. The human can then use their inductive bias/domain knowledge to trim correlations that happen to be spurious. Once the spurious concept and label tuple is detected, the authors then propose a transfer technique to edit the model. In the transfer editing technique, you train two 'white-box' models: 1) where the spurious concept has been removed, and 2) where the spurious concept is present. These whitebox concepts are basically softmax linear layers on top on the clip representations for the input samples. The hope here is that the weights of these two whitebox models capture the spurious direction. To perform transfer editing, you take a difference between the logits of the two whitebox models, and add that to the logits of the blackbox model. They couple this approach with ensembling and show that such an approach leads to improved model performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Overall, this papers sets out an important problem and presents a scheme for addressing that problem. I list below some nice aspects of this work: \n\n- **Modular Approach**: The paper separates detecting spurious correlation from fixing it.\n- **Clear scheme**: The paper describes its scheme very clearly, and tries to justify each step of the scheme.\n- **Demonstrates performance improvement**: The paper also shows that the transfer editing schemes and ensembling leads to improved performance across the board across all the tasks tested.\n- **Control experiment**: I liked that the authors included a control experiment for a model with no spurious signals. The approach shows the kind of null behavior you would want in that setting."
            },
            "weaknesses": {
                "value": "Below I discuss some of the weaknesses of the scheme presented here. \n\n- **Too many moving parts**: While the scheme presented is modular. As it stands, there are several decisions that need to be gotten right for the overall scheme to work. Here is what I mean: 1) it looks like the traditional concept vectors (derived from model embeddings) are ineffective, so we need a modified version, 2) One needs a background word, 3) One needs to train a linear classifier to estimate correlations, 4) One needs to train two separate linear classifiers again to do editing for each spurious concept that you want to remove. This means that if you have 20 concepts to remove, then you would be training 40 linear classifiers to remove the effect of these 20 spurious concepts for that label alone. If any of the steps that I have listed does not work, then the entire scheme does not work. \n\n- **Over reliance on CLIP**: I think the dependence on CLIP in this work is quite worrisome. I think the CLIP embeddings are effective probably because the CLIP dataset is quite large, so those embeddings don't suffer from the issues the authors noticed. For example, imagine that you wanted to now fix a model that solely relies on CLIP embeddings as its classifier, then I assume the approach here would be ineffective? \n\n- **Logit Correction in Transfer Editing**: I am surprised that the editing scheme here works since we can simply think of this as shifting the distribution of the logits. However, it requires that the output space for the black-box models be the same size as that of the model you want to edit."
            },
            "questions": {
                "value": "Here are some questions for the authors:\n- How do you think the transfer editing approach here relates to the task vectors approach? See: Editing models with Task Arithmetic, and Task Arithmetic in the Tangent Space. It seems like you could avoid training two white-box models by adopting the task arithmetic editing approaches in the above papers.\n\n- What is the justification for why the concept vectors from raw embeddings does not work? What if I have a model that just uses the clip embedding itself for classification, but the clip embedding has spurious correlations too? Is this approach just inheriting the limitations of clip representations? What about if I have satellite images or a setting where CLIP is not useful?\n\n- Ensembling: Did you test ensembling alone in Table 3? It would be interesting to see whether simply ensembling recovers the performance gains that you see in that table. I ask this because ensembling has been shown to give OOD benefits."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4585/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699294427129,
        "cdate": 1699294427129,
        "tmdate": 1699636436183,
        "mdate": 1699636436183,
        "license": "CC BY 4.0",
        "version": 2
    }
]