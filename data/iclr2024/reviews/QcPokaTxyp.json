[
    {
        "id": "BWqHgQTvwp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1625/Reviewer_ooAw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1625/Reviewer_ooAw"
        ],
        "forum": "QcPokaTxyp",
        "replyto": "QcPokaTxyp",
        "content": {
            "summary": {
                "value": "The study investigates the effect of visual grounding upon language understanding of language models. For this objective, the authors perform the training of vision-language models in three settings, i.e. language-only, visual-language, and visual-word. Experiments show that visual grounding can aid language learning but mostly in the low-data regime."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper has the following strengths:\n\n- The motivation is clearly stated and makes intuitive sense.\n\n- The experiments are conducted thoroughly with various benchmarks, datasets, and prototype models.\n\n- The obtained insights are strongly proven by the experiment results."
            },
            "weaknesses": {
                "value": "There are some details which can be raised from the paper:\n\n- Even though the authors provide various evidence to substantiate their claim about the visual grounding ability to help language models, such claim is opposite from the findings attained by previous research of visual grounding [1,2,3] to certain extent. The paper lacks a discussion towards these research.\n\n- Because language models haven proven their effectiveness in multiple applications nowadays, the contribution would become more appealing if the paper discusses what is the impact of its observation upon training language models.\n\n[1] Retrieve, Caption, Generate: Visual Grounding for Enhancing Commonsense in Text Generation Models, AAAI 2022.\n\n[2] Language Adaptive Weight Generation for Multi-task Visual Grounding, CVPR 2023. \n\n[3] Visual Grounding in Video for Unsupervised Word Translation, CVPR 2020.\n\n[4] Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision, EMNLP 2020."
            },
            "questions": {
                "value": "- Why is the capacity of visual grounding for language understanding differently found from previous works?\n\n- What benefit does the insight that visual grounding is beneficial for language understanding in low-data settings could provide for training language models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1625/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1625/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1625/Reviewer_ooAw"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1625/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697276188119,
        "cdate": 1697276188119,
        "tmdate": 1699636091114,
        "mdate": 1699636091114,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qqoxmqlvLx",
        "forum": "QcPokaTxyp",
        "replyto": "QcPokaTxyp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1625/Reviewer_XApH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1625/Reviewer_XApH"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the influence of visual information on word meaning learning of language models. \n\nThe authors train CLIP, GIT, and GPT-2 models in a controlled way and examine the word learning results on various benchmarks such as word similarity and PoS tagging. \n\nThe experimental findings suggest that visual signals bring slight improvements to word learning, especially under low resource regimes. Interestingly, CLIP + Word predictions and language model predictions correlate poorly, indicating that contrastive learning captures a different pattern for word distribution. GIT performs similarly with pure language model, suggesting that it mainly learns from the cross-word distribution, struggling to balance between vision and text.\n\nFurther analysis shows that grounded learning relates to concrete words more like humans than abstract words. Additional results on vision encoder variants, Flamingo architecture, and sentence processing tasks further validate the main results of previous experiments."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Investigating the effect of vision signals on language acquisition is an exciting topic, and the findings of this paper deepen our understanding of vision-aided language modeling. \n\n- The experimental setup and results are comprehensive, with different types of models trained using the controlled network architecture and dataset, various benchmarks are adopted for evaluation and different angles are analyzed."
            },
            "weaknesses": {
                "value": "- The findings are weak in terms of practicality, as they cannot be directly translated into improvements for existing models. \n- (Minor) Some experimental details are missing. See my questions (Q2 and Q3) below. \n- (Minor) The results bullets are somewhat difficult to follow. I suggest the authors organize these findings into a more natural story to improve the reading experience."
            },
            "questions": {
                "value": "Q1: What are the implications of these findings for future research? I am not sure if the better concrete word modeling ability in low-resource regimes would be a bonus when we have abundant image-text pairs or single modality data to train models.  I think some previous explorations such as Vokenization [1], Initialization and plug-in fine-tuning [2] and Distillation [3] can be investigated (or discussed accordingly) with the findings in this paper.\n\nQ2: How was the Flamingo model trained? Did you use the same 6-layer Transformer on the CC12M dataset? This seems not possible as it requires a special network for integrating vision information with cross-attention and an interleaved image-text dataset. \n\nQ3: What representation of Visual + language models (CLIP) was used for word-based tasks? Did you extract the corresponding word representation from the full sentence to perform the downstream tasks?\n\n[1] Vokenization: Improving language understanding with contextualized, visual-grounded supervision, Tan et al,  https://arxiv.org/abs/2010.06775\n\n[2] How much can clip benefit vision-and-language tasks?, Shen et al,  https://arxiv.org/abs/2107.06383\n\n[3] VIDLANKD: Improving Language Understanding via Video-Distilled Knowledge Transfer, Tang et al,  https://arxiv.org/abs/2107.02681\n\n[4] Can Language Models Understand Physical Concepts? Li et al, https://arxiv.org/abs/2305.14057"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1625/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1625/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1625/Reviewer_XApH"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1625/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698670669667,
        "cdate": 1698670669667,
        "tmdate": 1699636091031,
        "mdate": 1699636091031,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "u32ATzU8uy",
        "forum": "QcPokaTxyp",
        "replyto": "QcPokaTxyp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1625/Reviewer_ZHy9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1625/Reviewer_ZHy9"
        ],
        "content": {
            "summary": {
                "value": "This paper presents an analysis of the word learning / understanding capabilities of visually grounded models. The paper is motivated by human word/language learning which generally requires much smaller magnitudes of text data than language models, and thus investigates what differences lie in the word-learning dynamics/capabilities of models under different regimes of exposure to text and visual data (in the form of images) under a variety of evaluations pertaining to word learning. \n\nModels evaluated have the following key properties: \n* For language-only models a variant of GPT-2 is used. \n* For visually grounded models, the two main families of models evaluated are CLIP and GIT, however Flamingo is also evaluated in one experiment. \n* Models are also evaluated under varying sizes of text input windows, i.e. at the single-word level, at small contexts around target words, as well as at the full sentence level.\n\nTo assess word learning capability , a variety of evaluations are used: \n* Word similarity -- how model similarity between words correlates with human similarity judgements. \n* Lexical relation prediction -- training linear probes over model features to predict lexical relations such as synonymy or antonymy. \n* Semantic feature prediction -- linear regression over model features to predict strengths of different features of words. \n*  Part of speech prediction -- predicting part of speech tags from SVMs trained on top of model features. \n* Context-based word-understanding benchmark -- a new benchmark presented with the paper, in which models are tasked with ascribing higher probability to correct contexts over perturbed distractors. \n* Brain-response prediction -- linear regression over model features to predict brain response features to input text. \n\nThe paper finds that for a majority of experimental conditions, the multimodal models are generally either worse or comparable to the language-only model. Other findings include: \n* A more fine-grained analysis in human ranking correlation controlled by word features (e.g. prevalence) finds that multimodal models perform better than language models on concrete words, which appears intuitive. \n* Incorporating greater amounts of language context negatively impacts some multimodal model performance. \n* Fine-tuning visual encoders improves performance. \n* Models trained with smaller amounts of data benefit more from multimodality."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper is compellingly motivated, and the research direction of understanding the word-learning capabilities of multimodal models is an important area that may help inform the development of future models. \n\n* The paper presents an extensive set of experiments across a wide variety of experimental conditions for analyzing word-learning capabilities of multimodal models. \n\n* The paper does a good job of summarizing its findings/conclusions gleaned from the experimental results, I found the summaries in the subsection titles and conclusions at the end of each subsection helpful in digesting the results."
            },
            "weaknesses": {
                "value": "* I believe this is a minor weakness, but I was left wondering what experimental results would be like across a wider variety of model families /variants. I was surprised to see the relatively lower performance of Flamingo, and wonder how models such as InstructBlip, LLaVA, and CM3 would perform."
            },
            "questions": {
                "value": "* For experiments evaluating similarities I'm wondering if any metrics other than the cosine similarity was used? I ask because as I understand it CLIP was trained explicitly to maximize the cosine similarity between matching image/text pairs, so it seems intuitive to me that its similarity judgement capability would be well evaluated under that metric. However, I'm not sure if this is necessarily true of other models, and I wonder if the observed trends might be any different under other metrics such as L2."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1625/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1625/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1625/Reviewer_ZHy9"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1625/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698813854513,
        "cdate": 1698813854513,
        "tmdate": 1700721604755,
        "mdate": 1700721604755,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "x8bK7zfckU",
        "forum": "QcPokaTxyp",
        "replyto": "QcPokaTxyp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1625/Reviewer_QLPJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1625/Reviewer_QLPJ"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the problem of visually grounded word learning, and arrives at the main conclusion that visual grounding mainly help acquire word meanings in low-data regimes. The performance of word acquisition are measured by calculating the similarity between model prediction and human annotations. Other side findings are also presented in the paper."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Comprehensive evaluation on a wide range of tasks.\n- While some figures are hard to read, the paper is generally well-written."
            },
            "weaknesses": {
                "value": "- Motivation of the work: the main metric is the similarity between model prediction and human judgments, but there are at least two steps that may result in information loss, and I'm not sure how reliable the conclusions are with the presented approaches:\n    1. The authors used the cosine similarity between word representations to measure how the models acquire words, but other useful information that affects model preference may be encoded in the deeper architectures.\n    2. There may be disagreement between human annotators. For example, Brysbaert et al. (2014) have the word *can* labelled highly concrete, which can cause quite high disagreement among people.\n- Plausibility in terms of data exposure: while [*these (text-only) models are profoundly implausible as models of human cognitive development*] (Page 1), isn't the finetuning CLIP approach similarly implausible? Humans do not pre-train their visual and textual understanding systems on large parallel data; instead, human acquisition of words arguably happens in an incremental way.\n- Arbitrary definition of *human-likeliness* (Appendix A1.2). The evaluation metrics, while reflecting the model acquisition of word meanings to some extent and from certain perspectives, do not necessarily reflect the model's ability to learn words. \n- The line plots with multiple lines (Figs. 1B, 2B) are largely imperceptible. It would be better to use different line styles."
            },
            "questions": {
                "value": "- For CLIP-based settings, did you use a pretrained CLIP model or use the model architecture/objective with random initialization to train from scratch? If the former, isn't it exposed to many more image-caption pairs than your training data? If the latter, I'd be surprised that 4.3K pairs can lead to a decent performance and would meanwhile suggest the authors rename their models---CLIP is usually used to refer to the pretrained CLIP model.\n- Why did you specifically pick CLIP, Flamingo and GIT as the model? There are several models, such as [Kiros et al. (2014)](https://arxiv.org/abs/1411.2539) and its variants, working on learning visually grounded word and sentence representations. In terms of performance on image-caption retrieval or generation, they might not be competitive with recent work, but the task of this paper is to investigate the acquisition of word meanings, and there's no reason to stick to the recent popular models.\n- (minor) In the first sentence, you probably wish to use *NLM* instead of *LLM* as the abbreviation of *neural language models*?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1625/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699210191100,
        "cdate": 1699210191100,
        "tmdate": 1699636090879,
        "mdate": 1699636090879,
        "license": "CC BY 4.0",
        "version": 2
    }
]