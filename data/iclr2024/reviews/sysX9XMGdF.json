[
    {
        "id": "Uf0QhLPWiC",
        "forum": "sysX9XMGdF",
        "replyto": "sysX9XMGdF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5926/Reviewer_V15d"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5926/Reviewer_V15d"
        ],
        "content": {
            "summary": {
                "value": "1. The paper identifies a limitation in Message Passing Neural Networks (MPNNs) related to the handling of node attributes in graph datasets, specifically in how attributes lose significance during attribute transformation and feature aggregation stages.\n2. Even single-layer MPNNs weaken node representations by combining attributes equally, leading to a phenomenon called over-dilution, where node features become diluted and less informative.\n3. To address this issue, the paper introduces Node Attribute Transformer (NATR), a transformer designed to operate on node attributes, producing more informative node representations and mitigating the problem of over-dilution in MPNNs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Figures 1 and 4 provide clear visual distinctions between the over-dilution phenomenon and existing phenomena of over-smoothing and over-squashing.\n2. Over-dilution provides an original perspective, broadening the scope of MPNN limitations.\n3. The phenomenon is assessed by dividing it into two sub-phenomena: intra- and inter-node dilution, along with the introduction of corresponding factors.\n4. NATR is incorporated with existing MPNNs to demonstrably counteract the effects of over-dilution on node classification and link prediction."
            },
            "weaknesses": {
                "value": "1. The significance of the paper can be strengthened by exploring graph datasets, at least synthetic data, and ideally real-world examples, exhibiting over-dilution in deep layers without the presence of other phenomena, e.g., over-smoothing.\n2. Over-dilution is detected in the very first layer of MPNNs, distinguishing it from other phenomena, but there are no convincing real-world experiments to support the notable implications of this single-layer over-dilution.\n3. Over-correlation, documented in existing studies [Liu et al., 2023, Jin et al., 2022], aligns with over-dilution in the realm of *preservation of attribute-level information*, necessitating a comprehensive discussion to discern their nuanced differences.\n\nReferences\n* [Liu et al., 2023]: Enhancing Graph Representations Learning with Decorrelated Propagation, KDD'23\n* [Jin et al., 2022]: Feature Overcorrelation in Deep Graph Neural Networks: A New Perspective, KDD'22"
            },
            "questions": {
                "value": "1. What were the criteria for selecting the five real-world graph datasets, shown in Table 7, for studying over-dilution? \n2. Related to the previous question, is it the case that MPNNs were more susceptible to over-dilution on these datasets than other existing datasets?\n3. What characteristics were considered when choosing these datasets to ensure they accurately represent over-dilution without the interference of other phenomena, e.g., over-smoothing, over-correlation?\n4. How was it ensured that the superior performance achieved by NATR in settings with 4 or 5 layers, as demonstrated in Table 3, is solely attributed to reduced over-dilution and not a result of significantly mitigating *possibly more severe phenomena* such as over-smoothing, over-correlation?\n5. Were there insights into potential real-world scenarios or applications where single-layer over-dilution could have significant consequences?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5926/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5926/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5926/Reviewer_V15d"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5926/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697639080312,
        "cdate": 1697639080312,
        "tmdate": 1700742093891,
        "mdate": 1700742093891,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OuCRIork2G",
        "forum": "sysX9XMGdF",
        "replyto": "sysX9XMGdF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5926/Reviewer_LoKp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5926/Reviewer_LoKp"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a new limitation of Message Passing Neural Networks (MPNNs) (i.e., over-dilution). It shows two types of dilutions: intra-node dilution and inter-node dilution considering 1) the equal weight combination for attributes within each node, 2) the information from neighbors is diluted through aggregation. The authors also provide formal definitions of these concepts. To mitigate the problem, they propose a transformer-based method (NATR) by considering adaptively merging attribute representations. The experiments are conducted for link prediction and node classification tasks, showing the better performance of NATR."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The motivation to adaptively utilize attributes for each node is sound. \n2. The analysis about dilution factors and the formal definitions have some merits. \n3. The improvements in some datasets are impressive."
            },
            "weaknesses": {
                "value": "1. While the authors conduct the experiments on both link prediction and node classification, they only use three datasets (i.e., computers, photo, and cora ML) for node classification. OGB datasets for node classification are not included. I would like to see some results on ogbn-arxiv or ogbn-product. Even if the model may not perform well on these datasets, I suggest the author provide some analyses or insights about what kind of datasets would benefit more by using the proposed model.\n2. To me, it's not very clear for some parts of the analysis (e.g., Sec 6.2). In Sec 6.2, the author investigates the performance for nodes with bottom 25% and top 25% of inter-dilution scores. But for the base model and the version with NATR, the formula of $\\delta^{inter}_{Agg}(v) $ should be different? For NATR, it uses Eq (11). For GCN, it uses Eq (7). So, I wonder if the nodes are separated by only considering the original inter-dilution factor (i.e., Eq (7))?\n3. The proposed method is claimed to solve both intra-dilution and inter-dilution. For intra-node dilution, the method can assign larger weights to more important attributes. However, it is unclear to me how the method addresses the inter-node dilution. I would suggest the author elaborate more on this part."
            },
            "questions": {
                "value": "1. Based on my understanding, in the proposed method, each attribute has its own learnable representations. In this case, the number of learnable parameters will increase compared with previous MPNN baselines (e.g., GCN). I wonder whether the performance improvement is mainly from the increased number of learnable parameters. I would like to see some analyses in this regard.\n2. I am curious how the performance would change with different numbers of attributes in a graph. Is there any trend for this? \n3. Considering the motivation of avoiding the combination of attributes with different weights, I wonder if feature selection methods can help to alleviate this problem."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5926/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5926/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5926/Reviewer_LoKp"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5926/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698566171870,
        "cdate": 1698566171870,
        "tmdate": 1699636630472,
        "mdate": 1699636630472,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VgNktWMXQy",
        "forum": "sysX9XMGdF",
        "replyto": "sysX9XMGdF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5926/Reviewer_HxeZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5926/Reviewer_HxeZ"
        ],
        "content": {
            "summary": {
                "value": "The paper discusses a recent challenge in the field of Graph Neural Networks (GNNs), particularly focusing on Message Passing Neural Networks (MPNNs), and introduces the issue of \"over-dilution\" where node attribute information is diminished in the final representation due to excessive aggregation from many attributes (intra-node dilution) or overwhelming information from neighboring nodes (inter-node dilution). The authors propose a novel transformer-based architecture that treats attribute representations as tokens, which, unlike being a replacement, is an augmentation to existing MPNNs. This model aims to preserve attribute-level information more effectively by using attention scores to weigh attribute representations in the context of the aggregated node-level representation. The paper claims to contribute a new perspective on the problem of over-dilution by defining and analyzing it, which is distinct from the commonly discussed limitations of MPNNs such as over-smoothing, over-squashing, and over-correlation. The proposed transformer-based solution is theoretically and empirically validated for its efficiency in maintaining attribute-level information within graph-structured data representations."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The paper introduces the concept of over-dilution, a novel perspective in the study of GNNs, particularly MPNNs, that goes beyond the well-studied limitations of over-smoothing, over-squashing.\n* The proposed transformer-based architecture is not only theoretically grounded but also empirically tested, providing a strong case for its effectiveness in combating the over-dilution problem. This dual approach enhances the credibility of the findings."
            },
            "weaknesses": {
                "value": "* Experiments are not complete.\n* The story of this paper is weird. I don't know why the author include over-smoothing and over-squashing as a story and don't do any comparison between over-dilution and them."
            },
            "questions": {
                "value": "* For baselines, I think GCNII can be moved into the main paper and can you do it on all datasets? Because GCNII can alleviate over-smoothing, which I think maybe relevant to the paper.\n* Also, How is GCNII experiments done? Have you tried hyperparameter searching on it?\n* For datasets, even the authors state that the complexity is acceptable, the datasets the paper used are all small datasets. Can you provide results and time comparison with backbone on some larger datasets? like ogb-arxiv or ogb-citation2(maybe too large, ogb-ppa can also be a good choice).\n* Can the authors provide number of parameters of each model with backbone? How to know the improvement is not the result of adding new parameters in the transformer?\n* Can the authors provide details on what's the relationship between over-smoothing,over-squashing and over-dilution? Theoretically and empirically?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5926/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5926/Reviewer_HxeZ",
                    "ICLR.cc/2024/Conference/Submission5926/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5926/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699111658455,
        "cdate": 1699111658455,
        "tmdate": 1700565389203,
        "mdate": 1700565389203,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1ujmYONdIH",
        "forum": "sysX9XMGdF",
        "replyto": "sysX9XMGdF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5926/Reviewer_gjvj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5926/Reviewer_gjvj"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to study a new pheonomenon named over-dilution in message passing neural networks (MPNNs). It refers to the diminishing importance of a node's information in the final node representations learned by the neural networks. The authors propose NATR to address the proposed over-dilution problem. The key idea is to learn an attribute encoder and then train another transformer-based attribute decoder where Q is the node embeddings from MPNNs and K, V are the embeddings output by the attribute encoder."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "S1. Interesting new perspective to study the limitation of MPNNs.\n\nS2. Improved performance in tasks like link prediction and node classification."
            },
            "weaknesses": {
                "value": "Please see questions below."
            },
            "questions": {
                "value": "Q1. I am confused by Eq. (3). Isn't $z_t$ the same as the $t$-th value in $h_v^{(0)}$? Why is it $h_v^{(0)}$ rather than $h_v^{(k)}$ in some hidden layer $k$?\n\nQ2. Still about Eq. (3): this essentially measure some normalized correlation between one attribute and another attribute, i.e., how a infinitesimal perturbation on attribute $t$ would affect other attributes in node features $h_v^{(0)}$. It would be good to discuss the its connection to overcorrelation by Jin et al.\n\nQ3. Definition 3.2 is the same as Xu et al., so it is necessary to cite it in Definition 3.2.\n\nQ4. Hypothesis 2 seems related to degree fairness learned in several papers [1, 2, 3]. When the node degree is high, after normalization, the aggregation weight $\\alpha_{v, v}$ will be smaller than the sum of all other edge weights. It would be good to discussion some intrinsic connection to this line of work.\n\nQ5. Hypothesis 3 seems to be very related to over-squashing by Topping et al. It would be good to have more in-depth discussion on the difference between Hypothesis 3 and over-squashing.\n\nQ6. To me, it feels that NATR would help when the number of layers increases. But it seems the MPNNs used in experiments are pretty shallow. What would happen if we increase the layers to a larger number? How would NATR perform if we equip it with deep graph neural networks like RevGCN [4]? \n\nQ7. The over-dilution seems like some combination of feature correlation (Definition 3.1) and over-squashing (Definition 3.2, Hypothesis 3) to me. It would be better to discuss the difference between the over-dilution and these two scenarios.\n\n\n**References**\n\n[1] Tang, Xianfeng, et al. \"Investigating and mitigating degree-related biases in graph convoltuional networks.\" Proceedings of the 29th ACM International Conference on Information & Knowledge Management. 2020.\n\n[2] Kang, Jian, et al. \"Rawlsgcn: Towards rawlsian difference principle on graph convolutional network.\" Proceedings of the ACM Web Conference 2022. 2022.\n\n[3] Liu, Zemin, Trung-Kien Nguyen, and Yuan Fang. \"On Generalized Degree Fairness in Graph Neural Networks.\" arXiv preprint arXiv:2302.03881 (2023).\n\n[4] Li, Guohao, et al. \"Training graph neural networks with 1000 layers.\" International conference on machine learning. PMLR, 2021."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5926/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5926/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5926/Reviewer_gjvj"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5926/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699608082398,
        "cdate": 1699608082398,
        "tmdate": 1700717197223,
        "mdate": 1700717197223,
        "license": "CC BY 4.0",
        "version": 2
    }
]