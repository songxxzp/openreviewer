[
    {
        "id": "t4b5zd13vT",
        "forum": "381QSrWdF2",
        "replyto": "381QSrWdF2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1709/Reviewer_8S7u"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1709/Reviewer_8S7u"
        ],
        "content": {
            "summary": {
                "value": "This work considers (mini-batch) SGD on loss functions with re-scaling symmetry. This work shows that SGD tends to find a solution with balanced weights. Furthermore, this work also analyzes the stationary distribution induced by a continuously approximated SGD for a diagonal linear network, where several complicated behaviors of SGD are discussed, such as phase transition, loss of ergodicity, and fluctuation inversion."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The focus of this work, that is, the behavior of SGD, is an important and relevant topic to the community. \n+ Some of the findings on the differences between SGD and (noisy) GD might be interesting.\n+ The calculation of the stationary distribution of SGD for diagonal linear networks should be new to my knowledge."
            },
            "weaknesses": {
                "value": "- Some statements/writing are not precise and might be misleading. See more in the Question section. \n- Some messages are already known from prior papers. For example, it is known (references are actually mentioned in this work) that SGD should not be approximated by gradient flow or gradient Langevin dynamic and that the SGD noise is parameter dependent. This paper should have been more careful in terms of clarifying the contributions. \n- Not sure how relevant is the theory in this paper to practice. See more in the Question section."
            },
            "questions": {
                "value": "1. Discussions before Theorem 1. \"....For example, it appears in any neural network with the ReLU activation\". This might not be true. For example, $(u \\\\max \\\\{ w x, 0 \\\\} -y)\\^2$ is rescaling symmetric only for non-negative $\\\\lambda$. \n\n2. Theorem 1 and the follow-up discussions. Note that $C\\_1 $ and $C\\_2$ are functions of $u$ and $v$. Therefore, $\\\\lambda\\_{1m}, \\\\lambda\\_{1M}, \\\\lambda\\_{2m}, \\\\lambda\\_{2M}$ are not constants but functions of $u$ and $v$. Hence, eqs (5) or (6) do not directly imply that $u$ and $v$ are approximately balanced. Could the authors make some clarifications? \n\n3. Discussions after eq (8). There seems to be a typo in the display after \"....can be upper-bounded by an exponentially\ndecreasing function in time:...\". Please clarify. \n\nIn addition, in eq(7), the coefficient $\\\\alpha\\_1 v\\^2 - 2\\\\alpha\\_2 v  + \\\\alpha\\_3$ is not a constant but a function of $v$ (hence a function of $u$ and $w$). So the statement that \"....can be upper-bounded by an exponentially decreasing function in time\" might not be accurate. \n\n4. Discussions after eq (9). There might be a typo in the definition of $C(v)$. It should be the variance of the gradient of the loss. \n\n5. \"These relations imply that $C$ can be quite independent of $L$, contrary to popular beliefs in the literature...\". Here, should the word \"independent\" be revised to \"dependent\"? \n\n6. The diagonal linear network studied in this paper takes a 1-dimnensional input. However, prior works consider diagonal linear network with a multivariate input. So the setting in this work might be limited compared to prior works. \n\nOverall, I feel the theory in this work is not entirely precise. Plus, the model setup is limited, and the results might not be general and might be strongly rely on this particular setup."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1709/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1709/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1709/Reviewer_8S7u"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1709/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698436961798,
        "cdate": 1698436961798,
        "tmdate": 1699636099557,
        "mdate": 1699636099557,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Y4N8pcQUBY",
        "forum": "381QSrWdF2",
        "replyto": "381QSrWdF2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1709/Reviewer_Y3Mm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1709/Reviewer_Y3Mm"
        ],
        "content": {
            "summary": {
                "value": "The authors show that the noise of SGD from minibatching regularizes the solution towards a \"balanced\" solution whenever rescaling symmetries are present in the loss function. They then apply these results to derive the stationary distribution of SGD for a diagonal linear network. Then the authors characterize this stationary distribution, showing such phenomena as: phase transitions, loss of ergodicity, and fluctuation inversion. They show that these properties exist uniquely in deep networks, thus delineating a difference between deep and shallow networks."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The paper provides new insights into the behavior of stochastic gradient descent (SGD), such as: how the Langevin model is flawed in studying SGD, that the noise in SGD creates a qualitative difference between it and gradient descent (GD), analysis between networks with and without depth, loss of ergodicity, among many others. \n- Characterizing the stationary distribution of SGD analytically.\n- Provided insights into why the Gibbs measure is bad for SGD.\n- The paper is original, and very clearly written."
            },
            "weaknesses": {
                "value": "- The most complicated model they analyzed was a linear deep diagonal network, but I can imagine the general case being extremely difficult."
            },
            "questions": {
                "value": "- Can the properties found for SGD, such as \"the qualitative difference between networks with different depths, the fluctuation inversion effect, the loss of ergodicity, and the incapability of learning a wrong sign for a deep model\" be extended to loss functions without rescaling symmetries?\n\n- How well do you believe the results in the paper transfer to nonlinear neural networks (i.e. with a nonlinear activation function)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1709/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1709/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1709/Reviewer_Y3Mm"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1709/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698736189381,
        "cdate": 1698736189381,
        "tmdate": 1699636099470,
        "mdate": 1699636099470,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cXpMaWfSU2",
        "forum": "381QSrWdF2",
        "replyto": "381QSrWdF2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1709/Reviewer_wHcL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1709/Reviewer_wHcL"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a law of balance phenomenon to better understand the SGD dynamics. Based on the proposed method, the authors have theoretically shown the unique properties separating the deep and shallow networks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The theory is centered around the law of balance equation, which is clean and interpretable in some sense."
            },
            "weaknesses": {
                "value": "The boundness of the law of balance seems determined largely by the covariance matrices in equation 4. However, is there any guarantee on the condition of the matrices? what happens if the matrices are degenerate, and can you justify the matrices' degeneracy matter in practice?"
            },
            "questions": {
                "value": "I would like to know more details about the case when the matrices are degenerate or justifications that they are non-degenerate."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "no ethics concern"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1709/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698975526042,
        "cdate": 1698975526042,
        "tmdate": 1699636099382,
        "mdate": 1699636099382,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wDVPJkVVj3",
        "forum": "381QSrWdF2",
        "replyto": "381QSrWdF2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1709/Reviewer_eqby"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1709/Reviewer_eqby"
        ],
        "content": {
            "summary": {
                "value": "From the view of symmetry, this manuscript shows that SGD systematically moves towards a balanced solution when rescaling symmetry of loss function exists. The stationary distribution of model parameters of is also derived for simple diagonal linear networks. Many connections with other works also shown."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. A novel view, symmery of loss, is provided for analyzing the solution of SGD and its stationary distribution. \n2. The derived stationary distribution under different factors, including learning rate, batch size, data noise, model width and depth, is analytically derived. This result explains many interesting observed phenomenon in practice or colloborates with exsiting findings in other works."
            },
            "weaknesses": {
                "value": "1. The diagram of phase transition is a special case or general, which should have been clarified. \n2. Given the symmetry view,  whether the diagonal linear network is a representative architecture for investigating SGD? Could the authors comment more regarding this? \n3. Another concern is that when conducting the analysis, a L2-norm regularization term is added. How does this affect all the derived results and further interpretation?"
            },
            "questions": {
                "value": "See the above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1709/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699065500003,
        "cdate": 1699065500003,
        "tmdate": 1699636099312,
        "mdate": 1699636099312,
        "license": "CC BY 4.0",
        "version": 2
    }
]