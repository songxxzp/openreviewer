[
    {
        "id": "2mq5x0tsmU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3332/Reviewer_3m8H"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3332/Reviewer_3m8H"
        ],
        "forum": "4w4PDIT3h4",
        "replyto": "4w4PDIT3h4",
        "content": {
            "summary": {
                "value": "This work proposes two data augmentation methods DDA and D3A. It utilizes the pre-trained model to get the mask, and uses this mask to produce more appropriate data augmentation for keeping semantic-invariant information."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. D3A can gain better generalization ability than the baselines."
            },
            "weaknesses": {
                "value": "There are several main problems that I think this paper cannot be accepted:\n\n1.\tThe pre-trained part is not generalizable. This method overfits to the DMC-GB.  The good performance of D3A relies on the quality of the mask. I think this encoder cannot be applied to any other visual RL tasks. \n\n2.\tThe main method seems too tricky. The authors do not explain why they choose random conv as a must for augmentation, what about other types of strong augmentation method? Furthermore, the task-specific encoder, the specific augmentation, and some extra introduced hyper-parameters make this paper appear very tricky.\n\n3.    This paper lacks novelty. \"Find a proper mask, and keep the primary pixel\", I think SGQN [1] is a more general and acceptable method for sloving this problem. I believe that this field should not continue to develop in the direction of proposing better augmentation methods for keeping important pixels.\n\n[1] Bertoin, David, et al. \"Look where you look! Saliency-guided Q-networks for generalization in visual Reinforcement Learning.\" \n\n\nFor writting suggestions:\n1. The figures and tables should contain more descriptions, not just a title."
            },
            "questions": {
                "value": "The questions are mentioned above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethical concerns."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3332/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3332/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3332/Reviewer_3m8H"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3332/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697273562061,
        "cdate": 1697273562061,
        "tmdate": 1699636282571,
        "mdate": 1699636282571,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UCxlu9y8Ll",
        "forum": "4w4PDIT3h4",
        "replyto": "4w4PDIT3h4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3332/Reviewer_xszw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3332/Reviewer_xszw"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes two novel approaches, Diverse Data Augmentation (DDA) and Differential Diverse Data Augmentation (D3A), to address the challenges of generalization and sample efficiency in visual reinforcement learning. The methods leverage a pre-trained encoder-decoder model to segment primary pixels and avoid inappropriate data augmentation. DDA focuses on the consistency of encoding to improve generalization, while D3A further enhances generalization by using proper data augmentation for primary pixels while maintaining semantic-invariant state transformation. Extensive evaluation on DeepMind Control Suite tasks demonstrates significant improvements in the agent's generalization performance in unseen environments and increased sample efficiency of off-policy algorithms."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The introduction of Differential Diverse Data Augmentation is quite intriguing, and the method itself is intuitive and straightforward."
            },
            "weaknesses": {
                "value": "1. The author mentioned the use of a clustering algorithm for image segmentation but did not clarify how these images for clustering were collected. Was a random strategy employed for data collection?\n\n2. There are many object-centric works [1,2,3] that are quite similar to this paper. It would be good if the authors could highlight the difference. I am also curious to know if the proposed method would have an advantage over other object-centric methods.\n\n  [1]Unsupervised Visual Attention and Invariance for Reinforcement Learning. CVPR 2021.\n\n  [2] Look where you look! Saliency-guided Q-networks for generalization in visual Reinforcement Learning. NeurIPS 2022.\n\n  [3] An Investigation into Pre-Training Object-Centric Representations for Reinforcement Learning. ICML 2023.\n\n3. The experiments lack ablation studies on certain hyperparameters. For example, what criteria were used to determine the stabilized training steps? After all, different environments and tasks would require different training conditions.\n\n4. There is a lack of comparison with other pretraining methods. After all, this study utilizes pretraining, while the comparison methods are all end-to-end approaches, which may not be entirely fair."
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3332/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698674501108,
        "cdate": 1698674501108,
        "tmdate": 1699636282483,
        "mdate": 1699636282483,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AsSACxRlUp",
        "forum": "4w4PDIT3h4",
        "replyto": "4w4PDIT3h4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3332/Reviewer_ziWN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3332/Reviewer_ziWN"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method to better augment visual inputs for RL. The method relies on a simple model that produces a mask that selects the main object in the image (in this case the agent), and then augments the background and the main object differently. The proposed DDA doesn't augment the main object, while D3A introduces an adaptive strategy that, depending on how much the outputs of the model change, decides whether to augment the main object or not.\nThe method is tested on Deep Mind Control suite, and achieves impressive performance in the setting with added perturbations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is quite clearly written\n- The proposed achieves strong performance against the baselines on DMC tasks.\n- The idea of using quartiles for epsilon avoids having an additional hyperparameter for D3A."
            },
            "weaknesses": {
                "value": "- The method is only tested on DMC. While it achieves impressive performance, more tasks would be needed to see if the idea has merit.\n- The method is quite complicated, requiring an additional module to produce the mask. \n- The method is quite close to TLDA [1], and is not tested as extensively as TLDA\n\ncomments:\n- Minor suggestion: can you highlight the best baseline in Table 1, like underscore it for clarity?\n- Section 4.4 \"without being used a mask\" sounds weird\n- Algorithm 2, lines 26, 27: should this be outside the big if? I understand Algorithm 2 refers to Algorithm 1 to save space. It would be useful to have a full version in the appendix to avoid confusion.\n\nI'm willing to raise my score if authors provide additional evaluation in another environment (e.g. robotic manipulation).\n\n[1] TLDA: Don't Touch What Matters: Task-Aware Lipschitz Data Augmentation for Visual Reinforcement Learning https://arxiv.org/abs/2202.09982"
            },
            "questions": {
                "value": "1. How would the method handle more complicated, real-world environments where obtaining a mask is not as easy? In DMC, clustering the main object is fairly easy, while in more cluttered scenes it will be more complicated. This taps into a whole new area of research on segmentation, but I want to know if authors have thought about this."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3332/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698777981847,
        "cdate": 1698777981847,
        "tmdate": 1699636282402,
        "mdate": 1699636282402,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Dg7SPuhqwu",
        "forum": "4w4PDIT3h4",
        "replyto": "4w4PDIT3h4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3332/Reviewer_XRib"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3332/Reviewer_XRib"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a novel technique named Diverse Data Augmentation (DDA) and its enhanced variant, Differential Diverse Data Augmentation (D3A), tailored for targeted data augmentation in image-based Reinforcement Learning.  \nTheir approach is based on is the utilization of a segmentation network (Segnet) which is trained on a custom dataset to discern between foreground and background pixels in the observations. The authors leverage the predictions from this segmentation network to generate masks, facilitating the application of strong data augmentation to the background pixels. This ensures that the augmented observation undergoes only minimal semantic alterations. In the D3A variant, the authors incorporate a milder form of data augmentation on the foreground pixels, but only after confirming that the Q-value estimations between the augmented and original observations aren't drastically different. Both methods are applied using the SVEA framework: the augmented observations are exclusively presented to the Critics, leaving the target Critics unexposed to them. The efficacy of both methods is confirmed through empirical experiments on the DeepMind Distracting Control suite benchmark."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "*    The authors have devised an original strategy of applying differential data augmentation: intense augmentation on non-critical pixels and milder augmentation on task-relevant pixels. This layered approach offers the potential for enhancing robustness without overwhelming the primary information in the images.\n\n*    The paper introduces a criterion to determine if an augmented observation should be incorporated during the training process. Such a selective approach aims that only beneficial augmented data contributes to the learning, potentially reducing noise and improving convergence.\n\n*    The method's results on the DeepMind Distracting Control suite benchmark provide evidence of its practical utility. While limited to this benchmark, it's a step towards validating the approach's applicability in certain environments."
            },
            "weaknesses": {
                "value": "**Weaknesses**\n\n- A notable dependency of the method is its reliance on the Segnet network, specifically trained on a custom dataset crafted by the authors. This dataset facilitates supervised learning to distinguish between background and task-relevant pixels. The intensive human supervision required to curate this dataset raises concerns about the method's scalability and adaptability to more intricate environments.\n\n- The approach necessitates the identification of a threshold, determining when the Q-values of augmented observations deviate substantially from the original observations' Q-values. The definition of this threshold hinges on some form of \"stabilization\" during training. The paper would benefit from a more thorough discussion regarding the identification and practical implications of this \"stabilization.\"\n\n- The experimental results suggest that the methods might be overly tailored for the specific benchmark in question. For instance, DDA demonstrates superior performance on distracting backgrounds due to its emphasis on robust background augmentation, while D3A outperforms on color distractions that modify task-relevant object colors. Such specificity could limit the method's generalization across diverse settings.\n\n- The problem formulation section appears convoluted and would benefit from a more coherent presentation.\n\n- The ablation study lacks clarity in specifying the particular distracting setting upon which the performance metrics are based. Given the distinct performances of DDA and D3A under varying distracting scenarios, this omission is significant. Additionally, the paper doesn't provide clarity on what constitutes \"DDA without Random Augmentation.\" Is it merely SAC, or DDA with a predetermined data augmentation type? If it's the latter, the specific augmentation type ought to be explicitly mentioned.\n\n- While the authors claim to apply their method to SAC, in reality, the application is more in line with the SVEA framework, as there's a shared emphasis on excluding data augmentation from target critic estimations.\n\n- Several parts of the paper are marred by ambiguous language, unclear expressions, and typographical errors. Examples of such problematic statements include: \"expanding the latent sample space,\" \"migrate the trained representations to tasks for visual driving,\" and \"we define an transformation.\", \"+339% improvement\" .\n\nThe paper would undoubtedly benefit from a thorough editorial review to rectify these inconsistencies and improve overall clarity."
            },
            "questions": {
                "value": "Regarding the DDA approach, when a particular masked augmented observation is rejected based on Q-values estimation, why opt for using the original augmented observation instead of a masked one like in DDA? What motivated this design choice?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3332/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3332/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3332/Reviewer_XRib"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3332/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698784766606,
        "cdate": 1698784766606,
        "tmdate": 1699636282333,
        "mdate": 1699636282333,
        "license": "CC BY 4.0",
        "version": 2
    }
]