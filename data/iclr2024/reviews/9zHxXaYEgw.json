[
    {
        "id": "57fAR2ze7f",
        "forum": "9zHxXaYEgw",
        "replyto": "9zHxXaYEgw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1248/Reviewer_NikU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1248/Reviewer_NikU"
        ],
        "content": {
            "summary": {
                "value": "This work introduces a diffusion-based method for video generation. The proposed method leverages a flow-based image animator to learn motion representations thus enabling disentangle motion from appearance. An LDM is designed to learn the motion distribution by providing the starting motion \u03b11 as the condition."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This work tries to solve the challenging issue of disentangling motion from appearance. The method is well-motivated and the proposal method is simple to understand.\n2. A Linear Motion Condition (LMC) mechanism is designed in cLMDM to condition the generative process with the first motion code \u03b11.\n3. Qualitative results show the ability to generate long videos and enable disentanglement of motion and appearance."
            },
            "weaknesses": {
                "value": "1. The author only includes pickup methods for comparison, STOA methods are not included for comparison. Recent methods, such as MoStGAN-V, VDM, Video-LDM, VideoFactory, and Make-A-Video, should be included for comparison.\n\n2. The author should include experiments on more challenging datasets, such as MSR-VTT and UCF101."
            },
            "questions": {
                "value": "see weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1248/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1248/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1248/Reviewer_NikU"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1248/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698594929723,
        "cdate": 1698594929723,
        "tmdate": 1699636051067,
        "mdate": 1699636051067,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "l4fDhsJbbI",
        "forum": "9zHxXaYEgw",
        "replyto": "9zHxXaYEgw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1248/Reviewer_8SWB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1248/Reviewer_8SWB"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a temporal generative model LEO for synthesizing editable human performance video. The key idea is to represent motions with optical flow maps to disentangle appearances and dynamics. In particular, LEO leverages a latent diffusion model trained for predicting motions in an auto-regressive fashion, and decodes the latent to form flow maps for pixel-space appearance synthesis.\n\nTheir quantitative results show obvious improvement over prior work, with qualitative evidence demonstrating better spatio-temporal coherency."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is well-written and easy to follow, and the proposed solution sounds solid. Particularly:\n- Novel formulation of diffusion-based generative model for optical flow generations, which enables long-term motion generation\n- Explicit disentanglement of the video into appearance (pixel values) and motion (optical flow) that makes LEO better preserve the identity information in the input.\n- Auto-regressive motion generation with careful designs that achieve long-term video generation.\n\nThe quantitative and qualitative evaluations also show significant improvement over prior arts."
            },
            "weaknesses": {
                "value": "While showing promising results, LEO has some limitations, which are also observed in other baselines:\n- Geometry ambiguity: without any explicit notion of 3D geometry or semantic features, LEO often flips or morphs the limbs from one side to the other. This is particularly obvious in the TaichiHD videos.\n- Temporal coherency: while LEO improves greatly over the other baselines compared in the paper, the appearance can still drift off/morph arbitrarily between frames, especially for videos with occlusion/dis-occlusions or large motions.\n- Limitations and failure cases: these aspects are not presented in the papers and supplementary. Proper discussions on what LEO cannot do well can help the readers to better assess the contribution of the work, and also open up possible future directions."
            },
            "questions": {
                "value": "Below are the questions I have:\n- How does the proposed LEO compare to the approaches like Siarohin et al. 2019; 2021, where the motions are disentangled into region-based descriptors/flow-field?\n- What are the limitations of LEO? What are the failure cases? It would be great if the paper could show and discuss these topics."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1248/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698617917532,
        "cdate": 1698617917532,
        "tmdate": 1699636050999,
        "mdate": 1699636050999,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "N94ELIVONv",
        "forum": "9zHxXaYEgw",
        "replyto": "9zHxXaYEgw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1248/Reviewer_H4YQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1248/Reviewer_H4YQ"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a method to generate videos by disentangling the synthesis of appearance and motion. To this end, the authors propose a flow-based image animator and a latent motion diffusion model. In particular, the motion synthesis is conditioned on the starting motion code. This formulation allows the model to generate sequences of infinite length by changing the starting frame for each subsequence. The efficacy of the method is evaluated on multiple datasets of humans in motion."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The application of synthesizing videos of arbitrary length is relevant and challenging. \n- The main idea is simple and clearly presented.\n- The quantitative and qualitative results showcase the efficacy of the proposed model over the baselines on the TaichiHD, FaceForensics and CelebV-HQ datasets."
            },
            "weaknesses": {
                "value": "- It would be nice to see some human-specific baselines, especially since the focus of the paper is on humans, e.g., utilizing skeleton/3DMM guidance.\n- I believe a comparison (or at least discussion) to video-ldm [1] would be beneficial.\n- I am missing a section on the limitations and ethical considerations.\n\n[1] Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models"
            },
            "questions": {
                "value": "In general, I am positively inclined however I would suggest that the authors address the issues raised in the \"weaknesses\" section, especially regarding the human-specific baselines and the limitations/ethical considerations."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1248/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698838250848,
        "cdate": 1698838250848,
        "tmdate": 1699636050842,
        "mdate": 1699636050842,
        "license": "CC BY 4.0",
        "version": 2
    }
]