[
    {
        "id": "7cZLgwwUd2",
        "forum": "padGeokNrH",
        "replyto": "padGeokNrH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6335/Reviewer_kPGx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6335/Reviewer_kPGx"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes training generative models within the predictive coding framework from computational neuroscience. Specifically, they adapt techniques including Langevin sampling with amortised inference for a \u201cwarm-start\u201d, and preconditioning (from optimisation literature) to train latent generative models on several image datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper provides thorough justification for its methodology with theory. It investigates and compares several different approaches training the amortised warm-start model. There are several important ablations done (such as the diagonal preconditioning). \n\nThe experiment showing improved robustness to increased langevin step size when using preconditioning, across several image datasets, is important and interesting."
            },
            "weaknesses": {
                "value": "See questions."
            },
            "questions": {
                "value": "I\u2019m not sure I agree with the proposed set-up of comparing standard VAEs and the models proposed in this paper using identical hyperparameters for SGD. I think a more fair comparison would tune SGD hyperparameters independently for each of these models, or at least use SGD hyperparameters that were optimised for the baseline, and not necessarily for your model. \n\nDo you have wall-clock comparisons of training time? I feel the comparison between your models taking many fewer SGD iterations to train is unfair, since each SGD iteration takes many gradient steps to sample from the models. For the same reason, I feel the comparisons done in Figure 4 are unfair. Holding the training epoch constant and comparing sample quality across models is not a fair comparison, since your LPC models compute gradients several times more per SGD iteration than the VAE models.\n\nWhy is there an asterisk in the caption of Figure 2A?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6335/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698509131748,
        "cdate": 1698509131748,
        "tmdate": 1699636697220,
        "mdate": 1699636697220,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RIBt5vvz6a",
        "forum": "padGeokNrH",
        "replyto": "padGeokNrH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6335/Reviewer_9aXX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6335/Reviewer_9aXX"
        ],
        "content": {
            "summary": {
                "value": "The paper presents an algorithm to train generative models based on discretized langevin dynamics and predictive coding. The algorithm consists of updating thelatent variable in a generative model using langevin dynamics. To have a favorable initialization of the Langevin iterations, the authors propose to use a VAE to pick the initialization. Finally the predictive coding step in which the latent variable is updated is done using an Adam-like preconditioning. \n\nThe quality of the samples produced by the trained model is assessed against a model trained without preconditioning and against a VAE. The performances are found to be comparable."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed training method goes beyond the pointwise estimation of the latent variables that is typical of predictive coding. The langevin in fact,  in principle allows to sample from the posterior of the latent variables given the data and the parameters."
            },
            "weaknesses": {
                "value": "1. The main weakness lies in the fact that the proposed algorithm is not compared to vanilla predictive coding training. Without this comparison it is impossible to establish if adding the randomness actully brings any benefit.\n2. While it is said that fewer epochs are needed to learn, no assessment of the computational cost of running several steps of langevin dynamics for every parameter step is done and the need to use a VAE each time langevin iterations have to be initialized.  A comparison of the training times of the three algorithms is necessary.\n3. The authors claim that to compare fairly the three algorithms (PC langevin with preconditioning, VAE, and PC langevin without preconditioning) they use the same hyperparameters. I believe that since the three algorithms are different, the hyperparameters should have been optimized separately for each algorithm. Also it is not reported how these hyperparameters were chosen."
            },
            "questions": {
                "value": "1. How is the VAE trained in your experiments? In particular, which loss is minimized?\n2. Have you tried using the Metropolis adjusted Langevin dynamics, which is the proper dynamics to asymptotically sample from a probability measure?\n3. How did you choose the hyperparameters you used?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6335/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698744212647,
        "cdate": 1698744212647,
        "tmdate": 1699636697111,
        "mdate": 1699636697111,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0NNNieIknW",
        "forum": "padGeokNrH",
        "replyto": "padGeokNrH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6335/Reviewer_5e67"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6335/Reviewer_5e67"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a method for model learning which combines variational inference and Langevin diffusion and builds upon the predictive coding (PC) framework. An adaptive preconditioning scheme is proposed to increase the efficiency of Langevin sampling. The proposed method shows results on par with or exceeding variational autoencoders (VAEs) on image generation tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- **The main ideas of the paper are presented clearly.** The paper takes ideas from predictive coding, variational inference and Langevin diffusion, and the authors present the connections between these ideas in an illuminating way. **The adaptive preconditioning method for Langevin steps also seems quite novel.**"
            },
            "weaknesses": {
                "value": "- **No discussion of related works is provided/important references are missing.** Despite citing many recent developments in the fields of predictive coding and gradient-based Monte Carlo methods, no further discussion is made on how this work is connected to and/or provides novel perspectives compared to past work. As a consequence, I believe several important references are missing, for example (Hoffman 2017) is the original paper that explored the practical use of MCMC methods for deep latent variable models. In particular, (Hoffman 2017) also proposed using an inference network to initialize the chain. More recently, (Taniguchi 2022) proposed a similar model with Langevin dynamics, but where the Monte Carlo steps are done in parameter space. Their work included results on the same datasets (CelebA, SVHN, CIFAR10) and showed uniform improvement on the VAE counterparts.\n- **Novelty of the proposed method is questionable.** The authors proposed several different methods for training the encoder network: forward KL, Jeffery\u2019s divergence, backward KL. Both the discussion and experiment results show that the backward KL is most effective at learning the encoder, and this method is adopted for the remaining experiments. However, the backward KL objective is just the standard ELBO objective used in VAEs, making the proposed method an augmented version of VAE training. Since the proposed method run a Markov Chain from the initial distribution proposed by the encoder, it should produce a more accurate variational posterior at the cost of more computation. In this sense, it is not surprising that the model performs better (at least in achieving higher ELBOs). Maybe a more fair comparison would be to the importance weighted autoencoder (IWAE), since it also takes multiple posterior samples for a single input datapoint.\n\nReferences:\n\nTaniguchi, Shohei, et al. \"Langevin Autoencoders for Learning Deep Latent Variable Models.\"\u00a0*Advances in Neural Information Processing Systems*\u00a035 (2022): 13277-13289.\n\nHoffman, Matthew D. \"Learning deep latent Gaussian models with Markov chain Monte Carlo.\"\u00a0*International conference on machine learning*. PMLR, 2017."
            },
            "questions": {
                "value": "- In equations (6) and (7) should it be a minus sign instead of a plus sign between the two terms?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6335/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6335/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6335/Reviewer_5e67"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6335/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699073776265,
        "cdate": 1699073776265,
        "tmdate": 1699636697007,
        "mdate": 1699636697007,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FmYz480q5B",
        "forum": "padGeokNrH",
        "replyto": "padGeokNrH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6335/Reviewer_LejG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6335/Reviewer_LejG"
        ],
        "content": {
            "summary": {
                "value": "This work is pointed as using an adjusted version of gradient-based Langevin sampling for training a generative model. This is quite similar to SGLD (used for sampling from the posterior distribution of neural network parameters), but assumes access to a dataset to model. In order to use Langevin sampling to train a generative model, a couple of improvements are made to improve chain performance. First, the mixing time is improved via training an approximate inference model to over the dataset to warm start the chain. Different mechanisms of training the approximate inference model are explored (fwrd/rev KL + Jeffreys). A preconditioner is also formulated with respect to the Adam optimizer, reminiscent of [1] for RMSProp.\n\nPerformance of multiple divergences for training the warm-start model are compared, and reverse KL is ultimately chosen. \nFurther experiments are done on CelebA, CIFAR10, and SVHN to show quality of generative model bootstrapped with inference model and preconditioner vs VAE"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "I'm generally fine with this, SGLD has been used a lot for training Bayesian neural networks, so most of the literature is centered around how to overcome that particular problem (no dataset). This feels like a hole in the literature that is getting plugged, which is good. \n\n* ULD for training generative models\n* Approximate inference warm start network\n* Interesting Adam-based preconditioning, figure 3 makes sense if preconditioner is working. \n* Models converge faster on high-dim experiments"
            },
            "weaknesses": {
                "value": "I think the method is fine enough. What's really bothering me is the high-dim experiments and their scores -- what are/were the limitations here? \n* Comparing only against the VAE seems like its just not enough, especially when the results compared to the VAE is not especially convincing. \n* Preconditioner does not always help, and when it does its not by that much. Unclear why and when it is supposed to help empirically.  \nAll things considered, I'm not very convinced that the method is beneficial given the performance on the chosen datasets. For such numbers I would have expected more exploration or discussion of the results, and what the possible benefits could be."
            },
            "questions": {
                "value": "1. What are the computational limitations of this method? \n2. What makes SVHN so difficult for this method? \n3. I don't have any intuition for why FDD scores should be better/worse in this context. LPIPS is the usual score for perceptual quality."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6335/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699294211717,
        "cdate": 1699294211717,
        "tmdate": 1699636696885,
        "mdate": 1699636696885,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SSbFkdBoay",
        "forum": "padGeokNrH",
        "replyto": "padGeokNrH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6335/Reviewer_x7AB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6335/Reviewer_x7AB"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel sampling algorithm inspired by the predictive\ncoding framework from neuroscience. Where with noise an overdamped Langevin\nsampler is obtained which can be used to compute the gradient of the ELBO for a model.\nThis allows it to be used with a training regime for a deep generative model,\nobtaining results competitive with VAE-based algorithms."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper shows that the ideas in the predictive coding framework have\nquite practical applications and consequences in machine learning. The\nmethod is explained very clearly and the experiments are well-chosen\nto show that it is indeed competitive with VAEs on realistic datasets."
            },
            "weaknesses": {
                "value": "That the predictive coding framework when noise is added can be\ntreated as overdamped Langevin sampling is a novel and\ninteresting result. What is less clear is if it's significant\nenough as the result seems to be algorithm that's only competitive\nwith well-established VAE algorithms.\n\nWhile the method is clear, quite a bit of the paper is spent on\nbackground material. Much of which is not even used in the rest\nof the paper. For example, lots of space is spent on discussing\nall the different divergences that could be used for optimising\nthe inference network. But in the end, reverse KL is used since\nit works best. This is consistent with what is used for VAE\nalready and it feels that space could have been used on the more\nnovel aspects of this work.\n\nI would have really liked a bit more motivation for why someone\nwho prefers to use VAEs for training deep generative models should\nuse this algorithm instead."
            },
            "questions": {
                "value": "Why should someone use Langevin PC over a more conventional VAE?\nCould this preconditioner be applied to a VAE regime?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6335/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699501367411,
        "cdate": 1699501367411,
        "tmdate": 1699636696734,
        "mdate": 1699636696734,
        "license": "CC BY 4.0",
        "version": 2
    }
]