[
    {
        "id": "S5NCZCLWy4",
        "forum": "92yrETgM6G",
        "replyto": "92yrETgM6G",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7043/Reviewer_BDLx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7043/Reviewer_BDLx"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces calibration attacks, a new class of adversarial attacks that aim to miscalibrate the confidence scores of models without changing their accuracy. The authors propose four types of calibration attacks and demonstrate their effectiveness against image classifiers like ResNet and ViT across datasets. The attacks are difficult to detect using common adversarial defense techniques. Analyses show the attacks modify model representations and confidences as expected while minimally impacting gradient-based visualizations. Existing defense methods like temperature scaling and adversarial training provide limited robustness against calibration attacks. The authors propose two new tailored defenses and analyze model vulnerabilities, highlighting the need for further research into mitigating this dangerous new attack vector which could seriously impact reliability if deployed against real-world systems."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The idea of attacking the calibration of ML models is novel, and the work (analyses, discussion, etc.) is solid. Particularly, the authors conducted extensive experiments on adversarial attacks and defenses."
            },
            "weaknesses": {
                "value": "See **Questions** part."
            },
            "questions": {
                "value": "1. Questions about Figure 1.\nAs mentioned in its caption, red bars represent accuracy and blue bars represent confidence. However, the vertical axis in your image is also labeled as accuracy. Could you explain this in detail? Besides, I noticed that Figure 1 is in PNG format. A vector graphic format might be better.\n\n2. How to calculate the average confidence?\nIn my opinion, letting \"average confidence = the product of the sample proportions in each bin\" would provide a reasonable explanation according to your setting. However, in Table 1, I think the average confidence is not calculated in this way. Otherwise, we could have Accuracy = ECE +/- average confidence. \n\n3. Relation with previous works\nI list two more related works on robust calibration. It would be perfect if you could compare your work with theirs.\n[1] Tang Y C, Chen P Y, Ho T Y. Neural Clamping: Joint Input Perturbation and Temperature Scaling for Neural Network Calibration, arXiv:2209.11604.\n[2] Yu Y, Bates S, Ma Y, et al. Robust calibration with multi-domain temperature scaling, in NeurIPS 2022.\n\n4. The implementation of adversarial attack/defense algorithms.\nI found it very hard to parse the results of the experiments. I wonder if the authors could provide the corresponding code to the results in Table 1 (or at least a demonstration) so that I can reproduce and check the results.\n\nI will be very happy to reconsider my rating if the authors could address my concerns."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "I have no ethical concerns for this paper."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7043/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697700269549,
        "cdate": 1697700269549,
        "tmdate": 1699636827704,
        "mdate": 1699636827704,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hxMT4RUnFS",
        "forum": "92yrETgM6G",
        "replyto": "92yrETgM6G",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7043/Reviewer_YLZS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7043/Reviewer_YLZS"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a test-time attack that manipulates a classifier\u2019s confidence scores without changing the classifier\u2019s prediction. Two types of manipulation are considered: (1) increasing the classifier\u2019s confidence for the predicted class and (2) decreasing the margin between the confidence of the predicted and runner-up class. These manipulations can be applied to inputs randomly or to maximize miscalibration. The attacks are carried out using a variant of the square attack (a gradient-free algorithm) or projected gradient descent and are shown to be effective empirically on standard image datasets. The paper also studies the effectiveness of the attack under four calibration methods (e.g., temperature scaling, splines) and under defenses such as adversarial training."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper is comprehensive in that it covers several variants of the proposed attack (black-box, white-box, different confidence manipulations) while also exploring potential defenses. The empirical results are extensive: there is a good selection of datasets, models, and evaluation metrics.\n\nI appreciate that the paper studies a non-conventional threat model. A benefit of the proposed attack is that it has the potential to cause harm with a smaller perturbation strength compared to adversarial examples that cause misclassification. For that reason, it may also be more less susceptible to detection, particular if the confidence scores are manipulated randomly."
            },
            "weaknesses": {
                "value": "1. My primary concern with the paper is around originality and its failure to cite prior work. The paper claims to be the first to study attacks on confidence scores, however a very similar test-time attack was proposed by Galil & El-Yaniv (2021). More recently a training-time attack on confidence scores was proposed by Zeng et al. (2023) (which first appeared on arXiv in 2022). It\u2019s unfortunate these papers are not cited. There is also closely related work on certifying confidence scores by Kumar et al. (2020) and Emde et al. (2023) which ought to be cited.\n\n2. In order to better assess originality, I have compared this paper with Galil & El-Yaniv (2021). As far as I can tell, the differences between the attacks are minor:\n    - Galil & El-Yaniv focus on reducing the confidence of the predicted class, whereas this paper also considers increasing the confidence.\n    - Galil & El-Yaniv\u2019s attack algorithm is FGSM-based, whereas this paper uses the Square attack algorithm and PGD.\n    - Galil & El-Yaniv maximize/minimize the confidence of the predicted class directly, whereas this paper also considers the margin.\nApart from attacks, this paper also contributes some insights on defences, which is not something that Galil & El-Yaniv cover. However, overall I don\u2019t believe this paper\u2019s contributions are sufficiently original/significant, at least in its current state.\n\n3. Regarding the presentation of Section 5: I found it confusing that the calibration methods (TS, Splines, DCA, SAM) are presented alongside the defenses (AAA, AT, CA AT, CS). I think there is a risk that some readers may misinterpret the calibration methods as defenses, even though they are not designed to defend against attacks. \n\n4. A missing baseline defense: Kumar et al. (2020) propose Gaussian randomized smoothing as a method with certified guarantees on the confidence scores. While their guarantees may not be in perfect alignment with the $\\ell_\\infty$ attacks proposed in this paper, I think it\u2019s an important baseline to include, given it\u2019s designed to produce more robust confidence scores. \n\n5. Clarification of threat model: in order to conduct the _maximum miscalibration attack_ I believe the attacker needs to know the ground truth for each input, so they can perturb the confidence in the direction that causes maximum miscalibration. I wonder whether it is realistic to assume the attacker knows the ground truth for all inputs they want to attack. If they were able to obtain the ground truth cheaply, then it may suggest that the classification problem is not so difficult.\n\n**References**\n\n- Kumar et al., \u201cCertifying Confidence via Randomized Smoothing,\u201d NeurIPS 2020. https://proceedings.neurips.cc/paper_files/paper/2020/file/37aa5dfc44dddd0d19d4311e2c7a0240-Paper.pdf\n\n- Galil and El-Yaniv, \u201cDisrupting Deep Uncertainty Estimation Without Harming Accuracy,\u201d NeurIPS 2021. https://proceedings.neurips.cc/paper/2021/file/b1b20d09041289e6c3fbb81850c5da54-Paper.pdf\n\n- Zeng et al., \u201cManipulating Out-Domain Uncertainty Estimation in Deep Neural Networks via Targeted Clean-Label Poisoning,\u201d CIKM\u201923 https://dl.acm.org/doi/abs/10.1145/3583780.3614957\n\n- Emde et al., \u201cCertified Calibration: Bounding Worst-Case Calibration under Adversarial Attacks,\u201d\nAdvML-Frontiers'23 Workshop. https://openreview.net/forum?id=sj5K9jtrdm"
            },
            "questions": {
                "value": "1. Given the existence of prior work on calibration attacks, you could consider focusing more on the defense side (where there has been less work) or on variations of the threat model.\n\n2. For underconfidence attacks, the attack minimizes the margin between the scores of the predicted and runner up classes. Have you considered optimizing the scores of the other classes as well? For instance, one could imagine trying to perturb the scores to be close to uniform by maximizing the entropy of the scores.\n\n3. It seems the defenses are tested under the maximum miscalibration attack. I wonder how the results would differ for the random miscalibration attack. In particular, I wonder whether the compression scaling defense would be effective in that setting, since it seems to make strong assumptions on the way in which scores are perturbed.\n\n4. Is it possible to transfer these attacks to other classifiers? I expect that transfer may risk harming accuracy, especially if the attacked points are moved closer to the decision boundary. Perhaps over-confidence attacks are more reliable for transfer?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7043/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7043/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7043/Reviewer_YLZS"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7043/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698504001041,
        "cdate": 1698504001041,
        "tmdate": 1699636827579,
        "mdate": 1699636827579,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pDae2qtPZN",
        "forum": "92yrETgM6G",
        "replyto": "92yrETgM6G",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7043/Reviewer_PTEC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7043/Reviewer_PTEC"
        ],
        "content": {
            "summary": {
                "value": "The authors proposed four types of adversarial attacks targeting calibration specifically, which maximizes the error in model\u2019s prediction score without alternating the predicted label/accuracy. Building on top of some existing successful adversarial attack technique (e.g. SA), the authors achieve miscalibration through lowering the prediction on correctly classified cases and increasing the prediction on incorrect cases. It is shown to be effective on many models/datasets even in the presence of popular calibration methods. Authors also discussed the parameter choices and how they affect the performance of the attack.  In addition, the authors proposed new calibration methods that are capable of defending the attacks proposed in this paper."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The subject of this research seems novel. It is distinctive from most types of adversarial attack studies that aims to increase misclassification and lower the predictive accuracy. Instead, it has the constraint of not affecting this most notable performance metric while maximizing the error in the confidence level, which is often overlooked.\n- The experiments have good coverage on different cases and sufficiently demonstrated the authors' conclusion. The discussion is also thorough about the design choices in both the attack algorithm and the defending methods.\n- The paper is clearly written and easy to follow."
            },
            "weaknesses": {
                "value": "- The authors' motivation for calibration attack is that the prediction score instead of the classified label is directly used in downstream tasks. However, most of the experiment results are presented in terms of the calibration error, but did not explore further on its effect on the downstream. Without a concrete example, it is hard to assess the significance on the implication of this error on confidence score.\n- While the idea of attack on calibration is new as far as I know, the attack mechanism is largely based on the existing methods and does not seem to have significant novelty in itself.\n- In terms of the defense method, it is unclear whether it only addresses the attacks proposed in the paper or has more general effectiveness. Based on the pre-attack result in table 4, it doesn't seem to be superior than other existing calibration methods in terms of fixing general miscalibration, which gives me the concern that it might be only effective towards these specific attacks."
            },
            "questions": {
                "value": "- The attack algorithm is independent between samples, but is it possible to inferred information about the model on its tendency to be over/under-confident and reduces the query sizes?\n- Is it still possible to perform the attack if only the predicted class label can be queried?\n- Is it possible to attack the training process and have the model be over/under-confident on unaltered samples?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7043/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698654313590,
        "cdate": 1698654313590,
        "tmdate": 1699636827473,
        "mdate": 1699636827473,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Tvf0GQOoMM",
        "forum": "92yrETgM6G",
        "replyto": "92yrETgM6G",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7043/Reviewer_LAwv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7043/Reviewer_LAwv"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new type of problem to attack the calibration of a DNN without misleading its prediction results. Four attack goals are set: underconfidence attacks, overconfidence attacks, maximum miscalibration attacks, and random confidence attacks. The authors achieve the goals by designing new attack loss, and using existing white-box and black-box attack algorithms. Comprehensive experiments validate the effectiveness of the method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. It is good to see a proposal for a new attack problem.\n2. The authors explore different attack scenarios in the new problem.\n3. The conducted experiments are extremely extensive. I appreciate the comprehensive evaluation of various white-box, black-box attacks, as well as the defenses.\n4. The paper is well-written and easy to follow, and it gives a good survey of existing work."
            },
            "weaknesses": {
                "value": "1. The technical contribution is not strong. The modification from misleading the prediction to misleading the calibration is very straightforward. Because misleading the prediction is achieved by controlling the logits, making the ground-truth logits lower than other logits. And certainly, it is easy to manipulate the logits to be any distribution, hurting the calibration. \n\n2. It would benefit more readers if the authors put their emphasis on the significance of calibration attack, instead of the specific method. Since the paper is proposing a new problem, which is brave, the most important thing would be claiming that it is a worthwhile thing. I am not convinced by the only illustration of autonomous driving in the introduction. The design of each method looks too complicated and distractive.\n\n3. It may be a more concise and clear way to present Algorithm 1 and the two defense methods."
            },
            "questions": {
                "value": "Response to rebuttal: Thanks for the rebuttal. I agree that this is a decent work, but the contribution is not significant enough for ICLR, as also mentioned by Reviewer YLZS. The efforts are far from sufficient to convince the community that the new problem is significant."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7043/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7043/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7043/Reviewer_LAwv"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7043/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698814049707,
        "cdate": 1698814049707,
        "tmdate": 1700766571578,
        "mdate": 1700766571578,
        "license": "CC BY 4.0",
        "version": 2
    }
]