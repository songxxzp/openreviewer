[
    {
        "id": "JtMcluxKGN",
        "forum": "bshfchPM9H",
        "replyto": "bshfchPM9H",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3481/Reviewer_iXW4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3481/Reviewer_iXW4"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces RAPPER, a two-stage Reinforced Rationale-Prompted Paradigm designed to improve Natural Language Explanation (NLE) in Visual Question Answering (VQA) tasks. The first stage employs knowledge distillation from large language models (LLMs) to generate rationales that are fact-based. The second stage introduces a unique Reinforcement Learning from NLE Feedback (RLNF) to incorporate visual facts into the NLE generation. The paper claims that RAPPER outperforms existing state-of-the-art methods in VQA-NLE on two benchmarks, providing more plausible and faithful explanations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper addresses the problem of implausibility and hallucination in NLE for VQA, which is a novel contribution. The two-stage approach combining knowledge distillation and reinforcement learning is also unique and the RLNF technique for incorporating visual facts into NLE is particularly noteworthy.\n\nThe paper is well-organized and clearly articulates the problem, the proposed solution, and its advantages. The use of figures to illustrate the model architecture and the comparison with existing methods is helpful.\n\nImproving the plausibility and faithfulness of NLE in VQA has important implications for real-world applications, such as medical VQA, where interpretability is crucial."
            },
            "weaknesses": {
                "value": "The two-stage approach, while novel, adds complexity to the model. It would be beneficial to see a discussion on the trade-offs involved, such as computational cost.\n\nThe paper focuses on VQA tasks, and it's not clear how's performance of the proposed method when it was adapted to other vision-language tasks.\n\nNo human evaluation is conducted regarding the generation quality."
            },
            "questions": {
                "value": "How does the complexity of the two-stage approach impact the computational efficiency of the model?\n\nCould you elaborate on how the RLNF stage specifically tackles the hallucination problem in NLE?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3481/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698558840446,
        "cdate": 1698558840446,
        "tmdate": 1699636301413,
        "mdate": 1699636301413,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PPLWE91UJz",
        "forum": "bshfchPM9H",
        "replyto": "bshfchPM9H",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3481/Reviewer_iCAp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3481/Reviewer_iCAp"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a reinforced rationale-prompted paradigm (Rapper) for natural language explanation (NLE) in visual question answering (VQA). They aim to generate plausible and faithful NLEs to address issues like implausibility and hallucination in existing VQA-NLE methods. Rapper has two stages - knowledge distillation from large language models (LLMs) and reinforcement learning from NLE feedback (RLNF). In stage 1, it elicits pseudo rationales from LLM to encourage plausibility and filters rationales using QA model for quality.\nIn stage 2, it uses RLNF with answer and explanation scores as rewards to inject visual facts into rationales, improving faithfulness.\nRAPPER, when evaluated on VQA-X and e-SNLI-VE datasets, achieves new SOTA on both for NLE metrics. It shows better plausibility via higher CIDEr and SPICE scores compared to prior VQA-NLE methods and demonstrates improved faithfulness through higher RefCLIPScore than previous methods. The approach reduces hallucination and implausibility qualitatively over other approaches."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper offers a novel two-stage approach to inject both language-based facts and visual content into rationales.\nIt leverages powerful knowledge and reasoning capabilities of LLMs through distillation. RLNF provides a way to align rationales with visual input for faithfulness. Rationale prompting is interpretable and improves reasoning module's NLE. Training is end-to-end, does not need ground truth rationales.\nMoreover, the qualitative results show more precise and reasonable NLEs: if achieves new SOTA on VQA-X and e-SNLI-VE for all NLE metrics in both filtered and unfiltered settings. It also shows higher CIDEr and SPICE scores demonstrating enhanced plausibility of NLEs.\nImproved RefCLIPScore indicates increased faithfulness and reduced hallucination.\nAblations validate importance of both knowledge distillation and RLNF stages and analysis of derived rationales indicates progressively better quality.\nQualitative examples exhibit more visually grounded and plausible NLEs than prior methods. It also reduces cases of implausible and hallucinated explanations over other VQA-NLE approaches.\nThe claims seem reasonably supported by the quantitative and qualitative results on the standard benchmarks. The improved performance across NLE metrics substantiates the effectiveness of the Rapper approach for plausible and faithful explanation generation. The ablation studies validate the contribution of the individual components. The qualitative examples provide additional evidence that Rapper produces more precise and reasonable rationales and explanations."
            },
            "weaknesses": {
                "value": "Some potential weaknesses include:\nThe approach relies on eliciting high-quality pseudo rationales from the LLM, but the process for doing so is not extensively analyzed. In fact LLMs, especially smaller ones  (relative to e.g. GPT4) are prone to hallucinations. \nThe impact of different choices of LLM for knowledge distillation is not addressed.\nEvaluation is limited to VQA; extending Rapper to other VL tasks may reveal additional challenges.\nMore human evaluations on plausibility and faithfulness could further validate the approach."
            },
            "questions": {
                "value": "How did you determine the optimal hyperparameters (e.g. threshold \u03c4) for filtering pseudo rationales from the LLM? Was any tuning or analysis done to validate these settings?\nDid you experiment with different LLMs for knowledge distillation? If so, how did the choice of LLM impact the quality of the distilled rationales?\nYou mention the potential to extend Rapper to other vision-language tasks. What challenges do you anticipate in adapting the approach to other datasets and tasks?\nThe elicitation process for pseudo rationales is a key component of your approach but is not analyzed in depth. Can you provide more details on this process and how the prompts were designed?\nCan you discuss any trade-offs between plausibility and faithfulness you observed? Does optimizing one tend to hurt the other?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3481/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698717083951,
        "cdate": 1698717083951,
        "tmdate": 1699636301321,
        "mdate": 1699636301321,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QA1x10oM1W",
        "forum": "bshfchPM9H",
        "replyto": "bshfchPM9H",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3481/Reviewer_EbLA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3481/Reviewer_EbLA"
        ],
        "content": {
            "summary": {
                "value": "The paper is about mitigating implausibility and hallucination problems (non-informative or contradicting visual context) for generating natural language explanation (NLE) under VQA problems. To mitigate the issue, the authors introduced a notion of \u201crationale\u201d which is similar to chain-of-thought prompting. To combat the issue of generating rationale without training data, the authors distill rationale from LLMs into the rationale generator. To penalize hallucinated rationale, \u201cReinforcement Learning from NLE Feedback\u201d is used. The combination of proposed method brings a marginal improvement on benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is largely well-written and easy to understand.\n-  The function of each component in the method is clear and sound.\n- The method achieves SOTA on NLE benchmarks."
            },
            "weaknesses": {
                "value": "- The role of using rationale to improve implausibility and hallucination is unclear. It is well-known that chain-of-thought can improve reasoning. However, it is unclear to me if adding one more step, i.e., rationale could really mitigate hallucination.\n- While the method is sound, I\u2019m not very convinced that we cannot just use large vision language models and perform a chain-of-thought style prompting (which was actually the inspiration of this method)? How does large vision language models (e.g. BLIP-family or LLAVA models) perform?\n- In ablation study table, the impact of proposed method is small. Especially, RLNF effect is small.\n- A clear definition of \u201crationale\u201d is not presented in the paper. Only mentioned that it is like an \u201cintermediate\u201d just like in chain-of-thought prompting."
            },
            "questions": {
                "value": "- Please answer my questions in \u201cweaknesses\u201d section. I may raise score if the rebuttal is satisfactory."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3481/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699091036183,
        "cdate": 1699091036183,
        "tmdate": 1699636301223,
        "mdate": 1699636301223,
        "license": "CC BY 4.0",
        "version": 2
    }
]