[
    {
        "id": "8fAoF6l6bU",
        "forum": "s1ByDEbpI8",
        "replyto": "s1ByDEbpI8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1466/Reviewer_XFNL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1466/Reviewer_XFNL"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a prompting technique called Skills-in-Context (SKiC) prompting. The motivation for this prompting method is that compositional generalisation is an ability that helps to solve complex problems that are composed of existing skills, and the prompting method explicitly decomposes tasks into sub-skills. The prompting method can be used manually, by having a human construct a prompt for a task, as well as semi-automatically, by prompting LLMs to construct the prompt and having a human verify it. The authors evaluate their prompting technique on a set of reasoning tasks and show it improves over existing prompting techniques, sometimes by a large margin. The authors also do an ablation study where they find that for the semi-automatic method it helps if the prompt is constructed by the same model that is being tested, and an error analysis to find the most common source of errors is required skills for a task that are not in the prompt."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors compare their method with a large set of baselines and convincingly show their method is better for the evaluated reasoning tasks.\nThe authors present the interesting finding that it helps for a model to generate its own prompt, even if another model might construct a prompt that is inherently more correct / high-quality.\nThe authors do an error-analysis and identify the most common source of errors."
            },
            "weaknesses": {
                "value": "This paper is written around the premise that this prompting method unlocks compositionality, but the way the authors use the term is unlike existing literature. Systematic compositionality is the ability to understand and produce novel combinations from known parts. This paper claims the prompting method SKiC *\"teaches LLMs to generalize to harder problems than they have seen and to problems that require innovative compositions of existing knowledge (either in context or inside model weights).\"* and that the authors *\"develop an effective one-stage prompting strategy [..] to unlock the general compositional generalization capability in LLMs\"*. However, the authors do not investigate at all whether the LLMs that are tested are trained on the tasks they give them. For example, the authors call the last-letter concatenation task of concatenating the last letter of 4 words or more *\"out-of-distribution\"*, but a simple Google search reveals an example for 5 words (https://tutorialspoint.com/print-last-character-of-each-word-in-a-string). It seems pretty likely to me that more examples are in the pre-training data of these models. \nFurthermore, the nature of systematic generalisation requires near-perfect generalisation. For models that achieve less than 50% accuracy on some of the tasks, you cannot say they are generalising systematically, because they are not.\nOther aspects of the paper make me think the authors have a different definition of systematic generalisation than most of literature, because they for example claim that Nye. et al (2021) develop the scratchpad method to *\"unlock its potential compositional generalization capabilities\"*, but there is nothing in that paper that indicates this is the case. Scratchpad prompting is developed to allow intermediate steps of computation and with it improves performance on multi-step computation problems.\n\nTo summarise; how do you know that LLMs have not been trained on the tasks you call \"out-of-distribution\"? Without knowing that, you cannot say the models are systematically generalising to unseen tasks. Additionally, when models do not reach near-perfect accuracy, it cannot be called systematic generalisation. I'd rephrame this paper to better reflect what it actually does, which is developing a prompting method that the authors empirically show to be superior to past prompting methods."
            },
            "questions": {
                "value": "How do you define \"out-of-distribution\"?\n\nI encountered some typos etc. in the text:\n\t- \"Towards this goal, there have been a series of prompting strategies being developed to improve the reasoning and compositionality capabilities.\" paragraph 2 introduction\n\t- \"Put the asked words to a list. For example, put the words in 'apple' to D=['apple']\"; Figure 1\n\t- \"fine-tuneded baselines such as finetuned\" page 6"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1466/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698747827319,
        "cdate": 1698747827319,
        "tmdate": 1699636075532,
        "mdate": 1699636075532,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "s66etqBDz6",
        "forum": "s1ByDEbpI8",
        "replyto": "s1ByDEbpI8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1466/Reviewer_52w8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1466/Reviewer_52w8"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a prompting approach to improve compositional generalization in large LMs through an in-context learning approach. The approach is as follows:\n\n- First, provide the model with \"skills\" along with some examples showing how to invoke skills\n- Then, provide some examples of how to compose these skills to solve a task\n- Then with these demonstrations, see if the model can generalize to solving tasks that involve new compositions of tasks.\n\nAs with all prompting approaches, the main question is how much human knowledge / time went into designing the prompt and how generally applicable is the approach. Having said that, from results we find that the proposed approach (SKIC for Skill in-context prompting) improves performance over zero-shot and CoT prompted LMs, on challenging tasks including long digit addition / multiplication"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Lots of experiments of a simple approach, and very good results across the board.\n- The approach seems straightforward and easy to apply.\n- One of the novel aspects in the paper was to use an LM to mine a set of primitive skills from a dataset of demonstrations and then use the same LM to compose these skills. Here, there is much lower human intervention (though there are a few details I couldn't quite get about which experiments use author provided skills vs LM generated skills)."
            },
            "weaknesses": {
                "value": "I think this work could have explored some of the technical aspects of \"compositional\" skill utilization in LMs beyond simple prompting:\n\n1. What are the limitations of this kind of generalization? Are there some compositions that all models fail at? How do you reconcile this apparent success at compositional generalization with other work that shows poor compositionality in LMs?\n2. Can this be used for semantic parsing tasks where an LM is prompted to discover skills?\n3. Can this be applied to general NLU tasks (e.g. textual entailment) by prompting an LM to discover skills? \n4. Can the LM discover the somewhat \"hand-designed\" skills of \"extracting digits\" / \"list length\" for addition? If not, what skills does the LM discover there?\n\nCurrently, the paper lacks answers to a lot of these technical questions. It's also not clear if this approach is generally applicable. The notion of a skill may not always be very clearly described in terms of natural language. But i'm happy to improve my score if some of these weaknesses are addressed."
            },
            "questions": {
                "value": "- There is a whole sub-field in language and robotics where LLMs are used to decompose plans into sub-tasks. How do those approaches compare to SkiC?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1466/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698817368982,
        "cdate": 1698817368982,
        "tmdate": 1699636075461,
        "mdate": 1699636075461,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4TjnI1gBhU",
        "forum": "s1ByDEbpI8",
        "replyto": "s1ByDEbpI8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1466/Reviewer_v2fa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1466/Reviewer_v2fa"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a prompting method termed as \"SKiC\" (skills-in-context), which equips the large language models (LLMs) with a set of pre-defined skills by leveraging their in-context learning capabilities. First of all, the method constructs a set of potential skills either from human annotation (distill skills via human) or model generation (distill skills via prompting LLMs). The authors find it is crucial to demonstrate both the skills and the compositional examples within the SKiC prompts. Also, SKiC prompting is shown to be able to solve unseen complex reasoning problems better than previous methods (CoT/Decomp/etc.)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "**Clarity & Significance**\n\n- The effectiveness of the proposed method is clearly demonstrated through extensive experiments. The empirical observations indicate that the proposed method indeed improves the performance of LLMs across a range of sizes (davinci-003, ChatGPT, GPT4, etc.). The improvement is significant in many tasks, compared to several prior arts.\n- The overall presentation of this paper is clear and easy to follow. The evaluation including ablation studies is comprehensive. The appendix is very detailed."
            },
            "weaknesses": {
                "value": "**Novelty**\n\nI believe the idea of including skills in context has been proposed in many previous works, which made me unsure about its novelty and whether the contribution of this work is significant. For example, [1] also focuses on the challenge of large number addition/multiplication, and the authors also proposed to include basic skills and their composition in the context. They have also shown significant improvement with in-context skills prompting when there are more digits involved. I'd love to hear from the authors what the novelty of this work is given [1] was proposed and publicly available nearly one year ago.\n\nHowever, many recent works have found using tools can significantly improve performance in math reasoning tasks (e.g., GSM8k and MATH), such as PAL [2] and Program-of-Thought [3], and they have shown that tool-using skills (rather than natural or symbolic language skills) can bring more gain to math reasoning tasks. I understand the main focus of this work is not about tool-using, but the method proposed here is a bit less appealing to me considering the current SoTA can reach 80+ (see [4]), especially when considering [1] already discussed the tool-using case.\n\n**Soundness**\n\nMy main concern is that the skills used in the SKiC are somehow mined from the test set (e.g., you create basic addition/multiplication skills and the compositional examples for GSM8k because you pre-know it is a math reasoning task and preview some of its questions), which might not be the case in practice. This raises the question of how the framework's performance would be affected if the skills were taken from a different set of tasks. For example, will I see any improvement in BBH object counting or penguins on a table task (both are about numeric reasoning) with the SKiC skills you designed for GSM8k? In other words, I guess there is a pre-condition of the success of SKiC, which you can somehow preview the test set of the target tasks, and then you can prepare the needed skills beforehand (either from human annotation or from LLM prompting), which I don't believe is the true definition of \"solving unseen tasks\".\n\nWhile the paper does not explicitly address this issue, it is possible that the performance of the framework could be affected if the skills were taken from a different set of tasks. This is because the skills used in the framework are specific to the domain of the target task test set, and may not be applicable to other domains. For example, if the skills were taken from the Last Letter Concatenation and tested on the GSM8K/MATH tasks, I don't think SKiC can lead to optimal performance.\n\n[1] Teaching Algorithmic Reasoning via In-context Learning\n\n[2] PAL: Program-aided Language Models\n\n[3] Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks\n\n[4] Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification"
            },
            "questions": {
                "value": "**Impact of the in-context compositional examples**\n\n1. How did you pick the two/N examples in your prompts? I believe the performance will be heavily affected by the choice of these examples. Did you run experiments or have a discussion on their impact?\n2. Does the order of these examples matter? Telling from [1] I think the order is another factor that will affect the performance, but the work does not discuss it.\n\n\n[1] Calibrate Before Use: Improving Few-Shot Performance of Language Models"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1466/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698836623418,
        "cdate": 1698836623418,
        "tmdate": 1699636075402,
        "mdate": 1699636075402,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5FJVkjdrrW",
        "forum": "s1ByDEbpI8",
        "replyto": "s1ByDEbpI8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1466/Reviewer_wLQH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1466/Reviewer_wLQH"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new prompting approach for compositional reasoning, which forms exemplars as skills which can be flexibly composed by the LLM. This guides the LLM to ground its reasoning steps on skills which are already available in its knowledge. The method significantly improves performance on compositional generalization, achieving the  state-of-the art."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The idea proposed in the paper is intuitive but effective, and the paper is very well written.\nThe performance of the model is convincing, and the results show impressive performance on both composition over in-context skills and beyond in-context skills."
            },
            "weaknesses": {
                "value": "Tables 1 and 2 could be better organized in the page."
            },
            "questions": {
                "value": "1. In the beyond in-context skills setting, does the LLM use skills that are not specified in Figure 16?\n2. Is it possible to replace some of these skills with function calls to Python, for example, to decrease computation errors?\n3. More broadly what would be the pros and cons of letting the model access such \"external\" skills (if it is possible)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1466/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698859626544,
        "cdate": 1698859626544,
        "tmdate": 1699636075334,
        "mdate": 1699636075334,
        "license": "CC BY 4.0",
        "version": 2
    }
]