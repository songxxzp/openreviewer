[
    {
        "id": "07sydjGamM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1320/Reviewer_5dKH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1320/Reviewer_5dKH"
        ],
        "forum": "lDbjooxLkD",
        "replyto": "lDbjooxLkD",
        "content": {
            "summary": {
                "value": "This paper proposes a new approach to \"smooth\" the emergent abilities. That is, we can sample the model outputs from the logits many times until we get the correct results. The proposed approach can unlock the predictable scaling from emergent abilities. The paper also provides some insights about why chain of thoughts and rethink the relationship between CoT and emergent ability."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1) It is an important topic to make the emergent abilities predictable.\n2) The insights about the relationship between CoT and emergent ability are interesting. The authors also provide some theoretical evidence about the insights.\n3) The proposed approach is easy to implement."
            },
            "weaknesses": {
                "value": "1) The PathUntil seems to be very expensive in the early stage, because of the low probability of sampling the correct answer.\n2) The smoothness of PathUntil highly depends on the output length. For HumanEval it may be okay because the code is simple and short. However, it would be very hard to make it very smooth for the long answers. \n3) It would be helpful to provide a more detailed discussion between \"ppl on task data\" and \"passuntil on the task data\". I can understand these two are different, but this may be helpful to let more readers to understand the insight of this work."
            },
            "questions": {
                "value": "1) Can the PassUntil curve be more smooth if we select the \"top-K\" candidates before sampling? Such as setting a value for top_k in huggingface model.generate API"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1320/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697166027312,
        "cdate": 1697166027312,
        "tmdate": 1699636059532,
        "mdate": 1699636059532,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "o4RvlUQ6TA",
        "forum": "lDbjooxLkD",
        "replyto": "lDbjooxLkD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1320/Reviewer_jwFq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1320/Reviewer_jwFq"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel evaluation strategy, PASSUNTIL, to detect subtle improvements in small models and quantitatively predict the performance. It also presents the task scaling law, which provides insights into the relationship between model size and performance. Additionally, the paper explores emergent abilities through a mathematical analysis of multi-step reasoning and circuit hypotheses. Experimental validation is conducted to support the theoretical analysis."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Proposed the evaluation strategy \"PASSUNTIL\" with theoretically infinite resolution, enabling the prediction of task performance and the derivation of the task scaling law.\n\n2. Analyzed emergent abilities using a mathematical definition, challenging prevailing hypotheses and introducing an alternative circuit hypothesis based on theoretical derivations.\n\n3. Conducted experiments to validate the theoretical analysis and provided the first open-source attempt to investigate the predictability of task performance."
            },
            "weaknesses": {
                "value": "1.\tMotivation vs. actual design of the task scaling law:\n\nThe paper states that \u201cDespite the predictable decrement in LLM loss, task performance improvements are twisted during scaling\u201d and \"First, these works concentrate on training and validation loss metrics, which do not reliably predict task performance.\" This suggests that the loss metrics may not be highly dependable for predicting task performance. However, the task scaling law design still focuses on the correlation between PU and test loss. This may raise questions about its alignment with the initial motivation because the design of the task scaling law does not go beyond the scope of loss.\n\n2.\tDesign of the experiment to predict performance:\n\nIn the experiments for performance prediction, only the method designed in this paper was implemented, without conducting a comparison with other related methods previously proposed.\n\n3.\tThe analysis of emergent ability\n\nThis work challenges the prevailing hypotheses for emergent ability by employing a redefined metric F(N). However, it should be considered that the choice of metrics, including the new definition of F(N), may potentially impact emergence. How does this work explain the emergence observed in previous works, especially for multi-step reasoning? Does this contradict Theorem 2?"
            },
            "questions": {
                "value": "PU vs. test loss in the task scaling law design: This work seems to give a vague definition of \"pass\" in PASSUNTIL. Can PU represent all performance metrics of tasks when required?  In the task scaling law, can this relationship between PU and test loss be applicable to all tasks? In the paper \"Are emergent abilities of large language models a mirage?\" that this work cited, when the task performance metric is changed to Token Edit Distance, the relationship with loss does not align with the formula provided in this work.\n\nThis work challenges the prevailing hypotheses for emergent ability by employing a redefined metric F(N). However, it should be considered that the choice of metrics, including the new definition of F(N), may potentially impact emergence. How does this work explain the emergence observed in previous works, especially for multi-step reasoning? Does this contradict Theorem 2?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1320/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1320/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1320/Reviewer_jwFq"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1320/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698586705239,
        "cdate": 1698586705239,
        "tmdate": 1699636059419,
        "mdate": 1699636059419,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IJ7vUiJJAn",
        "forum": "lDbjooxLkD",
        "replyto": "lDbjooxLkD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1320/Reviewer_v7YM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1320/Reviewer_v7YM"
        ],
        "content": {
            "summary": {
                "value": "Previous work pointed out that limited resolution may yield biased estimates of LLM performance. This paper proposes sampling until a non-zero number of success is achieved, then shows doing so allows for better estimation of small quantities."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Overall, I think this is a good paper. I think it is well motivated, thorough and insightful. I would be happy to increase my score if a number of modifications are made (or if the authors tell me why I'm mistaken!)."
            },
            "weaknesses": {
                "value": "Here, I order my feedback in sequential order based on moving through the paper top to bottom.\n\n> We hypothesize that the perceived discontinuity from trivial to excellent performance might stem from limited evaluation resolution. By employing a more nuanced resolution, one could potentially uncover the scaling law for tasks. Our hypothesis diverges significantly from that of Schaeffer et al. (2023),\n\nI think this misstates Schaeffer et al. (2023). Their abstract states, \u201cwe provide evidence that alleged emergent abilities evaporate with different metrics or _with better statistics_\u201d. The \u201cbetter statistics\u201d refers to increased resolution. Specifically, their Figure 4 shows that increasing resolution by sampling significant amounts of additional data improves predictability on the original metric i.e. Accuracy. One of Schaeffer et al. (2023) et al.'s key takeaways is that predicting capabilities with scaling can, in certain cases, demand significantly higher resolution.\n\n>  PASSUNTIL deploys extensive random sampling in the decoding phase (e.g., 105 sampling times), and evaluates each sampling result until any generation passes the target test metric. Therefore, this evaluation strategy has infinite measurement resolution as long as computational resources are not bounded.\n\nAssuming unbounded computational resources is not a trivial assumption to make, and indeed, in the experiments, the authors do not sample as dictated by their procedure. Rather, the authors set a finite upper bound on the number of samples. Consequently, I am minorly concerned with their story and with their results since the Pass Until (PU) score they discuss is **not** the PU score they numerically compute. I think they should use PU as defined or offer a correction if a limited compute budget exists.\n\n> Theorem 1.\n\nThe authors note that the failure time $f = K - r $ follows a negative binomial distribution. I have two comments.\n\n1. Wikipedia seems to suggest that the author's estimator $PU := r/\\mathbb{E}[K]$ is a *biased* estimate (https://en.wikipedia.org/wiki/Negative_binomial_distribution#Maximum_likelihood_estimation), directly contradicting Theorem 1. The citation given is Haldane Biometrika 1945 \"On a method of estimating frequencies\" and a quick Google search discovers many Math.StackExchange & Cross Validate.StackExchange posts backing up Wikipedia. Could the authors please double check? I suspect the step in their derivation of dividing by $\\mathbb{E}[K]$ may be incorrect, but I don't have time to investigate. My guess is that if the estimator is defined as $\\hat{P}(s) := \\frac{r}{K}$, then its expectation $\\mathbb{E}[\\hat{P}(s)] := \\mathbb{E}[\\frac{r}{K}] \\neq \\frac{r}{\\mathbb{E}[K]}$, as the authors claim.\n\n2. When $r$ is known, the minimum variance unbiased estimator (MVUE) for a Negative Binmoial is known to be $\\frac{r-1}{K-1}$ (where K is the total number of trials to get $r$ successes, in the authors' notation; https://en.wikipedia.org/wiki/Negative_binomial_distribution#MVUE_for_p). I am curious to know why the authors chose to not use this estimator? If the authors have a compelling reason, I would be happy to listen. If the authors do not have a compelling reason, I would strongly recommend changing to this MVUE and recomputing their results; doing so should not require resampling from the models. Defining PassUntil as this MVUE would also salvage Theorem 1.\n\n> Table 1\n\nI think this Table could be much more easily understood as a Figure. Assuming you're using Python, I recommend doing the following in Matplotlib & Seaborn: Create two axes, one for Emoji Movie & Date Understanding. Then, the x axis of both matplotlib axes should be number of instances passed (from 0 to 41). The y axis should be the 4 models, and the 4 sampling options (BS, RS-1, RS-100, RS-10000) should be the hues. \n\n> Equation 4\n\nTwo comments here:\n\n1. I believe this equation is incorrect. Equation (3) is a product from $i=1$ to $i=|y|$, and Equation 4 correctly converts that product into a sum, but the right hand side has no dependence on $|y|$. Your Equation 4 states that generating a length 1 output and generating a length $10^9$ output have the same PU scaling. Rather, I believe you should have $\\exp(-c |y| N^{-\\alpha}).\n\n2. Once corrected, your Equation 4 is identical to Schaeffer et al.\u2019s $Accuracy(N)$ equation (Page 4) (modulo the slightly different definitions of the scaling law form). It would be good to acknowledge this.\n\n> Table 2: Prediction of our framework\n\nHere, I also recommend converting the table to a figure. Also, where are your confidence intervals or standard errors?\n\n> Figure 6: Scaling curve for task \u201cDates\u201d and \u201cIdentity\u201d. \n\nAt a granular level, where are the error bars? You should be able to repeat this experiment by bootstrapping which 20 questions are in the test set and which are the in-context examples.\n\nAt a higher level, I'm a bit skeptical about this contribution for a few meta-reasons.\n\n1. Previous emergent papers didn't emphasize this BIG-Bench task (Unnatural In-Context Learning) and to be honest, I haven't heard it mentioned as an emergent ability.\n2. The task specifies 8 subtasks (https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/unnatural_in_context_learning#subtasks) but this manuscript only considers 4 and I don't know why those 4 were chosen.\n3. The curves are fit with no held-out data for testing generalization of the fit.\n\nPut together, I'm unsure whether these specific tasks might have been cherry picked. BIG-Bench has >100 tasks, so I'd guess any pattern can be found if one looks for it. That doesn't mean the pattern is a general phenomenon; rather, by random chance alone, some curves will look concave or convex with respect to log N.\n\nLastly, by limiting the number of samples ($10^5$), I believe the PU estimator is no longer unbiased. The whole promise of PassUntil is that one can keep sampling so long as necessary. Is this correct? If so, how can we correct for the finitely many samples?\n\n> if F (N ) is a concave function of log N , then the task obeys super-scaling law growth, which is defined as \u201cemergent\u201d\n\nI have two thoughts here. On one hand, I feel like \"Emergence Still Exists But Becomes Continuous.\" is an oxymoron based on the definition offered by Wei et al. 2022, and that one shouldn't attempt to overwrite an established definition. But on the other hand, a better definition is a welcome invitation. Perhaps it would be better to more clearly title this section \"An Alternative Definition of Emergence\" or \"A Quantitative Definition of Emergence\" to make clear that the existing definition is being more precisely defined/refined.\n\n> Theorem 2. Suppose each reasoning step\u2019s success rate, measured by PASSUNTIL obeys the scaling law growth, then the multi-step success rate follows the sub-scaling law growth.\n\nDisclaimer: Empirically, this seems wrong to me. Looking at Figure 7 (right), subscaling behavior is _not_ what multi-step success task curves look like.  I think the problem is the assumption: \"Suppose each reasoning step\u2019s success rate, measured by PASSUNTIL obeys the scaling law growth.\" What I've observed is that LLMs exhibit non-trivial conditional dependencies such that if the first few tokens are correct, subsequent tokens are more likely to be correct, and vice versa. I'm not saying the math is incorrect. I'm saying that this assumption isn't a good fit to reality. This is also visible in Schaeffer et al. 2023's Figure 4: the independence assumption results in geometric decay, but LLMs don't decay this quickly.\n\n> Theorem 3. In this scenario, the success rate of the task is the majority voting of these circuits\n\nThis statement is unclear. I'm willing to buy that multiple circuits exist, but why should the decision-making be via majority vote. Is this an assumption? If so, I recommend changing the language to \"In this scenario, *if* the success rate of the task is the majority\nvoting of these circuits.\" But if not, then could the authors please clarify why decision-making is via majority vote?\n\n\n> set the sampling upper bound times to $10^5$\n\nIn my experience, this takes a significant compute budget. Could the authors please quantify how long such evaluations take in compute, time and money?\n\n## Possibly Unacknowledged Limitation\n\nSome LLM evaluation processes don't involve sampling. This includes non-generative tasks e.g., MMLU or greedy decoding aka temperature=0. In such evaluation processes, does PassUntil make sense? And if not, hopefully the authors can add a limitation section.\n\n## Suggested Alternative Title\n\nI won't take this into account for the review, but reading the title \"Unlock Predictable Scaling from Emergent Abilities,\" I don't know if \"from\" is the right preposition. I'd recommend a slightly different title like \"Predicting Emergent Abilities of LLMs with Infinite Resolution Evaluations\". I think that's more clear and also more exciting."
            },
            "questions": {
                "value": "1. Why did you need to train your own models? What's wrong with using existing models e.g. Llama, Mistral, Pythia? Using existing models so seems much cheaper.\n\n2. [Disclaimer: I'm not affiliated with the following paper] What do y\u2019all think of Arora & Goyal et al. 2023\u2019s A Theory for Emergence of Complex Skills in Language Models https://arxiv.org/abs/2307.15936? It seems to disagree with your Theorem 2, if I understand correctly. It'd be good to know what the relationship is between these."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1320/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1320/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1320/Reviewer_v7YM"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1320/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698621958549,
        "cdate": 1698621958549,
        "tmdate": 1699636059320,
        "mdate": 1699636059320,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FDpW0TAjzX",
        "forum": "lDbjooxLkD",
        "replyto": "lDbjooxLkD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1320/Reviewer_BHGc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1320/Reviewer_BHGc"
        ],
        "content": {
            "summary": {
                "value": "The paper aims to understand the scaling properties of large language models (LLMs) and discover a task scaling law to predict task performance. The authors introduce an evaluation strategy called PASSUNTIL, which utilizes massive sampling in the decoding phase and has theoretically infinite resolution. They conduct a quantitative investigation into the scaling law of task performance, leading to two main findings: (1) Task performances are predictable with PASSUNTIL; (2) The emergent abilities do exist and can be characterized by super-scaling law."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper presents a pioneering open-source effort to predict task performance in large language models, aiming to significantly contribute to and encourage future research in this area.\n\n2. The authors introduce PASSUNTIL, a novel evaluation strategy that appears to provide a more equitable assessment compared to existing metrics, showcasing their innovative approach to addressing the challenges in the field.\n\n3. The proposed scaling laws demonstrate a strong fit with the data across several datasets, highlighting the effectiveness and potential applicability of the authors' findings in various contexts."
            },
            "weaknesses": {
                "value": "1. The derivation of Equation (3) appears to have some discrepancies. (Please refer to Question 1)\n\n2. The number of evaluation datasets used in this study is somewhat limited in comparison to previous works on scaling laws. (Please see Question 2)\n\n3. The task scaling law seems somewhat arbitrary, as some tasks require standard scaling laws while others necessitate super scaling laws. (Please refer to Question 3)\n\nOverall, this paper serves as a valuable starting point, but it lacks a comprehensive investigation."
            },
            "questions": {
                "value": "1. Scaling laws are generally in an average sense over a dataset but not for individual tokens. For example, in a single generation, an error incurred in earlier tokens is likely to cause errors in subsequent tokens. \n\n2. This paper evaluates the \"emoji,\" \"date,\" and \"identity\" tasks. However, the number of evaluation datasets seems relatively small compared to existing works like Wei et al., 2022a. As a result, it is uncertain whether the conclusions drawn in this paper are broadly applicable. Could you please discuss the generalizability of your findings?\n\n3. The scaling laws in Equation (4) are for general tasks, irrespective of whether they involve multi-step reasoning. Ideally, they should be applicable to all tasks. However, Figure 6 demonstrates that it does not work well for multi-step reasoning tasks, and the authors propose another super scaling law to address this issue. It makes this paper a bit ad-hoc. \n\n4. Theorem 2 is based on the premise that emergent abilities are induced by multi-step reasoning. However, Appendix E of Wei et al., 2022a includes numerous non-reasoning tasks that exhibit emergent abilities. \n\n5. In the evaluation of PU, both beam search and random sampling decoding are utilized. Does the temperature setting influence the performance, and if so, how is this factor taken into account?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1320/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698658796882,
        "cdate": 1698658796882,
        "tmdate": 1699636059258,
        "mdate": 1699636059258,
        "license": "CC BY 4.0",
        "version": 2
    }
]