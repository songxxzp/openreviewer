[
    {
        "id": "Mmwlj1LXKE",
        "forum": "ajG8vLTHh5",
        "replyto": "ajG8vLTHh5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8443/Reviewer_nn5J"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8443/Reviewer_nn5J"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a Brain Masked Auto-Encoder (BrainMAE) that consists of two main components: an embedding-based graph attention mechanism and a self-supervised masked auto-encoding framework. BrainMAE uses a static graph transient state encoder (SG-TSE) and dynamic graph TSE (DG-TSE) to learn ROI representations. The trained ROI embeddings showed the distinctive traits of ROIs, resulting in the consistency of ROI embeddings across the different datasets. BrainMAE achieved improved performances in three distinct downstream tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is written clearly and well-organized.\nThe analysis of ROI embeddings for inter-individual consistency demonstrates the generalizability of ROI embeddings. \nThe paper conducts numerous experiments to validate the efficacy of the proposed BrainMAE, outperforming the comparative methods."
            },
            "weaknesses": {
                "value": "The proposed BrainMAE is based on self-supervised masked auto-encoding, so it needs to be compared with the existing self-supervised learning-based models, including the following:\n[1] Shi, Chenwei, et al. \"Self-supervised pretraining improves the performance of classification of task functional magnetic resonance imaging.\" Frontiers in Neuroscience 17 (2023).\n[2] Malkiel, Itzik, et al. \"Self-supervised transformers for fMRI representation.\" International Conference on Medical Imaging with Deep Learning. PMLR, 2022.\n[3] Thomas, Armin, Christopher R\u00e9, and Russell Poldrack. \"Self-supervised learning of brain dynamics from broad neuroimaging data.\" Advances in Neural Information Processing Systems 35 (2022): 21255-21269.\n  \nMany fMRI studies have demonstrated that the length of each time segment significantly affects the performance. Most related studies have empirically converged to window size values between 30 and 60 seconds [4]. However, the proposed method uses 15 seconds, which is too short for the window size. Do you have a rationale for this window size?\n[4] Savva, Antonis D., Georgios D. Mitsis, and George K. Matsopoulos. \"Assessment of dynamic functional connectivity in resting\u2010state fMRI using the sliding window technique.\" Brain and Behavior 9.4 (2019): e01255.\n \nSince the word embedding includes information about its meaning, the word embeddings in NLP can be used in all sentence positions.\nHowever, the position of ROIs is never changed in training sessions, which could limit the model's ability to learn ROI traits but could learn only absolute position information. It can be considered as learnable positional encoding. Since the graph attention mechanism is permutation-invariant, the changes in ROI order should not affect the performance if the ROI embeddings have their characteristics. Even though the paper shows the evaluation of pretrained ROI embeddings in Figure 3, it still needs additional evidence that these embeddings do not incorporate absolute position information.\n\nAfter learning, the fixed ROI embeddings E are used repeatedly in SG-TSE for an attention mechanism. However, since the same attention is obtained with a fixed Q and K, there is no need to use a transformer block.\n\nOne of the main reasons for learning and exploiting the ROI embeddings was to mitigate and circumvent the inherent noise in the fMRI signal. However, from the experimental results, the authors concluded that the lower performance of DG-BrainMAE than that of SG-BrainMAE was due to such inherent noise. These are conflicting."
            },
            "questions": {
                "value": "Check the comments in Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8443/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698195043906,
        "cdate": 1698195043906,
        "tmdate": 1699637053187,
        "mdate": 1699637053187,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "21DeWkrXPC",
        "forum": "ajG8vLTHh5",
        "replyto": "ajG8vLTHh5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8443/Reviewer_DteR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8443/Reviewer_DteR"
        ],
        "content": {
            "summary": {
                "value": "The paper explores the use of functional magnetic resonance imaging (fMRI) and its representation as graph of regions of interest (ROIs) to model the human brain. It categorises these graphs based on the way the functional connectivity (FC) is handled: (1) Fixed-FC, which relies on the FC representing the linear temporal correlations within the brain network, and (2) Dynamic-FC  which aims to model the evolving FC profile over time. Each one of these two approaches has their own pros and cons in the literature. To address this problem, the paper introduces the Brain Masked Auto-Encoder (BrainMAE), which consistently outperforms existing models in various tasks. Furthermore, leveraging the model's inherent interpretability, the paper provides insights into how the model its making its decisions. BrainMAE combines a graph attention mechanism and masked autoencoding to effectively capture the dynamics of brain activity while handling the inherent noise in fMRI data."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I found the approach in the paper interesting and novel, with the key strength being the obvious consistent outperformance to other models in the literature. Differently to common papers in this area, the paper systematically analysed both classification and regression tasks, the latter being known to be more challenging and thus sometimes not explored in methodological literature. With the exceptions that I explain in \"Weaknesses\", overall the paper is clear and the reader can understand what are the different components, as well as their distinct contributions in the ablation analysis provided. The way the paper introduced the three characteristics that each ROI embedding should have (in section 2.1) is also a strength of this paper, and thus making the evaluation of the paper significant, as it is later explored in a satisfactory way in section 3.2.3. The interpretability part is insightful and positive - while showing that expected results are achieved (e.g., with gender being in the 1st principal component), it also shows other unexpected results for future discussion by the community.\n\nI would like to say that the apparent similarities between the representations of brain regions and words in natural languages (as described in \"ROI Embeddings\" in Section 2.1) is interesting and new to me, and therefore I found this framing a (weaker) strength of the paper. Finally, the balance between figures and tables is devised well, bringing more points to both the quality and clarity of this work.\n\nBased on the good results compared with previous literature, and the novel way to combine previous methods in deep learning literature into a new field, I recommend acceptance. I only recommend marginally above the acceptance threshold at the moment because of what I've written in the Weaknesses and Questions sections of my review."
            },
            "weaknesses": {
                "value": "Beyond the questions and suggestions I will leave in \"Questions\", I identify what for me are two weaknesses of this paper:\n\n1. The paper doesn't use \"traditional\" ML models (eg, SVM, random forests) directly on the flatten upper-triangle of the FC matrix as baselines. Based on my experience when running new deep learning models on FC connectivity, what I've found is that a simple traditional ML model (with a reasonable simple hyperparameter search) achieves comparable, if not better, metrics than many DL-based models on some datasets. Therefore, to evaluate the utility of this model, it would be important to see how BrainMAE compares to these more \"traditional\" ML models.\n2. The paper is missing important metrics (e.g., sensitivity and specificity) in the evaluation section of the (binary) gender prediction. Given there's an appendix, and well-known limitations with accuracy and AUROC, I don't think it is a subjective comment to request these two other metrics in a paper that has a clear connection with the medical domain.\n\n\nTwo minor weaknesses that I've identified are:\n1. Page 5 includes a part that is confusing in terms of readability. We have figure 2 mentioning SG-, DG-, and FDG- models, but right below in \"BrainMAE Variants\" only two are mentioned. Technically, they are not connected in the flow of the text, but visually they are together in the paper.\n2. I understand that it's difficult to come up with these names, but in a work in which there is a key division between Static- and Dynamic-fMRI modelling, I find the static/dynamic naming for the graph attention component not the best choice as they seem to convey different meanings (one relates to use or not the entire fMRI timeseries, and the other relates to whether use the timeseries directly in the representation or one changed by an attention mechanism). Thus, I think it doesn't help with the clarity/readability of the paper to use these terms in two distinct contexts (unless I've missed something in the reasoning of the authors)."
            },
            "questions": {
                "value": "1. Was there a particular reason for the authors to choose masked AEs in favour of other AEs? From the Introduction section it doesn't seem clear to me why this was the case. If the reason is just that they haven't been explored before in this field, I think it's a reasonable motivation (and results show it might have been a good choice). However, it would be good to say it why this was preferred to, for example, (V)AEs or other encoding variants as motivation for this new method.\n2. I found the description on the \"Static/Dynamic graphs attention\" component confusing and unexpected. Calling it graph \"attention\" led me to think that some learning (for example in the form of some GNN) would happen, but from Section 2.1, it seems there's no learnable parameters, and what happens is that the representations are updated just once before being inputted to the TSE modules. If this is the case, and considering the important mentions to GNNs in the abstract, Introduction and Related Work sections, why haven't the authors decided to use a GNN instead of an attention transformation on a graph representation followed by a transformer? The authors do compare their work to other GNN-based baselines, but no baseline seems to have the same pipeline and pretraining procedure for a more direct comparison on the utility of GNNs. I do understand that at the end of the day there are methodological decisions that just need to be made, but this one is so close to GNNs that I do not understand why it hasn't been done, and would appreciate a clarification. \n3. Why have self-loops been removed in the graph attention component? Self-loops are common standard in graph/GNN literature to keep the information from the initial node on the representation. Thus, it's not clear why the authors made this decision and why that's not at least in the ablation analysis.\n4. The paper does a very good job explaining the different components of the model with regards to how it learns/creates the unsupervised brain representation. However, the paper gets a bit confusing (and even a bit unexpected) when in section 2.1 it is said in the \"Autoencoder\" component that the decoder is only used in pre-training phase (why is there such a division?); then, on section 3.1 when it is said that behaviour/demographic measurements are used (for what?); then, section 3.2.3 once again mentions that somehow we have a pretrained model for all datasets (why so much computational complexity?). Only in section 3.3.1 it is explained that the authors appended a task-specific linear head based on previous studies, and tables 1/2/etc also make it obvious that there are classification and regression tasks. For all these reasons, I hope it's clear why figure 1 (and consequently section 2) needs a quick mention to this task-specific head for better clarity/readability.\n5. Why haven't the authors included the task-specific fMRI data from HCP, but decided to include the task-specific fMRI data from the NSD dataset?\n6. Can the authors clarify what is the difference between the DG- and FDG-BrainMAE models? The FDG-BrainMAE is introduced as the model but without the \"static graph mechanism\". Isn't this basically the same as the DG-BrainMAE? My guess is that what the authors mean is that instead of the static graph mechanism in *the first layer* of DG-TSE, the FDG-BrainMAE's TSE module used only the dynamic graph representation across all blocks from the very beginning?\n7. How are the folds created in the 5-fold cross-validation procedure? Is it in a stratified fashion independently for each downstream task? Are the authors careful to include subjects never seen before in training completely separated in the test sets?\n8. Do the authors have any hypothesis on why the initial and final task blocks were particularly important for the model to make the final predictions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8443/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698300706879,
        "cdate": 1698300706879,
        "tmdate": 1699637053077,
        "mdate": 1699637053077,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lYd2zMN2Go",
        "forum": "ajG8vLTHh5",
        "replyto": "ajG8vLTHh5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8443/Reviewer_3Zps"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8443/Reviewer_3Zps"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a method BrainMAE which uses Graph Neural Networks (GNNs) to analyze functional magnetic resonance imaging (fMRI) data for understanding brain network connectivity. BrainMAE employs a Transformer-based architecture to reconstruct masked segments of fMRI signals. The approach is validated by its ability to encode functional connectivity in a reproducible manner across different datasets and downstream tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well-written and easy to follow.\n2. The paper is well structured and has a smooth and concrete flow.\n3. The experimental part in this paper is abundant."
            },
            "weaknesses": {
                "value": "1. While the BrainMAE model is presented as a fresh approach to fMRI data, it's hard to overlook the fact that all Transformer Layers, Masked Autoencoders (MAE) and Dynamic Graph Neural Networks are somewhat antiquated techniques. These methods are not only dated in the broader machine learning landscape but have also lost their novelty in computational neuroscience applications [1,2,3]. Frankly, the proposed method comes across as a 'engineering model'.\n2. No theoretical guarantees of the proposed method. The BrainMAE seems to be too heuristic. \n3. You mention your method is 'interpretable' in the title. However, I don't figure out the true interpretability of the method since BrainMAE has no scientific inductive bias incorporated. This is just a stacking of deep-learning-based techniques, which make the model highly black-boxed. Please refer to the Questions Section for my further concerns.\n\n\n[1] MAEEG: Masked Auto-encoder for EEG Representation Learning.\n[2] Pooling regularized graph neural network for fmri biomarker analysis.\n[3] Graph neural network for interpreting task-fmri biomarkers."
            },
            "questions": {
                "value": "For your Subsection 3.3.5, Representation and Interpretation Analysis. I hold the point that analyzing the representations of fMRI using principal component analysis (PCA) and self-attention scores are too post hoc  and lacks scientific insights. In fact, the representations extracted by BrainMAE and attention layers are still black-box."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "I have no concerns at this step."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8443/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8443/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8443/Reviewer_3Zps"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8443/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698725711649,
        "cdate": 1698725711649,
        "tmdate": 1700613740942,
        "mdate": 1700613740942,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pb74FvzFFV",
        "forum": "ajG8vLTHh5",
        "replyto": "ajG8vLTHh5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8443/Reviewer_8czh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8443/Reviewer_8czh"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors developed Brain Masked Auto-Encoder (BrainMAE) for representation learning on brain fMRI data. The authors combined two newly-defined graph attention modules with MAE to obtain the latent representations for downstream analysis. The method is evaluated on various datasets to demonstrate the robustness and functional relevance of the obtained representations, and compared with several existing baselines to demonstrate superior performances in downstream tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposed method is original and serves as one of the first attempts to use MAE in the field of brain imaging representation learning. Quality of the evaluations is good and convincing: the learned representations show good consistency between different datasets and good functional relevance."
            },
            "weaknesses": {
                "value": "The writing is generally good, but the flow of the paper can be improved as certain parts can be hard to follow. For example, the notations in Section 2 is not listed clear enough: \"node features\" are described in equation (3) before the actual input fMRI segment X is formally introduced in Section 2.2. Some of the statements may not be very well supported, mostly regarding the dynamic graph attention. I have listed my questions below."
            },
            "questions": {
                "value": "1. What exactly is shown in Figure 3A? Is this a representative t-SNE plot for one subject? Or did the authors take the mean ROI embeddings across all subjects and plotted t-SNE afterwards? Or something else?\n2. From the evaluations in all tables, it seems that SG-BrainMAE consistently outperforms DG-BrainMAE. If that is the case, I can't see the reasoning of introducing dynamic graph attention modules. I hope the authors could expand a bit on that. Also, why is it called \"dynamic\"?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8443/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8443/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8443/Reviewer_8czh"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8443/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699468205853,
        "cdate": 1699468205853,
        "tmdate": 1699637052817,
        "mdate": 1699637052817,
        "license": "CC BY 4.0",
        "version": 2
    }
]