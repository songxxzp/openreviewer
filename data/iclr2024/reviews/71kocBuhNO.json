[
    {
        "id": "TpW1APE72e",
        "forum": "71kocBuhNO",
        "replyto": "71kocBuhNO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6607/Reviewer_Z2AS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6607/Reviewer_Z2AS"
        ],
        "content": {
            "summary": {
                "value": "The authors present LogicBench, a natural language QA dataset covering 25 combinations of inference rules (e.g., modus ponens, disjuntive syllogism, etc.) and logics (propositional, first-order predicate, and non-monotonic). The generation procedure is clearly described and several examples are shown. A number of models are evaluated on the benchmark, and it is also used as a fine-tuning dataset to assess resultant improvements on other logical reasoning benchmarks. Related work is compared to and discussed throughout."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This is a very nicely designed benchmark that I am excited to use. I appreciate the effort the authors have taken on a number of fronts. 1) the authors unify a number of logics and inference rules into one global benchmark. 2) the materials are synthetically generated, but still reasonably naturalistic. 3) the paper is clear and well-written. 4) the contextualization with respect to other benchmarks breaks the space down very elegantly. \n\nThis paper is already marginally above the acceptance threshold and can be bumped to an accept if the below questions are addressed."
            },
            "weaknesses": {
                "value": "There are several opportunities to improve the contribution and presentation of the work. I will address these more specifically below under questions. 1) There are a number of places where the reader would appreciate more detail as to the background behind decision points during the creation of this benchmark. 2) Aside from inference rules, there are additional correlated metadata that would be helpful to add and consider when comparing model performance across subsets. 3) it is not clear what the reader gains from the fine-tuning experiment."
            },
            "questions": {
                "value": "- Since the synthetic generation process is completed partially via sampling from an LLM, there are of course concerns as to whether the generated problems actually follow the intended logical structure and are devoid of other errors. The authors dutifully note this, and as such, human-validate a subset of the data (500 instances), which they dub LogicBench[Eval] as opposed to the larger set (3750 instances) that are not validated in LogicBench[Aug]. It is perfectly reasonable to only validate a subset of the data given the size, but the authors should include a breakdown of the types of errors found when manually validating this subset. A table should be added describing sample statistics e.g., for a sample of 100 problems generated (as might be found in the raw Aug portion), how many need to be corrected for various discrepancies in logical formulation, typos, grammar, etc. This will help infer the ceiling on the Aug dataset.\n- In the description of the NL conversion, the authors note the following:\n`For instance, implication: \u201cp \u2192 q\u201d is\nexpressed as \u201cIf p, then q\u201d, conjunction: \u201cp \u2227 q\u201d is expressed as \u201cp and q.\u201d, and disjunction: \u201cp \u2228 q\u201d\nis expressed as \u201cAt least one of the following is true: (1) p and (2) q. Note that we do not know which\nof (1) and (2) is true. It is possible that only (1) is true, or only (2) is true, or both are true.\u201d`\nCan the authors comment on the unwieldy specification of \"or\"? I understand that the authors are attempting to explicitly distinguish \"or\" from \"xor\", but should make this clear and expand on why simpler versions were insufficient. Was a simpler specification attempted initially but resulted in extremely low performance across the board due to disambiguation difficulties? More information is needed here.\n- Some clarifying information on the metric is required. If I understand correctly, each instance has 4 variants? 1/4 of these shows the correct application of the rule, later measured by A(Yes), and 3/4 show variants that cannot satisfy the conclusion, later measured by A(No)? If this is not correct, perhaps this section can be clarified for other readers as well. For the A(No) problems, it appears to me that these contain both examples where the evidence provided cannot prove the conclusion or its negation, but also instances, where the evidence provided, can in fact prove the negation of the conclusion. For example, in Figure 2 Stage 3, Variation 2 implies \"not S1\", whereas Variations 3 and 4 can neither prove \"S1\" nor \"not S1\". Perhaps these patterns of variants should be noted differently in the dataset metadata. I would suspect certain audiences to care about the distinction between these types of errors.\n- Finally, it is unclear to me what we gain from the fine-tuning experiments. The performance increases on other benchmarks are minimal and it seems to me space would be better spent clarifying the benchmark details, such as those I've raised above, as these reflect the core contribution of the paper. I would suggest moving this to the appendix. If other reviewers disagree with me and find them informative, this suggestion may be disregarded and instead, the contribution explained to the reader more explicitly."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6607/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698695355055,
        "cdate": 1698695355055,
        "tmdate": 1699636753470,
        "mdate": 1699636753470,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mtb6iSWXuY",
        "forum": "71kocBuhNO",
        "replyto": "71kocBuhNO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6607/Reviewer_8r5S"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6607/Reviewer_8r5S"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a logical reasoning benchmark LogicBench that evaluates the logical reasoning abilities of large language models. LogicBench includes binary Yes/No questions of several propositional logic rules, first-order logic extended from the propositional logic rules and non-monotonic reasoning. The sentence pairs used to generate the dataset instances are generated by GPT-3. The authors observed that current models like FLAN-T5, Tk-instruct, and GPT series have better performance on the negative labels (No) than the positive labels (Yes). Further analysis demonstrates that ChatGPT has challenges in fully comprehend many reasoning problems and factors like negations and rule complexity bring challenges in logical understanding."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The logical reasoning problem is an important research direction in revealing the reasoning abilities of current pretrained language models. LogicBench includes several logical categories to picture the logical reasoning ability.\n- The analysis about unfolding the reasoning steps and investigating influential factors is helpful in diagnose the bottleneck of reasoning abilities."
            },
            "weaknesses": {
                "value": "- In general, it is not easy to conclude the findings of the evaluation results from LogicBench and how it actually demonstrates the logical reasoning abilities. For example, whether the prompting method is the best approach to let the model speak out the reasoning steps that corresponds to the reasoning decision given the variances and limitations of generations of GPT-3/4. Similarly, other discussions need more evidences to provide enough information to support the suspensions."
            },
            "questions": {
                "value": "- What are the source of the sentences used to generate the pairs in Figure 2 (e.g. 'Liam finishes his work' in Table 4)?\n- Are Yes and No questions balanced in the benchmark?\n- The authors indicate that the LogicQA and ReClor include richer reasoning formats, but will it necessarily undermine the impacts of finetuning on LogicBench?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6607/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698908354735,
        "cdate": 1698908354735,
        "tmdate": 1699636753325,
        "mdate": 1699636753325,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lBQl2nPYod",
        "forum": "71kocBuhNO",
        "replyto": "71kocBuhNO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6607/Reviewer_sePi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6607/Reviewer_sePi"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new benchmark for the evaluation of the logical reasoning ability of large language models. The main claim is that this new benchmark comparatively considers new types of reasoning including non-monotonic reasoning that is not considered in the existing ones. They evaluate multiple language models on this benchmark and provide further analysis of the difficulties those models have in reasoning. Finally, they show their synthesized data help models in improving logical reasoning when it is used in their LLM training (here T5) and tested on existing logical reasoning benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "They created a new benchmark/resource for QA when complex logical reasoning is needed to answer the questions.  \nCompared to the existing works, this paper is more extensive in its investigation of the types and patterns of logical reasoning needed for QA. They made sure to consider all different kinds including 25 inference rules.  The dataset can serve as a source of both evaluation and fine-tuning. The results are more detailed with respect to the difficulty of various reasoning rules and examined for a variety of language models."
            },
            "weaknesses": {
                "value": "\u2014The work is incremental in extending the existing benchmarks and providing more results of LLM evaluations on logical reasoning. \n\u2014Sometimes, I am not sure if the provided conclusions are reliable in general.  For example, being more correct in NO answers might be due to having bias to saying more NOs than Yes\u2019s and not more than that. \nIntuitively, PL should be easier than FOL. The analysis does not bring any insight into why this is not the case with LLMs reasoning. Again the conclusion must be affected by the biases in the data creation. Often, making general conclusions is hard for this synthesized benchmarking practice and from that perspective, this work is not progressive and remains under the drawbacks of similar previous work."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6607/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6607/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6607/Reviewer_sePi"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6607/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699066947094,
        "cdate": 1699066947094,
        "tmdate": 1699636753216,
        "mdate": 1699636753216,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rbh1g1ihZP",
        "forum": "71kocBuhNO",
        "replyto": "71kocBuhNO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6607/Reviewer_1d9G"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6607/Reviewer_1d9G"
        ],
        "content": {
            "summary": {
                "value": "This paper contributes Logic-Bench, a diagnostic dataset for systematic evaluation of logical reasoning capability of large language models. It covers 25 different reasoning patterns including propositional, first-order, and non-monotonic logics. The paper presents an evaluation for different LLMs in both zero-shot and few-shot settings. The authors also showed that LLMs trained with Logic-Bench yield better performance on several logical reasoning datasets such as LogiQA and ReClor."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper contributed a useful diagnostic dataset for evaluating the logical reasoning capability of large language models. \n- It covers a diverse range of logic reasoning types, especially the under-explored non-monotonic logics, which would be beneficial for future research. \n- The evaluation on different LLMs are comprehensive. \n- The paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "1) The main concern is that the dataset is synthetically generated with pre-defined logical rules and templates. Although this gives a comprehensive categorization of the logic types, it also leaves the question of how well it can generalize to real-world logical reasoning tasks? Although in Table 7 the authors showed that fine-tuning T5 with LogicBench leads to better performance, the performance improvement is quite marginal. \n2) A salient drawback is the synthetic nature of the dataset, crafted using predetermined logical rules and templates. This raises the question of its applicability and relevance to real-world logical reasoning challenges. While the authors showed in Table 7 that fine-tuning T5 with LogicBench leads to better performance, this enhancement was relatively slight. \n3) The few-shot learning performance from Table 13 hint at the potential of LogicBench to soon become less challenging. Remarkably, with few-shot prompting, GPT-4 already achieves an accuracy of roughly 90% in PL and FOL and about 85% for NM. \n4) The paper lacks the evaluation on modern open-sourced LLMs such as LLAMA, Alpaca, Vicuna, etc. \n5) Since the dataset is quite synthetic, I assume that when fine-tuning the model on a few examples, the model can achieve quite good performance. It would be beneficial for the authors to report this supervised performance as a reference for the potential upper-bound performance."
            },
            "questions": {
                "value": "- Given the first weakness, could the authors elaborate on:\n  1. Whether high/low performance on LogicBench truly indicates high/low real-world logical reasoning capabilities for LLMs?\n  2. Is there evidence to support the premise that training LLMs on synthetic logic data such as those in LogicBench genuinely enhances their general logical reasoning capabilities?\n- In addition to reporting the break-down performance of each reasoning type, it would be intriguing to see if the model has the capability for cross-type generalization. For instance, when trained exclusively on one reasoning form (like PL), can the model generalize effectively to another, such as FOL?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6607/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699245601523,
        "cdate": 1699245601523,
        "tmdate": 1699636753099,
        "mdate": 1699636753099,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0FNuxxOnhP",
        "forum": "71kocBuhNO",
        "replyto": "71kocBuhNO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6607/Reviewer_5vLY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6607/Reviewer_5vLY"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a new benchmark called LogicBench into the field of LLM reasoning. It consists of synthetically generated examples in natural language that follow a set of templated formulas derived from 3 different branches of deductive logic (propositional/first-order/non-monotonic). The examples are categorized into the various inference rules they are derived from and there is a 3 step generation procedure described. 2 sets of experiments are done: Evaluations on recent strong LM's are used to claim that LM's perform poorly on many of these structures, and the further evidence of the datasets utility is to show that it can be used as a training set for improving performance of downstream reasoning tasks."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "First of all, there is a sore need for better benchmarks that can help research in advanced LLM reasoning progress further. This dataset is aimed at that need, and in at least one sense it makes a useful contribution: it adds more complex reasoning schemas that  previously have not been used, especially on the non-monotonic side.  Non-monotonic reasoning is severely understudied in LLM's but it is a crucial part of human-like reasoning, so we will need more resources on this topic. \n\nThe rigorous characterization of the reasoning types by the inference rules used can be helpful in downstream uses and analysis. I also appreciate the use of characterizations of NM taken from the classical literature which have been under-used by modern NLP.\n\nThe variation generation part of the generation pipeline is a good idea and helps make the evaluation more robust. \n\nThe extra set of experiments showing that LogicBench(Aug) can be helpful in multi-task training of logic tasks is a useful conclusion.\n\nPerformance on some of these categories seem to go down with larger model, which if they hold up (see my concern below) would make them a good candidate for Inverse Scaling [1]. \n\n\n[1] https://arxiv.org/abs/2306.09479"
            },
            "weaknesses": {
                "value": "Notwithstanding my #1 strength above, I have to say that I have mixed feelings about the overall utility of this work to reasoning. In a sense, LogicBench is a sort of advanced version of ProofWriter type datasets, and so while I think it will have some marginal utility in the field. it also takes it in the wrong direction in a sense (in my opinion) by focusing on a set of clearly delineated but complex and unusual inference steps, whereas advanced \"human-like\" reasoning for LLMs will require doing multiple steps of reasoning in a very complex and realistic context where there are multiple sources of ambiguity to contend with. Examples derived from templated inference rules don't solve this problem. \n\nMy biggest concern is that the results of GPT-4 on even the simpler PL problems don't even beat a majority baseline of 50%, which seems very unexpected and possibly a bug of some kind. The authors do not even mention this surprising finding, which makes me wonder if I have misunderstood the experiments (Each task is binary Y/N, so random should be 50% ?)\n\nWith a complex benchmark like this, one generally hopes that there have been systematic checks for the integrity and reliability of the benchmark as having sensible answers and tracking the phenomenon to be measured. The authors only offer a vague assurance in the end of sec 3.3 without concrete evidence; i would love to have seen something like a small-scale human eval  on a subset to confirm the validity of the benchmark. \n\nOverall the paper is fairly clear in what it is doing, but there are places where the authors are vague, esp. sec 4.3 first section. Most of the points in this section aren't big surprises."
            },
            "questions": {
                "value": "1. You say this is the first benchmark for NMR, but there have been others e.g. BoardGameQA [1]\n\n2.  Table 2, i think the definition of EI is backwards.\n\n3. It seems when we generate natural language instantiations of each rule, we expect 'if' to always be treated as material implication, but it is well known that depending on the context 'if' can be interpreted in many other ways e.g. as a subjunctive, so it may be the case that the  more natural interpretation of a particular context sentence generated by the pipeline is such. Is this a problem for the reliability of the benchmark?\n\n4. Another surprise for me is that the PL benchmarks do worse with GPT-4 than NM, which again defies expectations. How do authors explain this?\n\n5. Is LogicT5 pre-trained from scratch on LogicBench(Aug) using the T5 model recipe or do you train on LogicBench starting from a pretrained T5-Large checkpoint? \n\n6. sec 3.2.1: the purpose of this step is to generate single atomic propositions. it seems we do this by generating entire triples for particular inference steps (like Modus Tollens) and then only extracting the individual atomic propositions. Why this complicated process?  \n\n\n[1] https://arxiv.org/abs/2306.07934"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6607/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699327861445,
        "cdate": 1699327861445,
        "tmdate": 1699636752959,
        "mdate": 1699636752959,
        "license": "CC BY 4.0",
        "version": 2
    }
]