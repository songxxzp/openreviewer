[
    {
        "id": "zZruTKjQSe",
        "forum": "SeiL55hCnu",
        "replyto": "SeiL55hCnu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6344/Reviewer_bkvJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6344/Reviewer_bkvJ"
        ],
        "content": {
            "summary": {
                "value": "The paper explores and proposes an interleaved generation framework based on prompting large-language models (LLMs) and pre-trained text-to-image (T2I) models. Additionally, it attempts to introduce an automated evaluation scheme based on LLMs for assessing Entity Consistency and Style Consistency."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper presents a interleaved generation framework based on prompting large-language models (LLMs) and pre-trained text-to-image (T2I) models, which includes user query composition, text generation, adding global context.\n\nThe paper also explore to present an LLM-based evaluation strategy for assessing interleaved content generation in two aspects: Entity Consistency Evaluation and Style Consistency Evaluation."
            },
            "weaknesses": {
                "value": "1. novelty.  The major concern with this paper is its excessive reliance on the APIs of existing models, GPT-4 and SDXL. The method appears more like a prompt engineering approach, devoid of the need for fine-tuning models, analyzing network structures, or delving into training strategies. It lacks the provision of novel insights, and from my perspective, this paper resembles more of a technical report than an academic contribution.\n\n2. Entity Consistency and Style Consistency.  Indeed, Entity Consistency and Style Consistency are two crucial challenges in Storytelling generation. However, this work attempts to address these challenges merely using simple prompt description constraints, which appears to be a weak approach. Recently, some works[1] [2] [3] based on personalized LORA seem to offer a more reasonable solution for tackling consistency issues.\n\n3. GPT4-version for evaluation. The proposed evaluation method appears to be quite rough and non-standardized. It seems to involve directly inputting the generated content into GPT-4 and asking whether it is reasonable or not, which is overly simplistic and engineering-oriented. Furthermore, the authors have not provided evidence for the validity of this method and have not offered new insights. At the very least, evaluations based on relevant personalized methods [1] [2] [3][4] would be necessary.\n\n[1] Gu, Yuchao, Xintao Wang, Jay Zhangjie Wu, Yujun Shi, Yunpeng Chen, Zihan Fan, Wuyou Xiao et al. \"Mix-of-Show: Decentralized Low-Rank Adaptation for Multi-Concept Customization of Diffusion Models.\" arXiv preprint arXiv:2305.18292 (2023).\n\n[2] Chen, Xi, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. \"AnyDoor: Zero-shot Object-level Image Customization.\" arXiv preprint arXiv:2307.09481 (2023).\n\n[3] Guo, Yuwei, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. \"Animatediff: Animate your personalized text-to-image diffusion models without specific tuning.\" arXiv preprint arXiv:2307.04725 (2023).\n\n[4] Ruiz, Nataniel, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. \"Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation.\" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 22500-22510. 2023."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6344/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6344/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6344/Reviewer_bkvJ"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6344/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698047854443,
        "cdate": 1698047854443,
        "tmdate": 1699636699012,
        "mdate": 1699636699012,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "eHCm5a3QcW",
        "forum": "SeiL55hCnu",
        "replyto": "SeiL55hCnu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6344/Reviewer_d9kj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6344/Reviewer_d9kj"
        ],
        "content": {
            "summary": {
                "value": "This paper unveils the OpenLEAF framework, an inventive concoction aimed at catalyzing the synchronous generation of image-text content. OpenLEAF meticulously interweaves Large Language Models (LLMs) with Text-to-Image (T2I) models, crafting sequences enriched with coherence and quality. The inclusion of a benchmark dataset paired with a robust evaluation technique augments the paper\u2019s appeal, cultivating a nuanced evaluative lens towards generated sequences."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(1) The paper presents a unified solution, offering a fresh perspective in the sphere of open-domain interleaved image-text generation.\n\n(2) OpenLEAF emerges as a cohesive framework, harmonizing the strengths of LLMs and T2I models to birth sequences radiant with quality and coherence.\n\n(3) The authors enrich the evaluation by incorporating a benchmark dataset and a fortified evaluation methodology, enhancing the objectivity and comprehensiveness of sequence evaluations."
            },
            "weaknesses": {
                "value": "(1) The paper's architectural foundation seems somewhat pre-ordained, leveraging predefined templates to navigate the realms of interleaved image-text generation. This approach echoes the contours of prompt engineering, utilizing well-established technical components like GPT-4 and SD-XL, making the technical contributions seem somewhat restrained and not profoundly innovative.\n\n(2) A shadow of vulnerability seems to cloak the proposed mechanisms aimed at ensuring entity and style consistency. The reliance on a diffusion-based image generation model brews uncertainties regarding the model\u2019s ability to consistently generate accurate and diversely styled results. The framework appears slightly handicapped in addressing or rectifying failures in such contexts, potentially moderating its utility.\n\n(3) The paper\u2019s experimental sections seem cluttered with multiple example cases, muddying the clarity of the presented insights. A thirst remains for a more enriched quantitative evaluation coupled with a nuanced exploration of failure scenarios to bolster the paper\u2019s analytical depth.\n\n(4) Some of the technical details are not clear. In User Query Composition, how to apply the Controls and ensure it would work (truly control the generated contents to follow such structure according the Controls). Is it possible that the model (GPT4) would not follow such templates to generate incorrect text results?\n\n(5) There are minor typographical errors. For instance, inconsistencies like color discrepancies (blue \"{\" and black \"}\") in Fig. 2 (b) subtly detract from the visual clarity.\n\n(6) The reliability of the BingChat evaluation in certain scenarios is doubtful. And a nuanced discussion or analytical dissection of these contexts seems conspicuously absent."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6344/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698780032594,
        "cdate": 1698780032594,
        "tmdate": 1699636698866,
        "mdate": 1699636698866,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "M5M6a1kqlW",
        "forum": "SeiL55hCnu",
        "replyto": "SeiL55hCnu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6344/Reviewer_vJS3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6344/Reviewer_vJS3"
        ],
        "content": {
            "summary": {
                "value": "Open-domain interleaved image-text generation is the task where images and text are generated in an interleaved and coherent fashion. The paper introduces OpenLEAF by leveraging LLM (uses GPT-4) and T2I (uses SDXL) models. Generation instructions with examples are passed to GPT-4 for in-context learning, along with the user prompt and control sentences for counts.\n\nThe interleaved content between image tags in response is used by SDXL to generate an image. Global context is passed to the T2I model for appearance and style consistency. This task is natural for visual story telling. \n\nResults (10 image pairs from generated interleaved results) are evaluated for consistency using BingChat whose effectiveness was validated through human evaluation and LLM analysis. The new benchmark contains 30 input queries spanning diverse tasks such as visual instruction generation (10), story generation (10), story rewriting (5), webpage/poster generation (5)."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The problem is interesting and important. Solution is simple."
            },
            "weaknesses": {
                "value": "Experimental results are not comprehensive. Size of data is very small. No ablation studies. No comparison with baselines. This is really ad hoc prompt engineering for in-context learning (a corollary). The paper really highlights the power of GPT-4."
            },
            "questions": {
                "value": "Figure 1: Graphical Story Rewriting does not seem to have the right results (seems to be from a different story). Are these cherrypicked as well?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6344/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698810296594,
        "cdate": 1698810296594,
        "tmdate": 1699636698753,
        "mdate": 1699636698753,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "if7rTMKSez",
        "forum": "SeiL55hCnu",
        "replyto": "SeiL55hCnu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6344/Reviewer_qwLm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6344/Reviewer_qwLm"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a framework that leverages ChatGPT and SDXL to address open-domain interleaved image-text generation. Additionally, the paper investigates the ability of large multi-modal models to assess the consistency of entities and styles in open-domain interleaved image-text sequences. The experiments provide evidence of the effectiveness of the proposed framework and the evaluation capability of LMM."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The writing is clear and easy to follow.\n\n2. The authors investigate the open-domain interleaved image-text generation task and explore the Language Model Metric's (LMM) assessment ability for this task.\n\n3. The authors propose a framework for adopting existing models, such as ChatGPT and SDXL, to address the interleaved image-text generation task."
            },
            "weaknesses": {
                "value": "1. Basically, the paper introduces a framework for the interleaved image-text generation task, primarily by combining existing models, such as ChatGPT and SDXL. Given this, the method may not exhibit strong technical novelty.\n\n2. The proposed method may require complex prompt generation to achieve the desired results, which might not be user-friendly for the average user."
            },
            "questions": {
                "value": "Please see above weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6344/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699105788799,
        "cdate": 1699105788799,
        "tmdate": 1699636698640,
        "mdate": 1699636698640,
        "license": "CC BY 4.0",
        "version": 2
    }
]