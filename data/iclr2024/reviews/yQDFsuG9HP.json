[
    {
        "id": "TsIzNvnhTU",
        "forum": "yQDFsuG9HP",
        "replyto": "yQDFsuG9HP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4441/Reviewer_xCTh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4441/Reviewer_xCTh"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors design a new human animation framework using a denoising diffusion probabilistic model, leveraging bidirectional temporal modeling and a denoising diffusion model to enhance temporal coherence and reduce artifacts in the generated animations. They proposed a bidirectional temporal diffusion model that can generate temporally coherent human animation from random noise, a single image, or a single video. They introduced a feature cross-conditioning mechanism between consecutive frames using recursive sampling to enable local Embed action background information in a globally consistent manner."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The structure of the article is reasonable, clear, and easy to understand. The design of pictures and tables is rigorous.\n2. The method section and experiment settings are introduced in the details."
            },
            "weaknesses": {
                "value": "- Most of the references are before 2022, and there are no latest research results in 2023.\n- Too many arXiv papers are cited in the references. The publication information of the article should be indicated.\n- The paper mentions three tasks of generating human animation from a single noise, a single image, and a single video, but the quantitative results only include the single image animation.\n- More quantitative results and visual examples should be added."
            },
            "questions": {
                "value": "Please see the questions above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4441/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698631733681,
        "cdate": 1698631733681,
        "tmdate": 1699636419260,
        "mdate": 1699636419260,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JB0EDAt1GV",
        "forum": "yQDFsuG9HP",
        "replyto": "yQDFsuG9HP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4441/Reviewer_rYhA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4441/Reviewer_rYhA"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to generate temporally coherent human animation from a single image, a video or a random noise. The authors propose the bidirectional temporal modeling to improve the temporal consistency, which is different from existing unidirectional formulation. Specifically, a diffusion based generative network is designed to decode the human appearance in both forward and backward direction, where  intermediate features are cross-conditioned over time. Experimental results on different generation tasks show better performance of the proposed method compared with existing unidirectional based methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper proposes a novel human animation framework, which models the human appearance bidirectionally to improve the appearance consistency within a video sequence.\n2. The proposed method achieves better performance and generalization capacity compared with existing works."
            },
            "weaknesses": {
                "value": "1. The results in Table 2 are not consistent with the reported results in MDMT. It would be better to explain the reason.\n2. The main objective of the paper is to improve the temporal appearance coherence of the generated video sequence. The reviewer is wondering how is the proposed bidirectional modeling compared to the texture inversion [A, B] technique in keeping the appearance consistent. \n[A] Nataniel Ruiz et al. DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation.\n[B] Rinon Gal et al. An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion.\n3. Related works on diffusion model based video generation are missing, such as NUWA, Make-A-Video, MagicVideo and so on, especially discussions on how these works enforce the temporal consistency.  \n4. The intuition on how the proposed bidirectional temporal modeling solves the motion-appearance ambiguity problem needs to be further explained. \n5. Typo in the line above Eq.(4), should be p(y_(t-1)/y_t) p(y_(t-1)/p_t)?"
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4441/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4441/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4441/Reviewer_rYhA"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4441/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698750662284,
        "cdate": 1698750662284,
        "tmdate": 1699636419164,
        "mdate": 1699636419164,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "N9FkmsruDB",
        "forum": "yQDFsuG9HP",
        "replyto": "yQDFsuG9HP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4441/Reviewer_mGPz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4441/Reviewer_mGPz"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel method for human animation from a single image, a video or a random noise. \n- The main contribution is bidirectional temporal modeling which can generate temporally coherent videos.\n- Both qualitative and quantitative results show the effectiveness of proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ The paper is well written and easy to understand.\n+ The bidirectional temporal modeling is novel and effective in improving the quality of generated videos.\n+ The proposed bidirectional diffusion process to overcome overfitting and the bidirectional attention block are both reasonable.\n+ The experiments are sufficient and demonstrate the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "- The datasets used for experiments are simple because the background is white. It is not clear how the proposed method works on real world scenario where the background is complicated.\n- Table 1, tOF does not follow the format of other metrics."
            },
            "questions": {
                "value": "- How does the paper compare with first order motion model (Neurips 2019) which has been a strong baseline in image animation?\n- How long does it take to generate a video in the experiments? Is it time consuming using a bidirectional diffusion model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4441/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698818953265,
        "cdate": 1698818953265,
        "tmdate": 1699636419098,
        "mdate": 1699636419098,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "eMevanpDZL",
        "forum": "yQDFsuG9HP",
        "replyto": "yQDFsuG9HP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4441/Reviewer_VxE3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4441/Reviewer_VxE3"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a method that generates animated human videos. Depending on the training protocol, the proposed method can generation human animations either unconditionally, or conditionally from images (single image animation) or videos (Person-specific animation). Compared to previous works that relies on autoregressive generation of frames, this paper proposes a bidirectional diffusion model, achieving better quality. The bidirection model takes two noisy frames as input, denoising one of them according to the input embedding that controls the direction.\nTo facilitate the evaluation of the model, a synthetic human animation dataset containing different characters having the same motion is built."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The proposed method outperforms prior art on a significant number of benchmarks.\n* The paper also designed a new, high quality synthetic dataset for better evaluation of the model. This dataset, if released, can be of great help to future works."
            },
            "weaknesses": {
                "value": "* The proposed bidirectional model is not new. It closely resembles a video diffusion model with temporal cross attention -- a standard technique used in video generation.\n* The writting of the paper could be improved. For example, it is unclear how the two directions of the bidirectional model is combined during inferenced. This information is very important for the understanding of the model."
            },
            "questions": {
                "value": "* The BTU-Net described in the paper has a temporal window of 2. Would increasing the number of frames further improve the performance? \n* How is the proposed model inferenced? Do you take the average of the results from both directions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4441/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4441/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4441/Reviewer_VxE3"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4441/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699267325047,
        "cdate": 1699267325047,
        "tmdate": 1700689926885,
        "mdate": 1700689926885,
        "license": "CC BY 4.0",
        "version": 2
    }
]