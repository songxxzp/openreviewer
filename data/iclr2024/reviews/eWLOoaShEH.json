[
    {
        "id": "BfTSVvzF6Q",
        "forum": "eWLOoaShEH",
        "replyto": "eWLOoaShEH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4380/Reviewer_PbML"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4380/Reviewer_PbML"
        ],
        "content": {
            "summary": {
                "value": "The authors argue that an RL agent should use language to predict the next state of the world, which will empower them with the ability to understand the world and thus generate a better policy, instead of directly learn to map language into actions. They propose to build a world model that can predict future language, video and rewards, and demonstrate that training an agent with the world model achieves better performance over other baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The motivation is interesting and convincing. The large language models learn rich knowledge about the world by only predicting the next word, so it is reasonable to hypothesize that utilizing language for future prediction is a better way to help agent understand the world.\n2. Experimental results show that the proposed method outperforms the baselines."
            },
            "weaknesses": {
                "value": "Although the motivation is promising, the method and experiments do not support the claim.\n1. It is confusing that the authors use a multimodal model including both text and images to demonstrate the idea of using language to model the world. Images also convey general knowledge and describe the state of the world, then why can't we also model the world with images / videos? The authors should provide more evidence to demonstrate the unique importance of language to support their claim.\n2. The method proposed in this paper is quite like the Dreamer V3 model [1] with additional text input. In Dreamer V3 paper, they have already demonstrated the effectiveness of their method, and the authors seem to simply apply it on environments that include text. Then, how to clarify that the improvements come from the the model architecture itself or the text part? There are no experiments to demonstrate this. Notice that the author even don't compare with other model-based methods that are more similar to their proposed method, although they claim they compared with them in the introduction.\n\n[1] Hafner et al. Mastering Diverse Domains through World Models. arXiv 2023."
            },
            "questions": {
                "value": "The paper mentioned that at one time step only one text token will be included in the observations and the model output. I don't quite understand the setting here. If this is the case, then the setting is quite limited and it also conflicts with the example \"I put the bowl away\" you use in the introduction?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4380/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4380/Reviewer_PbML",
                    "ICLR.cc/2024/Conference/Submission4380/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4380/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698214444113,
        "cdate": 1698214444113,
        "tmdate": 1700595417706,
        "mdate": 1700595417706,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "A9hC87HSoY",
        "forum": "eWLOoaShEH",
        "replyto": "eWLOoaShEH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4380/Reviewer_PggN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4380/Reviewer_PggN"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a conditional generative model that aligns both image frames and textual instruction tokens (one at a time) to produce multimodal future representations that can encompass visual frames, textual tokens, as well as motor actions, for controlling an agent in an environment.\nThe proposed method is claimed to align the visual-linguistic representations better, while encouraging the models to understand the world-dynamics in a generative modeling manner.\nThe method is tested on four simulated embodied environments where the agents follow certain language instructions, where performance gains are reported against two off-policy RL baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The observed multimodal alignment mechanism is interesting and with experimental justification.\n- The overall proposed method is neat, where the generative mechanism is a sound and interesting idea to model the visual-linguistic dynamics of the work.\n- Consuming all modalities in one model as conditional generative models is neat.\n- The paper is well written and easy to follow."
            },
            "weaknesses": {
                "value": "- The title is a bit over-claimed, in the sense that the proposed model is still learning to model \u201cone environment\u201d at a time, particularly for the action dynamics as multimodal representation generation. At least an experiment or novel method is required to learn to model some worlds (environments) and generalize to a held-out test world \u2013 this would justify the \u201cmodeling the world\u201d parts of the claims.\n- While claimed to be flexible, in many applications, the instructions of a task will only take place at the beginning of the episode while the rest is the robots\u2019 job to accomplish the instructed tasks, where the proposed multimodal alignment will only be performed from the beginning few frames of the episode. How does the proposed method work under such conditions? E.g., how would the method benefit from such an alignment in environments such as ALFRED [1] or TEACh [2]?\n- In Section 4.4, the performance of the actual SOTA models need to be reported as well, even if the proposed method is inferior to them. There are reasons why modularization and use of certain foundation models is beneficial in these long horizon complex (at least closer to) real world tasks.\n- The environments, if at all except for navigation, are all quite toy-ish, where the visual observations are of fairly low fidelity. Since the proposed method heavily relies on the future representation predictions, examining the method on more realistic embodied environments would strengthen the work more.\n\n[1] Shridhar, Mohit, et al. \"Alfred: A benchmark for interpreting grounded instructions for everyday tasks.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020.\n\n[2] Padmakumar, Aishwarya, et al. \"Teach: Task-driven embodied agents that chat.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. No. 2. 2022."
            },
            "questions": {
                "value": "- The proposed method shares some similarities with generative video-guided planning (at least at their high-levels), such as [3]. Could you elaborate more on why this is not an incremental concept on top of these works? (Also these works use supposedly much stronger generative models that can tackle more real-world visual observations.)\n- What if the language instruction has a much shorter token span and the visual frames are much longer? How do they pad to each other or what would be the token used when language is exhausted out?\n- Typos in \u201cFuture Prediction\u201d of Section 3.1 \u2013 \u201cwhih\u201d should be \u201cwhich\u201d.\n\n[3] Dai, Yilun, et al. \"Learning universal policies via text-guided video generation.\" NeurIPS 2023"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4380/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4380/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4380/Reviewer_PggN"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4380/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698362952731,
        "cdate": 1698362952731,
        "tmdate": 1700633882206,
        "mdate": 1700633882206,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RcMbhaELf3",
        "forum": "eWLOoaShEH",
        "replyto": "eWLOoaShEH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4380/Reviewer_X6yV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4380/Reviewer_X6yV"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the challenge of enabling RL agents to comprehend and act based on complex language input. The proposed framework, Dynalang, enhances agent performance by incorporating language signals into the prediction of future states. Notably, Dynalang builds upon DreamerV3 by introducing text tokens into observations at each step. Experimental results demonstrate its effectiveness across various games, such as Homegrid, Messenger, Habbit, and LangRoom, outperforming previous language-conditioned RL baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper addresses a compelling problem by enabling RL agents to understand intricate human language, expanding beyond straightforward task instructions, which is an understudied but important area in RL research.\n\n2. The paper's writing, especially in the introduction, effectively highlights the core problem and how Dynalang provides a solution.\n\n3. The study includes experiments across multiple game environments and consistently demonstrates improvements over existing language-conditioned RL methods."
            },
            "weaknesses": {
                "value": "1. The technical contribution is somewhat limited, primarily differing from DreamerV3 by adding text tokens to observations. A deeper exploration of Dynalang's components and their significance is needed. For example, an ablation study could help clarify the role of the language token in the world model.\n\n2. The paper lacks a detailed ablation study that could validate the importance of each component in Dynalang. Explaining why the language token is necessary, particularly if it only serves as input for the policy network, would provide valuable insights.\n\n3. While the paper explores various game environments, they appear simplistic. Evaluating the method on more challenging games, such as Crafter or Minecraft, would enhance the paper's credibility.\n\nOverall, the paper presents an intriguing idea but requires further validation and clarification to strengthen its foundation. I look forward to discussing these points further in the rebuttal stage."
            },
            "questions": {
                "value": "1. How does the paper ensure that the agent can effectively follow language corrections in the Homegrid environment? Are auxiliary reward signals used to guide agent learning?\n\n2. Could you provide more details on the training process? Is the network trained from scratch, or is the world model pre-trained?\n\n3. Have you considered using an LLM as the core of the world model, given its strong language modeling capabilities?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4380/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4380/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4380/Reviewer_X6yV"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4380/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698838018284,
        "cdate": 1698838018284,
        "tmdate": 1699636410757,
        "mdate": 1699636410757,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OA0XzoldkD",
        "forum": "eWLOoaShEH",
        "replyto": "eWLOoaShEH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4380/Reviewer_bqw2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4380/Reviewer_bqw2"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes Dynalang, an agent that grounds language to visual experience via future prediction."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The writing of this paper is clear, and the descriptions and justifications of the methods are comprehensible."
            },
            "weaknesses": {
                "value": "This paper appears to have limited novelty, seeming more like a combination of existing techniques."
            },
            "questions": {
                "value": "What are the primary challenges addressed by the article? And what are its main contributions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4380/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4380/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4380/Reviewer_bqw2"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4380/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698846147718,
        "cdate": 1698846147718,
        "tmdate": 1699636410674,
        "mdate": 1699636410674,
        "license": "CC BY 4.0",
        "version": 2
    }
]