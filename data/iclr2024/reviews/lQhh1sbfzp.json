[
    {
        "id": "9GhNVAQkTp",
        "forum": "lQhh1sbfzp",
        "replyto": "lQhh1sbfzp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1064/Reviewer_L4n8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1064/Reviewer_L4n8"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes DMS which can find improved structures and outperforms state-of-the-art NAS methods. It demonstrated improved performance on image classification, object detection and LLM."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed method is sound and straightforward."
            },
            "weaknesses": {
                "value": "- The image classification baselines are too weak. baselines should have 90%+ top-1 accuracy.\n- Shown in Table 6, the performance gain is marginal."
            },
            "questions": {
                "value": "- accuracy vs MACs Plot with Table 6. The performance gain seems marginal from the table.\n- explain how this loss_resource is designed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1064/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698608195906,
        "cdate": 1698608195906,
        "tmdate": 1699636032871,
        "mdate": 1699636032871,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Ns26NCriux",
        "forum": "lQhh1sbfzp",
        "replyto": "lQhh1sbfzp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1064/Reviewer_9aAa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1064/Reviewer_9aAa"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a new method called Differential Model Scaling (DMS) for optimizing network architectures, resulting in improved performance across diverse tasks. The study addresses the inefficiency of existing Neural Architecture Search (NAS) methods and proposes DMS as a more efficient alternative for searching optimal width and depth in networks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper introduces a novel, fully differentiable model scaling method, addressing a fundamental challenge in neural network architectures.\n-  The developed search algorithm efficiently identifies optimal network structures, potentially reducing computational costs in architecture search.\n-  The paper is well-written, with a clear and accessible style that enhances understanding, making it broadly accessible to the scientific community."
            },
            "weaknesses": {
                "value": "- The paper does not provide the code or the implementation details of the proposed method, which makes it difficult to reproduce and verify the results.\n- The paper does not explain how the layerwise mask affects the channel-wise mask in the differential topk. It is unclear how the two masks interact and whether they can be jointly optimized in an efficient way.\n- The paper lacks a proper control experiment to isolate the effect of the differential topk from other factors, such as the network architecture, the learning rate, and the data augmentation. It is possible that some of the improvements are due to these factors rather than the proposed method.\n- The paper introduces too many hyperparameters for the differential topk, such as the temperature, the sparsity ratio, and the regularization coefficient. The paper does not provide a systematic analysis of how these hyperparameters affect the performance and the stability of the method. It is also unclear how to choose these hyperparameters for different tasks and architectures.\n- The paper's ablation study is not comprehensive enough to demonstrate the advantages of the proposed method. The paper only compares the differential topk with a uniform scaling baseline, but does not compare it with other model scaling methods, such as compound scaling or progressive scaling. The paper also does not show how the differential topk performs on different network layers, such as convolutional layers or attention layers."
            },
            "questions": {
                "value": "- In Section 3.1.3, you use a moving average to update the layerwise mask. What is the motivation and benefit of this technique? How does it affect the convergence and stability of the optimization?\n- In Section 3.1.3, you adopt an L1-norm regularization term for the channel-wise mask. Why did you choose this norm over other alternatives, such as L2-norm or entropy? How does the choice of norm influence the sparsity and diversity of the channel-wise mask?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1064/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1064/Reviewer_9aAa",
                    "ICLR.cc/2024/Conference/Submission1064/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1064/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698684319897,
        "cdate": 1698684319897,
        "tmdate": 1700650844261,
        "mdate": 1700650844261,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WLt7EyFY3m",
        "forum": "lQhh1sbfzp",
        "replyto": "lQhh1sbfzp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1064/Reviewer_EkDu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1064/Reviewer_EkDu"
        ],
        "content": {
            "summary": {
                "value": "The paper discusses the challenges of manually designing network architectures and the nondifferentiable limitations of existing Neural Architecture Search (NAS) methods. To address these issues, the authors propose Differential Model Scaling (DMS), which offers increased efficiency in searching for optimal width and depth configurations in DNNs. DMS allows for direct and fully differentiable modeling of both width and depth, making it easy to optimize. The authors evaluate DMS across various tasks, including image classification, object detection, and language modeling, using different network architectures such as CNNs and Transformers. The results consistently demonstrate that DMS can find improved network structures and outperforms state-of-the-art NAS methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper proposes a differentiable top-k method, which could be used to select channels or layers in DNNs. The design of differentiable top-k  method is skillful and  meaningful. With normalized importance factors, a learnable parameter $\\alpha$ is used to select elements.   \n2. The whole DMS method merged the task loss and cost loss, With the guidence of cost loss, the DMS can search for efficient models.  \n3. Various experiments demonstrates the superiority of DMS over existing NAS methods. The pruning experiments  presents the method is better than SOTA pruning methods."
            },
            "weaknesses": {
                "value": "1. Different element importance methods are not studied. Some comparisions should be presented to underscore the DMS method.  \n2. More types of cost losses should be considered, such as latency or memory cost. Latency is a superior indicator compare to FLOPs.   \n3. As far as I know, gumbel top-k method is also differentiable, why you develop a new differentiable top-k methd?    \n4. The open source of the code will help to understand the paper."
            },
            "questions": {
                "value": "Please see the weaknesses.   \nBesides, in p.5, the authors demonstrate that \"Intuitively, $c_{i}^{'}$ indicates the portion of $c$ values larger than $c_{i}$.\". Here should be \"smaller\" instead of \"larger\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1064/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698723984723,
        "cdate": 1698723984723,
        "tmdate": 1699636032719,
        "mdate": 1699636032719,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "seAPIHqQa8",
        "forum": "lQhh1sbfzp",
        "replyto": "lQhh1sbfzp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1064/Reviewer_nwwR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1064/Reviewer_nwwR"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new method for differentiable architecture search using a new differentiable top-k operator. Elements (units, blocks, filters, any grouping of parameters) of the network are assigned importance parameters, $c$, that depend on a moving average of the [Taylor importance][taylor]. A learnable threshold $a$ is used to select $k$ elements whose $c$ exceed the threshold. A tradeoff in performance versus capacity is achieved by computing the model resource usage from $c$ and $a$ then constructing a loss function:\n$$\n \\text{loss}_{\\text{resource}} = \\log \\frac{r_c}{r_t} \\text{ if } r_c > r_t \\text{ else } 0\n$$\nwhere $r_c$ is the current resource consumption and $r_t$ is the target.\n\nDuring stochastic gradient optimization, $a$ will be pushed to maintain resource consumption at the desired level, while providing some slack for the model to still learn to perform the task.\n\nThe effectiveness of this method is tested in experiments on image classification on ImageNet, image detection on COCO and a large language model finetuning task.\n\n[taylor]: https://arxiv.org/abs/1906.10771"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The main contributions of the paper are the empirical results: outperforming [ZiCo][] by 0.7% with the same search time (0.4 GPU days). \nThis result appears to be well tested and therefore the paper achieves this goal. Similar results also support the method empirically on COCO and language model finetuning.\n\nThe authors describe the key difference between this work and similar architecture search methods is that it provides a differentiable and direct way to approach architecture search. In other words, other works allow a differentiable selection of which elements to include but do not allow easy optimization of how many elements to include.\n\nArchitecture search is a significant area of research and this paper submits a new method for consideration.\n\n[zico]: https://arxiv.org/abs/2301.11300"
            },
            "weaknesses": {
                "value": "In Section 3.2 the authors mention that this method bears some resemblance to pruning works, \"Our DMS follows a pipeline similar to training-based model pruning.\" This implies that the model should be compared to pruning based methods in experiments. However, the comparisons appear to be made to NAS methods, such as [JointPruning][]. A comparison to state of the art sparse methods, such as [RIGL][] would make the experiments more robust.\n\nThe function they have constructed for optimization is smooth but saturates outside of the active regions illustrated in Figure 2. This may cause vanishing gradient information. Any experiment to investigate whether this happens during training, or why it doesn't happen would be valuable.\n\nThe relationship between resource constraint and the top-k parameters is in the appendix but it's extremely important to the overall algorithm.\n\nThe top-k operator as described leads the reader to assume $k$ would be fixed but in practice it's not constrained and $k$ can be any value. Really it's just a binary mask that has a soft constraint to sum to a low enough value to meet the resource constraints.\n\n[rigl]: https://arxiv.org/abs/1911.11134\n[jointpruning]: https://ieeexplore.ieee.org/document/9516010"
            },
            "questions": {
                "value": "How sensitive is the method to the element importance measure $c$? It's computed as a moving average with a specific hyperparameter. It seems like the gradient estimate of $a$ depends on this being stable.\n\nIn Section 3.2 you say \"Compared with training-based pruning, our method eliminates the need for time-consuming pretraining since we think searching from scratch is more efficient thatn from a pretrained model...\". How does that save resources? Typically pretrained models are available for free, but training one yourself from scratch is extremely expensive? I don't understand what Table 4 means because the rows and colums refer to search and retraining but one can't have both a pretrained model and a model that is retrained?\n\nThe gains over prior architecture search methods seem to be relatively minor, such as 0.7% accuracy on ImageNet. What would be your argument against this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1064/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1064/Reviewer_nwwR",
                    "ICLR.cc/2024/Conference/Submission1064/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1064/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698784321743,
        "cdate": 1698784321743,
        "tmdate": 1700596451379,
        "mdate": 1700596451379,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4KSZ70Ab45",
        "forum": "lQhh1sbfzp",
        "replyto": "lQhh1sbfzp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1064/Reviewer_AHK9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1064/Reviewer_AHK9"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces Differential Model Scaling (DMS) to increase the efficiency of width and depth search in networks. The differential top-k introduces by the method to model structural hyperparameters in direct and differentiable manner lays the foundation of this approach. The method is evaluated fairly exhaustively on different image classification architectures like EfficientNet-B0, DeiT on ImageNet. Furthermore the method is also evaluated on myriad tasks like object detection on COCO and language modelling (with Llama-7B model). The proposed method achieves significant improvements over different NAS baselines and some handcrafted architectures."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The approach presented is very novel and well motivated. \n- Experimental evaluation (across different scales, applications, model variants) is exhaustive. The paper also ablates the initialisation scheme of the architecture thoroughly. The search time comparison between different methods is also provided, thus showing the compute savings of the method. \n- The presentation is clear and the paper is well written.\n- The contribution of the paper is very significant especially since it scales NAS methods to realistic search spaces."
            },
            "weaknesses": {
                "value": "- Search time comparison in some cases seems unfair/confusing (refer to questions)\n- Since the problem is cast as a NAS problem the search spaces used are not the ones very traditional to NAS (refer to questions)\n- The search needs to be repeated for every resource constraint and obtaining a Pareto-Front of objectives might be very expensive (unlike methods like OFA[1]  which directly approximate the whole Pareto-front)"
            },
            "questions": {
                "value": "- Search time comparison -> Since the observation from section 5 show that initialization from pre-trained models is very useful for differential scaling, did the authors include this in the search time computation. If a method relies on pre-trained models, then ideally the pre-training cost is a part of the total cost incurred. Could the authors clarify the intialization scheme used in each of the tables ie. table 1,2,3.\n- In the appendix the authors compare with one-shot methods like OFA [1] . The comparison in my opinion is unfair since the search is performed on different search spaces. Could the authors evaluated the method on the exact same search space as OFA? This would help differentiable the gains of the search-space v/s the method itself? Similarly  could a comparison be made with the AutoFormer [2] by evaluating the method on its exact search space [2]?\n\nI am willing to increase my score if my concerns are addressed as I believe this is a very interesting and impactful work. \n\n[1] Cai, H., Gan, C., Wang, T., Zhang, Z. and Han, S., 2019. Once-for-all: Train one network and specialize it for efficient deployment. arXiv preprint arXiv:1908.09791.\n\n[2]Chen, M., Peng, H., Fu, J. and Ling, H., 2021. Autoformer: Searching transformers for visual recognition. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 12270-12280)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1064/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1064/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1064/Reviewer_AHK9"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1064/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698917950200,
        "cdate": 1698917950200,
        "tmdate": 1699636032562,
        "mdate": 1699636032562,
        "license": "CC BY 4.0",
        "version": 2
    }
]