[
    {
        "id": "ohXOJwn3wT",
        "forum": "WOiOzHG2zD",
        "replyto": "WOiOzHG2zD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5579/Reviewer_9TYY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5579/Reviewer_9TYY"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to achieve open-vocabulary 3D generation by attempting to tackle the challenges of limited 3D data and therefore sparse text annotations, which would limit the model's ability to generalize to open-vocabulary queries. One way to approach the challenge is to add noises to text features to avoid models from overfitting. But it is not trivial to know how much noise is appropriate. Therefore, the authors propose to learn a noisy text fields, which learns the standard deviation of the Gaussian noise that can be added to each text feature. Other than the text-to-image task, this paper also proposes to learn a view-invariant image representation to facilitate better image-to-3D generation. As for the generator part, the authors use a 3D-aware GAN framework, with a GET3D-like generator and a text-3D and text-2.5D discriminator. Qualitative and quantitative experiments are reported to showcase the performance of the proposed method. For the quantitative part, TextFireld3D surpasses baseline methods, including Point-E, Shap-E and GET3D. Visual results also show reasonable generation quality both from text and image conditions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Overall, I like that the authors choose to use 3D-aware GAN instead of diffusion models to tackle the text-to-3D generation problem. GAN has clear advantages over diffusion model in terms of generation speed and smooth interpolation. But for the open-vocabulary text-guided generation task, GAN falls behind diffusion models. Therefore, it is exciting to see a GAN model surpasses diffusion baselines, e.g., Shap-E and Point-E. This paper can service as a strong baseline for the community of text-to-3D generation.\n2. The idea of learning the noisy text field is interesting. Experiments also suggest the effectiveness of this method."
            },
            "weaknesses": {
                "value": "My main concerns are around the experiments of this paper.\n\n1. I would expect to see more baseline methods being compared with. For example, in the ablation study, authors mentioned methods like SDFusion and TAPS3D.\n2. I would suggest also testing with the DreamFusion testing list, which contains over 400 prompts.\n3. For the image-to-3D generation, the evaluation is limited to showcasing two visual results, which is far from enough. I would suggest adding more visual results, especially showing multiple viewing angles of the generated object. Some quantitative evaluation is also expected.\n4. For the visual results, I would like to see more prompts from the DreamField or DreamFusion test sets, which include more challenging and complex examples like concept-mixing. It would help us better evaluate the open-vocabulary ability of the proposed method. Right now, the showcasing prompts only contain one simple concept and can be easily found in Objaverse training set. I understand that concept mixing is super hard given such limited training data. But at least, it is nice to see some failure cases and analysis on the failure modes.\n\nIf authors can provide more evaluations during the rebuttal. I would consider raising the score."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5579/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5579/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5579/Reviewer_9TYY"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5579/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698382257531,
        "cdate": 1698382257531,
        "tmdate": 1700684310597,
        "mdate": 1700684310597,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "x6NHyvAQFS",
        "forum": "WOiOzHG2zD",
        "replyto": "WOiOzHG2zD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5579/Reviewer_oUD4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5579/Reviewer_oUD4"
        ],
        "content": {
            "summary": {
                "value": "This paper works on text-to-3d shape generation. The paper observes that the current text-to-shape generation methods are usually V-L Optimized or 3D supervised. These methods either suffer from the problem of long-optimization or restricted open-vocabulary capability. To resolve these problems, the paper proposes TextField3D, which generates 3D shapes in real-time, taking both open-vocabulary text prompts and image as input. Specifically, the method adopts a NFGen model, that maps a single text prompt into a noisy text field. This noisy text field enables more open-vocabulary text prompts than single category name or template-generated text prompts. It also proposes an NTFBind model, which maps images from any view into a view-invariant image feature in the noisy text field. The noisy latent feature from the noisy text field is then fed into a conditional generative model. The method also adopts a 2.5D and a 3D supervision that ensures the generated shape has high-quality texture and geometry. Experiment shows the proposed method generates shapes of higher quality than some V-L optimized and 3D supervised methods. It also shows the effectiveness of the NTFGen and NTFBind modules in design choice."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The paper is clearly written. The motivation is clear to me - design a real-time supervised method whose performance is competitive with VL-optimized methods. \n+ The NTFGen model shows that it is able, to some extent, to increase the expressiveness of a single text prompt. \n+ The NTFBind model shows that it is better at aligning image features from any views to text features. \n+ The generated shapes visually seem to have better quality and texture than the compared methods thanks to the supervision from 2.5D and 3D."
            },
            "weaknesses": {
                "value": "This paper fails to convince me of the effectiveness of its major component in the following aspects:\n+ Shape Diversity is limited by the training dataset. Quoting from the original paper - \"With limited 3D data, can we train a real-time generator that is equivalent to V-L optimized methods\", I think this is impossible considering the method is training with a relatively small scale 3D shape dataset compared with VLMs. VL-optimized methods clearly can generate synthesized imagined shapes, like a chair with the shape of an avocado, but given the qualitative examples that the authors provide, this method seems not able to generate imagined shapes. Even though I don't think this is a major drawback of the proposed method, I think this claim is faulty. \n+ Open-vocabulary capability - The open-vocabulary capability is the major claim of this paper. However, in the qualitative experiment section, the paper only provides very simple prompts, like category names, adjective nouns, or a phrase with two nouns. I think these simple phrases are not complicated enough to prove the open-vocabulary capability of the method, especially considering the method is training with complicated enough captions generated by BLIP-2 or MiniGPT4. I hope the authors can provide more results from complicated text prompts as in the captions generated.\n+ View-invariant experiments. The paper claims that the NTFBind model produces a view-invariant feature, but the experiment provided is not strong enough to prove the point.  The experiment uses image features across views and image features across ShapeNet categories to prove that image features across views are more reassembled than features across categories. I think this setting is not strong and persuasive enough. A better experiment setup would be comparing features across views with features across instances in the same category. See CLIP-NeRF[1] Figure 2 for more details. \n+ Comparison. Though the paper compared with VL optimized methods and 3D supervised methods, it didn't directly compare with the TAPS3D, which has the same training setting as the proposed method. Both of them methods use an image captioning model to augment text prompts and work on a 3D-generated model. Though the paper provides an ablation study that replaces the major component to the TAPS3D component, I wonder if it will outperform the TAPS3D original method. \n[1] CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields"
            },
            "questions": {
                "value": "+ Some minor questions:\n1. For noisy text latent code, the paper uses a learned noise, which is referred to as \"dynamic noise\". I wonder how the method performs with a non-dynamic noise, which could set \\sigma to a static number. \n2. The noting of L_{img} and L_{txt} looks like two different types of loss, but they are actually the same type of loss. Changing the namings to make them more consistent would be better for reading.\n3. Textured Mesh Generator. Are they training from scratch, or training from the pre-trained GET3D?\n4. Sillhoutte loss. The silhouette loss in equations (4) and (5) is not introduced clearly. For a first-time reader, it might be a little confusing."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5579/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5579/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5579/Reviewer_oUD4"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5579/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698706230301,
        "cdate": 1698706230301,
        "tmdate": 1700683708921,
        "mdate": 1700683708921,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GMstJ1MHgJ",
        "forum": "WOiOzHG2zD",
        "replyto": "WOiOzHG2zD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5579/Reviewer_Fjdj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5579/Reviewer_Fjdj"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors present TextField3D, which is a conditional 3D generative model that enhances text-based 3D data generation by injecting dynamic noise into the latent space of text prompts, creating Noisy Text Fields (NTFs). This technique allows for the mapping of limited 3D data to a broader range of textual latent space, enhanced by the NTFs. The authors propose two modules to facilitate this process: NTFGen, which models the general text latent code in noisy fields, and NTFBind, which aligns view-invariant image latent code to the noisy fields, aiding image-conditional 3D generation. The model is guided through the conditional generation process in terms of both geometry and texture by a multi-modal discrimination setup, consisting of a text-3D discriminator and a text-2.5D discriminator. TextField3D is highlighted for its large vocabulary, text consistency, and low latency, positioning it as an advancement over previous methods in the field."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper successfully expands the GAN-based GET3D framework to handle extensive vocabulary datasets, achieving results on par with or surpassing those of diffusion-based models like point-e and ShapE. This marks a significant step forward for large-vocabulary feed-forward generative models.\n- Since text and 3D are not one-to-one mapping, , the introduction of Noisy Text Fields and their corresponding modules seem to be reasonable to me.\n- The results, both qualitative and quantitative, look diverse and of a relatively good quality compared to other feed-forward models.\n- The ablation studies are clear and comprehensive, providing detailed insights into the impacts of various modules, discriminators, and choices in noise range."
            },
            "weaknesses": {
                "value": "- My main concern is the potential overfitting problem. In Figure 12 and 13, certain prompts (e.g., \"A beer can\", \"A wooden crate\", and \"A cardboard box with graffiti\") generate unusually detailed outputs, showing a much higher level of details than others. Based on my experience,  the training dataset likely contains very similar examples. I am interested in understanding how the authors have addressed and evaluated the risk of overfitting associated with their method."
            },
            "questions": {
                "value": "1. I'm wondering how is the FID score calculated under the text-conditioned setting?\n2. Can you provide some more examples of the 9-shot experiments?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5579/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698832573589,
        "cdate": 1698832573589,
        "tmdate": 1699636573975,
        "mdate": 1699636573975,
        "license": "CC BY 4.0",
        "version": 2
    }
]