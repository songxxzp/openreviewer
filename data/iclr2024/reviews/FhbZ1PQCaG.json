[
    {
        "id": "7fB5DMCcN2",
        "forum": "FhbZ1PQCaG",
        "replyto": "FhbZ1PQCaG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission277/Reviewer_NgFj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission277/Reviewer_NgFj"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a memory module for Transformer-based architecture that can store and retrieve information for multiple Reinforcement Learning (RL) tasks. Memory update modifies existing information in the memory matrix based on the input sequence and the attention mechanism. Memory retrieval accesses the memory matrix based on content-based addressing. This memory module is integrated with a pre-trained Decision Transformer ((GPT2 architecture) for multi-task RL settings, coupled with a low-rank adaptation fine-tuning method (LoRA). The paper examines the proposed method on multi-game Atari and meta-world object manipulation benchmarks, showing consistent improvements in terms of generalization, adaptation and scaling."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The general motivation is good, a memory module will enhance memorization and can be potentially beneficial for the multi-task RL setting\n- The experiments show good results with clear improvement gains"
            },
            "weaknesses": {
                "value": "- The novelty is limited. The main idea is to integrate an external memory for Transformers to improve memorization and reasoning. There have been many works along this line of research.  The proposed memory read and write mechanism is also straightforward and heavily based on the mechanism (content-based attention, add, erase, ...) of NTM and DNC. \n- It is unclear why the proposed memory has advantages over other Transformers with internal/external memory or even just long-range attention [1,2,3,4,5]. More explanations are required in the method and more baselines need to be included in the experiments. In particular, the current baselines have only RMDT as a memory-based Transformer, which is not enough, especially when other memory-based Transformers can be adapted easily to offline RL in the same way as the Decision Transformer. Also, retrieval-based RL [6] can be a good baseline as well to highlight the benefit of internal memory. \n- The writing lacks sufficient background content. The paper should provide more details on Decision Transformer and offline RL setting.\n- Although the main message is about memory, there is no experimental analysis of how the memory module helps improve performance. Please consider ablation studies and visualization to prove that memory is the real contribution (not representation and LoRA tricks). There are some results in Appendix, but they are not helpful (see Questions for more discussion)\n- The related work section should include more memory-based Transformer papers \n\n[1] Rae, Jack W., Anna Potapenko, Siddhant M. Jayakumar, and Timothy P. Lillicrap. \"Compressive transformers for long-range sequence modelling.\" arXiv preprint arXiv:1911.05507 (2019).  \n[2] Martins, Pedro Henrique, Zita Marinho, and Andr\u00e9 FT Martins. \"$\\infty $-former: Infinite Memory Transformer.\" arXiv preprint arXiv:2109.00301 (2021).  \n[3] Wang, Weizhi, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and Furu Wei. \"Augmenting Language Models with Long-Term Memory.\" arXiv preprint arXiv:2306.07174 (2023).\n[4] Wu, Yuhuai, Markus N. Rabe, DeLesley Hutchins, and Christian Szegedy. \"Memorizing transformers.\" arXiv preprint arXiv:2203.08913 (2022).  \n[5] Wang, Sinong, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. \"Linformer: Self-attention with linear complexity.\" arXiv preprint arXiv:2006.04768 (2020).  \n[6] Goyal, Anirudh, Abram Friesen, Andrea Banino, Theophane Weber, Nan Rosemary Ke, Adria Puigdomenech Badia, Arthur Guez et al. \"Retrieval-augmented reinforcement learning.\" In International Conference on Machine Learning, pp. 7740-7765. PMLR, 2022."
            },
            "questions": {
                "value": "- Are $w$ in Line 202, 215, and 232 the same?\n- What is the motivation to compute the strength $\\beta$ using attention?  Why do we need to use $\\beta$ in both erasing and adding vectors?\n- Is $t$ in Line 222 the step in the trajectory? Can you provide an algorithm to explain clearly how memory read and write are executed within a trajectory?\n- Is Step 1 Line 187 important? Do you have an ablation study on Step 1? \n- Based on Table 5, it seems that LoRA is the main contributor to your method. Can you have the ablation on LoRA using Atari games? Also ablation study on memory adding and erasing would be helpful. \n- Can you have visualization to show that the memory stores important data and your model actually reads meaningful memory data from the memory? E.g., when taking action, the model refers to a meaningful timestep in the past to support your idea \"think before you act\"\n- Fig. 3 does your method perform well at 10M parameters?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission277/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698110083623,
        "cdate": 1698110083623,
        "tmdate": 1699635953385,
        "mdate": 1699635953385,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kgqnGeWb8s",
        "forum": "FhbZ1PQCaG",
        "replyto": "FhbZ1PQCaG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission277/Reviewer_qzjQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission277/Reviewer_qzjQ"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors introduce a novel agent algorithm called Decision Transformer with Memory (DT-Mem), which incorporates a memory layer between the attention and MLP layers in the Transformer architecture. This addition allows the agent to memorize knowledge in memory, rather than relying solely on learnable parameters. Empirical evidence demonstrates that DT-Mem outperforms existing methods in terms of generalization and can quickly adapt to new tasks through fine-tuning, such as LoRA."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper presents the Decision Transformer with Memory (DT-Mem) model, which incorporates a learnable memory module and demonstrates superior performance in pre-training, generalization, and fine-tuning. It distinguishes itself from related work, such as RMDT, by its ability to learn sequences in parallel training and use an advanced learnable memory architecture based on the NTM model.\n- In section 4, the paper's clarity for the proposed model is commendable, with a well-structured architecture diagram and detailed explanations for each step of inference and training.\n- The evaluation methodology includes several well-designed questions that effectively assess the hypotheses presented in the paper.\n- Comparative evaluation against diverse baselines, including another memory-equipped Decision Transformer model (RMDT), strengthens the paper's contributions.\n- In Figures 3 and 4, comparison with diverse size of MDT is interesting, through this, we can clearly see that the memorization through the neural network parameters is inefficient compared to the implicit learnable memory.\n- The additional experimental results in the appendix are helpful to understand deeply (especially Figure 10 is good)."
            },
            "weaknesses": {
                "value": "- Some explanations in the paper are unclear. For instance, the reference to \"Large Language Model based decision making agents\" needs clarification. It's unclear if this refers to Transformer-based agents or models within the Decision Transformer family.\n- The motivation for this work is not entirely clear. While the paper argues that memorization through neural network parameters can lead to weaker performance in forgetting during training, it remains unclear how this relates to generalization performance for unseen tasks.\n- The paper contains some comments that are difficult to understand, such as the mention of NFT and FT in Figure 5, where no results are presented for these abbreviations.\n- There's a minor typographical error in line 297, where \"we generat\" should be corrected to \"we generate.\""
            },
            "questions": {
                "value": "- Is the memory not initialized throughout the entire training process? Clarifying this point could help readers better understand the novelty of this approach, as memory initialization is typically performed per episode (e.g., NTM).\n- Could you investigate whether MDT can be fine-tuned using the LoRA technique? As LoRA is applicable to the general Transformer architecture, it would be insightful to assess the potential for fine-tuning MDT using this approach.\n- Have you considered testing an external memory-equipped DT? Many recent attempts to incorporate memory utilize a naive appending-style external memory, which lacks the sophistication of models like NTM. Comparing your memory architecture to this version could help highlight its strengths.\n- Expanding the tasks to include those with long-term dependencies, such as MemoryMaze, could be valuable. Given that DT-Mem has a memory module capable of retaining distant knowledge, it may excel in tasks where other DTs, except RMDT, struggle.\n- In line 243, the phrase \"the Transformer module to generate actions that can reduce this value as close to zero as possible\" may benefit from clarification. The objective appears to be maximization rather than reduction, as it involves the sum of rewards.\n- Figure 5 raises questions about the absence of NFT results. Are all results in the plot derived from fine-tuned models?\n- In Figure 6, where you mention the top 3 rollout, could you provide more context or clarification about what this entails?\n\n### Additional Comments\nDT-Mem demonstrates superior performance in pre-training, generalization, and fine-tuning, particularly evident in Figure 10. However, there seems to be a disconnect between the paper's motivation and the observed results. Clarifying the link between implicit memory's ability to mitigate the forgetting phenomenon and the improved generalization and fine-tuning performance would strengthen the paper's alignment and overall impact.\n\n### After reading the author's rebuttal\nWe thank the authors for their efforts to clarify their arguments. I agree with their rebuttal, in particular, the part related to the connection between their motivation and their methodologies. I hope they will try to test the appending-style external memory also, but I am satisfied to their rebuttal, so I increase my score to lean to the acceptance.\n\nTo authors, as I know, there is no prior work for the appending-style external memory + DT, but the external memory equipped agents have been studied actively. I leave some references.\n\nLampinen, Andrew, et al. \"Towards mental time travel: a hierarchical memory for reinforcement learning agents.\" Advances in Neural Information Processing Systems 34 (2021): 28182-28195.\n\nParisotto, Emilio, et al. \"Stabilizing transformers for reinforcement learning.\" International conference on machine learning. PMLR, 2020."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission277/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission277/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission277/Reviewer_qzjQ"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission277/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698735775645,
        "cdate": 1698735775645,
        "tmdate": 1700536624988,
        "mdate": 1700536624988,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0hsU2pQtUT",
        "forum": "FhbZ1PQCaG",
        "replyto": "FhbZ1PQCaG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission277/Reviewer_hmxu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission277/Reviewer_hmxu"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces the memory into the decision transformer. Basically, the memory is a matrix which store the embedding of the transition tuple, and when decision making using decision transformer, the retrieved information from the memory is used to generate the action. Experiments on Atari and Meta-world demonstrate the effectiveness of the proposed methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The motivation of this paper is clear, i.e., inspiring by the human decision making process.\nThe writing of the paper is good, i.e., easy to follow."
            },
            "weaknesses": {
                "value": "The main contribution of the paper is the memory. however, more in-depth investigation of the memory can be conducted. There are some issues about the clarity."
            },
            "questions": {
                "value": "There are several questions I want the author to address during the rebuttal:\n1. Some issues about the clarity:    \na. For Section 3.1. It seems that you focus on offline RL, rather than RL. The two fields have differences. You may need a brief introduction of offline RL, as well as existing methods as baselines, such as DT, MGDT, RMDT, HDT.    \nb. Figure 2 is somehow misleading. Figure 2a, does the memory module is a layer of the transformer? I think they are separated, however, the plot seems that the memory is stacked with the transformer. Figure 2b, the retrieved memory is another memory? I think should be the retrieved experiences, or other terms. Please make the terms unique and clear.\n\n2. Some issues about the technical contributions.  \na. The introduction uses large language model as the motivation, however, even using GPT-2 architecture, the embedding and the tokens I believe is not about words, it should be game-specific tokens. so please using transformer or decoder-only transformer to avoid any confusion.    \nb. It seems that the largest model used in the paper is 50M. Compared with LLM, it still very small. Does LoRA is necessary? LoRA can be used to any models, which is not a technical contribution of this paper. However, that may harm the performance of DT-Mem. So I would suggest just not using LoRA and fine-tuning all the model to focus on the memory part. This can help the reviewer to fully evaluate the importance of the memory.   \nc. About the memory. There are some related methods, neural episodic control (https://arxiv.org/abs/1703.01988) for the writing and lookup. The two methods share many similarities, so maybe add a detailed comparison of the proposed methods and all related methods, so we can fully understand the contributions.    \nd. Still about the memory.  What is exactly the difference between the external memory and the internal memory? Could you provide an example about the two kinds of memories, as well as the advantages of the proposed memory. \n\n3. Some issues about the experiments. I generally think the experiments are sufficient, but with some suggestions: i) can we use the prompting for the DT-Mem? as we know prompting is much easier than fine-tuning. And ii) what is the limit of the internal memory, given the fixed size, i.e., parameters, of the memory?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission277/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698806924660,
        "cdate": 1698806924660,
        "tmdate": 1699635953156,
        "mdate": 1699635953156,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CJzL2HQ0pm",
        "forum": "FhbZ1PQCaG",
        "replyto": "FhbZ1PQCaG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission277/Reviewer_gRQQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission277/Reviewer_gRQQ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes Decision Transformers with Memory, which introduces internal memory mechanism into RL field and improves training efficiency and generalization in both Atari games and meta-world object manipulation tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The experiments are sufficient and the results prove the superiority of this method.\n- The paper is well written."
            },
            "weaknesses": {
                "value": "- The internal memory formulation and some specific details, such as content-based addressing, seems a bit incremental from previous work, the authors should explain the difference more clearly in the method section."
            },
            "questions": {
                "value": "1. Fig. 2(b) is too simple, making it difficult to correspond one-to-one with the steps in the method part. The authors should make the figure more comprehensive and understandable.\n2. There are many papers demonstrating the ideas about internal memory, and the authors should explain the differences with similar methods in more detail in the method section.\n3. Add more analysis about different situations, such as the input misleading by the content stored in the memory (i.e., noise or dissimilar pattern), how does the method eliminates this type of impact."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission277/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission277/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission277/Reviewer_gRQQ"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission277/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699458719287,
        "cdate": 1699458719287,
        "tmdate": 1699635953092,
        "mdate": 1699635953092,
        "license": "CC BY 4.0",
        "version": 2
    }
]