[
    {
        "id": "xmk9BvbTI7",
        "forum": "tcFcKyJgRM",
        "replyto": "tcFcKyJgRM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6350/Reviewer_NxCH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6350/Reviewer_NxCH"
        ],
        "content": {
            "summary": {
                "value": "The work presents their design of a web action agent with LLM that can follow the natural language instruction and perform actions on the website. The agent consists of a high-level planner and a low-level actor, both of which invoke LLM for concrete output given different input context and prompt. The core idea is to decompose the task to achieve higher performance and generalization capacity."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The design of this method is sound and reasonable. \n\nExhaustive details of the prompt and results analysis are presented."
            },
            "weaknesses": {
                "value": "The evaluation is based on too few samples: 45 tasks on MIniWob++, 125 examples of two domains on WebArena, 5 distinct tasks with 20 scenarios on Ariline CRM, and 3 website with 10 searches per site on Live Websites.\n\nGiven the fact that human demonstration is collected to form the prompts to the LLM in HEAP, it should be actually evaluated on more diverse websites instead of fewer websites."
            },
            "questions": {
                "value": "1. Could the author elaborate on the demonstration collection process described in 4.2?\n\n2. What's the purpose of D_{label} on the end of page 4 (Sec 4.2)? How does this dataset utilized in the HEAP?\n\n3. What is the trainable component in the HEAP? e.g. which function / parameters described in Algorithm 1 is learnable? \n\n4. Why the models are not evaluated on the entire benchmarks but only a set of them. \n\n5. What does training size 21 in Table 1 last row mean? The HEAP was shown 21 samples, in which format, to train which part?\n\n6. It really depends on the demonstration collected and the diversity of evaluation cases that whether the benefit claim of \"sample efficient\" and \"generalization\" are sound."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6350/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697759866913,
        "cdate": 1697759866913,
        "tmdate": 1699636700345,
        "mdate": 1699636700345,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xEIR7Ij9Wt",
        "forum": "tcFcKyJgRM",
        "replyto": "tcFcKyJgRM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6350/Reviewer_4BRb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6350/Reviewer_4BRb"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a framework that learns hierarchical prompts from demonstrations for planning high-level tasks and executing them via a sequence of low-level policies. The approach decomposes complex web tasks into a sequence of high-level subtasks, each of which can be then solved by a sequence of low-level policies. This method learns hierarchical LLM prompts for both levels of tasks and policies. The approach was evaluated on a range of increasingly complex benchmarks and the results show that the proposed approach achieves excellent performance compared with existing approaches."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The approach introduces a novel hierarchical approach to prompt LLMs to perform web tasks. Experimental results on various complex web benchmarking datasets show the superiority of the proposed approach."
            },
            "weaknesses": {
                "value": "I recommend moving some implementation details like prompts into the main body to help the reader better understand the work."
            },
            "questions": {
                "value": "Do you label the high-level task plans?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6350/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6350/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6350/Reviewer_4BRb"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6350/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698555713761,
        "cdate": 1698555713761,
        "tmdate": 1699636700229,
        "mdate": 1699636700229,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fgGmSXNhF6",
        "forum": "tcFcKyJgRM",
        "replyto": "tcFcKyJgRM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6350/Reviewer_oBNG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6350/Reviewer_oBNG"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a method HeaP for LLM to perform tasks on the web. It first asks LLM to decompose the task into several steps as a planner, where each step is a call to low-level policies. Then within each low-level policy, LLM is called to predict the next action sequentially. Both the planner and low-level policy execution have prompts constructed automatically by collecting few shot examples from autolabeled human demonstrations. Extensive experiments over several datasets demonstrate the gain over ReAct which does not use hierarchical planning."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is overall easy to read, although some important methodological details like autolabeling and prompt construction are in appendix which makes it hard to read.\n- The experiments are extensive over 4 datasets with many tasks. The gain demonstrated is substantial."
            },
            "weaknesses": {
                "value": "- The idea of hierarchical planning with a high-level planner and low-level policies using LLM has been explored by many previous robotics works e.g. LLM-planner (https://arxiv.org/pdf/2212.04088.pdf and the line of works they cited). Additionally, PaP (https://aclanthology.org/2022.suki-1.8.pdf) and Parsel (https://arxiv.org/pdf/2212.10561.pdf) have also explored similar ideas of prompting LLM to generate a hierarchical plan but implementing low-level planners with programs. Implementing both high-level and low-level planners with LLM prompting has been explored in Decomposed Prompting (https://openreview.net/pdf?id=_nGgzQjzaRy). Considering these previous works, the novelty of this paper is limited to applying existing ideas to web datasets and potentially the technical details of autolabeling from human demonstrations.\n- The low-level policies are manually defined during autolabeling, making the framework limited in flexibility comparing to previous works that allow LLM to generate decompositions freely. \n- The only LLM prompting baseline compared against is ReAct, which demonstrates the benefits of hierarchical planning. However, such benefits have been demonstrated with the prior works mentioned above."
            },
            "questions": {
                "value": "n/a"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6350/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6350/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6350/Reviewer_oBNG"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6350/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698777532611,
        "cdate": 1698777532611,
        "tmdate": 1699636700111,
        "mdate": 1699636700111,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZMHbTgCDhJ",
        "forum": "tcFcKyJgRM",
        "replyto": "tcFcKyJgRM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6350/Reviewer_aRc1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6350/Reviewer_aRc1"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces HeaP, a framework that leverages Large Language Models (LLMs) to decompose complex web tasks into modular sub-tasks. It uses a hierarchical approach, learning high-level task plans and low-level policies from human demonstrations, enabling LLMs to perform web actions effectively. It addresses challenges related to the combinatorially large space of web tasks and variations in web interfaces. Experimental results demonstrate that HeaP outperforms previous methods with significantly fewer training examples on various web tasks and interfaces such as MiniWoB++, WebArena, and a mock airline CRM"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "-Originality: The idea is interesting in the way HeaP leverages hierarchical policies to decompose complex web tasks using a high-level task planner \n into modular  low-level web policies.\n\n-Quality: The paper is quite thorough in its experimental setup as it tests on 4 interesting datasets, including simulated and live websites, to assess the performance of the proposed approach. \n\n-Clarity: The paper is well-written and structured, making it easy for readers to follow and understand the proposed approach\n\n-Significance: The paper addresses a significant challenge in the field of natural language processing and machine learning, which is teaching LLMs to perform web-based tasks which can lead to a huge set of applications"
            },
            "weaknesses": {
                "value": "- The tasks are not that challenging and the results are very weak relative to how powerful the LLM model used here which is GPT-3.5. For example, it seems that the proposed method struggles with book-flight which is a basic constrained task and therefore this method is very far from being deployed in the real world\n\n- Using closed source methods like GPT-3.5 is expensive. I'd be curious to see how this method would perform with open source methods like Llama and Mistral.\n\n- No code was provided to asses and verify the results as well as understand the low level details of how the method is implemented"
            },
            "questions": {
                "value": "Please address the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6350/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699278170804,
        "cdate": 1699278170804,
        "tmdate": 1699636700000,
        "mdate": 1699636700000,
        "license": "CC BY 4.0",
        "version": 2
    }
]