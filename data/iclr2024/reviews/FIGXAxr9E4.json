[
    {
        "id": "6R9JabbavR",
        "forum": "FIGXAxr9E4",
        "replyto": "FIGXAxr9E4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4102/Reviewer_yvnK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4102/Reviewer_yvnK"
        ],
        "content": {
            "summary": {
                "value": "This study delves into the efficacy of data-balancing in reducing biases in CLIP models, highlighting its strengths and constraints. A novel data-balancing algorithm is introduced, showing success in mitigating representation biases, particularly through fine-tuning, but offering lesser impact on association biases, with a variable effect on task performance such as enhanced classification yet impaired retrieval. The research culminates with strategic guidance aimed at optimizing data balancing in multimodal learning systems."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Nice definition of various types of bias in data or model, this is a solid foundation to understand the problem\n\n2. Given a nice definition of bias such as gender, this work shows the effect of model training with various type settings. (More or less data, fine tuning with various length of training time) \n\n3. Tested on various datasets and backbone designs. \n\n4. The proposed balancing algorithm does seem to successfully diminish bias without compromising the quality of the model."
            },
            "weaknesses": {
                "value": "1. I hope to see figures that are easier to interpret.\n\n- For Figure 2 (top): It seems you intend to demonstrate that even with extra training data, bias persists. I struggled to determine which bars were being compared. A similar issue occurs with Figure 3.\n- Regarding Figure 4: At first glance, without referring to the captions, all the color bars appear identical. My initial interpretation was that the results were largely uniform across the board.\n\n\n2. I find myself somewhat perplexed by the conclusions drawn from the study's results. This confusion does not necessarily point to a flaw in the research but suggests that further clarification might be beneficial. I recommend referring to the detailed queries I've raised in the question section."
            },
            "questions": {
                "value": "1. In Section 4.1, the authors discuss various strategies, including adjusting the training set sizes and fine-tuning with or without dataset intervention. \n\n- While these strategies are standard in model training, the authors' approach underscores a well-known principle: merely increasing the size of a 'corrupted' dataset\u2014say, by tenfold\u2014will not address inherent issues. This scenario is a classic case of \"garbage in, garbage out.\"\n\n- Furthermore, the practice of fine-tuning a model on a specific dataset inevitably alters some pre-existing weights, adapting the model to new data characteristics. Consequently, it is not surprising that models fine-tuned on intervened sets exhibit improved bias metrics.\n\n- In summary of this question: What's new here? \n\n2. This study suggests that balancing our training datasets can mitigate bias. However, it also implies that researchers must painstakingly identify what constitutes bias and determine the features requiring adjustment. I am particularly interested in the authors' insights on how their proposed framework accommodates additional features present in real-world datasets.\n\n\n3. My concern extends to the interrelated nature of certain features, as highlighted in the occupation versus gender discussion in Section 4.2. While it is inappropriate to rely on stereotypes for inferring gender, assuming a 50/50 gender split across all professions disregards real-world disparities. It remains unclear how the proposed methods address \"association bias.\"\n- A link to \u201cU.S. BUREAU OF LABOR STATISTICS\u201d https://www.bls.gov/cps/cpsaat11.htm"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4102/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698525658949,
        "cdate": 1698525658949,
        "tmdate": 1699636375063,
        "mdate": 1699636375063,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "enkj3aPnCs",
        "forum": "FIGXAxr9E4",
        "replyto": "FIGXAxr9E4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4102/Reviewer_5bEw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4102/Reviewer_5bEw"
        ],
        "content": {
            "summary": {
                "value": "This paper studies two types of biases, representation biases (RB) and association biases (AB), in vision-language models such as CLIP. \n\nRB refers to that the model learns to prefer sensitive attributes (e.g., gender, age groups) in the training data. AB refers to that the model associates certain concepts with sensitive attributes. (e.g. occupations with a specific gender)\n\nStudying such biases is an important problem in the real-world as CLIP-like models are widely used in the industrial applications. In this paper, the authors first investigate the empirical evidence of both biases and how it transfers from data to the model. They show that RB is sensitive to the latest training data distribution thus fine-tuning (FT) is an effective approach to reduce RB. However, FT is weak on AB. They then propose a data balancing algorithm to alleviate both RB and AB by estimating an optimal weight for each data example."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The empirical evidence of RB and AB is well supported in the experiments.\n2. The proposed data balancing algorithm is principled with theoretical analysis.\n3. The paper studies an interesting and important problem which may have a wide impact in real-world industrial applications, such as recommender systems and advertising."
            },
            "weaknesses": {
                "value": "1. The number of sensitive attributes in the experiments is limited to only gender and occupation.\n2. Further experiments on proposed data balancing algorithm is lacking in the main text."
            },
            "questions": {
                "value": "1. How exactly the de-correlation between sensitive attributes and proxies is implemented?\n2. In AB experiment, why adding proxies has inconsistent performance?\n3. In data balancing algorithm, how to intuitively interpret $\\alpha$ and $\\beta$? and how $s$ is determined, is it calculating from the dataset over sensitive attributes?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4102/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4102/Reviewer_5bEw",
                    "ICLR.cc/2024/Conference/Submission4102/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4102/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698805891341,
        "cdate": 1698805891341,
        "tmdate": 1700470911337,
        "mdate": 1700470911337,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IeGtUZwJFf",
        "forum": "FIGXAxr9E4",
        "replyto": "FIGXAxr9E4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4102/Reviewer_yZxD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4102/Reviewer_yZxD"
        ],
        "content": {
            "summary": {
                "value": "This paper identifies the societal bias issue in CLIP models and provides possible explanations and workarounds. Specifically, the representation and association biases are taken into consideration. While these biases can be somehow addressed with the data balancing strategy, other issue emerges. In this regard, this paper discusses in detail the role of data balancing in handling bias issues, and provides useful insights."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is well motivated and clearly written.\n- Simple data balancing strategies are proposed to tackle the bias issue, demonstrating promising results.\n- Comprehensive experimental results and analysis are presented, which may benefit the reader in relevant fields."
            },
            "weaknesses": {
                "value": "- While AB is relatively easy to mitigate, RB seems much more difficult to remove. In this regard, I would suggest the authors to shed more light on possible reasons and solutions. For example, I assume data augmentation shall be a promising workaround, and encourage the authors to explore more."
            },
            "questions": {
                "value": "- To me, the representation and association biases respectively correspond to the distribution shift of marginal output probability p(y) and conditional output probability p(y|x). What about another widely-discussed distribution shift in domain adaptation/generalization literature, i.e., marginal input probability p(x)? Can it also be a significant bias issue in large multimodal models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4102/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698996851263,
        "cdate": 1698996851263,
        "tmdate": 1699636374855,
        "mdate": 1699636374855,
        "license": "CC BY 4.0",
        "version": 2
    }
]