[
    {
        "id": "y4HiTvFU07",
        "forum": "EanCFCwAjM",
        "replyto": "EanCFCwAjM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2052/Reviewer_mia4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2052/Reviewer_mia4"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a new method for sparse-view camera pose estimation. Instead of parameterizing a camera model as an intrinsic matrix and an extrinsic matrix, the authors propose to over-parameterize the camera as a collection of rays. The intrinsic and extrinsic are computed by solving a least-squares problem. A diffusion network is presented to predict the ray parameters. The method achieves state-of-the-art performance on the Co3D dataset."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "\u2022\tThe idea of over parameterization is novel. It enables robust camera pose estimation by involving least-squares optimization. Ideally, the method has the potential to predict the camera pose from a single RGB image since the ray representation does not rely on multi-view information.\n\n\u2022\tThe paper is well-written and easy to follow. The experimental results show much better camera pose estimation performance compared with some existing approaches."
            },
            "weaknesses": {
                "value": "As reported in Table 1, it seems that the presented over parameterization method plays a crucial role in the framework. The performance of Ray Regression (Ours) surpasses that of R+T Regression by a considerable margin. The diffusion model only results in a 3.8% improvement in the case of two images. To my understanding, the superior performance is primarily attributed to the least-squares optimization which accounts for a robust estimation. However, it is still quite confusing why the pose estimation benefits from the ray representation. \n\nBasically, the idea is to regress a ray represented as a 6D vector for each patch in the RGB image. It is arguably more challenging than predicting R and T. The difficulty lies in two aspects. First, it is a dense prediction problem. Second, it regresses 3D information from RGB images. One could also predict the corresponding 2D coordinates in the right image for each patch in the left image as an alternative. Intuitively, it is easier to predict 2D coordinates than 6D ray vectors. The authors argue that such a method could struggle in sparse view settings due to insufficient image overlap to find correspondences. It is unclear why the presented method is able to achieve better robustness. \n\nMoreover, it is confusing why the presented method can recover the translation. According to Eq.3, m is coupled with the translation t. Predicting m then demands a requirement of capturing information about the camera translation. However, the actual input of the network is a cropped image. The information regarding t loses after the cropping.\n\nThe authors only conducted experiments on the Co3D dataset, which makes the evaluation not convincing enough. There are several datasets that have been widely used in the literature such as Megadepth, ScanNet, and HPatches. It would be beneficial if the authors could show the effectiveness of the over parameterization on such datasets.\n\nAccording to Eq.7, the patches of all available images are jointly processed, which is computationally expensive. As reported by the authors, training the diffusion model takes four days on 8 A6000 GPUs, which is much slower than RelPose and RelPose++."
            },
            "questions": {
                "value": "\u2022\tMost of the equations in this paper make sense to me, but the explanation of Eq.5 is a bit confusing. What is the \u201cidentity\u201d camera? Are there any constraints on this equation? Does it still hold when the image depicts multiple planes? \n\n\u2022\tAs shown in Table 1, sometimes, the performance of the presented method decreases when more images are involved. By contrast, the performance of most competitors such as RelPose++ consistently becomes better with more images. I was wondering why the method is not compatible with multi-view images."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2052/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697814756340,
        "cdate": 1697814756340,
        "tmdate": 1699636137088,
        "mdate": 1699636137088,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JxHlKDOvOd",
        "forum": "EanCFCwAjM",
        "replyto": "EanCFCwAjM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2052/Reviewer_wRVJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2052/Reviewer_wRVJ"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a distributed representation of camera pose which treats a camera as a bundle rays allowing for a tight coupling with spatial image features, which is naturally suited for set-level level transformers. Furthermore, the authors propose a regress-based approach to map image patches to associated rays. To further capture the inherent uncertainties in pose inference, the authors also develop a denoising diffusion model. The experiment on CO3D dataset demonstrate the performance of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThe authors propose a novel representation of pose that allows a bundle rays to denote camera in the field of sparse-view pose estimation.\n2.\tTo inference the rays, the authors develop a deterministic regression network and a probabilistic diffusion model, and the experiment on the CO3D demonstrates the superior performance."
            },
            "weaknesses": {
                "value": "1.\tThe authors announce that the traditional representation of pose maybe suboptimal in neural learning in the part of introduction. However, no further discussion is given. More specific explanation is necessary, and the comparison with the proposed novel representation of pose is also required.\n2.\tThe punctuation is necessary at the end of each equation, please check it carefully.\n3.\tThe authors fail to state more details of the proposed network architecture. Moreover the training detail is also required.\n4.\tTo demonstrate the performance of the proposed novel representation, can authors undertake more experiments on more datasets?\n5.\tPlease check the format of REFERENCES."
            },
            "questions": {
                "value": "See the weakness part"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2052/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2052/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2052/Reviewer_wRVJ"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2052/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698681682333,
        "cdate": 1698681682333,
        "tmdate": 1699636137007,
        "mdate": 1699636137007,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Dwdrmti7Kr",
        "forum": "EanCFCwAjM",
        "replyto": "EanCFCwAjM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2052/Reviewer_v24e"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2052/Reviewer_v24e"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel method for estimating wide baseline camera poses from mutliview imagery by representing cameras as a bundle of rays through image pixels.  The rays are directly regressed from local image patches that they pass through using a vision transformer and then made more consistent with neighboring rays using diffusion applied to an image of the rays.  The bundle of rays can be converted to a standard pinhole camera model by a DLT fit of the camera parameters to the rays.  The authors train regression and diffusion models on data from the CO3Dv2 dataset and evaluate the models on held out data from that same dataset.  The method is compared to several other recent methods for camera pose estimation on the same data and demonstrates improvements in rotation and pose accuracy metrics compared to the prior work.  The primary contribution of the work is showing that regression of rays can result in more accurate camera models than trying to directly regress camera parameters as done in prior work."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The strengths of this paper are the novelty of the approach and the quality of results, which together are likely to have a significant impact in the field of wide baseline camera estimation.  Directly regressing rays intuitively makes sense as they more suited to regression by a neural network, since each ray depends on more local image information.  The paper makes this point clear and backs it up with experimental results.  Overall, the paper is written clearly and is easy to understand."
            },
            "weaknesses": {
                "value": "The main weakness of this paper is the somewhat contrived and limited dataset and metrics used in the experimental results.  The CO3D dataset consists of many turntable-like videos with a camera orbiting in a circle around a single object of interest at an approximately fixed distance.  The variability of camera poses is quite limited compared to images in the wild.  Furthermore the image is tightly cropped around the object of interest.  This tight cropping ensures that most rays sampled pass through this common object in all views, which could provide added benefit to the propose approach.  The cropping also provides a disadvantage to feature matching approaches such as used by COLMAP.  COLMAP benefits from having a larger context of the scene with more features to match.  However, the authors are just duplicating the experimental setup from prior work (RelPose), so they are not entirely at fault for these decisions.\n\nThe proposed algorithm also, presumably, does not estimate precise camera parameters and would need a further bundle adjustment step to achieve sub-pixel accurate camera models with comparable accuracy to COLMAP (under the conditions where COLMAP succeeds).  The metrics only measure if the camera rotation is within 15 degrees of correct angle and within 10% of the scene scale in position.\n\nIn terms of clarity of the work, one concern I have is that the authors often say \"sparse-view\" when it would be more accurate to say \"wide-baseline\".  For example, the abstract states that 3D reconstruction remains challenging for sparse views (<10).  It's not the reduced number of views that are the challenge, it's the wide baseline between those images.  More traditional methods like COLMAP would do just fine on 10 images from more similar viewpoints.\n\nAlso, I found the camera visualization in Figure 5-9 to be confusing.  Without the context of the 3D object or the coordinate system and with only a few cameras, it's hard to interpret what I'm looking at.  In many cases it's not even clear which cameras belong to which algorithm's results.\n\nMinor issues:\n\nTypo on middle of page 4: \"the camera camera extrinsics\"\n\nReferences to Tab 10 and Tab 4 at the start of Section 4.3 seem to point to incorrect tables."
            },
            "questions": {
                "value": "I'm curious whether the authors think that this method would be effective in more realistic multi-view imaging environments where the imagery is not tightly cropped to just one object and where the camera motion could be more general?  Have any experiments been run to see if this method works on images \"in the wild\"?\n\nIn the the experiments, while is COLMAP used with SuperPoint features and SuperGlue matching?  No justification is given.  Is this expected to perform better or worse than vanilla COLMAP on this dataset?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2052/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2052/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2052/Reviewer_v24e"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2052/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698724807379,
        "cdate": 1698724807379,
        "tmdate": 1699636136893,
        "mdate": 1699636136893,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Sq1S2OqRs9",
        "forum": "EanCFCwAjM",
        "replyto": "EanCFCwAjM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2052/Reviewer_NPGi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2052/Reviewer_NPGi"
        ],
        "content": {
            "summary": {
                "value": "In this paper, authors tackle the problem of Sparse-View Pose Estimation by distributed \"ray\" based representation of cameras. By defining the camera as a bundle of rays via Plucker coordinates, authors formulate regression and diffusion-based approaches to predict camera rays from a set of sparse RGB images. With a predicted ray bundle, camera parameters (intrinsics and extrinsic) can be easily recovered. The authors test their method in a sparse view setup for the CO3Dv2 dataset and show that their regression and diffusion-based methods outperforms current learning-based and correspondence-based methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- I think this is a good method of formulating camera pose and intrinsic recovery using a bundle of rays. Furthermore, the authors' observation that ray-based representation is well-suited for set-level transformers is well backed by the results. \n- The authors' \"regression\" based method outperforms other \"diffusion\" based methods, which shows that over-parameterization is really helping solve for camera geometry accurately. \n- The results outperforms currently available \"leaning\" based and \"correspondence\" based method in sparse view settings on the CO3Dv2 dataset, that's a big encouragement.\n- The authors also show that the method generalizes to out-of-distribution, in-the-wild scenes."
            },
            "weaknesses": {
                "value": "- One dataset is too small to see the applicability of a method. Since I see this method as superior to \"PoseDiffusion\", it would be great to see some results on the \"scene-centric\" dataset and compare it against PoseDiffusion.\n- It would be nice to see a \"memory\" requirement to run these models. Processing N image features together, I am assuming requires a good amount of GPU memory.\n- It would also be nice to see accuracy at different thresholds i.e. @5, @10, @15.\n- It would also be nice to see an ablation study where we do not scale the poses. Most of the applications require properly \"scaled\" poses.\n\n- The language is clear but I think the paper presentation is poor. Here are a few suggestions to improve the readability of the paper.\n1) For e.g., Fig 2. is really confusing where the authors are trying to show the camera to ray-bundle and ray-bundle to camera process.\n2) Fig 5. A qualitative comparison is hard to see and to make a good sense, as opposed to Fig 4 of PoseDiffusion paper for example. \n3) Also good to say in eq (3) that \"d\" is obtained by unprojecting rays from camera pixel coordinates, and \"m\" is obtained by considering point \"p\" as the camera-center since all rays intersect at the camera center.\n4) In section 4.3 evaluation Table numbers are wrong. Tab 10 -> Tab 1, Tab 4 -> Tab 2\n5) Fig 6 is very confusing. I think this needs to be redone."
            },
            "questions": {
                "value": "- I see runtime in the appendix, but how does this method scale with adding a number of views? Instead of 8 what if I had 32? What are the memory requirements and inference runtime graphs?\n- If I am interested in \"scaled\" poses, how would I get it?  \n- Authors say they stopped the backward diffusion process early and found that those estimates were better. How early did they stop the diffusion? And how did they choose when to stop? It would be nice to see some ablation analysis.\n- What are your views of the applicability of this method for \"scene\" centric datasets?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2052/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2052/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2052/Reviewer_NPGi"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2052/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698825662237,
        "cdate": 1698825662237,
        "tmdate": 1700685145380,
        "mdate": 1700685145380,
        "license": "CC BY 4.0",
        "version": 2
    }
]