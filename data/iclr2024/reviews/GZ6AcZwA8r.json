[
    {
        "id": "AaNayDAUzb",
        "forum": "GZ6AcZwA8r",
        "replyto": "GZ6AcZwA8r",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3490/Reviewer_CP9G"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3490/Reviewer_CP9G"
        ],
        "content": {
            "summary": {
                "value": "The paper introduced a class of MMD based graph kernels. It also introduced a class of deep MMD graph kernels that have learnable parameters. Some theoretical analysis such as robustness have been done. The experiments of graph clustering and classification showed the effectiveness of the proposed methods."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. Measuring the similarity between graphs is a practical and challenging problem.\n2. The proposed shallow and deep MMD graph kernels are novel and interesting. The deep MMD graph kernel is learnable in both supervised and unsupervised ways. \n3. The paper studied the robustness and generalization of the graph kernels. \n4. In the experiments, the proposed deep MMD graph kernel is more effective than classical graph kernels and graph neural networks. The superior performance in clustering is quite impressive.\nIn sum, this is a good work with solid theoretical analysis and numerical evaluation. The unsupervised deep MMD graph kernel makes a breakthrough in the area."
            },
            "weaknesses": {
                "value": "The paper has the following weaknesses.. \n1. The motivation of the two unsupervised loss functions (eq.10 and eq.11) in Section 4 haven\u2019t been sufficiently explained. \n2. For graph clustering, GNN-based clustering method should be compared in Table 1."
            },
            "questions": {
                "value": "It would be great if the authors could improve the aforementioned weaknesses and answer the following questions.\n1. In Definition 5, for the generalized MMD, how to efficiently determine $k$ in practice? It may have infinite choices.\n2. In Theorem 1, it seems that if $h$ is close to zero, the bound is loose. How to avoid small $h$ in practice?\n3. In unsupervised learning with loss functions (11) and (12), will model collapse (all $s_{ij}$ are zero or one) happen? This may be caused by zero or infinity weights in the deep model.\n4. More explanation about the usefulness of (11) and (12) are required.\n5. In Section 4.2, suppose $\\beta$ can be computed, how to obtain the stability parameter $\\omega$?\n6. In Table 1, the NMIs and ARIs given by all methods on datasets DHFR and PTC are very low. What is the possible reason?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3490/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3490/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3490/Reviewer_CP9G"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3490/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698570344678,
        "cdate": 1698570344678,
        "tmdate": 1700754278657,
        "mdate": 1700754278657,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SkaB5pJvvZ",
        "forum": "GZ6AcZwA8r",
        "replyto": "GZ6AcZwA8r",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3490/Reviewer_avjd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3490/Reviewer_avjd"
        ],
        "content": {
            "summary": {
                "value": "This work proposed a class of (deep) MMD based graph kernels, coupled with theoretical results. The proposed kernels are applied to graph classification and graph clustering on a few benchmark datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper proposed a class of (deep) MMD based graph kernels (MMD-GKs). The novelty is high. Different from classical graph kernels that have fixed features, the proposed Deep MMD-GKs are learnable, with or without labels.\n* The authors provided some theoretical analysis, which are useful and explainable. \n* The deep MMD-GKs outperformed the baselines in supervised learning and unsupervised learning on several graph datasets."
            },
            "weaknesses": {
                "value": "* The time cost comparison is missing. \n* Deep MMD-GK is more effective than the vanilla MMD-GK but the corresponding algorithm (Algorithm 2) is in the appendix.\n* The analysis of the experimental results should be strengthened."
            },
            "questions": {
                "value": "* Section 3.1 can be made more compact to provide more space for Section 4 and Section 5.\n* What is the advantage of the general MMD compared to the single-kernel MMD?\n* In Algorithm 1, for the input, the authors mentioned bandwidth. Does this mean the algorithm only use RBF kernels?\n* In formula (11), how to determine $S_+$ and $S_-$?\n* In Theorem 2, the influence of $n$ and $h$ hasn\u2019t been discussed.\n* What is the labeling rate in Table 2?\n* In Table 2, on BZR, the values given by supervised and unsupervised Deep MMD-GKs are the same. Is it a typo?\n* In Appendix G, are there any explanation for the additional results?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3490/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3490/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3490/Reviewer_avjd"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3490/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698637434088,
        "cdate": 1698637434088,
        "tmdate": 1700732713521,
        "mdate": 1700732713521,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "t1nTXT1NOA",
        "forum": "GZ6AcZwA8r",
        "replyto": "GZ6AcZwA8r",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3490/Reviewer_a9u9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3490/Reviewer_a9u9"
        ],
        "content": {
            "summary": {
                "value": "This paper discusses graph kernels based on maximum mean discrepancy (MMD) on the (empirical) distribution of embedded nodes. An extension of this MMD graph kernel similar to the deep graph kernel (Yanardag and Vishwanathan 2015) is proposed. For both graph kernel variants the authors provide a theoretical robustness/perturbation analysis. Finally, first experimental results on standard benchmark datasets are presented."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "* First empirical results on (arguably simple) benchmark datasets are promising.\n* Theoretical robustness analysis."
            },
            "weaknesses": {
                "value": "While the paper is interesting, I vote for rejection in its current form, due the following issues.\n\nBadly written and hard to follow paper:\n* The reader is not guided well through the paper. \n* While the robustness analyses might be interesting, they are completely unmotivated. The assumptions made and the achieved bounds are hard to interpret and set into context. It is not clear why the authors decided to include them and what the significance of them is. Please guide the reader more and put the results into context, e.g. discuss similar bounds. \n* The architecture overview in Fig. 1 is rather confusing and not explained well. Also the \"GCN\" probably stands for the GCN of Kipf&Welling, however it's not clear what it is doing in the Figure and what is meant e.g. with \"vanilla: done. GCN: updating\". \n* The central definition 6 and Assumption 1 are difficult to understand.\n\n\nDiscussion of related work is insufficient\n* Most of the actually related work is not discussed. E.g. many papers exist discussing MMD in the context of graph kernels and GNNs. E.g. [1, 2, Borgwardt's thesis, ... ] Similarly many related work exist that perform graph matching / optimal assignment [Kriege et al., survey, etc.], or modify WL using Wasserstein distances [3-5]. This makes the novelty difficult to assess. Please discuss.\n\nMisleading and wrong claims:\n* The authors claim that the proposed kernel has \"much lower computational cost\" then e.g. the WL kernel. This seems to be wrong. The WL kernel can be computed in near-linear time if the height is fixed. This is similar to the parameter $l$ here in $X^l$. In fact, the computation of WL up to height $l$ should be essentially the same as computing the $X^l$s. \n* two of the three main benefits of the proposed approach are (1) dealing with unequal sizes and (2) permutation-invariance. This is however the case for almost all graph kernels, including the popular ones like the WL-kernel, walk kernels, pattern-based kernels etc. \n* the claims about degree + power law distributions are unclear. Why should non-power law distributed degrees lead to \"inaccurate substructure representations\".  Also, the authors claim that GNNs resolve the previously mentioned issue and \"capture intricate higher-level interactions\". This is however rather misleading as most common GNNs (MPNNs) are bounded in their expressivity by WL and hence cannot capture \"more\" than the WL kernel (Xu et al., 2019, Morris et al 2019).\n* The cited \"MMD GAN\" paper is mentioned as \"MMD also aids in the training of GNN\", while the paper actually discussed training of *GANs* and does not deal with graphs at all.\n\nA lot of sentences are vague, handwavy, and unnecessarily wordy, e.g.\n* what should \"substructures derived from graphs are intertwined\" even mean exactly.\n* $X^l$ and $X^{(l)}$ are probably the same thing, but the latter is not defined.\n\nTypos and minor issues:\n* $\\mu_P$ is not defined. Similarly, most readers probably don't know what a characteristic kernel is in the context of MMD. Please make the paper self-contained.\n* \"to learn graph metric*s*\"\n* \"(2012) . \" the space before the dot \".\"\n\nI am happy to change my score if my concerns are addressed and the contribution/novelty is made clearer.\n\n[1] Ghanem, Hashem et al. \"Fast graph kernel with optical random features.\" ICASSP 2021\n\n[2] O'Bray, Leslie, et al. \"Evaluation metrics for graph generative models: Problems, pitfalls, and practical solutions. ICLR 2022\n\n[3] Togninalli, Matteo, et al. \"Wasserstein weisfeiler-lehman graph kernels.\" NeurIPS 2019\n\n[4] Samantha Chen et al. \"Weisfeiler-Lehman Meets Gromov-Wasserstein\" ICML 2022\n\n[5] Till Schulz et al. \"A generalized Weisfeiler-Lehman graph kernel\" Machine Learning 2022"
            },
            "questions": {
                "value": "Why do the authors not use the standard unbiased MMD estimator e.g. in Eq. (4)? I.e. the one normalized with $(n\\cdot(n-1))$ in the in-distribution sums, cf. Gretton et al. 2012."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3490/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3490/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3490/Reviewer_a9u9"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3490/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698771684458,
        "cdate": 1698771684458,
        "tmdate": 1700729658808,
        "mdate": 1700729658808,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uGjyPc9QS6",
        "forum": "GZ6AcZwA8r",
        "replyto": "GZ6AcZwA8r",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3490/Reviewer_9SJd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3490/Reviewer_9SJd"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new graph kernel to calculate the maximum mean discrepancy (MMD) which measures the similarity of two graphs. The main idea is to regard each graph as a sampling distribution from a latent metric space where a similarity kernel can be designed for efficiency. Based on such an MMD-based graph kernel, two methods are designed for unsupervised and supervised graph metric learning, respectively."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "+ The paper is well-written, and I find it a joy to read. The authors present a nice and coherent story to introduce the current gap and motivation of the idea.\n\n+ The proposed MMD-based kernels are technically sound and will have a significant impact on the graph metric learning domain.\n\n+ In-depth and well-organized theoretical analysis is provided to justify the approach in a principled manner."
            },
            "weaknesses": {
                "value": "- The major limitation is the lack of theoretical and empirical study of the efficiency of the proposed kernel. As highlighted in the introduction the proposed method aims to address the computation and memory cost of existing methods, no evidence shows this goal is achieved."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3490/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699038971972,
        "cdate": 1699038971972,
        "tmdate": 1699636301999,
        "mdate": 1699636301999,
        "license": "CC BY 4.0",
        "version": 2
    }
]