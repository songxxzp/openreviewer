[
    {
        "id": "0PXnk0xrlF",
        "forum": "vXf8KYTJmm",
        "replyto": "vXf8KYTJmm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8969/Reviewer_vYmk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8969/Reviewer_vYmk"
        ],
        "content": {
            "summary": {
                "value": "This work proposes ACBS, a modified version of beam search that produces output from the LM by conditioning on external signals, e.g. length. The authors argue that the unexpected behavior of the model is caused by the low-entropy noise sample and derive their proposed method. The experiments are conducted on two tasks (machine translation and story generation) with model scales up to 7B."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The motivation for this work is interesting.\n* The authors provide extensive qualitative examples."
            },
            "weaknesses": {
                "value": "* **Poor Presentation**: The presentation of this work is poorly written. Please proofread your manuscript before submission. Some examples are \n  * Section 1: distribution which the mode representsEikema & Aziz (2020) --- the citation should be included in parenthesis and there should be a white space.\n  * Section 1: training data data --> training data\n  * Footnote 1: the regurgitating the input --> the regurgitating of the input\n  * Section 2.1: distributions arbitrarily closely --> distributions arbitrarily close\n  * Section 3: translation model Tiedemann & Thottingal (2020) --> translation model (Tiedemann & Thottingal, 2020)\n  * Section 3: ROC stories dataset Mostafazadeh et al. (2016) --> ROC stories dataset (Mostafazadeh et al., 2016)\n  * Section 3: LLaMA model Touvron et al. (2023) --> LLaMA model (Touvron et al., 2023)\n* **Poor Theoretical Analysis**: The theoretical motivations presented in Section 2.1, 2.2, 2.3 are hard to follow. For instance, the authors write \"For the SVO translation example\", what is SVO? And there should be a proper citation. The mathematical derivations in those sections are not detailed enough, which clearly undermines the quality of this work.\n* **Lack of Details in Experiments**: There are necessary details are missing, including how the outputs are obtained from the LM which are then used to train the classifier; number of epochs; number of training samples.\n* **Limited Evaluations**: The evaluations are only considered up to 200 tokens which is too short under current literature, e.g. ChatGPT API has 4k length. I strongly suggest the authors to extend their evaluations up to at least 2k length.\n* **No human evaluation**: When comparing two decoding methods like in Section 5.2.1, simply using likelihood is not enough. Human evaluations are necessary to include to provide more evidence of the proposed approach."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8969/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698521273959,
        "cdate": 1698521273959,
        "tmdate": 1699637129610,
        "mdate": 1699637129610,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9vb47WfiWq",
        "forum": "vXf8KYTJmm",
        "replyto": "vXf8KYTJmm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8969/Reviewer_3nJt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8969/Reviewer_3nJt"
        ],
        "content": {
            "summary": {
                "value": "This paper shows that degenerate modal outputs are not necessarily an intrinsic property of language models themselves, but rather are likely a result of contamination in the training data. For improving the quality of text decoded from language models, the paper introduce an algorithm called ACBS (attribute-conditional beam search), which adds an additional constraint on the output to avoid the degenerate behavior. The experiment shows that ACBS are better than ordinary beam search and especially contirbutes to ameliorate empty-string degenerate behavior."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper provides a very detailed explanation to the model architecture, algorithms, and experimental data in the appendix, which is very useful for helping readers to understand the paper\u2019s work. \n2. Derivation and motivation are clear. Point out the phenomena and causes of degeneracy problem at the beginning and then solve it later.\n3. This paper is logical and flowing. The location and analysis of degeneracy problem are given progressively.\n4. The analysis of the low-entropy distractor is concise and easy to understand with examples.\n5. The experimental setup follows intuition, and argumentation process is basically based on experiments."
            },
            "weaknesses": {
                "value": "1. Missing experimental data in 5.3. The detailed experiment result about the comparison between ACBS and regular beam search should be compared in a table instead of directly stating the data in the paragraph. Otherwise, your results won't be convincing.\n2. In 3.2.1, figure 1a and figure 1b just represents the increase of empty sequence with source length, lack of the curve that shows the decrease of empty output with source length to better support the conclusion.  \n3. Lack of explaination about the difference between empty mode and empty output.\n4. The paper only gives two examples to illustrate that low-entropy distractor outputs and empty outputs have a high log prob in section 2 but does not provide enough mathematical reasoning, so the argument that the degenerate modal behavior is related to the entropy of the set of valid outputs is not strong enough.\n5. In section 3, the x-axis and y-axis markings in Figure 1a are not clear and Figure 1a does not offer enough support for the observed phenomenon that the probability of the empty sequence declines as the source length increases. The experiments in section 3 do not indicate the impact of contamination of the training data, which is emphasized in the abstract and conclusion."
            },
            "questions": {
                "value": "1. At the end of Section 3, how can we conclude that the degenerate modal behavior is related to the entropy of the set of valid outputs?\n2. Exact search experiments are meaningful, but the result that exact search performs good is not relevant to Subsequent Chapters. Maybe their relevance should be emphasized by experiments in terms of computational cost.\n3. The experiment to prove that ACBS not only benefit from removing empty outputs should be detailed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8969/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8969/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8969/Reviewer_3nJt"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8969/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698538658929,
        "cdate": 1698538658929,
        "tmdate": 1699637129487,
        "mdate": 1699637129487,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "09TuTXttR0",
        "forum": "vXf8KYTJmm",
        "replyto": "vXf8KYTJmm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8969/Reviewer_mRtz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8969/Reviewer_mRtz"
        ],
        "content": {
            "summary": {
                "value": "This paper argues that one source of so-called text degeneration is contamination of the training data with low-entropy noise, such as empty or nearly empty completions, or partial or total repetitions of the prompt. It substantiates this claim by performing exact MAP decoding, including on LLaMA. It offers a decoding solution, which is to do exact or beam search decoding with constraints on \"attributes\" like length."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "Overall, I really like this paper and favor acceptance.\n\nThe experiments with exact search provide convincing evidence of the claim about low-entropy strings. These experiments are also valuable because they reveal a new kind of degeneracy (copying the prompt) and they are the first to perform exact decoding on a large language model.\n\nThe observation that sampling has the opposite \"Achilles' heel\" is valuable, although not really the focus of this paper.\n\nThe attribute-constrained beam search algorithm is new. It's a nice idea that has essentially the same running time as standard beam search, and seems to work well. It's interesting that the example length-conditioned translations are good summaries of translations."
            },
            "weaknesses": {
                "value": "The explanation of degeneracy in terms of low-entropy strings is not new, and the authors may not be aware of the following two papers:\nOtt, https://arxiv.org/abs/1803.00047\nHoltzman, https://arxiv.org/abs/2104.08315\n\nAs an alternative to your decoding method, you could use an adaptive beam, using a wider beam for earlier timesteps that gets narrower for later timesteps. Then at timestep $a$, you would get higher-quality outputs with length $a$. I am not sure what schedule you would use for the beam size, but perhaps work by Brian Roark for adaptive beams in CKY parsing is relevant.\n\nThe evaluation of the attribute constrained beam search method for LLaMA consists of recording what percent of sentences get a higher reward according to the same reward model used in training, and giving a general subjective impression of sample outputs. I think this is a fairly weak evaluation, and it would be a lot better to elicit quality judgements from other people.\n\nStyle / minor points:\n\nThere is too much important information in the appendix, especially Algorithm 1, with many references from the text to the appendix.\n\nEveryone has their own writing style, but I feel that there are too many exclamation points for an academic paper. There are even two sentences in a row with exclamation points on page 4, but this is probably just an editing error."
            },
            "questions": {
                "value": "table 2: how can truncated beam search be better than itself?\n\ntable 3: why do the TBS translations seem shorter?\n\n5.3.1 In the example, the source sentence in the prompt is \"I love machine learning,\" but the output translates the source sentence \"My eyes are clear.\" Is that really what happened?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8969/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698772323003,
        "cdate": 1698772323003,
        "tmdate": 1699637129343,
        "mdate": 1699637129343,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0NqLnFcFy6",
        "forum": "vXf8KYTJmm",
        "replyto": "vXf8KYTJmm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8969/Reviewer_YHYo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8969/Reviewer_YHYo"
        ],
        "content": {
            "summary": {
                "value": "The authors provide analysis of why text generation models often suffer degenerated distribution mode. They attribute the problem to the contamination in the training data sampled from natural language distribution. They further find that the bad mode problem is alleviated when conditioned on a certain target length. To this end, the authors propose an attribute-conditional beam search algorithm which exhibits superiority compared with truncating methods when target length is given."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The authors provide a detailed analysis of bad mode problem.\n2. The authors propose an attribute-conditional beam search algorithm which exhibits superiority compared with truncating methods when target length is given. \n3. The authors conduct experiments on various NLP tasks."
            },
            "weaknesses": {
                "value": "1. **Analysis less than convincing in supporting the \"*bad mode problem*\"**.  \nIn Section 2, the authors claim that introducing noise into the data distribution can lead to model degeneration, even when the model is perfectly trained to fit the original data distribution. The authors provide examples to illustrate this concept. However, I found some of these analyses less than convincing. In Section 2.1, the authors argue that \"*If one in a billion sequences is replaced with a bad output, MAP on a perfectly trained model should give us one of the bad outputs*\". However, this argument relies on the assumption that \"*there might be 2^100 possible abstracts for a given scientific paper*\". It seems such an assumption never holds true in the case of a real dataset. In contrast, there is only one reference for a source in the typical setting.\n\n2. **Unclear logic between Section 2 & 3.**  \nIn Section 3.2, the authors provide experimental results that \"*the occurrence of empty sequences increases with source length*\". The authors attribute the empty mode problem to that \"*the entropy of valid outputs increases with input length, but the probability of the empty output does not decline enough*\". I am confused that this may contradict the analysis in Section 2 that attributes the bad mode to \"*low-entropy distractors*\".\n\n3. **Experiments are not serious**.  \n**a.** In Section 3.2.1 & Section 4, the authors conduct qualitative analysis only based on case study, lacking rigorous analysis and discussion.  \n**b.** In Section 5.2.1, the authors compare their proposed attribute-conditional beam search with truncated beam search when a target length is provided. The comparison is based solely on the log-likelihood of the search results, without considering the evaluation of generated quality, such as BLEU scores. This evaluation seems insufficient, especially considering the background that a language model's mode can lead to degenerated results. Furthermore, there's no comparison between the proposed attribute-conditional beam search and standard beam search (without length truncating, evaluated with both likelihood and BLEU), which appears to be weird and not convincing.\n4. **Concerns on the motivation and novelty.**  \n**a.** The authors proposed a length-conditioned beam search algorithm. However, this seems not very helpful to solving the bad mode problem in LLM, as a pre-determined length may be imprecise and lack the flexibility.  \n**b.** The novelty of proposed attribute-conditional beam search is limited. Those attribute-conditional sampling methods is well-studied, and the authors only adapt it to the beam search.\n5. **The paper has too many typos**.  \n**a.** Please check the format (should use ICLR 2024)  \n**b.** Confused paragraph numbering (Section 3.1 & 3.1.1)  \n**c.** wrong citing format"
            },
            "questions": {
                "value": "Please see the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8969/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8969/Reviewer_YHYo",
                    "ICLR.cc/2024/Conference/Submission8969/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8969/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698822688895,
        "cdate": 1698822688895,
        "tmdate": 1700739741129,
        "mdate": 1700739741129,
        "license": "CC BY 4.0",
        "version": 2
    }
]