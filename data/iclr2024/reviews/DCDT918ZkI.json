[
    {
        "id": "R5qfOrXtgs",
        "forum": "DCDT918ZkI",
        "replyto": "DCDT918ZkI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4570/Reviewer_TfP9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4570/Reviewer_TfP9"
        ],
        "content": {
            "summary": {
                "value": "The authors present a method to improve GNN adversarial robustness. The approach trains $K$ OOD edge detection MLPs that classify edges based on their internal representations of a GCN model and their input attributes. At inference time, the OOD detector ensemble predicts for each edge whether it is potentially adversarial or not, and removes the edge according to that decision. The authors present evidence based on a GNN robustness benchmark suggesting their defense outperforms existing ones. Further, the authors study robustness to poisoning attacks as well as inductive evasion attacks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The authors present compelling results on a diverse robustness benchmark\n* The approach is clear and well-motivated\n* The authors consider a wide range of settings including transductive evasion and poisoning attacks as well as inductive attacks"
            },
            "weaknesses": {
                "value": "* While the range of settings considered is wide, the set of models and dataset considered is quite narrow in return\n* The transductive evasion defense could potentially be reduced to the trivial perfect defense mentioned in [Gosch et al. 2023]\n* Regarding Proposition 1, the \"proof\" isn't really a proof, and, does not show a problem with adversarial training for graphs, but highlights that the definition of the perturbation set is too loose."
            },
            "questions": {
                "value": "* My most pressing concern is regarding the very recent work [Gosch et al. 2023]. I acknowledge that it is so recent that it is not reasonable to expect the authors to have it in their paper, yet their results are very relevant for this work. Specifically, the authors propose a trivial perfect defense for evasion attacks in transductive setting, which effectively memorizes the clean input graph and ignores the potentially perturbed graph at inference time (Proposition 1 of the referenced work). Assuming unique node attributes, wouldn't the perfect version of the OOD ensemble defense reduce to this trivial defense?\n* On a relate note, [Gosch et al. 2023] also notice the problem with the too loose definition of the set of allowed perturbations, and, in turn, present a method that employs local constraints. How does this affect Proposition 1 in this work?\n* How is the threshold $t$ determined? Is it the same across OOD detectors in the ensemble?\n* Typically, ensembles work via majority vote. In this work, the authors flag an edge as potentially adversarial if **one** of the detectors in the ensemble does. What is the reason for this?\n* The OOD detectors are trained specifically for an individual GNN classifier instance. I wonder if it is also possible to transfer the OOD detector ensemble to a different GNN instance?\n\nReferences\n---\nGosch, L., Geisler, S., Sturm, D., Charpentier, B., Z\u00fcgner, D., & G\u00fcnnemann, S. (2023). Adversarial Training for Graph Neural Networks. NeurIPS 2023. https://arxiv.org/abs/2306.15427"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4570/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4570/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4570/Reviewer_TfP9"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4570/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698411294837,
        "cdate": 1698411294837,
        "tmdate": 1700546332969,
        "mdate": 1700546332969,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9L23UhXEkU",
        "forum": "DCDT918ZkI",
        "replyto": "DCDT918ZkI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4570/Reviewer_sgRe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4570/Reviewer_sgRe"
        ],
        "content": {
            "summary": {
                "value": "The paper adopts an out-of-distribution perspective to re-examine graph adversarial attacks and analyze the distributional shift phenomena in both poisoning and evasion attacks at graph and edge levels. The authors propose an adversarial training method that trains multiple OOD detectors to improve the GNN\u2019s robustness. Through extensive experiments, we validate the adaptive and non-adaptive robustness of our approach."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The authors show that  the simple adversarial training will lead to the model learning incorrect knowledge\n2. The authors conduct extensive experiments over 25,000 graphs to compare the robustness of our methods with other baselines"
            },
            "weaknesses": {
                "value": "1. The paper is hard to follow\n2. The authors show that adversarial edges are OOD, which is straightforward\n3. Notations are hard to understand. \n4. The tested defenses are already shown to be vulnerable to adaptive attacks \n5. Lack of comparison with provable defense results."
            },
            "questions": {
                "value": "Proposition 1 is very hard to follow\n\nWhat are the key differences between OOD-detection-based Adversarial Training vs. standard adversarial training? \n\nThe tested defenses, shown in Figure 2,  are already shown to be vulnerable to adaptive attacks. Why do you choose them as baselines? \n \nWhy not evaluating the results of certified defense? How about the comparison with them?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4570/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698809628930,
        "cdate": 1698809628930,
        "tmdate": 1699636434912,
        "mdate": 1699636434912,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "r5WWftN9Ou",
        "forum": "DCDT918ZkI",
        "replyto": "DCDT918ZkI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4570/Reviewer_hRXc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4570/Reviewer_hRXc"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors theoretically show that AT can lead to models learning incorrect information.\nTo overcome this issue, they use AT paradigm incorporating OOD detection."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1) The idea of suggesting AT paradigm with OOD detection on GNN is novel.\n\n2) The theoretical part is sound.\n\n3) Experiments are sound. I specifically liked that they tested their solution even when the attacker has full knowledge of the detectors, and not only the standard model.\n\n4) paper is well written and easy to follow."
            },
            "weaknesses": {
                "value": "One question I had is - if the detector is an ensemble, how will it work against an EoT adversary? Can the authors test this kind of scenario?"
            },
            "questions": {
                "value": "See Weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4570/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4570/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4570/Reviewer_hRXc"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4570/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698869983618,
        "cdate": 1698869983618,
        "tmdate": 1699636434820,
        "mdate": 1699636434820,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Hfaglpnfpe",
        "forum": "DCDT918ZkI",
        "replyto": "DCDT918ZkI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4570/Reviewer_RUQz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4570/Reviewer_RUQz"
        ],
        "content": {
            "summary": {
                "value": "This paper studies improving the robustness of graph neural networks (GNN) to both evasion and poisoning attacks. The main idea is to model the generated adversarial edges from existing attacks as out of distribution (OOD) data, and trains OOD detectors remove adversarially perturbed edges. Empirical results show that the proposed approach outperforms existing baseline defenses in various settings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The idea of using the OOD nature of adversarial edges to train OOD detectors and remove perturbed edges is interesting.\n2. The proposed defense also achieves remarkable performance against different state-of-the-art attacks compared to existing baseline defenses."
            },
            "weaknesses": {
                "value": "1. Hypothesis 1 is the key high-level motivation for designing OOD detector based defenses. However, I am not sure how much this hypothesis can help in practical applications. Specifically, it is indeed true that the attacks can be more destructive when the distributions shift more significantly. However, we measure the empirical vulnerability by comparing the accuracy drops before and after the attack, and there might exist a situation where the attack is considerably successful (from practical point of view) without being significantly different from the in-distributions.\n2. The OOD samples are generated by PGD attacks, which can be restricted. The current experimental setting does not eliminate the possibility of adaptive attackers targeting the OOD detector, meaning there might exist some OOD samples that are still effective at inducing distribution shifts, but are not similar to PGD induced OOD samples. \n3. In the poisoning scenario, the whole process is based on the assumption that effective poisoning perturbations happen in the training nodes. This again does not capture the fact that, if it is simply impossible for attackers to induce more effective poisoning attacks by targeting other nodes in the graph."
            },
            "questions": {
                "value": "The common theme of the 3 weaknesses above is that the proposed approach does not provide a more fundamental reasoning on the effectiveness of the proposed approach. If the authors have a more fine-grained analysis, the quality of the paper will be improved."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4570/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4570/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4570/Reviewer_RUQz"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4570/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699038975731,
        "cdate": 1699038975731,
        "tmdate": 1700619724135,
        "mdate": 1700619724135,
        "license": "CC BY 4.0",
        "version": 2
    }
]