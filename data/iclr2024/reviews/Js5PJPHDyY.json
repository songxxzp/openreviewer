[
    {
        "id": "RTUack4CRc",
        "forum": "Js5PJPHDyY",
        "replyto": "Js5PJPHDyY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2168/Reviewer_e8KM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2168/Reviewer_e8KM"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a baseline for training-free few-shot adaptation of CLIP models, using a simple Gaussian Discriminant Analysis classifier. When ensembled with the original zero-shot CLIP classifier, the proposed method exhibits good performance, even better than some methods that require training. The proposed method can be extended to base-to-novel/unsupervised scenarios with some specific modifications."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed baseline is very simple and does not require training, which is very important in few-shot transfer scenarios where real-time response is often required.\n- If this baseline really has a very good performance, then it will be a good point for rethinking the whole field, which may in turn inspire future research in the right direction. \n- The paper is well-written and easy to understand."
            },
            "weaknesses": {
                "value": "- The most important weakness of this paper is a historical issue that stems from the whole literature: According to a paper also submitted to ICLR 2024 (Benchmarking Few-shot Transferability of Pre-trained Models with Improved Evaluation Protocols, https://openreview.net/pdf?id=O3Mej5jlda), the experiments conducted on the original few-shot transfer benchmark of CLIP are unreliable due to intrinsic design flaws, such as sampling uncertainty, unrealistic hyperparameter selection, etc. The performance can largely fluctuate for more than 10 points, just changing the seed for generating the few-shot task. The authors at least **must** re-evaluate their methods and other compared methods on the **same** dozens (e.g., 50) of randomly sampled few-shot tasks and report the 95% confidence intervals. I will increase my score to acceptance if the re-evaluated results still show that the proposed method obtains better performance with statistical significance.\n- The training-free metric-based classifier has been well-studied in the few-shot learning literature. The authors are encouraged to compare GDA with other classifiers, such as the Neareast-Centroid Classifier with Euclidean distance [1] and Mahalanobis distance [2].\n- The KNN method for Base-to-Novel generalization requires that the base classes should be very similar to novel classes. The method is expected to perform badly on dataset generalization settings.\n\n[1] Prototypical Networks for Few-shot Learning. NeurIPS 2017.\n\n[2] Improved Few-Shot Visual Classification. CVPR 2020."
            },
            "questions": {
                "value": "I have no more questions. Please see the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2168/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2168/Reviewer_e8KM",
                    "ICLR.cc/2024/Conference/Submission2168/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2168/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698400497815,
        "cdate": 1698400497815,
        "tmdate": 1700580045110,
        "mdate": 1700580045110,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PS0jzO1QmR",
        "forum": "Js5PJPHDyY",
        "replyto": "Js5PJPHDyY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2168/Reviewer_4kGv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2168/Reviewer_4kGv"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to revisit a classical algorithm, Gaussian Discriminant Analysis (GDA), and apply it to the downstream classification of CLIP. The method assumes that features belonging to each class follow Gaussian distributions, allowing the classifier to be estimated directly from the data without the need for training. By combining information from both visual and textual modalities, the estimated classifier is ensembled with the original zero-shot classifier within CLIP. The proposed method has been extensively evaluated on 17 datasets, demonstrating its effectiveness in achieving improved classification performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The incorporation of Gaussian Discriminant Analysis (GDA) into the CLIP model for training-free downstream tasks is interesting.\n- The proposed method is simple yet effective."
            },
            "weaknesses": {
                "value": "- Though simple yet effective, the overall novelty is limited. The proposed method simply treats the pre-trained CLIP model as a frozen feature extractor, and incorporates the idea of Gaussian Discriminant Analysis (GDA) to estimate the classifier without training for the downstream tasks. The key contribution might simply be the validation of the strong feature extraction capability of the pre-trained CLIP model.\n- It lacks of more insightful analyses. While the paper introduces the integration of Gaussian Discriminant Analysis (GDA) into the CLIP features, it falls short in providing a comprehensive and detailed analysis of the underlying mechanisms and the reasoning behind the method's effectiveness.\n- The effectiveness of the proposed method is dependent on the hyperparameter \u03b1, which controls the trade-off in the classifier ensemble. However, the paper lacks detailed evaluations regarding the sensitivity of this parameter."
            },
            "questions": {
                "value": "- What is the key novelty of the paper?\n- What are the main insights?\n- How sensitive is the method w.r.t. the hyperparamter \u03b1?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2168/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2168/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2168/Reviewer_4kGv"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2168/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698819833131,
        "cdate": 1698819833131,
        "tmdate": 1700709212593,
        "mdate": 1700709212593,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6kaQe24d8Q",
        "forum": "Js5PJPHDyY",
        "replyto": "Js5PJPHDyY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2168/Reviewer_CXou"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2168/Reviewer_CXou"
        ],
        "content": {
            "summary": {
                "value": "The paper focus on adapting the CLIP model to a target image classification dataset by constructing a classifier on the image features. Here, the classifier is constructed not using gradient based approaches, but weights and bias of the classifier are extracted from the statistics of the dataset. Following Gaussian Discriminant Analysis (GDA), authors assume features of each class to follow Gaussian distribution with identical covariance, thereby express the classifier in terms of class means and covariance extracted from the training dataset. Authors showcase the effectiveness of this method across various different tasks, and results remain competitive to gradient based training methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "-\tA simple and effective method.\n-\tEasy to implement in few lines of code.\n-\tA novel idea of following Gaussian Discriminant Analysis to improve CLIP performance on downstream tasks.\n-\tProvided ways to extend the approach to tasks like base-to-new generalization and unsupervised learning.\n-\tShown better results among training-free methods and competitive to training based methods."
            },
            "weaknesses": {
                "value": "Rather than listing weakness, I would like to discuss and clarify few questions. Please see the Questions section."
            },
            "questions": {
                "value": "-\tFor completeness, provide reference or formulations on how the numerator of eq. (2) is achieved under the assumption of Gaussian distribution.\n-\tDo all the datasets contain both validation and test set? How the hyperparameter $\\alpha$ is set for datasets without validation set (e.g. ImageNet)?\n-\tPlease report the $\\alpha$ value that is used for obtaining the results either in captions or appendix.\n-\tGDA constructing a linear classifier for CLIP. Discuss why GDA is superior to linear probe? Does linear probe performance depend on the training batchsize? Would increasing batchsize improve linear probe performance? \n-\tSimilarly, how the batchsize effects the results of Tip-Adapter-F?\n-\tHow does the split to base and new classes is made for each dataset?\n-\tI am skeptical regarding the dataset sampling using KNN for estimating the statistics for new classes. Please discuss the scenarios with new classes that are less similar or dissimilar to the training dataset and how such dataset sampling affect the results.\n-\tPrior work KgCoop [a] is missing in the base-to-new generalization comparisons.\n-\tPlease report the time required for preparing the classifier (computing mean and precision matrix) in Table 7 under Train time.\n\n[a] Hantao Yao, Rui Zhang, and Changsheng Xu. Visual-language prompt tuning with knowledge-guided 293 context optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern 294 Recognition (CVPR), pages 6757\u20136767, June 2023.\n\nI am willing to revise my rating based on the authors feedback."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2168/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698881572630,
        "cdate": 1698881572630,
        "tmdate": 1699636149913,
        "mdate": 1699636149913,
        "license": "CC BY 4.0",
        "version": 2
    }
]