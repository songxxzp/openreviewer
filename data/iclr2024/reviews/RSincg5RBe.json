[
    {
        "id": "gzXY1gdjFb",
        "forum": "RSincg5RBe",
        "replyto": "RSincg5RBe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6846/Reviewer_y9cz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6846/Reviewer_y9cz"
        ],
        "content": {
            "summary": {
                "value": "This work presents a novel hierarchical latent diffusion model for molecular graph generation. To be specific, this work introduces GLDM, a latent diffusion model for graphs using graph-level embeddings, and proposes HGLDM, a latent diffusion model that further incorporates structural information, for which these approaches enable efficient training and sampling while outperforming previous graph diffusion models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is well-written and easy to follow. \n\n- The motivation for using the latent approach, i.e., overcoming the mismatch between the continuous diffusion space and discrete data space and further reducing computational cost, is clear.\n\n- Using hierarchical embeddings of graphs for graph latent diffusion is novel and shows improvements in conditional molecule generation tasks compared to the naive latent diffusion model (GLDM) as well as previous diffusion models."
            },
            "weaknesses": {
                "value": "- Although this work states that the (hierarchical) latent approach for graph generation provides a scalable solution for molecule generation, the provided experiments are limited to datasets (e.g., GuacaMol) in which previous diffusion models (e.g., GDSS and DiGress) are applicable. In order to justify the scalability of the proposed method, it should be evaluated in a larger dataset.\n\n- The experimental setting for evaluating the computational efficiency is not clear. Is the training and sampling time measured in the same condition, e.g., training conducted via DDP and using the same number of V100 GPUs? \n\n- Generation performance on unconditional molecule generation tasks should be evaluated with more descriptive metrics, for example, FCD, Scaffold similarity [1], and Fragment similarity [1]. Reported metrics, i.e., validity, uniqueness, and novelty fail to measure how similar (e.g., chemical aspects) are the generated molecules to the molecules from the test set. In particular, under the current setting, GDSS seems to be showing comparable results in large datasets (ZINC250K and GuacaMol) with significantly fewer parameters.\n\n- The quantitative results of Tables 2 and 3 show that the performances of GLDM and HGLDM on unconditional generation tasks are almost the same, whereas there is a significant improvement using the hierarchical approach for conditional generation tasks. What is the reason for the hierarchical approach only effective in conditional tasks?\n\n- As the continuous diffusion model (e.g., GDSS) outperforms the discrete diffusion model (e.g., DiGress) in Table 2, the continuous diffusion model should be compared as a baseline in Table 3 (i.e., conditional generation task). Although GDSS does not explicitly present a conditional framework, recent work [2] proposes a conditional molecule generation framework using classifier guidance based on GDSS, which could be used as a baseline.  \n\n- The performance of GLDM (and HGLDM) comes from the effectiveness of using a latent representation of graphs compared to previous graph diffusion models, not from the diffusion processes. Thereby, analysis of the latent representation, e.g., interpolation in the latent space or clustering of the latent points with respect to certain conditions, would greatly strengthen this work.\n\n- Missing references on related works:\n  - Qiang et al., Coarse-to-Fine: a Hierarchical Diffusion Model for Molecule Generation in 3D, ICML 2023\n  - Xu et al., Geometric Latent Diffusion Models for 3D Molecule Generation, ICML 2023\n\n- I would like to raise my score if the above concerns are sufficiently addressed.\n\n---\n\n[1] Polykovskiy et al., Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation Models, arXiv 2018\n[2] Lee et al., Exploring Chemical Space with Score-based Out-of-distribution Generation, ICML 2023"
            },
            "questions": {
                "value": "- Please address the questions in the Weakness.\n\n- Is the results of Table 2 from a single run or an average of multiple runs?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6846/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6846/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6846/Reviewer_y9cz"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6846/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698643531899,
        "cdate": 1698643531899,
        "tmdate": 1699636793631,
        "mdate": 1699636793631,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GL6u39Y8DN",
        "forum": "RSincg5RBe",
        "replyto": "RSincg5RBe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6846/Reviewer_vEbk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6846/Reviewer_vEbk"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a latent diffusion model that aims to generate hierarchical levels of latent variables such as node-level, subgraph-level, and graph-level, simultaneously. To construct the latent space, the authors leverage the PS-VAE where the decoder converts the graph-level latent variables to the molecule by sequentially predicting the fragments and then predicting the links between fragments. To generate the latent variables of node-, subgraph-, and graph-level, the authors leverage the generative process of DDPM. The authors propose an architecture that models the dependency between node-, subgraph-, and graph-level embeddings alleviating the burden of considering the edge features. The proposed method is evaluated on the molecule generation tasks in the conditional and unconditional settings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "* The auxiliary generation of node and subgraph embeddings can enrich the forwarded information of the diffusion process. Specifically, even though PS-VAE requires only the graph embedding in the decoding stage, the authors propose to generate the node and subgraph embedding along with the graph embedding. Defining the correlated generative processes in this paper could convey more information to the model."
            },
            "weaknesses": {
                "value": "* It is not clear why the authors select a way to generate graph embeddings and then decode them. Accessing the graph embedding could contain less information than accessing the subgraph- or node- and edge-level latent variables. The \n* The name of the proposed method is misleading. The goal of the proposed method is to generate the hierarchical latent variables. However, the name (Hierarchical Graph Latent Diffusion Model) can be misinterpreted as a sequence of the diffusion models to generate the latent variables.\n* The authors report the validity, uniqueness, and novelty as the main results. However, these metrics seem to be restricted to only measure the sample diversity.\n>- For the validity, it is unfair to compare with the denoising diffusion model such as GDSS and DiGress, as the decoding stage of the proposed method and PS-VAE intrinsically do the validity check while linking the fragments. Therefore, reporting and comparing the validity are not enough to demonstrate the effectiveness of the proposed method.\n>- For the uniqueness and novelty, they demonstrate that the generative model can guarantee sample diversity. However, to demonstrate whether the generative model precisely learns the data distribution, reporting the uniqueness and the novelty is not enough. Please note that recent works [1,2] leverage FCD, Scaffold similarity, SNN and NSPDK to measure the difference of the generated distribution and the data distribution. Therefore, I believe that measuring the uniqueness and novelty is important, but to demonstrate the effectiveness, it would be better to measure the distributions of the generated molecules.\n* The efficiency of the proposed methods seems to come from the light model architecture with smaller dimensions than the model architecture used in the DiGress.\n\n[1] Vignac, Clement, et al. \"Digress: Discrete denoising diffusion for graph generation.\" arXiv preprint arXiv:2209.14734 (2022).\n\n[2] Jo, Jaehyeong, Seul Lee, and Sung Ju Hwang. \"Score-based generative modeling of graphs via the system of stochastic differential equations.\" International Conference on Machine Learning. PMLR, 2022."
            },
            "questions": {
                "value": "For clarification, I would appreciate if the authors provided an explanation of my questions.\n1. In Section 4.1.1, to my understanding, the number of subgraph embeddings should not be the number of nodes. If so, how do you sample the number of subgraph embeddings at the beginning of the sampling stage?\n2. How do you get the subgraph embeddings from the PS-VAE architecture?\n3. In Table 3, how did you measure the mean absolute error (MAE) on the unconditional setting? Does it mean training on the selected 100 molecules without the conditions?\n4. Why are some reported values different from the original papers? For example, the novelty and uniqueness of PS-VAE on the QM9 dataset and the validity of GDSS on the ZINC250k dataset."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6846/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698670806464,
        "cdate": 1698670806464,
        "tmdate": 1699636793430,
        "mdate": 1699636793430,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "nzzNHxdtnQ",
        "forum": "RSincg5RBe",
        "replyto": "RSincg5RBe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6846/Reviewer_NAaX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6846/Reviewer_NAaX"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a form of latent diffusion ala Rombach et al. for graphs, in particular, molecule generation. For this they combine a PS-VAE autoencoder with a DDIM style denoising diffusion model which leverages a hierarchy-aware GNN which uses a GAT style subgraph embedding update and PNA pooling for the graph embedding update at every layer.  The method is compared against it's constitutent components and two SotA Diffusion baselines (Digress/GDSS) as well as VAE and other methods on QM9,ZINC250K and Guacamol."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Overall a good \"Nothing to complain about\" paper.\n\nOriginality:\n\nLatent diffusion for graphs was a thing waiting to be done, but it is still worth doing. The hierarchical block is a nice construction, as is the continuous-discrete combo.\n\nQuality: The evaluation including Guacamol is good, the QM9 and ZINC250k benchmarks show impressive results.\nclarity: The paper is very clearly presented and the appendix, while sparse, gives most of the information required for presenting things.\nSignificance: Getting hierarchical graph modeling like this going is likely to have a very high impapct, iff the method generalizes."
            },
            "weaknesses": {
                "value": "- I'd like to see error bars indicating variance accross multiple seeds  if possible\n- While QM9 and ZINC250k performance is imprressive, these graphs are kind of solved. What is the performance on MOSES or shapenet?"
            },
            "questions": {
                "value": "1. To clarify, you are evaluating all datasets without hydrogens?\n2. The PS-VAE is not permutation equivariant right (or does it canonicalize things)? Did you do any experiments with a purely equivariant backbone?\n3. Purely because I found this [paper today](https://arxiv.org/abs/2210.02410) and found the idea exciting, if you manage to perform any diversity quantification using graph embedding similarity across the datasets, I'd be curious how the models differ. This is purely a nerd sharing a neat idea though, not a critique of the paper.\n4.  There is no limitations section which is always sus, are there really *no* downsides and limitations worth discussing?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6846/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698696084022,
        "cdate": 1698696084022,
        "tmdate": 1699636793264,
        "mdate": 1699636793264,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yVzW7hL4Go",
        "forum": "RSincg5RBe",
        "replyto": "RSincg5RBe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6846/Reviewer_bWg3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6846/Reviewer_bWg3"
        ],
        "content": {
            "summary": {
                "value": "The submission proposes a latent diffusion model for graph generation. The problem definition is very common and can be treated as a distribution-fitting problem. The framework utilizes PS-VAE as an encoding-decoding model. And apply a diffusion model over latent variables."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The presentation is good.\n2. The structure design for the diffusion model is reasonable."
            },
            "weaknesses": {
                "value": "1. Equation (2) is not correct. The probability for each node is computed twice. I think the correct definition should be $\\prod p(x_i) \\prod \\prod (e_{ij})$.\n\n2. The submission claims that they first introduce the latent diffusion model into graph generation. This overclaims the contributions. [1] and many other previous works use latent diffusion models for graph generation tasks. I think the basic idea is the same: only diffuse node variables, and decode edge types from them. This is very common in the area.\n\n3. It is not reasonable to design a hierarchical diffusion model. There is no need to sample three variables $z^x, z^M, z^G$ at the same time. As a hierarchical model, the decoding process of PS-VAE is $ G \\sim q(G|z^M)q(z^M|z^G)$. That is, decoding a subgraph from a graph-level vector by a GRU, and then predicting the connection for the subgraphs. So actually, we only need to define a diffusion model over graph-level $z^G$. During the sampling process, we first sample $z^G$ from the diffusion model, and then use decoding of PS-VAE to get the graph. The current framework actually learns the decoding part twice, during the training of PS-VAE, the relationship between each level has been learned already. However, the diffusion model learns it one more time.\n\n4. The results lack MMD metrics. I think it is very important to check the distribution of the graphs.\n\n[1] https://arxiv.org/pdf/2211.10794.pdf"
            },
            "questions": {
                "value": "1. \"However, these approaches sacrifice the random exploration ability to ensure that the final noisy data conforms to the appropriate discrete category distribution. \" Why do you make such claims? The definition for the distribution of the discrete variables is different. And people can also define a discrete diffusion process over them such as [1] and many other works. The performance is also very good and I think people should select models based on the specific problem. There is no any conclusion to support that continuous features is better than discrete process.\n\n[1]https://arxiv.org/pdf/2209.14734.pdf"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6846/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698839796528,
        "cdate": 1698839796528,
        "tmdate": 1699636793167,
        "mdate": 1699636793167,
        "license": "CC BY 4.0",
        "version": 2
    }
]