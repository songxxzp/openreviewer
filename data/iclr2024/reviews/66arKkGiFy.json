[
    {
        "id": "tApjb562mq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1943/Reviewer_WmLx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1943/Reviewer_WmLx"
        ],
        "forum": "66arKkGiFy",
        "replyto": "66arKkGiFy",
        "content": {
            "summary": {
                "value": "The paper proposes a theory of the plug-and-play unadjusted Langevin algorithm (PnP-ULA) to solve inverse problems, that builds and improves upon a prior work [1]. Specifically, the main theorem quantifies the distributional error of PnP-ULA under prior shift and the likelihood shift, which are both practical questions in the field of inverse imaging. Numerical experiments including the analytical GMM experiment and the image deblurring experiment solidify the correctness of the theorem."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is well-written, concise, and clear.\n\n2. The theory given in the paper is solid, with numerical experiments building support for the proposed theorem. The theory is practical and non-vacuous, as mismatch in the prior or in the likelihood happens (at least to a minimal amount) on virtually every application that you can think of.\n\n3. The subject of the paper is well-suited for the conference, given the popularity of diffusion models on solving inverse problems."
            },
            "weaknesses": {
                "value": "1. The prior mismatch model, and the likelihood mismatch model, are not too realistic.\n\n1-1. For the prior mismatch, it would be more interesting if one could see the effect when the underlying training distributions are different. For example, in the context of medical imaging, [5,6] demonstrated that diffusion models are particularly robust under prior distribution shifts. A discussion would be useful.\n\nNote that I understand why the authors opted for the number of parameters for a DNN when they assumed a mismatch from the perfect MMSE estimator. However, given the current landscape of ML/generative AI, this situation would be easily solvable by more compute, whereas solving the data distribution shift is a much harder and realistic problem.\n\n2. The authors cite [1] as an example of a mismatched imaging forward model. Correct me if I am wrong, but as far as I understand, when using unconditional denoisers as in PnP-ULA, [1] uses the exact forward operator that were used to generate the measurement. I believe references such as [2-4] would be more relevant.\n\n\n\n**References**\n\n[1] G\u00fcng\u00f6r, Alper, et al. \"Adaptive diffusion priors for accelerated MRI reconstruction.\" Medical Image Analysis (2023): 102872.\n\n[2] Wirgin, Armand. \"The inverse crime.\" 2004.\n\n[3] Guerquin-Kern, Matthieu, et al. \"Realistic analytical phantoms for parallel magnetic resonance imaging.\" IEEE Transactions on Medical Imaging 31.3 (2011): 626-636.\n\n[4] Blanke, Stephanie E., Bernadette N. Hahn, and Anne Wald. \"Inverse problems with inexact forward operator: iterative regularization and application in dynamic imaging.\" Inverse Problems 36.12 (2020): 124001.\n\n[5] Jalal, Ajil, et al. \"Robust compressed sensing mri with deep generative priors.\" NeurIPS 2021.\n\n[6] Chung, Hyungjin, and Jong Chul Ye. \"Score-based diffusion models for accelerated MRI.\" Medical image analysis 80 (2022): 102479."
            },
            "questions": {
                "value": "1. In the forward model shift experiment on color image deblurring, what happens if one takes $\\sigma > 3$, and taking to the extreme, when one uses a uniform blur kernel?\n\n2. For image deblurring, what happens when you have an anisotropic blur kernel, but you use an isotropic kernel for inference?\n\n3. Two different versions of references are given for [1]\n\n4. It is probably better to cite [2] rather than [3] for score-matching (pg. 2)\n\n\n\n\n\n**References**\n\n[1] Chung, Hyungjin, et al. \"Diffusion posterior sampling for general noisy inverse problems.\" ICLR 2023.\n\n[2] Vincent, Pascal. \"A connection between score matching and denoising autoencoders.\" Neural computation 23.7 (2011): 1661-1674.\n\n[3] Dhariwal, Prafulla, and Alexander Nichol. \"Diffusion models beat gans on image synthesis.\" NeurIPS 2022"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1943/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1943/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1943/Reviewer_WmLx"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1943/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697405231988,
        "cdate": 1697405231988,
        "tmdate": 1699636125687,
        "mdate": 1699636125687,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yD7JybYEpK",
        "forum": "66arKkGiFy",
        "replyto": "66arKkGiFy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1943/Reviewer_rc8y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1943/Reviewer_rc8y"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the error bound of the plug-and-play unadjusted Langevin algorithm (PnP-ULA) under mismatched posterior distribution owing to mismatched data fidelity and/or denoiser. After rigorously deriving their main theoretical results, they provide some numerical experiments with simple settings to further support their theory."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Sections 1\u20134 are generally clearly written. The readers can get what the authors try to convey without diving into the mathematical details.\n- The quantification of the error bound for PnP-ULA under a mismatched posterior distribution is of theoretical importance."
            },
            "weaknesses": {
                "value": "- The section associated with the numerical experiment is hard to dig out. Particularly, it's not easy to understand how and why the proposed setting can be adopted to validate the theoretical corollary.\n- As claimed by the authors, \"our results can be seen as a PnP counterpart of existing results on diffusion models.\", which therefore weakens the novelty of this paper.\n- It seems like the theoretical results drawn rely on \"oracle\" information that is unavailable in practice. So the practical use of this theoretical tool is largely unclear for me."
            },
            "questions": {
                "value": "See above\n\n____\nAfter rebuttal: the authors addressed my concerns, thus I raise my score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1943/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1943/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1943/Reviewer_rc8y"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1943/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698694094035,
        "cdate": 1698694094035,
        "tmdate": 1700926752449,
        "mdate": 1700926752449,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HK4LoueqbS",
        "forum": "66arKkGiFy",
        "replyto": "66arKkGiFy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1943/Reviewer_Uuqz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1943/Reviewer_Uuqz"
        ],
        "content": {
            "summary": {
                "value": "This paper considers sensitivity analysis of posterior sampling in inverse problems using diffusion models. It analyzes the effects of mismatches to the drift function on the stationary distribution of Langevin sampling.The mismatch can arise due to uncertainty in the forward operator and due to the denoiser not being exactly the MMSE denoiser.\n\nThe main result is that the stationary distributions differ proportional to a pseudometric that depends on the drift mismatch."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The considered problem is relevant, especially in medical imaging, where we want algorithms to be robust to mismatch in the forward model.\n\nInverse problems using diffusion models is also an active area of research and the proposed results could be relevant."
            },
            "weaknesses": {
                "value": "- The main result in Theorem 1 shows that the TV between the stationary distributions of two Markov chains that have different drift functions can be bounded in terms of the proposed ``posterior-$L_2$ pseudometric''. This pseudometric is defined in terms of the expectation of the difference between the two drift functions when samples are drawn from the stationary distribution of one of the drifts. It's not clear at all how this pseudo metric behaves, and whether it is sufficiently small for two drifts that are close. (the $\\epsilon$ used in the results is only for the mollification level present in the denoiser, and has nothing to do with the closesness of the drifts themselves). \n\n- It is also not clear how different the two stationary distributions are when compared to the continuous stationary distribution. This can be very different due to discretization error, etc.\n\n- There is very little comparison to existing results in the literature. Other than saying that their results are backwards compatible with Laumont et al 2022, the authors do not state what benefits / drawbacks their results face.\n\n- The paper considers Langevin sampling, which is known to not mix very well -- most theoretical results in the literature consider ODE / SDE solvers for an Ornstein\u2013Uhlenbeck process.\n\n- The upper bounds in Theorem 1 are specified in terms of $A_0, A_1, B_0, B_1$, without any mention on the dimension dependence of these quantities.\n\n- Some statements are unsubstantiated. In the contributions section, the authors claim \"This paper stresses that in the case of mismatched operators, there are no error accumulations.\" I don't see why this would we be true."
            },
            "questions": {
                "value": "Listed in the weaknesses section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1943/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698874300104,
        "cdate": 1698874300104,
        "tmdate": 1699636125540,
        "mdate": 1699636125540,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AayKm0FhGo",
        "forum": "66arKkGiFy",
        "replyto": "66arKkGiFy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1943/Reviewer_eQV2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1943/Reviewer_eQV2"
        ],
        "content": {
            "summary": {
                "value": "The authors study the influence of model mismatch on the invariant distribution of a certain model-based posterior sampler based on the unadjusted Langevin Algorithm. The model (a forward operator in some imaging application) and the prior (incorporated via a denoiser) factor in through the drift term. The authors prove that the distribution drift is controlled by the total size of the model mismatch (in the forward model and the denoiser)."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "It is great that the paper proves a theoretical result about a relevant topic where most work is highly empirical. The setting is very clear and the derivations seem sound (although I have not checked in great detail.) The authors work under fairly general assumptions and also illustrate their bounds empirically."
            },
            "weaknesses": {
                "value": "- The contribution is somewhat incremental given all the preparatory work in Laumont et al. 2022. Model mismatch is certainly a relevant topic and it is nice to have a paper about it so this is a somewhat subjective statement relative to papers I've reviewed for ICLR this year. \n\n- The prose is waffly, with too much hyperbole. An example: \"In this section, our focal point resides in the investigation of the profound impact of a drift shift on the invariant distribution of the PnP-ULA Markov chain\" which could be \"In this section we study the impact of drift shift on...\".\n\n- There are also numerous typos and broken sentences, especially in the appendices."
            },
            "questions": {
                "value": "- Under \"Contributions\" you say that you \"provide a more explicit re-evaluation of the previous convergence results...\", but I am not sure what this means.\n\n- In \"we focus on the task of sampling the posterior distribution to reconstruct various solutions...\", what is meant by \"various solutions\"?\n\n- In \"... Markov chain can be naturally obtained from an Euler-Maruyama discretisation by reformulating the process ...\", what is meant by \"reformulating\"?\n\n- Before equation (7), Wasserstein norm should be Wasserstein metric (or distance); before (7), TV distance should be TV norm (which is what is defined in (7)). (Also: why are Rd vectors bold in (8) and not in (7)?)\n\n- \"pseudometric between two functions in Rd\" -> taking values in Rd\n\n- In Corollary 1.3 which norm is || A^1 - A^2 ||?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1943/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699259256476,
        "cdate": 1699259256476,
        "tmdate": 1699636125474,
        "mdate": 1699636125474,
        "license": "CC BY 4.0",
        "version": 2
    }
]