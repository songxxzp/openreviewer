[
    {
        "id": "4F921OVNp5",
        "forum": "m7C04OET3V",
        "replyto": "m7C04OET3V",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3400/Reviewer_nMrZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3400/Reviewer_nMrZ"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel method for self-supervised learning (SSL) in tabular data domains using a pretext task based on the classical binning method. Instead of reconstructing raw values, the model is trained to reconstruct bin indices, which offers a unified approach to handle both categorical and numerical features. This method equips the encoder with the ability to handle irregularities typical in tabular data and standardizes features to alleviate discrepancies. Empirical evaluations across 25 public datasets demonstrated consistent improvements in representation learning performance for various downstream tasks. The binning technique not only improves unsupervised learning results but also offers effective pretraining strategies."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Introduction of a Novel Pretext Task: The paper introduces a unique approach for self-supervised learning in tabular data using the classical binning method. By focusing on reconstructing bin indices instead of raw values, it offers a new perspective in SSL for tabular datasets.\n\nAddressing Data Heterogeneity: The binning approach efficiently handles the inherent heterogeneity of tabular data (comprising both categorical and numerical features). It ensures all features are standardized, preventing any particular feature from dominating the learning process.\n\nWell-written: The paper presents its ideas and findings in a clear, organized, and articulate manner, making it accessible and informative."
            },
            "weaknesses": {
                "value": "W1 Performance:\n\nWhile the proposed framework works well in SSL settings, the performance in supervised settings is relatively marginal. Achieving the best results on MNIST, which is not purely tabular data, is less persuasive.  \n\nW2 Understanding the model's performance:\n\nIn SSL and supervised settings, there is limited theory analysis, discussion, or visualization to illustrate why the proposed method is effective or suboptimal. This is important as it might be critical for pushing the research of tabular data further."
            },
            "questions": {
                "value": "The main questions are listed in Weaknesses. I'd raise my score if they were appropriately addressed.\n\n\nCould the authors remind me of the difference between masking with random value in your paper and the previous pretext task of estimating mask vectors from corrupted tabular data? If it is not the same, could the authors compare it in your experiments?[1]\n\n\n[1] VIME: Extending the Success of Self- and Semi-supervised Learning to Tabular Domain"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3400/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3400/Reviewer_nMrZ",
                    "ICLR.cc/2024/Conference/Submission3400/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3400/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698456796980,
        "cdate": 1698456796980,
        "tmdate": 1700627331228,
        "mdate": 1700627331228,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rjsQ1zrWJ9",
        "forum": "m7C04OET3V",
        "replyto": "m7C04OET3V",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3400/Reviewer_j3rL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3400/Reviewer_j3rL"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new pretext task in pretext-task-based self-supervision that predicts the bin index or value of a given example  quantised/binned into intervals. The work closely follows the philosophy of VIME, with the pretext task being thought of an encoder, and the downstream classification / regression as the decoder. The method evaluates itself against 8 benchmark methods over 12 datasets (Tables 2-3), and performs ablation over 3 components of the binning operation, and offers reasoning over hyperparameters e.g. the number of bins."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The  idea of binning the ranges of input variables is novel, and seems to have been developed in the spirit of course-graining these ranges, something that tree-based supervised models seem to like, thereby bringing that courseness as an inductive bias from trees as Gorshiniy'22 suggests is useful. The presented evaluations cover both quantification axes. Those evaluations have been performeed on both binary and multi-class classification, and regression."
            },
            "weaknesses": {
                "value": "The connection to Gorshiniy'22 's idea seems hypothetical at best, and does not convince that quantising has the desired causality. \n\nThe paper goes into VIME's masking method as if it was proposed here. The masking method is indeed used in conjugation with binning, but  may be de-stressed here."
            },
            "questions": {
                "value": "In the ablation study, shouldn't the absence of grouping have beenc investigated, rather than its presence? Isn't the baseline the one with all three configurations True? \n\nHow would course-graining compare against a method like SSS'23 (Syed and Mirza) that assumes smoothness but adds jumps in discrete sub-Gaussian steps?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3400/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698664778210,
        "cdate": 1698664778210,
        "tmdate": 1699636291036,
        "mdate": 1699636291036,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YfJqXnfyHp",
        "forum": "m7C04OET3V",
        "replyto": "m7C04OET3V",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3400/Reviewer_m34i"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3400/Reviewer_m34i"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a pretext task aimed at enhancing self-supervised learning within tabular domains. The core concept involves discretizing numerical features through binning and encoding categorical features with one-hot vectors. The experiments conducted demonstrate that this pretext task leads to improved feature representations and more effective weightings for downstream tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The method is comprehensible, and the illustrations are clear.\n- The binning solution is intriguing, and this pretext task can be seamlessly integrated with numerous modifications of SSL methods.\n- The paper effectively demonstrates that the most crucial aspect of this pretext task lies in grouping similar values."
            },
            "weaknesses": {
                "value": "- The number of bins significantly impacts the performance of the representation, but it can be challenging to predefine.\n- It appears that this method primarily enhances a single network's performance on a specific dataset. What about neural networks trained on multiple heterogeneous tabular datasets?"
            },
            "questions": {
                "value": "- What is the recommended approach for defining the hyperparameter related to the number of bins?\n- Lately, several works like \"transtab\" have emerged, aiming to generate a single neural network for multiple heterogeneous datasets. Can the method proposed in this paper contribute to enhancing the representation of a global tabular network across various datasets with differing feature spaces?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3400/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3400/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3400/Reviewer_m34i"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3400/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698742707129,
        "cdate": 1698742707129,
        "tmdate": 1699636290964,
        "mdate": 1699636290964,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gq7VcQ2ItT",
        "forum": "m7C04OET3V",
        "replyto": "m7C04OET3V",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3400/Reviewer_wzCM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3400/Reviewer_wzCM"
        ],
        "content": {
            "summary": {
                "value": "This study aims to enhance the representation learning capabilities of deep networks for managing the heterogeneous features present in tabular data. Building upon previous insights in the field of tabular learning, this research highlights the challenge faced by deep networks in modeling irregular functions. To address this issue, the paper introduces an innovative self-supervised pretext task based on the classical data mining method of binning. Unlike previous methods that focused on reconstructing the original tabular cell values, this approach attempts to reconstruct bin indices. By doing so, the encoder captures irregular dependencies and mitigates feature heterogeneity. The study concludes with extensive empirical experiments conducted across various tabular datasets, demonstrating the method's efficacy in enhancing tabular representation learning performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper offers a fresh perspective building upon prior research [1]. The proposed solution is not only reasonable but also simplified, ensuring compatibility with various encoder architectures.\n - This paper is well-crafted, providing a precise and clear description of the method employed.\n - The paper meticulously outlines the experimental settings, conducting the experiments ten times with different random seeds and presenting standard deviations. This rigorous approach enhances the paper's credibility.\n - The code implementation is exemplary, featuring a well-organized structure that is easy to follow. It includes comprehensive parameters, pre-trained weights, and detailed training logs, making it highly reproducible.\n  - Remarkably, the method exhibits efficiency, requiring minimal computational resources, specifically just a single NVIDIA GeForce RTX3090.\n\n[1] Grinsztajn L, Oyallon E, Varoquaux G. Why do tree-based models still outperform deep learning on typical tabular data?[J]. Advances in Neural Information Processing Systems, 2022, 35: 507-520."
            },
            "weaknesses": {
                "value": "- The primary limitation of this work lies in its experimental results. Table 3 illustrates that this method still falls short of achieving comparable performance to the tree-based method even though they claim this method can mitigate the impact of feature heterogeneity. Additionally, the method's performance lags behind that of state-of-the-art neural network-based models such as T2G-Former.\n- The pretext task has only been tested on TRUE. It is advisable to diversify the experiment results by trying different backbone models to ensure the robustness and versatility of the proposed approach.\n- The paper overlooks significant related works, such as TabGSL [2], a work focused on supervised tabular prediction, as well as Tabular Language Models (TaLMs), TableFormer [3], and TABBIE[4]. Incorporating these relevant studies would provide a more comprehensive understanding of the research landscape.\n- It is recommended to incorporate interpretable analysis methods. For example, visualizing decision boundaries could validate whether binning as a pretext task effectively fits irregular functions, enhancing the paper's explanatory power.\n\n[2] Liao J C, Li C T. TabGSL: Graph Structure Learning for Tabular Data Prediction[J]. arXiv preprint arXiv:2305.15843, 2023.\n\n[3] Yang J, Gupta A, Upadhyay S, et al. TableFormer: Robust transformer modeling for table-text encoding[J]. arXiv preprint arXiv:2203.00274, 2022.\n\n[4] Iida H, Thai D, Manjunatha V, et al. Tabbie: Pretrained representations of tabular data[J]. arXiv preprint arXiv:2105.02584, 2021."
            },
            "questions": {
                "value": "- Why does the experimental performance lag behind tree-based methods, recap the motivation for this method is to enable neural networks to handle irregular functions akin to tree-based methods.\n- Why does the performance of this method lag behind the SoTA NN-based methods?\n- How long does this method take to pre-train on a single NVIDIA GPU?\n- How about the transfer ability for bining pretext task? Can it achieve zero-shot or few-shot learning?\n- How do self-supervised learning methods compare to methods based on large language models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3400/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3400/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3400/Reviewer_wzCM"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3400/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698839452705,
        "cdate": 1698839452705,
        "tmdate": 1699636290864,
        "mdate": 1699636290864,
        "license": "CC BY 4.0",
        "version": 2
    }
]