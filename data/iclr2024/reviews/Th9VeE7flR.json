[
    {
        "id": "IIWMgNafHF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2586/Reviewer_CpfT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2586/Reviewer_CpfT"
        ],
        "forum": "Th9VeE7flR",
        "replyto": "Th9VeE7flR",
        "content": {
            "summary": {
                "value": "This paper addresses source-free domain adaptation (SFDA) task, where a source pretrained model is adapted to an unlabeled target domain in the absence of the source data. The submission proposes to tackle the problem by label propagation, along with considering historical model embedding and introducing data augmentations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Motivation is clear and sound: The proposed method is based on label propagation, to deal with the noisy pseudo labels/predictions, historical feature embeddings are considered in the affinity matrix definition.\n\n- Experiments on several benchmarks show the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "- Label propagation is already investigated in the normal domain adaptation [a] (and also source-free domain adaptation [b]), the authors may consider discussing those works in the related works section.\n\n\n- One major improvement comes from data augmentations, which is a universal way to improve the performance in unsupervised learning. I think in the ablation study, results with Baseline + HAF should be provided on all datasets, to indicate how much improvement data augmentation could introduce into label propagation.\n\n- (Major) Some techniques used in the submission are quite general and not new, such as BN statistics adaptation (which is achieved by forwarding target data to source model before starting adaptation), and uncertainty weight in Eq. 8 (and also diversity loss in Eq. 9). As there is no ablation study about those techniques, I posit they can improve all existing SFDA methods. For fair comparison, at least results without BN adaptation should be provided. Or BN adaptation should be added to all existing methods in the main tables, otherwise we do not know how many gains are really from label propagation, which is the key module in the submission.\n\n- In the appendix, it mentions that there are two strong augmentations deployed, how about only using one or multiple ones in turn?\n\n- The hyperparameter $\\lambda$ is set manually (0.1 for office datasets and 0 for visda), how about the performance without using diversity loss, or using a decaying weight on this term?\n\n***Reference:***\n\n[a] Label Propagation with Augmented Anchors: A Simple Semi-Supervised Learning baseline for Unsupervised Domain Adaptation. ECCV 2020\n\n[b] Chaos to Order: A Label Propagation Perspective on Source-Free Domain Adaptation. ACM MM 2023"
            },
            "questions": {
                "value": "Please check the weakness above, without detailed ablation studies, currently I think the submission is a bit incremental by combining several general techniques.\n\nBy the way, I think the method is also somehow similar to BYOL. The label propagation part with weak augmentation could be regarded as the projection layer in BYOL, though the operation in the submission is in the output space (classification prediction). The authors could think about it."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2586/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2586/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2586/Reviewer_CpfT"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2586/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697437691107,
        "cdate": 1697437691107,
        "tmdate": 1700362599143,
        "mdate": 1700362599143,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "V1Ds3dTSiZ",
        "forum": "Th9VeE7flR",
        "replyto": "Th9VeE7flR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2586/Reviewer_113e"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2586/Reviewer_113e"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to leverage label propagation and self-supervised learning methods in SFDA (Source-Free Domain Adaptation) problem, aiming to obtain the global semantic information and reduce the confirmation bias in the target domain. Specifically, the authors use both historical model snapshots and the current model to build the affinity matrix and further adopt the label propagation technique to refine pseudo-labels for the target domain. Moreover, they also use batch normalization adaptation and self-supervised learning methods, including data augmentation, to improve their algorithm. The experimental results on several SFDA benchmarks verify the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well-organized and easy to follow.\n    \n- The idea of introducing label propagation in SFDA is interesting. However, some recent works [1] also mentioned this research topic, which reduces the novelty of this paper.\n    \n- The proposed method is concise, and the experimental results are relatively promising.\n    \n [1] Chunwei Wu, Guitao Cao, Yan Li, Xidong Xi, Wenming Cao, Hong Wang, \u201cChaos to Order: A Label Propagation Perspective on Source-Free Domain Adaptation.\u201d ACM Multimedia 2023"
            },
            "weaknesses": {
                "value": "- My main concern with this paper is about the novelty. The proposed method has two main contributions: the label propagation technique and the self-training framework. Recently, the self-training and data augmentation methods have been widely used in SFDA research. And some recent papers also adopted the label propagation in SFDA. [1,2] There lacks some necessary discussions compared with the state-of-the-art works.\n    \n- I have some questions about the experimental part. Please refer to the Question section.\n    \n\n[1] Chunwei Wu, Guitao Cao, Yan Li, Xidong Xi, Wenming Cao, Hong Wang, \u201cChaos to Order: A Label Propagation Perspective on Source-Free Domain Adaptation.\u201d ACM Multimedia 2023\n\n[2] Nazmul Karim, Niluthpol Chowdhury Mithun, Abhinav Rajvanshi, Han-pang Chiu, Supun Samarasekera, and Nazanin Rahnavard, \u201cC-sfda: A curriculum learning aided self-training framework for efficient source free domain adaptation.\u201d CVPR, 2023"
            },
            "questions": {
                "value": "I have some questions about the experimental part.\n\n- To build your affinity matrix while using the historical models, should you save all previous affinity matrices and also the neighbor banks? Is this method of storage consuming and effective?\n    \n- By observing the curves in Fig 4 left panel, does this mean the utility of the fused affinity matrix will be continuously decreasing during the adaptation process?\n    \n- Also, in Figure 4, right panel, why, at epoch 0 (the initial step), are the ratios of two affinities not equal?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2586/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2586/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2586/Reviewer_113e"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2586/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698814874105,
        "cdate": 1698814874105,
        "tmdate": 1699636196049,
        "mdate": 1699636196049,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tfMWMaDEEI",
        "forum": "Th9VeE7flR",
        "replyto": "Th9VeE7flR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2586/Reviewer_CVkN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2586/Reviewer_CVkN"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new approach for source-free domain adaptation called Enhanced Label Propagation through Affinity Matrix Fusion. This approach is designed to enhance the quality of pseudo-labels in the target domain by synergizing the predictions of an old model with the current model. To achieve this, the method constructs an affinity matrix and employs label propagation techniques to refine pseudo-labels. These refined labels are then used to calculate gradients from an alternative augmentation perspective. Additionally, the approach incorporates a prediction diversity loss and leverages BN adaptation techniques. Experimental evaluations are conducted on popular datasets such as Office-31, Office-Home, and VisDA-C, yielding results that are comparable to state-of-the-art methods."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well written.\n2. As stated by the authors, label propagation technology was originally employed for making predictions on unlabeled data in a semi-supervised setting. However, this paper introduces a novel application by using it to refine pseudo-labels."
            },
            "weaknesses": {
                "value": "1. This proposed method combines several previously established techniques, such as label propagation, consistent loss with two views, BN adaptation, and prediction diversity loss. It lacks novelty in terms of its technical components. It seems the sole innovation lies in applying label propagation technology for refining pseudo labels additionally using historical model outputs.\n2. The experimental results show only a marginal advantage on Office-31 and Office-Home datasets."
            },
            "questions": {
                "value": "1. Why exclusively employ the previous model h_{t-m} instead of a fused model (such as \\sum_{i=1}^{m} h_{t-i}) for retrieving historical information?\n2. Is there a particular rationale behind the choice of the negative dot product loss as opposed to other commonly used loss functions? I couldn't find any ablation results in the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2586/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698827493728,
        "cdate": 1698827493728,
        "tmdate": 1699636195958,
        "mdate": 1699636195958,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sZ5AaPVwm2",
        "forum": "Th9VeE7flR",
        "replyto": "Th9VeE7flR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2586/Reviewer_j3cS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2586/Reviewer_j3cS"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on source-free domain adaptation and proposes to utilize an affinity matrix derived from current and historical models during the label propagation process. The proposed method takes advantage of different snapshots of the model to obtain a more accurate representation of the underlying graph structure, significantly enhancing the efficacy of label propagation and resulting in more refined pseudo-labels. Extensive experiments show that the proposed method achieves superior domain adaptation performance, which does not require source domains."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "-a. It incorporates label propagation to leverage both local and global relationships between the instances in the target domain for source-free domain adaptation.\n- b. It investigates the critical role of data augmentation as a fundamental component in the self-training-based SFDA framework.\n- c. The experiments are extensive, and the performance of the proposed method is promising in source-free domain adaptation scenario."
            },
            "weaknesses": {
                "value": "1. The proposed method seems a bit similar to HCL (Huang et al., 2021) which also exploits current and historical models for source-free domain adaptation. It would be better to discuss and compare the differences, pros and cons of HCL and the proposed method.\n2. The proposed label propagation method seems similar the method of \u201cTemporal Assembling for semi-supervised learning\u201d. It would be better to discuss and compare the differences, pros and cons of this method and the proposed method.\n3. The ablation study is not clear. Why the baseline in Table 4 outperforms the source model by a large margin? Do all the other methods in other tables use the same baseline as in Table 4?"
            },
            "questions": {
                "value": "see Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2586/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698845618781,
        "cdate": 1698845618781,
        "tmdate": 1699636195888,
        "mdate": 1699636195888,
        "license": "CC BY 4.0",
        "version": 2
    }
]