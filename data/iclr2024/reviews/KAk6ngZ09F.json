[
    {
        "id": "mYF66WIPs3",
        "forum": "KAk6ngZ09F",
        "replyto": "KAk6ngZ09F",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2637/Reviewer_ZvvG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2637/Reviewer_ZvvG"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a contrastive loss trained model for data filtering"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The finding is interesting that a model's data filtering ability is not correlated with its image task performance. Actually, which is also discussed by [1].\n2. The finding is interesting and useful that data quality determines the trained model's filtering performance.\n3. The proposed data filtering network achieves impressive results on data filtering.\n\n[1] Yu, Haichao, et al. \"The Devil is in the Details: A Deep Dive into the Rabbit Hole of Data Filtering.\" arXiv preprint arXiv:2309.15954 (2023)."
            },
            "weaknesses": {
                "value": "1. The HQITP-350M dataset is the key to the success of the DFN, so it is disappointing that it cannot be publicly accessible by the research community."
            },
            "questions": {
                "value": "1. How many runs are conducted to get the quantitative experimental results in the paper (e.g., Table 1-6)? Are the standard deviations sufficiently smaller than the differences between different settings?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2637/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697935217711,
        "cdate": 1697935217711,
        "tmdate": 1699636203798,
        "mdate": 1699636203798,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "agUwY6QfNL",
        "forum": "KAk6ngZ09F",
        "replyto": "KAk6ngZ09F",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2637/Reviewer_2tx6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2637/Reviewer_2tx6"
        ],
        "content": {
            "summary": {
                "value": "This paper's focus is on dataset filtering for contrastive language-image pre-training (CLIP). The discussion on data-filtering networks (DFN) is motivated by the use of pretrained CLIP models (referred to as the DFN) to filter datasets by discarding image-caption pairs whose embeddings (as produced by the DFN) are not similar to each other. The paper's main observation is that accuracy of the DFN on a given task doesn't predict how good a subset it selects (even for that task). The paper then uses this to motivate the creation of new DFNs that are trained on high quality image-caption data and then fine-tuned on \"important\" datasets. The paper confirms the effectiveness of the approach by demonstrating competitive performance on ImageNet Zero-Shot, ImageNet Distribution Shift, VTAB, Retrieval etc when filtering DataComp's medium, large and xlarge data."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The problem of better understanding and improving DFNs is extremely interesting. \n\n2. The observation that a DFN with good accuracy on a given task doesn't necessarily induce a good dataset on that task is very interesting.\n\n3. Several large-scale experiments are conducted to explore the effectiveness of the approach."
            },
            "weaknesses": {
                "value": "It seems like the improvement in performance over other DFNs is primary in ImageNet (and dist. shift) Zero-Shot as well as Retrieval. It seems like without \"finetuning\" on MS COCO training set, Flickr30k training set, and ImageNet 1K training set with ImageNet templates as captions, these performance improvements nearly disappear. While the main paper does ablate over fine-tuning, it doesn't highlight just how significant fine-tuning on these particular datasets is (primarily because these are the datasets we wish to evaluate on). \n\nPersonally, I don't find the conclusion that training a DFN on high-quality image-caption datasets allows the induced subsets to be higher quality very surprising or interesting. I do think a fine-grained study into specific properties of the pretraining data (e.g. similarity with downstream tasks' data, diversity of data, data imbalance) can make this work interesting and help further our understanding of DFNs. In it's current form, apart from the observation on ImageNet accuracy of DFN and that of the model trained on the induced dataset, I don't see what the contribution of this work is towards understanding DFNs. \n\nWhile I recognize, the improvement in accuracy on ImageNet, Retrieval etc., the importance of fine-tuning the DFN on the evaluation datasets leads to me believe that this approach may not generalize to other downstream tasks (as evidenced by the < 1% improvement on VTAB)."
            },
            "questions": {
                "value": "1) Is there any explanation for why a DFN with higher ImageNet accuracy is not always better at selecting data for ImageNet?\n\n2) Could the authors go into greater depth about the impacts of fine-tuning i.e. is the key component of training a good DFN fine-tuning on the evaluation datasets. If so, how do the authors see these models as generalizing on unseen tasks? \n\n3) Is there a trade-off in accuracy on some datasets? For one of the models, could we see the per dataset accuracy for VTAB using different DFNs and see if there are trade-offs in which datasets improve and which don't?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2637/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2637/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2637/Reviewer_2tx6"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2637/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698458459520,
        "cdate": 1698458459520,
        "tmdate": 1700485321223,
        "mdate": 1700485321223,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SrL5UMcjWl",
        "forum": "KAk6ngZ09F",
        "replyto": "KAk6ngZ09F",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2637/Reviewer_Q5jf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2637/Reviewer_Q5jf"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the problem of how to filter large-scale web-scrawled data into the training data for training CLIP models. The paper proposes a data filtering network (DFN) for this task. \n* Training DFN: DFN is a small CLIP model pretrained on a small but \u201chigh-quality\u201d dataset based on the authors\u2019 insight: (1) the performance of DFN itself is not correlated to the induced model trained on DFN\u2019s filtered data (Figure 3) and (2) Induced model\u2019s performance degrades as the quality of data drops (Figure 4).\n* Using DFN for filtering (inference): DFN filters out image-text pairs whose cosine similarities are lower than a threshold (Section 2).\n\nBased on the insight above, the paper curates the High-Quality Image-Text Pairs (HQITP-350M) dataset containing 357 million image-text samples with human-verified captions to train DFNs. In the experiments, the paper shows that CLIP models trained on DFN-induced datasets (1) achieve state-of-the-art performance, (2) improve model efficiency (i.e., training a smaller model that achieves similar performance with previous larger models), and (3) improve VQA performance. The paper will release the DFN-2B dataset filtered by DFN to facilitate future research."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. The paper studies an important problem of filtering training data for CLIP.\n2. The paper provides insight into the problem\u2014high-quality pretraining data for DFN matters.\n3. The proposed method achieves state-of-the-art performance with better model efficiency.\n4. The paper will release the DFN-2B dataset, which is a good contribution to the research community.\n5. The paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "### Major Concern\n\n**[Details of Curating HQITP-350M]** The paper does not provide many details about how to create the high-quality HQITP-350M dataset for training DFNs. This is really important because it is the key for training a good DFN as the paper claims. For example, how does the human verification process work? For example, do human annotators make a binary decision of whether image and text are matched or give a score on the scale of 0 to 1? If it is the latter one, how to threshold the scores? How many annotators are requested? What is the source dataset for curating the HQITP-350M dataset? How many image-text pairs are filtered out by humans?\n\n### Minor Concern\n\n**[Definition of Dataset Quality]** The paper defines the \u201cquality\u201d of CLIP\u2019s training data as the image-text similarity. I wonder if the authors consider other aspects to define the data quality, such as the quality of images and the quality of captions [1].\n\n### Minor suggestions\n\nFor evaluating the robustness against distributional shifts, the paper can also discuss or include more evaluation on newer OOD ImageNet benchmarks: ImageNet-D, ImageNet-X, and ImageNet-W.\n\nMissing citations to OOD ImageNet datasets used in the paper: ImageNet-Sketch [5],  ImageNet-A [6], ImageNet-V2 [7], ObjectNet [8], ImageNet-R [9] (caption in Figure 5).\n\n### References\n\n[1] Thao Nguyen, Samir Yitzhak Gadre, Gabriel Ilharco, Sewoong Oh, and Ludwig Schmidt, \u201cImproving Multimodal Datasets with Image Captioning,\u201d in NeurIPS Dataset and Benchmark Track, 2023.\n\n[2] Evgenia Rusak, Steffen Schneider, George Pachitariu, Luisa Eck, Peter Vincent Gehler, Oliver Bringmann, Wieland Brendel, and Matthias Bethge, \u201cIf your data distribution shifts, use self-learning,\u201d TMLR, 2022.\n\n[3] Badr Youbi Idrissi, Diane Bouchacourt, Randall Balestriero, Ivan Evtimov, Caner Hazirbas, Nicolas Ballas, Pascal Vincent, Michal Drozdzal, et al., \u201cImageNet-X: Understanding Model Mistakes with Factor of Variation Annotations,\u201d in ICLR, 2023.\n\n[4] Zhiheng Li, Ivan Evtimov, Albert Gordo, Caner Hazirbas, Tal Hassner, Cristian Canton Ferrer, Chenliang Xu, and Mark Ibrahim, \u201cA Whac-A-Mole Dilemma: Shortcuts Come in Multiples Where Mitigating One Amplifies Others,\u201d in CVPR, 2023.\n\n[5] Haohan Wang, Songwei Ge, Eric P. Xing, and Zachary C. Lipton, \u201cLearning Robust Global Representations by Penalizing Local Predictive Power,\u201d in NeurIPS, 2019.\n\n[6] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song, \u201cNatural Adversarial Examples,\u201d in CVPR, 2021.\n\n[7] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar, \u201cDo ImageNet Classifiers Generalize to ImageNet?,\u201d in ICML, 2019.\n\n[8] Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh Tenenbaum, and Boris Katz, \u201cObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models,\u201d in NeurIPS, 2019.\n\n[9] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, et al., \u201cThe Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization,\u201d in ICCV, 2021."
            },
            "questions": {
                "value": "In the rebuttal, I expect the authors to address my concerns regarding the following:\n1. Details of collecting the HQITP-350M dataset.\n2. More discussion on dataset quality"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2637/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698731112733,
        "cdate": 1698731112733,
        "tmdate": 1699636203613,
        "mdate": 1699636203613,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4bwpBfnKTi",
        "forum": "KAk6ngZ09F",
        "replyto": "KAk6ngZ09F",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2637/Reviewer_NYkb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2637/Reviewer_NYkb"
        ],
        "content": {
            "summary": {
                "value": "The paper compares the performance of (primarily) various configurations of CLIP models to filter image-text datasets for subsequently pretraining vision transformers. The authors make interesting empirical observations regarding the same, and are able to achieve an impressive accuracy vs. compute tradeoff on image-net using their best configuration of data filtering networks."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- SoTA compute vs. accuracy trade-off on image-net.\n- Thorough & insightful analyses / recipes to understand the effect of different kinds of confounders while training data-filtering networks on downstream performance.\n- Expected public release of the best-performing filtered datasets."
            },
            "weaknesses": {
                "value": "- The proposed approach is still a heuristic for filtering data, and is limited to pointwise filtering for the sake of parallelizability.\n- The cost of filtering: (1) training the data filtering networks; and (2) linear inference over the entire dataset; might be too much to be amortized or ignored in the final compute vs. accuracy trade-off (more in questions)."
            },
            "questions": {
                "value": "- Can you highlight the cost of filtering in e.g. Figure 1?\n- (Not relevant to the final rating) What do you think about applying these data filtering networks to other domains, e.g., text, or speech, or music?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2637/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2637/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2637/Reviewer_NYkb"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2637/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698917920080,
        "cdate": 1698917920080,
        "tmdate": 1699636203526,
        "mdate": 1699636203526,
        "license": "CC BY 4.0",
        "version": 2
    }
]