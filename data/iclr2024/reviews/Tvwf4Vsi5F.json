[
    {
        "id": "j5IWP9FjBS",
        "forum": "Tvwf4Vsi5F",
        "replyto": "Tvwf4Vsi5F",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7997/Reviewer_CVr6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7997/Reviewer_CVr6"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a pragmatic defence against a newly proposed practical threat model - namely adversaries utilising transfer attacks via publicly available surrogate models (with no ability to fine-tune models or perform query-based attacks). The defence is motivated by a game-theoretic perspective, and evaluated against a range of public models, attack algorithms and datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "A very well written and presented paper. The game-theoretic motivation is clear and provides a reasoned foundation for the approach taken - though that approach does not fully solve the game-theory problem as posed, the framework at least justifies the concept of the approach as more than just arbitrary/ad hoc.\n\nThorough experimental results including extensive ablation studies. The investigations, for example, of the effects of removing various groups of models on the accuracy was interesting. Further, investigating the cosine similarity of perturbations between attacks was suggestive of further structure to be explored in future work.\n\nThe candidate defence outperforms SOTA by a reasonable margin, and provides a practical mechanism in contrast to SOTA adversarial training. I think because of this practical performance requirement for their approach with still SOTA or better results, it could be a useful technique to use in practise or to consider developing further."
            },
            "weaknesses": {
                "value": "The \"slogan\" is not very catchy! :-)"
            },
            "questions": {
                "value": "In section 7.3, PUBDEF's vulnerability to white-box and surrogate attacks is mentioned. It would have been good to see some results on this if possible in order to give an idea of the degradation of performance that such weakening of the threat model would provide, i.e. effectively an ablation study on the threat model assumptions.\n\nIs there some value to considering a case in which the set of publicly available models visible to the attacker and defender are different. Maybe there are communities not visible to the defender, or perhaps it could be because of time-based issues (e.g. defender model generated and frozen but subsequent public models appear which are available to the attacker)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7997/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697846024976,
        "cdate": 1697846024976,
        "tmdate": 1699636985310,
        "mdate": 1699636985310,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NMDP97IDJD",
        "forum": "Tvwf4Vsi5F",
        "replyto": "Tvwf4Vsi5F",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7997/Reviewer_nPyj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7997/Reviewer_nPyj"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a defense against transfer-based evasion attacks. The defense uses publicly-available pretrained models to create adversarial attacks and incorporates these attacks in the training process of the defended model. The process is similar to adversarial training, however it uses adversarial examples from (multiple) other robust models rather than from the trained model itself."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ introduction is well written\n+ formulation is clear"
            },
            "weaknesses": {
                "value": "- evaluation is not convincing\n- no analysis on how expensive the method is, compared, e.g., to adversarial training"
            },
            "questions": {
                "value": "**Evaluation is not convincing, missing: white-box evaluation, query-based evaluation, other norms, targeted attacks, strong ensemble transfer attacks**\n\nThe defense should be tested fully, also against white-box evasion attacks. This is motivated by the fact that the authors are trying to make the model robust, hence they should demonstrate the robustness against worst-case adversaries. Otherwise, there is a high risk that the defense might be broken by stronger attacks. As the approach is similar to AT, the authors should demonstrate that the decision boundary of the model is in fact becoming more robust to unseen adversarial attacks and perturbations, thus they should also test gradient-based attacks against the model itself. Additionally, the authors should test against an attack that is generated ensembling the gradients of multiple models, which should improve transferability and also test effectively the robustness claim that the authors are making.\nThe model should also be tested with variations of the attacks, e.g., with PGD with logit loss, or against APGD with DLR loss. The authors should demonstrate that the defense generalizes against other unseen attacks and variations, to really claim robustness. The defense should be robust regardless of the method used to find the adversarial attacks. \nThe model should also be tested against query-based attacks to properly validate the fact that it is not suffering from gradient masking problems.\nFinally, all the parameters of the attacks should be specified to ensure they are conducted properly and they are not suffering from optimization issues.\n\n**No analysis on how expensive the method is, compared, e.g., to adversarial training**\n\nThe authors should discuss how expensive is to train a model with this technique, compared, e.g., with AT. Additionally, they should specify how costly is to add further heuristics.\nFor example, the authors use a heuristic to choose the models to use for creating the transfer attacks. However, this heuristic seems expensive, as it requires to compute the adversarial accuracy (in transfer) against all available models. Does this require to launch a full evaluation against all models?\n\n\n**Clarifications needed**\n\n- the authors state that they propose \"using all available system-level defenses\", however the presented method is clearly a ML-level defense. The authors should clarify this aspect, as it is confusing to read and there is no discussion in the rest of the paper on which system-level defenses are mounted on the model, or if they are mounted at all.\n\n- the authors state that the drawback of AT is that it degrades clean accuracy, however this does not seem to be the case https://robustbench.github.io. The authors should clarify this aspect by detailing this statement.\n\n- The method assumes that the defended is aware of the same set of public models as the attacker. However, with the proliferation of public models (also supported in the introduction of the paper), this might not be realistic to assume that the defender is aware of all possible models available. Furthermore, assuming that the attacker has $s \\dot a$ attack strategies might bring an excessive number of combinations. The authors should clarify these statements and propose solutions for these limitations. In fact, when the authors claim that the proposed approach \"achieves over 90% accuracy against all four transfer attacks\", they should also disclose that this holds only when the set of target models is the same as the source models $T = S$."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7997/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7997/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7997/Reviewer_nPyj"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7997/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697961250825,
        "cdate": 1697961250825,
        "tmdate": 1699636985195,
        "mdate": 1699636985195,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "50fycG5ovF",
        "forum": "Tvwf4Vsi5F",
        "replyto": "Tvwf4Vsi5F",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7997/Reviewer_DtJG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7997/Reviewer_DtJG"
        ],
        "content": {
            "summary": {
                "value": "The authors approach transfer attacks from the practical perspective and propose training procedure that allows to achieve robustness with a small drop in clean accuracy. They empirically show that their rather simple approach to selecting models allows good performance with respect to SOTA white-box defences. They do this by performing extensive evaluations on a wide range of public models"
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "-\tThe paper considers a real-life scenario of defending specifically against black-box attacks.\n-\tEvaluation is very extensive (considering different 264 combinations of source models and attack mechanisms)\n-\tSolid results with potentially high relevance for real-life industrial applications"
            },
            "weaknesses": {
                "value": "The authors state the weaknesses and limitations of their approach quite fully in Section 7.3. I could add that although proposed heuristics and empirical results could be significant in practical applications, the methodological contribution of this work is rather incremental."
            },
            "questions": {
                "value": "The authors state that PubDef achieves much better results than SOTA models from RobustBench. Where in the main paper can we find the clean and robust accuracy of these SOTA models to compare? I found the presentation of the results a bit confusing."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7997/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7997/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7997/Reviewer_DtJG"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7997/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698786694975,
        "cdate": 1698786694975,
        "tmdate": 1699636985051,
        "mdate": 1699636985051,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TNbI4EXSj4",
        "forum": "Tvwf4Vsi5F",
        "replyto": "Tvwf4Vsi5F",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7997/Reviewer_nHrU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7997/Reviewer_nHrU"
        ],
        "content": {
            "summary": {
                "value": "The paper argues that the common modality for attack on ML models (image classifiers are used in experiments) is going to be via transfer attacks on public models since companies are likely to keep their model weights protected. The paper then recommends defenders to adversarially train against a reasonable subset of  public models and find that works against L-inf norm attacks that are transferred from public models. They call their technique PUBDEF.\n\nThe authors present the work in a game-theoretic perspective where the attacker only has access to public models.\n\nThe authors acknowledge some of the limitations of the work. In my judgement, they are quite significant. They mention that if an attacker somehow is able to infer model weights (say by training a surrogate model), then they can bypass the defense. For typical classifiers, I think this is a significant concern. One limitation that they do not appear to consider is the possibility of black-box attacks directly on the model (no transfer attack needed) and do not evaluate their defense against a black-box attacker. Thus, the setting is somewhat limited in which the attacker can only do transfer attacks on the protected model and nothing else. \n\n\nOverall, the motivation for the paper, the defense approach used, and evaluation all need to be better."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The threat model of transfer attacks is well known. The authors assume that this is the main likely modality in practice. That assumption is likely a reasonable assumption only in very complex and large models. In most other cases, including datasets and models that the authors consider, there are other practical attack strategies, including blackbox attacks and model stealing attacks. To authors' credit, they acknowledge some of these limitations (especially model stealing attack via training a surrogate), but that doesn't make the assumption more realistic.\n\nGiven their assumption, their experimental results appear reasonable. The main contribution is that the defender can choose a small subset of public models against which to adversarially train their model. They require that public models be trained on the same task.  In practice, they find that such a model is often robust against a broad range of adversarial attacks that are restricted to using the public models."
            },
            "weaknesses": {
                "value": "Weaknesses are unfortunately significant. \n\n-- The assumption underlying the paper is likely unrealistic in that most blackbox models will also permit querying. Thus, transfer attacks are not the only option for an adversary. Blackbox attacks are also a possibility and, in fact, may be the primary attack strategy. The proposed defenses were not evaluated against Square attack and other blackbox attack strategies.\n\n\n[Rebuttal]\n\nI considered the rebuttal provided so far.  The main concern its that the approach is susceptible to blackbox attacks (model stealing attacks are a potential issue as well, but I think the primary attack vector is likely to be blackbox attacks). An attacker will choose the best attack strategy available among many options, including transfer attacks, blackbox attacks, and model stealing attacks. The authors present preliminary results that showed that blackbox attacks would succeed and they would thus either need stateful defenses to work (which were unfortunately recently broken by Feng et al. 2023) or they need to combine their scheme with a noise-based blackbox defense where inputs are combined with noise before doing an inference. The latter combination showed some promise, exceeding the performance of a simple white box defense, but significantly lowering the reported natural accuracy and adversarial accuracy of PubDef by about 10% each in Table 1. \n\nI am raising my score, assuming that the authors are willing to include a deeper analysis of their work against blackbox attacks on CIFAR-10 and CIFAR-100 in the final paper and include a clear acknowledgment something along the following lines in bullet 3:\n\nBlackbox attacks are a potential concern against PubDef. When we started this work, a stateful defense was available, which would have thwarted blackbox attacks and could be combined with our scheme  with no or little change to reported results. Unfortunately, very recently,  Feng et al. broke current stateful defenses, but did not rule out potential new stateful defenses. If new stateful were to be developed, they should be combined with your scheme. In the absence of stateful defenses being available, we present preliminary findings  against a blackbox attacker (see Appendix, section X). They suggest that PubDef, unmodified, would be vulnerable to blackbox attacks and not provide an adequate defense. However, PubDef in combination with a noise-based defense shows some promise (see Appendix, Section X). For example, on the CIFAR-10 dataset, with a sigma of 0.02, PubDef  has an adversarial accuracy of 79.8 against the best attacker, with a natural accuracy of 88.9. This is a drop of adversarial accuracy  and natural accuracy of about 10% each  from the  results in Table 1 that only factored in transfer attacks, but still superior to simply using best white-box adversarial training in both clean and natural accuracy. [And then report additional results on other datasets and include them in the paper.]"
            },
            "questions": {
                "value": "I recommend authors to consider a different setting for their work where transfer attacks are more common due to the complexity of stealing models or doing direct blackbox attacks. The setting would be attacks on generative AI models such as ChatGPT. Recent work shows that transfer attacks are possible on such models (see a 2023 paper by Zico Kolter, Matt Fredrikson and others that  was also mentioned in a recent NYTimes article) where the attacker is able to append a suffix to a prompt to cause the model to output something that can be considered harmful in some way. Unfortunately, though, adversarial training is not the defense approach that is used in such settings (at least until now). So, the approach presented by the authors in this paper is unlikely to work and may require significant changes. But, the setting is likely more realistic for the assumptions they rely on."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7997/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7997/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7997/Reviewer_nHrU"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7997/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699225971683,
        "cdate": 1699225971683,
        "tmdate": 1700755402500,
        "mdate": 1700755402500,
        "license": "CC BY 4.0",
        "version": 2
    }
]