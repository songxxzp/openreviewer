[
    {
        "id": "bN6E0raLce",
        "forum": "IRcv4yFX6z",
        "replyto": "IRcv4yFX6z",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6646/Reviewer_GfNL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6646/Reviewer_GfNL"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces CAST (Concurrently Adaptive Segmentation Tokens).\nThis builds on existing work where superpixels are used as tokens in a\nvision transformer. The new contribution is that graph pooling is used to\nadaptively cluster segments to create a hierarchy, allowing segmentation (and\nclassification) at a coarse and fine-grained level. \n\nSEEDS is used to define superpixels via over-segmentation. A CNN produces\ninitial features on the original pixel regular grid. The mean of CNN features\nunder a superpixel define the feature-token for that superpixel. Positional\nencodings are created in a similar way.\n\nSuperpixels are used as leafs of a tree, which is adaptively defined by graph\npooling. This tree has L layers, and starting from the leafs, the next layer is\ndefined by first assuming that there are C coarser segments at the next level,\nand then for each token the network predicts a probability for which coarser\nsegment each finer segment should be assigned to. The argmax of the probability\nis the assignment. The number of coarser segments is defined adaptively via the\nFarthest Point Sampling algorithm.\n\n\nResults are reported on:\n\n* PartImageNet, where CAST is compared to a baseline ViT and SAM-B, where is shows a significant performance improvement over the baseline in most cases and also with lower computational costs.\n\n* DensePose Human part segmentation, and is comparable to the HSG baseline.\n\n* Pascal VOC Segmentation - where CAST and ViT are trained on COCO. Ablation studies are\n  done here. CAST outperforms the baseline in all reported cases.\n\n* Pascal VOC classification - where CAST is compared to ViT and Swin using a linear probing evaluation."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The algorithm design is well motivated.  \n\nThe technique is tested against strong baselines, and shows benefits in both quality and computational metrics. \n\nDesign choices are ablated.\n\nAdditional details and failure cases are given in the appendix."
            },
            "weaknesses": {
                "value": "The tone is a bit to strong and unscientific at times. Value judgements are\nplaced on things, instead of performing comparisons and reporting results. I\nrecommend rewording in certain areas:\n\n> We assert that our design is the proper approach for true vision transformer, distinct from the text-inspired ViT.\n\nThere is no \"proper\" approach. You can assert it might be a more natural choice in some circumstances, but in the instance where superpixel segmentation fails, then this is very much not the correct approach.\n\n> CAST not only discovers the hierarchy but also enhances flat semantic segmentation, indicating its superiority in learning dense representations compared to ViT.\n\nSuperiority is too strong. It might be ok, but given the overstated tone in the rest of the paper, I think: \"indicating that it learns richer dense representations\" might be a better wording. \n\n> Sec. 4.4. We conducted an ablation study on our design choices, confirming that CAST is better\nthan previous token pooling approaches for ViT (Marin et al., 2021; Sarfraz et al., 2019).\n\nIt's not confirmed. You made a measurement that contributed evidence towards\nthe idea. Confirming is something that someone who replicates your work will\ndo. I recommend changing the word \"confirmed\" to measured in most places."
            },
            "questions": {
                "value": "> The resulting input segment features are defined as Z0 = [Xclass; Xs] + Epos\n\nI've seen people simply adding positional encodings to the token features directly rather than concatenating them as additional feature channels. I must have missed the work that introduced / ablated this decision. Is there a reference you can point me to on this?\n\n\n> We initiate the next-level coarse regions by sampling centroids from segment tokens Zl\u22121 at level l \u2212 1 and compute Pl based on token similarity, with C representing the sampled centroid indices.\n\nAre these centroids in position space or feature space?\n\n\n> All models are self-supervisedly trained on IN-100 and evaluated using linear probing.\n\nIs there a reference for this? I'm not familiar with this and it seems like a traditional imagenet-style classification benchmark would be a better comparison where there was a defined groundtruth."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6646/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6646/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6646/Reviewer_GfNL"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6646/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698609618880,
        "cdate": 1698609618880,
        "tmdate": 1700693962296,
        "mdate": 1700693962296,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7F5oEIxTmY",
        "forum": "IRcv4yFX6z",
        "replyto": "IRcv4yFX6z",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6646/Reviewer_hsyp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6646/Reviewer_hsyp"
        ],
        "content": {
            "summary": {
                "value": "This work addresses image classification and hierarchical image segmentation. No pixel-wise ground truth is provided in training for segmentation, so the proposed ViT-based approach, CAST, is trained by self-supervision and supervised image classification. CAST replaces the fix-grid patches with superpixels, and uses a graph pooling to aggregate features to sampled cluster centroids. The resulting graph-pooled superpixel tokens are aimed at better incorporating the local-global hierarchical semantics, and can produce image segmentation. CAST is evaluated on multiple datasets, and outperforms SOTA."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper is well-written and easy to follow. The experiments are comprehensive, and the reported results are promising.\n\nThe motivation for using the superpixel token to replace the fixed-grid patch token in ViT is clear and interesting. As the initial superpixels are estimated based on low-level visual cues, the graph-pooled superpixel tokens incorporate appearance and shape details, as well as local-global hierarchical relationships. This seems to help give sharper segmentation masks than in related work."
            },
            "weaknesses": {
                "value": "Overall novelty seems limited. The proposed graph-pooling for aggregating fine-level tokens into coarse-level tokens is similar to the Multi-stage Token Aggregation in TCFormer (Zeng et al 2022) and the Token Merging in ToMe (Bolya et al 2023). A comparison of CAST with these two methods is missing.\n\nThe method increases complexity relative to using the fixed-grid of patches, since it requires estimation of superpixels and sampling the clusters. Performance seems to be critically dependent on the quality of superpixels and the cluster sampling methods. The paper poorly tests performance wrt varying quality of superpixels and cluster sampling. It is unsatisfying that these two components are not learnable, and that they are not end-to-end integrated with the rest of CAST."
            },
            "questions": {
                "value": "What is the \"MLP ratio\" in section 4.4 and Table 6.c? Above equation 3, it shows f(a,b) = a + MLP(b), but there is no coefficient there."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6646/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698699216787,
        "cdate": 1698699216787,
        "tmdate": 1699636759521,
        "mdate": 1699636759521,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Hp7FJjO4AL",
        "forum": "IRcv4yFX6z",
        "replyto": "IRcv4yFX6z",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6646/Reviewer_RnRE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6646/Reviewer_RnRE"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a novel method that adopts superpixel as the token for vision transformer. The superpixel tokens are hierarchically grouped together in each layer via graph pooling. This gives rise to a hierarchical segmentation result  using only image classification label. Experimental results demonstrate the usefulness of the proposed method."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- Using superpixel tokens is more appropriate for vision transformers. The proposed method delivers a hierarchical segmentation result using only image-level annotation.  I really appreciate the concept of using hierarchical segmentations as the tokens in different layers. In particular, the proposed method starts with a finest segmentation given by superpixels, and learns automatically the hierarchical segmentation. In my opinion, using different regions in hierarchical segmentation is a more natural and powerful way of image tokenization. \n- The experimental results are quite convincing. The generated hierarchical segmentation is visually impressive. As depicted in Table 1 and 3, the proposed method significantly outperforms some baseline methods (e.g., the powerful segment anything model), while being also efficient."
            },
            "weaknesses": {
                "value": "- The runtime analysis is missing. Compared with the vanilla ViT, the proposed method involves superpixel generation and graph pooling. Does this require much more extra runtime?\n- Ablation study of using different superpixel methods. Is this proposed method sensitive to the finest segmentation ?"
            },
            "questions": {
                "value": "Does this require much more extra runtime compared with classical ViT.\nIs this proposed method sensitive to the finest segmentation ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "10: strong accept, should be highlighted at the conference"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6646/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698911862537,
        "cdate": 1698911862537,
        "tmdate": 1699636759396,
        "mdate": 1699636759396,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GtHJBOtrXQ",
        "forum": "IRcv4yFX6z",
        "replyto": "IRcv4yFX6z",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6646/Reviewer_5FXt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6646/Reviewer_5FXt"
        ],
        "content": {
            "summary": {
                "value": "This paper presents CAST - a variant of ViTs that does hierarchical segmentation of inputs and tokens as part of its pipeline. The method is simple - input pixels are partitioned into super-pixels (in contrast to simple patches for ViTs) a convolutional network is applied over the input image and the resulting features are pooled across the superpixels (average pooling) to create the initial set of tokens. \nFrom here the method works by passing the set of tokens through ViT blocks and between them graph pooling layers which group tokens into a coarser set of tokens by sampling centroids and measuring similarity between tokens and centroids. \n\nThe method is trained with a self supervised objective (MoCo, read out from the last layer) and demonstrated to work well in a variety of contexts."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I think this is an interesting paper with good motivation and a simple and effective implementation of core ideas.\n\n* While simple, the use of super pixels and feature pooling to replace the awkward patch based embeddings of ViTs is a nice and original idea.\n* The method is quite elegant and naturally lends itself to several use cases which are demonstrated in the paper in a convincing manner (mostly)\n* Paper is well presented and well executed and was quite easy to follow.\n* A highly applicable method which should be easy to use in several contexts so significant to the community."
            },
            "weaknesses": {
                "value": "I have some (relatively minor) concerns about parts of this work:\n\n* The major thing which I find missing is more in-depth discussion of the role of super-pixels here - I feel the ablation table is missing one critical row which is a ViT with a super-pixel (+conv net) embedding layer instead of patch extraction. This is also a possible explanation to the results in Table 5. There is a chance that super-pixels are the source of most of the improvements in the paper (quantitatively, at least, clearly there would be no clear hierarchical structure in this case) and this is not discussed enough.\n\n* The positional embedding pooling is a bit odd - super pixels can be with quite arbitrary shapes and there is a chance the resulting average PE over the super pixels would bare little connection to the actual \"position\" of the super pixel. Can the authors comment on that?\n\n* While the authors claim there's a top-down element here (and arguably there is through training) I would argue it is not truly top-down at inference. The super pixels and segmentations do not depend and will not change based on what the top layers infer - the network has top-down pathway to inform them (other than gradients in training). This is of course fine, but it should be discussed.\n\nI hope to see a discussion about the points above and I would be happy to raise my score if these are discussed and addressed to."
            },
            "questions": {
                "value": "More minor questions:\n\n* How do segmentations are extracted from ViTs in the paper? it's not clear to me how the results in, say, Figure 4, were generated for ViT-S.\n\n* Is the PE added to the CLS token as well?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6646/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6646/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6646/Reviewer_5FXt"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6646/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699362826551,
        "cdate": 1699362826551,
        "tmdate": 1700650235431,
        "mdate": 1700650235431,
        "license": "CC BY 4.0",
        "version": 2
    }
]