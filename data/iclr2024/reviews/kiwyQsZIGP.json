[
    {
        "id": "nuEFT8PLKH",
        "forum": "kiwyQsZIGP",
        "replyto": "kiwyQsZIGP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3772/Reviewer_Unr2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3772/Reviewer_Unr2"
        ],
        "content": {
            "summary": {
                "value": "The paper aims to investigate 3 main research questions about the effect of evaluation approach on 1) predicting task-level performance, 2) validity of the ranking of various FSL methods and 3) model selection and FSL performance. Among other things, they concluded that performance is substantially different than the performance estimated by the validation procedures used in the paper. Among them, 5-fold CV is better than the others. For model selection LOO-CV is the best approach."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is about a topic that hasn\u2019t gotten much attention in the past."
            },
            "weaknesses": {
                "value": "Different methods use different backbone networks. Backbone network has a confounding effect that makes comparison between methods difficult.\nOracle estimator is not properly described.\nMy main concern is about the significance of the results and their usefulness and impact in real-world. For example, I am not sure to what extent it can be justified to do LOO-CV for model selection and then 5-old CV for performance estimation."
            },
            "questions": {
                "value": "See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3772/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698771226389,
        "cdate": 1698771226389,
        "tmdate": 1699636333639,
        "mdate": 1699636333639,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rfW37tWg4Q",
        "forum": "kiwyQsZIGP",
        "replyto": "kiwyQsZIGP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3772/Reviewer_CrkK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3772/Reviewer_CrkK"
        ],
        "content": {
            "summary": {
                "value": "This paper tackles the problem of validating the performance of few-shot learning (FSL) algorithms, on a given task (or episode), given only the small number of example in that task's train set (support set). This is a different endeavour than the aggregated performance usually reported to compare few-shot learning algorithms in general: it is usually estimated over many episodes, on much larger test sets (query sets) to reduce variance.\n\nExperiments compare different estimator of the generalization accuracy that only depend on the support set (hold-out, 5-fold and leave-one-out cross-validation, bootstrapping), against the accuracy as measured on the larger query set. They observe a large difference, which show none of these methods are reliable enough to be used as estimates of the generalization performance on a given task. Moreover, there is not much correlation between _rankings_ of models (or hyper-parameter values) on validation and test. However, these validation methods are usually _underestimating_ the test accuracy, so they may be useful as a lower bound (as the train error is usually an upper bound)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Originality\n--------------\nThe question of a validation procedure is often neglected in few-shot learning, and proponents of a new algorithm often only focus on the aggregate test performance. This is an issue when trying to apply these algorithms to new, specific small datasets, and trying to determine the best learner (and hyperparameters) for them.\n\nEstimators of generalization errors considered are not new (this was much more popular in machine learning when all datasets were much smaller than today), but systematically investigating them in the context of FSL is new, as are the observations on leave-one-out in that context.\n\nQuality\n----------\nThe research questions are clear, and explore well different aspects of the main problem (task-level validation for FSL), which is well motivated.\nThe investigation is well done, experiments align with the research questions.\nThe span of experiments, across datasets, models, and estimators make sense, and the results support the conclusions.\n\nClarity\n---------\nThe paper is quite clear and reads well. Often questions or remarks coming to my mind when reading a sentence were satisfactorily addressed in the next one or a paragraph later.\nVisualization and presentation of results are mostly clear.\n\nSignificance\n-----------------\nAlthough some of the conclusions may read like negative results, which are usually harder to \"sell\" as significant, I think this investigation is reveals really important points for any application of FSL to real world small-scale datasets. The issues exposed would affect anyone needing to validate the performance of a few-shot learner without access to a large labeled query set.\nEpisode-level hyper-parameter selection is also a pretty open problem, and it's good to have it explored."
            },
            "weaknesses": {
                "value": "1. One thing that could be explored further is the reliance of some learners on the assumption of a balanced support set. MAML seems to be particularly sensitive, as shown in Fig. 3 for CV LOO. This makes me wonder if maybe the \"Oracle\" accuracy would only be valid or accurate for balanced, N-ways k-shot support sets, and CV LOO or bootstrapping estimators might actually be closer to the performance of these few-shot learners on unbalanced support sets.\n2. If class balance is an assumption that can be made (based on the composition of a given task), then maybe estimators could be adapted to have splits respect that constraint. Cross-validation could have leave-one-shot out (the support set would be balanced, k-1 shot and the valid set would have one example of each class), sampling for bootstrapping could also be aware of classes. If it works, this might be a practical contribution.\n3. The class balance assumption, used in all datasets considered (as far as I can tell), may be too restrictive when developing procedures for real-world datasets."
            },
            "questions": {
                "value": "1. Could you also report training accuracy, for the different estimators (incl. oracle)? Or is it almost always 100% and there's no signal there?\n2. Is it the case that all the estimators actually underestimate the test accuracy (up to statistical noise)?\n3. If we can assume that tasks of interest have balanced training sets, can class-aware splits provide a better estimation? (see point 2 in \"Weaknesses\" above?\n4. If class balance cannot be assumed in general (as I'd think would be the case in real world applications), maybe it would be worth doing experiments on episodes with un-balanced support sets, either from other benchmarks, or altering the composition of episodes from usually-balanced ones."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3772/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699049043346,
        "cdate": 1699049043346,
        "tmdate": 1699636333557,
        "mdate": 1699636333557,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DwEJfSbY9s",
        "forum": "kiwyQsZIGP",
        "replyto": "kiwyQsZIGP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3772/Reviewer_ZAXf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3772/Reviewer_ZAXf"
        ],
        "content": {
            "summary": {
                "value": "This paper explores the reliability of evaluation methods for few-shot learning (FSL) models. The authors investigate how well current benchmarks and validation strategies predict the performance of FSL models on individual tasks. They find that cross-validation with a low number of folds is best for direct performance estimation, while bootstrapping or cross-validation with a large number of folds is more suitable for model selection purposes. However, they conclude that with current methods, it is not possible to get a reliable picture of how effectively methods perform on individual tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "+ The paper addresses an underexplored and realistic aspect of few-shot learning\u2014task-level validation. \n+ This paper is easy to read. \n+ It provides an extensive experimental analysis across different datasets and evaluators, offering a comprehensive view of the current state of FSL benchmarks.\n+ The paper identifies the best performing evaluation method, which is useful for future research and practical applications."
            },
            "weaknesses": {
                "value": "Firstly, it is hard for me to estimate the novelty and contributions of this paper. The paper is more like an analysis paper but the conclusions are not clear. From the end of the introduction, all three conclusions are weak. Moreover,\n- I don't fully understand the part related to the first question: all three evaluations (hold-out, cross-validation, and Bootstrapping) are widely used for few-shot evaluations. The only difference is using the support set only. But if we treat the support set as the whole set, there are no obvious differences. \n- Secondly, I cannot agree with the hypnosis in Figure 1, since the oracle set is larger than the estimate set and, there are a lot of variances for each estimate set, it is not surprising that the performance on the oracle set is more stable. \n\nA few other minor weaknesses include: \n- Even the best evaluation strategy identified is not entirely reliable, indicating that current evaluation benchmarks may be inadequate for individual task performance prediction.  \n- Regarding few-shot learning, besides the standard few-shot evaluation benchmarks, more few-shot evaluations are flexible -- in a lot of papers they also show the performance under a few-shot setting with different synthetic or real-world benchmarks, even for open-vocabulary images/videos. They are also not limited to the standard query-support evaluation. How can the conclusion drawn from this paper inspire those types of work?\n\n---\nThanks to the authors for providing the response. However, I don't think the response addressed my concerns.\n\nFor Q1, yes I agree the Aggregated Evaluation is what people mostly use for the ``standard'' few-shot setting but the other settings are also used for works of literature regarding few-shot learning. As the positioning of this paper is also NOT studying the standard few-shot problem, I don't think ignoring the evaluation methods used by other works of literature is fine. \n\nFor Q2 and Q3, the query set is used for evaluation but only the support set is used as labels seeing by the model under the few-shot setting. Therefore, I don't think a large query set is a big problem -- in real practice, the few-shot model should be able to learn from a few samples but can work with a large number of test samples. Moreover, if merging the query set with the support set, how should we evaluate the model? For the standard academic benchmarks, there is no real concept of a few-shot problem as we can always label more data for the few-shot categories. Providing enough labels and splitting them into query and support sets should be more considered for evaluation but not training.\n\nBased on this understanding, I keep my original rating."
            },
            "questions": {
                "value": "A few minor questions: \n- (Section 3, AE)\"We observe that in a true FSL setting, this is an unrealistic assumption.\" --> Why this is not a realistic setting? For few-shot in practice, we want to tune a model with a few samples and that model can handle a lot of samples, which means the query set is much larger. \n- (Section 3, TLE) \"Moreover, in realistic situations there is no labeled query set for evaluating a model, so both the model fitting and model evaluation must be done with the support set.\" --> Again, query and support set split is some data processing. If we treat the support set as the whole labeled set, the held-out strategy is essentially the same as the query-support split, right?\n- (Section 4.1) What are the evaluators? They should be clearly defined. Moreover, there are a lot of similar terminologies used in this paper, e.g. estimators, and oracle accuracy, which sound not general enough for an audience out of FSL."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3772/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3772/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3772/Reviewer_ZAXf"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3772/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699413597576,
        "cdate": 1699413597576,
        "tmdate": 1700778232026,
        "mdate": 1700778232026,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uabDwfScix",
        "forum": "kiwyQsZIGP",
        "replyto": "kiwyQsZIGP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3772/Reviewer_KXUd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3772/Reviewer_KXUd"
        ],
        "content": {
            "summary": {
                "value": "This paper aims at rethinking how we could better evaluate the performance of few-shot learning methods and how we could select models in few-shot settings. The authors measure the estimated accuracy from different estimators to show that they all have non-negligible gaps from the oracle estimator. Also, the authors investigate different ranking strategies for model selection in few-shot learning. Finally, the authors provide insights that existing evaluation approaches are all not competent enough. Future works may need to design specialized evaluation procedures for evaluating few-shot learning performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "++ This paper provides some interesting analysis and viewpoint on the evaluation, model selection for few-shot learning tasks. It could bring researchers to take a step back and reconsider the essential parts in few-shot learning, like what is a more proper way to evaluate models, and how to select models that can be better used in real-world cases.\n\n++ The experimental results are comprehensive, involving various meta-learning datasets and meta-learning algorithms, which enhances the soundness of the conclusions from this paper."
            },
            "weaknesses": {
                "value": "-- I am uncertain about how useful in real application would the conclusion from this paper be. After all the experimental investigation and analysis, the suggestion given by the authors is that every few-shot setting should design specialized evaluation procedure. This would be laborious and complicated for future works which makes this suggestion infeasible. Also, there is no example given by the authors about how to design the evaluation procedure based on certain specific tasks.\n\n-- I assume \"CV\" is short for \"cross-validation\" and \"CV LOO\" means \"cross-validation leave-one-out\", right? However, there is no explanation in the paper about what they mean, which causes confusion."
            },
            "questions": {
                "value": "-- From the results in Tab. 1, it seems that all estimators generally have under-estimated accuracy compared with the oracle method, so all of them should be pessimistic. However, in the conclusion section, it seems that only some of the estimators like LOO-CV and Bootstrapping are pessimistic estimators. I am wondering whether there exist an inconsistency or I have misunderstood the results in Tab. 1 and the conclusion on \"Performance Estimation\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3772/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3772/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3772/Reviewer_KXUd"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3772/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699421408618,
        "cdate": 1699421408618,
        "tmdate": 1699636333416,
        "mdate": 1699636333416,
        "license": "CC BY 4.0",
        "version": 2
    }
]