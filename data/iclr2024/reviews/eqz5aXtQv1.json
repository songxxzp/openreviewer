[
    {
        "id": "PbE7wrf9ez",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7768/Reviewer_NbSm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7768/Reviewer_NbSm"
        ],
        "forum": "eqz5aXtQv1",
        "replyto": "eqz5aXtQv1",
        "content": {
            "summary": {
                "value": "The paper proposes a new synthetic dataset: the Spatial and Temporal Understanding of Prepositions Dataset (STUPD). It is a large-scale video dataset for understanding spatial and temporal relationships. The dataset contains 150K visual depictions consisting of 30 static and dynamic spatial prepositions and 50K visual depictions across 10 temporal relations.\nThe synthetic dataset helps models perform better in visual relationship detection in real-world settings, verified on 2 real-world datasets: ImageNet-VidVRD and Spatial Senses."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed dataset STUPD makes contributions to relation reasoning: \n- It covers diverse spatial and temporal relations: 30 spatial prepositions and 10 temporal prepositions\n- It elaborates on the spatial relations that intrinsically involve motion\n- It is a large-scale dataset\n\nExperiments show that pretraining on STUPD increases performance on real-world visual reasoning tasks."
            },
            "weaknesses": {
                "value": "1. Details missing about the evaluation of  STUPD pre-training. From the suppl Sec.A.8.2, the SpatialSense/ImageNet-VidVRD experiment only covers 6/10 spatial relations. \n    - I have not found details about which relations have been conducted experiments on. ImageNet-VidVRD defines 132 predicates. Some of them are \"static\"  but not \"dynamic\" spatial relations. Also, the spatial relations are connected with verbs e.g., swim_behind, and fly_behind. It is not clear howto handle the different definitions of spatial relations during pre-training.\n    - There are some uncovered spatial prepositions among the 30 defined spatial relations. It seems that they have been collected but not evaluated. Since STUPD is a synthetic dataset that suffers from huge gaps with real-world images. It is important to verify its effectiveness on real-world tasks.\n\n2. I wonder about the necessity of prosing a new task of temporal relations. Their definition is detailed in Fig.2. Instead of a specific model to reason these temporal relations, it seems that we can directly apply temporal segmentation of each event and then compare the two segmented time stamps. For example, to judge \u201dA before B\u201d, we first get the temporal segmentation A: $[t_1, t_2]$, B:$[t_3,t_4]$; if $t_2 \\textless t_3$, then we say \u201cA before B\u201d."
            },
            "questions": {
                "value": "See \"Weakness\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7768/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7768/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7768/Reviewer_NbSm"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7768/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697351653395,
        "cdate": 1697351653395,
        "tmdate": 1699636949002,
        "mdate": 1699636949002,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Ara4VYi0oq",
        "forum": "eqz5aXtQv1",
        "replyto": "eqz5aXtQv1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7768/Reviewer_RbeZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7768/Reviewer_RbeZ"
        ],
        "content": {
            "summary": {
                "value": "Identifying temporal relationships, compared to the spatial relationship counterpart, has less attention in the field of computer vision so far. Thus, the authors are focused on generating a dataset that focuses on both spatial and temporal reasoning. Though the motivation sounds great to the researchers in this community, it is hard to understand why the temporal relationship should designed in the way the author presents. Also, it is hard to find how the temporal dataset could be used for other real-world tasks."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The main motivation for the dataset creation, which focuses on both spatial and temporal reasoning, is clear, and the dataset comparison table (Table 1) helps the readers understand the landscape of the field. Also, applying the dataset to two spatial real-world tasks with multiple baseline models also well-represents the effectiveness of this dataset. Lastly, visualization of the dataset helps the reader understand what the dataset looks like."
            },
            "weaknesses": {
                "value": "Although this dataset (partially) focused on temporal relationships, it is hard to understand why such categorization (in Figure 2) is valid. Also, I failed to find any experiment employing the STUTD dataset to improve the performance on real-world *temporal* relationship tasks. Apart from the main content, this manuscript may violate Sections 2 and 4.1 of the ICLR 2024 author's guidelines."
            },
            "questions": {
                "value": "[Major]\n\nA. Lack of use of the STUPD dataset as a temporal reasoning pretraining set. Unlike what the authors mentioned at the end of Section 4.2, there is a lot of video-based work that focuses on temporal reasoning. For instance, many datasets [1,2,3,4] exist in the video question-answering domain. It would be better to employ some models that try to resolve the tasks suggested by such datasets.\n\nB. Reason for splitting the temporal relationship into ten categories. Based on what the author said, \"before\" is a subset of \"by,\" \"while\" is a subset of \"during,\" and \"since\" is a subset of \"at\" (from 1st para of Sec 3.4.2). Also, I don't think \"by\" (which implies a deadline) is used to describe the timing of two events in general. At least from my end, it is natural to say, \"Turn in the assignment by midnight\" instead of \"Cut off the corners of the bread by the time you apply jam on the bread,\" for example. The author also mentioned 'redundant representation' in Section 2.1; in this regard, I failed to find any reason for keeping potentially redundant classes. Isn't it more natural to compress the classes into 7 instead of 10? If the author firmly believes a 10-class setting is much more meaningful, then I think it would be better to have an experiment in A but present the result with 7-class pretraining and 10-class pretraining.\n\nC. Clarity. In the #1 callout in Sec 3.4.1, the author pointed out that 'track' and 'tunnel' have fewer relationships than other object types. Why did such a case happen? Is it because of the limitation of the Unity3D platform, or is it because all the relationships came from another dataset?\n\n\n\n[Minor]\n\nA. Formatting\n- \\citet and \\citep are different. Please carefully check the ICLR 2024 author's guidelines; It is improperly used in over 80% of the manuscript.\n- The author frequently used  \"<\" and \">\" without any escape character. Thereby, those symbols are repeatedly presented as flipped '!' and '?' characters throughout the text.\n\nB. Typo\n- 2nd line of 2nd para of Intro (...in space of time\" **pre**. Examples of...): I cannot understand what **pre** is for.\n- 50,000 uses the middle comma, but 5000 doesn't (always?) throughout the text.\n\n\n[References]\n\n[1] Jang et al., TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering, in CVPR 2017.\n\n[2] Mun et al., MarioQA: Answering Questions by Watching Gameplay Videos, in ICCV 2017.\n\n[3] Xiao et al., NExT-QA: Next Phase of Question-Answering to Explaining Temporal Actions, in CVPR 2021.\n\n[4] Li et al., From Representation to Reasoning: Towards both Evidence and Commonsense Reasoning for Video Question-Answering, in CVPR 2022."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7768/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698354105644,
        "cdate": 1698354105644,
        "tmdate": 1699636948868,
        "mdate": 1699636948868,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "O9xT61MYXL",
        "forum": "eqz5aXtQv1",
        "replyto": "eqz5aXtQv1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7768/Reviewer_ztaS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7768/Reviewer_ztaS"
        ],
        "content": {
            "summary": {
                "value": "This papers proposes STUPD dataset, which addresses the lack of diversity in prepositions in existing datasets and the absence of dynamic prepositions that involve motion. It consists of 150K images and videos capturing 30 spatial relations and 50K video sets depicting 10 temporal relations, all with 3D info and bounding box annotations. Authors claim that pre-training on STUPD can significantly improve performance on real-world visual reasoning tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Addresses a gap in existing datasets by including a wider variety of prepositions and introducing dynamic prepositions.\n2. Provides a synthetic dataset with both spatial and temporal relations, which is crucial for a more holistic understanding of visual reasoning.\n3. Demonstrates the real-world applicability of the dataset through pre-training improvements on visual reasoning tasks.\nWeaknesses:"
            },
            "weaknesses": {
                "value": "1. The synthetic data may not fully capture the complexity of real-world scenarios.\n2. The paper could benefit from a more extensive validation of the dataset's efficacy across a broader range of models and tasks."
            },
            "questions": {
                "value": "1. How well do models trained on STUPD perform when applied to real-world data, considering the dataset is synthetic?\n2. How does STUPD handle ambiguous or context-dependent prepositions where the spatial or temporal relationship might not be clear-cut?\n3. What measures are in place to ensure that the synthetic data in STUPD is diverse and representative of real-world scenarios?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7768/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698806838127,
        "cdate": 1698806838127,
        "tmdate": 1699636948718,
        "mdate": 1699636948718,
        "license": "CC BY 4.0",
        "version": 2
    }
]