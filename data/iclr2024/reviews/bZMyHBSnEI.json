[
    {
        "id": "decjJXeyD4",
        "forum": "bZMyHBSnEI",
        "replyto": "bZMyHBSnEI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6765/Reviewer_6XP7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6765/Reviewer_6XP7"
        ],
        "content": {
            "summary": {
                "value": "This paper focus on improving multimodal fusion by developing a dynamic fusion framework adaptively model the cross-modality interactions hierarchically. The authors propose DEQ fusion which recursively executing nonlinear projections on modality-wise features and the fused features until the equilibrium states are found. Experimental results on various benchmarks can support their findings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Multimodal fusion is one of the most fundamental problem in multimodal fusion, thus it is worth to design novel fusion mechanism.\n2. The proposed method is novel and well motivated to me. To the best of my knowledge, most existing multmodal fusion methods tend to fuse information from multiple source in a static manner. The proposed method fuses features in a dynamic and recursive manner, which is new and interesting.\n3. The proposed method are evaluated on various multimodal benchmarks, including multi-omics analysis, image-text classification, audio-text sentiment analysis and visual question answering. I appreciate the extensive experimental results. Additionally, the authors claim that the proposed DEQ fusion is readily pluggable to existing multimodal frameworks, which is very promising."
            },
            "weaknesses": {
                "value": "1. Computational cost: Though recursively fusing multimodal information is quite novel and have a chance to get better performance, I wonder if the proposed method is more time-consuming that its counterparts? It seems such comparisons is lack in the main paper and supplementary. Given this point, I encourage the authors to share more explanations about this.\n2. Scalability: As the authors claim that DEQ fusion is readily pluggable to existing multimodal frameworks, I think it is deserving to further clarify how to combining DEQ fusion into existing multimodal fusion methods. For example, some pseudo code code will be very appreciated and make the proposed method easier to follow.\n3. Motivation: The authors claim that DEQ fusion is an unified framework that looks into three aspects simultaneously including stabilizing and aligning multimodal signals, integrating interactions across modalities from multi-level, dynamically perceiving information. In my view, many attention-based multmodal fusion methods may also achieve the aforementioned points. In section 3.3, the authors say that DEQ fusion is not depend on any unimodal feature extraction or preprocessing methods. However, there exists some attention-based fusion methods which may also independent on unimodal feature extraction methods. For example, a simple self-attention module or MMTM[1]. Could the authors give some further clarifications?\n\n[1] Joze, Hamid Reza Vaezi, et al. \"MMTM: Multimodal transfer module for CNN fusion.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020."
            },
            "questions": {
                "value": "Please refer to weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6765/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698638197661,
        "cdate": 1698638197661,
        "tmdate": 1699636779835,
        "mdate": 1699636779835,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Ap1vhe0Jcn",
        "forum": "bZMyHBSnEI",
        "replyto": "bZMyHBSnEI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6765/Reviewer_JNZo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6765/Reviewer_JNZo"
        ],
        "content": {
            "summary": {
                "value": "The article presents an approach involving dynamic multi-modal fusion, introducing a weight-tied architecture to amalgamate distinct modality features and derive the unified representation simultaneously. By seeking the equilibrium state, this method facilitates the acquisition of stabilized intra-modal representations and fosters interactions across different modalities."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ This study concentrates on the development of an innovative multi-modal fusion method, which endeavors to attain a state of equilibrium among features, markedly distinguishing itself from prior fusion processes.\n+ The explanation in this article clearly and meticulously depicts its fusion architecture."
            },
            "weaknesses": {
                "value": "- The phrase \"every level\" in the introduction implies a comprehensive integration of cross-modality interactions throughout the multi-modal fusion process. However, given the paper\u2019s focus on fusion of features, which is traditionally associated with late fusion, there seems to be a discrepancy. The paper apparently does not delve into early or middle fusion strategies. To reconcile this, one could interpret \u201cevery level\u201d as referring to different stages or aspects within the late fusion process itself, although this may require clarification from the authors for a precise understanding.\n-  The ablation studies conducted on the BRCA and CMU-MOSI datasets highlight the significance of the DEQ, a component not originally introduced by this work, overshadowing the impact of f_{\\theta} and f_{fuse}. This raises concerns regarding the efficacy of the designed architecture in fully capitalizing on the potential for interaction among modalities. It suggests a need for further investigation and possibly a reevaluation of the architecture to ensure optimal performance.\n-  The introduction categorizes three ways \u2018stabilizing and aligning..., integrating..., dynamically perceiving...\u2019 but provides limited insights, necessitating to explain the reason behind. It should be clarified that how each way contributes to better multi-modal learning. For instance, the rationale behind the need for a multi-modal model to eliminate redundancy is not clear. A thorough analysis is essential to elucidate why previous research has concentrated on these specific approaches, helping to strengthen the foundational knowledge and context for the study. \n-  Furthermore, it is recommended that the authors provide a more comprehensive explanation regarding the advantages of this design in stabilizing intra-modal representations. A thorough exploration of the key difference compared to previous research, particularly in terms of enhancing stability, would enhance the reader's understanding. \n-  While the proposed architecture aims to achieve stable intra-modal representations, its application is limited for it only targets to the fusion of features. This raises the question of whether the method could also encompass the learning of stable uni-modal encoders, fostering the acquisition of even more robust features. Exploring this avenue could potentially enhance the method\u2019s applicability and effectiveness."
            },
            "questions": {
                "value": "1. It is advisable to provide detailed explanations for the dimensions of each vector and matrix utilized in the study, like $z$ in Related work. \n2. Given that Multi-bench encompasses a diverse array of fusion strategies, it is crucial for the authors to specify which particular method was employed on MM-IMDB dataset. \n3. The vectors $\\alpha_i$ ought to be represented in bold to maintain consistency with standard mathematical notation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6765/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698723687342,
        "cdate": 1698723687342,
        "tmdate": 1699636779712,
        "mdate": 1699636779712,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fYhKikdVqW",
        "forum": "bZMyHBSnEI",
        "replyto": "bZMyHBSnEI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6765/Reviewer_qyjt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6765/Reviewer_qyjt"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a deep equilibrium (DEQ) method for multimodal fusion by seeking a fixed point of the dynamic multimodal fusion process and modeling feature correlations in an adaptive and recursive manner."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(1)\tThis method innovatively combines multimodal fusion with DEQ framework to iteratively achieve multi-level multimodal fusion while retaining single-modal information\n(2) \tThe experiments proves the effectiveness of the method, and the ablation experiment is relatively complete. The weight visualization in Figure 3 dynamically perceives modality importance for efficacious downstream multimodal learning, which is intuitive."
            },
            "weaknesses": {
                "value": "1. The method in this paper is compared with the weight-tied method, which shows that the method in this paper can converge. This is obvious because the method optimizes f\u03b8 by the formula z* = f\u03b8(z*,x), and does not impose such a constraint on the weight-tied method with a finite number of layers, and the weight-tied method certainly cannot converge.\n2. In the original DEQ paper, DEQ is proposed for memory efficiency, and the effect is similar to that of weight-tied, and it would be better if the article gave a comparative experiment with the weight-tied method. \n3. Some of the expressions in the paper are unscientific and abstract, such as the sentence on page 3:\u2019Our fusion design is flexible from the standpoint that f\u03b8i(\u00b7) can be altered arbitrarily to fit multiple modalities. It could be better expressed as \u2018Our fusion design is flexible from the standpoint that f\u03b8i(\u00b7) can be altered arbitrarily to fit multiple level features\u2019.\n4. The drawing is not intuitive."
            },
            "questions": {
                "value": "When the equilibrium state is reached, why an informative unified representation in a stable feature space for multimodal learning be obtained? What is the relationship between these two? The paper does not give proof."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6765/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698739246643,
        "cdate": 1698739246643,
        "tmdate": 1699636779596,
        "mdate": 1699636779596,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hWPqOAMLRf",
        "forum": "bZMyHBSnEI",
        "replyto": "bZMyHBSnEI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6765/Reviewer_fFE2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6765/Reviewer_fFE2"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed the Deep Equilibrium Multimodal Fusion (DEQ) algorithm for multimodal fusion. DEQ seeks a fixed point of the dynamic multimodal fusion process and models the feature correlations in an adaptive and recursive manner, which allows the DEQ algorithm to capture the complex dynamics of interactions between modalities. Extensive experiments demonstrated the effectiveness of DEQ."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(1) An interesting paper, the proposed DEQ method for multimodal fusion could be a new perspective in the field. By achieving equilibrium, the model could handle complex interactions between different types of data, potentially leading to better performance in tasks that require a comprehensive understanding of multimodal information. It is also a nice contribution to stability and robustness in the learning process for multimodal data.\n\n(2) The experimental results are promising."
            },
            "weaknesses": {
                "value": "(1) DEQ models can be complex and require significant computational resources for training and inference. The search for a fixed point can sometimes lead to difficulties in convergence, especially in dynamically changing environments/contexts. There may be challenges in generalising the fixed-point approach to different types of multimodal data or applications.\n\n(2) The paper may lack extensive evaluation against challenging applications, which is crucial to establish its real-world effectiveness. For example, I wonder how good the results of DEQ are in medical image fusion (e.g. CT, MRI, PET, etc.)."
            },
            "questions": {
                "value": "Some related work is missing. For example the below [1] [2].\n\n[1] Channel Exchanging Networks for Multimodal and Multitask Dense Image Prediction, TPAMI, 2022.\n[2] 'Equivariant Multi-Modality Image Fusion' (Zhao et al, 2023)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6765/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6765/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6765/Reviewer_fFE2"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6765/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699237721489,
        "cdate": 1699237721489,
        "tmdate": 1699636779492,
        "mdate": 1699636779492,
        "license": "CC BY 4.0",
        "version": 2
    }
]