[
    {
        "id": "w1MCqvyror",
        "forum": "m0x0rv6Iwm",
        "replyto": "m0x0rv6Iwm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6570/Reviewer_A4rR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6570/Reviewer_A4rR"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a generalisation of propensity scores from a binary setting (i.e. one training distribution and then testing after a single discrete distribution shift) to a continuous one where data drift occurs gradually and thus the propensity scores vary over time. The method is able to detect and react to drifts without being aware of them a priori. The authors offer in-depth theoretical insights as well as an empirical analysis on both supervised as well as reinforcement learning tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposed method can deal with continuous data drifts without knowing about them a priori by naturally extending propensity scores to continuous drift settings. Since this type of drift scenario is more likely to occur in practice than prior considered settings such as a single discrete shift, the method is likely useful in real-world problems. In the empirical evaluation, the method performs as good as or better than all considered baselines."
            },
            "weaknesses": {
                "value": "I admit I am no expert on dealing with data drift, however I am sure methods other than the proposed one and the baselines (which are all quite naive) exist. While it is good to see that on the given data the developed method outperforms the baselines, it seems to me like a rather low bar and not very surprising. I am worried that a comparison to other relevant (and more sophisticated) approaches to dealing with data drift are left unconsidered, for example I know that a huge area of work is addressing data drift issues with calibration techniques. I think in order to really show the merits of the method, the authors need to consider other state of the art methods of dealing with data drift."
            },
            "questions": {
                "value": "see weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6570/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6570/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6570/Reviewer_A4rR"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6570/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698747283472,
        "cdate": 1698747283472,
        "tmdate": 1699636745713,
        "mdate": 1699636745713,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LWvROSQZ99",
        "forum": "m0x0rv6Iwm",
        "replyto": "m0x0rv6Iwm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6570/Reviewer_evvV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6570/Reviewer_evvV"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new formulation of propensity scores to handle a time-evolving data distributions. Specifically, they propose to approximate the propensity scores given the seen data, the time the data was received, and the current time. Because the propensity score is conditioned on time, the authors propose this will better weight how related past data is to the current data distributions. The paper then explores the effectiveness of the estimated propensity scores through several classification tasks and in reinforcement learning baseline."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The method is simple and easily understandable to anyone familiar with variance reduction through importance sampling which are quite common throughout machine learning and reinforcement learning. This approach is seemingly novel as far as I can tell, which shows the need to explore these methods in the continual learning setting."
            },
            "weaknesses": {
                "value": "After discussing with the authors, I believe they have adequately addressed my concerns. I am updating my score to account for their changes (and the one addition I just asked for).\n\n\n------ before edits -----\n\nThere are several weaknesses in this paper\u2019s presentation and content. Some of these are less critical than others, but the overall value of the paper\u2019s ideas are undermined by a series of confusing notation choices and overly confusing writing practices. There are also several issues with the empirical section which need clarification or addressing in subsequent submissions.\n\n**Section 2**\n\n1. The first weakness comes from how this work is situated in the literature, specifically the effort to distance this work from related work in continual learning. Specifically, on page 3 in Remark 2. I would argue that your setting is exactly encapsulated by continual learning frameworks. For a framework https://arxiv.org/pdf/2012.13490.pdf is a good resource. Specifically, definition 1 is a good starting point. While these definitions take a reinforcement learning perspective, they have counterparts in supervised settings as well, and can be readily adjusted to the supervised setting.\n2. I\u2019m not sure I\u2019m understanding the construction of your dataset D, and subsequently p(t). To me, each datum is received sequentially and therefore t_i is unique for each entry. This means p(t) is just a uniform distribution for the number of timesteps (of course this is not entirely accurate as t is assumed to be from the reals and not integers, but in effect this is what I understand). Given how you use this distribution, I\u2019m assuming this is not the case. But it is not clear what t is here, how does t relate to how the data was generated? How is your dataset constructed? This confusion continues throughout the paper including all of section 2, and when defining objectives in section 3.\n3. The three lines before remark 4 are very confusing, and don\u2019t clarify how $g_\\theta(x,t)$ is estimated. Do you mean t\u2019 instead of s\u2019? This comes up again after remark 4 under equation 10. It is not clear how the dataset is being generated to estimate g_\\theta, and the subtle difference from equation 8&9 is completely unclear (yes different distributions of the data are used, but how). It is very possible Algorithm 2 will clarify this, but I think a clarification on what the time in the original dataset (see 1) will deal with a lot of the confusion here.\n\n**Empirical section:**\n\n4. You did not explain how relative accuracy is defined. I\u2019m left to assume from the first line of figure 1\u2019s caption that they are normalized by the performance of the everything benchmark, but how is not stated in the main paper. If this is normalized according to the accuracy of the everything baseline, I think these results might be misleading. The paper would benefit from a discussion on how well the baseline is actually performing to give more perspective on the other method\u2019s performance. It is also unclear how the models are being tested in these experiments. Are you testing the accuracy of the model on the data generated at t+1? How is this data gathered? Is this same data then added to the dataset. There are many details missing, likely related to the confusion mentioned above.understanding\n5. **Recent** and **Fine-tune** are not oracles given your problem definition. If you are testing on data generated from t+1, the oracle would be a model trained on data generated from this distribution (this model would represent the best we could hope for).\n6. Hyperparameters and other experimental details:\n   - How were the hyperparameters chosen for your method and for the competitors? This question applies to all experiments as I don\u2019t see it mentioned in the paper. \n   - How does the amount of computation for your method relate to the amount of computation for the baselines (for both training and inference). I notice that your method has a larger batch size than the baselines for both the reinforcement learning and supervised learning settings. Why was this chosen? \n   - How does your method relate in number of updates compared to the baselines? Because of the different training regimes (i.e. the inner loop w/ M for your method), it is unclear how this compares to the epochs.\n   - Are the number of parameters in your models comparable to the baseline parameters.\n   - In the reinforcement learning experiment what are the shaded regions signifying? Also, these should be made darker. Currently it looks like there is significant overlap in both the experiments meaning there is not a statistically significant difference between your method and SAC. \n\n\n# Edits/Suggestions\n- Line right after equation 4 \u201cTV(p_{T+dt}, p_T = \u201c\n- After section 2.2, the paper shifts towards integer-valued time. I think the previous sections could benefit from this treatment as well, focusing on the simpler case of integer time and leaving real-valued time to the appendix."
            },
            "questions": {
                "value": "1. Is the first assumption the same as saying it is lipschitz smooth/continuous?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6570/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6570/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6570/Reviewer_evvV"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6570/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698789787813,
        "cdate": 1698789787813,
        "tmdate": 1700680839028,
        "mdate": 1700680839028,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dR7GzyhVtL",
        "forum": "m0x0rv6Iwm",
        "replyto": "m0x0rv6Iwm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6570/Reviewer_bXrx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6570/Reviewer_bXrx"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to learn a time-vary propensity score to combat distribution shift in learning problems in which the distribution of data evolves over time. At a high-level, this method learns a weighting function $\\omega$ which places greater weight on historic data that evolved similarly in the past. Empirically, the time-varying propensity score increases classification accuracy in supervised learning problems and outperforms other propensity score baselines. Moreover, it improves the data efficiency in RL tasks."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is well-motivated and well-written. The problem, solution, and novelty, are all clearly discussed in the intro.\n1. The proposed method is sufficiently general to be applied to both supervised and reinforcement learning problems.\n1. The experimental setup is very clear and provides intuition or how different baselines should perform in different settings."
            },
            "weaknesses": {
                "value": "1. The baselines considered for the supervised learning experiments are reasonable, but the RL experiments don't consider these baselines. As written, it's unclear if the time-vary propensity score is any better than simpler methods. Also, ROBEL DClaw is missing the SAC + Context baseline. \n\n1. At the end of Section 2 \"Approach,\" it would aid the reader to include a brief summary of the line of thinking outlined in sections 2.1-2.3. I found Section 2 far easier to understand on my second read largely because I already had an idea of the arguments that followed.\n\n1. It would be useful to give a brief summary of the ablation findings (appendix C) in the main paper. For instance, mention that the proposed method performs just as well as Everything when there is no shift, \n\n1. The shaded regions for the RL curves should be darker; as of now, they are very difficult to see.\n\n**Typos:**\n1. Section 2.1: \"for some time large final time T\"\n2. Missing right parenthesis after Eq. 4: TV(p_{T+dt}, p_T)\n3. Section 2.2 last paragraph: \"is subtle when deals with data\"\n4. Just before Remark 5: \"We call it \u201cFinetune\u201d make the objective\"\n5. Section 3.1.3 last paragraph: \"We run this experiments\""
            },
            "questions": {
                "value": "1. Could the authors explain the CIFAR shifts described by Eq. 21? I'm hoping for a plain English description of the shift.\n\n1. Could the authors comment on the computation expense of each method? I imagine they're all similar in computation cost, since their losses have similar forms."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6570/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6570/Reviewer_bXrx",
                    "ICLR.cc/2024/Conference/Submission6570/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6570/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698806904284,
        "cdate": 1698806904284,
        "tmdate": 1700695034329,
        "mdate": 1700695034329,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0R5e8vJWEk",
        "forum": "m0x0rv6Iwm",
        "replyto": "m0x0rv6Iwm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6570/Reviewer_LkU5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6570/Reviewer_LkU5"
        ],
        "content": {
            "summary": {
                "value": "The authors tackle the problem of gradually evolving data which is an important problem in many real world settings. The paper introduces a time-varying propensity score that can detect gradual shifts in the distribution of the data. This helps with prioritizing samples from past data that evolved in a similar fashion. The proposed approach considers optimizing a beta-weighted version of the training loss. Previous research has mainly focused on handling single shifts between training and test data or limited forms of shift involving changes in features, labels, or the underlying relationship between them. \n\nOne key advantage of this method is its ability to automatically detect data shifts without prior knowledge of their occurrence.The proposed algorithm seems versatile in that it  can be applied in different problem domains, including supervised learning and reinforcement learning, and can be integrated with existing approaches. The authors conducted extensive experiments in continuous supervised learning and reinforcement learning to demonstrate the effectiveness of their method. The broader impact of this research is in enhancing the robustness of machine learning techniques in real-world scenarios that involve continuous distribution shifts."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Typically the propensity score considered doesn\u2019t account for the time varying aspect - the work offers a sound approach to do so. The method exploits the initial unconstrained distribution of data as a  \u201cgauge\u201d to choose $p_0(x)$ differently. With this, they build a model $p_t(x)$ as deviations from the marginal on $x$.\n\n*   The proposed method considers a learning theoretic perspective to minimize the risk on the immediate next task after T tasks until that point.  The work studies a simple situation where we model the time-varying propensity score using an exponential family. \n*    The experiments cover a broad empirical comparison in both continuous supervised and reinforcement learning including synthetic data sets with time-varying shifts, continuous supervised image classification tasks utilizing CIFAR-10 and CLEAR datasets, and a simulated robotic environment alongside MuJoCo.\n*    The empirical evaluation shows  1) Relative test accuracy achieved by the proposed  method is consistently better than other methods in continuous supervised learning benchmarks. 2) The performance in RL is particularly noteworthy wherein the proposed method can effectively model how data evolve over time with respect to the current policy without requiring to have access to the behavior policies that were used to collect the data. This is achieved by a simple introduction of a weighting scheme as proposed in existing loss."
            },
            "weaknesses": {
                "value": "Two major weaknesses/limitations of this work are: \n\n*    The assumption that data evolves gradually which might not be true in many real world situations and limits the scope of application of the work. Consider for e.g. the sudden shifts in the data distribution. Would the proposed approach be extendable to such settings? What do we need to cover this setting as well?\n*   Evaluating on test data seems very counterintuitive as typical settings would not have this privileged information except during training. Can you provide how this specific choice of evaluation can be overcome? I find it quite limiting to do so."
            },
            "questions": {
                "value": "Please see some of the questions in weaknesses.  A few other questions/clarifications are:\n\nHow is the `Finetune` baseline different from typical online-meta learning approaches? Finetune  typically would continue learning  a hypothesis on all past tasks and then adapts it further using data from the new task at hand that is in round T $p_T$. for e.g. in [1] and [2]. \n Is it primarily that in your method the desiderata is essentially different i.e  to obtain a small risk on pT +dt \u2014 as opposed to obtaining a small population risk on all tasks. Is it possible to get the best of both worlds instead i.e. get small risk on the latest task/future task while maintaining low risk across ALL tasks that the agent has seen, as otherwise there is a recency bias to only caring about the immediate next data point. \n\nWhen you claim that \u201c, existing methods for these settings do not employ time-varying propensity weights like we propose here.\u201d Can you comment how does your work position with respect to online meta learning settings such as [1] and [2] where the methods are also able to \u201cautomatically detect data shifts without prior knowledge of their occurrence\u201d without the knowledge of the number of tasks etc. \n\n\n[1] Nguyen, Nicolas, and Claire Vernade. \"Lifelong Best-Arm Identification with Misspecified Priors.\" Sixteenth European Workshop on Reinforcement Learning. 2023.\n\n[2]  Khetarpal, Khimya, et al. \"POMRL: No-Regret Learning-to-Plan with Increasing Horizons.\" arXiv preprint arXiv:2212.14530 (2022)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6570/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699311017794,
        "cdate": 1699311017794,
        "tmdate": 1699636744502,
        "mdate": 1699636744502,
        "license": "CC BY 4.0",
        "version": 2
    }
]