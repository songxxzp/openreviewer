[
    {
        "id": "05ySuH7DrX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1680/Reviewer_yj7F"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1680/Reviewer_yj7F"
        ],
        "forum": "DzxaRFVsgC",
        "replyto": "DzxaRFVsgC",
        "content": {
            "summary": {
                "value": "This submission proposes a finer-grained visual instruction tuning to endow a pretrained large language model with the capabilities to \u201csee.\u201d Compared to the literature, this work is different in that it has region-level image understanding and flexibility in combining multiple ROI\u2019s and reasoning between them. It initializes the model from pretrained on vision and language tasks, respectively, and then uses a two-step instruction tuning pipeline to build multiple visual reasoning capabilities into it."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "In general, the motivation, technique, presentation, and results look all solid to me. The anonymously released codebase looks usable, which, if true, would serve as a well-compiled dataset and starting point for later multi-modality research."
            },
            "weaknesses": {
                "value": "I have to say I haven\u2019t followed multimodality research for some time, so my judgment on the novelty of this work (compared to the literature) could be rusty. This might be an important aspect to consider for this work, but my score is assuming this work is novel. \n\nOn presentation, a flowchart of the two-stage training process might be useful to readers. Indicating which datasets are used in which fashion at which stage, with the purpose of bringing about which kind of capabilities."
            },
            "questions": {
                "value": "Some of my random thoughts, potentially useful to the authors:\n(1)\tHow about visual question-answering datasets? \n(2)\tIs it possible to design some synthetic tasks to train the model, perhaps inserted between the two stages you have right now, to smooth up the training process?\n(3)\tIs it possible to let the LM output coordinates somehow so to interact with users by referring to certain regions in the picture?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1680/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697318523063,
        "cdate": 1697318523063,
        "tmdate": 1699636096237,
        "mdate": 1699636096237,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BatQfuQ48w",
        "forum": "DzxaRFVsgC",
        "replyto": "DzxaRFVsgC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1680/Reviewer_fWBC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1680/Reviewer_fWBC"
        ],
        "content": {
            "summary": {
                "value": "The paper introduced the region-of-interest instruction with vision language models. By doing this, it proposed spatial instruction, combining language and the reference to region-of-interest I to an interleave sequence, enabling accurate region referring and enhancing user interaction. By spatial instruction tuning LLM with massive region-text datasets, the model can follow user instructions to solve diverse region understanding tasks, such as region caption and reasoning. The results show that the model outperforms the previous state-of-the-art approach on a wide range of region understanding benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper proposed a novel method with vision language tasks. It provides a detailed methodology for this.\n- A comprehensive discussion of experiments and results, where the figures are in good quality and readability.\n- The benchmark methods are of good quality."
            },
            "weaknesses": {
                "value": "- For region caption, the paper only compares the proposal to one model GRiT, which is limited."
            },
            "questions": {
                "value": "Can you interpret the performance of different tasks (ie Q->A, QA-> R, Q->AR) in Table 6?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1680/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698704025132,
        "cdate": 1698704025132,
        "tmdate": 1699636096140,
        "mdate": 1699636096140,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Ress3jgN9J",
        "forum": "DzxaRFVsgC",
        "replyto": "DzxaRFVsgC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1680/Reviewer_MYqZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1680/Reviewer_MYqZ"
        ],
        "content": {
            "summary": {
                "value": "This work introduces a new approach called spatial instruction tuning, which aims to enhance the fine-grained multimodal understanding of vision-language models. The proposed model, GPT4RoI, incorporates references to regions-of-interest (RoI) in instructions by replacing them with RoI features and interleaving them with language embeddings. By training on region-text pair datasets, GPT4RoI enables interactive and conversational experiences, allowing users to interact with the model through both language and drawing bounding boxes. GPT4RoI achieves remarkable results on the Visual Commonsense Reasoning (VCR) dataset."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "-\tFine-grained multimodal understanding: GPT4RoI enables region-level alignment and understanding by incorporating references to RoIs in instructions, allowing for more detailed analysis and reasoning.\n-\tInteractive user experience: Users can interact with GPT4RoI through both language input and drawing bounding boxes.\n-\tGPT4RoI achieves remarkable accuracy on the VCR dataset, surpassing existing models by a significant margin."
            },
            "weaknesses": {
                "value": "-\tExpanding from image-level to region-level instruction tuning seems like a natural progression, and the approach is straightforward without providing a fresh perspective. Some other papers also explore the region-level large language models [1] but lack the performance comparison. \n-\tIt appears that while this paper utilized more datasets for training, the improvement in results is relatively marginal, as shown in Table 5. \n-\tThis work lacks a comparison of parameters. The current models seem to be quite large, especially large language models. A comparison should be conducted at the same parameter level, e.g., Table 6. \n\n[1] ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning."
            },
            "questions": {
                "value": "See Weaknesses\n\n-\tComparisons to other works on the same parameter level."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1680/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698732044597,
        "cdate": 1698732044597,
        "tmdate": 1699636096069,
        "mdate": 1699636096069,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "o4UvaUSYrD",
        "forum": "DzxaRFVsgC",
        "replyto": "DzxaRFVsgC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1680/Reviewer_kN3t"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1680/Reviewer_kN3t"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to enhance current large multimodal models by injecting regional awareness. \nAuthors leverage the public available regional data as the instruction data."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Both qualitative and quantitative results demonstrate that now the model can have a sense of location. \n 2. The presentation is clear. \n3. The figures are easy to read."
            },
            "weaknesses": {
                "value": "1. Which part of the model design leads to positional awareness is unclear. Authors have \" five lightweight scale shuffle modules\", \"ROI Align\", \"add feature coordinates (Liu et al., 2018) for each level (positional embedding)\", \"extract region-level features with the output size of 14\u00d714\", which part really makes the model work? There is no ablation study. \n2. Finetuning on a specific dataset can lead to the case that the model forgets all other knowledge. For example, fine-tuning on the multichoice dataset will lead to the case that model can not speak out natural languages whatever you ask. \n3. From the qualitative examples, seems like the model can only produce short descriptions, which may not be suitable when answering with long context. I think the author's model may overfit to such datasets with short captions.  \n4. When converting LLaVa instruct dataset into the format with bounding box, I cast doubt on how accurate it is. If many instances appear in the same image, is it ok to attach so many detection results beforehand? Besides, the LLaVa instruct dataset is known to include hallucination\n5. How do authors claim on other methods which use textual coordinate as the grounding token? In this way, they can  not only use bbox as input, but also use them in output."
            },
            "questions": {
                "value": "1. What is the specific vision model you used? Official CLIP model does not have ViT-H at all.\n2.  For each region, it is compressed into one token? It is enough?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1680/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1680/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1680/Reviewer_kN3t"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1680/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698825621416,
        "cdate": 1698825621416,
        "tmdate": 1699636095990,
        "mdate": 1699636095990,
        "license": "CC BY 4.0",
        "version": 2
    }
]