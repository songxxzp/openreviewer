[
    {
        "id": "SFV4x8F4jG",
        "forum": "LGzTtvisL3",
        "replyto": "LGzTtvisL3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5440/Reviewer_66Ba"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5440/Reviewer_66Ba"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a privacy-preserving feature augmentation method to address the challenges of data scarcity and label-skewness in Federated Learning (FL). The work is well-motivated; however, there are several issues that the authors need to address to convincingly demonstrate the novelty and effectiveness of the proposed method. There are also many writing issues throughout the paper."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The backgroud of the problem and the motivation of the work are well explained"
            },
            "weaknesses": {
                "value": "Major issues:\n\n1. The authors claim that the proposed method is novel, but many components come from the literature. For example, the authors claim that they have devised a novel feature augmentation method, but \u03bb, a key parameter in the method, is drawn from existing literature.\n\n2.\u03bb1 and \u03bb2 in Equation 6 are critical parameters, but it is unclear how they are set.\n\n3.The paper does not discuss the impact of different model types on the proposed method or specify which layer (l-th) has been learned in the experiments. The evaluation solely relies on one AI model, MobileNet, and one dataset, CIFAR-10, which is not sufficient. \n\n4.The statement, 'we characterize the distribution of the low-dimensional features from the penultimate layer of local and global models,' lacks justification regarding why the penultimate layer is suitable for characterizing low-dimensional features.\n\nOther issues:\n-  Section 2.2 mentions low-dimensional feature overlapping and classifier bias as main label skew problems, but it does not elaborate on low-dimensional feature overlapping or explicitly define the problem.\n\n-The paper would benefit from a more comprehensive discussion explaining why sharing features can enhance model performance.\n\n-Notations in B and Bf, which are crucial concepts in the work, are not defined explicitly. Although the definitions of xi, yi, and fi can be inferred, it remains unclear what y^fi represents.\n\n-The sentence regarding (f^l_i , yi) and (fj, yf j ) should use the same notation consistently.\nThe use of '\u03bb \u223c Beta(a, a)' needs clarification. It would be helpful to explain what Beta() is and how it is relevant. The reference at the end of the sentence should specify which part of the information it pertains to.\n\nLanguage problems:\n-\"prompting the introduction of regularization techniques to address this issue.erm, Lr, can be included\"?\n- suggesting that the over-fitting is [severe]\n-FLea works in [an] iterative manner\n-The sentence \"one data batch with labels from Dk and one feature batch with labels from the received feature buffer F (t), termed by B = {(xi, yi) \u2208 Dk} and Bf = {(fi, y^f i ) \u2208 F (t)}, respectively (|B| = |Bf |).\" is incomplete\n- one for knowledge distillation [from] the global model"
            },
            "questions": {
                "value": "How are the values of \u03bb1 and \u03bb2 in Equation 6 are set?\n\nwhat is y^fi in Bf?\n\nWhat are the impact of model types on the proposed method?\n\nWhat is the Beta() function?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5440/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5440/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5440/Reviewer_66Ba"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5440/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698530909418,
        "cdate": 1698530909418,
        "tmdate": 1699636553167,
        "mdate": 1699636553167,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1EEiMHmVEm",
        "forum": "LGzTtvisL3",
        "replyto": "LGzTtvisL3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5440/Reviewer_BqfA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5440/Reviewer_BqfA"
        ],
        "content": {
            "summary": {
                "value": "Aiming to address both the scarcity and label-skewness of data simultaneously, this work proposes the sharing of features across clients  as an extension to the classical federated averaging algorithm. Specifically, features of the $\\ell$-th layer are extracted and distributed across clients. During local training, the shared features and their corresponding labels are used to refine the layers subsequent to the $\\ell$-th layer. To alleviate the privacy risks associated with sharing features, a decoupling loss function is introduced to reduce the correlation between the input data and its corresponding features."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "i. By illustrating the overfitting problem and the gathering effect of features (as shown in Figure 2), this work effectively demonstrates the advantages of sharing data in situations of data scarcity. This supports the proposed framework for feature sharing, particularly when data sharing is restricted due to privacy concerns.\n\nii.Given that federated learning is inherently a privacy-preserving framework, the act of sharing features does increase the risk of information leakage comparing with FedAvg. To mitigate this, the work introduces an additional decoupling loss to strike a privacy-utility trade-off. A comprehensive study evaluating the amount of private information compromised through feature sharing is performed.\n\niii. Experimental results indicate that FLea consistently and significantly outperforms the baselines in scarce and label-skewed data scenario."
            },
            "weaknesses": {
                "value": "i. My primary concern is that the comparison is limited to a single dataset (CIFAR-10) and a single architecture (MobileNet). The experiments cannot sufficiently demonstrate the generalizability of Flea across different applications.\n\nii. The issue of data scarcity and label skew has previously been studied, particularly in Bayesian Federated Learning frameworks. For instance, reference [1] conducted experiments in the same setting. Given that Bayesian FL does not necessitate the additional communication of features, it appears to be a safer alternative to Flea. A comparison would be valuable to ascertain whether Flea can achieve superior accuracy.\n\niii. The first two conclusions in the Section 3.1 are not very convincing. Specifically, \"The performance of FL methods decreases remarkably as data scarcity and label skew increase.\" This argument is concluded from \"FedAvg, its accuracy decreases from 75% to 56% when |Dk| reduces from 5000 to 100 in the IID setting. When local data is sufficient (|Dk| = 5000), its accuracy drops from 75% for the IID setting to 60% for the non-IID setting.\" However, it is unclear why \"|Dk| reduces from 5000 to 100\" can be compared with changing IID to non-IID setting. Additionally, \"Loss-based methods can address label skew only with sufficient local data.\" This argument is too strong. As mentioned in the second point, Bayesian frameworks are more amenable to data scarcity without requiring more data.\n\n[1] Confidence-aware Personalized Federated Learning via Variational Expectation Maximization, Zhu et al., CVPR 2023."
            },
            "questions": {
                "value": "i. The plot of accuracy as a function of $\\lambda_2$ can better clarify the privacy-utility trade-off.\n\nii. It seems that differential privacy can also be employed to obfuscate the features. Since DP is more mathematically rigorous and widely accepted, it may be more interesting to replace $\\ell_{dec}$ with DP."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5440/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5440/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5440/Reviewer_BqfA"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5440/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698533876719,
        "cdate": 1698533876719,
        "tmdate": 1699636553058,
        "mdate": 1699636553058,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rDSzItB6VU",
        "forum": "LGzTtvisL3",
        "replyto": "LGzTtvisL3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5440/Reviewer_mFBA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5440/Reviewer_mFBA"
        ],
        "content": {
            "summary": {
                "value": "The article conducts an in-depth analysis of prior research in the field of federated learning, culminating in a comprehensive exploration of the issues stemming from label-skew and data-skew, and their consequential impacts on overfitting and model bias. To address these issues, the article undertakes a rigorous theoretical examination, and subsequently introduces the Flea framework as a proposed solution."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper comprehensively examines the phenomena of Label-Skew and Data -Skew in the context of federated learning, presenting methods that have demonstrated a significant enhancement in model performance while preserving privacy"
            },
            "weaknesses": {
                "value": "1. The idea of broadcasting features in the paper appears to make a relatively modest contribution. The proposed under-explored scenarios of Label-skewed and Data-skewed resemble another expression of Non-iid data, which is not an under-explored scenario itself. It is unclear whether the authors can provide an explanation for the distinctions between these scenarios and Non-iid data.\n\n2. The concept of overfitting and Client-Drift resulting from Label-skewed and Data-skewed scenarios seems consistent and has been extensively investigated in prior research.\n\n3. Authors says this feature-sharing method is privacy-preserving, but it will leak more privacy compared to other algorithm without feature-sharing\n\n4. The experiments in the paper exhibit depth but lack breadth.\n   a) Lack of domain generalization: The paper's experimental dataset is limited to CIFAR10, without inclusion of other datasets.\n   b) Lack of model generalization: The experimental investigation in the paper is limited to a single MobileNet_V2 model, and the discussion regarding the selection of feature layer 'l' is somewhat vague.\n   c) The paper employs feature broadcasting but lacks a comparative study of communication costs, and the two communication rounds increase overhead.\n   d) Observing that three loss terms act on $\\theta_k$ in Eq 6, the ablation experiments are not presented in the main text. It is recommended that the authors consider reducing the discussion of Label-skewed and Data-skewed scenarios to make the core arguments more concise."
            },
            "questions": {
                "value": "This paper focuses on scarce and label-skewed data in federated learning. This is a kind of non-iid case, and a lot of papers study the non-iid and heterogenous scenarios in FL, why authors say this is the first research?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5440/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698731968992,
        "cdate": 1698731968992,
        "tmdate": 1699636552941,
        "mdate": 1699636552941,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FRN3wmhYJO",
        "forum": "LGzTtvisL3",
        "replyto": "LGzTtvisL3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5440/Reviewer_vPSz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5440/Reviewer_vPSz"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes feature level augmentation for FL on heterogeneous and low data regime. Alongside the model parameters server sends a feature buffer (that includes feature, label pair ) to clients as side information. The authors show that this side information can be used to increase the testing performance in label-skew and data scarce settings."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- As far as I'm aware sending feature, label pairs is a new idea. \n- The presentation of the algorithm through Figure 4 and the visuals presented in the first 3 sections are very helpful. \n- There is a decent increase in the performance compared to non-augmentation methods."
            },
            "weaknesses": {
                "value": "- More details needed on feature buffer, until very late it is not obvious what is the number of pairs and how it is collected.\n- The scale of the experiments (number of samples and clients) is good, but more datasets are needed for evaluation.\n- In section 3.1 authors report some performance changes due to data scarcity etc. but the setting is not clear (e.g. what is the communication frequency/local iterations).  \n- The parameter $a$ is very critical, yet its resulted sensitivity is not thoroughly analyzed in experiments. Also introducing such a parameter is not ideal for FL settings. \n- Communication and computation burden due to the added feature buffer is not examined adequately.\n- By choosing $\\lambda_2$ authors adjust how private the algorithm is but since there is no rigorous privacy utility tradeoff; it is hard to characterize the choice of $\\lambda_2$. I think this lack of rigor is undesirable in FL."
            },
            "questions": {
                "value": "Please address the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5440/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5440/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5440/Reviewer_vPSz"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5440/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698809035607,
        "cdate": 1698809035607,
        "tmdate": 1699636552844,
        "mdate": 1699636552844,
        "license": "CC BY 4.0",
        "version": 2
    }
]