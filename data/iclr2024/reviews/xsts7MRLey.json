[
    {
        "id": "Z91NkkQr3P",
        "forum": "xsts7MRLey",
        "replyto": "xsts7MRLey",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1464/Reviewer_wzDS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1464/Reviewer_wzDS"
        ],
        "content": {
            "summary": {
                "value": "This work explores the use of unsupervised domain adaptation (UDA) for time series classification (TSC), with a particular focus on deep learning methods. In UDA, which has been extensively explored in vision and natural language applications, two domains of data exist: a labelled source domain and an unlabelled target domain that has some form of shift in the time series data (e.g. differences in data used for training and data used during deployment). The objective is to leverage the labelled source data to make predicts in the target domain.\n\nIn addition to five existing datasets, this work proposes the use of seven new datasets for UDA TSC (taken from existing sources). This collection of datasets serves as a benchmarking evaluation tool for assessing the efficacy of different UDA TSC deep learning approaches, notably with different algorithms, hyperparameter optimisation approaches, and model backbones. Consistent experimentation is used to compare the performance of these different approaches and make observations of which elements contribute the most to performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "**Originality**  \nO1. The main novelty of the work lies in the proposal of additional datasets and a consistent, fair framework for evaluation the UDA TSC methods. This also extends to the insights that can be drawn from this evaluation.  \nO2. There is also some originality in the deep UDA methods that are used, notably using consistent a consistent backbone (InceptionTime) across different approaches.  \n\n**Quality**  \nQ1. The experimental setup is well-structured and makes steps to ensure fairness across all algorithms (e.g. limiting GPU time for training/hyperopt).  \nQ2. Results analysis provides some comparisons between the choice of different classifiers, hyperopt methods, and backbones.  \n\n**Clarity**  \nC1. Clear descriptions of all the different elements of the experiments are given (models, hyperopt methods, datasets, and pipelines).  \nC2. Figures are communicative and support conclusions drawn from the work.\n\n**Significance**  \nS1. This work could serve as a stable baseline for further developing UDA TSC deep learning approaches, helping to progress the area of research.  \nS2. Insights into the performance of different methods (e.g. InceptionRain seemingly being the strongest method) is useful for establishing the current SOTA and assessing the relative performance."
            },
            "weaknesses": {
                "value": "**Presentation of Results**  \nP1. While Figure 1 compares model performance within hyperopt methods (a, b, c), it does not provide an overall comparison of all models with all hyperopt methods. As such, it is difficult to determine which complete approach (model + tuning approach) actually has the best performance. An additional critical difference diagram comparing the (5?) top methods for each tuning approach would make this much clearer.  \nP2. While the figures provide some information, and the Appendix gives a full set of results for each dataset, it remains difficult to assess the margins between the approaches. A summary table of average accuracy across all datasets for each experimental configuration would be beneficial in conveying this information.  \nP3. Further variations of Figure 4 for other models/datasets would be useful to see if the revealed trend is consistent.  \nP4. I think the violin plots in Figure 7 are a strong way of communicating the results, and potentially should be moved to the main body if possible. As mentioned above, combining the some selection of the top methods for each tuning approach into a single plot would further aid comparison.  \n\n**Significance**  \nS1. I believe this work has the most potential if the evaluation is released to allow further development of methods. I appreciate the source code is planned to be released upon acceptance, but potentially taking this a step further and allowing for easy reproducibility/extensibility would improve the impact of the work and help progress the research area."
            },
            "questions": {
                "value": "1. To what extent is there dataset imbalance in the datasets? Additional results, for example using balanced accuracy, may be warranted if dataset imbalanced is high. At the very least, a discussion on any dataset imbalances would be helpful. I appreciate F1 score results are given in the appendix."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1464/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1464/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1464/Reviewer_wzDS"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1464/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698313415353,
        "cdate": 1698313415353,
        "tmdate": 1699636075031,
        "mdate": 1699636075031,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lqVdnkGAgt",
        "forum": "xsts7MRLey",
        "replyto": "xsts7MRLey",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1464/Reviewer_hmFL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1464/Reviewer_hmFL"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors present benchmark research on deep unsupervised domain adaptation (UDA) for time series classification (TSC). Specifically, seven new datasets are introduced for this TSC UDA task, and experiments of several existing TSC UDA baselines are tested on these datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "Strength:\n\n1.\tThe paper introduces 7 new datasets for TSC UDA task.\n\n2.\tThe paper conduct experiments on several existing UDA baselines on the new datasets.\n\n3.\tThe paper has potential to be a benchmark for the following TSC UDA research."
            },
            "weaknesses": {
                "value": "Weakness:\n\n1.\tThe major concern of the work is on the technical novelty. All the datasets and baselines (including hyper-parameter tuning methods) are from existing literatures, and there is no novel technical contribution proposed. \n\n2.\tFor UDA TSC, some important related works are missing, for instance (to name a few), unsupervised video domain adaptation [ref1], transfer gaussian process [ref2], and time-series domain adaptation [ref3]. The authors may need to present a more comprehensive related work section to discuss more related works. \n\n3.\tMore analyses on the new datasets are expected, for instance, the domain discrepancy analyses (both marginal and conditional can be involved) on different domain pairs. From the experiments results of the source only baseline, the domain discrepancy differs considerably among different domain pairs, see, Table 5 domain 0 and domain 3, Table 9 domain 9 and domain 18. \n\n4.\tMore analyses on the comparison results are also expected. For instance, analyzing why some baselines achieve positive transfer on some tasks but negative transfer on others, e.g., see OTDA/VRADA in table 9. It would be more interesting to see constructive insights or conclusions that can benefit the community. \n\n[ref1] Video Unsupervised Domain Adaptation with Deep Learning\n\n[ref2] Adaptive Transfer Kernel Learning for Transfer Gaussian Process Regression\n\n[ref3] Time Series Domain Adaptation via Sparse Associative Structure Alignment"
            },
            "questions": {
                "value": "Please refer to the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1464/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698635943999,
        "cdate": 1698635943999,
        "tmdate": 1699636074965,
        "mdate": 1699636074965,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uWnBkqUE5l",
        "forum": "xsts7MRLey",
        "replyto": "xsts7MRLey",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1464/Reviewer_KuTn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1464/Reviewer_KuTn"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a thorough benchmarking study on time-series unsupervised domain adaptation, primarily focusing on deep learning techniques. It examines the impact of model backbones and hyperparameter tuning approaches. Furthermore, the authors evaluate various existing unsupervised domain adaptation methods across multiple domains, including seven new benchmark datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The study delivers a detailed benchmark on unsupervised domain adaptation for time-series data, delving into the effect of domain adaptation algorithms, model backbones, and hyperparameter tuning strategies.\n2. The paper evaluates a range of unsupervised domain adaptation methods on datasets from diverse domains, including seven newly introduced datasets and existing benchmarks."
            },
            "weaknesses": {
                "value": "1. The discussion on the effect of model backbone in the paper is limited primarily to the Inception model. A broader examination involving diverse backbone models is crucial to substantiate the claim that \"backbones do not have a significant impact\".\n2. More discussions on different types of unsupervised domain adaptation methods would be beneficial. Specifically, it would be informative to explore under what specific conditions certain domain adaptation approaches may outperform others.\n3. Additional discussions regarding the choice of model backbones is helpful too. For example, I am curious if Inception is the best model backbone over all domains, or we need different model backbones for different time-series domains and data characteristics.\n4. Figure 2 (b) and 5 should be merged since they lead to similar findings."
            },
            "questions": {
                "value": "See Weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1464/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1464/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1464/Reviewer_KuTn"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1464/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698810719114,
        "cdate": 1698810719114,
        "tmdate": 1699636074893,
        "mdate": 1699636074893,
        "license": "CC BY 4.0",
        "version": 2
    }
]