[
    {
        "id": "NdYW13uAEI",
        "forum": "bDkisS75zy",
        "replyto": "bDkisS75zy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5044/Reviewer_bdF4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5044/Reviewer_bdF4"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a concatenated sample pretrained vision-language foundation model. By sequentially connecting multiple image-text pairs as pre-training inputs, it can jointly model visual content and event-level temporal cues using only image-text corpora. Extensive experiments show the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tMotivations: This paper presents a very important problem of how to capture time-level event clues using only image-text data. In the case of insufficient quality and quantity of video data, it provides a very important help for the pre-training of vision-language foundation model. The proposed method is very simple and effective. I think this work is easy to follow and most of the techniques are correct.\n\n2.\tExtensive experiments: A large amount of experimental evidence is provided in this paper, which fully verifies the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1.\tTechnical contributions: The proposed method is simple and effective. However, the proposed method is not surprising enough, because using pictures to enhance video pre-training has been quite explored in the field of CV/ vision-language pre-training field. This paper combines many existing pre-training methods, so the technology sharing is limited.\n\n2.\tMore explanation: The continuous frames in the video are similar, and there are relations between different events. Establishing event-level correlation includes two parts: (1) the first part is to distinguish between different events. (2) the second part is to make temporal inferences between similar or related frames. The proposed method randomly splices several pictures, but there is no correlation between these pictures, so the model can only distinguish different events, but can not make the model time sequence inference between frames. Therefore, I do not think that the proposed method fully corresponds to its motivations."
            },
            "questions": {
                "value": "1. In Table 9, why is it better to concatenate random images for training than to concatenate only semantically similar images?\n2. It is better to give the weights of the 6 losses (training objectives) and the size of the input image in the implementation details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5044/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5044/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5044/Reviewer_bdF4"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5044/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697787933590,
        "cdate": 1697787933590,
        "tmdate": 1699636494101,
        "mdate": 1699636494101,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jyfSb5qPmA",
        "forum": "bDkisS75zy",
        "replyto": "bDkisS75zy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5044/Reviewer_gWLj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5044/Reviewer_gWLj"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a new vision-language pre-training framework, called COSA. In particular, COSA augmented original image-text pairs by concatenating multiple examples as pseudo video-text pairs. Extensive experiments were conducted covering both video-language and image-language tasks, and demonstrated the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well written and easy to follow. In addition, the proposed method was supported by comprehensive experiments together with ablation studies, which made the paper a complete work.\n- The method COSA itself was simple yet effective to improve the learned representations for downstream tasks, and at the same time, it did not introduce extra computational costs."
            },
            "weaknesses": {
                "value": "- The method was more like a trick of data augmentation instead of a significant technical contribution, as it just simply concatenated images and their corresponding captions and it was not very surprising to observe performance improvements.\n- As it was mentioned in the paper that apart from modified objectives, COSA also included original objectives for pre-training on image-text pairs. It was a complicated design to have so many training objectives and it was unclear how they were weighted (seemed to be equally weighted). Even though there was an ablation study of training objectives in Table 7, it still did not explain well the contributions of each item.\n- The method leveraged the average pooled [CLS] token for each image as the final representation for the pseudo video. In this way, there was actually no temporal information considered. And the selected downstream tasks were less dependent on temporal information in the meanwhile. It would be better if tasks such as temporal action localization were included to show whether COSA can improve those tasks. In addition, since temporal information did not play any role in current method, I am afraid that using augmentations like mixup for videos/images might lead to similar performance gain, as shown in [1].\n- Previous works showed that using CLIP initialization could lead to better performance. Among compared baseline methods, some of them such as MILES [2] actually used ViT trained on ImageNet for image classification and it was not a fair comparison to COSA with CLIP initialization.\n\n[1] Hao, Xiaoshuai, et al. \"Mixgen: A new multi-modal data augmentation.\" Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2023.\n\n[2] Ge, Yuying, et al. \"Miles: Visual bert pre-training with injected language semantics for video-text retrieval.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022."
            },
            "questions": {
                "value": "- Is it possible for the authors to include tasks which rely much on temporal information like temporal action localization? This would provide better understanding of the proposed method.\n- It would be better if results with different initializations can be presented to remove my concern about better CLIP initialization.\n- It was worth trying data augmentations like mixup and it might lead to similar performance gain as demonstrated in the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5044/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5044/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5044/Reviewer_gWLj"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5044/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698552787035,
        "cdate": 1698552787035,
        "tmdate": 1700678123487,
        "mdate": 1700678123487,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OUYXw0B4aO",
        "forum": "bDkisS75zy",
        "replyto": "bDkisS75zy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5044/Reviewer_vqzh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5044/Reviewer_vqzh"
        ],
        "content": {
            "summary": {
                "value": "The work proposes the vision-language foundation model, which can jointly model visual contents and event-level temporal cues using only image-text corpora. Extensive experiments demonstrate that COSA consistently improves performance across a broad range of semantic vision-language downstream tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper proposes the effective method for video-text and image-text tasks.\n2. The experiment is very adequate.  The model consistently improves performance\nacross a broad range of semantic vision-language downstream tasks."
            },
            "weaknesses": {
                "value": "1. The reasons for the improvement brought by Concatenation lack detailed analysis. Why is there also improvement for image-text tasks? Why is it necessary to include the video dataset (web2vid)? Why wasn't the 1.2B model included in the video dataset?\n2. The data shown in Table 1 is confusing. The data for COSA-L is 417M, while the data volume for COSA is 415M.\n3. The results in Table 7 and Table 7 are also confusing. The best performance is based on 6 pretraining task? Which pre-training tasks were used in the overall experimental results of COSA? Is the WebVid2.5M dataset more important\uff0cthe results for COSA 4 frames?"
            },
            "questions": {
                "value": "see weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5044/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698653519545,
        "cdate": 1698653519545,
        "tmdate": 1699636493723,
        "mdate": 1699636493723,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gXWVrc0GdP",
        "forum": "bDkisS75zy",
        "replyto": "bDkisS75zy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5044/Reviewer_kord"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5044/Reviewer_kord"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed to concatenate image-text samples to mimic video-paragraph corpus in vision-language pre-training. The method is simple and the evaluation is conducted on various image/video datasets to demonstrate impressive performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The idea is simple and easy to reproduce. Meanwhile, the performance gain is impressive.\n2. The experiments are conducted on many benchmarks across image-text and video-text tasks, as well as different data scales. Also the ablation is comprehensive and covers most of the aspects of this method."
            },
            "weaknesses": {
                "value": "1. It makes sense that pseudo video-paragraph data in pre-training can mitigate the gap between pre-training and fine-tuning in image-text pertaining. However, intuitively, the discontinuity of semantics in pseudo video-paragraph data should hurt compared with relevant video-paragraph data because in downstream videos, image and text are indeed relevant. But in Tab9, it seems random sampling is better than relevant sampling, which is kind of counter-intuitive. Can the authors explain more about it?\n\n2. When having seen the same number of samples, whether COSA is better than SST in `image-text downstream tasks`? Basically, I want to see the comparison like Figure 4 in image-text downstream tasks. I am okay with this observation not holding anymore in image-text downstream tasks because essentially video-paragraph and image-text are different domains.\n\n3. I want to see how this method performs in zero-shot image-text tasks. Considering the domain gap, I suspect it might perform worse than some image-text pre-trained methods that COSA can outperform when finetuning."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5044/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698819270444,
        "cdate": 1698819270444,
        "tmdate": 1699636493619,
        "mdate": 1699636493619,
        "license": "CC BY 4.0",
        "version": 2
    }
]