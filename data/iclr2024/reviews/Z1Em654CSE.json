[
    {
        "id": "aPam1ictAs",
        "forum": "Z1Em654CSE",
        "replyto": "Z1Em654CSE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1927/Reviewer_QXZ6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1927/Reviewer_QXZ6"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel cross-modal RGB-Event dataset for Multi-Object Tracking (MOT), aimed at addressing challenges in object tracking in complex real-world scenarios such as low-illumination conditions, small object detection, and occlusions. Utilizing the advantages of Event-based vision, known for its superior temporal resolution, vast dynamic range, and low latency, alongside conventional RGB data, the authors strive to advance the field of MOT. The newly developed dataset comprises nearly one million annotated ground-truth bounding boxes and is tested using state-of-the-art MOT algorithms, revealing a significant enhancement in performance with the integration of event data. The paper also explores the efficacy of different data fusion techniques, highlighting the potential of mask modeling over simple averaging. Through rigorous assessment and comparison with existing methods and datasets, the authors underline the potential of their proposed benchmark in driving further research and improving the robustness and versatility of detection and tracking systems, particularly in challenging visual scenarios. Besides, the authors acknowledge certain limitations of their dataset including static viewpoints and isolated hard cases, and suggest future directions for refining fusion techniques, embedding methods for event data, and development of specialized box association algorithms to better utilize the unique attributes of event data in MOT."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Here are some strengths of the paper:\n\n\t1. The paper is well-written and easy to understand. The authors provide clear explanations of the proposed algorithm and its components, as well as the motivation behind their approach.\n\t2. The paper introduces a unique cross-modal RGB-Event dataset for Multi-Object Tracking (MOT), significantly enriching the resources available for research in this field.\n\t3. The focus on overcoming practical challenges such as low-illumination conditions, occlusions, and small object detection aligns the paper with real-world needs in computer vision.\n\t4. Through thorough evaluation using state-of-the-art MOT algorithms, the paper substantiates the benefits of integrating event data with traditional RGB data.\n\t5. The authors intend to make the source code and the dataset publicly available upon acceptance, which fosters reproducibility and allows other researchers to build upon their work."
            },
            "weaknesses": {
                "value": "Here are some potential weaknesses:\n\n\t1. The exploration of data fusion techniques is somewhat limited with the utilization of simplistic averaging and mask modeling, which might not fully exploit the potential of cross-modal data fusion.\n\t2. The paper seems to focus on early fusion strategies, where RGB and Event data are fused at the input level. However, it does not explore or discuss middle or late fusion strategies, which could provide different perspectives and potentially better performance.\n\t3. The paper could have delved deeper into proficient embedding methods for event data, which is essential for leveraging the high temporal resolution of event data effectively.\n\t4. The paper does not delve into the discussion or evaluation of transformer-based methods for Multi-Object Tracking (MOT), which have been emerging as powerful tools for handling sequences and spatial relationships in data. \n\t5. The paper does not provide information or discussion on the frame rate (FPS) of the tracker after incorporating event data. This is crucial as the processing speed is a vital aspect of real-time multi-object tracking applications.\n\t6. The paper aims to optimize detection performance through the integration of RGB and Event data, yet lacks discussion or specification on the particular detector used. This omission can lead to a lack of clarity and could hinder the reproducibility of the proposed methods."
            },
            "questions": {
                "value": "1. Could the authors elaborate on why only simplistic averaging and mask modeling were chosen for data fusion over more sophisticated techniques?\n\t2. Why were middle or late fusion strategies not explored, and do the authors anticipate different outcomes with these alternative fusion strategies?\n\t3. Could the authors provide more details on the embedding methods explored for event data and their impact on the system's performance?\n\t4. Have the authors considered integrating transformer-based methods for multi-object tracking, given their promise in sequence processing tasks?\n\t5. Can the authors provide the frame rate of the tracker post event data integration, and discuss its implications for real-time application?\n\t6. Could the authors specify the detector used, its integration with RGB and Event data, and the influence of the choice of detector on the results?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1927/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1927/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1927/Reviewer_QXZ6"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1927/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698735693656,
        "cdate": 1698735693656,
        "tmdate": 1699636123355,
        "mdate": 1699636123355,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QVjfybeCOR",
        "forum": "Z1Em654CSE",
        "replyto": "Z1Em654CSE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1927/Reviewer_FGic"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1927/Reviewer_FGic"
        ],
        "content": {
            "summary": {
                "value": "The paper first proposes the rgb-event multi-object tracking task which is new and interesting. It handles the low illumination, occlusion, and low-latency issues in the traditional rgb-based MOT task. It proposes a dataset that contains 12 videos for evaluation and also provides some baselines for future works to compare. For the baseline approach, the authors propose to fuse the dual modalities using concatenate or masking technique. This paper is well-written and the organization is good."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper first proposes the rgb-event multi-object tracking task which is new and interesting."
            },
            "weaknesses": {
                "value": "For the issues of this work:\n\nthe dataset is relatively small, 12 videos is not large-scale enough for current tracking tasks, especially in the big model era;\nthe baseline method is not novel, only simple fusion strategies are exploited; no novel fusion modules are proposed;\nTherefore, I tend to reject this paper and encourage the authors to collect a larger rgb-event mot dataset or a more novel mot tracking framework."
            },
            "questions": {
                "value": "1. the dataset is relatively small, 12 videos is not large-scale enough for current tracking tasks, especially in the big model era; \n\n2. the baseline method is not novel, only simple fusion strategies are exploited; no novel fusion modules are proposed;"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1927/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698763846752,
        "cdate": 1698763846752,
        "tmdate": 1699636123284,
        "mdate": 1699636123284,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UlxaiwcuZm",
        "forum": "Z1Em654CSE",
        "replyto": "Z1Em654CSE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1927/Reviewer_2xGM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1927/Reviewer_2xGM"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a dataset for combined RGB and Event camera tracking. The initial focus of the paper is to motivate the use of Event cameras for the task, considering challenges like low-illumination, occlusions etc. The dataset is then described in detail. The paper then applies existing algorithms post merging the RGB and Event camera data in the feature space. The results are presented on the proposed dataset."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The dataset with combined and calibrated RGB and Event camera data is valuable. And the data collection + annotation is the major strength of the paper.\n\n- The need for using Event camera is well motivated \n\n- The paper is easy to read and understand"
            },
            "weaknesses": {
                "value": "1. The method and experiments sections are vaguely presented. Several crucial details are missing:\n\n(a) It is not clear, if a separate backbone (Figure 4) is used for both Event and RGB cameras. If yes, how were they trained?\n\n(b) Was the proposed dataset used for training the detector?\n\n(c) Was anything beyond the detector was trained or updated in the method? Was the retrained detector used in all the baseline methods?\n\n(d) How was the Re-ID network trained? If not, which network was used for computing the Re-ID features?\n\n(e) A common observation in several prior MOT paper is that Re_ID does not really play a significant role. The performance largely depends on the detection proposals and the motion model. An ablation without using the Re-ID features would be useful. \n\n(f) The paper does not talk about the motion model at all. An ablation with and without using any motion model would add value to the paper.  \n\n(g) How exactly is averaging or masking done. Corresponding equations are warranted. It is extremely vague in the current form. \n\n\n\n2. The description is unclear at several places\n\n(a) What is e in Eqn1?\n\n(b) If \\delta is a scalar why does it vary with time (Eqn1 \\delta_t)\n\n\n\n3. If one uses consecutive frame differences instead of the frames from the event camera, will that achieve similar gains?"
            },
            "questions": {
                "value": "Please address the concerns raised in the weaknesses section. The method section is completely unclear in the current form."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1927/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698836859970,
        "cdate": 1698836859970,
        "tmdate": 1699636123211,
        "mdate": 1699636123211,
        "license": "CC BY 4.0",
        "version": 2
    }
]