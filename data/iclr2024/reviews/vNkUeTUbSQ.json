[
    {
        "id": "rDYHN8rKFs",
        "forum": "vNkUeTUbSQ",
        "replyto": "vNkUeTUbSQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1951/Reviewer_8YEo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1951/Reviewer_8YEo"
        ],
        "content": {
            "summary": {
                "value": "The authors analyze the internal representations learned by a policy network trained to solve mazes. They identify channels that contain the goal information and analyze several interventions that modify the behaviour of the agent."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper is very well-written. The idea is simple and the presentation is clear, which makes the contents easy to follow. The experiments are well-designed to illustrate the discussion. The overall goal of understanding the internal representations learned by our agents is an important topic and clearly deserves a study."
            },
            "weaknesses": {
                "value": "The scope of the paper is far too narrow. The study is exhaustive, but it is focused on a particular architecture in a particular environment. Is there any chance of applying those results to other agents in other domains? I don't see any simple way. Thus, this work would have a very low impact on the community. Besides, the presented analysis is based on the 11 layers _found by visual inspection_. While suitable for a single experiment, this method can't be applied broadly.\n\nThe numerical results you obtain are far from convincing. If the logistic regression can predict the goal reachability in 82% compared to naive 71%, then quite few hard cases were actually explained. Certainly, there are more important features. I'd like to see that you define like 20 different features, exceed 95% accuracy, and then identify those that contribute most. Now, I think that you still miss important features. Also, when you analyze the interventions in Section 3.1 (and others as well), I see quite little difference between no intervention and intervening on all 11 channels. I'm not convinced that you _control_ the policy. I'm convinced that _there is some correlation between the intervention and the intended behaviour_. Furthermore, I think this analysis would be useful if you exceed the impact of moving the cheese. Only then you can confidently claim that you can control the policy and convince me. Now, it seems more like a bias than a control.\n\nOverall, after reading this paper I would agree that you identified _some_ features that contribute to the behaviour, although (as you claim yourself) clearly there are much more of them, which makes the contribution even lower.\n\nI am willing to increase my rating if you prove me wrong in those claims."
            },
            "questions": {
                "value": "Can the results that you present be generalized to other architectures and environments?\n\nCan all those steps you describe (choosing the layer, identifying the channels, retargeting the goal, etc.) be automated?\n\nAre there any reasons for the network to learn the goal location as a separate feature? Technically, it could be arbitrarily mixed with other features (as long as it can be extracted with linear transformations), rendering the visual inspection impractical.\n\nWhy did you choose this specific layer to inspect? Are those observations valid for many layers and you've just chosen an arbitrary one, or was it a careful decision?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1951/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1951/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1951/Reviewer_8YEo"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1951/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698019065092,
        "cdate": 1698019065092,
        "tmdate": 1699636126682,
        "mdate": 1699636126682,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hr95ExU3Wl",
        "forum": "vNkUeTUbSQ",
        "replyto": "vNkUeTUbSQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1951/Reviewer_Ejpo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1951/Reviewer_Ejpo"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on a pretrained reinforcement learning policy which solves mazes problem. The authors find certain circuits correspond to one of these goals and identify eleven channels which track the location of the goal. What\u2019s more, they modify these channels by hand-designed interventions or by combining forward passes to partially control the policy."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1: The first to pinpoint internal goal representations in a trained policy network.\n2: The visualization of activations is relatively clear and intuitive.\n3: Experiments have been conducted to explore how the activation of specific channels affects the behaviors of policy."
            },
            "weaknesses": {
                "value": "1: The paper lacks further validation of whether the discovered intrinsic representation of the goal in the pre-training policy can be generalized to different policies. This would weaken the value attributed by the article to its exploration of the representation of the goal within the policy. This is because the phenomena mentioned in the article are only specific to a particular parameterized strategy rather than a general family of strategies trained on that environment.\n2. The experimental phenomena and conclusions of the author cannot fully support their core contribution. We cannot demonstrate from the experiments that the 'intrinsic representation' of the goal in the policy can be represented by the activations of these 11 channels that are selected by human visual inspection. \n3: The presentation of some of the experiments is confusing, and it may be helpful to detail the setup of these experiments in the appendix to help understand the work."
            },
            "questions": {
                "value": "Question 1: \nAre the activations influenced by the agent's location or different steps? When we aim to control the policy by adjusting the activations, should we modify the activation of the initial state or all other states?\nQuestion 2: \nDoes the number of the most effective channels change when the size of the maze is altered? Or will the features found in 11 selected channels maintain consistency with respect to such changes in experimental settings?\nQuestion 3: \nOn page 6, what does it mean by \"the geometric mean of the action probabilities to a given square from the start position\"? I'm asking because I'm uncertain about the calculation of the \"normalized path probability.\"\nQuestion 4:\nWhy did you choose activations after the first residual block of the second IMPALA block as your target? Are there any insights or observations behind it? \nQuestion 5:\nAs authors said, we can control the behavior of the policy by combining different forward passes of the network, is it possible to control the policy to exhibit more flexible behavior in this way? For example, let the policy tend to move toward the upper-left corner (neither the position of the goal nor the \"upper-right corner\" bias introduced in the training phase)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1951/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698750746353,
        "cdate": 1698750746353,
        "tmdate": 1699636126608,
        "mdate": 1699636126608,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zKiVPNZAOt",
        "forum": "vNkUeTUbSQ",
        "replyto": "vNkUeTUbSQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1951/Reviewer_RNSn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1951/Reviewer_RNSn"
        ],
        "content": {
            "summary": {
                "value": "The authors study generalization of an image-based RL agent trained to find cheese in a maze. During training time, the cheese is always in the top right corner of the maze. During test time, the cheese can be anywhere. As a result, in a new maze at test time, the agent sometimes finds cheese, and sometimes goes to the top right corner.\n\nThey carefully analyze the network's behavior and internal mechanisms, and find key situations in which the agent decides between finding the cheese, or just going to the top right corner. Next, they analyze the network structure and find key neurons in the network that track the cheese. By modifying these neurons, they can somewhat control the behavior."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors tackle a very important problem in RL - generalization. Their specific definition of generalization, \"goal misgeneralization\", is less studied, yet is extremely important in the context of modern LLMs, RLHF, and  alignment. Even if we give the agent the \"correct\" reward function, it may still act unpredictably in OOD situations.\n\nTheir analysis of the maze task and policy is quite deep, and has some interesting studies and findings. They find that the agent chooses to pursue the cheese or the corner based on visual proximity.  Their experiments on controlling the policy by modifying internal activations is quite interesting as well."
            },
            "weaknesses": {
                "value": "The paper's deep analysis of the maze-cheese task and policy is its strength, and also main weakness. Many of the analyses and experiments hinge on their knowledge of the task, and also their design of the policy. This leads me to question 1) if these findings hold true for more realistic, complicated and relevant tasks, and 2) if the particular methodology used here, can be applied to other RL agents.\n\nFor example, the interpretability and controllability of the policy hinges on architecture and input image - an image-only, CNN-based policy.  Because the task is a 2D image, and the policy is CNN based, the authors can manually inspect all feature maps to find correlations with the 2d position of the cheese.  \n\nMany tasks though, may be multimodal, non-image based, or even if they are image-based, may be first-person views of a 3d world. Many deep RL agents have different architectures - MLPs, LSTMs, Transformers, etc. The wide variety of possible tasks and agents seems to make it hard to use this approach for future studies.\n\nNext, I did not see any mention of seed variance. Is it possible that these findings only emerge with the correct seed? How general are these results across RL agents, even if we fix the task to the Maze task?"
            },
            "questions": {
                "value": "Could the authors address this point about task / policy specificity? \n\nCould the authors address the concern on seed variance? \n\nTop-right corner motivational vector - is the definition ordering swapped?\nFigure 7 - the columns seem to be out of order."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1951/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699041543187,
        "cdate": 1699041543187,
        "tmdate": 1699636126499,
        "mdate": 1699636126499,
        "license": "CC BY 4.0",
        "version": 2
    }
]