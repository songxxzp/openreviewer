[
    {
        "id": "brOFJzoXnm",
        "forum": "DkYQHewNcp",
        "replyto": "DkYQHewNcp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7388/Reviewer_C8pe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7388/Reviewer_C8pe"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a convolution dictionary learning method designed for neural data. They also propose a way to assess the statistical significance of their pattern detection and demonstrate speedup compared to other methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is clearly written and there is an appreciable progression from experiments on synthetic data to real data. Moreover, the authors present a method to assess the statistical significance of their convolutional pattern detection.\n\nFigures 3.D and 3.E are reassuring in that they seem to show that maximizing the variance in the objective eq.1 (which was motivated intuitively) does indeed correlate with pattern detection. \n\nFinally, beyond the interpretability of their method, the authors exhibit a speedup compared to other methods."
            },
            "weaknesses": {
                "value": "I am surprised in Figure 8 that there are few standard convolutional dictionary learning methods to compare against, given that convolutional dictionary learning is a field with a rich literature. Could the authors explain how their method differs from other convolutional dictionary learning methods used for neural data, e.g. [1]?\n\n[1]  Dupre La Tour et al. Multivariate Convolutional Sparse Coding for Electromagnetic Brain Signals. NeurIPS, 2018."
            },
            "questions": {
                "value": "Can the authors specify in the main text the data modality used in section 4.3.: are these measurements from cell calcium imaging? \n\nCan the authors explain the main argument for the speed of their method compared to other methods in Figure 8?\n\nWhat is f, on line 113?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7388/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697792229766,
        "cdate": 1697792229766,
        "tmdate": 1699636884580,
        "mdate": 1699636884580,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ydns0yCdKl",
        "forum": "DkYQHewNcp",
        "replyto": "DkYQHewNcp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7388/Reviewer_u7XE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7388/Reviewer_u7XE"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a way to find repeating spike patterns in multi-channel neural data. The method uses a novel loss function to learn multiple kernels that respond strongly to different recurring patterns. It is tested and found to perform well on several synthetic datasets as well as a dataset of mouse place-cell responses for which the ground truth is known via the mouse\u2019s position on a track. The method is shown to run more quickly than related past methods."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper addresses a substantial issue in analysis of large neural data. It seems to work well and efficiently. The method and the results are clearly presented. \n\nThe loss function seems elegant and well designed, and its explanation is clear. I didn\u2019t find the loss obvious at first but rather felt that reading this section broadened my mind a little."
            },
            "weaknesses": {
                "value": "I take it the point of unsupervised detection of neural patterns is to find them even when ground truth isn\u2019t known. However, the method wasn\u2019t applied to such data. Such an application couldn\u2019t be used to test the accuracy of the method, but it would help to illustrate qualitatively what can be expected from it in a realistic scenario, and it might provide an example of downstream use of the results."
            },
            "questions": {
                "value": "Line 74: Can \u201crepeating\u201d be defined more clearly? What kinds of variations aside from independent jitter are expected biologically, if any? \n\nWhat are the spike rates of the background activity? \n\nFigure 3E: Why does it appear that the network learns nothing for 150 epochs and then suddenly converges? This seems inconsistent with the choice of 100 steps in section 4.4, particularly the claim of faster convergence in lines 195-197. \n\nFigure 7: Could the red traces be overlaid on panels C and D as well? Also it appears that the slopes in these panels are smaller than the speed of the mouse. Is that expected? Why? The detections are clear in any case, which is the main point. \n\nLine 204: Is there really a 2D convolution operation? In the neuron dimension maybe you have a non-padded convolution with kernel size equal to input size, but I don\u2019t think it\u2019s standard to call that 2D. \n\nAppendix B.4 & B.5: The comparison with PP-Seq is hard to interpret because both the true and false positive rates of PP-Seq are higher. Can you change a threshold to match one of these measures and compare the other? \n\nThe dropout probabilities range from 0.2 to 0.4, and I was not sure how to relate that to spike statistics (e.g. Poisson or otherwise). Can this be clarified? \n\nFigure B.9: The sorted spike sequences look tighter here than in Figure 3. Are they? Why? Does it matter? \n\nFigure B.12: This looks qualitatively quite different than Figure 7 and perhaps more should be said about this in the main text. \n\nThe learned kernels seem to include all the neurons, whether or not they participate in the sequence. Is it desirable to ignore non-participating neurons? Figure B.16 seems to suggest one way to do this, i.e. by checking for a Gaussian-like kernel. Are there better ways?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7388/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698810806682,
        "cdate": 1698810806682,
        "tmdate": 1699636884441,
        "mdate": 1699636884441,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MB1qzmFXCA",
        "forum": "DkYQHewNcp",
        "replyto": "DkYQHewNcp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7388/Reviewer_qa1s"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7388/Reviewer_qa1s"
        ],
        "content": {
            "summary": {
                "value": "This paper presents an unsupervised method for identifying sequences in neural spiking data.   The method learns a set of K filters that summarize the spiking data subject to what seem like some pretty minimal constraints.  The method is applied to ground truth data and recordings from rodent hippocampus.  The method is faster and perhaps more reliable than other competing methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Right now, the priors that working neuroscientists bring to analyzing their data has a huge effect on the results they are able to discover.  It would be really very useful to have an unsupervised method to automatically extract sequential information from spiking neurons.  Not only would it be outstanding for analyzing data from freely moving animals (as in Fig 7)  but also would be really impactful for understanding population burst events, theta sequences, etc etc.\n\nSuch tools will become increasingly important as recording techniques continue to advance."
            },
            "weaknesses": {
                "value": "I am concerned about priors that may be ``baked in'' to the method (perhaps inadvertently).  At mimimum these priors should be made more explicit.  In particular, I'm concerned that the model seems to find ``straighter'' sequences than are present in the data (Fig 7).  The ground truth experiments all use linear sequences, exacerbating this concern.\n\nIt's not obvious that the method can generalize to sequences (such as PBEs, theta etc) that unfold over more than 1 continuous dimension."
            },
            "questions": {
                "value": "If ground truth includes sequences that unfold at varying rates can this method identify them?  For instance, suppose that there are place fields along a linear track but they are overrepresented near the ends.   The animal runs at a constant velocity. Now the sequence, rather than appearing as a straight line in, say, Fig 3b, would appear as a hook.  Can this method find those sequences as well?  I think this is a very important question as it seems that these kinds of sequences are very general.  Is there a way to make the filters more or less sensitive to these kinds of sequences?\n\nSuppose we had a set of place cells that tile a 2-D enclosure.  Would this method work?  I'm concerned that the filters will have to cover a 2-D surface with piecewise 1-D filters and this will fail really badly.  \n\nTake the situation in Fig. 7.  Suppose the animal starts out on the linear track at a constant velocity, stops half way through, backtracks for 10 cm, then turns around and continues along its original trajectory to the end.  What filters does this method find?  What should it identify?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7388/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698816254671,
        "cdate": 1698816254671,
        "tmdate": 1699636884324,
        "mdate": 1699636884324,
        "license": "CC BY 4.0",
        "version": 2
    }
]