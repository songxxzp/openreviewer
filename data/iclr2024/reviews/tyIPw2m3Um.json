[
    {
        "id": "VDz8b5jYLT",
        "forum": "tyIPw2m3Um",
        "replyto": "tyIPw2m3Um",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1676/Reviewer_kFkh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1676/Reviewer_kFkh"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the impact of adding a $\\beta$ term in the softmax equation. Let $m$ be the number of classes, they propose:\n$$p = \\frac{e^x}{\\sum_i e^{x_i} + \\beta e^x}, \\quad \\text{with } x \\in R^{m}, \\text{and } p \\in R^{m}$$\nWhen used alongside the cross-entropy loss, the formulation of the loss on a training sample $(x,y)$ becomes:\n$$\\ell(x,y) = -\\log(p_y) = - \\log(\\frac{e^{x_y}}{\\sum_i e^{x_i} + \\beta e^{x_y}}) $$\nThey show how $\\beta$ enforces a soft margin and modulates the gradient magnitude depending on the probability $p_y$. More precisely, they show how small $\\beta$s increase the gradient magnitude for larger probabilities---promoting a soft margin---, while larger $\\beta$s reduce the gradient magnitude for larger probabilities. They theoretically derive this observation and validate it empirically on $4$ vision datasets. Moreover, they draw a parallel with curriculum learning and calibration."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "I find this work well motivated: the softmax function is ubiquitous in modern machine learning and studying its various caveats is important. \nThe connection with calibration is interesting, and the results in figure 6 are very promising. Especially, the calibration improves as $\\beta$ increases, which allows the model to be less influenced by current samples having a $p_y$ close to $1$."
            },
            "weaknesses": {
                "value": "I found two main weaknesses in this work. The first one consists of the overall lack of clarity. I find the paper hard to read. Here are some parts I found confusing:\n- \"MSE takes into account more complex optimization scenarios\": What do you mean by that?\n- \"Hard mining strategy\": you could briefly introduce what this is. \n- in section 2, you talk about $J_j$ before introducing it\n- In figure 3: there are no legends for the top row, and the caption does not help to clarify the different curves being shown, the main text is also unclear about those i.e. what are \"post-training samples\", are those test samples? \n- In figure 3 still, it should be mentioned in the legend or in the caption that the different groups correspond to samples of varying difficulty\n- \"If we make excessive demands on the margin, some post-training samples cannot get any chance and will be \"sacrificed\" according to the soft curriculum learning strategy [...]\", what do you mean? \n- \"So it is convinced that the general softmax [...]\"\n- \"The beta smaller is, the gradient smoother is\"\n- \"Warm-up strategy achieves even better results.\"\n- \"Curriculum design that divides samples is crucial to curriculum learning idea.\"\n- \"Besides, based the previous analysis [...]\"\n- In figure 6: the y axis mentions accuracy and but also shows confidence, this is confusing and could be clarified in the caption. \n\nThe second weakness is the lack of rigor in the experiments: \n- In figure 4: which model is being used? How many parameters? How were the hyperparameters tuned? Are those averaged over multiple seeds, if yes can we see the standard deviation? \n- In table 2: The different values are quite close, and it is difficult to evaluate the robustness of the improvement without standard deviation. It should be possible to run the same experiment with different seeds for some of the smaller datasets. \n- In table 3: same as above, I would love to see standard deviations \n- For all the experiments, which experimental protocol was followed: which architecture, tuning, seeds, optimizer, ... I couldn't find those in the appendix either"
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1676/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1676/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1676/Reviewer_kFkh"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1676/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698766899496,
        "cdate": 1698766899496,
        "tmdate": 1699636095798,
        "mdate": 1699636095798,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mKf9xp4TYm",
        "forum": "tyIPw2m3Um",
        "replyto": "tyIPw2m3Um",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1676/Reviewer_KFsP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1676/Reviewer_KFsP"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the influence of introducing a margin parameter $\\beta$ in the softmax operator for classification problems. The idea is to relate the margin parameter $\\beta$ to the decay rate of the gradients so that the classification confidence can be manipulated. The margin parameter also gives rise to some improved performance in image data. I would view the margin parameter $\\beta$ as the major contribution of the paper, as existing study often focuses on the temperature parameter."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper provides a very detailed guide to understand the influence of the margin parameter $\\beta$ on the decay rate of the gradient. The study looks comprehensive and correct, which leads to the successful empirical verification.\n\nMost of the paper is well organized, although some part needs additional care."
            },
            "weaknesses": {
                "value": "Section 2 needs a revision. See more in questions section.\n\nAs far as I can tell, the classification error improvement is a bit marginal. The baseline accuracy should correspond to $\\beta = 1$ and the highlighted best obtained errors may not have significant improvement. Although this evaluation might be objective, but this concern can be partially addressed by providing a standard deviation computed in multiple runs, so that the statistical significance can be verified.\n\nGiven the concerns above, I am giving a negative rating. However, I am willing to discuss with the authors on the significance of the proposed method and potentially raise the score."
            },
            "questions": {
                "value": "The grammar around Equations (1) -- (6) should be polished.\n\nIn Equation (4), (5) and (6), is there a bracket around $z_i - z_c$?\n\nWhat is hidden in the approximate equality in Equation (5)?\n\nFigure 3 has a vague description: \"confidence of some samples during training\". The font in the figures is small."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1676/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1676/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1676/Reviewer_KFsP"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1676/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698807818555,
        "cdate": 1698807818555,
        "tmdate": 1700698313647,
        "mdate": 1700698313647,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yHqs9qkFAl",
        "forum": "tyIPw2m3Um",
        "replyto": "tyIPw2m3Um",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1676/Reviewer_cHJU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1676/Reviewer_cHJU"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a modification to softmax when using softmax in conjunction with cross-entropy for classification tasks to combat the following problem: when the magnitude of the partial derivative of the softmax with respect to the class outputs decays rapidly, model tends to overfit (but converges faster). On the other hand, if the magnitude of the partials decays too slowly, the model takes a longer time to converge (but tends to generalize better). The modification introduces a hyperparameter $\\beta$ where small $\\beta$ encourages rapid decay of the partials while large $\\beta$ encourages slow decay of the partials. To combine the best of both worlds, the paper proposes a warm-up scheme by starting with a small $\\beta$ so that the model will converge quickly and then increase $\\beta$ to discourage overfitting and overconfidence."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Paper proposes a simple modification to softmax in conjunction with a warm up scheme with respect to the margin parameter $\\beta$ to get faster convergence and better generalization."
            },
            "weaknesses": {
                "value": "The warm-up scheme does not seem to provide a significant advantage over prior proposed modifications to softmax (e.g A-softmax) or does worse according to table 2 in the paper."
            },
            "questions": {
                "value": "What does the training loss look like across epochs for the warm-up schedule (more specifically could you plot another curve in figure 5 displaying the loss over epochs for the warm-up schedule)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1676/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698823207145,
        "cdate": 1698823207145,
        "tmdate": 1699636095631,
        "mdate": 1699636095631,
        "license": "CC BY 4.0",
        "version": 2
    }
]