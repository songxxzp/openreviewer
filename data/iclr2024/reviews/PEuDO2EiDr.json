[
    {
        "id": "EbDNEhvCRP",
        "forum": "PEuDO2EiDr",
        "replyto": "PEuDO2EiDr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission891/Reviewer_A4oN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission891/Reviewer_A4oN"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a novel Recurrent Time-Frequency Separation Network architecture that performs audio-visual source separation tasks effectively and efficiently. The model is characterized by three parts. First, each modality goes through its own processing module, and then the cross-dimensional attention fusion (CAF) consolidates information from both modalities. The spectral source separation block performs masking-based separation. The separation results show promising improvement given the compact size and computational efficiency the new model architecture introduces."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The paper presents solid improvement compared to the existing baseline systems. Considering the amount of model compression the proposed model introduced, the improvement is significant.\n\n- All the procedures and modules are well-defined with enough details.\n\n- The choice of the model architectures makes sense, including the dual-path structure, attention-based consolidation, and complex masks.\n\n- Ablation studies are thorough."
            },
            "weaknesses": {
                "value": "While the paper is packed with useful information, there are still some parts that need elaboration.\n\n- As the authors mention, the dual-path RNN idea is not new to this problem. I understand that the authors chose SRU for their complexity-related considerations, but I also wonder if the audio processing module could benefit from its own self-attention mechanism, such as in the SepFormer model. \n\n- The spectral source separation module might be the weakest contribution, because complex masks have been extensively studied in the audio-only source separation literature. \n\n- I wish the paper provides more details on the TDANet block for video processing, which is relegated to the reference in the current version."
            },
            "questions": {
                "value": "- The authors chose to \"add\" f_1 and f_2 (eq 11) after the CAF processing. I think it's a little abrupt in the sense that there might be other choices that preserve the unique information that each vector learns, such as concatenation. Have the authors considered other ways to combine the two vectors?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission891/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698785372938,
        "cdate": 1698785372938,
        "tmdate": 1699636015633,
        "mdate": 1699636015633,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YiPLEJMydw",
        "forum": "PEuDO2EiDr",
        "replyto": "PEuDO2EiDr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission891/Reviewer_ZTD4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission891/Reviewer_ZTD4"
        ],
        "content": {
            "summary": {
                "value": "The authors build upon previous research in audio-only and audio-visual speech recognition by focusing on improving efficiency and fidelity of separated speech. They draw a lot of inspiration from the CTCNet paper and extend it to the TF domain to improve the efficiency. The solution has been evaluated on standard benchmark datasets and compared to previous state-of-the-art methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Audio samples of separation are available and source to be made available when the paper is published.\n2. The writing is easy to follow.\n3. Clear modeling details are provided."
            },
            "weaknesses": {
                "value": "1. The baseline methods listed in table 1 should include their references.\n2. RTFS-Net-12 is only about 10% more efficient that CTCNet. How much difference does that make in practical applications?\n3. Some of the comparison examples are not distinguishable to this reviewer. This makes me wonder how to interpret the relative SNR gains."
            },
            "questions": {
                "value": "1. Since the best performance is achieved with R=12, why not explore a higher R?\n2. Have the authors considered conducting studies with human listeners? If the target application is ASR, would it be helpful to measure WER in a recognition task?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission891/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698808187174,
        "cdate": 1698808187174,
        "tmdate": 1699636015554,
        "mdate": 1699636015554,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hNmXJUkMcS",
        "forum": "PEuDO2EiDr",
        "replyto": "PEuDO2EiDr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission891/Reviewer_siqV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission891/Reviewer_siqV"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes RTFS-Net, a new time-frequency (TF) domain audio-visual speech separation method. It introduces three main innovations:\n\n* RTFS Blocks independently model time and frequency dimensions of audio\n* Cross-dimensional Attention Fusion Block efficiently fuses audio and visual data\n* Spectral Source Separation Block preserves phase/amplitude information\n\nExperiments show RTFS-Net matches or beats prior time domain methods on LRS2, LRS3, and VoxCeleb2 datasets, while using 10x fewer parameters and 3-6x fewer computations.\n\nRTFS-Net is the first TF model to surpass most contemporary time domain methods for audio-visual speech separation. It demonstrates TF domain methods can achieve good performance at lower computational cost through novel modeling of time-frequency spectrograms."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* Achieves near state-of-the-art performance for audio-visual speech separation while being very parameter and computationally efficient\n* Outperforms all compared time domain methods, proving time-frequency domain modeling can achieve better performance if done effectively\n* Innovative modeling of time and frequency dimensions independently in RTFS Blocks\n* Attention-based fusion mechanism in CAF Block is very lightweight but fuses audio and visual data very effectively\n* Spectral Source Separation Block properly handles phase/amplitude to avoid losing audio information\n* Model code and weights will be released for full reproducibility"
            },
            "weaknesses": {
                "value": "* Testing is limited to only 2 speaker mixtures. Performance with more speakers is uncertain.\n* Missing PESQ evaluation in results which most other target speech extraction papers provide\n* Doesn't include latest SOTA model comparison: Dual-Path Cross-Modal Attention for Better Audio-Visual Speech Extraction, ICASSP 2023.\nhttps://arxiv.org/pdf/2207.04213.pdf. This gives superior performance for SI-SNRi and provides PESQ results as well. It does not provide MACs analysis."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission891/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission891/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission891/Reviewer_siqV"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission891/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698994464164,
        "cdate": 1698994464164,
        "tmdate": 1700434630030,
        "mdate": 1700434630030,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0rNdAm3Siu",
        "forum": "PEuDO2EiDr",
        "replyto": "PEuDO2EiDr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission891/Reviewer_1FXL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission891/Reviewer_1FXL"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors present a novel time-frequency domain audio-visual speech separation method (Recurrent Time-Frequency Separation Network), a unique attention-based fusion technique for the efficient integration of audio and visual information, and a new mask separation approach. Results show that the proposed approach outperforms the previous SOTA method using only 10% of the parameters and 18% of the MACs. The authors claim that this is the first time-frequency domain audio-visual speech separation method to outperform all contemporary time-domain SOTA ones."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "One main strength of this paper is the proposed Recurrent Time-Frequency Separation Network that processes the data in the frequency dimension, the time dimension, and the joint time-frequency dimension."
            },
            "weaknesses": {
                "value": "It is mentioned that the RTFS blocks share parameters (including the AP Block), leading to reduced model size and increased performance. Therefore, more description/explanation for this would be helpful."
            },
            "questions": {
                "value": "Are there any overlapping speech in the train/test data?\nDo the authors perform any downstream task like speech recognition on the reconstructed speech?\nOne possible downstream task for speech separation is speech-to-speech dubbing. In this case, both the speech and background/nonspeech sound are needed. Have the authors looked into reconstructing background/nonspeech sound?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission891/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission891/Reviewer_1FXL",
                    "ICLR.cc/2024/Conference/Submission891/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission891/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699482439551,
        "cdate": 1699482439551,
        "tmdate": 1700632278710,
        "mdate": 1700632278710,
        "license": "CC BY 4.0",
        "version": 2
    }
]