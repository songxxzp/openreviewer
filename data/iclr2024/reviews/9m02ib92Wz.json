[
    {
        "id": "kDKmk0ClVQ",
        "forum": "9m02ib92Wz",
        "replyto": "9m02ib92Wz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6868/Reviewer_kkF9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6868/Reviewer_kkF9"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors provide a new method to estimate influence functions for large-scale generative models. Their theoretical results show that their method is well suited for LoRA-based fine-tuning settings. their compare their method in terms of approximation error, mislabeled data detection, and influential data identification. They observe improvement over existing methods in influence approximation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ the idea if DataInf for approximating influence function is interesting.\n+ method is more efficient that existing work\n+ high-level of correlation between exact influence and approximated one shows the effectiveness of their approach."
            },
            "weaknesses": {
                "value": "+ Evaluation setup for mislabeled data detection is not well-defined. I need more details to understand this experiment.\n+ One of the main applications of influence functions is to find the most influential samples whose removal would significantly change model's behavior in inference time. The way this paper evaluates the most influential samples is not appropriate. In fact, we do expect influential samples to come from the same label but that is not enough. I would want to see if the most influential samples are actually influential in the generation process, e.g., model cannot generate that test image/text if those samples are removed in the fine-tuning process.\nA comparison of the effect of removing top $k$ detected influential samples with different methods on the quality of generation for a given test sample is needed."
            },
            "questions": {
                "value": "There is a recent line of work that casts doubt on the usefulness of approximating influence functions. They argue that for image generation tasks, utilizing an off-the-shelf visual encoder can be more helpful in identifying influential samples. I would expect to see a comparison between the proposed method and those kinds of methods [1, 2].\n\nIn fact, what is the benefit of this approach in detecting influential samples compared to those kind of methods if proposed method doesn't bring any improvement?\n\nWang, Sheng-Yu, et al. \"Evaluating Data Attribution for Text-to-Image Models.\" arXiv preprint arXiv:2306.09345 (2023).\nYang, Jiaxi, et al. \"Matching-based Data Valuation for Generative Model.\" arXiv preprint arXiv:2304.10701 (2023)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6868/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6868/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6868/Reviewer_kkF9"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6868/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698772107950,
        "cdate": 1698772107950,
        "tmdate": 1700609333109,
        "mdate": 1700609333109,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9y6N9xzN8X",
        "forum": "9m02ib92Wz",
        "replyto": "9m02ib92Wz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6868/Reviewer_oMqQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6868/Reviewer_oMqQ"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the author propose an efficient for the influence function. Different from LoRA, the author provide another efficient way to train the large language model."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "DataInf has superior efficiency in terms of computational and memory complexities compared to other methods such as LiSSA.\nThe method can be applied on some popular framework such as LoRA."
            },
            "weaknesses": {
                "value": "While DataInf is efficient, it uses an approximation that is not always equal to the exact computation. This could lead to significant errors in certain cases."
            },
            "questions": {
                "value": "Are there any specific model configurations where you would advise not using DataInf?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6868/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6868/Reviewer_oMqQ",
                    "ICLR.cc/2024/Conference/Submission6868/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6868/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698816424981,
        "cdate": 1698816424981,
        "tmdate": 1700674344402,
        "mdate": 1700674344402,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "nri1vJhreK",
        "forum": "9m02ib92Wz",
        "replyto": "9m02ib92Wz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6868/Reviewer_gXtX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6868/Reviewer_gXtX"
        ],
        "content": {
            "summary": {
                "value": "## Summary\n\n- The paper proposes a new method to calculate Influence Scores which is more computationally and memory efficient than existing techniques like (LiSSA and Hessian Free techniques like Tracin)\n- It is particularly suited for parameter-efficient fine-tuning techniques such as LoRA\n- The key approximation in DataInf is to swap the order of the matrix inversion and average calculation. They conduct experiments on approximation error analysis to study the effect of this approximation because these terms are not equal in the general case. Equation (4) in the paper.\n- To measure the efficacy of their proposed approximation, the authors conduct three sets of experiments: approximation error analysis, mislabeled data detection and influential data identification\n\t- The models used are RoBERTa model, stable-diffusion-v1.5 and LLaMA-2-13B model\n\t- DataInf is significantly more correlated with with exact influence values than other methods (LiSSA and Hessian free methods like Tracin) for all ranks. Correlation decreased with increasing rank which is why DataInf is specially suitable for LoRA models.\n\t- DataInf is better at identifying mislabeled examples\n\t- They also use DataInf to identify influential training examples for LLaMA-2-13B-chat model for text generation task and stable-diffusion-v1.5 model for text to image generation task.\n\t\t- They construct 3 datasets for text generation: (i) Sentence transformation (ii) Math word problems without reasoning (iii) Math word problems with reasoning\n\t\t- For text to image generation they construct two tasks (i) style generation (ii) subject generation\n\t\t- As metric, they report the percentage of training points with the same class as the test example among the top s influential training points. DataInf has significantly better recall and AUC scores than Hessian-free approach"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The result in Figure (2) is very interesting. Sometimes DataInf is even better than the exact method !"
            },
            "weaknesses": {
                "value": "- Do LoRA finetuning methods typically only use low ranks like 2,4,6 as used in the experiments in the paper?\n\n- The paper has rightly pointed out that there aren't many qualitative metrics for measuring the utility of influence scores. The authors try to address it through proxies. The results in Figure 2 are presented with only rank 4, what do the results look like for other rank values?"
            },
            "questions": {
                "value": "- Curious as to what is the computational and memory complexity of Hessian free methods like Tracin which are omitted from the table?\n- How did you obtain multiple training checkpoints required for techniques like tracin in your experiments? Esp for the llama model, are the checkpoints publicly released or did you do your own fine-tuning and use the fine-tuning checkpoints? Do the results for tracin improve with more checkpoints?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6868/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698855343357,
        "cdate": 1698855343357,
        "tmdate": 1699636797455,
        "mdate": 1699636797455,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OnBbXi2Wag",
        "forum": "9m02ib92Wz",
        "replyto": "9m02ib92Wz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6868/Reviewer_NdTy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6868/Reviewer_NdTy"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an efficient influence approximation method. The proposed method outperforms existing influence computation algorithms in terms of computational and memory efficiency."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper tackles an important and timely problem of estimating data influence in a scalable manner."
            },
            "weaknesses": {
                "value": "- One assumption made by the paper is the first equation on page 3, which states that the expectation of hessian is equal to the expectation of the second moment of gradients. This assumption only holds true when the loss function is $-\\log P(y|f_\\theta(x))$, where $f_\\theta(x)$ denotes the output probability of network parameterized by $\\theta$. However, consider the cross entropy loss function, which is $y \\log f_\\theta(x)$. There is no discussion of how the assumption applies to common loss functions. \n\n- Theorem 1 seems a loose bound. It will be much more convincing to empirically verify the introduced by the approximation and whether the error grows in $d^2$."
            },
            "questions": {
                "value": "- Section 4.1: What is the correlation analysis result when the data points are all clean?\n\n- Section 4.2: Why does exact influence sometimes underperform the approximation methods? How does the result vary with the mislabeling ratio?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6868/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698888296107,
        "cdate": 1698888296107,
        "tmdate": 1699636797350,
        "mdate": 1699636797350,
        "license": "CC BY 4.0",
        "version": 2
    }
]