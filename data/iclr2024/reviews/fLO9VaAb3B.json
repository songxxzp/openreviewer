[
    {
        "id": "jnEXnaWfWT",
        "forum": "fLO9VaAb3B",
        "replyto": "fLO9VaAb3B",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4470/Reviewer_1bFk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4470/Reviewer_1bFk"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces TS-LLM, a tree-search learning framework tailored for Large Language Models (LLMs), drawing inspiration from AlphaZero. The framework sets itself apart from prior works by providing a unique method to enhance the performance and capabilities of LLMs in reasoning tasks and RLHF alignment."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- TS-LLM differs significantly from methods like Tree of Thought (TOT) and Reasoning via Planning (RAP), focusing less on reasoning tasks and not relying on large-scale models or human-designed prompts.\n- The framework is adaptable for various tasks and LLM sizes.\n- TS-LLM includes a learned value function and a final-step outcome reward model (ORM), enhancing performance in both decoding during inference and training."
            },
            "weaknesses": {
                "value": "Despite its merits, the paper does not demonstrate a significant improvement over previous works. Major concerns are:\n\n1. The performance improvement is unclear and somewhat unconvincing.\n   - **GSM8k:** The results are inferior compared to the previous work (ToT-BFS).\n   - **Game24:** Despite MCTS-$\\alpha$ winning, it requires **nearly twice** the number of tokens compared to other approaches.\n   - **PrOntoQA:** Similarly, it requires **nearly twice** the number of tokens compared to other approaches.\n   - **RLHF:** (See the next point)\n\n2. Misapplication of RLHF Task: This paper's approach to the RLHF task seems misaligned with its intended purpose. Instead of tackling the fundamental challenges of RLHF, the study opts for a smaller agent (125M) and compensates with a higher token count to achieve elevated rewards through an open-source Reward Model. While this method may yield higher scores during training and potentially quicker convergence due to reduced sampling, the extended time required for \"search\" processes negates these benefits. Ultimately, this approach fails to address the crucial issues inherent to RLHF."
            },
            "questions": {
                "value": "1. In Table 4, why do DFS/BFS/MCTS yield identical results for the RLHF task? This paper should clarify this.\n\n2. In Figure 1, the presentation should be further clarified. The authors did not evaluate Game24 on a token-level, however the figure may mislead readers.\n\n3. Is the term \"$k$-node\" used in the context of BFS equivalent in meaning to \"$k$-node\" in the context of MCTS? \n\n4. Do BFS and DFS utilize the same Large Language Model (LLM) as MCTS-$\\alpha$ and MCTS-Rollout, or do they employ GPT-4 as their original Tree of Thought (ToT) configuration?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Not applicable"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4470/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698593918659,
        "cdate": 1698593918659,
        "tmdate": 1699636422729,
        "mdate": 1699636422729,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "L5cjNrZoZt",
        "forum": "fLO9VaAb3B",
        "replyto": "fLO9VaAb3B",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4470/Reviewer_xk9Q"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4470/Reviewer_xk9Q"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method to use tree search algorithms to improve the decoding capability of LLMs. There are a couple of existing methods that have been applying tree search to improve the reasoning and planning capability of LLMs. For instance, Tree-of-thought (ToT) uses depth/breadth-first search to improve the planning capability of LLMs. However, the paper points out there are two major drawbacks in the prior work. First, the current methods mainly focus on enhancing LLM's reasoning ability. It is unclear if these methods can be applied to different kinds of tasks such as RLHF. Second, the existing methods heavily rely on hand-engineering prompts. As a result, such algorithms lack their applicability and heavily rely on both well-designed prompts and large-scale LM. As a result, the paper proposes a method that is based on MCTS. The main algorithm consists of two major steps: (1) policy improvement: it first generates tree-search trajectories based on the LLM, the value function that predicts the returns, and the reward model that predicts the end return; and (2) policy evaluation: based on the generated dataset from policy improvement, it trains the value function and reward model to further improve the performance. Several experiments are shown in section 4. Table 2 shows the mixed signal of whether the proposed method, MCTS, is better than the existing approach or not. Table 3 shows an ablation study of the MCTS model in Game24 tasks with a different allowable computation budget. Finally, the paper concludes by saying that it proposes a new training paradigm."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The idea of adding a value function and a reward function to further improve the decoding capability of LLM is a natural extension of the direction of applying a traditional planning algorithm to LLMs.\n2. The experiments are very extensive, and the writing about the proposed method is easy to understand."
            },
            "weaknesses": {
                "value": "1. Based on the result in Table 2, it is unclear if the proposed method is better than the existing method such as BFS (ToT). For instance, in GSM8K, the BFS is better than the proposed methods such as MCTS-\\alpha and MCTS-Rollout. We spent a good amount of time getting value functions and the reward model working, but the performance of the proposed method is still worse than the simple baseline. As a result, I am not convinced that this approach will work better or not.\n2. Second, it seems that most of the experiments are about improving the value function and reward function, not about fine-turning the LLM. For example, if we do fine-tuning Table 2, are we able to improve the performance? Essentially, I want to see more results based on the question 5 written in the paper: Can TS-LLM further train LLM? I am happy to increase the score if the answers are answered."
            },
            "questions": {
                "value": "See the weakness section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4470/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698789473787,
        "cdate": 1698789473787,
        "tmdate": 1699636422632,
        "mdate": 1699636422632,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "i3HyttkgBa",
        "forum": "fLO9VaAb3B",
        "replyto": "fLO9VaAb3B",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4470/Reviewer_E6Fb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4470/Reviewer_E6Fb"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a set of MCTS-based algorithms for language generation using LLM that can replace beam search or further fine-tune the LLM. The general idea is to train a value/reward model based on ground truth correctness for reasoning task or rewards in datasets used for RLHF. By unifying the notion of desire paths in a flexible model, the implicit policy of vanilla LLM decoding can be combined with exploration or valuations of incomplete states, so heuristic-search algorithms like MCTS become feasible.\n\nThe paper considers token and sentence action spaces. The resulting set of decoding can be aggregated in multiple ways.\n\nThe experiments use zero-shot Chain-of-Thought (CoT) via fine-tuning as a baseline. The datasets include some common reasoning ones, a new one in reasoning and another one on RLHF. \n\nFor a more fair comparison, the results aim to compare algorithms under similar amounts of tokens generated, allowing COT to generate further paths for posterior aggregation.\n\nUnder the specific hyper-parameters used, the empirical results show MCTS as a promising direction."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Separation of concerns between the LLM as a selection of action/tokens vs the value of a state that can integrate information about past sequences and possible continuations.\n- Multiple algorithms cover most of the meaningful combinations.\n- Attempt to fair comparison on the effort to the number of tokens\n- Diverse set of datasets\n- Systematic combination of algorithms and settings\n- Some discussion on the challenges of the implementation"
            },
            "weaknesses": {
                "value": "- No details on the value/reward model besides the mention that it is a decoder-only model.\n- Explanation of the algorithms doesn\u2019t clarify when the value/reward models are used.\n\t- Some details are buried in the appendix but need to be clearer.\n\t- In particular, knowing how the evaluation function propagates information for training and during search is critical.\n- Some settings need to be fully explained and unclear what version was used.\n\t- For instance, the appendix says that sometimes intra or inter-tree search is used, but it\u2019s not clear which experiment uses which setting.\n\t- Part of that can be implied from the algorithm descriptions, but that\u2019s not the reader\u2019s job.\n- No discussion on how the hyper-parameters were set.\n\t- This is especially critical for MCTS, as it might be very sensitive to them.\n\t- Many other details, such as the 0.3 in the Diriletch distribution for adding noise, are important.\n- No specific statistics about the behaviour of the algorithms beyond the aggregated number of tokens.\n\t- For instance, knowing the number of roll-outs would be useful.\n- No discussion of the wall-time for decoding\n\t- While the paper discussed implementations, it\u2019s not clear what the actual performance might be. While the implementation can be challenging and improved if this method becomes more popular, we need to know about the current cost of solving the tasks.\n\t- For instance, keeping the tree in GPU memory can be expensive. Moving tensors in and out could reduce GPU utilization.\n- Analysis can be improved\n\t- For instance: *Highlight [page 8]:* The results of CoT-SC ORM-vote@10 underscore the diversity of sampled data in learning a better ORM.\n\t- But this is not affecting the MCTS ones, so the claim about the diversity is only for the ORM.\n\n\nGiven the lack of details on how the hyper-parameters were set, and how sensitive the results are to them, it\u2019s hard to qualify the contribution, and how likely these algorithms are to perform well in other domains. This perception can change depending on the answer of the authors.\n\nOther issues:\n- The notion of aggregation is misleading (Sect 3.2.3)\n\t- Aggregation refers to using a set of things to get something new, but in this case, we are just selecting an output, not combining them.\n- The most relevant work is in the appendix:  *Highlight [page 13]:* The most relevant work, CoRe (Zhu et al., 2022), proposes to finetune both the reasoning step generator and learned verifier for solving math word problems using MCTS for reasoning decoding which is the most relevant work to ours.\n\t- Please fix the reference to ACL 2023.\n- Make sure all references are used in the paper.\n\t- For instance, Self-consistency (4.1) doesn\u2019t cite the reference already in the paper.\n\nSuggestion / minor issues\n- Given that MCTS combines both the policy and the value, perhaps a comparison with BFS or DFS is not the more meaningful. Algorithms based on A* rely on the accumulated cost g \u2014in this case inverse to the likelihood of the prefix\u2014 and h, an heuristic estimating the value of the rest of path. While the value function in the paper does not take into account the length, it\u2019s still possible to consider their combination.\n- Please define the state $s$ by itself, not inside $g$:\n\t- *Highlight [page 4]:* given state s and action a, is known as: g ( s = (x 0:L\u22121, y 0:t\u22121), a = y t) = (x 0:L\u22121, y 0:t).\n- Why defining $g$ in page 4? Section 3.1 uses \\phi_theta\n- *Highlight [page 6]:* This helps us juxtapose the performance\n\t- Juxtapose? Perhaps it\u2019s a meaningful comparison\n- *Highlight [page 7]:* For value and ORM training, the data are generated by randomly sampling the SFT policy\u2019s rollouts on the training set\n\t- Please define SFT. I guess it\u2019s supervised fine-tuning. \n- Section \u201cBackground of Monte Carlo Tree-search Algorihtms\u201d should say which algorithm is closer to alpha-zero."
            },
            "questions": {
                "value": "- Is there any restriction about the context size of the LLM and the value/reward models?\n- Was any fine-tuning or training using multiple-GPUS or the A800 were used to run independent experiments?\n- Value and reward models\n\t- Is this a single decoder-only model that produces the intermediary values and the final rewards, or are they different models?\n\t- If they are different models, can you report on the difference between v^_theta() vs r^_theta() when evaluated in final states?\n- hyper-parameters:\n\t- How were the hyper-parameters setup, from Sect 4.1, task setup until the constants for MCTS.\n\t- How much budget was used to set these numbers?\n\t- How sensitive are the results to the particular parameters?\n- In general, how hard should it be to adapt the proposed methods to another domain?\n- Is BFS taking into account the likelihood of the previous path? The text only says to keep the ones with maximal value.\n\t- (Suggestion: see comment about A* above)\n\t- If it completely ignores the likelihood, then why does the decoding do something useful?\n- What are the computational requirements for only training the value/reward models without fine-tuning?\n- What is the distribution of wall-time per algorithm and dataset?\n\t- Section \u201cLimitation and future work\u201d discuss how limited is the implementation but it\u2019d still be informative to know what\u2019s the current status\n- Are there any meaningful combinations that were not tested due to complexity?\n\t- Section D includes details but it\u2019s not clear if all the challenges were overcome or some meaningful experiments might be missing.\n\t- This answer should be included in the discussion, or at least in appendix B. \n- Were the samples in the dataset always selected from the train set?\n- Is Path@N counting the number of paths up to a leave node or the number of separated paths? \n\t- For instance, for beam-search with size K, the number of leaves is at most K, but there were other paths partially explored that were discarded along the way.\n- Path@N for CoT-SC variants\n\t- *Highlight [page 7]:* For CoTSC variants, we present Path@N, and N is determined by maintaining the same level of token computation as TS-LLM\u2019s variants\n\t- How is the same level of token computation maintained? For instance, it could be the maximum average of the tokens used for all the variations of MCTS. Or perhaps, it was run with multiple values of N, and then return bigger N under the token of the other method. But I\u2019d expect that to be done per sample."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4470/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699202410927,
        "cdate": 1699202410927,
        "tmdate": 1699636422557,
        "mdate": 1699636422557,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HOPWWAnSev",
        "forum": "fLO9VaAb3B",
        "replyto": "fLO9VaAb3B",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4470/Reviewer_Zruf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4470/Reviewer_Zruf"
        ],
        "content": {
            "summary": {
                "value": "The work describes a new framework called TS-LLM that enhances the reasoning abilities of large language models (LLMs) during both inference and training. Traditional approaches like Chain-of-Thought and Tree-of-Thought rely on human-designed prompts and methods like beam search or sampling, but they are limited in scope and scalability. TS-LLM overcomes these limitations by incorporating a tree-search algorithm guided by a learned value function, similar to AlphaZero. This framework is versatile, applicable to various tasks, and can work with LLMs of any size without the need for complex prompting strategies. Its effectiveness is demonstrated through empirical evaluations in different areas, including reasoning, planning, and RLHF alignment, showing that it can handle complex tree searches up to 64 steps deep."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The experiment results are impressive.\n- The framework can work on different kinds of tasks. \n- The method makes sense."
            },
            "weaknesses": {
                "value": "- After ToT, using MCTS to search is not too novel. This paper uses MCTS directly.  However, I think it can still be accepted if this is the first paper that successfully uses MCTS.\n- This paper needs to have more experiments to show the improvement. For example, more models and more tasks. For more tasks, maybe you can find some here \u201chttps://github.com/Ber666/llm-reasoners\u201d. \n- The paper should discuss which kinds of tasks are better for using MCTS rollout and which are better for MCTS alpha."
            },
            "questions": {
                "value": "Have you tried using both rollout and value together? If the speed of rollout and the speed of the value network are very different, you can reference the paper \u201cMultiple Policy Value Monte Carlo Tree Search.\u201d"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4470/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699591470464,
        "cdate": 1699591470464,
        "tmdate": 1699636422470,
        "mdate": 1699636422470,
        "license": "CC BY 4.0",
        "version": 2
    }
]