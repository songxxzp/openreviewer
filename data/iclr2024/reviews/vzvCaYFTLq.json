[
    {
        "id": "CvBDliZdW4",
        "forum": "vzvCaYFTLq",
        "replyto": "vzvCaYFTLq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2085/Reviewer_vqsi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2085/Reviewer_vqsi"
        ],
        "content": {
            "summary": {
                "value": "The proposed method Sapling aims to retain LLMs\u2019 capacity in a specific knowledge domain and achieve inference speedup by reducing the model depth. It's based on knowledge localization phenomenon achieving model compression via successive layer dropping. The authors show > 2x memory saving and inference speedups through empirical results."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Originality:\n- Layer Dropping Strategy: The paper introduces a strategy to selectively drop layers from pre-trained models based on their significance, which is a creative combination of existing ideas in model compression and adaptation.\n- Combination of Adaptation and Compression: The approach of adaptively finding the layers to filter out is an insightful finding following backed by knowledge localization insights.\n\nQuality:\n- The paper quality is good and well written, experiments on variety of QA benchmarks show the efficiency wins for the proposed method.\n\nClarity:\n- The paper is well-structured and articulately written, ensuring clarity. The proposed method is simple and intuitive. \n- Understanding and insights of the model learning process during fine tuning adds clarity to why the method is performing well."
            },
            "weaknesses": {
                "value": "Limited Novelty\n- The proposed method seems like a synthesis of existing methods. Methods like layer dropping, knowledge localization are already existing. While the combination of these is creative but the paper lacks technical contribution that is truly novel. \n\nMinimal Theoretical underpinning\n- The paper introduces successive adaptation and layer dropping as key components of SAPLING, but there is a scarcity of theoretical rationale justifying these design choices. A stronger theoretical foundation explaining why these specific techniques were chosen, and how they synergistically contribute to the overall goal, would add significant weight to the paper\u2019s contributions."
            },
            "questions": {
                "value": "Quantitative Analysis: While the paper acknowledges the trade-off between model size and performance, a more detailed quantitative analysis of this trade-off would be beneficial. Specifically, understanding the diminishing returns or inflection points where further compression significantly hampers performance would provide valuable information for practitioners.\n\nProduction Real work deployment scenarios: The paper's primary focus is on optimizing LLMs for resource-constrained environments, but it lacks a thorough discussion on real-world deployment scenarios, challenges, and potential solutions. Providing practical insights and guidelines for deploying compressed models in various settings would add value to the paper. Specifically, some discussion around the robustness of the dropped layers depending on the fine tuning domain? How does the model perform on similar domain that it is not fine tuned on? These are cases that can come up in real work scenarios"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2085/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2085/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2085/Reviewer_vqsi"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2085/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698647469143,
        "cdate": 1698647469143,
        "tmdate": 1699636141000,
        "mdate": 1699636141000,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9OiCkpkCvn",
        "forum": "vzvCaYFTLq",
        "replyto": "vzvCaYFTLq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2085/Reviewer_5U84"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2085/Reviewer_5U84"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a novel method to reduce the model depth by exploiting knowledge localization in GPT style models and dropping layers that don\u2019t impact task accuracy during fine-tuning. Sapling framework for model compression introduced by this paper using calibration dataset to identify and prune the layers while fine-tuning to achieve ~50% compression.\nThe efficacy of this algorithm is demonstrated by evaluating LLaMA-7B model over wide range of benchmarks and comparing against baseline such as LLM.int8, GPTQ, AWQ."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well-written and and effectively motivates and extends the prior literature work on knowledge localization for finding and dropping layers that are not relevant for task accuracy.\n2. The paper includes exhaustive experiments covering various datasets used for calibration and compares against baseline methods such as full model, full fine-tuning and sparse fine-tuning. To compare memory consumption, baseline methods such as llm.int8(), GPTQ and AWQ are used.\n3. Exhaustive ablation studies are performed to validate the model performance on tasks different than what was used for calibration. Also, the layer dropping pattern is studied across different tasks to highlight the fact that localized knowledge pattern is effectively used for dropping layers."
            },
            "weaknesses": {
                "value": "1. All experiments are performed only on LLama7B model which might have caused the technique to be overfit to LLaMA 7B model.\n2. Computation cost of fine-tuning per dropped layer seems very high specifically for LLMs."
            },
            "questions": {
                "value": "1. Perform experiments on another set of architecture (can be higher #param for LLaMA or models such as MPT-7B etc).\n2. Include a section in the results with compute time comparison across different baseline techniques."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2085/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698816905389,
        "cdate": 1698816905389,
        "tmdate": 1699636140910,
        "mdate": 1699636140910,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UMOM3jgeKX",
        "forum": "vzvCaYFTLq",
        "replyto": "vzvCaYFTLq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2085/Reviewer_hKv9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2085/Reviewer_hKv9"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to compress the size of LLMs while domain specializing them by dropping layers that are less relevant to input sequences relevant to the given domain.  The paper draws inspiration from recent work showing knowledge in LLMs is localized and is quite orthogonal to much of the existing work on model compression."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Timely approach to model compression drawing on recent insights into how LLMs work.\n\nProposes a LLM model compression approach that does not require specialized hardware support."
            },
            "weaknesses": {
                "value": "The approach requires multiple iterations of training on the downstream task and the overheads of this step are not quantified in the paper.\n\nLack of quantitative comparison versus the layer dropping approach in Sajjad et al. 2023.\n\nNo supplemental material (code or extra experiments, etc).\n\nWriting issues such as font size in graphs being too small and some typos (\"despite with far\", \"adpating\", \"th one\", \n\nDownstream task accuracy drops with increased compression (e.g., Figure 1 and 2).  While keeping within 10% accuracy at 50% reduced overhead is impressive the accuracy drop may be too much for some use cases."
            },
            "questions": {
                "value": "Maybe re-write Equation 1 to say $y_{i+1} = ...  y_i ... $ because \"At i = 0, the input has $y_{i\u22121} = y_0$\" does not make much sense (unless I'm missing something here, at i=0, $y_{i-1}$ should be $y_{-1}$).\n\nHow does Algorithm 1 with Equation 3 or 4 compare with the null hypothesis of randomly picking a layer to drop at Line 13?    How does Algorithm 1 compare quantitatively with the layer dropping proposed by Sajjad et al. 2023?\n\nThe paper mentions fine-tuning complexity grows as O(N) where N is the number of layers to drop, but it is unclear whether this overhead is substantial or not.  I understand from Table 1 there is no impact on inference time, but reducing fine-tuning time is of interest.  What is the wall clock time it takes to run Algorithm 1?\n\nRegarding the scenario spelled out on Page 5 \"situations characterized by labor shortages\".  The current phrasing makes it sound like AI is already used in medical/financial situations.  Are there references you can provide to clarify what is referred to?  Is this passage providing speculation about a future scenario?   \n\nTable 2 caption suggests Equation 3 was used at Line 13 in Algorithm 1.  Table 3 does not say what the sampling method is.  How do results compare when using Equation 4 at Line 13 in Algorithm 1?\n\nWill code for Sapling be made public?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2085/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2085/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2085/Reviewer_hKv9"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2085/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699226895572,
        "cdate": 1699226895572,
        "tmdate": 1699636140840,
        "mdate": 1699636140840,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hv9z0SIZ0T",
        "forum": "vzvCaYFTLq",
        "replyto": "vzvCaYFTLq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2085/Reviewer_JG4e"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2085/Reviewer_JG4e"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose an efficient inference framework for LLMs based on layer dropping, called Sapling, that can achieve\ninference speedup on any hardware and deep learning systems by reducing the model depth. The authors claim that the proposed layer-dropping technique is based on the knowledge localization phenomenon they empirically observed and verified on LLMs. Evaluation results show that tuning with Sapling on LLaMA-7B leads to reliable performance, comparable memory saving, 1.2 to 8.5\u00d7 inference speedup on consumer-level hardware compared to state-of-the-art quantization algorithms,"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Designing techniques to improve LLMs' inference efficiency on commercial devices is an important aspect. This work has done a preliminary exploration of this direction. \n- The proposed method is intuitive and easy to understand."
            },
            "weaknesses": {
                "value": "- Although the authors claim that the proposed method is based on the knowledge localization phenomenon, I didn't find effective support for their claim on the knowledge localization phenomenon. \n- The evaluation is not convincing enough. I would expect a more comprehensive evaluation of the proposed method to prove its effectiveness across different settings. \n>- The method is evaluated only on a relatively small-scale LLaMA-7B model, it would be better to evaluate the proposed method on larger-scale LLMs which could have more challenges on their inefficiency issue. \n>- Other than quantization and unstructured pruning methods benchmarked in the paper, structured pruning (e.g., [1]) is also a series of methods that can achieve speed up on commercial devices. The authors should benchmark these methods to prove their effectiveness. \n>- Although the authors mentioned the potential inference speed improvement, I didn't find results on the latency reduction in the experiment section. Adding this would better help the reader to understand the performance of the proposed method. \n>- Currently, only V100 is considered as the target device. However, newer generations of GPUs are rapidly developing and providing more effective support for lower-bit inference and memory consumption (e.g., H100/A100). \n\n[1] Ma, Xinyin, Gongfan Fang, and Xinchao Wang. \"LLM-Pruner: On the Structural Pruning of Large Language Models.\" arXiv preprint arXiv:2305.11627 (2023)."
            },
            "questions": {
                "value": "Please refer to the weakness section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2085/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699461214381,
        "cdate": 1699461214381,
        "tmdate": 1699636140744,
        "mdate": 1699636140744,
        "license": "CC BY 4.0",
        "version": 2
    }
]