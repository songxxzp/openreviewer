[
    {
        "id": "WrG3Vwyror",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7961/Reviewer_u563"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7961/Reviewer_u563"
        ],
        "forum": "zLwCT9srfo",
        "replyto": "zLwCT9srfo",
        "content": {
            "summary": {
                "value": "The authors proposed a hierachical (or devide and conquer) approach to make re-compute/materialize decision by:\n  1. partition the graph into subgraphs hierachically\n  2. apply a modified ILP solver (H-ILP) on solved subraphs recursively\n  3. additionally, the base solver for the leaf subgraphs of the (hierachical) tree is modularized to swap between different algos\n\nto address the issue of existing approaches which (are either ILP based thus) don't scale to large graph or fail to optimize more general graph with long skip connections like Unet."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ The hierachical + ILP solution proposed by the authors is intuitive and practical in the sense that:\n  1. the search space of ILP based approach is too large to scale to graph with thousands of computational ops/nodes, a good graph partition can trim down the search space efficiently\n  2. non-ILP based approach has a hard time dealing with networks with long skip connections like UNET or ENCODER/DECODER architecutre.\n\n+ the base solver for the bottom/leaf subgraph is modularized, thus can swap to different algos as a graph/solver runtime tradeoff\n\n+ solid explanation/comparisons to related works/baselines, and the robustness to hierachy depth (figure 4) is a good indicator of its scalability to general/deep networks"
            },
            "weaknesses": {
                "value": "- The parition algorithms (especially the score function/cost model in equation 1) is a bit ad-hoc, I can grasp the intuition behind it, e.g., it tries to identify a subgraph with least IO and penalize on number of nodes in it so that it can minimize the memory required to checkpoint its IO while keeping the scale of each subgraph relatively small. However, graph partition is a long-studied problem and usually such a heuristic/greedy based algo don't scale very well in the sense that they are typically tailored for specific known targets and would fail overtime when target envolves, that being said, I would suggest:\n  1. try the partition algos on a densenet to see if it produces good result\n  2. alternatively make this partition algo also modularized as the solvers, what's more valuable/solid in this work is the intuitionn of hierachy (devide and conquer) and the H-ILP solver IMO\n\n- the presentation can be improved:\n  1. the 3.1 H-partition part contains a lot implementation details without much explanation where they come from, e.g., as is briefly mentioned above regarding equation (1), and additionally why do you need alpha and why is it 0.5, why did you choose 4 candidte groups in \"Formation of candidate groups\" rather than other numbers.\n   2. On the other hand, the caption of the most important figure 1 doesn't have enough details, what's the time vs memory plot? (I think they refer to options), what's direct solver (I only got base solvers for bottom subgraph and H-ILP hierachical solver), etc.\n   3. How does H-ROCKMATE beat the baselines in Unet/Encoder-Deccoders more concretely? An explanation or preferrably an illustrative exmaple would help readers understand the quality of it more intuitively."
            },
            "questions": {
                "value": "In addition to the questions in Weakness, here are a couple more questions:\n\n1. what does \"the higher level algorithm adapts the sub schedules\" conrectely mean in Correction terms for memory usage?\n\n2. would \"model = HRockmate(model, sample, memory_budget)\" work with Tensor Parallel packages like Megatron? as my guess of the implementation relies on model/graph tracing and Megatron can pose difficulties in such tracing due to collective communications."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7961/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7961/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7961/Reviewer_u563"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7961/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697419993497,
        "cdate": 1697419993497,
        "tmdate": 1699636978808,
        "mdate": 1699636978808,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QhLmWKlqB8",
        "forum": "zLwCT9srfo",
        "replyto": "zLwCT9srfo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7961/Reviewer_piuX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7961/Reviewer_piuX"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces H-Rockmate, which is a hierarchical approach to find a re-materialization strategy for large neural networks. It decomposes a dataflow graph into multi-level and find efficient solutions of each blocks in bottom level. A related ILP formulation is proposed to recombine low-level solutions. H-Rockmate can find similar performance as ROCKMATE in less time, making it more practical."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tH-Rockmate proposes a hierarchical decomposition method for the computation graph. Thus the size of the ILP problem is smaller. Experiments show that efficiency and performance haven\u2019t been compromised.\n2.\tOther re-materialization strategies can be integrated into their frameworks to achieve better performance.\n3.\tTheoretical analysis of their algorithm and ILP formulation is provided in the Appendix."
            },
            "weaknesses": {
                "value": "1.\tSince they claim H-Rockmate works for large neural networks, the sizes of neural networks used in experiments are the same as other works.\n2.\tThere are some typos in the Appendix, such as \u201cline ??\u201d."
            },
            "questions": {
                "value": "1.\tIf H-Rockmate is applied to a billion-level neural networks like LLaMA, how will the peak memory and iteration time be? I think experiments with larger neural networks than GPT2 is necessary.\n2.\tCan you introduce what constraints are considered in your main part of the paper and introduce detailed expressions in Appendix\uff1f\n3.\tWhat if modeling on-chip global memory to get a better scheduling? Can your method support this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7961/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698748954661,
        "cdate": 1698748954661,
        "tmdate": 1699636978684,
        "mdate": 1699636978684,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gkomvJr8aJ",
        "forum": "zLwCT9srfo",
        "replyto": "zLwCT9srfo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7961/Reviewer_J4PJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7961/Reviewer_J4PJ"
        ],
        "content": {
            "summary": {
                "value": "This paper tries to solve the problem of efficient scheduling of re-materialization of the training computation. Concretely, the paper proposes H-rockmate, a hierarchical solution to decompose the data-flow graph into a hierarchy of small-scale subgraphs and compute a re-materialization schedule that minimizes the computational overhead within a given memory budget. Empirical studies are conducted to evaluate the performance of the proposed method in terms of solver efficiency and end-to-end performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The summarization of this research area is clear and accurate; the related work section is well-organized.\n\n- The intuition behind the scheduling algorithm design is clear and straightforward.   \n\n- Based on the reported experimental results, the reduction of the solver execution time is significant."
            },
            "weaknesses": {
                "value": "- The writing of the paper can be significantly improved. First of all, there is a lack of a formal definition for the scheduling problem itself -- the current introduction of the problem is interweaved with the problem statement in Section 3.1. Additionally, section 3.2 is too casual; there is a lack of enough formalization about the mathematical representation of the problem -- I notice plenty of important information is left in the appendix. I do not think this is an appropriate trade-off; the technique content should be self-explained within the scope of the paper.   \n\n- I am a little confused by the presented results in Figure 3; I was expecting that when the budget is very low, every algorithm should be able to find the scheduling of re-computing every activation, while when the budget is very high, every algorithm should be able to find the scheduling of no-recomputation, but still there is some difference between each line. This was confusing. Additionally, the important hyper-parameters, such as batch size, are not enumerated in Section 4. \n\n- Another trivial detail is that the font style differs from other submissions I reviewed; please check the instructions to ensure you are using the requested font style."
            },
            "questions": {
                "value": "See my comments in the Weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7961/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699177659177,
        "cdate": 1699177659177,
        "tmdate": 1699636978582,
        "mdate": 1699636978582,
        "license": "CC BY 4.0",
        "version": 2
    }
]