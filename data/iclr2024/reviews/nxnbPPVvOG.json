[
    {
        "id": "Na49L87kmP",
        "forum": "nxnbPPVvOG",
        "replyto": "nxnbPPVvOG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3582/Reviewer_v15j"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3582/Reviewer_v15j"
        ],
        "content": {
            "summary": {
                "value": "This paper considers the trade-off between the variance (MSE) and bias (bias norm) for the regularized linear regression model, where the authors consider various Schatten norms for the regularized term. Then, the authors derive the expected MSE for the Gaussian and diagonal ensembles and compare it with the ridge regression model's one and show that the ridge regression model is not always the best in term of the trade-off by using some experiments on sythetic datasets.  There exit some similar works such as Bayatti and Montanari (2011), Samet et al. (2013) for Lasso, however the authors limit their work to the class of Schatten norms."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+  The authors can obtain an exact expression for the average test error (MSE) for the spherical Gaussian model, and this bound is nearly matched to the experiment results (cf. Figure 2).\n+ Experiments show that Ridge regression, which used Frobenius norm, is not always the best option for the linear model in term of the trade-off between the variance (MSE) and the bias (the bias norm). More specifically, the authors show that the Frobenius norm and the nuclear one are likely to have the same average MSE on the spherical Gaussian ensemble (model) or the diagonal matrix model, but using the nuclear norm usually achieves better MSE than the Frobenius norm. This fact also holds when mapping the dataset via a random fourier transform (RFF)."
            },
            "weaknesses": {
                "value": "+ The theoretical results only hold in the thermodynamic limit $N\\rightarrow \\infty, d \\rightarrow \\infty$ and $d/N\\rightarrow \\lambda$. This means that the results only hold when the number of observations is linear to the signal dimension. However, in common high-dimensional settings, the number of observations is usually sub-linear to the signal dimension. \n+ The result looks not an extension of the Gauss-Markov theorem since it only holds under expectation over $X$ when $X$ is an Gaussian ensemble or a diagonal ensemble. The Gauss-Markov theorem works for any $X$. \n+ It looks more interesting to compare your experiment results with other norms (outside the class of  Schatten norms) such as between the nuclear norm and Lasso (norm-$1$). \n+ Too many typos. Please check and correct them."
            },
            "questions": {
                "value": "How do your results in comparison with other norms which don't belong to Schatten class of norms such as Lasso (norm-$1$)?\n\nBesides, please check and correct the following typos and unprecise.\n\n+ $L \\in \\mathbb{R}^{k \\times N} \\rightarrow L \\in \\mathbb{R}^{d \\times N}$\n+ p.2, line 21 from the top: $\\mbox{var}\\_{\\epsilon}=L^T L$ should be changed to $\\mbox{var}\\_{\\epsilon}=\\sigma^2 L L^T$. To keep the later, you may change the definition of $\\hat{\\beta}$ to $L^T(X)Y$ throughout your paper.\n+ In the definition 1, you aim to minimize the variance subject to a constraint on bias by $C$. But, in the later (Theorem 2, Figure 1, etc.), it seems to me that you don't mention $C$ again, but only mention $\\alpha$. At least you should mention what is $\\alpha$ as a function of $C$ or vice versa in Theorem 2.\n+ In Figure 1, you only plot for the very special case when $X$ is diagonal. You should also plot for other cases of $X$. \n+ Typo in Theorem 2.1, the expectation should be over $\\epsilon$ only since you already take expectation over $X$ in MSE, and $Y$ is a function of $X$ and $\\epsilon$. \n+ Right below Figure 2: Figure 3.1 $\\rightarrow$ Figure 3. \n+ Similarly, Figure 3.2.1 $\\rightarrow$ Figure 5."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3582/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3582/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3582/Reviewer_v15j"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3582/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697725876575,
        "cdate": 1697725876575,
        "tmdate": 1699636313168,
        "mdate": 1699636313168,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TDtw25ZBrf",
        "forum": "nxnbPPVvOG",
        "replyto": "nxnbPPVvOG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3582/Reviewer_ACbc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3582/Reviewer_ACbc"
        ],
        "content": {
            "summary": {
                "value": "The paper considers a variant of linear regression with constraints placed on a \"bias operator\". Under this framework, the paper discusses an extension of the Gaussian-Markov theorem, showing empirical and theoretical evidence for its main result, Theorem 2. Later discussions in the paper surround \"flatness\" and \"deepness\" of various losses considered within the paper."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper tackles an interesting class of linear regression models and delivers a thorough presentation of various aspects of the problem, from the problem definition, main theorem statement, to several case studies, all of which help to paint the overall picture of the problem. The constrained setup considered in the paper is also interesting and intuitive. Considering that linear models are a core concept of machine learning, the paper is of sufficient interest to ICLR."
            },
            "weaknesses": {
                "value": "There are several dimensions of weaknesses presented in the paper:\n\na. Clarity and overall quality of presentation. The paper does not appear to be carefully edited and revised, with multiple typographical errors in the first paragraph of the introduction alone (examples: \"somewht\" in line 3, lack of period at end of sentence in line 4, reverted quotation marks on line 5, etc.). The graphs, equations, and tables in latter parts of the paper can also benefit from detailed revisions. These issues surrounding clarity and presentation are not constrained to the first paragraph and can be found throughout the paper and also the supplementary material.\n\nb. Discussion of main theorem. While the problem setting itself is interesting, the discussions of the main theorem (Theorem 2) leave an impression that it can be further discussed. For example, what are the values of alpha? Although it is shown in the supplementary that alpha is a consequence of solving Equation 1 using Lagrange multipliers, it is also unclear how large (or small) the value of alpha is and how it impacts the interpretations of the main result. \n\nc. Considering that one of the paper's main claims is a \"flat minima\" phenomenon, the paper would benefit from stronger theoretical results (apart from simulation-based arguments) surrounding this claim. \n\nd. Possible typo in main theorem. In the main theorem's statement for the nuclear norm, the result relies on $\\max(\\Sigma,\\alpha)$: should $\\Sigma$ be replaced with something like the maximum eigenvalue?"
            },
            "questions": {
                "value": "Several questions were listed in the above \"weaknesses\" section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics concerns were found."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3582/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698671653571,
        "cdate": 1698671653571,
        "tmdate": 1699636313055,
        "mdate": 1699636313055,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dohISJ0ASE",
        "forum": "nxnbPPVvOG",
        "replyto": "nxnbPPVvOG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3582/Reviewer_cHcu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3582/Reviewer_cHcu"
        ],
        "content": {
            "summary": {
                "value": "The Gauss-Markov theorem says that the best unbiased linear estimator is the pseudoinverse of the data matrix (in the paper denoted by $X$), which can be obtained by minimizing the Frobenius norm of the estimator $L$ subject to the constraint that $L$ is the left inverse of the data matrix, $LX=I$. The paper generalizes this formulation by relaxing this constraint to $||LX-I||_p\\le C$ where $||\\cdot||_p$ is the Schatten $p$-norm (vector $p$-norm on the singular values), thus allowing some bias $C$. A special case is the ridge/Tichonov regression, obtained for $p=2$. The parameter $C$ (or a monotonically related parameter $\\alpha$) is determined by validation, by minimizing the test error. The advantage is that for $p\\neq2$, the minimum of the test error over $C$ may be flatter than for $p=2$. \n\nThe authors derive a closed form solution for the optimal estimator $L$ for $p=1$ (nuclear norm) and $p=\\infty$ (spectral norm). Next, they derive explicit form (as integrals) of for the test error in thermodynamical limit for two special distributions of the data: spherical Gaussians (elements of $X$ and noise in right-hand sides are i.i.d. normal) and diagonal data (when the Gramian $X^TX$ is diagonal). The integrals are solved in closed form for $p=1$ and $p=\\infty$. This theoretical formula is shown to agree with test error on synthetically generated data.\n\nThe cases $p=1,2,\\infty$ are compared in a simulated experiment in which $\\alpha$ with smallest test error is estimated by x-validation, where best $\\alpha$ is selected by \"grid search\" from 9 log-spaced values. This showed that the nuclear-norm regression is comparable to or better than (depending on methodology) ridge regression. A similar result is obtained for the similar experiments for nonlinear regressors (random Fourier features)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The observation that a wider minima of $\\alpha$ can be achieved at the cost of a little bias is interesting.\nThe theoretical results (test errors) are non-trivial to derive.\nThe text is clear enough, though clarity could be improved by more effort."
            },
            "weaknesses": {
                "value": "I cannot assess novelty reliably because my expertise is mainly in optimisation rather than estimation (however, I understand all parts of the main paper well). In fact, rather than extending Gauss-Markov theorem, the paper generalizes ridge regression (please, consider changing the title accordingly). It is well-known that ridge regression has non-zero bias but a smaller variance than pseudoinverse.\nSo the main novelty seems to be that of flatter minima of test error, rather than the generalization of ridge regression.\n\nThough the minima for the nuclear-norm regression are indeed flatter than for the ridge regression, the difference is sometimes only minor, as seen in Figure 2. It is true that the experiments show that in estimating $\\alpha$ by x-correlation, the nuclear norm most often wins. However, this might be due to the experimental methodology. E.g., if there were more than 9 values of $\\alpha$, the deeper minima of the ridge regression might have been hit much more often.\n\nA major weakness, in my opinion, is that the experiments are done only on synthetic data. The applications of linear regression are abundant, so it should be possible to find many suitable real datasets for this.\n\nMinor/fixable issues:\n\n1st formula in section 2.1: symbol $L(X)$ is used here but then never more. Change to $L$.\n\nThm 2: Letter $\\Sigma$ is usually used to denote the diagonal matrix with singular values. For vector of singular values, better use $\\sigma$ or $s$.\n\nThere are many small mistakes/typos in the text. E.g., references to figures in sections 2.2.4 and 3 refer to non-existent figures (e.g., figure 2.2.4 in section 2.2.4).\n\nIn the 5th line of section 2.2.1, can the formula for MSE be simplified (i.e., calculate the mean value in closed form)? It is confusing that the MSE has quite different form in sections 2.2.1 and 2.2.2 (this lets the reader wonder if this difference is substantial or just due to little care for text clarity). This might deserve a comment.\n\nMost displayed equations are unnumbered, which is not friendly for reviewers (given that lines are not numbered in the ICLR style).\n\nThe asterisk symbol in Proposition 2.1 and in the first displayed formula in section 2.2.2 has not been defined.\n\nThe integrals for $Err(\\alpha)$ in sections 2.2.1 and 2.2.2 are almost the same, up to the integrating measures. I wonder if they are correct or there are typos in them..?\n\nPOST REBUTTAL: The authors have clarified my objections, to certain extent. I am therefore raising my evaluation. However, please note that I cannot assess novelty reliably."
            },
            "questions": {
                "value": "It would be helpful to vary the number of values of $\\alpha$ in the grid method (currently this value is 9) in the experiments, i.e., to make it a hyperparameter. Pls see my remark on this in \"weaknesses\".\n\nIt would be helpful in Figure 5 to report not only winners but also MSE for different models (as in Figure 6 left) - because a winner can win by only a small margin.\n\nWhy are any experiments on real data not included? Is there a theoretical obstacle? Or, perhaps, you believe that the results of such experiments would not be informative enough? Please comment (also in the paper, if accepted)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3582/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3582/Reviewer_cHcu",
                    "ICLR.cc/2024/Conference/Submission3582/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3582/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698704767588,
        "cdate": 1698704767588,
        "tmdate": 1700820913182,
        "mdate": 1700820913182,
        "license": "CC BY 4.0",
        "version": 2
    }
]