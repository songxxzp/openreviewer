[
    {
        "id": "aWxMIfTpWd",
        "forum": "AZGIwqCyYY",
        "replyto": "AZGIwqCyYY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9361/Reviewer_GtBu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9361/Reviewer_GtBu"
        ],
        "content": {
            "summary": {
                "value": "This work leverages the power of meta-learning algorithms coupled with graph neural networks to find a shared representation of physical systems across various functional forms of Hamiltonian. In contrast to previous work, here the focus is on obtaining a representation which is valid across different physical systems. The performance of the framework is evaluated over a range of physical systems aiming to showcase its adaptivity to unseen settings."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "$\\underline{\\textrm{Originality}}$: the paper presents great originality in providing a Hamiltonian representation learning framework that can generalize across multiple system domains. This is in contrast to the common approach of providing system-specific models.\n\n\n$\\underline{\\textrm{Quality}}$: the paper is very well written. First, it provides the reader with all necessary background as well as motivation for the task addressed. Next, methods and results are well constructed. \n\n\n$\\underline{\\textrm{Clarity}}$:  the main ideas conveyed in this paper are clearly constructed and explained and supplementary information assists with providing further details and ablation studies. \n\n\n$\\underline{\\textrm{Significance}}$:  the main significance of the paper is in defining a new task, generalizing upon existing approaches, and suggesting to derive a framework that learns a representation that is not domain-specific."
            },
            "weaknesses": {
                "value": "The paper presents an appealing goal, providing generalized Hamiltonian representations consistent across different physical domains. However, given the presented quantitative and qualitative results it is hard to judge the actual generalization and performance of the framework as detailed in the following points:\n\n1. The notion of $\\textit{generalization}$: ideally when discussing generalization in DL we would like to obtain a single pre-trained model which can then be used for diverse applications. With respect to the presented framework, this would suggest training the model(s) on a single task and then using the same network for prediction on all held-out systems. Similar to the setting presented in Ricci et al. (2023). However, here presented results always consider a single held-out-system. Providing an ablation over the number of systems used for training will allow for strengthening the claim of generalization and applicability for real-world applications.\n\n2. Baselines: it would be beneficial to extend the baselines presented in the paper in two directions:\n(i) optimal;  Training over the tested task, using all regimes (meta-, pre-, and vanilla HNN). This will allow a better assessment of the quality of the generalized model and (ii) within system generalization; following the background presented in section 2 it will be valuable to add a comparison to frameworks that are similar in nature and allow $\\textit{within}$ model generalization, e.g. CoDA (Kirchmeyer et al. 2022) or within the same functional form of the Hamiltonian, e.g. iMODE (Li et al. 2023). Here training over the same train-test splits."
            },
            "questions": {
                "value": "1. Can the authors provide additional ablation studies following the weaknesses presented above? Specifically, it will be beneficial to present the performance as a function of the number of systems used in training (see weaknesses 1.) and add additional baselines  (see weaknesses 2.) \n2. Judging from the presented results the current framework is not suitable for larger systems, could the authors suggest possible extensions that may allow? What would be the necessary refinements that could be incorporated in the meta-learning configuration to allow for that?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9361/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698488282166,
        "cdate": 1698488282166,
        "tmdate": 1699637177328,
        "mdate": 1699637177328,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MPDVvgTzbR",
        "forum": "AZGIwqCyYY",
        "replyto": "AZGIwqCyYY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9361/Reviewer_xLig"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9361/Reviewer_xLig"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a novel meta-learning method aiming to learn a unified Hamiltonian representation such that it can be generalized to unseen physical systems. Hamiltonian Neural Networks were utilized as the backbone of the method for learning the unified Hamiltonian representations of different physical systems, via meta-learning pipelines of a variation of MAML. Experiments demonstrated the proposed methods achieved lower relative error of trajectories and energy when adapted to different systems."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(1) Unlike many existing works that focus on learning system dynamics under similar\u00a0physical law, the proposed methods aim to learn the unified representations across diverse system domains via meta-learning the\u00a0Hamiltonian of the given system. This sounds\u00a0significant\u00a0and promising.\n\n(2) Both quantitative and qualitative results demonstrated the proposed method achieved\u00a0better adaptation\u00a0to various new systems compared with baselines."
            },
            "weaknesses": {
                "value": "The evaluation can be strengthened\u00a0by considering comparing the proposed methods with other\u00a0Few-shot Learning and\u00a0Physics-informed Neural Networks methods for\u00a0system domain generalization under both \"consistent\" and \"different\" physical laws."
            },
            "questions": {
                "value": "In Figure 4, why does a lower CKA value of the meta-trained model suggest it learned more similar representations during adaptation? As the authors mentioned, should a low CKA value indicate different representations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9361/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9361/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9361/Reviewer_xLig"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9361/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698725487783,
        "cdate": 1698725487783,
        "tmdate": 1699637177215,
        "mdate": 1699637177215,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FBXRP3lnzq",
        "forum": "AZGIwqCyYY",
        "replyto": "AZGIwqCyYY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9361/Reviewer_jozM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9361/Reviewer_jozM"
        ],
        "content": {
            "summary": {
                "value": "The paper reports the performance of MAML applied to domain generalization over different Hamiltonian dynamics. The performance gain by MAML is analyzed with multiple indices over different combinations of meta-training and meta-test data. The key findings include the superiority of the meta-trained models compared to the pre-trained and randomly initialized models and an implication that by meta-learning, the representations obtained by the models tend to be more specific to each system."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The experiments clearly show the superiority of meta-learning, at least within the limited number of Hamiltonian systems. \n\nThe paper is well written. The motivation, the method, and the experimental results are very clearly reported."
            },
            "weaknesses": {
                "value": "The limitation of meta-learning for (Hamiltonian) dynamics is not clearly investigated. This makes it difficult to assess the range where the claims made in the paper should be valid. In other words, the claims are rather weak because their applicability seems unbounded with the current set of experiments. When the meta-learning approaches for dynamics may not be beneficial? For example, what happens if you meta-train a model only with conservative systems and try to adapt it to a dissipative system? Such experiments to investigate the limitations of the empirical findings would strengthen the paper.\n\nThe paper only reports the performance of a well-known method (MAML) merely applied to a particular setting. This could certainly be a kind of contribution in which ICLR audience may be interested, but I think that in such a paper, with a purely experimental point of view, the claims should be made more carefully. Specifically, as stated above, the cases where meta-learning is not necessarily beneficial should also be revealed, with which the claims would become more falsifiable and convincing."
            },
            "questions": {
                "value": "I don't have particular questions. It would be great if the authors could additionally report the results of some experiments to investigate the limitation of meta-learning in this context, although this is not a question."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9361/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9361/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9361/Reviewer_jozM"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9361/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698757708558,
        "cdate": 1698757708558,
        "tmdate": 1700573192364,
        "mdate": 1700573192364,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ux7s4C5vaw",
        "forum": "AZGIwqCyYY",
        "replyto": "AZGIwqCyYY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9361/Reviewer_hDu2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9361/Reviewer_hDu2"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to use meta-learning to learn generalized representations across different types of dynamical systems. The meta-learning step helps improve the adaptation to unknown systems with fewer data points (compared to randomly initialized and pre-trained baselines) by virtue of generalized representations. The authors also analyze the representations learned by different baselines and meta-learning by using centered kernel alignment (CKA) to gain insights into better performance by the meta-learning model."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is easy to follow and the motivation to learn a generalized model is clear. \n- The experiments are performed with different numbers of data-points for the adaptation task to evaluate the robustness of the approach. \n- The analysis using CKA gives further insight into how the meta-learning model learns closer representations of the adaptation task. \n- The implementation and task curation are described in detail for reproducibility."
            },
            "weaknesses": {
                "value": "- The main contribution of the paper seems to be utilizing meta-learning to efficiently adapt to new systems. However, it is not clear from the paper if it is as simple as just using off-the-shelf implementation or if there are some challenges to doing this. \n- Also, I would like to see some discussion around why meta-learning is preferred over other representation learning methods e.g. Domain Generalization, and why optimization-based methods surpass other approaches in meta-learning.\n- The experiments are not sufficient. First, there is no comparison with existing baseline models in domain generalization or meta-learning. Second, the comparison of the meta-model and pre-trained model is not fair, and I would suggest the author fine-tune the pre-trained model on the K-shot support set. Third, no visual comparison of the predicted dynamics and the ground truth, making the conclusion less convincing."
            },
            "questions": {
                "value": "Please check the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9361/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9361/Reviewer_hDu2",
                    "ICLR.cc/2024/Conference/Submission9361/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9361/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698761034922,
        "cdate": 1698761034922,
        "tmdate": 1700712736967,
        "mdate": 1700712736967,
        "license": "CC BY 4.0",
        "version": 2
    }
]