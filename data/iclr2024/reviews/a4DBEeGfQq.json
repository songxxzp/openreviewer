[
    {
        "id": "duQIbs6gkc",
        "forum": "a4DBEeGfQq",
        "replyto": "a4DBEeGfQq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5510/Reviewer_kF3D"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5510/Reviewer_kF3D"
        ],
        "content": {
            "summary": {
                "value": "This paper studies improving efficiency of graph contrastive learning. The authors propose a structural compression framework, StructComp, that adopts a low-rank approximation of the diffusion matrix to obtain compressed node embeddings. They show that the original GCL loss can be approximated with the contrastive loss computed by StructComp, with an additional benefit of the robustness. Experiments on seven benchmark datasets show that StructComp greatly reduces the time and memory consumption while improving model performance compared to the vanilla GCL models and scalable training methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(+) The proposed structural compression idea is new and interesting;\n\n(+) The presentation and organization are clear and easy to follow;"
            },
            "weaknesses": {
                "value": "(-) The applicability of StructComp seems to be limited;\n\n(-) Several claims have not been verified;\n\n(-) Some related works have not been compared or discussed;"
            },
            "questions": {
                "value": "1. The applicability of StructComp seems to be limited:\n\n- Theoretically, StructComp has to rely on the approximation of the diffusion matrix for a specific graph and GNN model, as demonstrated in Theorem 4.1. How well StructComp can approximate to more complicated graphs such as heterophilous graphs, and more complicated while commonly used GNNs such as GraphSage, GIN, GAT, or even more interesting variants such as PNA?\n\n- Empirically, what is the exact setting of StructComp for node classification? Can StructComp be applied to both transductive and inductive node classification?\n\n- How well can StructComp approximate different augmentations in GCL?\n\n2. Several claims have not been verified:\n\n- The paper claims that StructComp can work for large scale graphs, while the benchmarked datasets are rather small or medium scale. Although it\u2019s claimed that ogbn-products and arxiv are large datasets, while they are indeed medium scale datasets according to OGB (https://ogb.stanford.edu/docs/nodeprop/). To support the claim, it\u2019s expected to evaluate StructComp in large datasets such as papers100M, reddit, or OGBG-LSC datasets.\n\n- The paper also claims that StructComp has better robustness and stability with the additional regularization, while no evidence\u2019s been found.\n\n3. Some related works have not been compared or discussed:\n\n-  Some GCL works have not been discussed in the paper, for example, [1,2,3].\n\n- Why not comparing efficient GCL baselines such as CCA-SSG, and GGD discussed in the paper?\n\n4. How the time and memory cost are computed? Do they count in the preprocessing steps?\n\n\n**References**\n\n[1] Calibrating and Improving Graph Contrastive Learning, TMLR\u201923.\n\n[2] Single-Pass Contrastive Learning Can Work for Both Homophilic and Heterophilic Graph, TMLR\u201923.\n\n[3] Scaling Up, Scaling Deep: Blockwise Graph Contrastive Learning, arXiv\u201923.\n\n[4] Structure-free Graph Condensation: From Large-scale Graphs to Condensed Graph-free Data, arXiv\u201923."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5510/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5510/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5510/Reviewer_kF3D"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5510/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698642089750,
        "cdate": 1698642089750,
        "tmdate": 1700561852415,
        "mdate": 1700561852415,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AaEoXdzQBe",
        "forum": "a4DBEeGfQq",
        "replyto": "a4DBEeGfQq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5510/Reviewer_iDuQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5510/Reviewer_iDuQ"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces StructComp, a scalable training framework for Graph Contrastive Learning (GCL). By replacing the message-passing operation in GCL with node-compression, StructComp achieves significant reductions in both time and memory consumption. The authors provide both theoretical analysis and empirical evaluations to underscore the effectiveness and efficiency of StructComp in training GCL models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The storyline is relatively clear, it is easy to follow for the authors.  \n2. The experiment results are amazing, especially the time-saving.  \n3. The used method is quite simple."
            },
            "weaknesses": {
                "value": "1. Lack of discussion of graph partition: The paper lacks a comprehensive discussion on graph partitioning. Given that the efficacy of the method hinges on graph partitioning\u2014a classic NP-hard problem\u2014a detailed exploration of its impact on the proposed method is warranted. A cursory introduction does not suffice.  \n2. Inadequate theoretical provements: The theoretical justifications provided are somewhat limited. The authors' attempt to establish the equivalence between the compressed loss and the original loss is based solely on the ER model, which may not be representative of real-world datasets.  \n3. Lack of the discussion about the limitations."
            },
            "questions": {
                "value": "1. Adding more discussions about graph partition: The authors should delve deeper into the topic of graph partitioning, as highlighted in the first weakness.\n\n2. Considering not over-claiming your work: It's crucial to avoid overstating the contributions. While the authors assert that they have provided theoretical proof, the strong assumptions (like the ER model) limit its applicability. It might be prudent to either temper such claims in the abstract and introduction or offer more exhaustive proof.\nIn essence, while I acknowledge the novelty and results presented in this paper, I urge the authors to provide a more in-depth rationale behind their impressive outcomes. Without this, the paper leans more toward a technical report than a comprehensive research paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5510/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698752664700,
        "cdate": 1698752664700,
        "tmdate": 1699636564096,
        "mdate": 1699636564096,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qOmghInatf",
        "forum": "a4DBEeGfQq",
        "replyto": "a4DBEeGfQq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5510/Reviewer_v4Cz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5510/Reviewer_v4Cz"
        ],
        "content": {
            "summary": {
                "value": "This paper aim at resolving the scalability issue of graph contrastive learning training. In graph representation learning, the most compiutation  overhead comes from message passing, where its complexity grows exponentially wrt the num of layers in GNN. \n\nTo overcome this scalability issue, the authors propose **StructComp** trains the encoder with the compressed nodes. StructComp allows the encoder not to perform any message passing during the training stage."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "I like the idea of using compressing nodes to replace the need of message passing."
            },
            "weaknesses": {
                "value": "- The theoritical results only hold in linear-GNN, which over-simplifies the problem. It is well know that deep neural network behave different from linear model in contrastive learning [1]. Without consider non-linearity, the problem in Eq. 4 is simply matrix decomposition problem (e.g., [2] section 3).\n\n[1] Understanding the Role of Nonlinearity in Training Dynamics of Contrastive Learning. https://arxiv.org/abs/2206.01342\n[2] Understanding Deep Contrastive Learning via Coordinate-wise Optimization https://arxiv.org/pdf/2201.12680.pdf\n\n- Experiment datasets are too small (even arxiv dataset is small)... please try some large-scale graph datasets (e.g., Yelp, Reddit datasets that previously GraphSaint paper) to validate the effectiveness. Especially when this paper is focussing on improving the scalability issue. \n\n- Repeat experiment multiple times instead of just once. For example Figure 4."
            },
            "questions": {
                "value": "How theoritical results could be generalized to non-linear models?\n\nDoes the proposed method work for graphs with multiple node/edge types?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5510/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698820810159,
        "cdate": 1698820810159,
        "tmdate": 1699636564010,
        "mdate": 1699636564010,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ruxtljuUs1",
        "forum": "a4DBEeGfQq",
        "replyto": "a4DBEeGfQq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5510/Reviewer_PXMh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5510/Reviewer_PXMh"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes Structural Compression (StructComp), a new training framework that improves the scalability of graph contrastive learning (GCL) models. The key idea is to substitute propagation with a sparse, low-rank approximation of the diffusion matrix to compress the nodes. Contrastive learning is performed on these compressed nodes, reducing computation and memory costs. Theoretical analysis shows the compressed loss approximates the original loss and StructComp implicitly regularizes the model.  Experiments on various single-view and multi-view GCL methods demonstrate StructComp's improvements in performance and efficiency."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThe paper is well-written and easy to follow. The problem is motivated well, and the method is explained clearly. \n\n2.\tScalability is a major bottleneck hindering wider adoption of graph neural networks. This work makes an important contribution by enabling efficient training of GCL models."
            },
            "weaknesses": {
                "value": "1. Additional experiments could help verify claims on scalability and robustness of StructComp: \n\n- Evaluating on larger datasets like papers100M and or OGBG-LSC datasets  would better support scalability claims, since the experimented datasets are rather small or medium scale.  \n\n- Would be great to verify the model stability/robustness with the proposed regularization, since it is claimed in the presentation. \n\n\n2. Would be great to discuss the approximation quality to the diffusion matrix  of StructComp for more complicated graphs and other models architectures (like GAT, GraphSAGE) .\n\n3. There is a lack of  comparisons with certain related works, such as recent graph contrastive learning methods  [1-2] \n\n\n[1] ] Wang, H., Zhang, J., Zhu, Q., & Huang, W. (2022). Can Single-Pass Contrastive Learning Work for Both Homophilic and Heterophilic Graph?. arXiv preprint arXiv:2211.10890.\n\n[2] Li, J., Sun, W., Wu, R., Zhu, Y., Chen, L., & Zheng, Z. (2023). Scaling Up, Scaling Deep: Blockwise Graph Contrastive Learning. arXiv preprint arXiv:2306.02117."
            },
            "questions": {
                "value": "See the weakness above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5510/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698838670299,
        "cdate": 1698838670299,
        "tmdate": 1699636563926,
        "mdate": 1699636563926,
        "license": "CC BY 4.0",
        "version": 2
    }
]