[
    {
        "id": "1k43kwisPy",
        "forum": "F76bwRSLeK",
        "replyto": "F76bwRSLeK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6054/Reviewer_apJf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6054/Reviewer_apJf"
        ],
        "content": {
            "summary": {
                "value": "The authors train a sparse autoencoder to find interpretable hidden units in an LLM. The sparse autoencoder linearly transforms the learned representation in a layer of the LLM and then applies a rectifying nonlinearity. The resulting code is regularized by L1 penalty so as to prefer sparse representations and it is trained to do well at reconstructing the LLM representation through a linear map. Using previously published methods to quantify the level of interpretability of a feature with a score,  the authors show that some of the resulting sparse features (those with the highest interpretability score) are more interpretable than those obtained with ICA, PCA and random projections. Other experiments study the effect of perturbing one of these sparse features on the output as well as whether some features are associated with a single word (monosemantic features)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Interpretability is an important subject for improving the safety of frontier AI systems, so this work belongs in a socially important research niche. The main contribution of showing greater interpretability with sparse features derived from the learned LLM layers is useful, although similar sparse representations have already been shown to be useful from an interpretability point of view for neural networks."
            },
            "weaknesses": {
                "value": "The paper claims in the abstract and conclusion that the results show *greater* monosemanticity than other methods but I do not see such comparisons in the paper (it should be in section 5.1). Either I missed something or one of the main claims is not supported.\n\nThe authors used linear encoders (followed by a ReLU) while it is well known that this is suboptimal (the optimal encoder needs to be much more nonlinear). A useful contribution would have been to evaluate and compare different types of encoding mechanisms for obtain a sparse representation. A simple variant is to introduce hidden layers (and depth) in the encoder. There are also optimization-based encoders that work very well and are well-known for sparse coding.\n\nMost of the pieces in this paper (the sparse autoencoder and the interpretability score) are pre-existing, and so is the idea and demonstration that sparse features yield greater interpretability. This makes the overall level of originality and significance of the paper pretty low.\n\nIn the first paragraph of sec. 5.1, the authors talk about 'real-word features'. Please clarify what that means. If it is a \"human-understable concept\", one crucial problem is that such a concept (except for single-token cases studied in the paper) can be represented in many ways as a sequence of words, making any practical extension of the proposed idea (in sec 5.1) not obvious at all (or maybe not even feasible).\n\nIn section 5.3, it would be good to add evaluations using not just the top hidden layer (which has a very special semantics because it lives in the space of token embeddings) but also lower hidden layers\n\nIn section 6 about limitations, I would venture that two reasons for the poor reconstruction could be that (a) the proposed linear encoder does not have enough expressive power or (b) there exists no sparse linear basis to explain the LLM layers or (c) there is no compact natural language description for the sparse features. \n\nFinally, the whole study is only looking at the *most interpretable* features in the lot, but in order to obtain any kind of safety guarantee, one will need to make sense of *all* the information in the LLM layers. We seem very far from that, which makes me doubt that the whole approach will ever be providing sufficient safety reassurances one day."
            },
            "questions": {
                "value": "Please try to address the questions I raised in the previous section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6054/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698592760909,
        "cdate": 1698592760909,
        "tmdate": 1699636651366,
        "mdate": 1699636651366,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lC2LAtV2HG",
        "forum": "F76bwRSLeK",
        "replyto": "F76bwRSLeK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6054/Reviewer_k1em"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6054/Reviewer_k1em"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a way to make the individual features of Large Language Models more interpretable by learning simple autoencoders with activation sparsity. They demonstrate that the learned features are more interpretable than original or simple methods like PCA via several ways, such as automated interpretability, example neurons and helpfulness in detecting circuit components."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "Tackles a very important issues with mostly sound methodology and original results. The improvement in interpretability is quite noticeable, and evaluation is diverse.\nFigure 4 is a good clear explanation of 1 feature, and the comparison to the relevant feature in standard residual stream is very useful. \nFigure 5 is also interesting and a good example of the potential usecases of this method."
            },
            "weaknesses": {
                "value": "Missing a lot of important basic information.\n - What is R? It is an important part of many text captions such as Figure 2 and Figure 3 but never explained in main text. Only explained in Appendix C.\n- Almost all the results do not mention the details of how they were achieved. In particular, table 1, and figures 2, 4, 5 don't mention the model used at all, and most figures do not mention R and alpha used to train the in question.\n\nIn general the main text needs more discussion of the basic methods and results like reconstructions loss, some of the content from Appendix B and C should be moved to main text.\n\nThe high reconstruction loss seems like a problem for the usefulness/faithfulness of this method, and I would like more discussion on it, such as the i.e. large increase in perplexity discussed in section 6.2. What R and Alpha were used for this result? How does this change as a function of different parameters like R and alpha or different layers?"
            },
            "questions": {
                "value": "- Figure 2: Is identity ReLU the default basis?\n- Section 3.2: Why is ReLU applied to the default and random basis? It is said its applied to make activations nonnegative, but why is this necessary? The actual model also uses negative activations right?\n- What is the intended way to use these learned features? Adding the autoencoder changes the network behavior, and increases computational cost. Are you supposed to use the expanded network in practice to make explainable decisions, or is this better thought as an approximation to explain decisions made by the unmodified network."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6054/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698624335055,
        "cdate": 1698624335055,
        "tmdate": 1699636651253,
        "mdate": 1699636651253,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XDBQaVVF3J",
        "forum": "F76bwRSLeK",
        "replyto": "F76bwRSLeK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6054/Reviewer_oRDV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6054/Reviewer_oRDV"
        ],
        "content": {
            "summary": {
                "value": "This paper is addressing the challenge of polysemanticity in DNNs. In DNNs context neurons appear to activate in multiple, semantically distinct contexts. This paper is trying to address the challenge of disambiguating neurons. They suggest sparse auto-encoders for this challenge."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "+ Very interesting research question"
            },
            "weaknesses": {
                "value": "- Overly complex approach, where the auto-encoder will likely add noise to the results\n- No real baseline comparison \n- Experimentation is weak \n- Results are difficult to interpreter and would need to be better presented or colored in the context"
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6054/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698786127906,
        "cdate": 1698786127906,
        "tmdate": 1699636651148,
        "mdate": 1699636651148,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pUhTTQRaPx",
        "forum": "F76bwRSLeK",
        "replyto": "F76bwRSLeK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6054/Reviewer_FVSC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6054/Reviewer_FVSC"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes using sparse autoencoders to learn interpretable and monosemantic features from the internal activations of language models. The key idea is to reconstruct model activations as sparse linear combinations of latent directions, which aims to disentangle the model's features from superposition. The authors train autoencoders with an L1 sparsity penalty on various model layers and analyze the resulting feature dictionaries.\n* The learned dictionary features are shown to be more interpretable than baseline methods like PCA/ICA according to automated scores.\n* The features localize target model behaviors more precisely for a counterfactual evaluation task.\n* Case studies demonstrate some highly monosemantic features that influence model outputs in predictable ways.\n* The approach is scalable and unsupervised, requiring only unlabeled activations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Applies sparse coding in a novel way for model interpretability. The approach is intuitive and theoretically motivated.\nProvides solid evidence that the learned features are more interpretable and monosemantic.\nDemonstrates the technique can pinpoint features for analyzing model behaviors.\nCase studies show highly intuitive individual features.\nThe method is scalable and unsupervised."
            },
            "weaknesses": {
                "value": "The reconstruction loss is not fully minimized, indicating the dictionaries do not capture all information.\nThe method works less well for later model layers and MLPs compared to early residual layers.\nMore analysis needed on:\n1) generalizability of behaviors across tasks,\n2) sparsity of dependencies between features.\n\nLimited comparison to other interpretability and disentanglement techniques.  For example, beta-VAE, infoGAN, FactorVAE had similar disenanglement goals.  And Michaud 2023 \"The Quantization Model of Neural Scaling\" suggested a spectral clustering approach for identifying \"monogenic\" signals in large models. The paper would be strengthened if it provided a more complete comparison to previous disentanglement approaches.\n\nThe authors could mention the previous works in the field of sparse autoencoding and dictionary learning for word embeddings. Subramanian et al. [1] similarly found linear factors for word embeddings, in this case using a sparse autoencoder. Zhang et al. [2] solved a similar problem using methods from dictionary learning. Their method discovers elementary structures beyond existing word vectors.\n\n[1] Spine: Sparse interpretable neural embeddings Subramanian, A., Pruthi, D., Jhamtani, H., Berg-Kirkpatrick, T. and Hovy, E., 2018. Proceedings of the AAAI Conference on Artificial Intelligence, Vol 32(1).\n\n[2] Word embedding visualization via dictionary learning Zhang, J., Chen, Y., Cheung, B. and Olshausen, B.A., 2019. arXiv preprint arXiv:1910.03833."
            },
            "questions": {
                "value": "How well do the features transfer across different models and architectures?\nCould you incorporate model weights or adjacent layer features to improve dictionary learning?\nWhat changes allow learning overcomplete MLP bases?\nHow well do the features generalize to unseen tasks? More thorough evaluation would be useful.\nThe authors claim the method is scalable, but have not demonstrated it on very large models. Experiments on models with billions of parameters could better support the scalability claims."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6054/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698837722381,
        "cdate": 1698837722381,
        "tmdate": 1699636651040,
        "mdate": 1699636651040,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CLgs4HlIkg",
        "forum": "F76bwRSLeK",
        "replyto": "F76bwRSLeK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6054/Reviewer_V2wZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6054/Reviewer_V2wZ"
        ],
        "content": {
            "summary": {
                "value": "Superposition refers to the hypothesis that neural model representations are in fact linear compositions of sparse features. This paper attempts to identify these sparse features using dictionary learning. More specifically, the authors train an autoencoder with sparsity penalties on language model representations of interest (as both input and output), and use the learned sparse encoding as a proxy to understand the original language model. The resulting sparse encodings are passed through an auto-interpretation model to identify meaningful features. Under the auto-interpretation evaluation, the learned sparse encoding by the proposed method beat several baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is well written and easy to follow. The idea of using sparse autoencoder to tackle superposition makes intuitive sense, and it turns out that it works surprisingly well. The activation patching experiments validates the effectiveness of the proposed method beyond the auto-interpretation metrics, which emphasizes the method's practical value."
            },
            "weaknesses": {
                "value": "Using dictionary learning for superposition isn't particularly novel.\nAuto-interpretation score hasn't been shown to correlate with actual interpretability use cases yet.\nActivation patching experiment was only conducted on one relatively synthetic dataset."
            },
            "questions": {
                "value": "Any comments on how the proposed method scales with complexity of the underlying task / number of possible features? Is the approach bottlenecked by auto-interpretation method?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6054/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699509766672,
        "cdate": 1699509766672,
        "tmdate": 1699636650941,
        "mdate": 1699636650941,
        "license": "CC BY 4.0",
        "version": 2
    }
]