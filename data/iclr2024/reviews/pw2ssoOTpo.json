[
    {
        "id": "A11dkgT3fu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1803/Reviewer_6ZJ5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1803/Reviewer_6ZJ5"
        ],
        "forum": "pw2ssoOTpo",
        "replyto": "pw2ssoOTpo",
        "content": {
            "summary": {
                "value": "The paper introduces CIFAR-10-Warehouse, a substantial dataset comprising 180 diverse datasets with images from the original CIFAR-10 categories, sourced from various sources including real-world searches and stable diffusion. CIFAR-10-Warehouse serves as a valuable resource for advancing research in model generalization analysis, accuracy prediction, and domain generalization. CIFAR-10-Warehouse creates a challenging testbed, shedding light on the complexities of model performance in diverse, real-world scenarios. Additionally, the paper highlights potential applications in fields such as learning from noisy data and out-of-distribution detection. This paper advances the understanding and evaluation of model generalization in machine learning research."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(1) The paper introduces a novel and extensive CIFAR-10-Warehouse dataset with a diverse collection of 180 datasets. The authors' approach of curating datasets from real-world searches and stable diffusion represents a novel and creative way to construct a comprehensive testbed for model generalization analysis.\n\n(2) The paper maintains a high quality in dataset creation and experimentation. The dataset creation process appears thorough, involving both real-world image searches and diffusion model generation, while ensuring privacy and adhering to licenses. The experiments conducted on CIFAR-10-Warehouse are extensive, employing various methods and classifiers with detailed analysis of the results. \n\n(3) The paper is generally well-written and organized, making it easy to follow. The methodology for dataset creation and evaluation tasks is clearly explained.\n\n(4) CIFAR-10-Warehouse serves as a valuable resource for researchers, providing a unique dataset that covers a wide range of real-world scenarios and challenges the generalization abilities of machine learning models. The paper's exploration of potential applications in learning from noisy data, domain adaptation, and out-of-distribution detection highlights its significance in various domains."
            },
            "weaknesses": {
                "value": "(1) While the paper provides details about the data collection process, it lacks a discussion on potential biases and limitations introduced during data collection from search engines. Biases in search engine results can affect the diversity and representativeness of the dataset, which should be acknowledged and addressed.\n\n(2) The paper focuses on domain generalization within the context of CIFAR-10-Warehouse. However, CIFAR-10 is indeed a relatively small dataset with low-resolution images and a limited number of categories compared to larger-scale and more diverse datasets like ImageNet or the Wilds[1]. This work does not extensively discuss how the findings from this dataset can be applied to real-world scenarios or other domains. \n\n(3) This work could be strengthened by discussing potential real-world applications beyond the scope of image classification. Evaluating and extending the methods on datasets with different characteristics and applications would provide a more practical significance for real-world applications.\n\n(4) While the paper introduces several domain generalization and accuracy prediction methods, it could benefit from including additional state-of-the-art baseline methods, e.g., GVRT[2], and VNE[3], for a more comprehensive comparison. This would help establish a clearer benchmark for the proposed methods.\n\n\n[1] WILDS: A Benchmark of in-the-Wild Distribution Shifts\n\n[2] Grounding Visual Representations with Texts for Domain Generalization. ECCV 2022\n\n[3] VNE: An Effective Method for Improving Deep Representation by Manipulating Eigenvalue Distribution. CVPR 2023"
            },
            "questions": {
                "value": "(1) Can you elaborate on the potential biases introduced during data collection from search engines or how did you ensure the collected data is diverse and representative?\n\n(2) How might the findings from CIFAR-10-Warehouse generalize to other larger-scale datasets or tasks beyond image classification?\n\n(3) Analyzing and discussing the failure modes of domain generalization would provide insights into scenarios where these methods might not work well. Can you discuss and provide examples of failure modes for the domain generalization and accuracy prediction methods on the CIFAR-10-Warehouse testbed?\n\n(4)The paper mentions addressing limitations and publishing future versions of CIFAR-10-Warehouse but does not provide a concrete roadmap for future research. A discussion of potential future directions, and how the dataset can evolve would be insightful."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1803/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697322932226,
        "cdate": 1697322932226,
        "tmdate": 1699636109751,
        "mdate": 1699636109751,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vFrzb6jxWA",
        "forum": "pw2ssoOTpo",
        "replyto": "pw2ssoOTpo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1803/Reviewer_tRct"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1803/Reviewer_tRct"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new benchmark dataset for domain generalizations. Compared to the previous datasets, CIFAR10-Warehouse contains a lot more number of domains with both real-world images and images synthesized by stable diffusion. Extensive benchmarking and comparisons are conducted on this new dataset in terms of two generalization tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper provides a new dataset with a much larger number of domains compared to existing domain generalization dataset. The idea of multi-domain dataset gives the researcher a new perspective on how to evaluate the domain generalization methods. \n2. The experiments are quite extensive and gives some interesting insights on domain generalization."
            },
            "weaknesses": {
                "value": "1. Can the authors give more analysis and justification on why they divide different domains based on color and cartoon/no cartoon? since there are a lot of other ways to categorize different domains, such as other styles besides cartoon. \n2. Can the authors give more empirical analysis on the advantage and difference of the proposed dataset compared to existing datasets. For example, are the performance comparison on CIFAR-10-W and existing datasets aligned? Are there any contradicted conclusions or new observations based on the experiment results on CIFAR-10-W?"
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "May need to check the license of the images in the dataset."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1803/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1803/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1803/Reviewer_tRct"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1803/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698648227855,
        "cdate": 1698648227855,
        "tmdate": 1699636109682,
        "mdate": 1699636109682,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "h93y2vG7RT",
        "forum": "pw2ssoOTpo",
        "replyto": "pw2ssoOTpo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1803/Reviewer_VNrf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1803/Reviewer_VNrf"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a dataset called CIFAR10-W. I detail the construction of the dataset as the creation of the testbed seems to follow standard practices in DG, but done over a wide model zoo of classifiers.\n\n**Construction**\nA subset consists of all 10 classes of CIFAR10 with one colour and from the same source (all images are 224x224 but resized to 32x32 in experiments)\n\nIt consists of the following 180 subsets by (a-d):\n\n*a) Querying 4 search engines*: Google, Bing, Baidu, 360 with a total of 12 colours (Google and Bing differing on 1-2 colours) using the two queries:\n1) category_name \n2) category_name cartoon \nThis creates {2 queries} x {12 colours} x {4 engines} = 96 subsets\n\n*b) Querying other search engines*: Sogou, with the same 12 colours as Baidu/360, Pexe with 20 colours and Flickr with 15 colours. No cartoons queried from here.\nThis creates 12+20+15 = 47 subsets.\n\nThis creates 143 subsets which contain real images in total. 95 of these search by keywords (CIFAR-10-W KW) and 48 belong to the additional cartoon domain (CIFAR-10-W KWC).\n\n*c) Querying Stable Diffusion 2.1*: This is done the two prompts for the same 12 colours as Baidu/360/Sogou for synthetic versions of the real data:\n1) high quality photo of {color}{class name}\n2) high quality cartoon photo of {color}{class name}\n\nThis creates 12 x 2 = 24 subsets\n\n*d) 13 subsets created by using special prompts with SD-2.1*: Prompts given in Table 4 -- with background, context where target objects do not naturally co-exist.\n\nAdditionally:\n- Cleaned annotations and details on previously labeled incorrect labels.\n\n**Comparison**: CIFAR10-Cs benchmarks refer to a collection of CIFAR10-C, 10.1 and 10.2.\n\n**Testbed**: Testbed comprises of two tasks:\n- Model Accuracy Prediction using Unlabeled Test Sets \u2013 I am unfamiliar with this task\n- Domain Generalization, similar to DomainBed \u2013 I am vaguely familiar with this task\n\nIt is important to note that they performed evaluation over a wide range of models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "**S1) Good benchmark [Critical]**: I like the proposed dataset, which is one of the core contributions as:\n(1) It is a realistic domain shift in contrast to synthetic corruptions\n(2) It is large-scale and has 224x224 images (albeit is a 10 class classification problem)\n(3) It has a large number of domains (36-180 domains)\n(4) It has cleaning annotations available!\n\n**S2) Comprehensive evaluation [Critical]**: The paper seems to have tested methods across a lot of different models and across different subsets of the C10-W on two different tasks. Appendix sections were quite a delight to go through.\n\n**S3) DG results quite cleverly done [Important]**: I liked the setup using alternative sources like Yandex, etc. These aspects seemed quite thoughtful to me"
            },
            "weaknesses": {
                "value": "**W1) [Critical] Why is MAE of prediction scores the main metric reported in the paper for comparing methods in Table 2?**\n\nWhy I ask this (my understanding, could be caused by misinterpretation): \n- MAE of prediction scores, rather than prediction scores seems like a bad measure as indicated by Figure 2 and elaborated in Fig 9 \u2013 no correlation to accuracy, while Figure 7 shows prediction scores have high spearman rank order correlation (a strong measure of correlation!). \n- Hence, I can discern very little about the predictiveness of the accuracy from the MAE score as they\u2019re uncorrelated! \n\n**W2) [Important] Conclusions made from Task 1 need improvement-- Similar but to a lesser degree for Task 2.**\n\nDetails for Task 1:\n\n*(C1) This benchmark is a harder benchmark compared to CIFAR-10-Cs.*\n\n- Corruptions can be made harder by increasing the magnitude easily, increased hardness as the primary feature of the testbed seems weird. I would be interested in rather highlighting whether networks don't capture certain aspects introduced here.\n \n*(C2) Predictions are more consistent across classifiers here.*\n\n- One can easily argue that predictions are consistent here because the shift is not varied\u2013 it simply changes colours rather than the diverse, varied corruptions studied in CIFAR-10-C  benchmarks. \n- Note that this does have a distribution shift across cartoons and SD classifier, but SD classifier results are varying across classifiers too! \n\n**W3) [Important] Little analysis of the dataset itself, far more focus on task/models**\n\n- While this weakness is vaguely stated, I have fleshed out some components in Q1 to concretely ask what analysis would be helpful from my view.\n- However, I do not work in either of the fields so it is hard for me to accurately ask, but there seems to little analysis done which is concerning."
            },
            "questions": {
                "value": "**Q1) How many visually distinct domains exist within these 180 subsets? Specifically, how different are images sampled from different search engines?**\n\nI found 36 distinct domains:\n- The 12 colour palettes seem like distinct domains\n- Cartoon, real-world images and SD images seem distinctly different\n\nBy visual inspection by me, images across different search engines look similar (ordering the images by the same colour would have made my job, and an interested readers\u2019 easier in Figures in Appendix F). Note that popular datasets are also collected from different search engines but treated as one domain-- do different engines introduce a noticeable shift?\n\n*Counterclaim to the point: Significant drop in performance in Baidu and 360.*\n- Is it due to a lesser number of images or because of the domain gap? [Maybe separating the search engines in Figure 1 would be have been very informative for a reader, alongside more analysis of drift]\n- I suspect those engines have fewer images which might be causing the accuracy drop.\n\nThe benchmark seems to pitch that there are 180 clearly distinct domains, would be concerning if rather there are only 36 visually distinct domains. \n\n**Q2) What all components would be released publicly?**\n\nI presume the dataset, along with the cleaned annotations and licenses would be released.\n- Would the scraping code be released?\n- Would the code for classifiers tested be released?\n- Will the trained models/features be released?\n\nCould the authors address the weaknesses, these questions and check if I missed pointing out some strengths? That would help me make a more balanced evaluation. I like the benchmark itself but the experiments and conclusions drawn from it need improvement in my view. Note that I am not familiar at all with Task 1 and only vaguely familiar with Task 2, indicated in my confidence."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1803/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1803/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1803/Reviewer_VNrf"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1803/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698751411393,
        "cdate": 1698751411393,
        "tmdate": 1699636109568,
        "mdate": 1699636109568,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "N7YhBZJfIN",
        "forum": "pw2ssoOTpo",
        "replyto": "pw2ssoOTpo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1803/Reviewer_Xmsr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1803/Reviewer_Xmsr"
        ],
        "content": {
            "summary": {
                "value": "This paper constructed a thorough dataset for OOD evaluation based on CIFAR-10. They also provide a benchmarking analysis for a thorough list of accuracy prediction and domain generalization methods, which reveals interesting findings including identifying difficult settings that current methods fails. They also pointed out other directions that this dataset might be useful including denoising, unsupervised domain adaptation and OOD detection."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The paper is cleanly written and easy to read. The contribution can be pretty beneficial to the field as a thorough and high-quality benchmark dataset is the foundation for methods improvements, not to mention that OOD is a crucial problem in the field. The dataset built in this paper is quite thorough and high quality in my opinion as it includes not only many more domains compared with previous efforts but also includes state-of-art generative methods as well as real-world data for the dataset build. The author also provides interesting experiments that point out the limit of current state-of-art accuracy prediction as well as domain adaptation methods, which can certainly inspire corresponding methods improvements to be developed in the future."
            },
            "weaknesses": {
                "value": "Only want to point out this one typo: you seem to have an unfinished sentence at the last line of page 6."
            },
            "questions": {
                "value": "I am curious whether a finer-grained dataset like CIFAR-100-warehouse can be built as well. I know that many current state-of-art can suffer at finer-grained classification tasks. It could be interesting future work."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1803/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1803/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1803/Reviewer_Xmsr"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1803/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699036553380,
        "cdate": 1699036553380,
        "tmdate": 1699636109507,
        "mdate": 1699636109507,
        "license": "CC BY 4.0",
        "version": 2
    }
]