[
    {
        "id": "5fDpmmZfcl",
        "forum": "CfXh93NDgH",
        "replyto": "CfXh93NDgH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6631/Reviewer_HVLz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6631/Reviewer_HVLz"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces Evol-Instruct, a data augmentation pipeline to automatically generates diverse and complicated instructions and responses using LLMs. By instruction-tuning a Llama model on the generated instances (the resulting model is referred to as WizardLM), the paper shows that WizardLM outperforms Vicuna and Alpaca on automatic benchmarks and human evaluation."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed method is straightforward and simple. \n- The paper conducts both automatic and human-based evaluation, increasing the reliability of the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "- The paper lacks the number of baselines for comparison. For example, there are recent LLaMA-based instruction-tuned models such as Tulu (Wang et al, 2023) and Orca (Mukherjee et al, 2023). However, the paper does not compare the performance nor discuss the related papers. \n- The process of generating training instances largely relies on a single LLM (ChatGPT). If the prompts and design choices are optimized towards a single blackbox LLM, the transferability to other LLMs is questionable. Is the proposed data generation method also effective for other LLMs using the same experimental design (Using LLMs other than ChatGPT for training data generation)? \n- In Figure 5, the paper calculates the difficulty of the instruction by simply using ChatGPT. However, the instructions are also generated by the same LLM by prompting to make the previous instructions harder. Therefore, the validity of the difficulty estimation used in this paper is questionable. To test whether generated instructions \"actually\" become more challenging, the paper should calculate the difficulty by using different evaluators or other metrics. \n\nReference:\nWang et al, 2023: How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources\nMukherjee et al, 2023: Orca: Progressive Learning from Complex Explanation Traces of GPT-4"
            },
            "questions": {
                "value": "- For human evaluation, how many labelers labeled the same instances? Could you share the exact inter-labeler agreement between human labelers? \n- Does using the full 250K instructions lead to better performance on the automatic evaluation setting?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6631/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698303608874,
        "cdate": 1698303608874,
        "tmdate": 1699636757265,
        "mdate": 1699636757265,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Im6wBBi0Le",
        "forum": "CfXh93NDgH",
        "replyto": "CfXh93NDgH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6631/Reviewer_tNq7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6631/Reviewer_tNq7"
        ],
        "content": {
            "summary": {
                "value": "The paper explores the challenges large language models (LLMs) face in following user-specified instructions. To address these issues, the authors introduce a novel approach, \"Evol-Instruct,\" which uses LLMs to automatically produce open-domain instructions at various complexity levels. By starting with a base set of instructions, the Evol-Instruct method evolves them to increase complexity, either deepening existing instructions or introducing new ones. The resulting instructions are then used to fine-tune the LLaMA model, producing a new model named WizardLM. Through both automatic and human evaluations, WizardLM demonstrated superior performance compared to other models like Alpaca and Vicuna."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Innovative Approach: The Evol-Instruct method offers a fresh perspective on generating complex instructions without relying solely on human input.\n- Comprehensive Evaluation: The authors provide both automatic and human evaluations to assess the performance of WizardLM.\n- Broad Applicability: WizardLM's superior performance in varied benchmarks, including code, math, and general conversation, suggests its broad applicability.\n- Consideration of Instruction Complexity: The paper highlights the significance of instruction complexity in enhancing LLM performance."
            },
            "weaknesses": {
                "value": "- Reliance on LLMs: The Evol-Instruct method's dependence on LLMs may introduce biases from the original training data.\n- Uncertainty in Evolution Direction: The random selection between In-depth and In-breadth evolving may not always yield the optimal instruction complexity.\n- Potential Overfitting: The process of evolving instructions multiple times might risk overfitting the model to specific types of instructions.\n- Paper Organization: This paper is not well-organized enough, especially for \u201c3 APPROACH\u201d. For example, the \u201cPrompts of In-Depth Evolving\u201d and \u201cPrompts of In-Breadth Evolving\u201d should contained within \u201cInstruction Evolver\u201d, but are listed in a parallel format. There is limited content in \u201cResponse Generation\u201d."
            },
            "questions": {
                "value": "- How do you ensure that the evolved instructions maintain their intended purpose and don't deviate from the original goal?\n- Are there any measures to prevent potential biases introduced during the instruction evolution process?\n- How does WizardLM handle instructions that are not part of the evolved set?\n- Would incorporating more human feedback during the instruction evolution process help in refining the quality of instructions? If so, how?\n- How scalable is the Evol-Instruct process for even larger models or datasets in the future?\n- What\u2019s your opinion of learning with Evol-Instruct and in-context learning with demonstrations? The correlations and differences, and the pros and cons."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6631/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6631/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6631/Reviewer_tNq7"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6631/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698587652751,
        "cdate": 1698587652751,
        "tmdate": 1700633045695,
        "mdate": 1700633045695,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yIEXyUmvbB",
        "forum": "CfXh93NDgH",
        "replyto": "CfXh93NDgH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6631/Reviewer_fF8a"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6631/Reviewer_fF8a"
        ],
        "content": {
            "summary": {
                "value": "The paper argues that human written instruction tuning data as well as GPT-generated self-instruct data lack difficulty and diversity. To address this issue, they propose a procedure to modify existing self-instruct data of Alpaca and make them more diverse and more difficult. The results show that after finetuning on this dataset, the resulting model outperforms Alpaca and Vicuna on a number of standard academic datasets and a small WizardEval dataset.\n\n----\nThe authors have satisfactorily resolved my concerns during the rebuttal. Hence, I increased the score from 5 to 6."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper identifies an important problem: the instruction tuning data lacks diversity and difficulty. I agree this is indeed a problem.\n\nThe reported evaluation results are good, and lends support to the claim that the new dataset is better than baselines."
            },
            "weaknesses": {
                "value": "## Major Issues ##\n\nThe authors did not perform a thorough evaluation of the created dataset. \n- The authors do not compare against diverse instruction tuning datasets such as Natural Instructions, Supernatural Instructions, and the training data of FLAN-T5. \n- The experiments focus on Llama and ignore other LLMs such as T5, Falcon, Mistral (which has a model not instruction tuned), and so on. \n- The evaluation of difficulty and diversity by ChatGPT is unconvincing, as the paper presents no evidence that ChatGPT is good at these evaluations. It is also dubious to report only the average score of difficulty and diversity, as these are clearly different concepts. \n- The WizardEval dataset is too small and the paper contains little detail about its construction, so we do not know if the dataset can serve as a sound and fair evaluation benchmark. \n- I do not understand the argument in the section Analysis of In-breadth Evolving. \n\nOne possible evaluation of difficulty and diversity is to divide all the training data into several subsets with increasing difficulty / diversity. If ChatGPT is capable of evaluating the training data, then perhaps more difficult and more diverse training data will lead to better performance. Note that this is different from the current grouping based on evolution iterations, which includes a confounding variable: the number of evolution iterations. \n\n## Minor Issues ##\n\nThe prompts to ChatGPT contain a few grammatical issues. For example, \"Your objective is to rewrite a given prompt into a more complex version to make those famous AI systems (e.g., ChatGPT and GPT4) a bit harder to handle\" should be \"a more complex version which those famous AI systems (e.g., ChatGPT and GPT4) find a bit harder to handle\". It is the prompt that should be hard to handle, not the AI systems. \n\n\"But the rewritten prompt must be reasonable and must be understood and responded by humans.\" Here, whether the prompt is responded to by humans or by AIs cannot be controlled by the prompt generation network. The authors probably meant that it is possible for humans to respond correctly to the prompt. \n\nHowever, these are likely minor issues because ChatGPT may not have sufficient logic reasoning to understand the subtle differences. \n\n## Writing ##\n\nThe paper makes extensive citations to related papers, including many that are recently posted on arxiv. Though it is good to acknowledge similar work, sometimes the cited papers do not support or relate to the text. For example, in the following sentence\uff1a\n\nHuman annotators are prone to fatigue and cannot sustain high-intensity work to produce a sufficient\nproportion of high-difficulty instructions (Zhang et al., 2023; Xiao et al., 2023; Manakul et al., 2023;\nZhong et al., 2023)\n\nThe provided citations are unrelated to whether human annotators can sustain high-intensity work. This may create the impression that the paper attempts to mislead the reader, though it is not the authors' intention to do so. \n\nElimination evolving is a vague and ungrammatical name. I suggest renaming this step. Perhaps failure handling or incorrect prompt filtering?"
            },
            "questions": {
                "value": "- How do we know the answers to the newly created prompts are correct?\n- How do we know the difficulty and diversity scores by ChatGPT are correct? Please do not say because other papers did similar things. The other papers could be wrong, too."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6631/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6631/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6631/Reviewer_fF8a"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6631/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698854117050,
        "cdate": 1698854117050,
        "tmdate": 1700657310422,
        "mdate": 1700657310422,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "k3GRtuH3mH",
        "forum": "CfXh93NDgH",
        "replyto": "CfXh93NDgH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6631/Reviewer_JU8z"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6631/Reviewer_JU8z"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a novel pipeline for creating large amounts of instruction data with varying levels of complexity using LLM. The proposed approach starts from a set of seed instructions and first uses Evol-Instruct, a suit of prompt templates that can make LLMs such as ChatGPT to rewrite seed instructions into more complex ones. Then all generated data are used for fine-tuning open-source LLMs such as LLaMA. The authors used the proposed method to collect a set of instruction data and fine-tuned LLaMA into \"WizardLM\", and conduct a suit of evaluation on various benchmarks. Experimental results show some improvement over representative baselines including Alpaca and Vicuna models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The idea of using LLMs to rewrite and synthesize more complex instructions is interesting and intuitive.\n2. The paper is overall well-written and easy to follow. \n3. The authors evaluate WizardLM on a wide range of datasets/benchmarks and the experimental results look promising."
            },
            "weaknesses": {
                "value": "1. The technical contribution of the proposed method is not very significant because compared to self-instruct, it is only adding the command for LLM to generate more complex instruction. The success of the proposed method largely depend on the abilities of powerful LLMs such as ChatGPT. While interesting and intuitive, I'm not sure the technical contribution of the manuscript is suitable for conferences such as ICLR.\n\n2. The authors compare WizardLM with Vicuna by using the same amount of generated instructions. However, the cost or token consumption used for collecting the datasets (especially compared with Alpaca) should also be controlled or at least mentioned. \n\n3. The manuscript lacks comparisons with other methods for instruction data generation methods such as Baize, CAMEL, etc. For benchmark results, the role of the seed instructions is very important. I assume the seed data is very different, which could be a major cause of the performance difference. Therefore, more detailed ablation study is required to make the results more convincing.\n\n4. Also, it would be very helpful to test the proposed data generation methods using other LLMs (e.g., LLaMA) instead of ChatGPT to better understand the proposed data synthesis pipeline.\n\n4."
            },
            "questions": {
                "value": "Please see the above weakness section for questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6631/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699190668458,
        "cdate": 1699190668458,
        "tmdate": 1699636756891,
        "mdate": 1699636756891,
        "license": "CC BY 4.0",
        "version": 2
    }
]