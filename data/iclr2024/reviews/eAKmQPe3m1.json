[
    {
        "id": "UgS2GjIVHo",
        "forum": "eAKmQPe3m1",
        "replyto": "eAKmQPe3m1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1660/Reviewer_7QAx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1660/Reviewer_7QAx"
        ],
        "content": {
            "summary": {
                "value": "The authors present a text-to-image model that achieves good quality results while using much smaller networks and datasets than current SOTA models. The key to the improvement seems to be some combination of (1) transformer-based diffusion (2) multi-stage training (5 stages for a single model) (3) careful dataset curation."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Being able to train a high-quality generative model with an ~order of magnitude reduction in resources is a very important contribution, as the cost of training large models limits access to large, well-resourced organizations.\n\n- The results are good, and the evaluations are sufficient. The FID score is a bit high compared to newer models, but I agree with the authors that this is (at best) an imprecise metric for quality.\n\n- The paper is overall well written and easy to read."
            },
            "weaknesses": {
                "value": "I have found some inconsistencies in reported results:\n\n- The GPU days reported in A.5 contradict the main table; Summing the last column and using their 2.2 scale factor for converting from v100 to a100 days yields 753 A100 days vs. the 675 reported in the main table. It's a relatively small difference (11%) but concerning that the reported numbers don't align.\n\n- Some of the baseline numbers are inconsistent / misleading. For example, the authors report that Imagen consumed 15B images. The Imagen authors report a dataset size of 800M. It seems that 15B, at least roughly, corresponds to the total number of non-unique images seen during training, e.g. (batch size) x (training steps). If we applied this metric to PixArt-alpha (using the table in A.5), we would get a number of 31B - 2x the Imagen results. I did not dig into the other baseline numbers, but these should be checked carefully because they are central to the paper's contributions.\n\n- There are 3 primary improvements (dataset curation, multi-stage training, and unet->transformers), however there are no ablations quantifying how much of an effect each one has on model performance. While the results and final model are good, a lack of ablations limits the research contribution of the paper. \n\n- The authors do not discuss the training of the VAE; since this is an important part of \"learning the pixel distribution of natural images\", the training time and data quantity of the VAE should be included in the paper."
            },
            "questions": {
                "value": "- My intuition says that training cost and CO2 emissions should be directly proportional to GPU hours, however these proportions are inconsistent. For example, Fig 2b shows a 14x increase in cost of Imagen over PixArt, while the ratio of reported GPU hours is 10. For GigaGAN, Fig2b shows a 9.4x increase, where as Table 2 shows a 7x increase in GPU hours."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1660/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1660/Reviewer_7QAx",
                    "ICLR.cc/2024/Conference/Submission1660/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1660/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698083852014,
        "cdate": 1698083852014,
        "tmdate": 1700716013794,
        "mdate": 1700716013794,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UACSwsOaZp",
        "forum": "eAKmQPe3m1",
        "replyto": "eAKmQPe3m1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1660/Reviewer_Bg3n"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1660/Reviewer_Bg3n"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new text-to-image synthesis model, called PixArt-$\\alpha$. The proposed approach introduces several improvements to conventional models on the side of the training process and architecture. In particular, the paper proposes a training strategy decomposition, in which 1) the learning of dependencies between pixels; 2) text-image alignment; 3) high-resolution synthesis, are all modelled in different subsequent phases. On the architecture side, the paper builds on top of Diffusion Transformer pre-trained on class-conditional ImageNet, proposing several tricks to adopt it to text-to-image synthesis with minimal overhead. In addition, the authors re-visit the training datasets used to train text-to-image models. By applying LLaVA to label existing datasets, much more dense labelling of images in terms of nouns/image is achieved.\n\nIn effect, the proposed approach achieves state-of-the-art quality of synthesis but significantly reduces the training time and the needed amount of training data. The method is shown to be on par or better than models like Imagen, SDXL, while using only 10% of the training time and 10% of the training samples."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "Overall, this is a very actual paper with very strong experimental result, big significance and potential impact.\n\n- $\\underline{\\text{Contribution}}$. The proposed method achieves state-of-the-art results with resources that are an order of magnitude lower than of current mainstream methods. Given that biggest T2I are very expensive to train, this is a big step forward for the community to democratize T2I models for broader audiences, to decrease costs and the carbon footprint required for training. \n- $\\underline{\\text{Insights}}$. The paper introduces valuable insights for the community, particularly on the side of dataset design. The paper demonstrates that the design of previous datasets like LAION suffers from deficient descriptions and infrequent diverse vocabulary usage, which led previous models for many additional epochs needed until convergence. The provided VN/DN analysis can influence the next generation of image-text datasets.\n- $\\underline{\\text{Results}}$. The paper demonstrates strong experimental results. In particular, visual results are impressive, beautiful, and have good alignment to conditioning text. The model is shown to outperform big models like DALLE-2 or SDXL by human perception. Overall, the quality of results clearly matches the bar or ICLR.\n- $\\underline{\\text{Applications}}$. It is demonstrated that the proposed method supports applications that are generally expected from big T2I models, like personalization (DreamBooth, ControlNet etc).\n- $\\underline{\\text{Presentation}}$. The paper is very well written, the ideas are very easy to follow. Explanations are high-level and clearly deliver lessons for the community. More technical discussions are presented in supplementary."
            },
            "weaknesses": {
                "value": "I do not see major issues that would preclude publication, but I would still ask a couple of questions:\n\n- How expensive was the dataset collection? I expect running a big image-text model like LLaVA on millions of images required a lot of time, GPU ressources, and memory. Should this non-trivial step be in some way included to the analysis of costs and co2 emissions?\n- Although using a model like LLava clearly improves VN/DN, it probably introduces new biases to the distribution of text prompts? Does this affect the scope of text prompts for which PixArt-$\\alpha$ works well or poorly?"
            },
            "questions": {
                "value": "What are the plans of the authors regarding open-sourcing? \nIs it expected to be released 1) complete training code; 2) collected dataset; 3) \"internal\" dataset?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1660/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698673043493,
        "cdate": 1698673043493,
        "tmdate": 1699636094029,
        "mdate": 1699636094029,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TWcgNokh9n",
        "forum": "eAKmQPe3m1",
        "replyto": "eAKmQPe3m1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1660/Reviewer_GFA3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1660/Reviewer_GFA3"
        ],
        "content": {
            "summary": {
                "value": "PIXART-\u03b1 is a novel Text-to-Image (T2I) model capable of generating photorealistic images from text descriptions while maintaining low training costs. The model's approach involves decomposing the T2I task into three distinct stages, which include pixel dependency learning, text-image alignment learning, and high-resolution and aesthetic image generation. This is achieved through the modification of the diffusion transformer architecture, incorporating cross-attention layers, simplifying adaptive normalization layers, and utilizing re-parameterization techniques for an efficient T2I Transformer. Additionally, the model leverages high-informative data from a vision-language model (LLaVA) for generating quality image captions, drawing from the SAM dataset and fine-tuning using JourneyDB and an internal dataset."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The PIXART-\u03b1 model is particularly cost-effective to train, making it an attractive option for researchers and organizations with limited computational resources, as it minimizes the financial and hardware requirements associated with training a state-of-the-art T2I model.\n\nIt employs a straightforward and simplified approach to achieve its text-to-image synthesis, ensuring that the model's architecture and training process are accessible and comprehensible to a broader range of researchers and practitioners.\n\nNotably, PIXART-\u03b1 boasts fewer tunable parameters, which not only contributes to its cost-effectiveness but also makes it more manageable and less prone to overfitting or complex hyperparameter tuning, streamlining the implementation and optimization process."
            },
            "weaknesses": {
                "value": "Given its smaller dataset, there are concerns about the generalizability and compositional capabilities of the model for a broader range of concepts.\n\nA more extensive test for out-of-domain generalizability would add valuable insights into the model's adaptability and effectiveness in diverse scenarios."
            },
            "questions": {
                "value": "What led to the decision of utilizing ImageNet data as the pretraining source instead of a pretrained VQGAN, and how did this choice impact the model's performance and capabilities?\n\nThe selection of the Diffusion Transformer as the basis for the model's architecture over the more conventional approach of the Latent Diffusion Model (Unet) is mentioned in Appendix A10, but can a more comprehensive analysis, including ablation studies, be provided to thoroughly compare the strengths and weaknesses of these architectural choices?\n\nThe model's performance raises the important question of determining the minimum number of images required for training while still ensuring generalizability and maintaining the compositional properties of Text-to-Image models. Can the authors shed some light on what is the minimum set of images required to train such a model from scratch. What should be the properties of such a dataset?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1660/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698797501033,
        "cdate": 1698797501033,
        "tmdate": 1699636093917,
        "mdate": 1699636093917,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UT55rKdFW2",
        "forum": "eAKmQPe3m1",
        "replyto": "eAKmQPe3m1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1660/Reviewer_VQLf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1660/Reviewer_VQLf"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces several recipes to accelerate the training of text-to-image foundation models. These include\n- Use pretrained DiT\n- Use Flan-T5 XXL\n- Use LLaVA captions.\n- AdaLN / AdaLN single architectures to reduce model size.\nOverall, these methods allow training of a reasonable quality model in 10% of the resources used than Stable Diffusion, which makes it more possible to democratize the training recipes in text-to-image foundation models."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "Originality: the empirical evaluation of synthetic captions in text to image generation is not systematically studied until DALL-E 3, and the new AdaLN architecture might be useful.\nClarify: the paper is quite clear about most details about the training, which makes reproducibly much more likely.\nSignificance: the paper mostly explores valid heuristics for training text-to-image foundation models quickly, some conclusions can be helpful in the community: 1) DiT architecture instead of UNet, 2) the use of synthetic captions, 3) the use of SAM dataset."
            },
            "weaknesses": {
                "value": "The paper mostly is a combination of multiple ideas that exist in the literature, so \"novelty\" in the traditional sense is somewhat limited."
            },
            "questions": {
                "value": "1. The SAM dataset blurs human faces in their training, won't this cause problem in generation cases where generating a face (not closed up) is needed?\n2. How does the model generate images with more extreme aspect ratios?\n3. The DiT architecture has a fixed patch size. As resolution becomes higher, so will the number of tokens be higher. Will this cause a bottleneck in training and inference (such as 1k resolution)?\n4. DALL-E 3 technical report mentions the pitfall of \"overfitting\" to automated captions, is this the case in PixArt model? If not, how is this mitigated?\n5. Since the dataset size is smaller, does it have trouble producing named entities, such as celebrities? \n6. How critical is training DiT on ImageNet needed? While being able to start with an existing model is good it also limits the possibilities to explore different architectures. \n7. The CLIP score of the CLIP-FID curve of Pixart seems worse than SD 1.5. Is there any reason for that?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1660/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698819513880,
        "cdate": 1698819513880,
        "tmdate": 1699636093821,
        "mdate": 1699636093821,
        "license": "CC BY 4.0",
        "version": 2
    }
]