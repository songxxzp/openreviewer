[
    {
        "id": "C5Rpz9OUSJ",
        "forum": "PKICZXVY9M",
        "replyto": "PKICZXVY9M",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2931/Reviewer_v4h7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2931/Reviewer_v4h7"
        ],
        "content": {
            "summary": {
                "value": "The paper works on improving the OOD generalization of finetuned vision-language models. Specifically, a class-conditional feature generator and adaptive self-distillation mechanism are proposed to serve the goal."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "[$\\textbf{Interesting Idea}$] The idea of generating unknown-class features is interesting.\n\n[$\\textbf{Presentation Quality}$] The presentation is clear and easy to follow."
            },
            "weaknesses": {
                "value": "[$\\textbf{Unconvincing Statement}$] The work mentions that it is the first to unveil the pitfalls of finetuning VLMs by prompt learning can cause overfitting on base classes, resulting in poor performance on novel classes. However, CoCoOp \u201cConditional Prompt Learning for Vision-Language Models, CVPR 2022\u201d has already observed this and proposed conditional prompt learning to address it.\n\n[$\\textbf{Unclear Model Design}$] This work uses known image and text features as K and V, while unknown text features as Q to generate unknown image features. The rationale behind this is not clear. It would be nice to explain this in more details.\n\n[$\\textbf{Missing Related Works}$] For finetuning methods, there are many works that need to be discussed, e.g., \u201cCLIP-Adapter: Better Vision-Language Models with Feature Adapters\u201d, \u201cTask Residual for Tuning Vision-Language Models\u201d, \u201cImproving Zero-Shot Generalization for CLIP with Synthesized Prompts\u201d, \u201cMaPLe: Multi-modal Prompt Learning\u201d and \u201cSelf-regulating Prompts: Foundational Model Adaptation without Forgetting\u201d.\n\n[$\\textbf{Small Performance Gains}$] The results in Table 1 show the improvements from adding the proposed method are rather limited. Moreover, the performance is much worse than some SOTA methods, e.g., \u201cMaPLe: Multi-modal Prompt Learning, CVPR 2023\u201d, \u201cImproving Zero-Shot Generalization for CLIP with Synthesized Prompts, ICCV 2023\u201d and \u201cSelf-regulating Prompts: Foundational Model Adaptation without Forgetting, ICCV 2023\u201d. It would be nice to see the performance of these SOTA methods by adding the proposed components."
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2931/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2931/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2931/Reviewer_v4h7"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2931/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697982469781,
        "cdate": 1697982469781,
        "tmdate": 1699636236638,
        "mdate": 1699636236638,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ogTCr4fcvC",
        "forum": "PKICZXVY9M",
        "replyto": "PKICZXVY9M",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2931/Reviewer_Rp4n"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2931/Reviewer_Rp4n"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method to regularize ERM for OOD generalization with CLIP models. The method uses a feature prediction network to hallucinate image features corresponding to unknown texts at training time. The resulting training procedure is more robust, since it takes into account synthesized features from unseen classes. The authors also propose a self-distillation mechanism to complement the method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- I found the method interesting and novel.\n- I found the paper easy to follow. Section 3.2 and Figure 2 are especially informative and organized very intuitively.\n- The method seems like it could be useful."
            },
            "weaknesses": {
                "value": "- the self-distillation mechanism is easy-to-think-of, but this is okay, since it is not the main innovation.\n- My main concern with this paper would be the results. In particular, they are not state-of-the-art (see [Maple] and [Clipood]). Furthermore, the reported CoOp performance seems low. With some tuning, CoOp can be much better, e.g. [KgCoOp] reported a harmonic mean of 74.6 for CoOp on average, compared to 71.7 reported by the authors.  From personal experiments, I know that simply finetuning both encoders along with the prompt with cross-entropy can achieve much better results on these benchmarks, (80.3 % HM on the base-to-novel benchmark, average of 11 datasets). However, the authors seem to focus on just prompt tuning, so it might be ok.\n\nI'm incline to think that that the method is interesting enough for acceptance, even though, in my opinion, the results are not state-of-the-art.\n\n[Maple] Muhammad Uzair Khattak et al. \"MaPLe: Multi-modal Prompt Learning\"\n\n[Clipood] Yang Shu, Xingzhuo Guo et al. \"CLIPood: Generalizing CLIP to Out-of-Distributions\"\n\n[KgCoOp] Hantao Yao et al. \"Visual-Language Prompt Tuning with Knowledge-guided Context Optimization\""
            },
            "questions": {
                "value": "None."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2931/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2931/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2931/Reviewer_Rp4n"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2931/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698270672476,
        "cdate": 1698270672476,
        "tmdate": 1700519776726,
        "mdate": 1700519776726,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UvMt5WbYwu",
        "forum": "PKICZXVY9M",
        "replyto": "PKICZXVY9M",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2931/Reviewer_Y9rR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2931/Reviewer_Y9rR"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the limited generalization capabilities of existing vision-language models, which struggle to handle open-domain visual concepts. The authors propose a novel approach called OGEN to improve the out-of-distribution (OOD) generalization of finetuned models. OGEN introduces a class-conditional feature generator that synthesizes OOD features using only the class name of any unknown class, helping to regularize the decision boundary between in-distribution (ID) and OOD data. Additionally, an adaptive self-distillation mechanism is employed to prevent overfitting. Experimental results demonstrate that OGEN achieves considerable improvements in OOD generalization performance across different settings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The forgetting problem is important in foundation models during fine-tuning.\n* The over-fitting observation can support the paper's main claim.\n* The proposed method is reasonable."
            },
            "weaknesses": {
                "value": "I believe it would be beneficial for this paper to include a comparison with relay-based methods, such as sampling a subset from Lioan-5B and using it for replay. The \"class-conditional feature generator\" seems to serve as a proxy for the replay data, so it would be valuable to directly explore the use of replay methods. As a result, I find the novelty of the proposed approach to be somewhat limited considering this concern."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2931/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2931/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2931/Reviewer_Y9rR"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2931/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698569158968,
        "cdate": 1698569158968,
        "tmdate": 1700549575989,
        "mdate": 1700549575989,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MwCINaRT0K",
        "forum": "PKICZXVY9M",
        "replyto": "PKICZXVY9M",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2931/Reviewer_sauS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2931/Reviewer_sauS"
        ],
        "content": {
            "summary": {
                "value": "The paper improves the out-of-distribution (OOD) generalization of vision-language models, especially CLIP, when they are finetuned on downstream tasks. The paper makes the following contributions:\n\n- It reveals the overfitting problem of existing finetuning methods, such as prompt learning, that degrade the OOD performance of CLIP models.\n\n- It proposes a novel method called OGEN, which consists of two components: a class-conditional feature generator and an adaptive self-distillation mechanism.\n\n- The paper evaluates OGEN on various downstream tasks and datasets and shows that it consistently improves the OOD generalization of different finetuning methods for CLIP models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed method addresses a novel and important problem of improving the OOD generalization of vision-language models, especially CLIP when they are finetuned on downstream tasks.\n\n- The paper proposes a novel method called OGEN, which consists of two components: a class-conditional feature generator and an adaptive self-distillation mechanism. The feature generator synthesizes OOD image features given the name of an unknown class, by extrapolating from the most similar known classes. The self-distillation mechanism uses an adaptive teacher model that is an exponential moving average of past model checkpoints within a local time window. The teacher model guides the student model to avoid overfitting and maintain a good trade-off between in-distribution and OOD performance.\n\n- The paper evaluates OGEN on various downstream tasks and datasets and shows that it consistently improves the OOD generalization of different finetuning methods for CLIP models. It also provides comprehensive ablation studies and analysis to validate the effectiveness of each component of OGEN."
            },
            "weaknesses": {
                "value": "(1) The proposed method mainly compared with CoOp (IJCV'22), Co-CoOp (CVPR'22), and VPT (ECCV'22).\nHowever, before the deadline of ICLR, the state-of-the-art methods are released here: https://github.com/muzairkhattak/PromptSRC\nMaPle (CVPR'23) and  PromptSRC (ICCV'23) need to be discussed in this paper.\n\n(2) In Tab 3 and Tab 4, the proposed class-conditional feature generator slightly decreases the performance of the base classes.\nIn the appendix Fig 5, there are some explanations regarding the performance increase in the new classes and performance variation in the base classes. These discussions need to move to the main script.\n\n(3)\tThe first step of OGEN, novel class extrapolation, is problematic. It is unreasonable to utilize base features to extrapolate novel features, since there are usually large conceptual gaps between base and novel classes. The authors provide a special case, that is \u201ccat, bear->raccoon\u201d. But in CIFAR-10, for example, I think the \u201cship\u201d class is not conceptual close to any other classes.\n\n(4)\tAnother contribution of this paper, claimed by authors, is Adaptive Local Mean Teacher (ALMT), which I think is just a trivial trick of hyperparameter tuning. The difference between ALMT and conventional MT is just modifying the sliding window size. The novelty is quite low, and the performance improvement brought by ALMT over \u201cNo distillation\u201d is insignificant (less than 1%), as shown in Table 6.\n\n(5)\tThe performance of OGEN is too low and outdated. In existing prompt learning papers in CVPR\u201923 (such as [1,2]) and ICCV\u201923 (such as [3,4]), the \u201cNew\u201d accuracy in base-to-novel generalization setting is already about 75%, but OGEN can only achieve around 70%. \n\n(6)\tThe paper writing is quite poor and hard to follow. The objective function is missing in Sec 3.3.\n\n[1] CVPR 2023. MaPLe: Multi-modal Prompt Learning\n[2] CVPR 2023. LASP: Text-to-Text Optimization for Language-Aware Soft Prompting of Vision & Language Models\n[3] ICCV 2023. Self-regulating Prompts: Foundational Model Adaptation without Forgetting\n[4] ICCV 2023. Read-only Prompt Optimization for Vision-Language Few-shot Learning"
            },
            "questions": {
                "value": "My main concern is the baselines selected to compare in this paper are too old (methods published in 2022). MaPle (CVPR'23) and  PromptSRC (ICCV'23) need to be discussed in this paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2931/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698789938676,
        "cdate": 1698789938676,
        "tmdate": 1699636236380,
        "mdate": 1699636236380,
        "license": "CC BY 4.0",
        "version": 2
    }
]