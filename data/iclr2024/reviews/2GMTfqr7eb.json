[
    {
        "id": "kBJfR0MwM9",
        "forum": "2GMTfqr7eb",
        "replyto": "2GMTfqr7eb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5609/Reviewer_96a3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5609/Reviewer_96a3"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a knowledge distillation method for self-supervised learning on efficient network architectures such as MobileNetV3, EfficientNet-B0. The author proposes a simple technique called, RETRO, that reuses the teacher\u2019s projection head for students during the knowledge distillation and this simple technique achieves significant improvements on self supervised learning results of efficient architectures."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is easy to read and the method is quite simple and can be easily plugged into any self-supervised learning pipeline.\n- The problem of improving the self-supervised learning for efficient architectures is important, especially on why these models are harder to train with SSL pretext task.\n- Despite the simplicity of the proposed technique, RETRO achieves consistent improvements in all the tasks (linear evaluation as well object detection/segmentation) that the authors present and the results are impressive."
            },
            "weaknesses": {
                "value": "- Despite its simplicity, the method seems merely an engineering trick which has worked on the knowledge distillation for efficient networks. \n- Considering the limited novelty in the approach, I expected the authors would have put more effort in understanding why it works.\nThe paper also does not touch much on why efficient architectures are harder to perform SSL. Is there any specific reason based on the architectural design of these efficient networks? Generally, in supervised learning these efficient architectures are known to be better.\n- Is this specific choice of keeping only the frozen projection head necessary in the student model? What if the consistency is maintained at earlier layers as well as the final embedding of the teacher and student networks. It should work in a similar manner.\nIdeally, one would expect an ablation study on what part of the teacher network should be embedded in the student network to make SSL work? \n- What if instead of the projection head, first few layers of the teacher network are enforced in the student model.\n- Do the authors test the same approach of keeping projection head from SSL performed on one network to another (not necessarily efficient)? Can this save training time on another network architecture?"
            },
            "questions": {
                "value": "Please look at the weaknesses mentioned above.\nIs the knowledge distillation in RETRO performed while doing pre-training on teacher model or after the SSL training has been performed on teacher model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5609/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698647555922,
        "cdate": 1698647555922,
        "tmdate": 1699636578345,
        "mdate": 1699636578345,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "icCpEoyRUV",
        "forum": "2GMTfqr7eb",
        "replyto": "2GMTfqr7eb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5609/Reviewer_qqvw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5609/Reviewer_qqvw"
        ],
        "content": {
            "summary": {
                "value": "This paper introduced a method to improve knowledge distillation of light-weight models. They propose to clone the projection head of the teacher to simplify mimicking its preceding features. The method optimizes a contrastive loss between the student and Mean student, and a consistency loss between the student and teacher networks, simultaneously."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1) The writing of the main context is clear and well organized.\n2) The authors provide thorough explanations of their motivation and methodology, and support their claims with experiments.\n3) Quality and reproducibility are acceptable."
            },
            "weaknesses": {
                "value": "1) The main weakness is the incremental novelty of this work when compared with previous works, particularly DisCo.\n2) The method does not consistently lead to significant improved accuracy over baselines in table 1. It is suggested that authors perform some statistical tests to measure if the performance gains are statistically significant and discuss the properties of the architectures and datasets which leads to performance improvement.\n3) The computational cost of the method is higher compared to the baselines such as SEED and DisCo.\n4) The method is limited to CNN architecture.\n5) The related work is lacking KD publications from 2023 such as [1].\n\n[1] Song, Kaiyou, et al. \"Multi-Mode Online Knowledge Distillation for Self-Supervised Visual Representation Learning.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."
            },
            "questions": {
                "value": "This work can be improved if the authors address my concerns above. \n\nWhy are the pretraining algorithm of the teachers unidentical in the experiments?\n\nWhat are the design choices of the adapter layer in figure 5?\n\nThere is a typo in equation 1."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5609/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5609/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5609/Reviewer_qqvw"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5609/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698802482183,
        "cdate": 1698802482183,
        "tmdate": 1699636578246,
        "mdate": 1699636578246,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lUW26uy02p",
        "forum": "2GMTfqr7eb",
        "replyto": "2GMTfqr7eb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5609/Reviewer_dnj5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5609/Reviewer_dnj5"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a self-supervised method for lightweight models. They use a pretrained teacher and use two loss to train the student: 1. SSL loss on the student 2. Reuse teacher\u2019s MLP layer on student and regress teacher\u2019s features. They show that this simple idea outperforms DisCo on ImageNet and transfer learning to other tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "[+] The proposed method is simple and effective. \n[+] Comprehensive comparison to DisCo on ImageNet and transfer learning shows the effectiveness of their method compared to DisCo."
            },
            "weaknesses": {
                "value": "[-] Although it\u2019s an interesting insight that using teachers MLP improve the student\u2019s representation, this work lacks technical novelty in my opinion. \n\n[-] Teacher model in the experiments are old SSL methods with only CNN architecture. For example, DINO ViT-B has 80% ImageNet Linear Top-1 acc (DINOv2 has 86.3 with ViT-L). Current setup with weak/outdated teacher is not practical and convincing. I highly encourage the authors to evaluate their model on recent SOTA SSL methods to improve the impact of the paper. \n\n[-] Missing citations : \n\n[b] SimReg: Regression as a Simple Yet Effective Tool for Self-supervised Knowledge Distillation. K L Navaneet, Soroush Abbasi Koohpayegani, Ajinkya Tejankar, Hamed Pirsiavash\n\n[a] effective self-supervised pre-training on low-compute networks without distillation. Fuwen Tan, Fatemeh Saleh, Brais Martinez"
            },
            "questions": {
                "value": "-"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5609/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5609/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5609/Reviewer_dnj5"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5609/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698813481144,
        "cdate": 1698813481144,
        "tmdate": 1699636578144,
        "mdate": 1699636578144,
        "license": "CC BY 4.0",
        "version": 2
    }
]