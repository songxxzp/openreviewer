[
    {
        "id": "G6YAcMFhDz",
        "forum": "SkeoEFlF0E",
        "replyto": "SkeoEFlF0E",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8136/Reviewer_LDaU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8136/Reviewer_LDaU"
        ],
        "content": {
            "summary": {
                "value": "The paper considers a modified CP decomposition in which the multi-vector inner product defining each element is replaced by a sum of neural networks, whose input dimension is equal to the order of the tensor. The experiments focus on tensor completion for binary tensors, with a cross-entropy loss function. The experiments include comparison with alternate decompositions for accuracy on a few tensor completion test problems. Additionally performance and interpretability is considered for classification tasks."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The proposed model is relatively simple and makes sense, though theoretical/application grounding of the method is not quite clear.\n + The results demonstrate improvements in accuracy over a variety of baselines, which seem to be achieved with a more compact model (though model size is not explicitly evaluated).\n + The results include both performance evaluation and downstream tasks.\n + The paper is clear / reasonably well-written, in both presentation of the method and its evaluation."
            },
            "weaknesses": {
                "value": "- Besides the definition of the new algorithm, the paper has practically no theoretical analysis, besides a simple cost quantification based on CP parameters.\n - In discussion of closely related works on tensor decompositions with factors replaced by neural nets, the following sentence is used as contrast \"However, the way they used neural network entangled associations of all components makes it difficult to identify the contribution of entities.\" This seems overly broad, and should be justified by particular aspects of each of the related methods.\n - Comparison of different models in accuracy for relative to model size would be of interest.\n - The accuracy improvements compared to other models seem pretty minor.\n - I have some concerns regarding whether state-of-the-art methods are being used as baselines here, please see the questions.\n\nOverall, the paper pursues an interesting hybrid method that combines a neural network with tensor decomposition, and shows that it is competitive for tensor completion. However, there is little theoretical motivation or analysis to motivate the effectiveness of the approach. The experimental results point to improvements in accuracy, but make some simplifying assumptions on baseline methods that make it difficult to draw strong conclusions from the results. Given that ICLR has a theoretical focus, I would recommend rejection of the paper in its current form, but I believe the ideas and experimental studies are of value and should be published after revision."
            },
            "questions": {
                "value": "* Are the baselines implemented by the authors or are existing codes used? I would be concerned that Adam is not an efficient method (does not lead to the more accurate decomposition) for training some of the baselines.\n * Why is CPD not trained relative to cross-entropy? There are existing works for CP completion with generalized loss functions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8136/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8136/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8136/Reviewer_LDaU"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8136/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698768527894,
        "cdate": 1698768527894,
        "tmdate": 1699637007801,
        "mdate": 1699637007801,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1BEtLRUR8N",
        "forum": "SkeoEFlF0E",
        "replyto": "SkeoEFlF0E",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8136/Reviewer_ZGpK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8136/Reviewer_ZGpK"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a neural tensor decomposition method called NEAT that captures nonlinear patterns in sparse tensors while maintaining interpretability. Aligned with classical CPD, NEAT decomposes a tensor into a sum of components, where each component is modeled by a separate MLP. This allows the expression of nonlinearities in an additive, separable way. NEAT is evaluated on link prediction for multiple real-world sparse tensors. NEAT also produces interpretable embeddings that allow the discovery of patterns in different components."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed model shows a novel and reasonable way to combine the simple but interpretable CPD with the deep module. Besides state-of-the-art link prediction performance on multiple datasets, the design of the down-streaming task with finetuning is interesting. Also, the presentation is smooth and easy to follow."
            },
            "weaknesses": {
                "value": "- Compared to prior deep tensor work, the main selling point of the work is the interpretability based on the component(rank)-wise modeling. The experiment part (section 4.4) also highlights it. However, as component-wise nonlinear MLPs are used, do the learned latent factors really reflect the useful pattern in data? In other words, is it possible that meaningful patterns will be encoded in component-wise MLP, otherwise the latent factor's component?  More discussion or numerical experiments to investigate the consistency are encouraged.\n\n- As I recognize the importance of the interpretability of tensor factor, is the interpretability from component-independent formulation better or more helpful than such component-cross methods? For example, for Tucker decomposition, the learned Tucker core can also show interpretability to help people understand the inner pattern of data. Some claims on why component-wise interpretability is crucial should be highlighted. Otherwise, the novelty and contribution of the proposed work could be limited.   \n\n- The experiment setting and analysis could be further enhanced. Some other non-linear tensor methods could be added as baselines, such as Gaussian-Process-based[1]. The downstream task setting by finetuning is interesting, but some discussion and analysis on why the proposed design could enhance the generalization performance are encouraged. \n\n[1]Tillinghast, Conor, et al. \"Probabilistic neural-kernel tensor decomposition.\" 2020 IEEE International Conference on Data Mining (ICDM). IEEE, 2020."
            },
            "questions": {
                "value": "See weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8136/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698777506196,
        "cdate": 1698777506196,
        "tmdate": 1699637007691,
        "mdate": 1699637007691,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "d8Hk6dlXnP",
        "forum": "SkeoEFlF0E",
        "replyto": "SkeoEFlF0E",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8136/Reviewer_3Eaz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8136/Reviewer_3Eaz"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a neural network model (NEAT) for sparse tensor decomposition. NEAT replaces the direct inner products of factors in CP decomposition with learnable MLPs. The authors showed NEAT outperformed CP and other neural network based tensor decomposition models in several real world tensor completion datasets and one downstream task. The authors showed the latern factors learned by NEAT can be used for interpretability."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper is clearly written and fairly easy to follow. The authors conduct thorough experiments to show the performance of the proposed model."
            },
            "weaknesses": {
                "value": "* The contribution is somewhat limited. Rank-k CP decomposition is the sum of the outer products of k rank-1 factors. The proposed NEAT model uses learnable MLPs to replace the outer product. There have been many papers doing similar things, some of which are cited in this paper as well. It is unclear how the proposed NEAT model is different comparing with the existing neural network based models. From section 2, it seems the biggest advantage of NEAT is that there's no cross factor interaction, as NEAT is a direct extension of CP decomposition model.\n* The analysis of the empirical evaluation is not strong enough to justify the contribution. The chosen datasets are not complicated enough to demonstrate that NEAT significantly outperforms baselines. The interpretability part is good to have but is not good enough to convince readers that the proposed model is outstanding on that part."
            },
            "questions": {
                "value": "* In section 2, the authors mentioned that \"... neural network entangled associations of all components makes it difficult to identify the contribution, ...  the proposed method NEAT ... simplifying the discovery of non-linear latent patterns\".  Could the authors please elaborate on this? Does it always hold that simple methods are better? What types of tensors would be better fit by complex models, and what types of tensors would be better fit by the proposed model or CP model?\n* How many extra parameters are introduced by NEAT when comparing with CP when the factors CP rank are the same? Could it be the case that, for some sparse tensors, the total number of parameters of NEAT is more than the number of observed entries? In that case, how does NEAT do in terms of overfitting?\n* Does the proposed model work for non-sparse tensors?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8136/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698805343750,
        "cdate": 1698805343750,
        "tmdate": 1699637007540,
        "mdate": 1699637007540,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ztdPp516Ao",
        "forum": "SkeoEFlF0E",
        "replyto": "SkeoEFlF0E",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8136/Reviewer_XfPM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8136/Reviewer_XfPM"
        ],
        "content": {
            "summary": {
                "value": "The article presents a new tensor decomposition method called Neural additive tensor decomposition (NEAT), which extends the standard CP Decomposition with non-linear functions. The method incorporates ideas from neural tensor models, and applies Multi-layer perceptrons (MLPs) to each rank-1 CP factors. The proposed method captures non-linear interactions and also helps in interpretability of results.  Numerical results are presented on different datasets to illustrate the performance of the proposed method in comparison to SoTA neural and other tensor decomposition methods."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "Strengths:\n1. The paper presents an interesting new tensor decomposition that captures non-linear interactions \n2. The decomposition appears to have an easily interpretable form, and this advantageous in many applications\n3. The pair presents extensive numerical results, which show that the proposed outperforms other compared methods."
            },
            "weaknesses": {
                "value": "Weakness:\nFrom a numerical computation perspective, \n1.  The computational cost of the method could be an issue for large tensors.\n2. The decomposition is non-unique and the optimization problem looks difficult to solve for a good minima."
            },
            "questions": {
                "value": "The paper presents an interesting new tensor decomposition, which has several advantages and will likely be useful in a number of applications involving tensors. The paper is well written. Numerical experiments section is extensive and presents many different results illustrating the superior performance of the prosper method, and studies different aspects of the method. \n\nI have the following few minor questions:\n\n1.  How is the optimization problem m in eq (8) computed? Is it just by ADAM or autograd?\n\n2. Movielens seems to be a very easy datasets for neural methods. They all achieve very high accuracy. Is there a particular reason for this?\n\n3. How is the downstream task performed (transductive and inductive) for CP and Tucker decompositions? These do not have parameters to train (and freeze). Are the factor matrices simply used as input features to the classifier?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8136/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698811672120,
        "cdate": 1698811672120,
        "tmdate": 1699637007421,
        "mdate": 1699637007421,
        "license": "CC BY 4.0",
        "version": 2
    }
]