[
    {
        "id": "xszzRr0U9d",
        "forum": "WNQjN5HzXt",
        "replyto": "WNQjN5HzXt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6015/Reviewer_WuQd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6015/Reviewer_WuQd"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a general Sim2Real adaptation framework, AUGCAL, for semantic segmentation and object recognition.\nAUGCAL introduces synthetic data augmentation and model calibration on synthetic data to reduce miscalibration during synthetic training. Combining with standard unsupervised domain adaptation methods on unlabeled real-world data, those techniques further improve SIM2REAL performance on real-world data.\nEspecially, this paper provides theoretical analysis to show the target calibration loss can be bound by the source calibration loss. This is the reason that calibration on synthetic data is beneficial for the performance of real-world data. Also, it shows the necessity of introducing the augmentaiton.\nExperiments are conducted on GTAV to Cityscapes and VisDA SIM2REAL, and show that the proposed framework can improve the baselines easily."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "[Clarity] The paper is well structured, and I find myself easy to follow.\n\n[Baseline] Baselines with AUGCAL outperform those without AUGCAL in all metrics.\n\n[Implementation] The presented method looks straightforward yet effective and easily re-implementable.\n\n[Novelty]\n- Augmentation and calibration are complementary and can be integrated easily to achieve the goal.\n- Miscalibration is an overlooked factor for unsupervised domain adaptation, and this paper proposes an easy way to handle it.\n- The paper provides insights on how the calibration loss on synthetic data can reduce miscalibration on real data via a theoretical analysis. This is beneficial for other related research fields."
            },
            "weaknesses": {
                "value": "I listed some suggestions in Questions."
            },
            "questions": {
                "value": "[Augmentation] The choice of augmentation should satisfy the property 1 in Sec. 3.2.2. Even Tab.1 shows that PASTA/R.Aug-SIM are closer to Real than SIM based on MMD, it is not intuitive. It would be better to clarify it more.\n- For PASTA, it is designed to bridge the syn-to-real gap and maybe reduce the artifacts of synthetic data. But I am not sure why R.Aug even closer to Real.\u00a0\n- Also, if we have large-scale unlabeled real-world data, I am wondering if style transfer-based augmentation will have better performance because it directly uses unlabeled real-world data.\n- Last, as property 1 is a distribution distance, I am not sure how those appearance augmentations affected distribution.\n\n[OC] After Eq.3 and in Sec.4, the paper introduces overconfidence (OC) as a metric. But I am not sure of the exact definition.\n\n[Eq.9] Eq.9 shows a useful upper bound. It would be more interesting to provide more insights regarding this bound. For example, what issues (e.g., a large gap between syn and real) and why will those issues lead to the loose of the upper bound?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6015/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698735436992,
        "cdate": 1698735436992,
        "tmdate": 1699636645338,
        "mdate": 1699636645338,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DV2cwv9n6b",
        "forum": "WNQjN5HzXt",
        "replyto": "WNQjN5HzXt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6015/Reviewer_dFNn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6015/Reviewer_dFNn"
        ],
        "content": {
            "summary": {
                "value": "This paper starts with a nice theory demonstrating that to achieve better calibration loss in target domain, one should minimize the miscalibration in source domain and reduce the distributional distance between source and target domains. Then, to address this, the paper proposes AUGCAL, which augments the source data and apply a calibration loss on it. Experiments demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The theory is nice and understandable.\n2. The paper is well-written and experiments are extensive."
            },
            "weaknesses": {
                "value": "1. Usually adding calibration loss will have lower ECE but also lower accuracy (in your case mIoU). Can you give some intuition in why adding calibration loss on source give better mIoU as shown in Table 3 (a)?\n2. This paper proposes two properties and empirically verifies PASTA and RandAugment satisfy the criterion. It would be good to give some intuition in the main text.\n3. [1] uses StyleNet which learns a style transfer from source to target, it would be interesting to analyze whether this inherently satisfy the property 1.\n[1] Donghyun Kim, Kaihong Wang, Kate Saenko, Margrit Betke, and Stan Sclaroff. A unified framework for domain adaptive pose estimation. In ECCV. Springer, 2022.\n4. Do the two AUG choice meet the property 2? Is the \\epsilon as small as before AUG?\n55. Since equation 9 is the upper bound of calibration loss, how is this related to mIoU?"
            },
            "questions": {
                "value": "please see weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6015/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6015/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6015/Reviewer_dFNn"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6015/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698767024553,
        "cdate": 1698767024553,
        "tmdate": 1699636645211,
        "mdate": 1699636645211,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pAMxYNS1Xz",
        "forum": "WNQjN5HzXt",
        "replyto": "WNQjN5HzXt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6015/Reviewer_iaFo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6015/Reviewer_iaFo"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes AUGCAL to improve the confidence calibration and model reliability of existing unsupervised domain adaptation approaches. Specifically, the authors propose to replace the original images with strongly augmented ones and additionally optimize for calibration loss on the augmented images. Detailed analytical justification has been derived to show how AUGCAL can reduce miscalibration on real data. Extensive experiments on different datasets and different UDA backbones have been conducted to show the effectiveness of AUGCAL."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Extensive experiments on different tasks, datasets and UDA backbones are provided in the paper to evaluate the proposed method. \n\n2. Theoretical derivations have been provided to motivate the design of AUGCAL.\n\n3. The proposed AUGCAL is simple, which only introduces two small changes to the existing UDA pipelines."
            },
            "weaknesses": {
                "value": "1. The technical contribution of this paper is a bit limited. The key components of AUGCAL are the data augmentation and DCA based calibration loss. However the data augmentation technique is commonly used in domain adaptation and the DCA proposed by previous work for model calibration.\n\n2. The proposed method is only compared with diffident backbone UDA models. A more convincing comparison would be to compare with existing confidence calibration methods, such as [Wang et al. 2022], [Gong et al. 2021].\n\n3. The paper exceeds the page limit of 9 pages."
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6015/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6015/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6015/Reviewer_iaFo"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6015/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1700466444958,
        "cdate": 1700466444958,
        "tmdate": 1700466444958,
        "mdate": 1700466444958,
        "license": "CC BY 4.0",
        "version": 2
    }
]