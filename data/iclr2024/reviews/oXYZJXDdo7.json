[
    {
        "id": "KVavUIO4Zk",
        "forum": "oXYZJXDdo7",
        "replyto": "oXYZJXDdo7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5008/Reviewer_6EhH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5008/Reviewer_6EhH"
        ],
        "content": {
            "summary": {
                "value": "The paper explores text generation, specifically by creating a larger vocabulary which additionally consists of phrases extracted from a large corpus via certain rules, and whose representations are formed by a transformer model encoding the wider context they appear in. \nWhen decoding the model is then able to either generate standard tokens, or longer phrases. \nResults are presented on open generation and question answer generation tasks, showing that the proposed model is able to perform more accurately than 3 other recent retrieval based baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* Solid set of empirical results are presented showing the model performs well on the tested tasks. \n* Comparisons against recent baselines are given. \n* Additional phrase representations (produced by the same model used in training) are able to be added at inference time, with the decode model able to operate with these extra, new phrases. \n* The method removes the inference-time dependence on document retrieval that other retrieval augmented generation papers have. It does move this phrase creation process (along with the embedding of these) to training time."
            },
            "weaknesses": {
                "value": "* The claims about inference speed are \n* Can examples or further clarification be given for the 3.1 sentence \"enhancing the accountability of the output\"? This isn't clear, at least to me. \n* There are a lot of heuristics in extracting the phrases. This may not be easy to repeat, or result in the same level of gains on other datasets or related problems. \n* The decoding method seems very custom. Forcing a limited use of phrases, and blending top-k and top-p sampling. What happens if you just arg-max decode from the resulting model? Does it emit phrases way too often?\n* Distributional sparsity  -- this section is not very clear. \n* Is the likelihood estimation of summing paths well motivated? I'm not sure this is principled, but open to this being further justified. \n* \"For efficiency issues\" in 4.1, does this mean for stability? Keeping the embeddings (of tokens and phrases) means the problem is stable I presume. I think this needs more explanation however. \n* The numbers in table 1 are not described."
            },
            "questions": {
                "value": "* Was the vanilla LM trained with the phrase extended vocabulary? Or was this just using the gpt2 tokenisation alone? What happens if you  do this, rather than blending the contrastive phrase loss with the common CE loss?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5008/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698167089124,
        "cdate": 1698167089124,
        "tmdate": 1699636489086,
        "mdate": 1699636489086,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XGKBMwrdEL",
        "forum": "oXYZJXDdo7",
        "replyto": "oXYZJXDdo7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5008/Reviewer_uQM4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5008/Reviewer_uQM4"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a novel method for language modeling that instead of generating tokens, retrieves phrases from a phrase-based index. This differs a lot from standard language models, which generate text by selecting tokens from a fixed, finite, and standalone vocabulary. Furthermore, this new approach leverages a more balanced encoding architecture for both the input and target tokens, as opposed to a single token embedding layer on the target side employed in standard language models. Moreover, their paradigm is the first that performs text generation through direct phrase retrieval, steering away from common 2-staged pipeline approaches, and thus removing the dependence on document retrieval and achieving lower latencies.\nThe authors shed light on how to determine the training oracles that allow this kind of training, and they propose to initialize them using linguistic heuristics. Also, in order to allow the model to adjust its own generation paths based on the capabilities it has acquired, they also bootstrap the oracles through iterative self-reinforcement, which gradually refines the oracles with each iteration by transitioning from imitating the oracles to reinforcing its own preferences.\nIn this new paradigm, text generation is achieved by copying retrieved phrases corresponding to constituent units in a syntactic parse tree, but the model still has the ability to generate individual tokens.\nThe effectiveness of their models is validated on various downstream tasks, including open-domain and domain-specific question answering, as well as open-ended text generation, attaining substantial improvements over standard LMs and several retrieval-augmented baselines. Transitioning to phrase retrieval improves interpretability and factuality on text generation tasks, as the semantics of phrases are enhanced by their surrounding contexts, and each retrieved phrase can be traced back to its original document. Finally, enlarging the phrase index during inference, and the plug-and-play feature of the index are shown to be effective and efficient methods for boosting the model's performance and adapting to out-of-domain distributions respectively, without any further training."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- A novel approach for retrieval augmented generation \n- Holistic evaluation not only by measuring the fluency in open-ended text generation but also by carrying out comprehensive evaluation in a wide range of knowledge-intensive tasks, such as open-domain question answering.\n- Plug-and-play feature of the phrase index, as a way of adapting to out-of-domain distributions (such as the Medical domain) by simply changing/extending the phrase index with a domain-specific index without any further training.\n- Paper is generally well written and easy to follow\n- Good explanation of how standard LLMs can be viewed as dual-encoding matching networks connecting different prefixes and tokens, and shedding light into the architecture imbalances between the prefix and the target encoders."
            },
            "weaknesses": {
                "value": "Weaknesses:\n- More implementation details regarding the size of the phrase index, etc would be good to have in the paper.\n- The work might also benefit from some discussion regarding scalability of the phrase index\n\nMinor suggestions:\n\n- As Figure 1 is the main overview of the approach proposed in the paper, a more detailed footnote would be appreciated.\n- Section 6 \"Results\" wouldn't be better under subsection 5.2.2, as it reflects on results from the Open-Ended Text Generation experiments.\n- Typos: Section 2, line 2. \"The\" after \"Hence, \" should be in lower-case.\n\nMissing references:\n- Minjoon Seo's work on phrase index QA: https://arxiv.org/pdf/1804.07726.pdf, https://arxiv.org/pdf/1906.05807.pdf\n\n\nOverall I think this is an interesting method and the authors perform extensive experimentation to empirically justify their approach"
            },
            "questions": {
                "value": "- Could you discuss any potential limitations or failure cases of the model, providing insights into scenarios where the proposed approach might not perform as effectively?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5008/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698760538227,
        "cdate": 1698760538227,
        "tmdate": 1699636488980,
        "mdate": 1699636488980,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BsX3IuseMx",
        "forum": "oXYZJXDdo7",
        "replyto": "oXYZJXDdo7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5008/Reviewer_iZXf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5008/Reviewer_iZXf"
        ],
        "content": {
            "summary": {
                "value": "This paper combines retrieval with test generation and introduces an approach at retrieves context-aware phrases from a database of documents for generation. They use a set of linguistics heuristics combined with a bootstrapping method to extract phrases. The authors have done studies to show the effectiveness of their method and perform ablation study on the effect of different elements. They also study the inference speed of their approach and compare it with other approaches."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is well written and easy to follow.\n- Their approach on text generation and selecting phrases is novel and introduces an interesting approach to text generation.\n- The authors study the effectiveness of their approach well and provide comparisons with other approaches.\n- Their zero-shot results on knowledge intensive tasks is convincing of the effectiveness of their approach."
            },
            "weaknesses": {
                "value": "- Lack of any human evaluations: Although there are automatic metrics for text generation, there still a need to have humans judge the generation.\n- The paper does not provide deep insights into the observed results. For example, Section 6, Main Results, related to Table 4, it is not clear why the MAUVE score has such a huge jump for their method, or why finetuning the base model drops this score by a lot."
            },
            "questions": {
                "value": "- What corpus is used to finetune the base model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5008/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5008/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5008/Reviewer_iZXf"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5008/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699212178486,
        "cdate": 1699212178486,
        "tmdate": 1699636488899,
        "mdate": 1699636488899,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ODlNfBYq4Z",
        "forum": "oXYZJXDdo7",
        "replyto": "oXYZJXDdo7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5008/Reviewer_Uubi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5008/Reviewer_Uubi"
        ],
        "content": {
            "summary": {
                "value": "Retrieval augmented generation models are very powerful  in making generation more attributable and trustworthy. The proposed approach in the paper belongs to this family. It is inspired from CoG (Lan et al.)  that retrieves phrases from similar contexts, however, unlike CoG, it doesn\u2019t employ a two-stage pipeline, specifically document retrieval followed by grounded phrase extraction. The proposed approach removes the dependence on document retrieval. \n\nInterestingly, the authors propose to use linguistics-motivated heuristics to initialize the training oracle phrases, followed by a bootstrapping mechanism through self-reinforcement to refine the oracle with each iteration. This linguistically inspired approach could be very useful in providing meaningful attributions to their sources. \n\nThe experiments on Open-book qa and open ended generation show consistent improvements over competitive baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposed approach seems very interesting and will be useful to the generation community. As I mentioned earlier, the linguistically inspired approach could be very useful in providing meaningful attributions to their sources. \n\nStrong results on a variety of benchmarks from Open book qa and open ended generation tasks."
            },
            "weaknesses": {
                "value": "The authors proposed a very interesting approach but I felt a lot of important details are missing. Please see my questions/comments below. It is unclear whether or not the code will be released from this work.\n\nAnother weakness of the work I believe is that this approach will not be robust to languages or domains where our syntactic parsing capabilities are limited."
            },
            "questions": {
                "value": "\u201ceach phrase possesses a relatively complete and well-defined meaning\u201d -> Will this approach be not generalizable to languages and domains where the availability of syntactic parsers is limited? Also is it feasible to annotate the whole training set?\n\n\u201cIncorporating high-frequency phrases can significantly increase the total number of phrases, leading to an extremely large candidate pool\u201d -> Won\u2019t the low-frequency phrases significantly increase the size of the candidate pool? \n\nSecond paragraph under \u201cSemantic similarity\u201d: I felt lots of details were missing here to better understand the quality of phrases, and the feasibility of the proposed approach. The Appendix A do not provide all necessary details. Is this done on the pretraining corpus? What trivial constituents were dropped out and why (some examples would help)? \n\nSec 3.2.2: I found the explanation a bit confusing. Could you add an algorithm and/or an example demonstrating the algorithm? \n\n\u201cIf no such phrase is found, we retain the previous target.\u201d -> When would this occur? When the candidate pool is empty? \n\n\u201cwe also add the token vocabulary to our phrase table\u201d -> Are they subword units? What is the vocabulary? \n\n\u201cWe train our model on the training set of MiniPile2 (Kaddour, 2023)\u201d -> What is this dataset? Is it a pretraining set of finetuning set? How are they used during training? This dataset is discussed again in 5.2.\n\n\u201cNote that the sum of all possible paths can be computed efficiently using dynamic programming with time complexity O(n 2 ), where n represents the number of tokens in the text\u201d -> Will this be limiting for long form outputs? \n\nTable 1: Are the numbers for baselines taken from the respective papers or are they reproduced by the authors?\n\nSec 6: Results: This should be Sec 5.2.2?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5008/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699314735764,
        "cdate": 1699314735764,
        "tmdate": 1699636488767,
        "mdate": 1699636488767,
        "license": "CC BY 4.0",
        "version": 2
    }
]