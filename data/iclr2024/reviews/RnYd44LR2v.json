[
    {
        "id": "ttuFrhuJKV",
        "forum": "RnYd44LR2v",
        "replyto": "RnYd44LR2v",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8245/Reviewer_YQdD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8245/Reviewer_YQdD"
        ],
        "content": {
            "summary": {
                "value": "This paper provides a large-scale benchmark for evaluating the adversarial robustness of models on datasets under distribution shift (OOD robustness). It supports 23 dataset-wise shifts (e.g., image corruptions) and 6 threat-wise shifts (i.e., different adversarial attacks) and is used to assess the OOD robustness of 706 pre-trained models. Based on the experimental analysis, this work has some insightful findings, e.g.,  1) robustness degrades significantly under distribution shift, 2) ID accuracy (robustness) strongly correlates with OOD accuracy (robustness) in a linear relationship. Based on the finding of the linear relationship, the authors propose to predict the OOD performance of models using ID performance. Finally, some explorative studies such as data augmentation have been conducted and demonstrated to be useful to improve the OOD robustness."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Large scale evaluation with diverse distribution shift. OODRobustBench supports 29 types of distribution shifts and 706 types of models that can provide a good platform for researchers to further analyze the OOD robustness problem.\n\n- Some interesting findings, e.g., adversarial training can boost the correlation between the ID accuracy (robustness) and OOD accuracy (robustness), and no evident correlation when ID and OOD metrics misalign.\n\n- Useful guidance. I like the part of Section 5 that explores the usefulness of using multiple methods (even though they are from existing works) to enhance the OOD robustness.\n\n- This benchmark systematizes the question of whether robustness acquired againt a specific threat model transfers to other threat models. Although this is not the first work that investigate this aspect, this is to my knowledge the first large-scale benchmark considering this question."
            },
            "weaknesses": {
                "value": "- Only one seen attack method (MM5) has been used for the evaluation, which is not practical for robustness analysis. \nSome findings are expected or have already been studied in existing works. \n\n- Some works [1, 2, 3] studied the linear correlation between ID performance and OOD performance, the authors need to add some related discussions. Instead of the finding of the correlation between ID accuracy (robustness) and OOD accuracy  (robustness) that is expected, the finding that there is a weak correlation between ID accuracy and OOD robustness for ImageNet is more attractive and needs a more detailed explanation.\n\n- Section 4.3 analyzes the upper limit of OOD performance using the linear correlation. However, the authors did not consider factors that could further improve the limit such as OOD generalization methods which makes the conclusion not that convincing. Instead of analyzing the limits,  it is better to try more model accuracy prediction methods such as [1, 2, 3] to evaluate their effectiveness in assessing OOD robustness.  \n\n[1] Agreement-on-the-Line: Predicting the Performance of Neural Networks under Distribution Shift, Neurips 2022.\n[2] Leveraging Unlabeled Data to Predict Out-of-Distribution Performance, ICLR 2022. \n[3] Are labels always necessary for classifier accuracy evaluation? CVPR 2021\n\nThis is a borderline paper. Even though this paper provides the first benchmark for OOD robustness evaluation, there are some concerns that need to be addressed, the limited seen attack methods used, some findings already revealed by existing works, and lack of the study of OOD generalization methods."
            },
            "questions": {
                "value": "1. Compared to existing works [1, 2, 3], can you please summarize the new findings from OODRobustBench?\n2. After improving the OOD robustness using the methods in Section 5, do you think the findings revealed by the previous sections will change?\n3. Do you think other OOD generalization methods like unsupervised representation learning for OOD generalization [4] can help increase the limit of OOD robustness?\n\n[1] Agreement-on-the-Line: Predicting the Performance of Neural Networks under Distribution Shift, Neurips 2022.\n[2] Leveraging Unlabeled Data to Predict Out-of-Distribution Performance, ICLR 2022. \n[3] Are labels always necessary for classifier accuracy evaluation? CVPR 2021\n[4] Towards Out-Of-Distribution Generalization: A Survey. Arxiv"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8245/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8245/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8245/Reviewer_YQdD"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8245/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698678482546,
        "cdate": 1698678482546,
        "tmdate": 1699637025242,
        "mdate": 1699637025242,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FXpdmEzbci",
        "forum": "RnYd44LR2v",
        "replyto": "RnYd44LR2v",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8245/Reviewer_1Jrx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8245/Reviewer_1Jrx"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the out-of-distribution (OOD) generalization of adversarial robustness, when there is a shift on either the test data or threat model of the adversarial perturbation. The authors built a benchmark for such evaluation and conducted a comprehensive evaluation of many models. The paper shows a linear trend between the in-distribution (ID) performance and OOD performance on many models under adversarial attacks, but there are also models showing stronger robustness beyond the linear prediction in Section 5."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "* The paper presents a benchmark (OODRobustBench) for evaluating the adversarial robustness under distribution shifts (either a shift on the test data or threat model of the adversarial perturbation), based on existing OOD test sets and variants of adversarial perturbations. \n* The paper conducted a comprehensive evaluation on many models. \n* The paper showed a linear trend between the ID adversarial robustness and the OOD adversarial robustness on most of the models, which is consistent with the linear trend observed in prior works (Taori et al., 2020, Miller et al., 2021) on dataset shifts without adversarial attack."
            },
            "weaknesses": {
                "value": "* This work is a straightforward combination of existing evaluations with little new contribution or understanding:\n  * For the evaluation with data shifts, compared to existing works on evaluation with OOD datasets, this work simply adds existing adversarial attacks. Methods and conclusions are almost the same as previous works (Taori et al., 2020, Miller et al., 2021) on the linear trend.\n  * For the evaluation on threat shifts, this work is almost the same as existing works mentioned in the \"robustness against unforeseen adversarial threat models\" paragraph in Section 2 but only adds more existing models. \n\n* Some discussions on the experiments are not very accurate:\n  * \"Surprisingly, VR also clearly boosts effective robustness under dataset shift\neven though not designed for dealing with these shifts\" and \"Advanced model architecture significantly boosts robustness and effective robustness under both types of shift over the classical architecture\": I don't agree these \"significantly boost\" the effective robustness. The gains are only around 1%~2%, which are not larger than the normal variations between different models in the linear fit (Figure 1).\n  * \"Training with extra data boosts both robustness and effective robustness for both dataset and threat shifts compared to training schemes without extra data (see Fig. 6a). The improved effective robustness suggests that this technique induces extra OOD generalizability.\" It is already known that altering the training data can interfere with the traditional effective robustness evaluation rather than truly improve effective robustness (Shi et al., 2023).\n  * In Section 5, the authors rename the existing effective robustness from previous works (which have been widely adopted) into \"effective accuracy\" while redefine \"effective robustness\" to be the effective robustness under adversarial attacks. This is confusing. I would suggest the authors keep the original definition for effective robustness but give a new name for the particular effective robustness in this work (e.g, adversarial effective robustness). \n\nShi, Z., Carlini, N., Balashankar, A., Schmidt, L., Hsieh, C. J., Beutel, A., & Qin, Y. (2023). Effective Robustness against Natural Distribution Shifts for Models with Different Training Data. arXiv preprint arXiv:2302.01381."
            },
            "questions": {
                "value": "* How are the results and conclusions in this paper fundamentally different from those in existing works? (See the weaknesses above.) What are the new implications of this work, beyond those already known in existing works (vulnerability against distribution shifts, linear trend, etc.)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8245/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698807587170,
        "cdate": 1698807587170,
        "tmdate": 1699637025116,
        "mdate": 1699637025116,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EhTcqqC1jJ",
        "forum": "RnYd44LR2v",
        "replyto": "RnYd44LR2v",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8245/Reviewer_ogvR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8245/Reviewer_ogvR"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the adversarial robustness of image classifiers in presence of out-of-distribution (OOD) tasks. Given a model which has been (adversarially) trained on a specific dataset to be robust to a chosen attack (in-distribution task), the paper suggests to test how its robustness behaves when using either images from a different distribution (OOD dataset) or a different type of attack (OOD threat model): this provides an overview of generalization of robustness. Moreover, evaluating many existing and newly trained classifiers, the paper provides insights on which are the most relevant factors to achieve OOD robustness, which might be used to develop techniques for more robust models."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Studying the generalization of adversarial robustness is a relevant topic (for example, as at test-time attackers are not limited to use the same attack seen during training), which has received only limited attention by prior works.\n\n- The paper provides extensive evaluations on many classifiers spanning different datasets and (seen) threat models. This gives clear trends and insights about how to improve future robust models for better generalization."
            },
            "weaknesses": {
                "value": "- Similar analyses are already present in prior works, although on a (sometimes much) smaller scale, and then the results are not particularly surprising. For example, the robustness of CIFAR-10 models on distributions shifts (CIFAR-10.1, CINIC-10, CIFAR-10-C, which are also included in this work) was studied on the initial classifiers in RobustBench (see [Croce et al. (2021)](https://arxiv.org/abs/2010.09670)), showing a similar linear correlation with ID robustness. Moreover, [A, B] have also evaluated the robustness of adversarially trained models to unseen attacks.\n\n- A central aspect of evaluating adversarial robustness is the attacks used to measure it. In the paper, this is described with sufficient details only in the appendix. In particular for the non $\\ell_p$-threat models I think it would be important to discuss the strength (e.g. number of iterations) of the attacks used, since these are not widely explored in prior works.\n\n[A] https://arxiv.org/abs/1908.08016  \n[B] https://arxiv.org/abs/2105.12508"
            },
            "questions": {
                "value": "- See the weaknesses mentioned above.\n\n- Many non $\\ell_p$ attacks have been proposed (see e.g. [A, B]). Is there a specific reason for the choice of those used in the paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8245/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698920290638,
        "cdate": 1698920290638,
        "tmdate": 1699637024976,
        "mdate": 1699637024976,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1sdPLF35P2",
        "forum": "RnYd44LR2v",
        "replyto": "RnYd44LR2v",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8245/Reviewer_UQFk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8245/Reviewer_UQFk"
        ],
        "content": {
            "summary": {
                "value": "The proposed approach combines the research direction of adversarial robustness and domain shift into a single benchmark: OODRobustBench. It measures how the adversarial robustness of networks trained on in-distribution data varies when evaluated on test data under distribution shift. It provides an evaluation over 706 robust models to draw insights into correlation of OOD and in-distribution (ID) robustness, upper limit on OOD robustness, and effect of training setup on OOD robustness."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper thoroughly explores the intersection of adversarial robustness and OOD generalization works in developing the OODRobustBench benchmark. It considers multiple ablations across ID and OOD robustness and performs 60.7K adversarial evaluations.The paper is also well written and very easy to parse."
            },
            "weaknesses": {
                "value": "While the proposed evaluation is rigorous, I believe that approach fall-short of being a standardized benchmark. It\u2019s unclear what criterions are allowed for robust models. Would the benchmark include all adversarial robust approaches, such as preprocessing based defenses. If yes, how would the trend with robustness in such approaches correlate with OOD robustness? \n\nIs the specific trend in natural accuracy between IN and OOD data particular to robust models? Can authors provide some results/citation on how the correlation between IN-OOD accuracy correlated with IN-ODD accuracy on non-robust models?"
            },
            "questions": {
                "value": "In figure 1.4, interestingly ID unseen robustness is higher than the seen robustness at lower robustness levels. This result is apparently counter-intuitive, as the trend quickly diminishes, at higher robustness levels. Can authors shed more light on this phenomenon.\n\nCan authors also provide a concrete comparison on how the proposed benchmark is different from robustbench, in particular in-terms of benchmarking in-distribution robustness. \n\nTo aggregate performance across different attacks in each group (OOD_d/OOD_t), shouldn\u2019t harmonic mean be used to achieve better stability."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8245/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8245/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8245/Reviewer_UQFk"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8245/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699507604363,
        "cdate": 1699507604363,
        "tmdate": 1699637024817,
        "mdate": 1699637024817,
        "license": "CC BY 4.0",
        "version": 2
    }
]