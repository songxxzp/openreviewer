[
    {
        "id": "KvFU9J8AwO",
        "forum": "WQYHbr36Fo",
        "replyto": "WQYHbr36Fo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission945/Reviewer_VesV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission945/Reviewer_VesV"
        ],
        "content": {
            "summary": {
                "value": "Submission 945 presents a visual self-supervision method suitable for dense (/local) tasks such as detection/segmentation, etc. It motivates itself by presenting demonstrations that show that attention maps (/point affinities) do not localize well to object-parts and this is due to the false positives generated by current self-supervision methods.\n\nIt presents a variant of CutMix for dense self-supervision that mixes tokens from the input image with tokens from an external image. The mixing strategy is developed such that a token from the input image is largely surrounded by tokens from the output image so that the input token is positionally \u201cout of context\u201d. It then presents a regularizer for any visual self-supervision loss such that features extracted from the \u201cin context\u201d and \u201cout of context\u201d tokens (taken from the input image) are similar."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The presented work is reasonably thorough in its experiments and generality. In particular, I like that it presents a rationale for both ViTs and CNNs.\n- While confusingly presented, the high-level idea of asking token-wise features to be invariant to some of their surrounding tokens is a simple idea and appears to lead to better attention maps and downstream performance."
            },
            "weaknesses": {
                "value": "(in no particular order)\n\n### Poor presentation\nUnfortunately, the unclear writing and figures significantly dampened any enthusiasm for this paper and it took multiple repeated readings to get a high-level sense of what is proposed. As a few examples,\n- Paragraph 3 of the Introduction only makes sense if you already know the entire method.\n- Figures 1(a) and 2(a) are really hard to understand and are used to motivate the work. For example, what do the red dashed lines, the arrows, and the \u201cViTs-based measuring pipeline\u201d indicate in 2(a)?\n- I don\u2019t understand the written presentation of the mask generation (Sec. 4.1.1, par. 1) at all and its associated algorithm in the appendix uses undefined notation that is hard to follow (if it is defined elsewhere, please also add it to the caption). As a result, figure 4 does not immediately follow either.\n\nIMO the paper requires a significant revision for clarity.\n\n### Engineered token mixing has been done previously:\nThe proposed method has two main contributions: a cutmix style augmentation at the token level and a self-supervised loss leveraging that augmentation. However, while presented as new here, mixing tokens from different images in a carefully engineered way has been done before in TokenMix (ECCV\u201922, https://arxiv.org/abs/2207.08409 ) in the context of supervised classification and some other papers that follow up on it. Please discuss these works and clarify any differences, so as to better contextualize the key novel contribution of the self-supervised loss function that takes advantage of token mixing here.\n\n### Unclear motivation and relationship to current work:\nIn my reading, the main motivation of this work is achieving higher quality part-level attention maps by reducing the dependence between a token\u2019s features and its surroundings. It is then unclear to me if this strategy can then capture long-range nonlocal dependence \u2013 could you please comment on this point and clarify if I misunderstood?\n\nMoreover, in recent work, DinoV2 (https://arxiv.org/abs/2304.07193) demonstrated that high-quality part-level representations can be learned by simply scaling up model and dataset sizes without considering the \u201ccoupling\u201d between objects and background. I am not asking for comparisons, but I would like the response to briefly clarify the motivation of the proposed strategy in this context."
            },
            "questions": {
                "value": "### Suggestions:\n- Please clarify the differences between TokenMix and similar token mixing works and this paper.\n- Please revise and improve the writing and presentation of the first four sections of this paper to make it more immediately accessible.\n- Please briefly discuss the motivation for the decoupling regularizer in the context of existing methods such as DinoV2 achieving high-quality part-level attention maps without considering the \u201ccoupling\u201d phenomenon.\n- Figure 5 on page 7 is what finally made the method click for me \u2013 please move it to earlier in the paper.\n\n### Minor questions:\n- Experimental clarifications would be beneficial: Why is 800 epochs of pretraining on COCO specifically chosen for all methods? How were the hyperparameters tuned for the baselines? Also, most of the experiments do not mention splits.\n- Apart from the proposed masking, there appears to be no other augmentations mentioned and there is no code. Were standard augmentations such as jitters, flips, blur, etc. (https://github.com/bytedance/ibot/blob/main/main_ibot.py#L574) also used? Did the augmentation strategy match the existing methods on which decoupling was applied?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission945/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission945/Reviewer_VesV",
                    "ICLR.cc/2024/Conference/Submission945/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission945/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698706174775,
        "cdate": 1698706174775,
        "tmdate": 1700721545509,
        "mdate": 1700721545509,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "50Dx1WjK2T",
        "forum": "WQYHbr36Fo",
        "replyto": "WQYHbr36Fo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission945/Reviewer_GFda"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission945/Reviewer_GFda"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a new method called Region Collaborative Cutout for self-supervised learning to alleviate the object coupling issue. This simple and straightforward method achieves evident gains over previous methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is clearly written, and it is easy to catch the main motivation and solution. And I think the proposed method is well motivated.\n\n2. Multiple previous methods are used as baselines to build the proposed Region Collaborative Cutout upon, and non-trivial improvements are observed. Besides, the method is proved effective for both CNNs and ViTs.\n\n3. The ablation studies are comprehensive and convincing."
            },
            "weaknesses": {
                "value": "I am not very familiar with the research line of SSL. So I have no further suggestions for this paper. Generally, I like this simple yet effective method. It further highlights that constructing appropriate positive pairs by delicately designed strong augmentations is important.\n\nHowever, one of my slight concerns is about the whole area of SSL since DINOv2 was released. It is pre-trained on extremely large-scale and curated data with several practical SSL optimization targets. It is very strong in many applications, such as retrieval, segmentation, and detection. Even with a frozen DINOv2 backbone, we can achieve state-of-the-art performance in some challenging tasks. Therefore, could the authors discuss the position of this submission by taking the recent SSL trend into consideration?"
            },
            "questions": {
                "value": "No further questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission945/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698904381578,
        "cdate": 1698904381578,
        "tmdate": 1699636021127,
        "mdate": 1699636021127,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2K5IeP2wz1",
        "forum": "WQYHbr36Fo",
        "replyto": "WQYHbr36Fo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission945/Reviewer_wwZs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission945/Reviewer_wwZs"
        ],
        "content": {
            "summary": {
                "value": "This paper considers the problem of information leakage from the neighboring contextual regions in dense self-supervised learning. The contributions of this paper include the identification and confirmation of the coupling phenomenon when pairs have a limited overlap, the design of a decoupling branch, a novel region collaborative cutout augmentation, and the effectiveness of the proposed approach in the dense self-supervised learning frameworks. This approach can be applied to both CNNs and ViTs as the approach is only related to the augmentation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper is well written and easy to follow.\n\nThe proposed method is simple and can be directly combined with existing dense self-supervised learning without introducing additional loss types.\n\nThe proposed method, RCC and decoupling branch, are demonstrated to be effective with several existing self-supervised learning methods in the experiments."
            },
            "weaknesses": {
                "value": "While the augmentation in Fig. 5 is easy to understand, the random masking in Fig. 2(a) is vague in terms of the illustration purpose.\n\nThe main text is expected to be self-contained. However, there are several cases where content is put in the appendix. For example, the figure 7(a) is useful to understand the definition of variables in equation 4, however, is put in the appendix. One possible way to address this problem is to merge Fig. 7(a) with part of Fig. 3. Again, the Alg 2 is another example for the understanding of the proposed RCC (region collaborative cutout).\n\nThe original contribution of the approach may be limited as the proposed RCC could be viewed as the combination of thresholding the cutout ratio within a region and filling the cutout region with background images. This is different from the Cutout, but more like combining existing strategies.\n\nThe order of Tables 4-6 does not match the appearance in the text."
            },
            "questions": {
                "value": "Context within the region. The shape of each object varies and is irregular. And the region is defined by a bounding box (at least in object detection task), There would be context information in the bounding box. How to measure or address the coupling or leakage for this part of information?\n\nThis method would introduce another hyperparameter, the threshold of cutout ratio. How to set this across different datasets?\n\nAblation study. The numbers about COCO Det. in Table 6 do not match the numbers reported in Table 2 for iBOT. This is because the training epochs are different. If we view the results in Table 2 as the converged one, the comparison made in Table 6 may not lead to a convincing conclusion. While three experiments are presented in this section, a more interesting ablation study would be the effectiveness of the decoupling branch. \n\nWhile the authors claim that the proposed method can be combined with existing SSL methods, there is a concern that the proposed method may not work well for the method with contrastive loss within each batch. The reason for this is that the decoupling branch serves a similar purpose. However, there may be additional contribution as the losses are computed at different levels with different masks."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission945/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698964079309,
        "cdate": 1698964079309,
        "tmdate": 1699636021055,
        "mdate": 1699636021055,
        "license": "CC BY 4.0",
        "version": 2
    }
]