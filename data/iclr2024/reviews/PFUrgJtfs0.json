[
    {
        "id": "PqjmQ10Sqg",
        "forum": "PFUrgJtfs0",
        "replyto": "PFUrgJtfs0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5390/Reviewer_aM1T"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5390/Reviewer_aM1T"
        ],
        "content": {
            "summary": {
                "value": "In this work, authors systematically dissect 9 popular hybrid Transformer networks on two representative organ and pathology segmentation datasets and explore whether Transformers are still beneficial under these challenging conditions."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper is easy to read."
            },
            "weaknesses": {
                "value": "1.\tThere is no contribution or novelty. This is more like comparing 9 frameworks.\n2.\tExperimental design has some flaws. There are so many transformer networks and picking 9 out of those should have a valid assumption, which is missing in the paper.\n3.\tLacking literature review."
            },
            "questions": {
                "value": "1.\tOnly quantitative analysis is provided in the manuscript. What about qualitative analysis? Showing segmentation masks would help readers to understand which method performs well, especially when it comes to the medical AI domain, qualitative analysis is a must.\n2.\tA comparison of intermediate attention maps is missing. \n3.\tRepresentation similarity can be visualized as a heat map comparing network layers. This would give some understanding to the reader of how feature extraction works and whether it\u2019s similar across all (or part of) networks or not."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5390/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5390/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5390/Reviewer_aM1T"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5390/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698625587022,
        "cdate": 1698625587022,
        "tmdate": 1699636545631,
        "mdate": 1699636545631,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Si9Bd6eCvy",
        "forum": "PFUrgJtfs0",
        "replyto": "PFUrgJtfs0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5390/Reviewer_Z59e"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5390/Reviewer_Z59e"
        ],
        "content": {
            "summary": {
                "value": "Summary: This paper investigates thoroughly recent transformer architecture for medical image segmentation and challenges the recent trend of developing novel transformer architecture. The authors have systematically ablated different key components and studied their effect on the performance. They have concluded that transformers and their long-range dependency modeling are often not the critical components of the architecture."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Strengths:\n\n+ Good categorical benchmark on different transformer component\n\n+ Detailed analysis of their performance and representational behavior."
            },
            "weaknesses": {
                "value": "Major comments:\n\n- While the medical image segmentation task, the utility of the transformer, can be brought under scrutiny, this is not true for panoptic/instance segmentation and video segmentation, not only because of the data set size but also because of the fundamental difference in network architectures. This makes the criticism of transformers very specific to U-net-like models popular in the medical imaging community, which makes the paper relatively less appealing to the general image segmentation community.\n\n- While the paper quite convincingly points out flaws in the current practice of architectural design in medical image segmentation, the paper did not bring any new ideas to mitigate the issue, which remains a  major weakness and is hard to address within the rebuttal period. Hence, despite being a good review and investigative paper, I am not sure whether it is a good fit for ICLR.\n\n- The use of volumetric error overlap is confusing in concluding model behavior. Given that all models considered provide points estimate, how can the author assert that the apparent difference in model behavior is not a result of the underlying uncertainty? It will be good to know the volumetric error map between three runs of the same models as a reference to the uncertainty because the authors took the same approach for representational change measurement.  And how are the thresholds 0.95, 0.85, etc. chosen? Seems quite arbitrary."
            },
            "questions": {
                "value": "see weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5390/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698855535538,
        "cdate": 1698855535538,
        "tmdate": 1699636545522,
        "mdate": 1699636545522,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TnefOO6D4H",
        "forum": "PFUrgJtfs0",
        "replyto": "PFUrgJtfs0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5390/Reviewer_BnaE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5390/Reviewer_BnaE"
        ],
        "content": {
            "summary": {
                "value": "The study analyzed the effectiveness of nine Transformer-based models for segmenting medical images, focusing on two datasets centered on organ and pathology segmentation. It was found that convolutional layers are essential to these models' performance, whereas the transformer layers may not be as vital. Additionally, the researchers questioned the assumed significance of long-range dependencies\u2014a characteristic feature of transformer models\u2014in the context of medical image segmentation."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "In a probing study on the role of transformer models within medical imaging segmentation, the authors challenged their utility compared to traditional CNN architectures. They conducted a comparative analysis of nine cutting-edge architectures by substituting transformer blocks within these models. Their research unveiled interesting results, revealing minimal performance disparity between the original and modified models. This suggests that the transformer's capability may be underutilized in the medical imaging segmentation tasks evaluated."
            },
            "weaknesses": {
                "value": "The study's scope was confined to a narrow selection of segmentation datasets, and the ablation studies conducted were restricted in its current form."
            },
            "questions": {
                "value": "1. The authors' research on medical imaging segmentation with Transformer-based models, focusing on organ and pathology within specific datasets, may not capture the potential benefits of long-range dependencies in all medical imaging scenarios. For example, cardiac video segmentation may reveal different results due to the temporal dynamics involved. The authors could provide insights on whether their findings are applicable to such medical imaging tasks where Transformers might show utility. \n\n2. Can the authors clarify whether the transformer blocks were pretrained with natural image datasets. \n\n3. The study used nnU-Net as a benchmark for evaluating performance. Its status as an industry benchmark relies heavily on advanced data augmentation and meticulous hyperparameter optimization. The nine Transformer-based architectures assessed may not employ these sophisticated techniques. For an equitable comparison, it's crucial that all other variables, such as data augmentation protocols, are standardized across models. Without this uniformity, any observed differences in performance could be attributed to varying methodologies rather than inherent architectural distinctions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5390/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699148263612,
        "cdate": 1699148263612,
        "tmdate": 1699636545406,
        "mdate": 1699636545406,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6J3AUJplUP",
        "forum": "PFUrgJtfs0",
        "replyto": "PFUrgJtfs0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5390/Reviewer_kZHs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5390/Reviewer_kZHs"
        ],
        "content": {
            "summary": {
                "value": "This paper analyzes 9 transformer-based networks for medical image segmentation over two public datasets and shows the limitations of these transformer networks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper systemically analyzes the roles of transformer encoders in medical image segmentation tasks.\n\n2. The authors show that once more time adding transformers layers blindly is not necessarily linked to superior performance, especially in medical image analysis.\n\n3. The authors conduct quite extensive experiments."
            },
            "weaknesses": {
                "value": "1. The authors only tested on two public datasets, which might not be convincing enough for an investigative paper to validate the claims.\n\n2. Some of the findings by the authors were already identified in the ViT paper in 2020, such as the Transformer will be better when facing a larger dataset but might be worse when having a small dataset such as in medical imaging.\n\n3. The scope of the paper is more investigative rather than innovative, which makes it look more like a technical report/survey rather than a research paper."
            },
            "questions": {
                "value": "1. The success of transformers is generally due to having less inductive bias and intuitively any application that does not benefit from such fact might not find having such layers helpful. From the ViT paper, the size of the dataset also matters a lot in showing the performance of transformers. The findings 2) and 3) seem to be the direct translation of the above two points, and thus might not be super novel and meaningful.\n\n2. Observation 2 in section 3.1 might not be too meaningful since convolutional blocks are all removed in up/down sampling paths. Replacing transformer blocks with convolutional blocks might be more fair.\n\n3. What does the tick/cross mean in the right table of Figure 3? It would be more clear to add a description directly in the caption.\n\n4. It would be more interesting to discuss how sensitive the transformers are to hyperparameters and summarize the common practice of selecting a reasonable set of hyperparameters."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5390/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5390/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5390/Reviewer_kZHs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5390/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699199230278,
        "cdate": 1699199230278,
        "tmdate": 1699636545302,
        "mdate": 1699636545302,
        "license": "CC BY 4.0",
        "version": 2
    }
]