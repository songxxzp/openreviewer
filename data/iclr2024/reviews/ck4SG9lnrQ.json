[
    {
        "id": "o3qNLe246y",
        "forum": "ck4SG9lnrQ",
        "replyto": "ck4SG9lnrQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7312/Reviewer_ZnNq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7312/Reviewer_ZnNq"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a  Chinese multi-task benchmark dataset CMMLU  for better evaluating the language understanding ability of LLMs in the context of Chinese. Compared to previous benchmark datasets, besides the general tasks, CMMLU consists of many Chinese-specific tasks.\n\nMeanwhile, this work has also conducted a lot of experiments to check the performance of the 20 most popular non-Chinese-specific and Chinese-specific LLMs.  The experimental results provide a good reference for developers to choose the LLM in the context of Chinese."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. CMMLU is specifically designed for evaluating Chinese LLMs. It not only consists of general natural language understanding tasks, but also some region-specific tasks such as Chinese driving rules, food culture, and qualifications. Thus, CMMLU can better reveal the real LLM performance in the Chinese scenarios.\n\n2. This work invested many efforts in collecting non-publicly available questions to reduce the possibility that the collected questions have already been learned by LLMs.\n\n3. This work has evaluated many multilingual LLMs and many Chinese LLMs at the same time. Meanwhile, the authors also compare the best Chinese LLM Baichuan2-13B with the best LLM GPT4 by subjects. This comparison can answer the question of why we need Chinese LLMs/benchmarks in the Chinese scenarios.\n\n4. Many deep analyses have shown many interesting and useful findings."
            },
            "weaknesses": {
                "value": "1. Although this work is technically sound and solid, CMMLU lacks enough novelty or other special contributions.  The major highlight is that CMMLU consists of some Chinese-specific tasks. This is more or less like an A+B incremental work.\n\n2. All questions are formatted as multiple-choice with 4 choices.   This may make it difficult to comprehensively test the performance of LLMs.\n\n3. The experimental methodology of most experiments is language-agnostic.  It only simply compares general LLMs vs Chinese LLMs and non-Chinese-specific tasks vs  Chinese-specific tasks.  I think more experiments should be deeply combined with the Chinese cultural and Chinese linguistic characteristics.\n\n4. This work needs to analyze the correlation between the performance reported by CMMLU and the real performance measured in the representative downstream NLU tasks. Otherwise, it is difficult to determine whether CMMLU can reflect the NLU performance of a LLM."
            },
            "questions": {
                "value": "Besides the questions in Weakness, there are some minor questions:\n\n1) What form will this dataset be released in the future?\n\n2) Besides the Chinese-specific tasks and data source, is there any other Chinese-specific feature that has been considered in CMMLU?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "This article involves data collection, especially non-public data collection.  This faces some copyright risks.\n\nHowever, I did not see how to address these ethics issues in this work."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7312/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7312/Reviewer_ZnNq",
                    "ICLR.cc/2024/Conference/Submission7312/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7312/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698489102733,
        "cdate": 1698489102733,
        "tmdate": 1700405478545,
        "mdate": 1700405478545,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dbnMm8L6xM",
        "forum": "ck4SG9lnrQ",
        "replyto": "ck4SG9lnrQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7312/Reviewer_qmrE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7312/Reviewer_qmrE"
        ],
        "content": {
            "summary": {
                "value": "This paper introduced a fully Sinicized Chinese test benchmark, CMMLU, specifically designed to evaluate the knowledge and reasoning capabilities of language models in a Chinese context. CMMLU covered 67 topics ranging from basic disciplines to advanced professional levels, with answers specific to the Chinese region."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper conducted extensive experiments, including on the proprietary GPT-4 (even though OpenAI consistently updated GPT versions without much fanfare). \n\nThe content was detailed and held significant practical value for the Chinese domain."
            },
            "weaknesses": {
                "value": "However, an LLM passing professional exams doesn't necessarily indicate its true capabilities, raising concerns about construct validity. \n\nThe crisis of research replication based on language models was severe, and the evaluation methods had limitations. \n\nAssessing the political biases inherent in the language models presented in the benchmark was challenging and required naturalistic observation."
            },
            "questions": {
                "value": "1. One concern I had was that this Chinese test benchmark did not include evaluation criteria for Chinese machine translation. Many studies are now focusing on evaluating the generalized machine translation capabilities of LLMs. Given the extensive work the authors did on this benchmark, how did authors view the evaluation criteria for Chinese translations?\n\n2. The outputs of LLMs were uncertain. Even a minor change in a prompt could lead to variations in the output. In light of this benchmarking paper, how did the authors perceive this issue? How should the benchmark address the inherent unpredictability of LLMs?\n\n3. Typically, the Chain-of-Thought method had proven successful on LLMs. However, this paper concluded that the Chain-of-Thought was not effective in enhancing model performance, which contradicted the feedback received from practical use of LLMs with the Chain-of-Thought. A more detailed analysis and explanation were requested.\n\n4. LLMs demonstrated strong In-Context Learning capabilities. It would be worth exploring whether adding appropriate knowledge to the prompt could answer benchmark questions to validate the benchmark's effectiveness.\n\n5. It was known that LLMs would respond cautiously to safety questions when posed in English. However, when asked in less common languages, they might provide bolder answers, potentially bypassing restrictions. Did the CMMLU safety benchmark consider addressing this phenomenon?\n\n6. How did the authors ensure that the proposed test benchmark was free from data contamination?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7312/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7312/Reviewer_qmrE",
                    "ICLR.cc/2024/Conference/Submission7312/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7312/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698685558994,
        "cdate": 1698685558994,
        "tmdate": 1700450837757,
        "mdate": 1700450837757,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VekoXgLTRE",
        "forum": "ck4SG9lnrQ",
        "replyto": "ck4SG9lnrQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7312/Reviewer_CUim"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7312/Reviewer_CUim"
        ],
        "content": {
            "summary": {
                "value": "This paper introduced CMMLU, a benchmark designed to assess the multi-task language understanding capabilities in Chinese. The authors ran the benchmark on various open-source and API-based models and performed extensive analysis to identify several factors that impact model performance and propose actionable directions for enhancing LLMs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. The CMMLU benchmark is very comprehensive, covering a wide range of subjects.\n\n2. The paper addresses the significant gap in evaluating Chinese language and cultural context understanding, a critical aspect given the dominance of English-centric benchmarks.\n\n3. The work can be very useful for Chinese LLM community.\n\n4. The paper provides an in-depth analysis of the performance of various LLMs, under different evaluation settings.\n\n5. The paper also provides very interesting findings in terms of chain-of-thought, SFT/RLHF, etc."
            },
            "weaknesses": {
                "value": "1. A human baseline is lacking for the benchmark. It'd be great to see what level of accuracy human can get on the benchmark.\n\n2. There's no discussion on the difficulty distribution of questions in each subset. A well designed benchmark or test should cover questions spanning all difficulty levels from the easiest to the hardest. It's unknown what the difficulty distribution is for each subset. If difficulty distribution is very centric (for example, all samples in a subset are all very easy or very hard), then models will be likely to get them all correct or all wrong, which cannot provide a **smooth** estimation of the model's ability. A non-smooth evaluation can also be related to the phenomenon of \"emergent ability\". See me question 2."
            },
            "questions": {
                "value": "1. In page 1, \"numerous tasks within CMMLU have answers speci\ufb01c to China, which may not be universally applicable or considered correct in other regions or languages.\". Do you think it would be a good idea to have questions **with answers that are not generally agreed upon worldwide** in the datasets? How many samples of this kind are there in the benchmark?\n\n2. When you evaluate open-source models, have you seen \"emergent ability\" in terms of model's size? More precisely, are there some tasks that can only be solved by a large model? If so, then what are the difficulty distributions of those tasks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7312/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7312/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7312/Reviewer_CUim"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7312/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699661857726,
        "cdate": 1699661857726,
        "tmdate": 1700432242489,
        "mdate": 1700432242489,
        "license": "CC BY 4.0",
        "version": 2
    }
]