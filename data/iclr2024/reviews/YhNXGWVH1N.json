[
    {
        "id": "jaYFnjLNfn",
        "forum": "YhNXGWVH1N",
        "replyto": "YhNXGWVH1N",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7227/Reviewer_zM1b"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7227/Reviewer_zM1b"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes LeanFlex-GKP, a new structured pruning method that builds on recent work in grouped kernel pruning (GKP). This method is a one-shot, post-train, and data-agnostic. The key idea is to make the number of groups flexible across layers rather than having dynamic operations during filter grouping or pruning stages."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper clearly identifies limitations of existing GKP methods in terms of complexity from dynamic operations and proposes a sensible alternative via flexible group counts.\n- The method delivers empirical results across a wide range of model architectures and datasets.\n- As a one-shot, post-train, data-agnostic technique with minimal hyperparameters, LeanFlex-GKP is far easier to use out-of-the-box compared to many existing methods."
            },
            "weaknesses": {
                "value": "- The contribution of this paper is limited. This paper seems an incremental work of TMI-GKP, and the main difference between this work and TMI-GKP is the group count evaluation.\n- It would be better if the authors can provide the experimental settings, such as hyperparameters they used for different models.\n- From my understanding, LeanFlex-GKP also includes dynamic choices of clustering schemes in each of its convolutional layers as TMI-GKP. I would appreciate it if the author can give results of the performance of LeanFlex-GKP and TMI-GKP. I can only get limited information from Table 1 to Table 5."
            },
            "questions": {
                "value": "- Could the benefits of dynamic group counts extend to other structured pruning granularities like filter or channel pruning?\n- Is there an optimal strategy for setting group counts or do they need to be exhaustively evaluated?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7227/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698596637707,
        "cdate": 1698596637707,
        "tmdate": 1699636860600,
        "mdate": 1699636860600,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ymbBiH0Ih6",
        "forum": "YhNXGWVH1N",
        "replyto": "YhNXGWVH1N",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7227/Reviewer_HKS2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7227/Reviewer_HKS2"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes that the best practice to introduce the dynamic operations to GKP is to make Conv2d flexible under an integral optimization, and proposes a one-shot, post-train, data-agnostic GKP method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The writing is clear.\n2. The background about Different Structured Pruning Granularities, Grouped Kernel Pruning, and Dynamic Structure Pruning is clearly introduced.\n3. The idea of making Conv2d flexible under an integral optimization is interesting."
            },
            "weaknesses": {
                "value": "1. The evaluation lacks comprehensiveness.\n\n- Baselines are restricted. A lot of related works about iterative structured pruning [1-10] and one-shot pruning [11-15] are not compared.\n\n- Most of the evaluation focuses on the ResNet and VGG architectures. It will be better if more model architectures are evaluated, especially small models like MobileNet, EfficientNet, and ShuffleNet.\n\n- In Section 4, it will be better if some insights can be provided, not only listing numbers.\n\n2. The Ablation Study is missing.\n\n3. The structure can be improved. The Introduction occupies too much space. The first 3.5 pages are all about the Introduction. \n\n4. The discussion about the related works is insufficient, such as iterative structured pruning [1-10], one-shot pruning [11-14], Grouped Kernel Pruning [15], and automatic pruning (with little-to-no hyper-parameter tuning) [2, 3].\n\n[1] EigenDamage: Structured Pruning in the Kronecker-Factored Eigenbasis\n\n[2] Automatic Attention Pruning: Improving and Automating Model Pruning using Attentions\n\n[3] Amc: Automl for model compression and acceleration on mobile devices\n\n[4] Layer-adaptive sparsity for the magnitude-based pruning\n\n[5] Chipnet: Budget-aware pruning with heaviside continuous approximations\n\n[6] Provable filter pruning for efficient neural networks\n\n[7] Accelerate CNNs from Three Dimensions: A Comprehensive Pruning Framework\n\n[8] EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning BT\n\n[9] DMCP: Differentiable Markov Channel Pruning for Neural Networks\n\n[10] GDP: Stabilized Neural Network Pruning via Gates with Differentiable Polarization\n\n[11] Only train once: A one-shot neural network training and pruning framework\n\n[12] Evolutionary multi-objective one-shot filter pruning for designing lightweight convolutional neural network\n\n[13] One-shot layer-wise accuracy approximation for layer pruning\n\n[14] One-shot Network Pruning at Initialization with Discriminative Image Patches\n\n[15] Group-based network pruning via nonlinear relationship between convolution filters"
            },
            "questions": {
                "value": "1. Can authors compare the proposed method with more related pruning works [1-14] (See above)?\n\n2. Can some ablation study be provided?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7227/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698972236349,
        "cdate": 1698972236349,
        "tmdate": 1699636860491,
        "mdate": 1699636860491,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uyaRnx6ga1",
        "forum": "YhNXGWVH1N",
        "replyto": "YhNXGWVH1N",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7227/Reviewer_SmAM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7227/Reviewer_SmAM"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose a fine-grained pruning approach, which exploits grouped kernel pruning (GKP) with self-designed clustering and pruning methods, corresponding to high performance and general efficient inference speed. The authors identify that the existing methods correspond to coarse-grained pruning with inferior accuracy performance. In addition, they propose a grouping method to enable exploiting general infrastructure with the pruned model. Then, they propose a L-2 geometric method-based grouped kernel pruning method to perform the pruning operation. Furthermore, they exploit a post-pruning group count evaluation to evaluate the pruned model. They conducted experimental comparison with 5 baseline approaches, which demonstrates the advantages of the proposed approach."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The authors propose a find-grained pruning method that can have higher accuracy performance.\n2. The introduction section is detailed with the presentation of the background explanation.\n3. The experimental results seems promising."
            },
            "weaknesses": {
                "value": "1. The structure of the paper can be improved. Sections 1 and 2 are too detailed that Section 3 corresponds to only a small part, which is not enough to explain the major contribution.\n2. 5 baseline approaches are compared while some lossless approaches or other baselines can be added as baseline approaches.\n3. It is not clear whether the proposed method can achieve lossless pruning. Some theoretical analysis may be beneficial to the paper.\n4. Many grammar errors, e.g., an higher ***, they can be run, with in, is determine by, with lowest etc.\n5. Dependable experience may be independable experience."
            },
            "questions": {
                "value": "1. I wonder if \"dependable experience\" should be independable experience. \n2. I wonder if the proposed approach is lossless. In addition, I wonder if the proposed approach can be applied to other structures.\n3. I wonder if the authors can compare the proposed approach with lossless pruning methods."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7227/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699162827215,
        "cdate": 1699162827215,
        "tmdate": 1699636860381,
        "mdate": 1699636860381,
        "license": "CC BY 4.0",
        "version": 2
    }
]