[
    {
        "id": "6UOh1AMZrO",
        "forum": "IaKxCsJSOO",
        "replyto": "IaKxCsJSOO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2428/Reviewer_dbnt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2428/Reviewer_dbnt"
        ],
        "content": {
            "summary": {
                "value": "The authors examine the problem of offline RL via RvS approaches, and note that one (underexamined) issue is tractability - answering flexible probabilistic queries faithfully. Due to the nature of the dataset generation and stochastic MDP structure, this is particularly difficult. The authors use TPMs for RvS to address this problem, showing that they are better at achieving the requested returns, and perform competitively, particularly with suboptimal data. Furthermore, they are better suited for constrained-action RL."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "This is a very well-written and clear paper. The authors do a very good job of concisely going through the material (offline RL and tractability), and motivate their solution with theory and empirical evidence. There is an extensive experimental section with different environments, many baselines, and various examinations of the results."
            },
            "weaknesses": {
                "value": "The improvement in scores over baselines don't seem so large. In particular, with constrained actions one might expect the TPM approach to produce more gains. However, I don't see this as a particular demerit for insightful research."
            },
            "questions": {
                "value": "I do not have any questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2428/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698658547147,
        "cdate": 1698658547147,
        "tmdate": 1699636178397,
        "mdate": 1699636178397,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Qli5hn11qG",
        "forum": "IaKxCsJSOO",
        "replyto": "IaKxCsJSOO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2428/Reviewer_WTWy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2428/Reviewer_WTWy"
        ],
        "content": {
            "summary": {
                "value": "The paper looks at the \"tractability\" (ability to answer probabilistic queries) issue in offline RL and uses TPMs (Tractable Probabilistic Models) for solving complex RL tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is very well written. I especially enjoyed reading section 4.\n\n2. The experiments are well-designed and the baselines are good as well (though do note the comments in the next two sections)."
            },
            "weaknesses": {
                "value": "The points are in the spirit of making the paper stronger and in some cases, did not contribute to the score.\n\n1. It will be useful to have some details about the Probabilistic Circuits in. the main paper while the bulk of the details can be in appendix (as is the case now).\n\n2. The experiments used medium (or medium-expert) datasets. It will be useful to see the behavior with expert only and weak datasets.\n\n3. It is not clear if the proposed approach outperforms \"Bayesian Reprameterized RCRL\" baseline. While I do not think it is important to beat all the baselines, it is useful to (i) understand how the two methods stack against each other (sharing standard deviation numbers for BR-RCRL will help) and (ii) the difference between the two approaches. The authors should add more details about why they think the two approaches are tied so closely.\n\n4. The authors should consider adding results with another TPM (even if for a subset of tasks) so that it is clear that their approach works across different TPMs.\n\n5. It is not clear why the authors used the TT baseline (in place of a stronger alternative) for experiments in 6.2 and 6.3"
            },
            "questions": {
                "value": "Listing some questions (to make sure I better understand the paper) and potential areas of improvement. Looking forward to engaging with the authors on these questions and the points in the weakness section\n\n1. Are terms like \"training time optimality\" introduced by this paper? If yes, could we consider using existing terms like \"expressivity\"?\n2. The authors mentioned a sampling approach where \"we first sample from a prior distribution p(a_t | s_t ), and then reject actions with low expected returns\" (section 4.1). Did they use this in conjunction with any of the baselines (to make them stronger) ?\n3. Was beam-search used in the baselines ?\n4. In table 1, could the authors report the standard deviation for all the approaches."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2428/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698695386464,
        "cdate": 1698695386464,
        "tmdate": 1699636178302,
        "mdate": 1699636178302,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7k6JHYCg3F",
        "forum": "IaKxCsJSOO",
        "replyto": "IaKxCsJSOO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2428/Reviewer_94tr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2428/Reviewer_94tr"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes to use tractable probabilistic models (TPM) in reinforcement learning via supervised learning (RvS) approaches such that the computation of the multi-step value estimate can be done in a more tractable manner (in replacement of the potentially high-variance Monte-Carlo estimate needed in normal autoregressive generative models). The authors showed that through thorough empirical analysis that obtaining the multi-step value estimate accurately and act according to it is crucial in achieving good performance at the inference time, and previous approaches have failed to do so to some extent. In contrast, the use of TPM readily addresses such issue and consequently results in performance improvement on a range of offline reinforcement learning tasks tested (nine original D4RL locomotion tasks, a modified gym-taxi task, and three action-constrained safe RL locomotion tasks)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is very well-written with clear presentation of the method and informative empirical results with comparison to relevant baseline methods. The analysis (Section 3) of the correlation between inference-time optimality score (how well an action is selected based on the model's estimate of the return) and the actual return achieved is convincing, and it motivates the proposed TPM-based solution well. \n\nTo the best of my knowledge, this paper is the first that uses TPM in offline RL and the thorough empirical study (especially the analysis on the estimated returns vs. actual returns in Figure 1 and 2) brings insights on how useful TPM is in the context of RL/offline RL/RvS."
            },
            "weaknesses": {
                "value": "The main weakness of the paper is the lack of convincing evidence that the proposed algorithm Trifle can also bring significant performance benefits to offline RL tasks. \n- The harder D4RL tasks are not evaluated (Section 6.1). The nine tasks evaluated are relatively saturated at the moment and it is hard to see much performance gain (as seen in Table 1) on top of existing approaches. It would be great if the authors could test the method on harder tasks such as antmaze tasks. \n- The performance improvements on two of the three domains considered (two MuJoCo domains, in Sec 6.1 and Sec 6.3) are marginal. There does seem to be a descent performance gain on one of the custom task (on the gym-taxi environment presented in Sec 6.2) but it is not a standard task that people have evaluated on (which is fine, but a more comprehensive set of experiments would make a stronger case). \n\nOther minor comments:\n- I found Theorem 1 to be a bit out of place because showing a problem is NP-hard brings little information on how easy it is to approximate the solution, which is what people mostly care about in practice.\n- I found the details of the action-space-constrained task (Sec 6.3) to be quite terse. How is the constraint being conditioned (is it a boolean variable or the threshold value discussed in the caption of Table 2?) How is it being incorporated into the TPM?"
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2428/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2428/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2428/Reviewer_94tr"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2428/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698860081028,
        "cdate": 1698860081028,
        "tmdate": 1700683088856,
        "mdate": 1700683088856,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dsCCfJe4ld",
        "forum": "IaKxCsJSOO",
        "replyto": "IaKxCsJSOO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2428/Reviewer_WNH3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2428/Reviewer_WNH3"
        ],
        "content": {
            "summary": {
                "value": "The paper considers the problem of RL via sequence modeling, an offline RL paradigm given offline trajectories of the form (s, a, r, R) where R denotes the return-to-go. This is achieved by optimizing and sampling from p(a_{t: T} | s_t, E[V_t] >= v). \n\nThe authors highlight that there are two main challenges in training such policies. (1) Estimation accuracy: Ability to estimate the expected return of a state and a corresponding action sequence. (2) Tractability issues: Ability to efficiently sample from this distribution. The authors claim that both of these issues are jointly responsible for poor test time performance of RvS based methods. \n\nWhile fixing the estimation accuracy part is already an active area of research, the authors focus on fixing the tractability part by using Tractable Probabilistic Models (TPMs) for sampling P(V_t >= v | s_t, a_t). The authors experimentally show that their method works well in practice and beats many other offline RL methods on various benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Address the tractability issue in RvS which seems to have been overlooked in the past works. \n2. Incorporate TPMs into RL via sequence modeling. \n3. Extensive experimental evaluation."
            },
            "weaknesses": {
                "value": "1. Limited intuition on what the TPM is doing. I would have appreciated if the authors can incorporate a section on what TPMs are and what they are designed to do. While I do see some discussion in Appendix B.1. on Probabilistic Circuits, a colloquially accessible introduction to TPMs in the main body is highly appreciated. \n\n2. Triffle does not seem to be significantly better than other RvS algorithms."
            },
            "questions": {
                "value": "1. While the authors show in Theorem 1 that when |A| = 2^K, solving the sampling issue given the Naive Bayes Distribution could be NP-hard, can you please discuss why for this setting, sampling using (2) is efficient even when given oracle access to P_{GPT} and P_{TPM}? Can the authors discuss why computing Z or sampling using 2 is efficient when |A| = 2^K?\n\n2. It is not clear from Table 1 and Table 2 if Trifle is significantly better than other RvS approaches. Can you please discuss any concrete examples where Trifle significantly (with a reasonable margin) outperforms other RvS approaches?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2428/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698890665689,
        "cdate": 1698890665689,
        "tmdate": 1699636178072,
        "mdate": 1699636178072,
        "license": "CC BY 4.0",
        "version": 2
    }
]