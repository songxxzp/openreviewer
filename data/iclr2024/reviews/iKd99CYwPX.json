[
    {
        "id": "fhI4uXpH0U",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3549/Reviewer_X2Za"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3549/Reviewer_X2Za"
        ],
        "forum": "iKd99CYwPX",
        "replyto": "iKd99CYwPX",
        "content": {
            "summary": {
                "value": "This paper proposes a deterministic diffusion method for video prediction and robotic control. The main claim is that training from a better distribution can lead to faster inference. The authors conduct extensive experiments on the BAIR Robot pushing dataset and PHYRE, which show that their approach can boost generation and policy learning under many conditions.\n\n---\n\nPost rebuttal comments: I think the authors provide a lot of useful responses, and some parts of my initial review are incorrect (have been fixed). However, after the clarification, this paper looks even more like an engineering improvement over the prior work Heiz et al. Therefore, I keep my scores unchanged."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed method is verified on distinctive tasks and the demo videos look cool.\n2. The authors provide a deterministic diffusion model for sequential tasks, which are very general. The idea of utilizing a better initialization makes sense.\n3. The paper introduces various toy examples to illustrate the idea. This is a strong benefit."
            },
            "weaknesses": {
                "value": "1. This paper is not particularly novel.\n\n(1) The major change of this paper is using a better initialization. Although such an initialization may benefit the diffusion procedure, this may not be a significant change. The main model and workflow are not changed, so the proposed initialization methods look like an engineering trick.\n\n(2) The method of selecting the source distribution seems too trivial. When using perturbed history, the agent might get stuck in the previous information, which may harm the performance. The idea of using DDIM inversion is not novel as well.\n\n(3) This paper does not compare to many works in speeding up diffusion models. The only compared baselines are standard diffusion models without speeding up tricks. It's better to compare to IADB or other works.\n\n2. This paper is extensively based on Heitz et al. (2023), which is not a well-known paper yet. The proposed initialization scheme should be tested in other well-known diffusion schemes and that paper should be presented in a separate section."
            },
            "questions": {
                "value": "I don't have other questions at this moment."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3549/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3549/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3549/Reviewer_X2Za"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3549/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697257618277,
        "cdate": 1697257618277,
        "tmdate": 1700612404158,
        "mdate": 1700612404158,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QkQqXaLnsD",
        "forum": "iKd99CYwPX",
        "replyto": "iKd99CYwPX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3549/Reviewer_eaWM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3549/Reviewer_eaWM"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a deterministic diffusion framework that samples (sequential) data more efficiently and effectively than established DDPM / DDIM frameworks.  The framework is based on IDAB, where mixed densities are iteratively deblended from the source to target distribution.   The core idea is to initialize the source distribution with those more closely resembling the target distribution, instead of starting from Gaussian noise.  Such a non-Gaussian distribution can be obtained using the available context or intermediate distributions sampled from a trained DDIM model.  This paper demonstrates better efficiency and stability of sequence predictions than DDIM/DDPM baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The idea of denoising from a source distribution closer to the target makes sense.  Figure 1 and 3 clearly demonstrates the idea.\n2. The paper is well written and easy to follow."
            },
            "weaknesses": {
                "value": "1. The compared baselines are incomplete.  Diffusion models have sparked great interest in the machine learning community, and there are multiple works focusing on improving the sampling efficiency of diffusion models.  For example, Progressive Distillation[1], DPM-Solver[2], 3-DEIS[3], and Consistency Models[4].  Especially, Consistency Model even requires only 1 diffusion step for sampling. I would expect Consistency Model to be the strong baseline across all experiments.\n\n2. The efficacy of deterministic deblending module is unclear.  One can stack two separate DDIM modules by replacing the IADB with another DDIM.  Probably such a cascade (yet incremental) trick could also good performance and model efficiency.\n\n3. It seems unclear to me when to using DDIM or sequence history to initialize the source distribution.  For video prediction, the authors use deblend from history.  For policy prediction, the authors use DDIM intermediate estimate.  There's no enough ablation study about why the authors choose to use DDIM intermediate estimate for action prediction.  I would expect it's because that using sequence history is more stable, but having troubles with capturing multi-modality.  On the other hand, using DDIM intermediate estimate seems to contradict with the abstract, where authors mentioned \"Drawing on recent work on deterministic denoising diffusion, we initialize the denoising process with a non-Gaussian source distribution obtained using the context available when predicting sequence element\".  DDIM starts denoising from Gaussian noise.\n\n\n[1] Progressive distillation for fast sampling of diffusion models.  Salimans, Tim and Ho, Jonathan.  ICLR 2022.\n\n[2] Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. Lu et al. NeuRIPS 2022.\n\n[3] Fast sampling of diffusion models with exponential integrator. Zhang et al. ICLR 2023.\n\n[4] Consistency Models. Song et al. ICML 2023."
            },
            "questions": {
                "value": "1. To clarify my concerns, I would expect the following experiements:\n\n    * On BAIR and PHYRE, compare DDIM & DDPM & ASPeed that deblend previous history or Gaussian noise.  It would be clearer about the choice of deblending history (see Weakness 3).\n    * On Robomimic and Push-T, compare to i) Consistency Model, and ii) two-stage DDIM, which resembles ASPeed but replaces the second-stage deterministic deblending module with another DDIM (see Weakness 1 and 2).  If stacked DDIM works, probably this paper should be positioned as cascade diffusion models instead.\n\n2. One interesting (and I personally think would be more impactful) question is: can we generalize the first-stage distribution to some coarse-semantic / prior-knowledge distribution.  Take a simple example--face reconstruction, can we initialize the source distribution with mixtures of eigen faces (where we can introduce stochasticity)?  So that we can control the whole generation process (either start from some specific prior or randomly perturbed prior to start from pure noise)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3549/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698574695217,
        "cdate": 1698574695217,
        "tmdate": 1699636309097,
        "mdate": 1699636309097,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UjbvKS0qYV",
        "forum": "iKd99CYwPX",
        "replyto": "iKd99CYwPX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3549/Reviewer_HtfP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3549/Reviewer_HtfP"
        ],
        "content": {
            "summary": {
                "value": "This paper applies an iterative $\\alpha$-(de)blending approach to generative modelling in sequential tasks (context conditioned video and action prediction). Here, the forward diffusion process is formulated as a linear interpolation between the source distribution in some context and a target distribution given the same context. The core approach is to speed up diffusion processes by drawing data from an improved source distribution, with two options considered. The first takes advantage of temporal similarity and uses a Gaussian noise perturbed prior sequence of states as the source distribution, while the second, more expensive approach, uses a pre-trained DDIM diffusion model. Results on video prediction tasks (BAIR and PHYRE) show improved video prediction quality with fewer diffusion steps, and improved rewards when compared to baseline models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The core contributions of this work are to show that $\\alpha$-(de)blending (Heitz et al. 23) is effective in the video domain, where stronger sequential priors over source distributions can be exploited. This is a sensible application of this approach, and the empirical evaluations in this work help to justify this.\n\nThe proposed approach produces good results, outperforming baselines.\n\nSection 4.1 nicely explains the motivation behind the choice of improved distributions and how this reduces the number of steps required for generation."
            },
            "weaknesses": {
                "value": "The strength of the contribution is limited, and to my mind better suited to a workshop paper, particularly given that the proposed approach closely follows Heitz et al. 23.\n\nThe results are not particularly surprising, particularly when the relatively weak baselines used for comparison are considered. As mentioned in 5.1, the DDIM sampling approach starts from Gaussian noise, while the proposed approach bootstraps from the perturbed history. The core idea to bootstrap the diffusion process from a good initial guess has been applied to accelerate diffusion processes previously eg. Lyu et. al., which may be a better baseline to consider.\n\nHowever, the idea of using another trained generative model (in this case DDIM) as a source distribution, while effective, seems extremely expensive, and needs to be justified/ better motivated. The comparisons in Fig 8 seem unfair, as the proposed approach takes 10 steps, but this requires an additional 3 steps for blending step (please correct me if I am wrong). This seems to indicate that 10 DDPM steps is doing comparatively well on this task.\n\nThe paper would benefit from significant levels of polish (proof reading, figure axis labelling, better choices of axis limits - eg. Fig 8a, these axes should reflect maximum and minimum rewards obtainable to put these results into context, figure order, making sure captions fully describe content)."
            },
            "questions": {
                "value": "Fig 8. Why is 100 DDPM worse than 10 DDPM in push-T/ tool hang? How well does 30 DDIM/ DPPM (I think this is equivalent to 10 Aspeed steps?) steps do in comparison to Aspeed? \n\nFig 10. These ablations don't seem comprehensive enough to answer questions or support the claims. How dependent are these choices on the nature of the task?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3549/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698727392449,
        "cdate": 1698727392449,
        "tmdate": 1699636308987,
        "mdate": 1699636308987,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7W92nR2VAe",
        "forum": "iKd99CYwPX",
        "replyto": "iKd99CYwPX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3549/Reviewer_1n8o"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3549/Reviewer_1n8o"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors aim to expedite inference by leveraging the properties of the sequence prediction task."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The authors extend the iterative \u03b1-(de)blending approach, applying it to sequential tasks\n2. The model shows improved NFE at inference time"
            },
            "weaknesses": {
                "value": "1. While the authors' effort to extend a proposed diffusion model to a conditional or sequential generation task is commendable, it lacks a significant degree of innovation. The work does not present any new insights or methods, nor does it offer any architectural advancements that could potentially enhance the performance of conditional generation. The novelty factor of this study could be improved with the introduction of unique approaches or methodologies.\n\n2. The iterative \u03b1-(de)blending method utilized in this research appears to bear a close resemblance to the rectified flow, as detailed in this [paper](https://arxiv.org/pdf/2209.03003.pdf). An important opportunity seems to have been missed by the authors in not acknowledging this significant work. A thorough review of related literature is crucial in any research endeavor to avoid overlooking key contributions in the field.\n\n3. The concept of selecting an appropriate source or prior distribution is indeed valuable. However, it's worth noting that a similar discussion has already been conducted in [PriorGrad](https://openreview.net/pdf?id=_BNiN4IjC5). Furthermore, other methods from VAE research such as [SVG-LP](https://arxiv.org/pdf/1802.07687.pdf) provide more straightforward approaches to learning a source distribution. It is essential to acknowledge these existing methods and differentiate the new work from them.\n\n4. The study's comparative analysis is somewhat lacking, with only DDPM and DDIM used as baseline samplers. The field has seen numerous new samplers designed to enhance sampling speed, such as rectified flow, [DPM-solver](https://arxiv.org/abs/2206.00927), and [Karras et al. 2022](https://arxiv.org/pdf/2206.00364.pdf). It is not necessary to compare the new model with all existing ones, but it would be beneficial to include a broader range of models in the comparison to provide a more comprehensive evaluation.\n\n5. In terms of video prediction, the study could benefit from the inclusion of at least one higher-resolution dataset. A resolution of 128x128 would be sufficient to provide a more challenging and realistic evaluation of the model's performance. This would enable a more robust assessment of the model's capabilities in real-world applications."
            },
            "questions": {
                "value": "see above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3549/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3549/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3549/Reviewer_1n8o"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3549/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698762116113,
        "cdate": 1698762116113,
        "tmdate": 1700638535146,
        "mdate": 1700638535146,
        "license": "CC BY 4.0",
        "version": 2
    }
]