[
    {
        "id": "TmdEreF5qy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4394/Reviewer_2kKp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4394/Reviewer_2kKp"
        ],
        "forum": "f5H8WGLQm5",
        "replyto": "f5H8WGLQm5",
        "content": {
            "summary": {
                "value": "This paper proposed a parameter-efficient adapter for fine-tuning vision-language foundation models. The unified architecture for the proposed adapter can not only support visual or textual single modality, but also support both together by sharing knowledge together in cross-modality. The contributions for this adapter are threefold, 1) residual learning for language queries within adapter and in multi-modal encoder; 2) knowledge sharing of cross-modality only in down-projection layers; and 3) a parameter-free frame-aware attention mechanism to extend image approach to video inputs. Six cross-modal experiments are validated on the proposed adapter, and the authors demonstrate it outperforms other SOTA methods on both accuracy and tunable parameters. Furthermore, it can achieve or even surpass full-fine tuning results on these datasets.\n\nThe authors have addressed all my comments carefully during rebuttal phase. I have no other concerns and it is indeed a stronger paper now. Hence I raise my rating."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Good motivation for the topic and its approach. Efficient fine-tuning is indeed needed for VLM so that transfer learning is feasible for wide range of applications. This makes both academia and individual works on fine-tuning large models possible who usually have insufficient hardware resources.\n2. Clear and nice writing. It is easy to understand its concept and contributions. Appreciate it.\n3. Novelty on residual learning for textual information. Good observation and approach to improve the performance of UniAdapter. The residual part for both inside adapter and in multi-modal encoder parts are great ideas to apply residual learning for text.\n4. Strong experiments. Extensive experiments and very detailed ablation studies to support its argument.\n5. Great contributions for releasing the code."
            },
            "weaknesses": {
                "value": "1. Although the authors propose using residual learning to preserve the integrity of language queries during the cross-attention process in multimodal encoders, it is not clear why only textual queries need to be preserved, not visual queries. Specifically, in Fig. 2(b), why if text info may be missing which needs a residual learning, why not visual info after up-projection linear layer not be added to the cross-modal output? Similarly for Fig. 2(a), why there is no extra UniAdapter for visual modal be needed i mluti-modal encoder? This needs to be explained carefully, with evidence/experiments.\n2. It is not clear what is \"the noise and misalignment problem in videos\" and how the proposed PFA can mitigate these issues. Need more insights be explained or visualizations, not only demonstrated by ablations.\n3. Regarding equation (7), how to justify only using text token feature is good in this case? How about other features f^{t}_{CLS,i}?\n4. In Table 2, need ablation studies on top of UniAdapter, not Adapter. On Adapter is good and helpful, but it is needed to be put on top of the proposed solution to see its full benefit, i.e., +Query-residual, +PFA, +Weight-sharing all in Full UniAdapter. Would like to see the enhancement on the full version.\n5. There is supposed to have two textual query-residual learnings need to be validated, i.e., in both Fig. 2(a) and Fig. 2(b). However, in Table 2 there is only one +Query-residual Adaption. Is this a combined experiment for both residual learning? Would like to see this ablation with separate results."
            },
            "questions": {
                "value": "See weaknesses. Please respond to all the questions and request there."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None."
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4394/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4394/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4394/Reviewer_2kKp"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4394/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697433720016,
        "cdate": 1697433720016,
        "tmdate": 1700331171051,
        "mdate": 1700331171051,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1lnoN5cAp9",
        "forum": "f5H8WGLQm5",
        "replyto": "f5H8WGLQm5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4394/Reviewer_1KDb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4394/Reviewer_1KDb"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces UniAdapter as a method that unifies unimodal and multimodal adapters for efficient cross-modal adaptation in vision-language models. The key components of UniAdapter are:\n- Knowledge sharing design: use a shared down-projection layer among all adapters, while learning modality-specific up-projection layers\n- Additional residual connection for language queries \n- Parameter-free frame-aware attention to bring together video and image modalities. This is achieved by emphasizing tokens within important frames while suppressing those in noisy or irrelevant frames during cross-attention.\n\nThe proposed method emphasizes reductions in the number of tunable parameters while achieving competitive practical performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Quality & Significance: The problem of using a unified adapter architecture (and potentially shared weights) for modeling single-modal and multi-modal interactions is interesting. While the algorithm design lacks originality, the empirical evaluation (e.g., the ablations studies) is good, and a wide array of baselines is considered.\n2. Clarity: The presentation is clear, and the ideas are easy to follow.\n3. Reproducibility: The paper provides the code repo to reproduce the experiments, which is beneficial for future work to build on top of it."
            },
            "weaknesses": {
                "value": "1. Lack of novelty: the overall design and each of the three components of UniAdapter are not interesting. In particular, using shared weights in the lower layers followed by layers with specialized weights is common in multi-task learning literature. Weight-sharing has also been employed by previous parameter-efficient fine-tuning work like Compacter (Karimi Mahabadi et al., 2021). Using residual connections is again a commonly seen trick. More importantly, the performance improvement that resulted from combining these three techniques is not impressive at all compared with vanilla adapters, as shown in the middle rows of Table 2.\n \n2. Related work: The absence of related work published in 2023 from the first three sections of the paper is surprising. Only a few recent methods are used as baselines in the last experiment section.\n\n3. Comparison fairness: In Table 2, the highlighted best-performing result is UniAdapter with r=512, which uses 19.0M parameters, significantly more than the middle rows. The comparison is kind of unfair, and it would be better to include \"Adapter with r=512\" for a fair evaluation. \n\n4: Scaling: The paper mentions that UniAdapter is currently only integrated with BLIP. It raises the question of how the method scales to larger models, such as BLIP2, SimVLP, BEIT 3. Further investigation into the scalability and applicability of UniAdapter is needed."
            },
            "questions": {
                "value": "See Weaknesses 2-4."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4394/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4394/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4394/Reviewer_1KDb"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4394/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698601313847,
        "cdate": 1698601313847,
        "tmdate": 1699636412523,
        "mdate": 1699636412523,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "G4gjmnrnYy",
        "forum": "f5H8WGLQm5",
        "replyto": "f5H8WGLQm5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4394/Reviewer_BCaT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4394/Reviewer_BCaT"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces UniAdapter for efficiently transferring vision-language pre-trained models to various cross-modal downstream tasks like video-text retrieval, image-text retrieval, and video and visual question answering. UniAdapter adopts adapters for different modalities (image and video) and tasks while sharing knowledge through partial weight-sharing strategies. UniAdapter outperforms existing state-of-the-art methods and surpasses several full fine-tuning approaches."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper addresses the challenge of unified parameter-efficient cross-modal transfer learning, enabling the efficient use of a pre-trained vision-language model across a variety of cross-modal downstream tasks.\n\n2. The proposed method UniAdapter, a novel framework designed for efficient adaptation, manages to feature a unified adapter architecture that allows for significant parameter efficiency while maintaining or improving task performance.\n\n3. Extensive testing on various cross-modal benchmarks where UniAdapter demonstrated superior performance with fewer parameters compared to previous models.\n\n4. The authors have made the code and models publicly available, promoting transparency and facilitating replication and further research."
            },
            "weaknesses": {
                "value": "1. While parameter sharing shows advantages in terms of the number of parameters, in reality, the extra parameter count may not be a significant issue. Although the authors have compared the time and memory usage with full fine-tuning, it is uncertain whether this method would retain its advantages if other comparative methods were scaled up in computational resources without regard for parameter amount.\n\n2. The adapter mainly implements some reuse design for multimodal tasks, with its structure not deviating significantly from classical approaches. It is unclear if this is optimal for cross-modal applications. Has the author explored distinct design strategies for different modalities?\n\n3. The method is based on BLIP-base, suggesting potential limitations in the types of models to which it can be applied. Has the author attempted to validate the approach on alternative backbones?\n\n4. The experimental design appears to be somewhat disorganized; it is challenging to discern controlled variables in the comparative analysis presented in each table. This lack of clarity complicates the evaluation of the actual impact of different components of the method."
            },
            "questions": {
                "value": "Please refer to the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4394/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698981591454,
        "cdate": 1698981591454,
        "tmdate": 1699636412425,
        "mdate": 1699636412425,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BHN2Y9HfPE",
        "forum": "f5H8WGLQm5",
        "replyto": "f5H8WGLQm5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4394/Reviewer_euaM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4394/Reviewer_euaM"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel and parameter-efficient adapter that facilitates the transfer of pretrained knowledge to various vision-language downstream tasks. The experimental results demonstrate the remarkable effectiveness of this approach."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The Uni-adapter incorporates partly shared layers, leading to a reduction in trainable parameters while simultaneously improving performance.\n\n2. The experiments conducted in this paper encompass a wide range of common datasets for downstream tasks, illustrating the generalization and effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1. Some of the ideas presented in this paper have been explored in previous works. For example, (1) the sharing of layers across multiple modalities has been addressed in [1]; (2) the aggregation of video (frame) features in a parameter-free manner, such as through attention or averaging, has also been discussed in [2].\n\n2. Regarding feature visualization, I suggest conducting a comparison between the results of the non-shared architecture, the up-shared one, and the all-shared one. \n\nRef: [1] Image as a Foreign Language: BEIT Pretraining for All Vision and Vision-Language Tasks. https://arxiv.org/pdf/2208.10442.pdf [2] CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval. https://arxiv.org/pdf/2104.08860.pdf"
            },
            "questions": {
                "value": "Please refer to weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4394/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699624152173,
        "cdate": 1699624152173,
        "tmdate": 1699636412333,
        "mdate": 1699636412333,
        "license": "CC BY 4.0",
        "version": 2
    }
]