[
    {
        "id": "ZQDfXMml1q",
        "forum": "sGd02fkoAE",
        "replyto": "sGd02fkoAE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8882/Reviewer_zd5v"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8882/Reviewer_zd5v"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes FusionViT, a 3D object detection framework that fuses 2D images with point cloud data. This framework consists of three components, including a 2D image model, a point cloud model, and a fusion model. All three components are based on vision transformer architectures. The method is evaluated on Waymo Open Datset and KITTI benchmarks for 2D & 3D object detections. The results show FusionViT can achieve performance that is competitive with latest works in 2D/3D object detections on those datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "* This paper is an interesting exploration to use \"pure\" ViT architectures for 3D object detection. This is a sound research objective as ViT has demonstrated very strong performance in image classification and as a very strong model for visual embeddings. It is generally useful to explore adoption of this backbone in dense prediction in 3D tasks.\n\n* The paper presents a good amount of details and illustrations of the method. For the most part, concepts and algorithms are defined using precise language, assisted with helpful illustrations. \n\n* The proposed method is simple and clean, yet it shows strong performance against baselines that is a reasonable sample of recent works. The benchmarks are done on Waymo dataste and KITTI. Both are popular datasets suitable for evaluation of a 3D object detector."
            },
            "weaknesses": {
                "value": "* It seems to be an exaggeration to claim that this work is \"the first study to investigate the possible pure-ViT based 3D object detection framework\".  Transformer based architectures for 3D tasks seem to have been explored extensively in the literature, see [1] for a survey.\n\n* The literature survey in this work is unfortunately not effective. While it lists some of the recent and classical fields, it fails to clearly define the relevance of the current work against the prior approaches. It will help position this work better if such an explicit analysis is presented.\n\n* The paper does not seem to provide much justifications on the various design choices. There are a few well-known 3D object detection paradigms in the literature, voxel based, pillar based and projection based. The paper focuses on the voxel based paradigm. It does not seem to be particularly favorable for a pure ViT architecture. Compared to pillar based approaches it is likely slower and harder to train due to the larger complexity in the attention layers. Compared to a projection based approach it is likely harder to align with the image features (being an 3D detection framework that fuses 2D and 3D). Given this large design space and obvious concerns, I think the paper should provide more rationale and ablation studies to justify the design choices.\n \n* While ViT architectures are shown to be very capable for classification/embedding tasks, it does have a few significant shortcomings in practice. For example, due to lack of the inductive bias it typically requires a large dataset for training to achieve competitive performance. Also, the quadratic complexity in attention layers makes it much harder to be used in dense prediction tasks. These are particularly relevant for 3D object detection using LiDAR as an input modality. It would be reasonable to expect a paper that set out to explore a \"pure-ViT based 3D object detection framework\" should provide deep analysis on those issues and propose effective mitigations of those shortcomings. It would also be reasonable to expect the same paper to demonstrate the superiority of ViT based architecture versus other sensible architecture choices, such as Swin based methods, despite the possible shortcomings. However, neither is presented in this work. \n\n* The comparison in Table 1 and Table 2 shows promising result of FusionViT compared to some reported numbers of prior works. But as an object detection system, it is critical to provide additional contexts of the accuracy achieved as more expensive systems typically have an edge in terms of accuracy. In this case, I think it is a minimum requirement to compare prior works at similar FLOPs. Better still, latency measured in clearly defined hardware platform should be provided.\n\n[1]Transformers in 3D Point Clouds: A Survey  https://arxiv.org/pdf/2205.07417.pdf"
            },
            "questions": {
                "value": "* Beyond high level number, is there a detailed complexity/accuracy tradeoff provided for the reported results in Table 1 & Table 2?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8882/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8882/Reviewer_zd5v",
                    "ICLR.cc/2024/Conference/Submission8882/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8882/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698806557854,
        "cdate": 1698806557854,
        "tmdate": 1700716405282,
        "mdate": 1700716405282,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "E1rWXDQbK5",
        "forum": "sGd02fkoAE",
        "replyto": "sGd02fkoAE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8882/Reviewer_qYip"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8882/Reviewer_qYip"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a vision transformer based lidar and camera fusion for 3D object detection. \nMulti-modal data provides different views of the same scene which makes it more feature-rich compared to single modality models. The paper is motivated by a lack of \u201cpure-ViT\u201d based 3D object detectors and proposes a model that uses independent ViT per modality (CameraViT and LidarViT) to extract single-modality features and fuses them using another ViT (MixViT) and performs bounding box regression and classification on the final output. The CameraViT operates on mini-patches and the LidarViT branch uses voxelization followed by filtering empty voxels and sampling to address the input size problem. The MixViT module operates on the concatenated features to address feature misalignment and modality differences. Experiments are performed on the Waymo Open and KITTI datasets, and show improvement over existing multimodal fusion works."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper proposes a ViT-only approach for single modality representation learning and  multi-modal fusion in the context of 3D object detection and achieves comparable performance with other camera-lidar fusion based approaches. The lidar-only ViT branch uses voxelization to reduce dimensionality and shows good performance compared to existing lidar-only 3D detectors. The paper also shows ablation studies for the different components which indicates that all the proposed model components are contributing to the performance."
            },
            "weaknesses": {
                "value": "The paper is motivated by the absence of \u201cpure-ViT\u201d based multi-modal 3D detection models. However, the paper doesn\u2019t explain why such a ViT-only approach is expected to be beneficial for the task.\n\nThe overall architecture is not quite novel in that most multi-modal fusion approaches, e.g., DeepFusion, Transfusion, DeepInteraction and any of the BEV fusion based approaches use single modality representation learning followed by multi-modal fusion. It\u2019s unclear what the novelty of the architecture shown in Fig. 1 is.\n\nMixViT uses a large MLP on the concatenated feature which is similar to how DeepFusion uses a localized MLP to learn alignment. The large MLP approach is still learning similar alignment but inefficient in terms of feature utilization.\n\nThere\u2019s comparison missing with DeepInteraction (NeurIPS 2022) around the same time as other papers which performs camera-lidar fusion for 3D object detection.\n\nThe paper doesn\u2019t show any robustness experiments with lidar-camera spatio-temporal misalignment or robustness of MixViT in the presence of single modality failures. \n\nPerformance on NuScenes is also missing in the paper.\n\nMinor\n-------\nThere are a several typos in the paper. For example,\n1. \"Swim\" Transformer written in several places\n2. Section 3.5, what is \"Multi-Level Perceptions\"?"
            },
            "questions": {
                "value": "1. Why is pure-ViT expected to perform better than other approaches in the context of multi-modal 3D object detection? \n2. How well does the approach generalize to different data domains. For example, LidarViT to different types of lidar sensors.\n3. How does the approach perform under single modality failure and spatio-temporal misalignment?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8882/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8882/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8882/Reviewer_qYip"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8882/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698813349068,
        "cdate": 1698813349068,
        "tmdate": 1699637117640,
        "mdate": 1699637117640,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5OMflU3XzJ",
        "forum": "sGd02fkoAE",
        "replyto": "sGd02fkoAE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8882/Reviewer_Jv34"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8882/Reviewer_Jv34"
        ],
        "content": {
            "summary": {
                "value": "This work introduces FusionViT, a transformer architecture that fuses lidar and camera inputs for 3D object detection. \nTheir model is broadly composed of three components - CameraViT, LidarViT and MixViT.  \nThey train the components in three stages - CameraViT and LidarViT perform 2D and 3D detection, respectively,  \nand MixViT is trained to fuse the multi-modal features for 3D detection.  \nThey perform experiments on the KITTI and Waymo datasets and show their pure-transformer architecture can outperform other fusion baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* LidarViT and lidar transformers are a fairly unexplored area (to my knowledge) and is a solid contribution. \n* The high-level idea is straightworward and can be more-or-less summarized by Figure 1.\n* Numbers are strong considering the proposed work pivots to a full transformer architecture compared to the baselines."
            },
            "weaknesses": {
                "value": "Experiment section, besides main results on KITTI and Waymo, not sufficient.  \nAblations are not that informative. The paper currently has two ablations now - one which is comparing sum vs. concat for fusion and the other compares removal of LidarViT, CameraViT, MixViT  \nModel runtime is key for object detection, and with these heavy transformer models, it would good to see some numbers on this.  \nThe authors also introduce a \"corner loss\" in Section 3.5, which I would have liked to see in the ablations.\nWhat other key design choices were made?\n\nWriting needs improvement. The writing in Sections 3.2, 3.3 is clear enough to understand,  \nbut the mathematical notation is overloaded and actually makes it harder to understand.  \nAn alternative is to summarize Equations 1, 2, 3 with a figure."
            },
            "questions": {
                "value": "Table 1: from the writing, I assume the first row is performing 2D detection (comparing DETR, Swin, CameraViT).  \nThe second two rows are 3D detection with lidar and lidar-camera fusion, respectively. Why is the first group being compared to the second two?\n\nIs there any difference in the camera/fusion architecture when dealing with single-view (KITTI) and multi-view (Waymo) camera images?  \nHow/is the camera pose information being used in the positional embeddings?\n\nSection 3.6: the authors state they train the model in 3 stages due to \"some potential issues of large memory consumption\", but still run the model  \nend to end for training the MixVit? Are the subsequence layers frozen in this stage?\n\nThe authors use the word \"cubic\" to describe the 3D representation of the scene - is there any difference between this term and \"voxel\"?\n\nMinor typos:\n* Swim-transformer\n* hungingface"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8882/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699471264803,
        "cdate": 1699471264803,
        "tmdate": 1699637117526,
        "mdate": 1699637117526,
        "license": "CC BY 4.0",
        "version": 2
    }
]