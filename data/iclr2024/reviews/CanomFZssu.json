[
    {
        "id": "eVAV86405O",
        "forum": "CanomFZssu",
        "replyto": "CanomFZssu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1730/Reviewer_va1S"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1730/Reviewer_va1S"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors address the challenge of unsupervised graph anomaly detection, a crucial task in various real-world applications. Existing methods often conflict in their focus between local inconsistency mining (LIM) and message passing. LIM emphasizes identifying high similarities between abnormal nodes and their neighbors, while message passing, commonly employed by graph neural networks (GNNs), tends to make connected nodes similar, leading to local anomaly signal loss. To reconcile this conflict, the authors propose GADAM, a novel framework that not only resolves the tension between LIM and message passing but also utilizes message passing to enhance anomaly detection through a unique approach to anomaly mining beyond LIM. The effectiveness and efficiency of GADAM are extensively evaluated on nine benchmark datasets, including two large-scale OGB datasets. The results demonstrate that GADAM outperforms existing state-of-the-art methods, showcasing superior effectiveness and efficiency in unsupervised graph anomaly detection."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The proposed method is technically sound, i.e., to overcome the issues of LIM in previous anomaly detection methods.\n- The proposed method has been tested on both synthetic and real-world anomalies and the experimental results show the effectiveness of the proposed method.\n- The conducted experiments are comprehensive including performance comparison, efficiency comparison, and ablation studies."
            },
            "weaknesses": {
                "value": "- Although the effectiveness and efficiency of the proposed method have been empirically studied from different aspects, this is no theoretical guarantee of the proposed method.\n- The first step of MLP-based LIM aims to identify local anomalies only. However, if the given data contains more contextual anomalies, will the proposed method fail to capture them?\nThe influence of different types of anomalies on the proposed method is not discussed."
            },
            "questions": {
                "value": "1. How to calculate H_{global} in details?\n2. If there is no prior information about the ratio of anomalous nodes in a given graph, how should one select k_{ano}? If the selected k leads to more anomalies than actual ones, will the proposed method be impacted negatively?\n3. What are the detailed steps in injecting anomalies? What the performance will change if different numbers of contextual or structural anomalies are injected (especially the proposed method first detects local structural anomalies)?\n\n--------------------------\n\nAfter rebuttal, my concerns have been addressed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1730/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1730/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1730/Reviewer_va1S"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1730/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698711979713,
        "cdate": 1698711979713,
        "tmdate": 1700700061467,
        "mdate": 1700700061467,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "j0YQh5ScOa",
        "forum": "CanomFZssu",
        "replyto": "CanomFZssu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1730/Reviewer_6vV6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1730/Reviewer_6vV6"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a model for unsupervised anomaly detection in graphs. In particular, the model focuses on 2 types of anomalies: 1) contextual (a node having attributes that are very different from its neighbors') and 2) structural (dense subgraphs). The innovation is a two-stage model that first uses an MLP to find possible contextual anomalies and then uses a message-passing graph neural network with an attention mechanism to find dense subgraphs of anomalous nodes."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Takes inspiration of previous work on contrastive learning and recent work on attention for message passing \n- Competitive time complexity\n- Experimental evaluation is fairly comprehensive and shows modest improvement over existing methods, of which there are plenty"
            },
            "weaknesses": {
                "value": "- Performance reported for competing methods is lower than in the papers where these methods were proposed. It raises the question of whether the models are sufficiently tuned\n- No comparison to methods in the vast literature outside deep learning / neural networks. See for example https://arxiv.org/abs/1404.4679"
            },
            "questions": {
                "value": "- The AnomalyDAE paper reports an AUC of over 97 for the BlogCatalog dataset. Do you have an explanation for the discrepancy in your paper (AUC of 76.58)? Were the authors of that paper injecting anomalies in a totally different way?\n\n- The AUC for Sub-CR, DOMINANT, and  COLA is higher in the Sub-CR paper. Do you think hyperparameters for the baseline models are sufficiently tuned?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1730/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698733736023,
        "cdate": 1698733736023,
        "tmdate": 1699636101542,
        "mdate": 1699636101542,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "faPlI9ge1g",
        "forum": "CanomFZssu",
        "replyto": "CanomFZssu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1730/Reviewer_deYB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1730/Reviewer_deYB"
        ],
        "content": {
            "summary": {
                "value": "The authors investigate the two main paradigms, both used in many unsupervised graph anomaly detection models, message passings (MP) and local inconsistency mining (LIM). Traditionally, many graph anomaly detection models utilize the LIM for determining nodes that have inconsistency with their surroundings and mark them as anomalous. However, the use of MP with many GNN layers tends to smooth out everything, which decreases the contrastive difference between a node and its surroundings. The authors aim to address this conflict by proposing GADAM, a method that first performs LIM via a regular MLP network to get local anomaly scores without using message passing. It then combines these local scores with global scores that use an attention-based adaptive message passing that enables nodes to selectively absorb normal/abnormal signals from their surroundings. The authors then demonstrate the benefit of the models compared with baselines on several public datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Strengths:\n- An interesting approach in unsupervised anomaly detection. The over-smoothing problem is a well-known problem in GNN, and it may impact the performance of GNN-based anomaly detection models. Most unsupervised graph anomaly detection models work by finding inconsistent nodes among the neighbors. The use of message passing in GNN may impact the finding as the over-smoothing problem may make the nodes less inconsistent.\nI would argue that the conflict between LIM and overs-smoothing could be resolved within a message-passing framework by designing a better message-passing flow, imposing stricter bottlenecks in the encoder-decoder process, or other means. However, the paper proposed a different way outside the message-passing framework.\n\n- The methods combine both local perspective and global perspective to create combined perspective anomaly scores.\n- The authors demonstrate the benefit of the model against many GNN baselines"
            },
            "weaknesses": {
                "value": "I have a few concerns and questions regarding the paper:\n1) In the contrastive pair construction (Section 3.1 item (2)), the authors mentioned that the method uses the complete adjacency subgraph. Does it mean that it includes all 1-hop neighbors? If that's the case, then the receptive field of the method is limited to just the immediate neighbors. This may limit the expressiveness of the model. There may be cases where determining normal/abnormal nodes requires more than just looking at the immediate neighbors. \n2) If in (1), it is not just 1-hop neighbors but all k-hop neighbors, then the size of the subgraphs can be potentially very large in densely connected graphs. How does the model address that?\n3) In the shuffling process, there are some chances that neighboring nodes are accidentally picked as negative pairs. How does the model account for those cases?\n4) The representations of a subgraph G in the local context are just an average of the representation of the node embeddings in G. Then, the anomaly scores for each node are measured by the difference between its node representation and the subgraph representation. It is possible that the node embedding of each neighbor is really different than the target node's embedding, but the average of the neighbor embeddings is similar to the target node. Doesn't it also introduce an over-smoothing problem? It could be worse than the standard message passing, where the aggregation is not just standard averaging but parameterized by a weight matrix. Here, the aggregation is just a plain average.\n5) The use of attention mechanisms may be counter-intuitive in unsupervised anomaly detection that is based on finding abnormal signals from the surroundings. The ability of the attention mechanism to ignore certain signals may make the model ignore suspicious signals, which will result in an abnormal case tagged as a normal case. Has the author tried to replace the attention mechanism with another mechanism, like convolution, that forces the model to utilize all signals?\n6) The experiment results of the baselines presented in the paper are rather different from the ones reported in the original paper for the same datasets. For example, in the original AnomalyDAE paper, the AUC are 97.81 (BlogCatalog), 90.05 (ACM); whereas in this paper, they are 76.58 (BlogCatalog), 75/16 (ACM). Could the authors explain more about it?\n7) How to decide topk% included in the second stage. How does the selection of this percentage affect the results?\n8) The paper is missing some unsupervised graph anomaly detection papers:\n     - Ding, Kaize, Jundong Li, Nitin Agarwal, and Huan Liu. \"Inductive anomaly detection on attributed networks.\" In Proceedings of the twenty-ninth international conference on international joint conferences on artificial intelligence, pp. 1288-1294. 2021.\n     - Huang, Yihong, Liping Wang, Fan Zhang, and Xuemin Lin. \"Are we really making much progress in unsupervised graph outlier detection? Revisiting the problem with new insight and superior method.\" arXiv preprint arXiv:2210.12941 (2022).\n     - Yang, Shujie, Binchi Zhang, Shangbin Feng, Zhanxuan Tan, Qinghua Zheng, Jun Zhou, and Minnan Luo. \"Ahead: A triple attention based heterogeneous graph anomaly detection approach.\" In Chinese Intelligent Automation Conference, pp. 542-552. Singapore: Springer Nature Singapore, 2023.\n     - Fathony, Rizal, Jenn Ng, and Jia Chen. \"Interaction-Focused Anomaly Detection on Bipartite Node-and-Edge-Attributed Graphs.\" In 2023 International Joint Conference on Neural Networks (IJCNN), pp. 1-10. IEEE, 2023.\n     - Wang, Qizhou, Guansong Pang, Mahsa Salehi, Wray Buntine, and Christopher Leckie. \"Cross-domain graph anomaly detection via anomaly-aware contrastive alignment.\" In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 37, no. 4, pp. 4676-4684. 2023."
            },
            "questions": {
                "value": "Please answer my questions in the previous section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1730/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698980038583,
        "cdate": 1698980038583,
        "tmdate": 1699636101465,
        "mdate": 1699636101465,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1YPycJpAX9",
        "forum": "CanomFZssu",
        "replyto": "CanomFZssu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1730/Reviewer_WYWU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1730/Reviewer_WYWU"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the problem of unsupervised node-level anomaly detection in a graph through an adaptive message passing framework. The paper rightly motivates the fact that message passing in GNNs results in local anomaly signal loss. The authors have proposed an approach to obtain local and global anomaly scores through MLP and hybrid attention based adaptive message passing. The paper has conducted experiments on anomaly detection on real world datasets with both injected and ground truth anomalies, and used a good number of baseline approaches from the literature."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The idea of using local anomaly score to compute the global anomaly score is interesting.\n2. The paper uses a good number of baseline algorithms from the literature."
            },
            "weaknesses": {
                "value": "1. The problem formulation seems to be problematic. Authors have mentioned \"Structural anomalies are densely connected nodes in contrast to sparsely connected regular nodes\". What about the networks that are dense by nature? Moreover, if density is the only criteria for being a structural anomaly, why can't we use degree of a node as the metric to find the structural anomalies? Are we over-complicating the solution of a relatively simple problem?\n\n2. I am not convinced with the proposed solution. s^local (in Eq. 6) can only capture contextual anomalies, but not the structural anomalies because of the L2 normalization in Eq. 3. Where exactly are we capturing the structural anomalies in this framework?\n\n3. What is the difference between H_local and H_global? They seem to be computed in the same way through Eq. 3. Why are we calling the second one as \"global\"?\n\n4. Some of the claims in the paper should have been explained more. Can you please elaborate the reason of \"as the training advances, the influence of pre-attention gradually diminishes, while the opposite for post-attention\".\n\n5. In Table 1, overall anomaly detection performance is reported. I am curious to see the performance on contextual and structural anomalies separately.\n\n6. As mentioned before, degree centrality seems to be a good metric to capture the structural anomalies. Can you please include this heuristic as a baseline to compare with?\n\n7. How do you capture the nodes which are both contextual and structural anomalies? Also, will your approach be able to capture \"Combined Outliers\" (or combined anomalies) as introduced in \"Outlier Aware Network Embedding for Attributed Networks\" (AAAI-2019)?"
            },
            "questions": {
                "value": "Please see the questions above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1730/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699557416544,
        "cdate": 1699557416544,
        "tmdate": 1699636101383,
        "mdate": 1699636101383,
        "license": "CC BY 4.0",
        "version": 2
    }
]