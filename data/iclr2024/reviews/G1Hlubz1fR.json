[
    {
        "id": "yjllqJrLf4",
        "forum": "G1Hlubz1fR",
        "replyto": "G1Hlubz1fR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7467/Reviewer_ZWwD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7467/Reviewer_ZWwD"
        ],
        "content": {
            "summary": {
                "value": "The paper mixes different ideas from the multi-task learning and parameter-efficient mixture of experts literature: Methods such as `MoLora` introduced the idea of using low-rank LoRA modules as \"experts\". Methods such as `Poly` or `MHR` show that a new task can be learned as a combination of adapters modules (~ mixture of experts) where the combination weights are task-specific.\n\nThis paper additional proposes to explicitly capture the fact that each task can have shared knowledge (shared across all tasks) as well as task-specific knowledge at the adapters level. In this setting, a new task is now learned as a soft combination of **(i)** $A$ LoRa modules shared across all tasks and **(ii)** $B$ task-specific LoRA modules ($B$ separate modules for each task). In this case, all combination weights are task-specific, and when $B = 0$, we recover previous approaches such as `Poly`. \n\nIn the proposed framework, every adapter is a LoRA module and the combination weights are binary, i.e. each expert can be activated or not. During training, the binary decisions are approximated with Gumbel-sigmoids to allow for backpropagation.\n\nThe proposed approach is evaluated on `T5-Large` and `GLM-10B` as backbone, on the SuperGlue and Super Natural Instructions datasets, and compared to a set of recent baselines (LoRa, MoE-LoRA, Poly and MHR)"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- **interesting research direction:** I think the overall goal of the paper is clear and interesting. Parameter-efficient Mixture-of-experts with LoRA modules is a recent and interesting research direction, and generalizing this framework to multi-task/domain learning as was done for other \"adapters\" type of work seems very natural. Similarly, the idea of separating task-specific and shared knowledge has shown some success in multi-task learning literature, and makes a lot of sense in that setup.\n\n- **Diversity of the experimental setup:** The experiments consider two backbones, as well as two multi-task benchmarks with quite different number of tasks. The proposed method is also compared to several recent baselines\n\n- **Experimental improvement**: While the improvement in experiments is not always consistent, it does show significant gains in some benchmarks, in particular with the decoder-only `GLM` backbone."
            },
            "weaknesses": {
                "value": "- **Explicit vs Implicit separation of task-specific and shared modules**: To my understanding,  the proposed method can be seen as subcase of some of the baselines mentioned : If the number of adapters in `Poly` is set to $A + BT$, then in theory the model could learn to make certain modules task-specific by setting $w_i^t$ to 1 for only certain pairs of $(i, t)$. In contrast, the proposed method makes this structure explicit by defining task-specific modules; However, this still requires additional $BT$ task-specific adapter modules, hence an increased number of parameters. From the experiments, it is not clear to me if the number of parameters/modules used by the baselines and the proposed method are comparable.\n\n- **Experimental analysis**: In general, I find the proposed idea interesting, but I lack some experimental insights to better understand the proposed method. For instance, some points I found unclear are the following:\n  -  1) For instance, the number of task-specific modules $B$ for each task seems to be an important parameter, but I did not see a lot of discussion/ablation about it; It seems that $B$ is set to 1 throughout the paper\n  - 2) Similarly, the proposed method can use $A + B$ modules per task, while baselines only consider $A$ task-shared modules: It is not clear to me if the setup is fair to the baselines. Considering other  configurations would be interesting.\n  - 3) From the results in **Table 2**, it seems that the performance improvement is not consistent: For instance with the encoder-decoder `T5` model, improvement mainly comes from the `WiC` task, which could mean this task is more likely to interfere with others. However for the decoder-only `GLM`, we see that adding a task-specific module yield significant improvement across all tasks. It is not clear to me why that is the case: Since the tasks are the same in both case, it does not seem to be directly related to the multi-task setup; could it be related to the number of LoRA modules with respect to the architecture (*see point 2*) ?\n\n**Note post-rebuttal:** I still have some reserves about the method's robustness and sensitivity to the task specific/shared setup ratio, but in light of the rebuttal addressing my main concerns and added ablation experiments, I'll raise my score from 5 to 6"
            },
            "questions": {
                "value": "- **Minor suggestion on writing:** \n  * I found the introduction hard to read as it mixes introduction with some related work towards the end, and introduces several concepts without really contextualizing them (e.g. in two paragraphs, the text jumps from MoLora $\\rightarrow$ Poly $\\rightarrow$ MHR $\\rightarrow$ CGC and PLE).\n  * Using $A$ and $B$ in Equation 5 is a bit confusing as these were introduced earlier as the number of shared and task-specific modules"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7467/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7467/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7467/Reviewer_ZWwD"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7467/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698749432490,
        "cdate": 1698749432490,
        "tmdate": 1700607208950,
        "mdate": 1700607208950,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qcB1dtM0v8",
        "forum": "G1Hlubz1fR",
        "replyto": "G1Hlubz1fR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7467/Reviewer_WuXZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7467/Reviewer_WuXZ"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new multi-task learning architecture based on LoRA finetuning framework. They propose the task-common skill sets and task-specific skill sets. Also they learn the task-specific combination weights of task common skill sets using Gumbel-Sigmoid. In the experiment, they adopt two different architectures (encoder-decoder -- T5 and decoder-only -- GLM) and show their performance on several multi-task benchmark in NLP."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper clearly states the difference between their model architecture and previous baseline models.\n2. With GLM model, their proposed method outperforms the baseline models for a significant gap.\n3. They compare to other baseline models in a fair way.\n4. The paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "1. The method does not achieve significant improvement with T5 architecture.  (But I am not expert in NLP tasks and I am open to other reviewers' opinions regarding the performance.)\n2. The model design seems incremental to the previous methods Poly and MHR. \n3. It is unclear about the optimization loss function and the main paper does not discuss this."
            },
            "questions": {
                "value": "1. Since you have used Gumbel Sigmoid to optimize the w_i, what is the distribution of all w_i's learnt in the model? Is there any specific loss to force w_i to be close to 0 or 1? What is the performance variance if you need sample the w_i based on Bernouli distributions in the final evaluation?\n\n2. In the experiment results, it is clear that the method with GLM-10B outperforms baselines a lot and the method with T5 stands close to the baselines. In the paper, the authors claim the difference is due to the different model architecture. However, GLM-10B and T5_large also differ a lot in the model capacity. How do you know the difference of performance compared to the baselines is due to the architecture instead of model capacity?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7467/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7467/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7467/Reviewer_WuXZ"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7467/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698892870179,
        "cdate": 1698892870179,
        "tmdate": 1700675873248,
        "mdate": 1700675873248,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MuOsFWZo0k",
        "forum": "G1Hlubz1fR",
        "replyto": "G1Hlubz1fR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7467/Reviewer_soi2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7467/Reviewer_soi2"
        ],
        "content": {
            "summary": {
                "value": "The paper tackles the task of parameter efficient fine-tuning via Low-Rank Adapter (LoRA) which parametrized the weight updates as low-rank composition to significantly reduce the number of learnable parameters and computation. \nThe work proposes a novel approach, called Customized Polytropon, which extends LoRA to Multi-Task Learning setup by learning multiple LoRA adapter corresponding to different tasks.\nThe key insight is to decompose the learnable parameter into Task-Common adapters, which is shared among all tasks, and Task-Specific adapters specifically trained for different tasks.\nGiven this set of adapters, the proposed framework would learn to combine them for different task, thus enable knowledge transfer among tasks while efficiently learning these adapters.\nThe paper conducts experiments on SuperCLUE and Super Natural Instruction datasets as well as on T5-Large and GLM-10B models to show its effectiveness."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ The paper is self-contained which makes it accessible to a wide range of reader. Moreover, the paper is also easy to follow.\n+ The proposed method of decomposing learnable parameter into a task-common and task-specific portions is sensible as well as easy to develop in practical setting.\n+ The problem of parameter efficient finetuning is impactful which helps to democratize LLM technology on consumer device."
            },
            "weaknesses": {
                "value": "+ As the adapter in LORA are low-rank linear projection of parameters in attention modules, a combination of task-common and task-specific LORA adapters seems to be equivalent to just Mixture of LORA as addition of linear transformations is still a linear transformation (rows 2 and 3). Thus, it is unclear why decomposing learnable parameters would improve performance.\n \n+ The experimental section misses some studies to show the effective of hyper-parameters in the model. For examples, what is the how number of adapters in task-common and task-specific modules effect the performances? What is the impact of rank or the number of tasks toward final performance? These experiments would offer better insight into how robust the proposed method is under different setting.\n\n+ The performance seems to saturate when being applied to strong base model such as T5-Large. It seems to suggest that the effect of task-common components vanish when the base model can generalize well toward different downstream tasks. This could be an interesting phenomena can be study using stronger base model such as llama and llama2."
            },
            "questions": {
                "value": "+ Could the authors clarify on how they combine different adapters? This would help with understanding as well as reproducibility.\n+ Experiments on the effects of the number of adapters/ranks/tasks could provide better insight on the robustness and limitations of the proposed methods\n+ Stronger base model can be used to stress-test the generalizability of the proposed framework."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7467/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698962120275,
        "cdate": 1698962120275,
        "tmdate": 1699636899373,
        "mdate": 1699636899373,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3NEMVvJpL0",
        "forum": "G1Hlubz1fR",
        "replyto": "G1Hlubz1fR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7467/Reviewer_yvMZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7467/Reviewer_yvMZ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an approach called Customized Polytropon (C-Poly) for multi-task learning using parameter-efficient modules. The key idea is to explicitly separate task-common skills that can be shared across tasks, and task-specific skills that are unique to each task.  Note that this augments the previously published Poly method (by introducing task-specific adapters, combining subsets of shared and specific tasks, and can allow better interpretation by the parameters learned for selection and weighting).\n\nThe model consists of components related to task-common skills (shared low-rank across tasks), and low-rank adapters for each task.  Low-rank adapters (E.g. LoRA) are used to improve param efficienty also.  This approach appears to mitigate negative transfer effects, and improve learning over compared methods.  \n\n\n//Having read the responses: I think its a nice idea, the results are good, but some more analysis / polishing of the paper could be useful before this paper is published (See also response to comments).  To be frank I`m still borderline about this.  I will however increase my score to reflect the authors effort in responses and updates."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- this paper presents an intuitive approach to combine shared and specialized skills\n\n- results are good in comparison to previous methods\n\n- sample efficiency is improved; the approach of explicitly separating task-common and task-specific skills to mitigate negative transfer\n\n- offers some more interpretability due to selecting particular skills"
            },
            "weaknesses": {
                "value": "The method introduces additional hyperparameters like number of common/task-specific skills which may require tuning\n\nIt is not clearly analyzed if certain tasks benefit more from common or specialized skills\n\nThe interpretability via learned task hierarchies is not explored much in experiments\n\nResults are comparable largely to previous work\n\nThis paper is an extension of a previous work, with some (nice) but perhaps small increments."
            },
            "questions": {
                "value": "How is the number of common and task-specific skills determined? Is there a systematic way to set these hyperparameters?\n\nHow does performance scale with increasing number of tasks?\n\nHow are skills initialized?  does this affect selection?\n\nCan you provide ablation studies controlling for common vs task-specific skills? \n\nMore in-depth discussion / results on interpretability\n\nIf new tasks appear after training, can the model adapt?\n\nResults are comparable largely to previous work"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7467/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7467/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7467/Reviewer_yvMZ"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7467/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699002994401,
        "cdate": 1699002994401,
        "tmdate": 1700658965582,
        "mdate": 1700658965582,
        "license": "CC BY 4.0",
        "version": 2
    }
]