[
    {
        "id": "rfZmBK9m99",
        "forum": "qbw861vueP",
        "replyto": "qbw861vueP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2890/Reviewer_9sSt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2890/Reviewer_9sSt"
        ],
        "content": {
            "summary": {
                "value": "The authors formulate the dynamic sparse training problem as a bi-level optimization problem.  Based on the bi-level formulation, the authors propose an algorithm that solves the mask and weights in the network simultaneously.  Experiments show that the proposed algorithm outperforms the other methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors formulate the DST problem as a bi-level optimization problem. To solve the computational issue, the authors discard the hessian term in the gradient estimation, which does not result in any performance drop according to the experimental results. Further, to deal with the discrete issue of the mask, the authors introduce a condition to update the discrete mask. The experimental results show that the proposed algorithm outperforms the other methods."
            },
            "weaknesses": {
                "value": "1. The proposed algorithm does not have any convergence guarantee. Different from other bi-level optimization algorithms, the algorithm is based on a rough estimation of the gradient (eliminating the inversion of the second-order derivative) and is facing the discrete issue of the mask ( the estimation of the gradient is not at the point of the continuous variable of the mask but a hard discretization of the soft mask).  Thus, it is important to have a theoretical guarantee to ensure the performance of the proposed algorithm.\n\n2. The upper-level objective is non-natural to me. Since the upper-level problem is also a finite sampling objective, we need some regularization to get good results. Meanwhile, the nature regularization is the l2 norm (i.e., $||\\theta^*(\\psi)||$).  With this regularization, we can formulate the upper-level problem as the same as the lower-level problem. Thus, the problem can be formulated into a single-level problem, which is easier to solve than a bi-level problem.  Further, people use bi-level formulation in machine learning usually when upper-level and lower-level objective contains different dataset,"
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2890/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698650012580,
        "cdate": 1698650012580,
        "tmdate": 1699636232551,
        "mdate": 1699636232551,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gl3breD5Ub",
        "forum": "qbw861vueP",
        "replyto": "qbw861vueP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2890/Reviewer_MEAC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2890/Reviewer_MEAC"
        ],
        "content": {
            "summary": {
                "value": "Dynamic Sparse Training (DST) methods are the state-of-the-art in sparse training and sparse mask finding methods. They typically work by learning both a mask and weights for a model jointly during training, however while the model weights are learned using back propagation, the mask itself is typically learned using a heuristic such as random (SET) or gradient norm (RiGL). The authors propose a DST method, BiDST, to instead jointly optimize the mask and weights using an approximation to the bi-level optimization framework. The authors present state-of-the-art results on BiDST compared to existing state-of-the-art DST methods using ResNets on CIFAR-10/100 and ImageNet."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper is well written in the majority, with a clear motivation, background and experimental setup. The unfortunate exception to this writing is some of the key parts of the methodology most important to comparing it with existing DST methods, and understanding the speedups claimed.\n* One of the weaknesses of the current DST methods is the heuristic nature of mask updates, anything that gets away from heuristics on this front is a welcome development and the proposed method is indeed a data-driven approach.\n* The bi-level methodology is intuitively suited to the DST framework, as described by the authors, at least in the original form.\n* The experimental methodology, in terms of models, datasets, training regimes, sparsity levels and compared baselines is excellent, and it is obvious that the authors are very familiar with the existing DST literature from their usage in this, in particular the comparisons across different multiples of training epochs.\n* Although the background is short, and awkwardly positioned in the paper, it does cover much of the relevant literature in terms of citations, if perhaps not explanation. Much of this is also covered implicitly in the methodology/results in comparing to existing methods.\n* For the most part, the results would appear to be state-of-the-art for a DST method overall, and are very motivating for the proposed method, at least assuming that BiDST is in fact comparable to existing DST methods - i.e. it is in fact sparse training (not dense), and is not significantly more expensive (although the latter at least appears to not be true). The authors discuss that they attempt to make the results comparable by training BiDST for less time, which is good, although at the end of the day it's not clear to me how fair a comparison this is."
            },
            "weaknesses": {
                "value": "* The reason directly optimizing the mask is typically not done is of course that a (hard) binary mask is non-differentiable. Using a soft mask in any form would necessarily be dense, not sparse, training. I think the authors do not focus on explaining what they do with respect to both of these things enough in the paper as it is currently written, as I'm still not confident I understand how this is addressed by BiDST. I believe the authors use the Gumbel-Softmax trick, and while that might explain how they learn a binary mask using gradients, that would mean that BiDST uses dense training as far as I can see. And yet the authors demonstrate sparse training acceleration in their results (and this itself is unclear). I have asked this in the question section in more detail, and a comprehensive answer by the authors is vital in my being able to understand this paper better as a reader and a reviewer. Understanding the fact of if BiDST is in fact doing dense weight/mask updates is key in understanding if it is fair to compare BiDST with existing DST sparse training methods, such as SET, as the main motivation of these methods is to reduce compute during training.\n* In the explanation below Equation 8 in section 2.2, the authors casually mention that they simply replace the second-order partial derivatives with 0 in their derivation of the weight/mask-update rules for BiDST. Isn't it precisely these second-order partial derivatives that carry much of the information of the relationship between the mask and weight parameters? There is no discussion on the effect of this, or why this is a reasonable thing to do aside from the fact it is expensive to calculate. Note that the first-order partials are retained. After all the fanfare of BiDST doing optimization \"properly\" compared to existing DST methods in the motivation, this is quite a let-down, and I think the authors should note this earlier/be careful of giving the impression of over-claiming and be much more transparent that they are crudely approximating the bi-level optimization methodology.\n* As explained in 3.1, BiDST experiments are presented using fewer training epochs to ensure a \"fair\" comparison due to the \"mask learning computation cost\", however it's not detailed what exactly is the fair number of epochs to compare and why, and I didn't see any numbers or details on what the \"mask learning computation cost\" overhead is w.r.t. existing DST and other sparse training methods.\n* It is not clear how significant some of the ImageNet results are, being within 0.2 percentage points of the baselines in some cases. Because of this it really stands out as suspicious that although the authors stress that ImageNet experiments are all performed three times, we are not given the variance across these three runs to aid us in understanding the significance of the results, as for example done with CIFAR-10/100. The authors say \"we omit these because it's less than 0.1% std dev (is this percentage or percentage points?). Why not just show them in the table? I would not ask for multiple imagenet runs normally, but if you've performed them and are stressing that you did, why hold back on showing the variance?\n* In section 3.3, the authors use IoU to analyze the evolution of the mask during training and compare to existing DST methods, claiming \"BiDST achieves better mask development\" based on low IoU compared to other DST methods. However, while potentially interesting, the motivation for this analysis is nowhere near convincing enough to make such a bold claim.\n* In section 3.4, when explaining the \"sparse training\" speedups shown, the authors explain \"training acceleration is obtained from compiler optimization that skips the zeroes weights\". If this is a compiler optimization, it's static analysis, i.e. a fixed mask. How is this possible for a DST method that is changing masks potentially every iteration, or is the \"training engine\" actually to speed up inference timings? This explanation is important as speeding up unstructured sparsity on real-world hardware is something that is not easy, and in fact is well worth a standalone publication if it was truly being solved so easily by the authors. \n* The background is very short, and awkwardly positioned in the paper. It appears much of the relevant background is distributed throughout the method implicitly by citing baselines and methodology, but personally I'm a fan of the traditional consolidated background before a method."
            },
            "questions": {
                "value": "I have currently rated the paper borderline as although the results are impressive, and the method appealing, I don't believe that the explanation of the method is detailed or clear enough right now to understand if this is in fact comparable to other sparse training/DST methods. I would appreciate detailed feedback from the authors to address the main points for which I am currently confused on after reading their paper:\n\n* Is BiDST sparse training or dense training? i.e. are dense gradients or dense weights used during training? Please detail how explicitly. I'm confused because learning the mask necessarily means that the mask cannot be binary (it's approximated by Gumbel-Softmax I believe), and yet you show speedups during training due to \"sparse training\". Indeed the mask update rule in Equation 11 appears to be dense, and it would appear this is run every iteration. There is some text on selecting a subset of the mask, but that doesn't go into anywhere near as much detail as you need to make this point clear and understandable.\n* In the explanation below Equation 8 in section 2.2, the authors casually mention that they simply replace the second-order partial derivatives with 0 in their derivation of the weight/mask-update rules for BiDST. Why this is a reasonable thing to do aside from the fact it is expensive to calculate? What is the effect on the optimization/ability to learn the mask/weights jointly?\n* As explained in 3.1, BiDST experiments are presented using fewer training epochs to ensure a \"fair\" comparison, however it's not detailed what exactly is the fair number of epochs to compare and why. How many steps does a \"single\" BiDST iteration take compared to e.g. SET? Give numbers/details on what the \"mask learning computation cost\" overhead is w.r.t. existing DST and other sparse training methods.\n* What exactly is the variance of the ImageNet results, not clear if stddev is 0.1% is percentage or percentage points.\n* In section 3.4, if this is a compiler optimization, i.e static analysis, then it must necessarily be on a fixed mask? How is this possible for a DST method that is changing masks potentially every iteration, or is the \"training engine\" actually to speed up inference timings? If it is not static analysis/a fixed mask, explain in detail how speedups are achieved on real-world hardware for unstructured sparsity."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2890/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698816800103,
        "cdate": 1698816800103,
        "tmdate": 1699636232479,
        "mdate": 1699636232479,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1z9yFiwRWz",
        "forum": "qbw861vueP",
        "replyto": "qbw861vueP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2890/Reviewer_xhao"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2890/Reviewer_xhao"
        ],
        "content": {
            "summary": {
                "value": "The paper discusses a class of methods known as dynamic sparse training (DST). These are methods for efficiently training sparse neural networks. The paper draws connections between DST and bi-level optimization (BLO) and explains that BLO is a suitable framework for the two levels of mask and weight training in sparse learning. They evaluate a particular instantiation of this framework which iterates between applying a gradient update to the mask variables and training the masked weights with SGD."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors explain in detail how sparse training and bi-level optimization are related.\n\nThe method performs well, and the authors propose a method of restricting the mask search space in order to keep training efficient."
            },
            "weaknesses": {
                "value": "L2 regularization is generally applicable to other DST methods and should be tested. It appears BiDST with the smallest L2 penalty in the appendix would underperform relative to the other baselines.\n\nFor Figure 4, I do think it is interesting to point out that BiDST changes the mask more than the baselines do, but I'm not convinced that a quickly changing mask (Figure 4) is necessarily something we want. \n\nI am also wondering why Figure 4 does not also reflect the frequency of the mask changes. In the tables it shows that BiDST usually makes fewer mask updates, but looking at Figure 4 I would assume all methods make one mask update per epoch."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2890/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2890/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2890/Reviewer_xhao"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2890/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698817094802,
        "cdate": 1698817094802,
        "tmdate": 1699636232408,
        "mdate": 1699636232408,
        "license": "CC BY 4.0",
        "version": 2
    }
]