[
    {
        "id": "lghDujdlJY",
        "forum": "bfRDhzG3vn",
        "replyto": "bfRDhzG3vn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3457/Reviewer_ZUcH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3457/Reviewer_ZUcH"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a continual learning method, COCONUT, for spoken language understanding. This method uses two contrastive learning objectives in order to mitigate the catastrophic forgetting issue. COCONUT is evaluated on two popular SLU datasets, FSC and SLURP. Results show that COCONUT outperforms baselines when combined with some knowledge distillation techniques. A thorough ablation study is also presented to show the importance of the design of both contrastive losses."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "**Originality**: The modified version of the proposed contrastive loss, Negative-Student Positive-Teacher loss, is novel.\n\n**Quality**: The paper has presented detailed and thorough analysis of experimental results. However, the results of the proposed method is not strong enough.\n\n**Clarity**: The writing of this paper is coherent.\n\n**Significance**: The paper has some impact on the speech community, especially those working on spoken language understanding. The design of NSPT loss can be applied to other tasks, both within the speech domain and in other modalities."
            },
            "weaknesses": {
                "value": "(1) COCONUT does not outperform other baselines in a more challenging dataset (SLURP), limiting its contribution.\n\n(2) The architectural design of COCONUT is rather complicated and onerous."
            },
            "questions": {
                "value": "What is the running time of COCONUT compared to other baselines? Does it cost significant more time to run COCONUT?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3457/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698638239460,
        "cdate": 1698638239460,
        "tmdate": 1699636298500,
        "mdate": 1699636298500,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tpioej9fyN",
        "forum": "bfRDhzG3vn",
        "replyto": "bfRDhzG3vn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3457/Reviewer_YNzV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3457/Reviewer_YNzV"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a contrastive learning framework to overcome catastrophic forgetting in the class-incremental learning setting of spoken language understanding, which contains two main losses. First, the paper modifies the standard supervised contrastive loss from Negative-Teacher Positive-Teacher to Negative-Student Positive-Teacher, while applying loss on rehearsal samples only. Second, a multimodal contrastive loss is also use to align the audio and text features."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The modification of loss function is interesting to mitigate catastrophic forgetting for seq2seq SLU models.\n2. Experiments on two benchmarks and the ablation studies verify the effectiveness of proposed method over the previous baselines, as well as the two proposed losses."
            },
            "weaknesses": {
                "value": "The main weakness of this paper is the unclearness in text and the insufficient in experiments. Please see the Questions part for details."
            },
            "questions": {
                "value": "1. For the design of NSPD loss, 1) since the author highlights that the NSPD loss is conducted only on rehearsal samples, what if the loss is conduct on all samples?; 2) In equation 5, why i belongs to I instead of I_c? (according to the figure 2, it seems that the repulsion is applied on new class (i.e., I_c) samples).\n2. For the design of multimodal loss, when the loss is applied, only in the first task (to initialize the projection) or in every task? I think the main motivation of this paper is to overcome catastrophic forgetting. However, the multimodal loss is more like a trick for better results instead of for catastrophic forgetting? By the way, is this paper the first work to apply multimodal loss on SLU tasks?\n3. For the text encoder, in my understanding, it is just an embedding layer instead of a module? If this is the case, I think there is no need to call it text encoder. For example, this kind of embedding layer also exists in GPT-3.5. But it is common to call that GPT-3.5 has only a text decoder (decoder-only structure) instead of a text encoder (embedding layer) and an auto-regressive decoder. The text encoder in the paper is quite confusing. Please let me know if I understand mistakenly.\n4. For the experiments, 1) More results are needed at different ER ratio (2-4%, 5-10%) to show that the proposed method can produce consistent improvement in different settings. 2) Just a question, why the results on FSC-6 in Table 1(76.46) and Table 2(77.09) are different?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3457/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3457/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3457/Reviewer_YNzV"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3457/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698650085783,
        "cdate": 1698650085783,
        "tmdate": 1700623400470,
        "mdate": 1700623400470,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Do3Fl3PBgC",
        "forum": "bfRDhzG3vn",
        "replyto": "bfRDhzG3vn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3457/Reviewer_eVfc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3457/Reviewer_eVfc"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses spoken language understanding (SLU) in a continual learning setting. End-to-end joint ASR-SLU approach is used (no cascade).\nA new approach called COCONUT is presented that uses both experience replay and contrastive learning losses (NSPT: a contrastive KD loss and MM a multimodal loss that aligns audio-text representations).\nExperiments are made on two SLU benchmarks: SLURP and FSC; the exact SLU task is intent classification. The continual learning setting used is the one from (Capellazzo & al 2023). Experiments show that COCONUT can compete with experience replay  (ER) of buffer capacity of 1% (but is worse than ER with buffer capacity 2%)"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "-a new approach for SLU in a CL setting that is better than a strong experience replay (ER) benchmark\n\n-experiments on 2 popular SLU benchmarks that demonstrate the effectiveness of the proposed appproach"
            },
            "weaknesses": {
                "value": "-more details on continual learning setting used would have been welcome (ref to (Capellazzo & al 2023) is not very self-explanatory)\n\n-experience replay (ER) baseline with buffer capacity of 2% is still better than COCONUT and it is unclear how using twice memory (2% instead of 1%) is a real bottleneck in real applications (authors could have commented this more)"
            },
            "questions": {
                "value": "-how COCONUT could be adapted to more speech tasks ?\n\n-experience replay (ER) baseline with buffer capacity of 2% is still better than COCONUT and it is unclear how using twice memory (2% instead of 1%) is a real bottleneck in real applications (authors could have commented this more)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3457/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698775886148,
        "cdate": 1698775886148,
        "tmdate": 1699636298335,
        "mdate": 1699636298335,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "d4OqGZvkS8",
        "forum": "bfRDhzG3vn",
        "replyto": "bfRDhzG3vn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3457/Reviewer_6YBX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3457/Reviewer_6YBX"
        ],
        "content": {
            "summary": {
                "value": "- The paper targets end-to-end SLU using a seq2seq style of model.\n- This work aims to address problems of balancing efficient and performant continual learning of new tasks (intents) without the effects of catastrophic forgetting.\n- To these ends, they introduce a model architecture and set of losses to take advantage of student-teacher network training, contrastive learning, and multi-modal (speech/text) alignment.\n-  The approach (COCONUT) is applied to two datasets, FSC and SLURP, where the approach is highlighted.\n- COCONUT is shown to outperform other methods in almost all cases, except when combined with the next best approach (S-KD) or when using a larger memory for ER.\n- Ablation shows impact of memory size on effect of COCONUT vs ER."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Clear presentation of motivations behind the combination of losses, the decisions behind whether to use student vs teacher examples, etc.  Nice figures and appropriate complexity to educate without losing the goal of the paper in the weeds."
            },
            "weaknesses": {
                "value": "For readers that may not be as familiar with results of other SLU work (both E2E and non-E2E), inclusion of results from other work could be useful. Or if such comparisons are not fair, perhaps a note in the table to that effect. The text mentions the other work which describes those rows (like S-KD), but it could be nice to see numbers from other work itself as well (?) for clearer context as well as results of conventional SLU approaches that are not E2E."
            },
            "questions": {
                "value": "It may be possible to better include results from prior work and non-E2E approaches to give more context to the results within the tables.\n\nAlso, one grammar note, I believe \"sensible\" should be \"sensitive\" here in the last paragraph of section 5.3:\n          Note that, however, the model does not seem very sensitive to the temperature for the Avg Acc, whereas the Last Acc is more influenced."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3457/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698820194920,
        "cdate": 1698820194920,
        "tmdate": 1699636298264,
        "mdate": 1699636298264,
        "license": "CC BY 4.0",
        "version": 2
    }
]