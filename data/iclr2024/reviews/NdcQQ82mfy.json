[
    {
        "id": "lzgl9tAqdQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5005/Reviewer_pX8W"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5005/Reviewer_pX8W"
        ],
        "forum": "NdcQQ82mfy",
        "replyto": "NdcQQ82mfy",
        "content": {
            "summary": {
                "value": "Update on November 20:\n\nI raised my score to 6 based on the authors' responses. I am willing to keep discussing with the authors and the other reviewers to achieve a fully discussed final score.\n\n\n---\nThis paper proposes a novel iterative collection and filtering framework to leverage a combination of online and offline RL techniques. Experiments show it consistently outperforms the previous SOTA approaches. Moreover, the authors claim that the proposed approach can be regard as a plugin orthogonal to peer methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The experiment is thorough. The authors provide very detailed empirical results to demonstrate the effect of the proposed approach.\n2. Clear writing, the paper is clearly structured and easy to go through flow.\n3. The proposed approach is technically sound. The employment of RL in this task is technically sound."
            },
            "weaknesses": {
                "value": "1. Concerns about the motivation. Personally, I am not convinced about the motivation to introducing RL in the branching task. As reported in some previous work [1,2], the number of expanding nodes of FSB is significantly less than that of SOTA ML approaches, indicating that *FSB is a good enough expert policy to imitate*. Instead, the bottleneck in this topic may mainly lie in the low IL accuracy. The lower accuracy may due to the unsatisfactory model structure or the insufficient information in the widely-used bipartite graph states (observations, more precisely). However, currently I found no definitive answer about that in recent researches. The ablation study in a recent research [3] gives clues that the historical information is effective as the process is a POMDP. Thus, in my personal opinion, the latter reason seems closer to the truth. Thus, based on these results, I think all researches that proposing more complex online RL framework is somewhat incremental, as improving the IL accuracy seems to be the key.\n2. Concerns about the unnecessary complexity for introducing RL. Though usually higher asymptotic performance, many RL approaches are usually sensitive to the hyperparameters, making their application requiring much manual tunings. I am doing research in both RL and CO, empirically, I found their combinations can be fragile. In this paper, the online RL is mainly used for data collection. However, as far as I know, RL approaches are usually sensitive to the data distribution due to the deadly triad. For example, TD3 [4] may fail in MuJoCo tasks when the initial 20k random steps of data collection is turned off; BCQ [5] may fail when the offline data are collected with hybrid policies. Thus, in real-world CO tasks, I prefer use simple GNN+IL approach rather than other complex but fragile approaches, even if they claim higher performance in the four synthetic benchmarks. For this paper, I am concerned about the complexity for introducing RL.\n3. More explanations is required. In this paper, the RL based approach achieves lower IL accuracy while higher e2e performance. Thus, I am interested in what kind of multi-step information it learns from the input states. I believe this is more meaningful than simply reporting performance improvements, as it can guide us in designing better input features as mentioned in Point 1. If only the approach is given but the explanations is missed, then this paper is more like just a application of existing RL approaches to a new \"Atari\".\n4. Limited improvement compared to ML4CO-KIDA. The performance of HRL-Aug reported in Table 7 seems to be marginal to ML4CO-KIDA.\n\nAnyway, the comments above may be too tough for studies in this research field. Thus, if the other reviewers all give more  positive scores, I will be willing to keep discussing with the authors and the other reviewers to achieve a fully discussed score.\n\n[1] Gasse, Maxime, et al. \"Exact combinatorial optimization with graph convolutional neural networks.\" Advances in neural information processing systems 32 (2019).\n\n[2] Gupta, Prateek, et al. \"Hybrid models for learning to branch.\" Advances in neural information processing systems 33 (2020): 18087-18097.\n\n[3] Seyfi, Mehdi, et al. \"Exact Combinatorial Optimization with Temporo-Attentional Graph Neural Networks.\" Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Cham: Springer Nature Switzerland, 2023.\n\n[4] Fujimoto, Scott, Herke Hoof, and David Meger. \"Addressing function approximation error in actor-critic methods.\" International conference on machine learning. PMLR, 2018.\n\n[5] Fujimoto, Scott, David Meger, and Doina Precup. \"Off-policy deep reinforcement learning without exploration.\" International conference on machine learning. PMLR, 2019."
            },
            "questions": {
                "value": "1. Is the online RL training necessary? Intuitively, using an offline-trained policy to collect data and then update the policy with the collected data iteratively seems to be enough. Why is the online updating of the policy during data collection necessary?\n2. What is the dual bound change when a child node is infeasible? In SCIP, the solver add the objective and the current global primal bound to the constraints, making the infeasible child appears in high frequency. Thus, in the calculation of FSB scores, the score of infeasible child is set to a dynamic large value. How about that in this paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5005/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5005/Reviewer_pX8W",
                    "ICLR.cc/2024/Conference/Submission5005/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5005/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697424161809,
        "cdate": 1697424161809,
        "tmdate": 1700413323351,
        "mdate": 1700413323351,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1PoTdYtmGx",
        "forum": "NdcQQ82mfy",
        "replyto": "NdcQQ82mfy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5005/Reviewer_Gdrc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5005/Reviewer_Gdrc"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a hybrid RL approach for variable selection in mixed integer programming (MIP). The proposed approach uses online RL to select between a rule-based expert and imitation learning based policy. The data collected from this online agent is filtered by a mechanism referred to as the offline agent and sent to a GCNN-based imitation-learning agent. This process repeats iteratively for a number of iterations. Results are presented on a number of integer programming problems and compared to an open-source and commercial solver with expert branching rules and two imitation learning methods. The proposed method outperforms the baselines on most domains and an ablation shows the importance of the proposed sub components."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Overall the results in the paper look strong. I do not have much experience with the MIP problem and cannot comment on the baselines but from the presentation they seem relevant and appropriate. From an RL and algorithmic perspective there are some additional ablations that could be interesting, but the presented ablations seem reasonable to me."
            },
            "weaknesses": {
                "value": "To me, the primary weakness in the current draft is the presentation. The ordering of the Tables in the Experiments section are confusing - tables are introduced and described in a non-sequential order and there are typos and odd sentences in the text that make the descriptions hard to follow at times. For example Table 3 and 4 show the Ablation results of comparing HRL-Aug to ML4CO-KIDA while the main results are presented later in Table 7 and 8. Just re-ordering the Table sequences would make it much easier to read.\n\nI have a number of questions and clarifications relating to the algorithm which I will described under `Questions` but could also count as weaknesses. \n\nApart from that there are a few typos and textual clarifications I will list below:\n\n    1. Page 4 sentence 1: \u2018Framework\u2019 should be \u2018framework\u2019.\n    2. Section 3.3: off-policy methods should be \u2018offline methods\u2019.\n    3. Section 4.1: Dataset has `We\u2019 appear in many places which should be `we\u2019. \n    4. Section 4.4 mentions: \u201cWe genuinely appreciate the collaborative effort of the reviewers and their invaluable role in shaping our work\u2019. This is an awkward phrase that isn\u2019t warranted before the review process? To me, it seems likely this is a vestige of a resubmission."
            },
            "questions": {
                "value": "My questions mostly relate to the RL part of the proposed approach.\n\n1. What agent was used to train RL online? Was it a REINFORCE-like algorithm or something more complex? This may be important because the convergence guarantees of RL algorithms are often made in the discounted setting with \\gamma < 1. It is also important to specify the precise algorithm used for reproducibility. \n2. I\u2019m confused as to the \u2018Offline RL\u2019 agent. To me, offline RL implies learning a policy i.e. an action selection mechanism - using fixed trajectories of data that were generated by some behavior policy (or policies). In the paper, what is described is more of a filtration mechanism where effectively a function is fit to returns generated in the first iteration and then subsequently used to threshold the trajectories to sample. Could the authors clarify this point?\n3. There are a number of choices made in the paper whose impact is not clear. For example, the filtration mechanism parameters are fit on the first iteration and kept constant after. Equation 4 uses a \\lambda parameter to regularize the fit.The online agent is trained every 5 iterations (Freq). Some unspecified choice of `z\u2019 defines how the trajectories are sub-sampled. It is not clear to me how important any of these choices are. Ideally the paper would include ablations on these but I can understand the difficulty of presenting so much information with a page limit. If the authors could indicate why these choices were made (with possible empirical evidence in the Appendix), I think it would make the paper stronger."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5005/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698670026943,
        "cdate": 1698670026943,
        "tmdate": 1699636488376,
        "mdate": 1699636488376,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zScUKKqTLG",
        "forum": "NdcQQ82mfy",
        "replyto": "NdcQQ82mfy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5005/Reviewer_f3Ne"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5005/Reviewer_f3Ne"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the use of imitation learning (IL) and Reinforcement Learning (RL) to tackle Mixed Integer Programming (MIP) problems. The main contribution of the paper is a framework that is specifically designed for MIP problems. Empirical results on a number of MIP problems show that the proposed method can achieve good performance with reduced model training time."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "**originality**\n- The paper's main novelty is a new framework that is designed to use IL and RL to tackle MIP problems. \n\n**quality**\n- The overall presentation is good\n- The paper discusses related works in a fairly clear manner\n\n**clarity**\n- Overall the paper is clear and easy to follow, however, the explanation on how the agent works can be improved\n\n**significance**\n- The paper studies how IL and RL can be applied to the MIP problems, which seems to be an important research direction\n- The improved training time and better performance can be a significant result."
            },
            "weaknesses": {
                "value": "Discussion on the proposed method:\n- I find the writing on how the proposed method works is a bit confusing, this might be partly due to the complexity of the problem. For example, what exactly actor critic method did you use in the online setting (for example, A type of SAC? DQN?)? And how exactly does the online RL agent decide whether to use the expert or to use the learned policy? (if for example, you say the action space is discrete, with 2 actions, one to choose the expert and the other to choose the learned policy for the online RL agent and it is a Q-learning type of agent, then it becomes much clearer) Currently I don't fully follow what is happening here. (also see questions)\n- Another concern is the proposed method seems to only apply to only the MIP problems, and although the empirical results are interesting, it is a little unclear to me how much technical novelty is in the design of this framework and whether the contributions in this paper is significant enough."
            },
            "questions": {
                "value": "- How time consuming is strong branching compared to your method? Can you provide a wall-clock time comparison? \n- It is a bit unclear to me how exactly does the online agent decide whether to use the learned policy or expert policy? \n- Page 5, section 3.3, \"batch DRL\" is a general concept and is essentially the same as \"offline DRL\", which is initially discussed in some earlier papers (e.g. \"Off-policy deep reinforcement learning without exploration\" by Fujimoto et al.). The method in Chen et al., 2020 (Best Action Imitation Learning) is one of the imitation learning methods to tackle the batch/offline DRL problem. You might want to change the writing here to be more accurate. \n- What is the \"ActorCritic\" algorithm you are using? \n- Table 6 why there is no highlight on the best performing method for each task? Or the values are not related to performance?- Typically RL agents can take time to train, why is it the case that the proposed method, despite its complexity and a multi-stage/agent setup, can reduce training time compared to other methods?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5005/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698813806826,
        "cdate": 1698813806826,
        "tmdate": 1699636488289,
        "mdate": 1699636488289,
        "license": "CC BY 4.0",
        "version": 2
    }
]