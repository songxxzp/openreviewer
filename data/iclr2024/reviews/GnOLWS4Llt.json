[
    {
        "id": "ukWWR7ITJH",
        "forum": "GnOLWS4Llt",
        "replyto": "GnOLWS4Llt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8057/Reviewer_dUhS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8057/Reviewer_dUhS"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to implement offline reinforcement learning through better trajectory stitching. The main challenge here is that the problems being looked at involve sequences of observations rather than clear states. Because these states aren't well-defined, it's not easy to link similar ones together, making the \"stitching\" hard. To address this, the authors present an approach to learn a compact representation of history comprising only features relevant to action selection. They adopt a bisimulation loss to do this and show that with this approach, offline reinforcement learning can be more efficient in terms of the worst-case sample complexity."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is written in a very clear and understandable manner.\n\n2. The concept of stitching together trajectories based on observations rather than states is quite applicable, especially since in many real-world situations, we don't have direct access to the game's state.\n\n3. The proposed solution is strongly backed by thorough theoretical analysis."
            },
            "weaknesses": {
                "value": "1. This solution appears to be heavily dependent on the concept of trajectory stitching, which might limit its usefulness in other offline reinforcement learning (RL) scenarios.\n\n2. I\u2019m a bit uncertain about the novelty of training an offline RL model based on observation histories. It seems that in most offline RL research, what an agent can access is already observations (rather than states) from previous data collections."
            },
            "questions": {
                "value": "Can you explain how your solution might work with other offline RL methods that don't focus much on trajectory stitching (for example models with a pessimistic formulation mentioned in the related works)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8057/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8057/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8057/Reviewer_dUhS"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8057/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698682664383,
        "cdate": 1698682664383,
        "tmdate": 1699636996080,
        "mdate": 1699636996080,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zLAnPPgTbZ",
        "forum": "GnOLWS4Llt",
        "replyto": "GnOLWS4Llt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8057/Reviewer_Hq7u"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8057/Reviewer_Hq7u"
        ],
        "content": {
            "summary": {
                "value": "The authors focus on the sample efficiency issue of  offline RL in the setting of POMDP with observation histories. Based on the observaton that two different trajectories would always have different histories, which makes stitching become much harder because no real state can be observed,  the authors propose to learn a more  realistic belief state using the bismulation loss and show that this may significantly reduce the sample complexity when working in this space because there is no need to consider complexity introduced by the observation sequence any more."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper show the usefulness of state abstraction in the setting of offline RL."
            },
            "weaknesses": {
                "value": "see below"
            },
            "questions": {
                "value": "1) The reason that the bound of the sub-optimality of the learnt policy listed in Theorem 5.1 is much tighter than the bound listed in Theorem 4.1 is mainly due to the fact that the space sizes follow $|Z|<<|H|$, which is assumed. However, the state represenation space could still be very large. In addition, this bound does not work in non-tabular POMDPs.\n\n2) The loss function $J(\\phi)$ of the proposed method aims to clone the bisimulation metric $|\\hat{s}(z,a)-\\hat{r}(z',a')|-D(\\hat{P}(\\cdot|z,a),\\hat{P}(\\cdot|z',a'))$ . However, the target is approximately estimated via $\\hat{r}$ and the dynamics $\\hat{P}$, which in turn are estimated based on the offline dataset D. Hence in essential, the proposed method belongs to model-based offline RLs and should compare the proposed method to current model based offline RL methods.\n\n3) The comparison study conducted in Table 1 may not be fair, because the three methods listed are not designed to learn from observation histories, but they can only get the information from the current sampled tuples. In this sense, the good  performance of the proposed method may due to its capturing additional information from the historical data through state abstraction. \n\n4) We  suggest the authors  attach more experiments on some widely used standard benchmarks, such as MuJoCo suites, to evaluate the effectiveness and feasibility of the proposed methods."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8057/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698803099850,
        "cdate": 1698803099850,
        "tmdate": 1699636995959,
        "mdate": 1699636995959,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZjR0Cy0o4A",
        "forum": "GnOLWS4Llt",
        "replyto": "GnOLWS4Llt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8057/Reviewer_HexG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8057/Reviewer_HexG"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on demonstrating that \"trajectory stitching,\" a characteristic of offline RL algorithms in standard MDPs, is absent when these algorithms are applied in POMDPs, where observation histories influence the training and action selection of the policy. It begins by effectively motivating the issue with theoretical insights and subsequently proposes a bisimulation metric. This metric simplifies the observation histories, and this metric is readily usable by any offline RL algorithm."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is well-motivated and is easy-to-follow.\n2. It thoroughly explores the theory necessary to both - highlight the issues with current methods and introduce the proposed solution.\n3. The introduced bisimulation metric is versatile, easily integrating with popular offline RL algorithms."
            },
            "weaknesses": {
                "value": "1. The paper, despite its detailed theoretical sections, lacks comprehensive empirical evidence to substantiate its claims. It\u2019s unclear why the metric was exclusively tested and compared with CQL, omitting comparisons with more recent offline RL baselines such as IQL. Evaluation of this metric in a continuous action space environment, compared against IQL or XQL, would be beneficial.\n\n2. The observed performances of CQL+Bisimulation and vanilla CQL in VizDoom are notably similar, as is the case with ILQL with and without bisimulation in Section 7.3. Does this imply that the bisimulation metric primarily enhances performance in less complex environments like GridWorld?"
            },
            "questions": {
                "value": "1. Currently the results are presented solely in singleton games. Can the findings of this paper be extended to other environments, particularly where training and testing distributions vary?\n2. What criteria were used to determine the percentage of trajectories to be omitted in Filtered BC?\n3. In reference to Table 2, do the reported results encompass the entire human World dataset, or is it a selected subset? Additionally, how effectively does the method execute in the online Wordle environment? (see https://github.com/Sea-Snell/Implicit-Language-Q-Learning#playing-wordle)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8057/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698847707467,
        "cdate": 1698847707467,
        "tmdate": 1699636995845,
        "mdate": 1699636995845,
        "license": "CC BY 4.0",
        "version": 2
    }
]