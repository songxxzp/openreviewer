[
    {
        "id": "FZYCmCxtRQ",
        "forum": "AbXGwqb5Ht",
        "replyto": "AbXGwqb5Ht",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission962/Reviewer_Htsq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission962/Reviewer_Htsq"
        ],
        "content": {
            "summary": {
                "value": "The paper establishes that initialising the network with an ODE results in preserving smoothness of weights throughout training.  The authors do this through the framework of Polyak-\u0141ojasiewicz condition."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "Pros:\n\n- (Significance) It is an important result that shows the impact of initial conditions on the training of ResNets; furthermore, it is crucial that while the paper is mostly theoretical, the settings the authors propose are realistic and can generalise to the practical ResNet models.  I also checked the derivations and could not find any problems with them. \n\n- (Quality) The paper is well written, with thoroughly addressing the reproducibility aspects (through both derivations and the experiments)\n\n- (Clarity) The paper is clearly written, with great attention to detail\n\n- (Originality) While the notion of interpretation of ResNets as a discretisation of (Neural) ODEs has been a widely-discussed topic, it has not been analysed in the opposite direction. The authors address an intriguing question of whether every ResNet corresponds to a (Neural) ODE and show, to my knowledge, previously undocumented, impact of initial conditions on whether the ResNet can be presented as a discretisation of an ODEs"
            },
            "weaknesses": {
                "value": "Cons: \n\n-  (Significance elements) There are some limitations to the analysis, which are related to implicit generalisation to the convolutional and other sparse scenarios (see questions below)."
            },
            "questions": {
                "value": "Q 1: It seems to me that both fully-connected and convolutional neural networks would be the particular cases where the ODE parameter matrices A, V, W are considered sparse (some or most of the entries are fixed to zero and the rest are optimised according to Equation (5)). The experiment, depicted in Figure 3, suggests that results in Theorem 4 might be expanded to the more generic sparse scenarios.  Might be good if the authors could confirm this observation.\n\nQ2: If it happens that the weights are initialised randomly, for every finite depth of the neural network (as there\u2019s finite time and finite depth), there could be still a Lipschitz-continuous function (ii) over that set (as locally Lipschitz function on a compact set is Lipschitz as per, e.g. https://faculty.etsu.edu/gardnerr/5510/cspace.pdf ). However, what\u2019s appearing in Figure 3 appears to go beyond literal application of Theorem 4: not only the function is compact but also it appears that the Lipshitz constant is preserved to be small during the optimisation. \n\nQ3: Another interesting aspect is that  it would be intuitive to see smaller Lipshitz constant could be beneficial for out-of-distribution recognition. I wonder if the authors could clarify on this aspect? To what extent would the proposed derivation generalise for higher-than-two layers of multilayer perceptrons?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission962/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698696335699,
        "cdate": 1698696335699,
        "tmdate": 1699636022095,
        "mdate": 1699636022095,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9k10nXyEqP",
        "forum": "AbXGwqb5Ht",
        "replyto": "AbXGwqb5Ht",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission962/Reviewer_Hmrt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission962/Reviewer_Hmrt"
        ],
        "content": {
            "summary": {
                "value": "The paper establishes a connection between deep residual networks and neural odes.  Prior works have attempted to provide a theoretical framework for the same, but they come with several shortcomings -- like considered only simplified models with linear activations. The authors train the neural network with gradient flow and show that in the limit of $L \\rightarrow \\infty$, the network converges to a discretization of neural ode."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Most prior works in this domain make simplifying assumptions which prevents their analysis from being directly applicable to practical models. The authors overcome those assumptions"
            },
            "weaknesses": {
                "value": "N/A\n\nI am giving a score of 6, given my inability to appropriately verify all the proofs in the main text and appendices."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission962/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission962/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission962/Reviewer_Hmrt"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission962/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698788713709,
        "cdate": 1698788713709,
        "tmdate": 1699636022027,
        "mdate": 1699636022027,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Ntzmg8wOsu",
        "forum": "AbXGwqb5Ht",
        "replyto": "AbXGwqb5Ht",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission962/Reviewer_6832"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission962/Reviewer_6832"
        ],
        "content": {
            "summary": {
                "value": "This paper shows that the dynamics of a residual connection converges that to of an ODE. \n\nFirst, the authors show that under certain conditions (like the boundedness of the Frobenius norms of the weight matrices) for finite training time as the depth of the network tends to infinity the dynamics of a resnet converge to that of an ODE. This is assuming that the model is trained with clipped gradient descent. \n\nSince here the authors show that the Frobenius norm of the different of the weight matrices is upper bounded by a value that is O(exp(T)), the to show the result for when both T\u2192 \\infty and L \u2192 \\infty, the authors also assume that residual network satisfies a PL condition. \n\nTherefore, the authors show that under these condition as T\u2192 \\infty and L\u2192 infty, the dynamics of a residual network tend to an ODE and converges to a function that interpolates the training data."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is very well written and easy to follow, where the results and their implications are clearly stated.\n\nThe authors show weights of a resnet convert to an ODE while considering the dynamics of the network as well, which is new and interesting. \n\nGiven that the result shows that the dynamics of the resnet architecture converges to that of an ODE, and therefore as the authors mention one can utilize existing generalization bounds for neural ODEs etc can under certain conditions now be applied to resnets, given the results that has been shown by the authors.\n\nThe authors also show that their results/assumptions hold very well on their synthetic dataset.\n\nFor the cifar dataset, the authors show that their results hold when using smooth activations and the dynamics may deviate from neural ODE if one uses a non-smooth activation like ReLU."
            },
            "weaknesses": {
                "value": "The results are under the condition that the resnet is trained under clipped gradient flow, which is often not used in practice. However, I do understand that the main contribution of the paper is to characterize the behaviour of the model in the limit, so it shouldn\u2019t affect the main contribution of the work."
            },
            "questions": {
                "value": "For the experiments on cifar, the authors show the plot of a random coefficient and its value as one goes deeper into the layer. However, it would be more beneficial to see the difference in Frobenius norm of the weight matrices as well."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission962/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698792290741,
        "cdate": 1698792290741,
        "tmdate": 1699636021956,
        "mdate": 1699636021956,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "24MRJ1UBrw",
        "forum": "AbXGwqb5Ht",
        "replyto": "AbXGwqb5Ht",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission962/Reviewer_NuSB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission962/Reviewer_NuSB"
        ],
        "content": {
            "summary": {
                "value": "This paper shows that an implicit regularization of deep residual networks towards neural ODEs still holds after training with gradient flow. In particular, this is shown for both large depth (via gradient clipping in finite time) and large time limits (assuming PL condition). Along the way, interesting aspects of the problem are discussed and the results are demonstrated numerically on simple simulations as well as on CIFAR10."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The derived results and the neural ODE limits seem rather interesting. The theoretical analyses seem rigorous and their discussions are quite thorough, though I haven't read the proofs in detail. It could be that I have missed a similar related work in the literature, but I do think this seems to be an important contribution, and thus could spark interesting future work on implicit regularization as well as furthering understanding of residual networks via the toolbox of ODEs. Besides, the paper is, in general, very well written, and nicely presented."
            },
            "weaknesses": {
                "value": "- One of the key drawbacks is requiring weight-tying or weight sharing. While results with weight-tied networks don't look bad, still this would have been nice to take care of. Can the authors elaborate on where all and where exactly does it interfere with the proof strategy?\n\n- The other issue is that the linear overparameterization $m > c_1 n$ would, in practice, be somewhat unreasonable and is thus a bit of a stretch. Maybe this is a norm in the literature, but do the authors have some ideas how to go around this?"
            },
            "questions": {
                "value": "Please see above. Besides:\n\n\n- Would it be possible to extend the approach to incorporate layer normalization?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission962/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699503018190,
        "cdate": 1699503018190,
        "tmdate": 1699636021858,
        "mdate": 1699636021858,
        "license": "CC BY 4.0",
        "version": 2
    }
]