[
    {
        "id": "TNXwSxcvC1",
        "forum": "mEJAp7Gh2F",
        "replyto": "mEJAp7Gh2F",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7004/Reviewer_yTpt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7004/Reviewer_yTpt"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces SPIL (base skill prior based imitation learning), which improves generalization of robotic manipulation tasks to new environments by biasing skill embeddings toward one of three predefined base skills for robot manipulation: translation, rotation, and grasping. The encoder learns a continuous skill embedding based on a discrete skill prediction of which of the three discrete skills should be executed, the current image observation (from two angles), and language instruction. A VAE, trained to maximize ELBO, maps H-length action sequences into a continuous skill embedding space and decodes them back into action sequences.\n\nAuthors evaluate their method on CALVIN and demonstrate similar to HULC performance on in-domain tasks but stronger performance on out-of-domain tasks. They also evaluate their method zero-shot in the real world where results again outperform HULC."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(1) The results look promising on generalization to new envs, outperforming HULC.\n\n(2) Authors set up a real world environment and got good results there over HULC.\n\n(3) Appendix has a derivation for the loss, though I did not look into this carefully."
            },
            "weaknesses": {
                "value": "Method Weaknesses\n\n(M1) Requires training a classifier on a sum of action magnitudes per dimension over the H-length horizon, which presumably requires ground truth labels on what skill an input action sequence corresponds to. Needing labels (or even needing to tune such a classifier if it were trained in an unsupervised fashion) is a significant limitation and not possible with most robotics datasets.\n\n(M2) Unable to scale to more skills: If a new skill is needed in a new domain, everything would need to be trained from scratch.\n\n(M3) Hard-to-classify action sequences: With explicit skill labeler supervision, it seems hard for SPIL to provide a skill embedding for H-length subtrajectories that have a mix of multiple base skills (such as one that encompasses the transition between translation and grasping an object).\n\n(M4) Skills are blind to the state, the meaningfulness of action sequences (ie: the skill embedding space has no understanding of which skills are good vs just encoding random behavior), and are only as informative as what is expressed in the action magnitudes of each dim. A skill not conditioned on the current state is hard to efficiently adapt to a new domain with a sufficiently different state distribution, as the learning algorithm must determine which skills are appropriate for the current state. For instance, grasping-related skills are not good to execute when an object is already in the robot\u2019s gripper.\n\n(M5) Skill priors have been studied a lot. Behavior Priors (Parrot [1], SKiP [2]--modulo the human feedback, OPAL [3]). How do the authors orient this work to those previous skill-learning frameworks? Appendix A4 compares to previous skill-based methods SpiRL and SkiMO, but these do not look like domain-generalization results. Based on Tables 1 and 2, Table 7 looks like in-domain results with train == test env. Is this correct? If so, these results do not seem particularly relevant to the paper\u2019s argument for better domain generalization.\n\n\n\nExperimental Results Weaknesses\n\n(E1) Lacking architectural ablations in general. There are a lot of modules in the architecture, and it is not clear to me why each of them is necessary. Some of them, including the discrete skill selector I mentioned earlier, seem to restrict the expressivity of this skill embedding space. What is the performance of the method without predefined base skills? This seems to be the crux of the paper\u2019s contribution, so an ablation is well-advised.\n\n(E2) All 10 Real robot rollouts on each task have \u201cidentical starting positions.\u201d What is the value of doing 10 rollouts with a (presumably deterministic) policy? Are object positions randomized too or made to match, as closely as possible, to the sim?\n\n\n\nPresentation Weaknesses\n\n(P1) Writing in several places needs work, including fixing grammar issues. Citations are not formatted properly (entirely separated by parentheses), hurting readability.\n\n(P2) Method section in general was quite hard to understand. Notation is confusing. For instance, $x$ is not defined in equation 2. I\u2019m assuming it is an action sequence from an expert demo. Later, in section 3.3, $\\tau_t$ is suddenly introduced, and it seems to represent the same thing as $x$, except that it is a predicted action sequence. If I\u2019m interpreting these variables correctly, perhaps a better naming would be $\\tau$ for expert demo action sequence, and ${\\hat{\\tau}}$ for the predicted action sequence. Naming of modules needs to be made less confusing. There are 4 modules that start with \u201cSkill,\u201d out of 5 modules total (Figure 2).\n\n(P3) Figure 2 references huber loss, on the reconstructed actions, but there seems to be no mention of huber loss in the paper (until the Appendix, where I see a $\\| x - \\hat{x} \\|_2$ term). It is also unclear what the cat loss in Figure 2 refers to.\n\n(P4) Listing equation 2 and then 4 seems a bit redundant. They look really similar besides renaming some variables. It would probably be less confusing to introduce the elbo loss in one equation instead of similar-looking equations 3 pages apart. Perhaps the loss equation 19 can replace equation 4. \n\n\n\nReferences:\n\n[1] \u201cParrot: Data-driven Behavioral Priors for Reinforcement Learning.\u201d Singh et al. https://arxiv.org/pdf/2011.10024.pdf\n\n[2] \u201cSkill Preferences: Learning to Extract and Execute Robotic Skills from Human Feedback.\u201d Wang et al.  https://arxiv.org/pdf/2108.05382.pdf\n\n[3] \u201cOPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning.\u201d Ajay et al. https://arxiv.org/pdf/2010.13611.pdf"
            },
            "questions": {
                "value": "(1) It seems like $w_k$ in equation 1 are learned (in training a Bi-LSTM). Where did the ground-truth action labels for Equation 1 come from? Does the learned skill embedding z-space not cluster grasping, rotation, and translation separately?\n\n(2) It would strengthen the paper to visualize the z-space (t-SNE) and color z-space points based on which of the 3 skills they are labeled as.\n\n(3) Is $p(z|y)$ just fitted to the action sequences in the data based on their class labels? Is it trained before the phase depicted in Figure 2? If there are two different phases as suggested by Figure 1 and 2, they should be clearly labeled as such.\n\n(4) Is the encoder from figure 1 $q_{\\phi}(z|x)$ finetuned in figure 2 as $q(z|x,y,c)$? Or are these different \u201cencoders\u201d?\n\n(5) Section 3.2.2: How are two embeddings (language goal embedding and language embedding) extracted from the language instruction alone? I understand that there is a shared task embedding space in joint language + goal image embedding space, but this part of the paper is not explained well.\n\n(6) Section 3.2.2: The authors write: \u201cThe policy $\\pi(\\cdot)$ should also identify the optimal base skill $y$ under the current observation.\u201d If $y$ is already an input to the decoder, why should the decoder predict $y$ again?\n\n(7) Looking only at the figures, how can the decoder (Skill generator) be frozen in Figure 2 if it only takes $z$ as input in Figure 1, and in Figure 2, is additionally conditioned on $y, c$?\n\n(8) Remove the list of percentages in 4.2.1, as they are not important to the argument of the paper on generalization to new envs."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7004/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7004/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7004/Reviewer_yTpt"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7004/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698539800369,
        "cdate": 1698539800369,
        "tmdate": 1699636820785,
        "mdate": 1699636820785,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9lEGELBTY7",
        "forum": "mEJAp7Gh2F",
        "replyto": "mEJAp7Gh2F",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7004/Reviewer_h9E8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7004/Reviewer_h9E8"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose a novel skill-prior based imitation learning algorithm. The proposed algorithm is able to learn skill priors from unstructured data, and use those skill priors in a language conditioned imitation learning setup. The structure of the paper is the following: first, the two stage algorithm is introduced, which first learns the skill prior distribution from the play dataset, and then learns a language conditioned imitation learning policy off of the demonstrations with labels. Next, the authors present some experiments, first in a sim environment, Calvin, and then in a real robot benchmark that they created. Unfortunately, the paper ends there without much more details, such as ablation experiments."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is comprehensive, showing the formulation of the skill prior informed imitation learning formulation, and learning the priors from the play data. The primary strengths of this paper are:\n1. Simplifying the skill prior space. Generally, discrete skill prior based works struggle from the chicken and egg problem of classifying skill priors from data and learning them properly. By constraining the skill priors to three semantic kind of actions (translation, rotation, grasp) the algorithm makes the problem tractable.\n2. Showing the algorithm scales to a real robot: a lot of time the results in simulation based papers can overfit to certain kinds of environments or quirks in the simulation, but the robot experiment show that the risk of such is not high."
            },
            "weaknesses": {
                "value": "However, there are certain major shortcomings in the evaluation in the paper and the algorithm, which are detailed below:\n1. The algorithm seems very much \"overfit\" to the Calvin benchmark, while not being very generalizable beyond the setup. As a primary example, even the three basic \"skills\" seem to be overfitting to the Calvin demo behaviors, since it ignores possible robot behaviors that mix two of these skills. One easy example is opening a hinged door requires rotation and translation at the same time, which isn't covered by the algorithm's use case. \n2. Another example could be the fact that the paper only focuses on skill-based manipulation algorithms, which is again a quirk of the Calvin benchmark's high-frequency control setup. However, recently there has been improvements in high-frequency controls that does not use a notion of skills, such as [1] or [2], which can be combined with learning-from-play-data algorithms such as [3] for a skill-free formulation. To show that skills are necessary for language conditioned imitation, either a comparison with such an algorithm, or a comparison on a different benchmark such as Language Table [4] would be quite useful.\n3. Similarly, the real world performance is quite poor from the algorithm, which could be a case of the preset skills not really capturing the diversity of human behavior, but this question is left unanswered in the paper. The authors seemed to be content by beating out the single real baseline, HULC, which also seem to be a poor fit for the problem in hand.\n4. While the language conditioning is presented as an important part of the algorithm, the \"grounding\" abilities are not convincing enough to show that it is a major part of the presented algorithm. Without a proper ablation experiment, this is hard to reliably conclude, which is also not presented in the paper. Similarly, ablation over the horizon may be quite important here, which is also not present in the main paper.\n5. Finally, how important is extra play data if there is already sufficient language conditioned, labelled data available to learn a policy? If that is the case under which we are operating, can this algorithm still be called \"learning from unstructured data\"? Such questions can be answered by varying the dataset size, but because of an overdependence on Calvin as a benchmark, the authors are unable to present a real answer/experiment for this.\n\n[1] Zhao, Tony Z., et al. \"Learning fine-grained bimanual manipulation with low-cost hardware.\" arXiv preprint arXiv:2304.13705 (2023).    \n[2] Chi, Cheng, et al. \"Diffusion policy: Visuomotor policy learning via action diffusion.\" arXiv preprint arXiv:2303.04137 (2023).    \n[3] Cui, Zichen Jeff, et al. \"From play to policy: Conditional behavior generation from uncurated robot data.\" arXiv preprint arXiv:2210.10047 (2022).    \n[4] Lynch, Corey, et al. \"Interactive language: Talking to robots in real time.\" IEEE Robotics and Automation Letters (2023)."
            },
            "questions": {
                "value": "1. How was the horizon length of 5 decided upon?\n2. As I understand, the three base skills are interchangable, so how are they labelled as \"translation\", \"rotation\", and \"grasp\"?\n3. How large were the real world datasets?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7004/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698786199661,
        "cdate": 1698786199661,
        "tmdate": 1699636820665,
        "mdate": 1699636820665,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iKyGLtcC3z",
        "forum": "mEJAp7Gh2F",
        "replyto": "mEJAp7Gh2F",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7004/Reviewer_WrLY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7004/Reviewer_WrLY"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a skill-based language-conditioned policy. The objective is for the robot to understand human language commands, breaking down into skills to be executed consecutively. The architecture composes of a skill selector, labeler, base skill locator and generator."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The work considered language-conditioned skill-based policy, which is a good problem to study because language contains high level information that can be naturally broken down into skills.\n\n2. Experiment setting: The tasks considered are unseen tasks that are not trained on during training, which is a good setting to evaluate skill learning."
            },
            "weaknesses": {
                "value": "1. Implementation of basic skills: The basic skills translation, rotation, and grasping are quite limited, as they only cover certain basic motion; they do not reflect the true distribution of real-world manipulation tasks. \n- There are also quite a few existing works on using predefined skill primitives like MAPLE (https://arxiv.org/abs/2110.03655), Dalal et al. (https://proceedings.neurips.cc/paper/2021/file/b6846b0186a035fcc76b1b1d26fd42fa-Paper.pdf). How do the authors compare this work to prior works that also uses skill primitives? \n\n2. Tasks are short horizon and limited: The tasks used in this work are very short horizon, e.g. \"lift blue block\". In other skill learning / skill primitive works, this could be already considered as a unit of a basic skill like lifting; there is no need to break it down into smaller units. Also, the point of using skills is to tackle those long-horizon tasks like \"first lift blue block, then toggle switch\". Therefore, I would consider tasks like this unable to evaluate the effectiveness of skill learning.\n\n3. Missing baselines: the work compared with several skill-based RL works; but it fails to compare with MAPLE and Dalal et al. mentioned above."
            },
            "questions": {
                "value": "1. Implementation of basic skills: How are the three basic skills translation, rotation, and grasping implemented? Could you provide more details on how does these three skills decide their hyperparameters, e.g. how to know the translation distance or rotation angle?\n\n2. Suppose the robot needs to learn a new skill (e.g. pouring), does the skill classifier needs to be retrained?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7004/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7004/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7004/Reviewer_WrLY"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7004/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698828962808,
        "cdate": 1698828962808,
        "tmdate": 1699636820523,
        "mdate": 1699636820523,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Um8EXkpzYx",
        "forum": "mEJAp7Gh2F",
        "replyto": "mEJAp7Gh2F",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7004/Reviewer_iKk6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7004/Reviewer_iKk6"
        ],
        "content": {
            "summary": {
                "value": "The authors present Skill Prior based Imitation Learning (SPIL), a framework for robotic imitation learning that breaks down a task into 3 base skills: translation, rotation, and grasping. The framework includes a low-level policy for generating action sequences from skills and a high-level policy that generates sequences of skills. The authors show strong performance on the CALVIN benchmark as well as a real robot using sim2real transfer."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The idea of decomposing a robotic manipulation task hierarchically using base skills is interesting and seems sound to me. Using translation, rotation, and grasping is widely applicable to many robot embodiments. The results on CALVIN are strong, demonstrating state-of-the-art performance. Any nonzero success on sim2real transfer is impressive."
            },
            "weaknesses": {
                "value": "While the basic idea of the paper seems sound, as far as I can tell, I believe it suffers from significant clarity issues. The method is complicated and has a lot of moving parts that are not fully explained. I find it difficult to evaluate the soundness and contribution of the paper due to these issues.\n\n- In Section 3.2.1, it is not clear at all that trans, rot, and grasp correspond to groups of dimensions of the action space. The variables $x$ and $y$ are also not defined. The base skill classifier switches from $p(y = k \\mid x)$ to $q(y = k \\mid x)$ in the next section.\n- While I generally figured out what was going on from Figure 1, I found the explanations in Section 3.2.2 fairly unclear. I also found Section 3.3 quite difficult to follow: for example, the \"plan embedding\" is never defined. I think the methods section could be improved by spending more time concretely explaining the authors' instantiation of skill learning rather than speaking so much in the generic terminology of variational inference.\n- It is never explained how action sequences are sampled from the dataset. Wouldn't most action sequences include multiple base skills?\n- If I understand correctly, the base skill locator is just an embedding lookup table for each of the 3 base skills. This could be clarified.\n- Please fix the missing parentheses around references; it makes the paper more difficult to read."
            },
            "questions": {
                "value": "- How are the action sequences sampled during training? It seems to me that most action sequences would be likely to include multiple base skills, e.g., translation and grasping. How does the method deal with this?\n- How were the magic scales $w_k$ chosen?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7004/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7004/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7004/Reviewer_iKk6"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7004/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699067230916,
        "cdate": 1699067230916,
        "tmdate": 1699636820392,
        "mdate": 1699636820392,
        "license": "CC BY 4.0",
        "version": 2
    }
]