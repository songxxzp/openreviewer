[
    {
        "id": "Faa2amnvRV",
        "forum": "6YZmkpivVH",
        "replyto": "6YZmkpivVH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4071/Reviewer_wGVm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4071/Reviewer_wGVm"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a scalable framework for identification of signals modeled via manifolds that searches for a best match template via optimization, in contrast with the common matched filter approach that searches for a best match in a fixed set of templates. The proposed approach uses a kernel space embedding of the manifold data points, where the navigation of the manifold uses gradient descent and is trained via the popular unrolled optimization approach."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposed approach relies on a combination of kernel methods and data-centric optimization of iterative approach parameter vectors and matrices.\n\nTheoretical results provide probabilistic accuracy guarantees that depend on properties of the manifold."
            },
            "weaknesses": {
                "value": "The presentation is not always clear. The proposed algorithm is not crisply stated.\n\nFor several common applications of manifold (e.g., delay of arrival estimation and other 1-D manifolds), there is no comparison between the proposed approach and existing parametrizations (e.g., polar, spline, etc.). The parametrization have been helpful in reducing the computational complexity and the density of samples needed during navigation. In this sense, the comparison with only matched filtering is too coarse given the extent of the literature.\n\nSome practical considerations are addressed via \"brute force\", e.g., pushing for global optimality by increasing the number of initializations of the algorithm.\n\nWhile Section 4 says any embedding can be used, several assumptions are made as the narrative progresses.\n\nSince this is a data-centric method, there should be more discussion of the quality and quantity of manifold sampling needed to have acceptable performance.\n\nThe experimental section does not illuminate the performance of the embedding parametrization, e.g., what is the quality of the manifold samples obtained from TPoP vs. other methods, including the aforementioned parameterized approximations. There is also no discussion of training computational complexity or storage requirements for the experiments, including a comparison to MP or other methods. Finally, the computational comparison is given only in terms of \"complexity\", not running time.\n\nThe potential upside to MP implemented using FFT is not limited to the noiseless case or the single-dimensional (time series) case; a comparison including this implementation would be fair.\n\nMinor comments:\n\n* The connection between the Jacobians in (15) and the embedding distances in (16) should be stated more explicitly.\n* Similarly, the relationship between (17) and (6) should be explicitly described. \n* When discussing the computational complexity of TpopT after Figure 4, the authors should revisit the description of the parameters involved.\n* There is a typo after eq. (22) \"?\""
            },
            "questions": {
                "value": "In (16), what is the value of i?\n\nHow is the formulation of supplement Theorems 2 and 12 (in particular eq. 2) comparable to Theorem 1 in the manuscript? \n\nCan the authors define the logarithmic map used in supplement eq. 20?\n\nIn supplement eq. 23, what does $$\\Pi$$ represent?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4071/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698362449448,
        "cdate": 1698362449448,
        "tmdate": 1699636371439,
        "mdate": 1699636371439,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1oNEwqC8Si",
        "forum": "6YZmkpivVH",
        "replyto": "6YZmkpivVH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4071/Reviewer_uRtk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4071/Reviewer_uRtk"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors provided a proof of convergence of Riemannian gradient descent on the signal manifold, and demonstrated its superior dimension scaling compared to MF. We also proposed the trainable TpopT architecture that can handle general nonparametric families of signals.  In my view, this work represents a significant accomplishment."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The authors investigated the TpopT (TemPlate OPTimization) as an alternative scalable framework for detecting low-dimensional families of signals which maintains high interpretability, proved that it has a superior dimension scaling to covering and proposed a practical TpopT framework for nonparametric signal set."
            },
            "weaknesses": {
                "value": "Please review the comments below."
            },
            "questions": {
                "value": "1. My primary concern revolves around the assumption of $\\sigma$ in Theorem 1. I understand that this assumption can simplify the conclusion, but it implies that the variance of noise should be much smaller than the initialization assumption, which is exceedingly challenging in practical scenarios. Furthermore, I have observed that the proof relies on a bounded Riemannian Hessian matrix with the constant $L$, yet the main paper does not introduce any assumptions regarding the existence of the Riemannian Hessian. I propose that the authors consider removing the assumptions related to $\\sigma, \\tau$, and $\\epsilon$ and instead directly utilize $L$ and $\\tau$ to characterize the convergence rate.\n\n2. In the main paper, the computational complexity of all methods should be presented in a tabular format.\n\n3. In the experimental results, it is essential to include a convergence comparison of all methods to effectively demonstrate the superiority of TpopT.\n\n4. According to Figure 5, my understanding is that when the number of hidden layers in the MLP increases, its performance may possibly become the best. If that is the case, what would be the advantage of TpopT?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4071/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4071/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4071/Reviewer_uRtk"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4071/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698690569608,
        "cdate": 1698690569608,
        "tmdate": 1699636371346,
        "mdate": 1699636371346,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8gq92fC4ep",
        "forum": "6YZmkpivVH",
        "replyto": "6YZmkpivVH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4071/Reviewer_Y4xN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4071/Reviewer_Y4xN"
        ],
        "content": {
            "summary": {
                "value": "This paper is about signal detection. It proposes to replace (parameterized) template matching via exhaustive search over the search space by optimization. The running assumption is that the template space is a manifold (e.g. translations and rotations of a prototype, or gravitational waves generated by a model which depends on a small number of parameters).\n\nThe authors use Riemannian gradient descent on the template manifold. They prove that it converges to the \"best\" template (at the smallest angle) provided that it is initialized sufficiently close to the global optimum, where the value of \"sufficiently close\" depends on the curvature. Experiments on stylized gravitational waves and rotated and shifted MNIST suggest that the proposed method performs well in some settings."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "I am not on top of the latest research in learning-based template matching, but I like this paper. It is well presented, written in a sober way, the results are clear and the application is important. It is commendable that the authors derived theoretical results and identified a parameter region where their method should outperform MF. The learning strategy via unrolling Riemannian gradient descent with kernel-smoothed gradients is elegant and well motivated."
            },
            "weaknesses": {
                "value": "On the negative side, the experiments are much too stylized, especially the MNIST one. Are there no real, complex datasets where one could test the proposed methods? (I am quite sure there must be.) At the very least one shuold eavlauate the performance on the MNIST toy example with noise, including a challenging setting with a lot of noise. It is also not completely clear (even after reading F.1) how much noise \\sigma = 0.3 in the gravitational wave example actually corresponds to. When you say that the signal amplitude is constant at a=1, does it mean that it's normalized so that the inf-norm is 1? (I see that for waveforms it's the l2 normalization---are \"waveforms\" here \"templates\"?) It would be great (and necessary) to considerably improve the experiments.\n\nAnother thing that I am missing is the sliding window aspect (especially in the gravitational wave application). In streaming detection applications the signal is very long and one can take advantage of the FFT to efficiently compute the dot product with a template at many shifts (if S is generated by other groups which admit a FFT then those can be included as well). In experiments in this paper (at least that is how I interpret it) S is generated by varying some physical parameters but not the shift where the template occurs. It is not clear to me what would be faster in a real streaming application, especially when the signal manifold has dimension as low as 2.\n\nFurther, one can expect the landscape like the one in Figure 4 (right) whenever the involved signals are oscillatory. This problem is well studied for example in full waveform inversion where it's known as cycle skipping. It is often addressed by moving to some optimal transport-based loss instead of l2 (dot product). I also wonder about the suitability of PCA-style dimension reduction tactics for such oscillatory signals."
            },
            "questions": {
                "value": "In Section 4 (paragraph Embedding), are there conditions on d, n, N under which the suggested dimension reduction makes sense? Does the topology of S play a role anyhow?\n\nOne other confusing thing is about interpolation to get Jacobians (not smoothing). In (12) you introduce a kernel estimate of the Jacobian at some \\xi_i which is in the dataset (it needs to be since you need s_i to compute (12)), but then later you need it for arbitrary points. Do you then obtain it by linear interpolation? Just synthesizing via PCA seems to result in a globally linear space which is not what you want.\n\nPracticalities: what is \\kappa in practice? How to know it? Can you given an estimate (and compare complexity with MF) in some cases?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4071/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699015736742,
        "cdate": 1699015736742,
        "tmdate": 1699636371277,
        "mdate": 1699636371277,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3FeLyeDQCB",
        "forum": "6YZmkpivVH",
        "replyto": "6YZmkpivVH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4071/Reviewer_X8Hz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4071/Reviewer_X8Hz"
        ],
        "content": {
            "summary": {
                "value": "This paper considers the template matching problem :\n\n$$\\max_{s\\in S} \\langle s, x\\rangle$$\nwhere $x$ is a fixed observed signal, and $S$ is a manifold. $S$ is described with samples $s_1, \\dots, s_n$. \nThe baseline for this problem is matched filtering, which enumerates the set of samples and solves $\\max_{s\\in \\{s_1, \\dots s_n\\}} \\langle s, x\\rangle$.\nFirst, the authors analyze the theory of Riemannian gradient descent on $S$ to solve the problem. They show that if the algorithm is initialized close enough to the solution, we get exponential convergence. \nThen, the authors turn to a practical algorithm to solve the problem when one can only access samples $s_1, \\dots, s_n$ describing the manifold. \nThey propose to learn an embedding to a lower dimensional space and optimize over it: they build anchor points $\\xi_1, \\dots, \\xi_n$ using a dimensionality reduction technique and then construct a function $s(\\xi)$ from these points, in order to maximize $\\langle s(\\xi), x \\rangle$.\nThe function is constructed by approximating its Jacobian with weighted least squares. The corresponding iterations are then unrolled in a neural network to make the whole procedure learnable.\nThe authors validate the method on a gravitational wave detection problem and on the mnist problem, where the goal is to detect the digit \"3\" from the other."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is quite well written and is pleasant to read.\nThe problem tackled here is interesting, and the numerical results are encouraging."
            },
            "weaknesses": {
                "value": "The main weakness of the paper is that it proposes a pipeline that contains many different steps that are then unrolled. The authors do not propose an ablation study where we can clearly see the benefits of each step in the pipeline: what about a method without unrolling? what about a method that implements gradient descent without the smoothing of the jacobians? What about preconditioning? What about a method directly differentiating through the embedding map $s\\to \\xi$? What is the impact of the number of training samples on the performance of the unrolled method? What is the role of the hyperparameters? \n\nThe other main weakness of the paper is its theoretical analysis. The proposed method is an unrolled gradient descent over the parameters $\\xi$ that aims at approximating gradient descent over a **parameterization** of the manifold (i.e., $\\min_\\xi \\langle s(\\xi), x\\rangle$, where ideally $s(\\xi)$ describes the whole manifold $S$). The theoretical part of the paper is about Riemannian gradient descent over $S$ itself. There are, therefore, barely any links between the method proposed by the authors in practice and that studied in theory, and the efficiency of the proposed practical method is not grounded in any theory."
            },
            "questions": {
                "value": "- I think it would be great to discuss how much of the results in thm.1. are due to the linearity of the objective function: what happens when the objective function is no longer linear? \n- Page 5: what is $\\phi$ ? is it the same things as $s$? it is not clear what the domain of $\\phi$ is. \n- In the implementation of gradient descent, one needs to compute $s(\\xi)$. The authors explain in detail how they approximate the Jacobian of this map, but how is $s(\\xi)$ itself approximated for $\\xi \\notin \\{\\xi_1, \\dots, \\xi_n\\}$?\n- The authors choose a compactly supported kernel to reduce computations, but it still requires to compute pairwise distances : how much computations does it really gain?\n- Eq.17 is an affine equation between $\\xi$ and $x$. Why would we need to learn all the matrices $W(\\xi_i, k)$ when we can simply learn the full linear operator, which has far fewer parameters ?\n\n\nHere are some Misc. remarks:\n- some citations should be in parenthesis\n- The sphere $\\mathbb{S}^{D-1}$ should be defined\n- Trivializations (Lezcano Casado, Mario. \"Trivializations for gradient-based optimization on manifolds.\" Advances in Neural Information Processing Systems 32 (2019).) are a good reference for the discussion around eq.6.\n-Equations 7 and 8 are missing references; to the best of my knowledge these are not novel. \n- A reference for multidimensional scaling, and its equivalence to PCA, is welcome.\n- Fonts in figures are sometimes too small, they should be the same size as the main text's figures.\n- The provided `TpopT_MNIST.ipynb` is not runnable, since the file `data_MNIST/data_dim3.pkl` is not provided. The `get_gradient` function also calls a `X_base` variable never defined. Please provide self-contained code."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4071/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699025664801,
        "cdate": 1699025664801,
        "tmdate": 1699636371217,
        "mdate": 1699636371217,
        "license": "CC BY 4.0",
        "version": 2
    }
]