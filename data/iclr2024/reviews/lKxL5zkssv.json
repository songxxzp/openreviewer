[
    {
        "id": "4fbHPXelTK",
        "forum": "lKxL5zkssv",
        "replyto": "lKxL5zkssv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6614/Reviewer_UuaC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6614/Reviewer_UuaC"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a CLIP-guided Multi-sUbject visual neural information SEmantic Decoding (CLIP-MUSED) method which generalizes single-subject decoding models to multiple subjects in visual neural decoding tasks. Different from other multi-subject decoding methods, CLIP-MUSED uses a Transformer-based fMRI feature extractor to effectively extract global features of neural responses. CLIP-MUSED uses low-level and high-level tokens to encode individual differences and uses the topological relationship of visual stimuli in CLIP representation space based on RSA to guide the representation learning of tokens."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is easy to read, and generally well written.\n2. The use of RSA is novel and contributes to the topological relationships of visual stimuli in the CLIP representation space as prior knowledge to guide neural representation learning in the shared space.\n3. The experiments have clearly verified the benefits of the proposed methods."
            },
            "weaknesses": {
                "value": "1. The use of letters in the article needs to be consistent. For example, the representation of X.\n2. In order to reduce the computational cost, using 3D-CNN to reduce the dimension will have an impact on the results? The results of this comparison are not shown.\n3. The high-level feature in RSMs uses the average of the image and text features from the last layer of CLIP. What are the advantages of using the CLIP image encoder or text encoder alone compared to directly using it alone? Can you provide corresponding results for an explanation?"
            },
            "questions": {
                "value": "1. How to set labels for semantic classification of N subjects after concatenating low-level and high-level token representations of different image stimuli?\n2. In Section 2.1 \"X^{(n)}\" is recommended to be consistent with the previous notation. The rest of the article should also be consistent.\n3. The meaning of B in \"B visual stimuli\" in Section 2.2 should be explained.\n4. Is the underline in the AUC results of MS-SMODEL-ViT in Table 1 redundant? It should remain in the same format as other tables."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6614/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6614/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6614/Reviewer_UuaC"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6614/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698740706827,
        "cdate": 1698740706827,
        "tmdate": 1699636754535,
        "mdate": 1699636754535,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oWk46Yh2mv",
        "forum": "lKxL5zkssv",
        "replyto": "lKxL5zkssv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6614/Reviewer_qr5q"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6614/Reviewer_qr5q"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a transformer based cross-subject fMRI alignment method that uses learned special tokens (an approach also used in other ViT/language works which perform classification). \n\nSpecifically, the paper discusses the following issues in traditional hyperalignment approaches:\n1. Mapping functions are restrictive (I assume this is in reference to orthogonal procrustes, linear mappings, or kernelized mappings)\n2. Requirements for a per-subject mapping function, which can be expensive when deep networks are used\n3. Need for identical stimuli or stimuli from the same semantic category\n\nThey propose the following approach:\n1. Using a transformer to model long-range dependencies\n2. Using special tokens to model subject-wise information\n3. CLIP distances to guide the model, with the average of the text & visual branches used for the high level feature."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "CLIP-MUSED is an interesting approach that shares model parameters and only varies a per-subject token. The use of CLIP for feature extraction and feature alignment is solid. The method allows for the use of multi-subject data without a linear increase in the number of model parameters, which is beneficial for scalability\n\nThe paper provides a detailed methods section that covers the technical aspects of the CLIP-MUSED, including the use of Transformers and CLIP and the design of an RSA based loss. The high level approach is clear.\n\nThe paper is well-organized, making it relatively easy to follow the arguments and understand the methodology."
            },
            "weaknesses": {
                "value": "**In my view there are two weaknesses of the paper:**\n1. The relatively weak decoding the authors perform, which is implied to be just category decoding. It is very strange to me that they use so much compute relative to traditional methods, and end up just doing category decoding. I believe the results would be strengthened by performing visual decoding in the form of full image reconstruction, at least for NSD (single image passive viewing task). If this is not possible, I think an alternative would be perform image retrieval (show top-5) on a test set, where the test set contains images not used for the training of any subject. This would likely require training the decoder on a single subject's stimuli only, then testing on other subjects.\n2. The fact that the subject-wise token is not computed via amortized inference (via an encoder conditioned on a few BOLD responses). But instead, they are fixed per subject. In my view this weakens the approach somewhat as you cannot take this approach and apply it to a new subject in a few-shot fashion, after the paper talks about how their method does not require subjects to view the same stimulus.\n\n**Math typos:**\n1. In both the paper eq 10, the authors discuss using the F-norm (frobenius norm), but in practice they are using the squared frobenius norm in the code (line 42 of `losses.py`). This is fine, as the squared F-norm is everywhere differentiable, but I ask the authors clarify/fix this.\n2. In the paper eq 15, the authors apply a orthogonal constraint such that matrix $z_\\text{llv}$ and matrix $z_\\text{hlv}$ are orthogonal to each other via the regularization loss. Same issue listed above applies. You should probably also clarify here that you want the low/high level representations for each stimuli to be orthogonal, otherwise it is not super clear.\n\n**Minor typos:**\n1. Page 3. Original `The contributes are summarized as follows`, should be `The contributions are summarized as follows`\n2. Page 3. It seems that $X^{(n)} \\in \\mathbb{R}^{n_i \\times d} $ should be $\\mathcal{X}^{(n)} \\in \\mathbb{R}^{n_i \\times d} $\n\nOverall I think the proposed method is interesting, but the experiments currently do not fully demonstrate the strength of the method. I would gladly reconsider if the authors can clarify my questions, and provide additional experiments to support their method."
            },
            "questions": {
                "value": "Questions:\n1. For Figure 1 in the paragraph below you mention `two bird images t1 and t2` as well as `t1 and t2 are birds, while t3 (duck) and t4 (building blocks) are not`.\n\n    **a**. I encourage the authors to clarify this, or use other examples in the figure and paper. Ducks should be birds.\n\n2. In the paper, it is discussed that `Linear transformation and MLP are unsuitable for high-dimensional voxel responses`. However no justification on the MLP aspect is given, and the citation (Yousefnezhad & Zhang) explicitly uses a kernelized MLP approach. Could you clarify?\n\n3. In section 3.2, you never define what a backbone network is. Does it just take as input the fMRI BOLD responses and output a category? In that case, is the ViT a 3D model?\n\n4. It is not clear what exactly you are decoding. You mention that HCP has a wordnet label, and NSD has the 80 categories (I assume the binary 0/1 mask on if an object from a category is present in the image).\n\n5. Can you clarify if you use ROI based patching for NSD, and the CNN based patching approach for HCP? Is the CNN jointly trained with the transformer?\n\n6. How do you extract visual features for HCP given that the stimulus set consists of video clips?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/a"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6614/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6614/Reviewer_qr5q",
                    "ICLR.cc/2024/Conference/Submission6614/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6614/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698781819807,
        "cdate": 1698781819807,
        "tmdate": 1700601745954,
        "mdate": 1700601745954,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FzHtpfrmD1",
        "forum": "lKxL5zkssv",
        "replyto": "lKxL5zkssv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6614/Reviewer_mWYc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6614/Reviewer_mWYc"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the problem of alignment of the response to visual stimuli across multiple subjects. This is an important problem for visual neural decoding tasks. In general, models for visual neural decoding tasks are either trained on a single subject (leading to problems such as overfitting) or must correct for differences across anatomical structure and functional topography of the brain in different subjects. The paper proposes a new method for multi-subject functional alignment that goes beyond the SOTA, namely hyperalignment and category-based methods. This new method uses transformer-based models to capture long-range dependencies that exist in the functional connectivity between brain regions. Intersubject differences are modelled through extra subject-specific tokens and the CLIP representation space is used to achieve a high consistency between cortical representations of visual stimuli."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper addresses an important problem in neurosciences and the proposed multi-subject alignment method is novel and interesting. Both the use of transformers to model long-range associations in brain regions and the use of the CLIP representation space to guide neural representation learning in a shared representation space is a nice idea! The experimental results on the two fMRI datasets are somewhat limited (esp. due to the small size) but demonstrate a proof-of-concept nicely."
            },
            "weaknesses": {
                "value": "I found that the abstract is not as clearly written as is the introduction. From the abstract, it is unclear what the paper sets out to achieve nor how this is done. The introduction does much better job at this. Perhaps the authors could try to formulate the problem addressed and the contributions more clearly (I appreciate that this is more difficult to do in the space available). The evaluation is rather limited, especially since the size of the datasets is quite small."
            },
            "questions": {
                "value": "- Why do extract low-level and high-level feature RSM seperately?\n\n- The HCP dataset contains more than 158 subjects. Why did you not use all subjects? How were the 158 subjects selected from the HCP? Randomly? Why is this further reduced to 9? What is the bottleneck here?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6614/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698831743759,
        "cdate": 1698831743759,
        "tmdate": 1699636754273,
        "mdate": 1699636754273,
        "license": "CC BY 4.0",
        "version": 2
    }
]