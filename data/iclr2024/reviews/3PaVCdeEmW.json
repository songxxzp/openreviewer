[
    {
        "id": "mjVbaAptlR",
        "forum": "3PaVCdeEmW",
        "replyto": "3PaVCdeEmW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2551/Reviewer_ehYp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2551/Reviewer_ehYp"
        ],
        "content": {
            "summary": {
                "value": "This paper introduce a simple and effective multilingual alignment framework (AFP), there are two major components: 1. multilingual contrastive learning on internal representation, 2 Cross-lingual instruction tuning on the outputs.  Through the alignment, the author argues that the performance of low-resource languages will be improved through the knowledge transfer.  Experiments on 52 languages show that  with a small number of training tokens, the propose method could improve the cross-lingual ability of generative models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is clearly written and easy to understand. \nThe proposed MCL and CIT method is solid in improving the cross-lingual ability of pre-trained language models.\nAnd I like the experiment design and analysis: 1. Extensive experiment on 52 languages to show the cross-lingual ability, also got good results on unseen languages Thai and Turkish . 2. use embedding distance and uniformity to measure the distribution of multilingual representation.\nThe content in Appendix will make it easier for others to reimplement this work."
            },
            "weaknesses": {
                "value": "1. I think the innovativeness of this paper is limited:  the proposed multilingual contrastive learning on internal representation is not new, as the author listed in Sec2.1 Para 1. Also in \"On learning universal representations across languages\" Wei. ICLR2020, the proposed contrastive learning method for universal representation learning is similar.    \n    On the other hand, the cross-lingual instruction tuning also has some similar work: like \"Few-shot Learning with Multilingual Generative Language Models\" use cross-lingual demonstrations in the tuning process.  As there are no comparison system in the main experiment, I suggest the author add one or two related work to better show the merits of this work.\n\n2. A small issue with the machine translation experiment,  I think the parallel data used in AFP framework should be added into the baseline model through fine-tuning or demonstrations in prompt for a fair comparison.  And the improvement in BLEU is marginal to me. \nSo I am a little skeptical on the improvement of MT task.   Also a typo in Sec 3.3 BLUE --> BLEU"
            },
            "questions": {
                "value": "Please refer to the weakness for context:\n1. Is there any related work in cross-lingual instruct tuning that could be added as the compare system?\n2. For the MT experiment, is it possible to add the parallel data used in AFP framework into the baseline model for a fair comparison?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2551/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2551/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2551/Reviewer_ehYp"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2551/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698119066884,
        "cdate": 1698119066884,
        "tmdate": 1700537424393,
        "mdate": 1700537424393,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ugzH9oBRXL",
        "forum": "3PaVCdeEmW",
        "replyto": "3PaVCdeEmW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2551/Reviewer_GUFo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2551/Reviewer_GUFo"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes align after pretrain (AFP) method to improve multilingual capabilities of LLM. AFP consists of two training objectives:  (1) contrastive learning objective to align embedding spaces and (2) cross-lingual instruction objective to improve cross-lingual generation. AFP is evaluated on 4 tasks: natural language inference, paraphrase detection, reasoning and machine translation. The evaluation using XGLM and BLOOM models show that when AFP is used, it boosts the multilingual performances of the base models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper combines two popular ideas in NLP to improve multilingual capabilities of LLM. Although the idea of making pretrained LM more multilingual is not new. [Earlier work](https://arxiv.org/abs/2002.07306)  has shown the extreme of making bilingual LM out of English only LM. \n- The AFP method is simple, and it should work for languages that have parallel data.\n- Analysis of embeddings after AFP"
            },
            "weaknesses": {
                "value": "- The motivation for AFP is because current public LLMs are English centric, thus I wonder the value of this approach and its impact compared to training a native multilingual LLM from scratch such as [PolyLM](https://arxiv.org/abs/2307.06018) , which is multilingual by design. I think it\u2019s important to have PolyLM in the evaluation to understand the gap in multilingual capabilities  and where AFP stands.\n\n- Evaluated tasks are simple (except few-shot machine translation). XNLI, PAW-S, XCOPA, XStorzyCloze, and XWinograd don\u2019t seem to test the resulting model\u2019s ability to generate languages. For instance, for XNLI, the model only needs to generate one token (yes/also/no), for XCOPA, for reasoning tasks, the model doesn\u2019t need to generate the model, instead it is used to score the answers and find the best one. Thus I think that the generative abilities in multilingual setup are not properly measured. I wonder if other multilingual tasks such as summarization/QA are more appropriate for evaluation.\n\n- The crosslingual finetuning step leverages machine translation outputs, which is prone to error.\n\n- While the evaluation is done based on the existing cross-lingual datasets, which is perhaps outdated in the age of LLMs with emergent abilities. In order to advance multilingual ability, i think the first step (and also the very important one) is to have an adequate  multilingual benchmark for LLMs. Without that, it's difficult to assess any claim about  improving multilingual generative models.\n- The cross lingual finetuning step leverages machine translation outputs, which is prone to error. LLMs are known for hallucinating their generation, thus cross-lingual finetuning could make it even worse for generation in non-English languages. Does AFP cause more hallucination in language generation? Can you measure that? And what is the strategy to prevent such cases."
            },
            "questions": {
                "value": "See the question in the above section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2551/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698702493730,
        "cdate": 1698702493730,
        "tmdate": 1699636191572,
        "mdate": 1699636191572,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "267iAtCW2r",
        "forum": "3PaVCdeEmW",
        "replyto": "3PaVCdeEmW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2551/Reviewer_1odi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2551/Reviewer_1odi"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes align after pre-train (AFT), a cross-lingual alignment framework that enhances cross-lingual abilities of multilingual generative language models. AFT utilizes translation pairs to align the internal sentence representations across languages, through multilingual contrastive learning. Besides, AFT performs cross-lingual instruction tuning that makes the language model respond in target languages given source language prompts. With two objectives combined, the authors trained multilingual LMs on the basis of several multilingual LLMs. The experimental results demonstrate that the alignment greatly improves the cross-lingual ability of the models on several multilingual tasks under zero-shot and few-shot settings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper conducts extensive experiments where AFT is evaluated on various multilingual LLMs (XGLM, BLOOM, and Llama) on four types of multilingual tasks. The results show that AFT consistently improves the LLMs across models, tasks, and setups. Besides, the AFT is also evaluated beyond the bilingual setup, with the alignment extended to 52 languages.\n- The provides ablation studies on the training objectives and different alignment methods. The ablation studies support the effectiveness of multilingual contrastive learning and cross-lingual instruction tuning when they are used together.\n- The paper presents visualization of the natural language representations from different languages, and demonstrates the alignment effects on the hidden representations inside LLMs."
            },
            "weaknesses": {
                "value": "- The novelty of the proposed method is limited. The AFT framework is a combination of cross-lingual contrastive learning and instruction tuning objectives, both of which have been shown to be effective for enhancing cross-lingual abilities in related works. (1) InfoXLM[1] utilizes cross-lingual contrastive learning with translation pairs to enhance multilingual LMs, and demonstrates its effectiveness in improving cross-lingual transferability. The difference is whether to put the contrastive object to MLM-trained models or  CLM-trained models. (2) BLOOMZ[2] applies multilingual multitask finetuning (a.k.a. instruction tuning) and observes better zero-shot performance.\n-  In section 2.2, the paper claims that the proposed cross-lingual instruction tuning (CIT) is proposed to further align the outputs, which is more difficult than the multilingual instruction tuning. However, I did not find ablations to support this. I would guess the gain is mainly from instruction tuning instead of its cross-lingual alignment effect of CIT.\n- Insufficient literature review in the related work section.\n\n[1] InfoXLM: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training\n\n[2] Crosslingual Generalization through Multitask Finetuning"
            },
            "questions": {
                "value": "- Does cross-lingual instruction tuning work better than multilingual instruction tuning? \n\nAfter applying machine translation to the instruction tuning data, you could obtain at least twice the training data (N times training data for N target languages). It is unclear why multilingual instruction tuning, which has N times data, would perform worse than the proposed cross-lingual instruction tuning."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2551/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2551/Reviewer_1odi",
                    "ICLR.cc/2024/Conference/Submission2551/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2551/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698889176911,
        "cdate": 1698889176911,
        "tmdate": 1700682578644,
        "mdate": 1700682578644,
        "license": "CC BY 4.0",
        "version": 2
    }
]