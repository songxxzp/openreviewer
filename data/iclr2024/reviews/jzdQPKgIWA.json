[
    {
        "id": "F1XSMtjZoU",
        "forum": "jzdQPKgIWA",
        "replyto": "jzdQPKgIWA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7102/Reviewer_gKAo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7102/Reviewer_gKAo"
        ],
        "content": {
            "summary": {
                "value": "In MARL settings where one agent interacts with one peer, this paper proposes learning a context-conditioned policy for the agent to adapt to different peers without knowing their type. The focus here is on being able to balance exploration-exploitation in the agent's interaction with the peer. To achieve this, the agent learns to predict the peer's type as an auxiliary task and jointly trains the type prediction and policy networks so that the agent's policy prefers exploratory actions when there is uncertainty about the peer's type; otherwise exploitative actions are preferred to maximize the agent's reward. The authors consider two environments: Overcooked in which the agent-peer interaction is cooperative and Kuhn Poker in which the agent-peer interaction is competitive. In both cases, experimental results indicate that the proposed framework enables the agent to adapt to its peer under partial observability, as well as over short and long horizons."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The authors have presented a thorough discussion of the related work in their problem setting and clearly  highlighted connections to and differences from prior work. A number of baselines from prior work have been considered in Sec 4.1 to demonstrate the superior performance of the proposed approach. \n\n2. This paper's approach of jointly training auxiliary peer type prediction network with the context conditioned RL policy and using intrinsic reward to guide the explore-exploit trade-off during training, is simple yet effective in the proposed settings. Although this framework relies on several restrictive assumptions, the authors have also highlighted many of those in Section 5."
            },
            "weaknesses": {
                "value": "1. The proposed framework depends on the availability of the finite set $\\Psi$ of peer types, therefore either a diverse set of peers should be available at training time or the agent might fail to generalize to previously unseen types of peers during evaluation. This is a major limitation of this approach. \n\n2. This work assumes that the policy followed by the peer is stationary and fixed, which makes it a much simpler setting than for example prior work in [1]. Although the authors claim that the one agent - one peer framework followed in the paper will generalize to multiple peers, it perhaps would not be as easy to extend to more complicated settings for example, when multiple peers cooperate with each other to compete against the agent. I suspect this would be the case because the agent only predicts the type of a peer from the set $\\Psi$, and it is not practical to assume that all possible interactions between multiple peers can be enumerated in $\\Psi$ during training. \n\n[1] Banerjee, A., Phade, S., Ermon, S. and Zheng, S., 2023. MERMAIDE: Learning to Align Learners using Model-Based Meta-Learning. arXiv preprint arXiv:2304.04668."
            },
            "questions": {
                "value": "1. It would help to add an algorithm box for the evaluation phase - particularly, I am confused whether or not the policy or peer type prediction network is fine-tuned during the adaptation to test agents. What are the differences between training and test settings? This would also help me better understand the results in Fig 4. \n\n2. In Fig 4, is it possible to design a baseline that upper bounds the performance of the learning based approaches? For example, an expert policy that has access to an oracle for the peer type. \n\n3. In Sec 4.4, please clearly specify the definitions of each baseline - Fig 5 labels \"PAIC-reward\" and \"PAIC-reward-aux\" but they are not mentioned in the text. \n\n4. In Sec 4.5, last sentence: \"While there are minor differences,...\" - where is this result shown?\n\n5. In Table 3, for Train 1, why is there a drop in performance for larger pool size N (i.e. N=18 to N=36)? \n\n6. Fig 6 - Is the t-SNE plot for Overcooked? Please make it explicit."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7102/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698680529798,
        "cdate": 1698680529798,
        "tmdate": 1699636838804,
        "mdate": 1699636838804,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YEeRnfVvZP",
        "forum": "jzdQPKgIWA",
        "replyto": "jzdQPKgIWA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7102/Reviewer_cQ6z"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7102/Reviewer_cQ6z"
        ],
        "content": {
            "summary": {
                "value": "This paper presents an approach, called Fast Peer Adaptation with In-Context policy (PAIC), for multi-agent settings where agents need to quickly adapt to diverse peer behaviors in both cooperative and competitive scenarios. The main contributions of the paper are as follows:\n\n1. The paper recognizes a different method to identify peer patterns, and adapt accordingly based on the history.\n\n2. PAIC tries to balance exploration and exploitation, allowing agents to optimize their performance during peer adaptation. It promotes exploratory actions when the context is uncertain. An intrinsic reward mechanism is introduced based on peer identification accuracy. \n\n3. The proposed method is evaluated in both competitive (Kuhn Poker) and cooperative (Overcooked) environments, demonstrating faster adaptation and improved performance compared to existing methods when facing novel peers."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well-prepared, with clear writing and figures.\n\n2. The paper introduces an important question: exploration in ad-hoc cooperation.\n\n3. The reviewer was impressed by the strong empirical performance of the proposed method."
            },
            "weaknesses": {
                "value": "1. (Major, about \"in-context learning\") \n\n1.1 The reviewer failed to discern the rationale behind the authors' proposal of the in-context policy as a novel concept. In-context policies fundamentally rely on previous trajectories, a characteristic shared by nearly all multi-agent policies. What distinguishes this approach is the utilization of trajectories from various instances. However, if an RNN or a Transformer is used to represent agents' policies, it might also remember information from previous episodes.\n\n\n1.2 The omission of peer information in these trajectories could potentially misguide the trajectory embedding.\n\n1.3 The justification for employing a Multi-Layer Perceptron (MLP) to encode trajectories merits a more thorough validation. As the authors suggest, utilizing MLP treats the context as a collection of state-action pairs, which disregards the intra- and inter-episode temporal order. This oversight could certainly have a negative impact on context encoding performance. Furthermore, the reviewer does not concur that alternative network architectures are incapable of capturing long-term dependencies and mitigating overfitting. Transformers, for instance, have been widely adopted in the encoding of multi-agent trajectories.\n\n2. Why identifying peers can be regarded as an intrinsic reward that encourages exploration?"
            },
            "questions": {
                "value": "1. The the context only include ego information? How can a peer be identified without its information?\n\n2. What if a transformer is used for context encoding? Would it be better than an MLP?\n\n3. In Figure 5, what is the difference between PAIC-reward and PAIC-reward-aux?\n\n4. More ablations on other scenarios are expected."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7102/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698692581952,
        "cdate": 1698692581952,
        "tmdate": 1699636838685,
        "mdate": 1699636838685,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EaLz5Ta1f1",
        "forum": "jzdQPKgIWA",
        "replyto": "jzdQPKgIWA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7102/Reviewer_VfPt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7102/Reviewer_VfPt"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel end-to-end method called Fast Peer Adaptation with In-Context Policy (PAIC) for training agents to adapt to unknown peer agents efficiently. PAIC learns an in-context policy that actively explores the peer's policy, recognizes its pattern, and adapts to it. The agent is trained on a diverse set of peer policies to learn how to balance exploration and exploitation based on the observed context, which is the history of interactions with the peer. The paper introduces an intrinsic reward based on the accuracy of the peer identification to encourage exploration behavior. The method is evaluated on two tasks involving competitive (Kuhn Poker) or cooperative (Overcooked) interactions with peer agents, demonstrating faster adaptation and better outcomes than existing methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* PAIC achieves faster adaptation and better outcomes compared to existing methods in both competitive and cooperative environments.\n* The introduction of an intrinsic reward based on the accuracy of peer identification encourages exploration behavior, which is crucial for efficient adaptation.\n* The method is evaluated on two diverse environments, Kuhn Poker and Overcooked, showcasing its effectiveness in both competitive and cooperative settings."
            },
            "weaknesses": {
                "value": "* The paper only considers purely cooperative and competitive environments. It would be interesting to see whether PAIC can handle more complex mixed-motive environments.\n* The paper assumes that the peer agent does not update its policy during test time. However, in the real world, peers may be able to tune their policies online, which could pose a challenge for PAIC."
            },
            "questions": {
                "value": "* How does PAIC perform in more complex mixed-motive environments, where agents have to balance their own interests and the collective welfare?\n* Can PAIC adapt to non-stationary peers, where the peer agent is also updating its policy during test time?\n* What are the implications of PAIC for human-AI interaction, and how can it be evaluated in real-world settings involving human peers?\n* Are there any potential ethical considerations or challenges when applying PAIC to real-world scenarios, such as human factors and user feedback?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7102/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698807555732,
        "cdate": 1698807555732,
        "tmdate": 1699636838566,
        "mdate": 1699636838566,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "atZIrobrdn",
        "forum": "jzdQPKgIWA",
        "replyto": "jzdQPKgIWA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7102/Reviewer_wMXp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7102/Reviewer_wMXp"
        ],
        "content": {
            "summary": {
                "value": "The authors introduce a context encoder, which is enhanced with an additional task to distill concise information from sequences of {observation, action}. Subsequently, policies are influenced by this hidden context data to promote more desirable actions. The auxiliary task aids the context encoder in categorizing the observed sequences into a finite set of types. By minimizing the loss associated with the auxiliary task, the context encoder effectively learns to adapt to various types of agents, without the need to predefine agent types as seen in prior studies. While the paper lacks a detailed discussion on the convergence properties of PAIC, empirical evaluations in Kuhn Poker and Overcooked environments reveal superior performance compared to some relevant baseline methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Originality\nThe paper represents a compelling fusion of context encoders with an auxiliary task that resembles classification. The paper effectively highlights the distinctions when compared to a closely related study.\n\nQuality\nThe paper incorporates motivating examples, and it includes an ablation study where various PAIC-specific parameters are examined.\n\nClarity\nThe encoder and the auxiliary task (peer identification) and the extrinsic reward defined therein, are explained well, and limitations have been identified.\n\nSignificance\nThe achieved performance significantly outperforms the baseline methods, and the in-context policy learning framework seems well-suited for scenarios involving two agents, although the paper does not delve into the convergence behavior of PAIC in such extended settings."
            },
            "weaknesses": {
                "value": "The paper faces several challenges in its related work and critical analysis. While the idea of utilizing latent representations of trajectories is not novel, and concepts like cross-entropy loss and auxiliary tasks are commonly used in various classification problems, including class-incremental continual learning, the paper falls short in discussing how these methods can be effectively extended to multi-agent settings. This lack of extension and exploration makes it challenging to gauge the originality and innovation of PAIC in comparison to existing approaches.\n\nMoreover, the paper does not address fundamental questions and considerations that are crucial for understanding its applicability and limitations. It fails to provide insights into how PAIC can be extended to scenarios with more than two agents, raising questions about adaptation in situations where agents significantly diverge from each other. Discount factors and their impact on PAIC are not discussed, which weakens the quality of the evaluation section.\n\nExperiment details can be improved. Important aspects, such as how the baselines were fine-tuned, the training process of the Generalist against all peers are left unexplained. The impact of varying N_test values and the absence of a discussion on convergence properties further hinder a comprehensive understanding of PAIC. The paper does not provide theoretical evidence supporting its claim of solving the partially observable stochastic games (POSG), and the problem formulation lacks in-depth derivations and convergence analyses. Without addressing these issues and discussing how PAIC can be extended to more complex scenarios or under what conditions it converges, it is challenging to assess the significance and practicality of PAIC."
            },
            "questions": {
                "value": "How does PAIC scale beyond two agents?\nHow do some other recurrent models compare against PAIC? What is the expected/observed advantages/disadvantages of the recurrence?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7102/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699282735609,
        "cdate": 1699282735609,
        "tmdate": 1699636838452,
        "mdate": 1699636838452,
        "license": "CC BY 4.0",
        "version": 2
    }
]