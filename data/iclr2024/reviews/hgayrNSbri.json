[
    {
        "id": "GYtZfx4siI",
        "forum": "hgayrNSbri",
        "replyto": "hgayrNSbri",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5139/Reviewer_yd7b"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5139/Reviewer_yd7b"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on image captioning with retrieval augmentation. Speicifically, the author proposed to retrieve a set of captions relevant to the input image via CLIP cross-modal retrieval. The retrieved captions are then sent into a pre-trained language model for summarization. The summarized results are regarded as the caption of the input image.\n\nTo improve the quality of CLIP retrieval, the author also proposed a light-weight projection layer that can be solved in closed-form to better align image and language modalities.\n\nTo augment the available caption set, the author also proposed an iterative self-improvement strategy by including captions generated by the model itself and improving the projection weight."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "Training the projection weight $\\textbf{W}$ is extremely low cost."
            },
            "weaknesses": {
                "value": "1. The main point of this paper is a $\\textbf{lightweight}$ image captioning system. However, the proposed method is only lightweight during training in terms of training time. For inference, the model needs to encode the image, retrieve several captions, and then feed the retrieved captions into an LLM. The overall procedure may even be slower than a vanilla VL model for image captioning. For training, the proposed method did $\\textbf{NOT}$ save trainable parameters compared to recent VL models like LLaVA, which only has a single fc layer to bridge the vision and language modalities. As the reviewer can tell from the paper, the \"lightweight\" part is only on the training speed and resources. This greatly reduced the contribution of this paper in the reviewer's opinion.\n\n1. The performance of the proposed model is far below current SoTA models. Even though the proposed method saves training time and resources. The performance decrease is disproportional.\n\n1. More details and analyses about the retrieval set should be discussed. For example, what retrieval set does the author use during training and testing/validation? What retrieval set does the author use for different datasets such as COCO and Flickr? Is the performance sensitive to the quality or distribution of the retrieval set?\n\n1. There exists other retrieval augmented image captioning works that achieve reasonable performance such as REVEAL [1], RA-CM3 [2], HAAV [3], etc. Current approaches typically train the model jointly with the retrieval.\n\n    [1] https://arxiv.org/abs/2212.05221\n\n    [2] https://arxiv.org/abs/2211.12561\n\n    [3] https://arxiv.org/abs/2305.16295"
            },
            "questions": {
                "value": "Please see the weakness section, particularly about the details and analyses of the retrieval set."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5139/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698725715473,
        "cdate": 1698725715473,
        "tmdate": 1699636507592,
        "mdate": 1699636507592,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RpLZgCYjn7",
        "forum": "hgayrNSbri",
        "replyto": "hgayrNSbri",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5139/Reviewer_ZWpM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5139/Reviewer_ZWpM"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a novel and cost-effective method to bridge the modality gap between pretrained language models (LMs) and pretrained vision-language models (VLMs) for enhanced image captioning. The modality gap refers to the misalignment between image and text representations in the shared embedding space, which can hinder the performance of image captioning models. The proposed method utilizes a linear mapping optimized via a least-squares solution to bridge this gap, enabling efficient computation on CPUs without requiring gradients. This approach allows for competitive performance on image captioning tasks compared to other lightweight captioning methods, particularly when reference-based metrics are employed."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper introduces a novel method of using a linear mapping to bridge the modality gap in image captioning, avoiding the need for costly end-to-end training. The proposed method requires only 1 million trainable parameters, which is significantly lower than other deep learning models, making it computationally efficient. The Iterative Self-improvement Loop Introduces a process of iteratively refining the mapping with synthetic captions generated by the model, which could lead to better performance metrics over time without additional human-labeled data. The method also achieves strong results on well-known benchmarks like MS-COCO and Flickr30k, indicating that the method performs well compared to existing lightweight captioning approaches. Overall, the paper provides a practical, efficient, and potentially more accessible solution to the problem of image captioning, with particular relevance to real-world applications where resources might be limited."
            },
            "weaknesses": {
                "value": "Generalizability Issue: The method is heavily relied on the referenced dataset which poses a critical limitation of the method. While the method is efficient, it may not generalize well to all types of images or domains, especially those significantly different from the reference dataset. If the testing dataset is very different from the reference set, it may cause catastrophic outcomes by error propagation and require much longer rounds of iteration. This would diminish the efficiency advantage. Further, if the reference dataset has limited topic/semantic coverage, then the potential hallucination issue would also happen as the LLM was only asked to summarize the referenced captions. Not matter how many iterations would be conducted, the hallucination may not be removed. \n\nLimited Experiment Issue: Additionally, the performance is benchmarked on specific datasets. Since both Flickr and COCO are two very common datasets and share large similarity in terms of image distribution thus there might be a lack of evidence on how the model performs on out-of-distribution data or in more diverse real-world scenarios. \n\nLLM Issue: The method also heavily relies on LLM, however the LLM never saw the image but only can summarize. The iterative refinement with synthetic captions could potentially introduce or amplify errors if the synthetic captions are not of high quality.\n\nComputational Resources: While the model has fewer parameters, the computational cost and efficiency are not only about the number of parameters but also about the complexity of operations and the size of the input data. For every image, the method requires to pre-computation of all the reference image and text data, not to mention the following newly updated synthetic captions for every iteration.\n\nMetric Sensitivity: The paper proposes a new metric for evaluation, but it\u2019s not clear how sensitive the results are to the choice of metric and whether the proposed metric has been validated extensively.\n\nSeverely Limited Novelty/Details of the Caption: A good caption is not just a high-level summarization of the image but should also cover essential fine-grained details. The characteristics of this method determines it cannot include fine-grained details in the caption.\n\nNovelty of the method: The method may lack novelty but is a practical improvement in real world scenarios."
            },
            "questions": {
                "value": "1. If the testing dataset is very different from the reference set, it may cause catastrophic outcomes by error propagation and require much longer rounds of iteration. This would diminish the efficiency advantage. How would you resolve this problem?\n\n2. How would you guarantee the reference set necessarily has all the sufficient scenes against the testing set? If not, then certain critical scenes in testing set images can never be addressed properly. What is the criteria of selecting the reference set and testing set to ensure this method can work properly? If you cannot find a criteria, then this method has a fundamental issue.\n\n3. Have you done experiment when the reference set and testing set are very different from each other?\n\nPlease see other questions in the  \"Weakness\" section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics concerns."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5139/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699417629289,
        "cdate": 1699417629289,
        "tmdate": 1699636507506,
        "mdate": 1699636507506,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iTSQ4WTFPq",
        "forum": "hgayrNSbri",
        "replyto": "hgayrNSbri",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5139/Reviewer_mspE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5139/Reviewer_mspE"
        ],
        "content": {
            "summary": {
                "value": "This paper trains linear mapping (W) on top of the CLIP image encoder and text encoder to improve or refine the caption of a given dataset. During training, the mapping weight (W) uses the least-square error as a metric. During inference, the trained mapping will be used to rerank the top K similar captions from a pre-collected image-text pair pool, and then use a Large language model to refine the captions."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The topic of image captioning is important.\n2. The proposed self-improvement loop is interesting."
            },
            "weaknesses": {
                "value": "The method presented is not one of image captioning; rather, it involves the enhancement of captions or the retrieval of text related to images. This is predicated on the assumption that there exists a preliminary set of fairly accurate captions for each image from which to draw. To better reflect this and to prevent any potential overstatement or confusion, a slight modification of the title is recommended.\n\nThe utility of the proposed method is constrained by a significant precondition: a repository of captions must be supplied to enable the retrieval process via mapping weights. Typically, we have only the image at our disposal without a pre-existing collection of initial or possible captions to draw from.\n\nThe principal innovation of this study is the introduction of a light linear mapping. However, this comes with two main limitations: (1) such a lightweight approach may be best suited to datasets that are small or of moderate size. The presumption here is that the learned weight \n$W$ is sufficient to bridge the gap, which may not hold true for datasets larger than, for example, COCO\u2014where a lightweight parameter might prove inadequate. (2) The drawback of a light parameterization is the necessity to maintain a substantial pool of captions to ensure coverage for any given image. Hence, the total memory required includes not just $W$ but also this extensive caption repository. Absent this, the method would be unable to perform image captioning autonomously.\n\nThe comparison in Table 1 appears somewhat skewed. The proposed method relies on the availability of a predetermined pool of captions for retrieval, in contrast to the baseline models which do not require such a resource."
            },
            "questions": {
                "value": "1 How to compare your method to the large multi-modal models, such as llava, instructBLIP, BLIP2, and MiniGpt-4? Any comparison on the image captions generated by those large multi-modal models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5139/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699475903619,
        "cdate": 1699475903619,
        "tmdate": 1699636507412,
        "mdate": 1699636507412,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rt4VBqMC6z",
        "forum": "hgayrNSbri",
        "replyto": "hgayrNSbri",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5139/Reviewer_RtHk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5139/Reviewer_RtHk"
        ],
        "content": {
            "summary": {
                "value": "This paper targets the problem of lightweight captioning by proposing to optimize a linear mapping between the visual and text embedding space. They argue that this is crucial to close the modality gap. What differentiates this work from those tuning such embedding layer via a Cross-entropy loss is that they tune the linear mapping via simple least squares. The authors build a dataset of (image, text) pairs which can then be used to optimize for the linear mapping. One the linear mapping is obtained, they caption an image by first retrieving closest captions from a dataset and prompting these to an of-the-shelf LLM. They also propose a self-improvement phrase where the model is used to generate captions for the training set and then again used to refine the linear mapping. Authors claim similar performance to prior light-weight captioning methods by only tuning the linear layer and that too on a CPU in much less time. They also shows experiments on cross-dataset transfer and also ablation studies on which features to use for optimizing linear mapping. They also highlight an issue with clip-score metric.\n\nOverall the idea seems relevant to the field. However, the method proposed in the paper lacks novelty and the experimental section is also weak."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Addresses an important problem of designing light weight image captioning systems\n- Proposes a training-free (at least no NN is trained) method to align visual and textual space. Argue that this closes the modality gap.\n- Idea of using LLM to summarize the nearest NN captions for an image is interesting (although it can lack grounding esp. when the retrieved captions might be missing the key concept)\n- Show comparable results on 2 datasets. Also, show that the self-improvement phrase results in improvement\n- Highlights an issue with clip-score metric, where it seems to be assigning higher score to hallucinated example"
            },
            "weaknesses": {
                "value": "- Overall the idea lacks novelty and depth. The key idea of doing retrieval augmented captioning is taken from prior work. Although the paper differs in how the training is being done (just doing least squares instead of training some layer), the performance is mostly below the previous works on most metrics (e.g. on Bleu small cap vs recap is 36 vs 29, numbers are similar on spice metric)\n- Authors have shown experiments on cross-dataset transfer, why didn't they consider other datasets such wizwiz, MSR-VTT.\n- Another issue that I see with this work is that the caption might not be grounded in the image. Based on Fig1, the final caption seems to be a summary of the retrieved captions which might not always be accurate. For example, what happens if the image contains a concept that is not a part of any caption\n- I agree that the finding about clip-score is interesting but it was not adding value to the key idea in the paper"
            },
            "questions": {
                "value": "Please see weakness section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Discrimination / bias / fairness concerns"
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5139/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5139/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5139/Reviewer_RtHk"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5139/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699486667360,
        "cdate": 1699486667360,
        "tmdate": 1699991749106,
        "mdate": 1699991749106,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NR8D4pRnWA",
        "forum": "hgayrNSbri",
        "replyto": "hgayrNSbri",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5139/Reviewer_rPFf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5139/Reviewer_rPFf"
        ],
        "content": {
            "summary": {
                "value": "In this research, the authors address the modality gap issue in pre trained vision-language models (VLMs) without the need for costly finetuning. They propose a cost-effective solution using a linear mapping optimized through a least-squares approach, achievable within minutes even on a CPU.  The method also includes an iterative refinement process using synthetic captions from the LM, allowing explicit optimization for image captioning metrics. The results demonstrate competitive performance on MS-COCO and Flickr30k datasets, especially in comparison to lightweight captioning approaches, and highlight the limitations of reference-free metrics such as CLIP-score."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is well-written and easily comprehensible. However, there is room for improvement in conveying the analysis and intuition behind the discussed concepts.\n\nWhile the paper addresses a crucial issue in the multimodal domain, the proposed solution's persuasiveness could be strengthened.\n\nThe author has presented qualitative and quantitative results across various datasets."
            },
            "weaknesses": {
                "value": "The author proposes bridging the modality gap through a cost-effective approach using a linear mapping optimized through a least-squares solution. However, there is a need for a more in-depth discussion on how this method differs from existing solutions in the realm of joint models.\n\nThe utilization of a \"linear mapping optimized via a least-squares solution\" is a fundamental constraint optimization technique. This may raise questions about the novelty of the method. To enhance the novelty of the method, the author should delve into a comparative analysis with existing joint model solutions found in the Visual Question Answering (VQA) literature and other multimodal joint representation studies. This discussion would shed light on the distinctive aspects and advancements introduced by the proposed approach.\n\nComputational Complexity: The text mentions that certain computations, such as least-squares linear model fitting, can be done on a CPU within minutes. It would be valuable to provide a brief discussion or estimate of the computational complexity involved in each step, giving readers an idea of the method's efficiency.\n\nIterative Self-Improvement: The iterative self-improvement process is described, but the rationale behind choosing high-scoring captions for augmentation could be clarified. Explaining why high-scoring captions are selected and how this contributes to the refinement process would add depth to the method's justification.\n\nNotation and Terminology: The use of symbols and notation is clear for the most part, but some terms could be defined or explained more explicitly. For instance, it would be helpful to explicitly state what \"W\" represents in the context of the linear model. Additionally, a brief glossary or notation table might aid readers in understanding the symbols used throughout the section. The hyperparameters \"k\" and \"l\" are introduced but not thoroughly explained. Providing more insight into the rationale behind choosing specific values for these hyperparameters or discussing their impact on the results would strengthen the method's transparency.\n\nHowever, I feel that the paper misses one of the core aspects of machine learning practice: readability and reproducibility of results. What core mechanism of the proposed explanation method is not clear here. The author should provide an algorithm or pseudocode to reproduce the results, which this paper misses"
            },
            "questions": {
                "value": "Please refer to the weakness section for this."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5139/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699511338246,
        "cdate": 1699511338246,
        "tmdate": 1699636507193,
        "mdate": 1699636507193,
        "license": "CC BY 4.0",
        "version": 2
    }
]