[
    {
        "id": "LSEcq2rvAy",
        "forum": "NhUinwpVSQ",
        "replyto": "NhUinwpVSQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2317/Reviewer_FTr9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2317/Reviewer_FTr9"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a discrete latent code into the TD-VAE framework to better model discrete actions in videos and disentangle the actions from states. Experiential results on MNIST, KTH, and Vizdoom show that their model, policy-disentangled VAE, can recognize the discrete action in the input videos and generate the futures conditioned on other actions."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The paper is easy to understand."
            },
            "weaknesses": {
                "value": "- The method evaluation is not convincing. As a 2024 submission on video generation, it only tests on three easy benchmarks: Moving MNIST, KTH with only one type of video (walk/run), and Vizdoom. In addition, the baseline method it compared with is MoCoGAN in 2018. This evaluation setting is definitely not at the bar of 2023. Even papers from three years ago like Dreamer [1] (ICLR 2020) have a more difficult setting.\n- The contribution is limited. Although the method is named policy-disentangled VAE, it is not conditioned on \u201cpolicy\u201d, but discrete actions like move left or move right. This can be achieved in papers 5 years ago like World Models [2] (NIPS 2018).\n\n- In addition, the idea of introducing discrete modes in VAE for better future modeling is not new and has been explored in other fields at least 3 years ago. (e.g., Trajectron++ [3] (ECCV 2020) in motion forecasting)\n\n\n[1] Hafner, Danijar, et al. \"Dream to control: Learning behaviors by latent imagination.\" arXiv preprint arXiv:1912.01603 (2019).\n\n[2] Ha, David, and J\u00fcrgen Schmidhuber. \"World models.\" arXiv preprint arXiv:1803.10122 (2018).\n\n[3] Salzmann, Tim, et al. \"Trajectron++: Dynamically-feasible trajectory forecasting with heterogeneous data.\" ECCV 2020"
            },
            "questions": {
                "value": "In general, I think the evaluation setting is too easy for 2023. Showing performance that is compatible with Dreamer v3 [4] (2023) or even Dreamer (2020) on tasks they evaluate will be a plus.\n\n[4] Hafner, Danijar, et al. \"Mastering diverse domains through world models.\" arXiv preprint arXiv:2301.04104 (2023)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2317/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2317/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2317/Reviewer_FTr9"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2317/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698420232664,
        "cdate": 1698420232664,
        "tmdate": 1699636164130,
        "mdate": 1699636164130,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qjxmLihrDO",
        "forum": "NhUinwpVSQ",
        "replyto": "NhUinwpVSQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2317/Reviewer_bJsN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2317/Reviewer_bJsN"
        ],
        "content": {
            "summary": {
                "value": "### Problem Statement\n\nThe paper addresses the problem of overlooking the underlying intentions or policies driving the actions in videos while utilizing deep generative models for video generation. Traditional models primarily treat videos as visual representations of actions performed by agents without delving into the motivations or intentions behind those actions. This lack of attention towards the intention behind actions can limit the understanding and representation of behaviors exhibited in videos.\n\n### Main Contributions\n\nThe main contribution of the paper is the introduction of a novel model called Policy Disentangled Variational Autoencoder (PDVAE). PDVAE aims to learn the representation of the policy, akin to the underlying intention guiding the actions, without supervision, alongside learning the dynamics of the environment conditioned to the policy. Unlike previous models, PDVAE can generate diverse videos aligned with a specified policy, and even allows for altering the policy during the generation to produce varied behavioral outcomes in videos. The model differentiates videos based on the policy of an agent and can generate videos where each agent adheres to the specified policy.\n\n### Methodology and experiments\n\nThe PDVAE extends beyond Temporal-difference Variational Autoencoder (TD-VAE) by disentangling the state and policy, thereby providing a more nuanced understanding and representation of agents' behaviors in videos based on their underlying intentions or policies. Through qualitative and quantitative experimental validations on three video datasets (Moving MNIST, KTH action dataset, and VizDoom), the paper demonstrates the effectiveness of PDVAE in capturing and generating videos based on policy representations, opening up new avenues for more intention- and context-aware video generation."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This work is essentially an extension to the referred TD-VAE work, introducing so-called \"policy\" latent variables to condition the generative decoder. The derivation introducing the latent variable by decomposing the ELBO is neat. The resulted architecture is sophisticated with carefully designed loss.\n\nThe smooth transition on the change of the policy latent is intriguing in that the generated video respects the dynamics of its content while being controllable."
            },
            "weaknesses": {
                "value": "### Limited evaluation tasks and baselines\n\nThe video generation mothod is evaluated on three datasets and compared to one baseline, which is not persuasive with respect to the versatility and superiority of the method.\n\n### Lack of ablation study\n\nDespite the sophisticated architecture, there is little ablation study analyzing the importance of its various components.\n\n### Writing\n\nAlthough the article conveys the ideas successfully, I find it sometimes repetitive, not very well organized, and not precise enough. Grammar mistakes also slightly hinder my comprehension.\n\n### Minor\n- The \"policy\" in this work is really a set of learnable embeddings disentangled from the latent space, while the authors' description tends to confuse it with the concept of policy in reinforcement learning, which is a function / distribution over actions. I understand that the learnable embeddings are intended to capture the \"policy\" which hypothetically controls the \"style\" or \"mode\" of the video generation, but it would help readers understand if the authors could further clarify this and differentiate the distinct concepts.\n- It would be beneficial if the authors could include the generated videos in supplement materials for better demonstration.\n- Please fix the missing space between the caption of Figure 8 and the main text below it."
            },
            "questions": {
                "value": "- How is the policy space visualized as a 2D scatter in Figure 6?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2317/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2317/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2317/Reviewer_bJsN"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2317/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698647299040,
        "cdate": 1698647299040,
        "tmdate": 1699636164041,
        "mdate": 1699636164041,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "nbqMZHdeHV",
        "forum": "NhUinwpVSQ",
        "replyto": "NhUinwpVSQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2317/Reviewer_y3xx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2317/Reviewer_y3xx"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a policy disentangled VAE that can generate diverse videos with user-specified policy during generation. It extends TDVAE by adding policy to the posterior distribution in ELBO. In order to learn the policy from observations, it proposes a lossy compression module and policy module to map observations to a fixed set of latent codes and train it with reconstruction loss.\n\nIt experiments in three environments: Moving MNIST, KTH Action and VIZDOOM. The metrics used are video/image quality metrics, e.g. FID, FVD, and policy accuracy. It shows better video quality and policy accuracy compared to MoCoGAN."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "It studies an interesting problem with a lot of potential applications. For example, it could be used in gaming and movie production and many robotic tasks.\n\nThe proposed method PDVAE is consistently better than MoCoGAN in terms of policy accuracy and video quality.\n\nThe writing and math in the paper are clear and easy to follow."
            },
            "weaknesses": {
                "value": "The paper builds on top of TDVAE. Why not compare with TDVAE in terms of video quality regardless that TDVAE cannot generate policy-conditioned rollouts? Can we prompt TDVAE with a demonstration (e.g. digits moving left) to achieve similar results as PDVAE?\n\nThe environments and tasks used in the evaluation are very toy cases where the action is always discrete. I wonder if the policy extraction module and the policy-conditioned dynamics model can deal with continuous actions. Does it work with real videos, e.g. driving videos, instead of synthetic ones?\n\nHow do you compare with other recent text2video approaches, e.g. Control-A-Video? Will a prompt like \u201cA person running in the style of KTH dataset\u201d already solve the problem?"
            },
            "questions": {
                "value": "How does it compare with TDVAE, which the method is built on?\n\nHow does PDVAE work on real videos and continuous action space?\n\nHow do you compare with modern text2video diffusion models? Could they solve the task in zero-shot?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2317/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2317/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2317/Reviewer_y3xx"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2317/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698686167065,
        "cdate": 1698686167065,
        "tmdate": 1699636163956,
        "mdate": 1699636163956,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "K93IVKw9tp",
        "forum": "NhUinwpVSQ",
        "replyto": "NhUinwpVSQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2317/Reviewer_6csw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2317/Reviewer_6csw"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a method (PDVAE) to generate visual representations (video / image sequences) of agents based on policies (and previous states) which are meant to capture the motivation of the agents. This method uses unsupervised learning to learn representations of policies and dynamics of the partially-observable environment without labels. The core ideas are (1) learning a disentangled representation of the policy and state, and (2) adding the policy to the posterior distribution over latent states to derive the ELBO (building on TD-VAE)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper introduces a novel architecture using VAE that builds on TD-VAE, but separates the representation of the policy from the state. There is a compelling argument that this disentanglement must be done to generate visual representations of agents that exhibit behavior with the appearance of intentionality in dynamic environments.\n\nThe paper is not a big improvement over the primary benchmark MoCoGAN, but MoCoGAN has access to the labels that PDVAE does not have. Compared to MoCoGAN- without labels, PDVAE performs much better.\n\nThe methodology appears sound, though I have not checked it too closely."
            },
            "weaknesses": {
                "value": "The writing could be improved, but overall it is fairly clear.\n\nThe results overall are not too strong, but they are reasonable and well-motivated, and may be an influence on future work on video generation. Unfortunately, only a single benchmark is used.\n\nMinor Errors:\n\nThe authors should add a sentence early on to explain what is meant by \"codebook\" in the VAE and how this relates to the action space.\n\nPage 4 (Section 3.1): \"..given as the observations do...\" doesn't quite make sense. Not entirely sure what this is supposed to say.\n\nPage 8 (Section 5.2): \"...capable of generating videos generates a video that is not...\" --> \"...capable of generating videos that are not...\""
            },
            "questions": {
                "value": "Are there no other benchmarks that would be appropriate to compare against other than MoCoGAN?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2317/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2317/Reviewer_6csw",
                    "ICLR.cc/2024/Conference/Submission2317/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2317/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698724065170,
        "cdate": 1698724065170,
        "tmdate": 1700688665022,
        "mdate": 1700688665022,
        "license": "CC BY 4.0",
        "version": 2
    }
]