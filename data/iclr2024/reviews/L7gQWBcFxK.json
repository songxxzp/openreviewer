[
    {
        "id": "1ew0CVXU05",
        "forum": "L7gQWBcFxK",
        "replyto": "L7gQWBcFxK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3630/Reviewer_qfPH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3630/Reviewer_qfPH"
        ],
        "content": {
            "summary": {
                "value": "Rather than the usual uniform sampling for stochastic gradient descent, this paper considers adaptive and importance sampling to perform variance reduction by attributing higher sampling probabilities to more important data points. The challenge with implementing adaptive and importance sampling in practice is that computing or even approximating the optimal sampling probabilities for importance sampling essentially requires computing the full gradient, at which point one might as well perform the full gradient descent. \n\nThis paper proposes an algorithm that approximates the probabilities for adaptive/importance sampling and then considers a specific approach that uses the cross-entropy loss gradient, e.g., for the purposes of classification tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ Variance reduction in stochastic gradient descent is an important research direction with significant implications and high relevance to the ICLR community.\n\n+ The proposed algorithm improves on uniform sampling in the provided experiments.\n\n+ Experiments conducted on a large number of datasets."
            },
            "weaknesses": {
                "value": "- Sufficient details have not been provided to formally understand the guarantees of the subroutines in Algorithm 1, such as ComputeImportanceSampling\n\n- No convergence analysis is provided for Algorithm 1 and thus it is difficult to ascertain under what conditions the importance sampling probabilities can be quickly approximated\n\n- It is not clear to me what the general framework in Section 4 is proposing, perhaps additional pseudocode would be helpful\n\n- There are a large number of popular methods for either acceleration or variance reduction for SGD that should be compared for a thorough empirical evaluation (for instance, any number of the references in the related work section)"
            },
            "questions": {
                "value": "- Is there any analysis you can provide for convergence?\n\n- After the approximate probabilities for importance sampling are obtained, how does the algorithm efficiently use these values to sample a data point? That is, are the sampling probabilities organized into a data structure of some sort that maps each real number (possibly implicitly through a set of intervals) to a specific data point?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3630/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697860662799,
        "cdate": 1697860662799,
        "tmdate": 1699636318701,
        "mdate": 1699636318701,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "G36CrxgX3X",
        "forum": "L7gQWBcFxK",
        "replyto": "L7gQWBcFxK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3630/Reviewer_FEz4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3630/Reviewer_FEz4"
        ],
        "content": {
            "summary": {
                "value": "The paper unifies approaches on importance and adaptive sampling. For this the paper proposes a simple importance function that is logit gradients for which there exists a close form expression for classification networks. They keep track of the importances of the samples across epochs and use that importance to sample the data and compute the weight function $w(x)$. \nOn multiple varieties of problems like image Classification, image regression, point set classification they show that they converge faster than prior works on importance sampling and adaptive sampling."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is well motivated and placed within the literature. The method is intuitive, and the paper is generally written well (see comments below). The biggest strength is the experiments where the authors show wall-clock speedups of their algorithm, instead of some hypothetical quantity. The experiments cover the usual set of benchmarks."
            },
            "weaknesses": {
                "value": "The only issue I see is with the specific motivation of the proposed importance function. I had a very tough time parsing the text below Eq 5 on page 5. For eg: The gradient norm in eq has two quantities $x$ and $x_i$. Also, the lipschitz constant is written as $l_{m(x, \\theta)}$ indicating that the constant of the network is input dependent. Is this intended? Additionally, this bound on the gradients is not informative about the chosen importance function. Eqn 6 makes it seem that the proposed importance function is valid only for Lipschitz neural networks. Is my understanding correct? If so, the proposed method is limited in its applicability. \n\nA discussion on the theoretical properties of the importance function is needed: why is this importance function better than the one in DLIS? In the absence of this, it is tough to attribute the successes of this method. For eg: the way the importances are tracked in Algorithm1 (Lines 13, 14) and the subsequent sampling in Line 7 (instead of the more involved update process in DLIS paper) may be more \"implementation friendly\" and thus the noticed performance improvements. \n\nFinally, the experiments are limited in the range of experiments covered. The paper covers experiments on MLPs and CNNS. It would interesting to see how the method performs on modern transformers."
            },
            "questions": {
                "value": "In addition to the comments on the importance function, I have the following questions:\n* What is cost of first epoch to estimate a starting importance function? Does that mean that you run a standard SGD loop and track $q$ without any other sampling? \n* What is value of $\\alpha$ used in Eqn 7? How important is it? \n* Why is the DLIS performance on Oxford flowers getting worse after a point? What is sparsity of dataset? \n* Last figure on page : there is a reference to \"difference between various weighting strategies and the full gradient norm\": What is full gradient norm here? \n\n## Minor\n* In page 2: paragraph on importance sampling: what is \"inherently multivariate\" mean? \n* Same as above: What is good compromise between optimization quality across all dimensions simultaneously mean? What dimensions? \n* The next para, last sentence is incomplete: `Recently, We....`\n* Page 3. $N$ is used before it is introduced. Change to $|\\Omega|$. \n* Last figure on page 9 is uncaptioned. \n\nOverall, I think its a very interesting contribution with good practical use. If the authors can provide a convincing rebuttal, I would be willing to raise my scores."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3630/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698617493483,
        "cdate": 1698617493483,
        "tmdate": 1699636318620,
        "mdate": 1699636318620,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1Yrba67vbQ",
        "forum": "L7gQWBcFxK",
        "replyto": "L7gQWBcFxK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3630/Reviewer_yYPP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3630/Reviewer_yYPP"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a gradient expectation estimation based on adaptive sampling and samples weighting approach. The presented framework is flexible to any function calculating the samples importance. However, the authors propose an efficient importance function based on the loss gradient of the output layer."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The presented approach is interesting and is sound. \nThe overhead required for calculating the weights for the resampling is relatively low making the approach attractive given the shown gain in the overall training time."
            },
            "weaknesses": {
                "value": "The title of the paper sounds problematic from grammatical point of view as the word adaptive is an adjective unlike importance. \nWhile the sampling scheme is well founded, it seems that the presented approach can suffer from overfitting issues. The authors explicitly propose to add an \\epsilon to the value of the importance to avoid focusing on a small set of the data points.\n\nWhile the presented experiments confirm faster convergence in most cases, it is relatively limited and further discussion on potential drawbacks of the proposed sampling, e.g., robustness to label noise and generalization would have been helpful.\n \nThe authors make reference to a paper they presented earlier which should have been avoided given the double blind review process: \"Recently, We propose an efficient algorithm and an importance function which when used for importance or adaptive sampling, shows significant improvements.\""
            },
            "questions": {
                "value": "What is meant with the sentence: In this method, as outlined in eq. (3), each data point\u2019s weight, denoted as w(xi), remains constant at N. ?\n\nBeyond the weight calculation overhead what are limitations and potential drawbacks of the proposed method?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3630/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698782117497,
        "cdate": 1698782117497,
        "tmdate": 1699636318539,
        "mdate": 1699636318539,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "i01aNFYXOE",
        "forum": "L7gQWBcFxK",
        "replyto": "L7gQWBcFxK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3630/Reviewer_iVwr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3630/Reviewer_iVwr"
        ],
        "content": {
            "summary": {
                "value": "The authors proposes an sampling strategy that depends on gradient of the loss for training machine learning problems, with both importance and adaptive sampling. It was tested on several classification as well as regression tasks with results that look promising. It demonstrates that by focusing more attention to samples with critical training information, one might be able to speed up convergence without adding computational cost."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. I especially like the visualization of the importance sampling in Figure 1, where 800 data-point are presented with a transparency proportional to their weight according to our method for a classification task. It clearly shows how the algorithm works intuitively."
            },
            "weaknesses": {
                "value": "The paper can be improved in several ways:\n\n1. The way the authors cite the references sometimes is confusing. It is hard to distinguish the main context from the reference. Please consider use paper numbers only or use parenthesis version.\n\n2. The paper lacks discussions of related paper. For example, https://arxiv.org/pdf/2104.13114.pdf also considers the importance sampling problem by sampling data points proportionally to the loss, instead of norm of gradient.\n\nFor another example, https://arxiv.org/pdf/2306.10728.pdf also proposes adaptively sampling methods for dynamically selecting data points for mini-batch. I'd love to see the authors discussed more about these papers.\n\n3. I'd like to see the authors elaborate more on the algorithms. For example, ComputeSampleImportance is mentioned in line 13 without further explained in this section.\n\n4. There seem to be many typos for the math in the paper. For example,\n\n$$\\mathcal{L}_{\\text {cross-ent }}=-\\sum_i y_i \\log s_i,$$\n where $s_i=\\frac{\\exp \\left(m\\left(x_i, \\theta\\right)_l\\right)}{\\sum_l^J \\exp \\left(m\\left(x_i, \\theta\\right)_l\\right)}$. Please correct the subscripts from (8) - (11).\n\nFor another example, the explanation after (4) is a bit confusing: where $m\\left(x_i, \\theta\\right)$ is an output layer, $x$ is the input data and $J$ means the number of classes. Try to directly use $x_i$ instead of $x$.\n\nOne more example, why does both $x_i$ and $x$ exist in (6)?\n\n$$\\left\\|\\frac{\\partial \\mathcal{L}(x)}{\\partial x}\\right\\|=\\left\\|\\frac{\\partial \\mathcal{L}(x)}{\\partial m\\left(x_i, \\theta\\right)} \\cdot \\frac{\\partial m\\left(x_i, \\theta\\right)}{\\partial x}\\right\\|$$\n\n5. It is hard to tell the algorithm performance differences for some figures. For example, in Figure 5 (left), the authors claim that at equal epochs (left), our methods (Ours IS & AS) show improvements compared to LOW Santiago et al. (2021) and DLIS weights. It is really invisible to see the difference. The authors may consider plotting the log scale results.\n\n6. There is no reference number to the figure in page 9."
            },
            "questions": {
                "value": "1. I feel a bit confused in Ours IS vs Ours AS. Does Ours IS mean that you set $w(x) =1 / p(x)$  and Ours AS mean that you set$w(x) =1 / N$? If so, why do you claim your adaptive sampling approach subsumes the adaptive weighting of Santiago et al. in page 3? I think they are different because Ours AS just do the sampling with non-uniform distribution and there is no weighting."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3630/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699309251240,
        "cdate": 1699309251240,
        "tmdate": 1699636318461,
        "mdate": 1699636318461,
        "license": "CC BY 4.0",
        "version": 2
    }
]