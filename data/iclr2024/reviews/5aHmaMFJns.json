[
    {
        "id": "Wwri4ghWNv",
        "forum": "5aHmaMFJns",
        "replyto": "5aHmaMFJns",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission755/Reviewer_3FoC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission755/Reviewer_3FoC"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a framework called RAFA that combines model-based reasoning and acting for autonomous LLM agents. The key idea is to use the LLM to plan a future trajectory that maximizes long-term rewards, take the first action from the planned trajectory, collect feedback, and replan at each new state. Theoretical analysis shows the regret bound under a series of assumptions. The method is evaluated on text-based environments like game of 24, ALFWorld, BlocksWorld, and TicTacToe."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The idea of combining model-based planning and short-term execution is logical and aligns well with model predictive control techniques in deep RL.\n- Theoretical analysis accounts for limitations like approximate planning and partial observability. Derives regret bound under stated assumptions.\n- Overall presentation, structure, and writing quality are good. Key ideas and algorithms are clearly explained."
            },
            "weaknesses": {
                "value": "1. The proposed model-based planning framework is identical to existing techniques like PETS [1], Planet [2], and Plan2Explore [3] from the deep RL literature. These prior works are not mentioned or compared anywhere in the paper, which significantly weakens the novelty claims. PETS, Planet, and Plan2Explore also learn dynamics models to plan future trajectories within a model predictive control framework [4]. The lack of citation and comparison with these highly relevant prior works makes the technical contributions unclear. I would also expect to see the authors compare with these traditional approaches that do not rely on LLM.\n\n2. The assumptions required for the theoretical results are very strong and may not perfectly hold in practice. For instance, Assumption 4.1 requires the LLM to accurately reconstruct the true transition dynamics from the provided prompts. However, black-box LLM models have imperfect mathematical reasoning capabilities, so perfectly modeling the dynamics via prompting is unrealistic for complex environments with high-dimensional and highly stochastic dynamics. The authors provide no empirical evidence or analysis on how well this assumption actually holds. If the core assumptions are violated, then the theoretical results lose significance since similar analyses exist for traditional model-based RL methods.\n\n3. The experimental validation uses simple toy domains like game of 24, ALFWorld, BlocksWorld and TicTacToe. These environments seem trivial for planning and optimization algorithms to solve. The necessity of using a powerful LLM is unclear when traditional planners could potentially succeed. Moreover, the provided prompts contain many examples and solutions, which greatly simplifies the problem. The authors should test the approach on more complex planning tasks where traditional methods fail but the LLM succeeds, such as Minecraft. \n\n4. It is also necessary to study the sensitivity to the number of examples provided in the prompts. Since the prompts contain many examples in a few-shot learning set-up, ablation studies should analyze the scaling of success rate as the number of prompt examples is varied. This would shed light on how much the pre-provided solutions are aiding the LLM versus solving tasks from scratch (or maybe can not solve the problem with very few prompt examples).\n\nThe above concerns weaken the contributions and should be addressed.\n\n\n[1] Chua, Kurtland, et al. \"Deep reinforcement learning in a handful of trials using probabilistic dynamics models.\"\u00a0Advances in neural information processing systems\u00a031 (2018).\n\n[2] Hafner, Danijar, et al. \"Learning latent dynamics for planning from pixels.\"\u00a0International conference on machine learning. PMLR, 2019.\n\n[3] Sekar, Ramanan, et al. \"Planning to explore via self-supervised world models.\"\u00a0International Conference on Machine Learning. PMLR, 2020.\n\n[4] Moerland, Thomas M., et al. \"Model-based reinforcement learning: A survey.\"\u00a0Foundations and Trends\u00ae in Machine Learning\u00a016.1 (2023): 1-118."
            },
            "questions": {
                "value": "Please see the weaknesses section for the main concerns. I also have some questions as follows:\n\n1. How long does it take to learn for each task? What computing resources are needed (such as memory) and how much would it cost to finish one task? How does the computation consumption compare to traditional planning/optimization methods?\n\n2. Can the authors provide the results on different base LLM models, including both API-based ones and open-source ones?\n\n3. I am also curious how much effort the authors spent on tuning the prompts. For example, how many trials of revising the prompts before it can work for a particular task? What are the observations and what is the strategy to tune the prompts?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission755/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698093365176,
        "cdate": 1698093365176,
        "tmdate": 1699636002982,
        "mdate": 1699636002982,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PfQvIzR8GU",
        "forum": "5aHmaMFJns",
        "replyto": "5aHmaMFJns",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission755/Reviewer_Aro2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission755/Reviewer_Aro2"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a novel framework, \"Reason for Future, Act for Now\" (RAFA), aiming to optimize the actions of large language models (LLMs) in real-world scenarios. This framework is designed to achieve a task using the fewest possible interactions with the external environment by melding reasoning with acting. The theoretical analysis establishes a regret bound for the proposed framework, which is corroborated by superior empirical performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "**In-Context Learning Utilization**: RAFA astutely leverages the inherent in-context learning ability of LLMs to bolster reinforcement learning efficiency.\n\n**Comprehensive Experimental Evidence**: The paper furnishes extensive experimental evaluations that underscore RAFA's efficacy and robustness."
            },
            "weaknesses": {
                "value": "**Irrelevant Theoretical Analysis**: Although the LLM is seemingly exploited in the theoretical analysis, what the theory really draws upon is actually a posterior inference oracle named LLM. Admittedly, a large transformer model pre-trained on carefully curated dataset may become such an oracle, but the assumption that a pre-trained large language model inherently serves this purpose is questionable. Besides, the analysis claims to draw connections between RAFA and Thompson sampling, but it is not clear how the posterior sampling is accomplished, which is absent from both the main text and the actual implementation in appendix.\n\n**Misrepresentation of Existing Methods**: The paper's assertion that existing techniques like graph-search or MCTS are akin to open-loop methodologies seems misleading. Techniques like MCTS can be effortlessly reconfigured for closed-loop control, wherein planning is grounded in the current knowledge base, and the inaugural action in the plan is executed. This approach essentially mirrors the RAFA framework, calling into question its claimed novelty."
            },
            "questions": {
                "value": "1. How is the posterior sampling accomplished?\n2. How does the RAFA framework tackle the challenge of efficient exploration in its operations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission755/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission755/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission755/Reviewer_Aro2"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission755/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698680967210,
        "cdate": 1698680967210,
        "tmdate": 1699636002904,
        "mdate": 1699636002904,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kL5YdWXvpE",
        "forum": "5aHmaMFJns",
        "replyto": "5aHmaMFJns",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission755/Reviewer_n4NZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission755/Reviewer_n4NZ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to cast the problem of designing language model agents as an RL problem. The proposed algorithm RAFA leverages the in-context learning ability of LLM to do model-based posterior sampling reinforcement learning (PSRL). Specifically, RAFA builds a memory $\\mathcal{D}$ of environment transitions and then uses an LLM to infer the environment dynamic in a novel, \"non-parametric\" and \"in-context\" fashion.\n\nThe policy rollout involves three components that are all in-context learning algorithms that are conditioned on the current $\\mathcal{D}$:\n\n- A **model**  predicts the next state conditioned on an input state-action pair\n- A **critic** predicts the value of transitions/trajectories\n- An **elite** proposes a number of potential actions based on an input state.\n\nDuring planning, the algorithm uses an MPC-style action selection. At each step of the rollout of length $U$,  the elite proposes $B$ action for $s_u$ and the model predicts the next state for each action. At the end of the rollout, the critic evaluates the value of each trajectory and selects the initial action $a_0$ that leads to the highest cumulative return. Then the agent takes a_0 and adds the new transition to $\\mathcal{D}$.\n\nThe paper then proceeds to analyze the theoretical properties of the proposed algorithm and the associated assumptions and shows that the algorithm achieves $O(\\sqrt{T})$ regret where $T$ is the number of environment steps. The proof methodologies employed are reminiscent of those typically found in standard PSRL literature., although I have not thoroughly checked the correctness of every detail in the appendix due to the time constraint.\n\nThe paper concludes with empirical assessments across four benchmarks: Game of 24, AlfWorld, BlockWorld, and Tic-Tac-Toe. In these evaluations, RAFA demonstrates superior performance when juxtaposed with selected baseline algorithms, occasionally by a significant margin."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Overall, I think the idea of this paper is quite neat. Formalizing the LLM reasoning through RL could be the first step towards formalizing the emergent but rapidly growing fields and laying a good theoretical and conceptual foundation. The proposed algorithm makes intuitive sense and the empirical performance seems competitive. I can see that many future works could build on this framework of treating LLM reasoning as RL."
            },
            "weaknesses": {
                "value": "The paper presents an interesting concept; however, there are areas that require further attention and clarification to fully realize its potential. I would like to offer a constructive critique on some aspects that, if addressed, could enhance the submission.\n\n**Theoretical contribution**\n\nOverall, my biggest concern is with the theoretical claims made by the paper. My concern is not with the correctness of the paper but how much utility the theoretical results have for the actual performance of RAFA. While the theoretical foundations of the paper appear sound, the practical implications of these theories on the RAFA algorithm's performance require further exploration. It is essential to clearly articulate the direct benefits of the theoretical results to the algorithm to prevent any misinterpretations about their significance.\nThe paper is currently written in a way that could lead to unknowledgeable readers thinking that this theory (e.g., regret guarantee) is crucial for the algorithm. My primary research area is not RL theory but I am reasonably familiar with the main developments in RL theory. It seems to me that the theoretical component is not particularly innovative in the context of existing PSRL literature ( for some examples, [1,2,3]). If the authors do not agree with my assessment, then I think it would be good to have a more detailed and explicit discussion about how the theoretical result of this work relates to existing works in PSRL.\n\nSince all of the theory is rooted in generic RL, where does LLM come in? I believe that the answer is entirely in the assumptions made by the theoretical results, specifically, Assumption 4.1 and those in Appendix D. Assumption 4.1 assumes that LLM is capable of doing generic posterior sampling given any $\\mathcal{D}$. This assumption seems overly optimistic, considering the fact that, in general, good posterior sampling is computationally challenging in all but the simplest cases. The paper cites [4] and [5] as supporting evidence that LLMs can do posterior sampling on natural text data, but [4] only uses a small-scale synthetic dataset, and [5] updates a continuous embedding to approximate $\\theta$, neither of which seems to be close to the kind of posterior sampling ability required by this work. Of course, it is entirely possible that LLMs can indeed do the kind of posterior sampling required here, but in that case, I think more thorough empirical verification is needed. For example, one can construct a simple environment where one can compute posterior sampling exactly and measure how well the results produced by the LLM match the analytical solution.\n\nMoreover, the empirical setting used to demonstrate the algorithm's performance involves a relatively small number of environment steps, which raises questions about the applicability of the regret analysis to the algorithm's real-world performance. Additional empirical evidence in this area could substantiate the theoretical claims.\n\n\n**Formatting**\n\nThe current formatting of the paper detracts from its readability and overall presentation. The text is densely packed, and the spacing between figures and tables is too narrow, often resulting in severe margin violation.  The absence of a conclusion section is also notable. I recommend a revision of the manuscript to address these formatting issues. This might involve condensing the theoretical content in the main body to allocate space for a conclusion and to resolve the formatting challenges. As I mentioned earlier, the regret guarantee, while interesting, may not be as central to the understanding and application of the algorithm as suggested. I am open to further discussion on this point if the authors have a different perspective.\n\n\n\n## Reference\n\n[1] Model-based RL with Optimistic Posterior Sampling: Structural Conditions and Sample Complexity. Agarwal et al.\n\n[2] Model-based Reinforcement Learning for Continuous Control with Posterior Sampling. Fan et al.\n\n[3] (More) Efficient Reinforcement Learning via Posterior Sampling. Osband et al.\n\n[4] An Explanation of In-context Learning as Implicit Bayesian Inference. Xie et al.\n\n[5] Large Language Models Are Latent Variable Models: Explaining and Finding Good Demonstrations for In-Context Learning. Wang et al."
            },
            "questions": {
                "value": "- does this value iteration using LLM as the critic converge?\n\n- I don't fully understand how $\\mathcal{D}$ is being incorporated into the prompts for the model, critic, and elite even after looking at the example in the appendix. Could you give me a concrete example?\n\n- Why sometimes do you use MCTS and sometimes you use other tree searches? How should the users choose between them and what are the trade-offs? \n\n- Why do you say you search all sequences in the pseudocode? Wouldn't that create $B^U$ number of trajectories which is not scalable at all?\n\n- How many queries are needed for each run? In traditional RL, the number of environmental steps is important because that is the bottleneck. I am not sure if this is the case here so the number of environment steps seems less useful. For a more fair comparison to the baselines, I think it would be fairer to compare the number of queries vs return to rule out the possibility that other methods can get better performance via more queries."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission755/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission755/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission755/Reviewer_n4NZ"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission755/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698729708394,
        "cdate": 1698729708394,
        "tmdate": 1700716849224,
        "mdate": 1700716849224,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fTF0FdwjMG",
        "forum": "5aHmaMFJns",
        "replyto": "5aHmaMFJns",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission755/Reviewer_P2KV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission755/Reviewer_P2KV"
        ],
        "content": {
            "summary": {
                "value": "The paper presents an approach (RAFA) to use the pretrained LLMs as sequential decision-making agents, by designing a system in which LLM instances are asked to first generate and evaluate a plan, and then to execute it in the environment. The paper includes a theoretical analysis based on Bayesian regret bounds and an empirical investigation in text-based domains."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- I find the general perspective of the paper to be promising. Analyzing LLM-based agents using the conceptual and theoretical tools traditionally associated with reinforcement learning research can provide interesting insights and a more rigorous perspective on modern AI agents.\n- The discussion of related work is quite complete."
            },
            "weaknesses": {
                "value": "- I am skeptical of how the theoretical background and analysis are related at all to the algorithmic and empirical setting. I would expect any meaningful theory about LLM-based agents to capture something about LLMs, but Assumption 4.1 basically says that the theory is going to assume access to the true posterior and ignores anything about the underlying models. I think this completely disconnects the theoretical results from the practice presented in the paper. In addition, I am not entirely sure of how the Bayesian perspective, in which the first part of the algorithm is supposed to estimate the posterior, is connected to the Model-Predictive Control-based algorithm that is actually employed in the paper.\n- The paper is so compressed and poor in terms of space and content organization that it is very hard to read. As a notable example, page 9 contains many plots too close one to the other and overflowing beyond the regular page limits and margins; moreover, it does not contain any conclusion, making it hard for the reader to further contextualize and understand these results.\n- The empirical evaluation does not seem to be rigorous. First, I do not fully understand why different models (GPT3.5, Vicuna, GPT4) have been employed in the different environments. What was the rationale behind the choice? Why one system and not the other for a specific task? Then, I do not see any error bars in the performance plots and table, nor any information about the number of repetitions of experiments."
            },
            "questions": {
                "value": "- What are the empirical results under a single LLM model?\n- How many repetitions did you use for the empirical results? Do the results vary that much if you have more?\n- Is there a way to avoid the complete disconnect between theory and practice?\n- Can you reformat the paper to avoid the abuse of vspaces and other techniques that make it hardly readable?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission755/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698787355368,
        "cdate": 1698787355368,
        "tmdate": 1699636002737,
        "mdate": 1699636002737,
        "license": "CC BY 4.0",
        "version": 2
    }
]