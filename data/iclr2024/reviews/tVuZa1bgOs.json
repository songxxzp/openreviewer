[
    {
        "id": "nPGflasV01",
        "forum": "tVuZa1bgOs",
        "replyto": "tVuZa1bgOs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5557/Reviewer_chuK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5557/Reviewer_chuK"
        ],
        "content": {
            "summary": {
                "value": "This paper studied the bias within V-L datasets and proposed a method to augment image-text pairs with multiple new emergent works. In detail, by decoupling the object and attribute in the images, locating the objects, adjusting the text, and inpainting the new images, this work rebuilt the V-L dataset in different scales and retrained baseline models. On several tasks, the proposed method is evaluated and compared with CLIP."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The bias within V-L training and problems of the datasets are important to dig.\n\n+ The thought of using SOTA tools to augment the datasets is non-trivial. And the proposed pipeline looks sound."
            },
            "weaknesses": {
                "value": "- The whole paper lacks many details of method design choices, data curation, method, training, and inference details, thus hindering the readers to fully understand the effectiveness of the proposed method. Only empirical results are not enough.\n\n- No bias analysis, which is the most important point in the introduction.\n\n- Lacking experiments using different key tools, e.g., LLM, detector/grounder, image generator, etc, which may affect the data generation a lot.\n\n- More solid analyses should be given to probe the relation between the data and the bias and performance of different tasks.\n\n- Prompt: The font color is hard to read."
            },
            "questions": {
                "value": "1. I suggest the authors also discuss the problems in the causal view, like counterfactual samples.\n\n2. Any user study of the image generation quality, prompt quality, and rationality of the V-l relations?\n\n3. CLIP-ft: fine-tuning CLIP on the augmented data? Missing details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5557/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698412234688,
        "cdate": 1698412234688,
        "tmdate": 1699636571554,
        "mdate": 1699636571554,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9VqoseN2Hi",
        "forum": "tVuZa1bgOs",
        "replyto": "tVuZa1bgOs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5557/Reviewer_NefH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5557/Reviewer_NefH"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces an approach to address reporting bias in vision-language datasets. The proposed method, known as bimodal augmentation (BiAug), incorporates language models, including ChatGPT, GroundingDino, and Stable-diffusion-inpainting. These models are integrated to supplement the attribute information of objects in the datasets. Ultimately, this integration leads to the creation of a new vision-language dataset."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper stands out for its excellent writing and organization. Its clarity enhances the ease of comprehension and makes it highly accessible to readers.\n\n2. The paper's motivation is not only intriguing but also holds significance within the context of vision-language tasks. The novelty lies in addressing the crucial issue of reporting bias. While it may appear that this paper seamlessly integrates large models, the rationale behind their usage is sound and nuanced, underpinning its innovative approach."
            },
            "weaknesses": {
                "value": "1. It appears that the current method focuses on data augmentation for vision-language (VL) datasets by altering one attribute of one object at a time. It would be beneficial if the author could elaborate on whether they have considered modifying multiple attributes and multiple objects simultaneously.\n\n2. In addition to conventional attributes like color, shape, and material, has the author explored more detailed attributes, as in [1], provided by Large Language Models (LLM)?\n\n3.  It is advisable for the authors to conduct a broader range of experiments and provide more visualization results, particularly in the context of Attribute Recognition Games (ARG) experiments.\n\n4.  The meaning of the abscissa in Fig. 5 is not immediately clear. Further clarification or labeling may be needed for readers to grasp the content effectively. Furthermore, Fig. 5's subcaption would benefit from further adjustment.\n\n5. The paper would benefit from a more extensive discussion of its limitations and potential directions for future research, offering a more comprehensive evaluation of the current method's scope and boundaries.\n\nIf the author can address my concerns, I would be willing to consider raising my scores.\n\n[1] Sachit Menon and Carl Vondrick. Visual classification via description from large language models. arXiv preprint arXiv:2210.07183, 2022"
            },
            "questions": {
                "value": "See `Weakness' above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5557/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698740556744,
        "cdate": 1698740556744,
        "tmdate": 1699636571450,
        "mdate": 1699636571450,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8QnUG9tvEY",
        "forum": "tVuZa1bgOs",
        "replyto": "tVuZa1bgOs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5557/Reviewer_ANJ5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5557/Reviewer_ANJ5"
        ],
        "content": {
            "summary": {
                "value": "This paper focused on the problem of reporting bias in visual-language models. The authors proposed BiAug, a data augmentation framework through object-attribute decoupling. BiAug synthesizes vision-language examples and constructs hard negatives by utilizing foundation models including LLMs to extract objects, generate attribute descriptions and negative descriptions, a grounding object detector to detect objects, and an inpainting model to create new images. This paper compares with CLIP baselines on several object-attribute understanding benchmarks and zero-shot retrieval tasks. The results show the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed BiAug framework utilizes several foundation models to generate synthetic and negative samples to disentangle objects and attributes. This is helpful to mitigate the reporting bias of VLMs.\n2. The experiments on ARO dataset shows the effectiveness of proposed frameworks by comparing with two CLIP baselines. The experiments also show a clear trend of enlarging the dataset size, demonstrating the promising scalability of the proposed augmentation framework."
            },
            "weaknesses": {
                "value": "1. This paper utilizes multiple foundation models as black-box tools. It lacks of intuitive measurement of the quality of foundation models' output. For example, is there a numerical evaluation of the box quality generated in step 2 in Figure 2? In step 1 how is the quality of the \"guessed\" potential objects?\n2. This paper claims also to achieve improvement in general text-image retrieval benchmarks. However, in figure 5 table (c) and (d), the performance of BiAug and Clip-ft is rather close on the 40k subset, and with more examples, BiAug didn't show clear improvement compared with either the BiAug or CLIP-ft baseline on the 40k subset. These results can't really show a solid improvement on the two datasets.\n3. This work only compares with two baselines: CLIP and CLIP-ft. It is unclear whether BiAug can also benefit other VL models such as NegCLIP (Yuksekgonul et al., 2022) which has been trained with explicitly constructed negative samples."
            },
            "questions": {
                "value": "See Weakness section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5557/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5557/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5557/Reviewer_ANJ5"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5557/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699220861099,
        "cdate": 1699220861099,
        "tmdate": 1699636571353,
        "mdate": 1699636571353,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "k7uaV5G86z",
        "forum": "tVuZa1bgOs",
        "replyto": "tVuZa1bgOs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5557/Reviewer_6Azz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5557/Reviewer_6Azz"
        ],
        "content": {
            "summary": {
                "value": "**POST REBUTTAL NOTE FOR AUTHORS:**\n\nI appreciate the effort put into this paper and acknowledge that I have read your responses.\n\n-------------------------------------\n\n**PRE REBUTTAL REVIEW:**\n\nThis paper focuses on the phenomenon of \"reporting bias\" in VLMs. \"Reporting bias\" refers to the case where part of the text description of an image is not present due to omission of implicit/commonsense knowledge. Authors claim that mitigating \"reporting bias\" may help with two issues: 1) biased captions 2) skewing the VLM towards frequently occurring patterns. \n\nTo mitigate the \"reporting bias\", authors propose \"bi-modal augmentation\" with the goal of disentanglement of the object-attribute associations. More specifically, BiAug augments a source dataset with explicit commonsense knowledge. \n\nBiAug has three phases: 1) Cross-mode object extraction:identifies the objects that are not explicitly referred to in the caption, 2) Decouple object-attribute association: creates a pair of descriptions for the image and 3) Image synthesis: in-painting the image with the pair of descriptions found in phase (2).\n\nThe augmented datasets produced by BiAug are used to train models, which are then evaluated against benchmarks for object-attribute understanding and general vision-language retrieval tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "-  \"Reporting bias\" is an important concept that affects the performance of machine learning models in understanding and processing real-world data.\n\n- The paper tackles the \"reporting bias\" problem and aims to provide a more nuanced dataset that includes objects and attributes often omitted due to reporting bias.\n\n- This paper could have implications for AI fairness: By addressing reporting bias, the research moves towards creating AI systems that can potentially reduce cultural and linguistic biases, contributing to the broader goal of AI fairness and ethics.\n\nOverall I found the problem to be very interesting and timely."
            },
            "weaknesses": {
                "value": "- I find the framework overly complicated and built on fragile assumptions. For example, why can we assume that the grounding object detector can find objects that a different color or shape of it is still a valid object? How can we make sure that the detected grounding object is big enough for the in-painting model to process this? Also see my questions below. I believe this overall complexity hinders the generalization of the results. \n\n- Overall I did not find the paper easy to read. This is partly due to the fact that the methodology is very complex with lots of pieces. However, I think the authors need to spend more time thinking about the structure and also definitely re-writing the abstract. \n\n- Many important concepts used in the paper are not clearly explained. For example \"grounding object\" need clear definitions.\n\n- I find it hard to understand how the framework described in Figure 2 could be reliable for large scale experiments.\n\nMinor points:\n\n- I suggest having the term \"reporting bias\" italic or quoted as it is not a well-known term/concept. And it is not clear that it's a technical term. Specifically it is used in the beginning of the abstract as well as the introduction. I think both sentences in the start of the abstract and the start of the intro need to be re-written/clarified."
            },
            "questions": {
                "value": "- Where do the samples in Figure 1 come from? Are they from a real dataset? Are they generated? \n- In Figure 2, in step 3, why \"tennis ball\" is dropped? It was part of the original text. Why \"dog\" is selected as the grounding object?\n- In section 3.2 how do you select from the list of predefined categories? For example, in figure 2, how come the \"shape\" is not selected for \"grass\"? Of course it does not make sense to modify the shape for grass, but how do you account for that and select \"color\"?\n- In 3.3 how do you mask the detected object? \n- In the example given in figure 3, what happens if non of the predicted objects are inside the image?\n- In figure 4, the \"blue boat\" and \"red boat\" samples, what was the missing reference object here according to your framework? What happened in the extraction phase? boat was already part of the original caption. \n- What is \"other\" in the preset attribute list?\n\nOverall, I found the figures and methodology very confusing. I am willing to modify my score based on the clarifications that the authors may provide. However, I am concerned with the overall writing and presentation of this work and I believe restructuring the paper to present the methodology in a more digestible and elaborate format could be very useful."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5557/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5557/Reviewer_6Azz",
                    "ICLR.cc/2024/Conference/Submission5557/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5557/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699468206060,
        "cdate": 1699468206060,
        "tmdate": 1700852093206,
        "mdate": 1700852093206,
        "license": "CC BY 4.0",
        "version": 2
    }
]