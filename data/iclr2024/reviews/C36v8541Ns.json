[
    {
        "id": "Xg7ojJaAv5",
        "forum": "C36v8541Ns",
        "replyto": "C36v8541Ns",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6520/Reviewer_W4Xb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6520/Reviewer_W4Xb"
        ],
        "content": {
            "summary": {
                "value": "This work analyzes the certified radius that can be provided to off-the-shelf classifier that are also Lipschitz continuous. The authors use previous results (namely, that a certified radius can be derived deterministically for Lipschitz predictors, and that randomized smoothing provides probabilistic certified radius as well as Lipschitz smoothed predictors) to show that there exist an optimal way to determine the strength of randomized smoothing that should be applied to a Lipschitz function. In addition, the authors make the observation of changing  the mapping function to the probability simplex as well as better finite-sample concentration inequalities to derived slightly improved guarantees on popular benchmarks (CIFAR10 and ImageNet)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- While it has been known that randomized smoothing provides a Lipchitz continuous randomized classifier, with constant inversely proportional to the smoothing strength, it has been unclear what one can gain by incorporating the (natural) assumption that base (or sub) classifier is also Lipschitz. In my opinion, this is the main contribution of this paper, which is nice."
            },
            "weaknesses": {
                "value": "- The paper is at times verbose, and at times unclear. Some comments are exaggerated or incorrect (See below).\n- Nomenclature is often incorrect (see below).\n- In my view, the contributions of employing a better concentration inequality to provide tighter tail guarantees for finite samples is small, as is the fact of changing the function that maps the sub-classifier predictions to the probability simplex."
            },
            "questions": {
                "value": "**Major Points**\n\n* Throughout the paper, the authors refer to softmax (and other functions) as \"projections\" onto the probability simplex. The softmax is not a projection onto the simplex. In fact, this reviewer believes that the softmax is not a projection onto any set (is it?). Throughout the manuscript, the word \"projection\" is used in lieu of the term \"function\" or \"map\". \n\n* Randomized smoothing can provide a certified radius at varying levels, depending on the strength of the added noise. It is clear to me that leveraging the fact that $f$ is Lipschitz continuous allows the authors to find an optimal randomized smoothing strength, as indicated in Prop. 4. Such a smoothing strength results in a certified radius. What is unclear to me is, what can be done if one requires certification at a different radius? \n\n* page 3: The definition of $L(f)$ that the authors use require $f$ to be differentiable. This assumption is not necessary (as best as I can tell) for their results to hold (and indeed, not applicable to ReLU networks). So, just define $L(f)$ by its standard definition (without assuming differentiability).\n\n* The authors comment on some related works pertaining the analysis of Lipschitz-continuity for devising certified radii. There exist extensions of these ideas that refine the analysis to requiring only local Lipschitzness, as in [A], which lead to improvements on the certified radius. Can the authors comment on relations to that work, and on whether their results could apply in a local way as in [A] ?\n\n* page 4: the authors mention that \"these bounds are relevant for the composite classifier defined as $\\Phi^{-1} \\circ f$. However, I believe they mean the classifier $\\tilde{F}_s$, no? \n\n* The authors argue in Section 3.3 that different functions s(z) have different impacts in terms of the variance of s(Z) vs the variance in Z. This is quite obvious, and, respectfully, I find the Example 1 not useful/trivial. ii) It is unclear, however, how such a map should be chosen to be optimal. It seems to me (from their experimental section) that they simply compute different options based on different alternatives for s(z) and choose the one that results in larger empirical margin. Is this correct? Shouldn't one be able to provide a \"best\" choice for s(z)?\n\n* After Prop. 3, the authors mention that \"This refinement on the bound was possible by supposing Lipschitz continuity on the base classifier $f$. Note that its Lipschitz constant can be arbitrarily high, so this assumption is quite light\". What do the authors mean by \"light\" here?\n\n\nReferences:\n\n(A) Muthukumar et al, \"Adversarial Robustness of Sparse Local Lipschitz Predictors, SIAM Journal on Mathematics of Data Science, 5:4, 2023\n\n\n\n**Other points**:\n* page 1: the authors write that there's a \"circular dependency: the regularity of the smoothed classifier depends on the Lipschitz property of the base classifier and the variance of the Gaussian convolution which governs the induced level of smoothness.\" Yet, i don't see how this is a circular dependency.\n\n* page 3: what do the authors mean by \"furnished by $f$\"?\n\n* page 3: \"concerning the $\\ell_2$ norm\" -> \"w.r.t.... \"\n\n* page 3: In defining the margin of $f$ at $x$, it should be \"of $f$ at $(x,y)$\", since its definition uses the correct label $y$.\n\n* Prop. 1: The first sentence is incomplete. Please revise English usage.\n\n* The authors use the term \"sub-classifier\" to refer to the un-robustified function $f$, alas sometimes they refer to it as a \"classifier\" (as in Prop.3)\n\n* It might help the reader to make explicit what the final classifier is (which I believe is $\\argmax_{s,t} \\tilde{F}^t_s(x)$, no?)\n\n* In the conclusion: \"In this paper, we demonstrate a significant correlation between the variance of randomized smoothing and two critical properties of the base classifier\" I don't think the authors demonstrate any \"correlation\" between these things. Maybe they meant \"connection\"?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6520/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6520/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6520/Reviewer_W4Xb"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6520/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698717442347,
        "cdate": 1698717442347,
        "tmdate": 1700605677544,
        "mdate": 1700605677544,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0NKCLAeAF9",
        "forum": "C36v8541Ns",
        "replyto": "C36v8541Ns",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6520/Reviewer_ay9y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6520/Reviewer_ay9y"
        ],
        "content": {
            "summary": {
                "value": "In this paper authors study the interplay between randomized smoothing, Lipschitz bounds and margin of the classifier, to design a new robustness certification method with improved guarantees.  \nMore specifically, they show that when the Lipschitz constant of a base classifier $f$ is low, the robustness radius of the smoothed estimator $\\tilde f$ can be lower, and the variance of the associated estimators is also lower. Therefore, with the same cost (in $n$ the number of samples) a bigger certified radius can be estimated at risk level $\\alpha$.  \nThe robustness radii rely on a margin, typically the difference between the two highest logits. Author show that `argmax` might not be the best option, and that other options like `sparsemax` have better properties.\n\nFinally, the new certification method is tested and compared against randomized smoothing and Lipschitz-based certification, on Imagenet and Cifar-10 tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "### Originality\n\nThe work takes a new look at an existing topic, by incorporating Lipschitz constraints into randomized smoothing. It borrows tools from different fields, like concentration inequalities, projections onto the simplex (argmax, softmax, sparsemax), and robustness certification against adversarial attacks.  \n\n### Clarity\n\nFig 1. helps a lot to the understanding.\n\n### Significance\n\nThe proposed algorithm LVM-RS can be seamlessly ingrated into existing frameworks, it is very general (with different temperatures and simplex projections), and not too expensive to run. The empirical results are convinving, since the new method improves upon both (i) Randomized Smoothing (ii) 1-Lipschitz networks of (Wang & Manchester, 2023)."
            },
            "weaknesses": {
                "value": "I struggled a lot to understand the paper at times. There is a lack of details, and even with the help of the related literature (e.g. Cohen et al) I couldn't be sure of what the authors were trying to say. My questions are detailed below. Clarifications would benefit a lot to the paper."
            },
            "questions": {
                "value": "> Most of the randomized smoothing approaches simplify the margin $-\\max_{y\\neq k}\\hat f_k(x)\\geq -(1-\\hat f_y(x))$ (Cohen et al., 2019)\n\nCan you clarify where in the paper of Cohen? Are you referencing their choice $p_A=1-p_B$ in top of their p6 ?\n\n> For each combination of (s, t), we determine the risk-corrected margin $M_2(p_{st} , \\alpha)$.\n\nIs it statistically correct (w.r.t $\\alpha$) ? Since you perform multiple tests with the same scores $f(x+\\delta_i)$ ? \n\n### Sets and domains of functions\n\nIt is not always clear from context what are the definition of functions. For example:\n* $\\hat f$ is implicitly defined in top of page 4, I assumed you meant $\\hat f_k()$ in the argmax. So $\\hat f:\\mathcal{X}\\rightarrow\\mathbb{R}^c$\n* but $\\Phi$ is a scalar function (per its definition), so what is the meaning of $\\Phi^{-1}\\circ \\hat f$ ? Is it vectorized?\n* so, is $g$ a scalar or a vectorized function Eq (3) ? \n\nIn Sec 2.1 the notation $L()$ is defined as a maximum of gradient norm over $\\mathbb{R}^d$. But the gradient is only defined for scalar functions, not for multivariate functions (in which case it is a Jacobian, not a gradient). This makes sense to define $l(g)=\\max_k L(f_k)$. But, then, if $g$ is multivariate, how can you write $L(g)$ ? What does it mean?  \n\nI suggest to define clearly each notation, and specify the domain of each function, not in an implicit manner in the middle of a formula.\n\n### Runtime\n\nWhat is the runtime cost of your method compared to randomized smoothing?\n\n### Typo\n\nThe paper can be hard to read. In p6:\n\n> Cohen et al. (2019); Salman et al. (2019); Carlini et al. (2023) use argmax simplex projection they can use the Clopper-Pearson Bernouilli tailored confidence interval ... etc\n\nThis paragraph should be rewritten, the first two sentences are not grammatically correct.  \n\n> Proposition 2\n\nIs it a definition of the `shift` function in left hand side? Use `:=` notation like `\\vcentcolon=` if so. Otherwise, define it somewhere. Please add a discussion of what $Z_i$ is supposed to be in *your* context.  \n\n> Once the variance has been minimized via Lipschitz constant regularization and that we can leverage\nlow resulting empirical variance with Empirical Bernstein\u2019s inequality a vital aspect for achieving\na significant certified radius is negotiating the margins-variance trade-off\n\nAdd punctuation.  \n\n> This is because the compensatory shift, incorporated\nto address the inherent risk, becomes minimal\n\nWhat does this sentence mean?\n\n> Equation (4)\n\n$R_2$ and $R_3$ inequality should be reversed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6520/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6520/Reviewer_ay9y",
                    "ICLR.cc/2024/Conference/Submission6520/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6520/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698776467231,
        "cdate": 1698776467231,
        "tmdate": 1700219891496,
        "mdate": 1700219891496,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Bc8HcGvOP9",
        "forum": "C36v8541Ns",
        "replyto": "C36v8541Ns",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6520/Reviewer_12zu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6520/Reviewer_12zu"
        ],
        "content": {
            "summary": {
                "value": "This paper studies methods to obtain improved randomized-smoothing (RS) certificates against $\\ell_2$ bounded adversarial attacks. The main contribution is to incorporate the lipschitz constant of the _base_ classifier $f$ into RS certificates to obtain improved bounds on the certified radius of the smoothed classifier $g$. Additionally, RS certificates hold with a probability $1 - \\alpha$, where $\\alpha$ is small. The auxiliary technical contribution of this paper is to utilize the variance of the output of $f$ to control $\\alpha$ better than prior work. Empirical evidence supports the theoretical results, demonstrating improved certificates over prior work."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. RS is currently the state of the art certification method for obtaining certificates of robustness against $\\ell_p$ attacks. However, the interaction of the properties of the base classifier with the final certificate is not very well understood. This paper attempts an understanding of how the Lipschitz constant of the base classifier affects the final certificate \u2014 this is an interesting problem to study and is potentially very useful for the community. \n\n2. Further, the paper revisits computation of the confidence $\\alpha$ that the RS certificate hold with, and demonstrates how the variance of $f$ (the base classifier) can be used to improve $\\alpha$, via a better concentration inequality. Exploring improved concentration inequalities for RS is also a useful research direction."
            },
            "weaknesses": {
                "value": "1. Writing: The writing of the paper is quite wordy in many places, and confusing at times, leading to major issues in readability and clarity. Details:\n\n\t-\tAbstract: It is unclear what \u201cvariance introduced by RS\u201d, \u201csimplex projection\u201d, \u201cvariance-margin tradeoff\u201d mean. It is unclear what does bernstein\u2019s inequality have to do with any of this. \n\n\t-\tIntroduction: Would be good to define prediction margin. All methods with certified radius deal with a way of controlling /estimating the Lipschitz constant of the network, it is unclear why is Tsuzuku et. al. specifically mentioned. The statements are also quite wordy here (\u201cradius that encompasses \u2026 constant\u201d), a simple $|f(x) - f(x + \\delta)| \\leq L ||\\delta||$ would suffice here.\n\t-\tFigure 1: What is radius binding, risk shift, simplex projection? \u201cRisk\u201d is used throughout the introduction without a definition. \n\t-\tP2: The circular dependency mentioned in paragraph 2 was not clear, what exactly is the circular dependency?\n\t-\tThe definition of R(f, x) is written in a very convoluted fashion, isn\u2019t it simply $\\min_\\epsilon$ such that $\\exists$ adversarial example $\\in B(x, \\epsilon)$. The benefit of the written form of the definition is unclear. \n\t-\tThe correct earliest reference for Randomized Smoothing is not Cohen et al. 2019, please see the related work section in Cohen et al. 2019 for the correct references. \n\t-\tDefining the certified radius for the composition of $\\Phi^{-1}$ and $\\tilde f$ is confusing, since the radius is to be used for certifying $\\tilde f$, and not the composition. \n\t-\tSection 2.4: Margins and Robustness \u2014 This is again too wordy \u201cpromise in bolstering classifier defense\u201d, \u201cinherent synergy between them\u201d, \u201cour exploration dives deep into this nexus unravelling novel insights \u2026\u201d. The message again simply seems to be that greater margin = larger certified radius (at fixed L), and could be stated in a single line.\n\t-\tSection 3.1: Relation between certified Radii. The highlight of this section seems to be the last paragraph, that variance plays a crucial role in the RS certificates. It is unclear what does the rest of the section add to this message. Perhaps it would be better to simply stress this strong point, and detail Table 3.1.\n\t-\tTable 3.1 is hard to parse, could simply plotting the difference between R2 and R3 convey the message better?\n\n2. Technical Correctness: Related to the point above, there are several places where some additional steps / statements are needed to support the technical claims made in the paper:\u2028\n\t-\tP3, Proposition 1. Isn\u2019t the condition for correct prediction simply that margin > 2 * Lipschitz Constant * perturbation radius? ( Assume class 1 is majority at x, class 2 is second highest, then $|f1(x) - f1(x + v)| \\leq L ||v|| = L \\epsilon$, and $|f2(x) - f2(x + v)| \\leq L epsilon$. Then, if $f1(x) - f2(x) > 2 L \\epsilon$, then $f1(x + v) > f2(x + v)$). Why is the constant $\\sqrt{2}$, instead of $2$?\u2028\n\t-\tAppendix P12, Proof of Proposition 3. \n\t\t-\tWhy is $u \\in R^c$ defined? Doesn\u2019t seem to be used in the proof. \u2028\n\t\t-\tAre there are several typos confusing $\\delta$ with $\\delta\u2019$? For instance why is $E_\\delta [v^\\top \\delta h(x + \\delta)] = E_\\delta [v^\\top \\delta\u2019 h(x + \\delta\u2019)]$? The next step then replaces one of the $\\delta$ with $\\delta\u2019$, which is unclear too. \u2028\n\t\t-\t**Major 1**: Why is $v^T \\nabla \\tilde h(x) = (1/\\sigma^2) E[\u2026]$? Expanding $v^T \\nabla \\tilde h(x)$ gives $v^T \\nabla \\tilde h(x) = E[v^T \\nabla h(x + \\delta)]$, it is unclear how to proceed from here. \u2028\n\t\t-\tIn the step marked (i) in the proof, is there some typo? $|h(x + \\delta) - h(x + \\delta\u2019)|$ is replaced by $\\min(1, 2L) |v^\\top \\delta|$, why is this true? Isn\u2019t this bound $L ||\\delta - \\delta'||$?\n\t\t-\t**Major 2**: In the construction for the lower bound on $J$, $h_0 = (\u2026) \\min (\u2026, l(h)\u2026)$, but how can the constructed classifier depend on its own lipschitz constant. Some clarity is needed on why one can do this, and why this classifier exists. \n\n\n3. Technical Clarity: There are several places where additional clarity would help the reader.\n\t-\tIt is unclear how one obtains (4) by using the bound mentioned in the paragraph preceding it. A few lines would be great for readers not familiar with the proof in Cohen et al. 2019. \u2028\n\t-\tThe Gaussian Poincare inequality seems to be very similar to other well known concentration inequalities for functions of gaussian random variables. A brief contextualization would be great. For instance, how does this relate to the standard concentration inequality for Lipschitz functions of sub gaussian random variables: $P( |h(Z) - E h(Z)| > \\delta ) \\leq \\exp(-c \\delta^2 / L(h))$ (for a reference, see Theorem 5.2.2 in High Dimensional Probability by Roman Vershynin)\u2028\n\t-\tEq (5), where is alpha in the RHS? Specifically, it is unclear how the $p_1, p_2$ have any explicit dependence on $\\alpha$. I believe the dependence is that the number of samples n affects both alpha and $p_1, p_2$. But this is not clear at all by looking at (5). \n\n\t-\tRelated to the above point, now since the dependence was not specified in (5), it is unclear how $M_2$ is computed in the main algorithm Algorithm 1. Specifically, what is $\\bar M_2(p, \\alpha)$ on Line 6 of Algorithm 1, how does it relate to $s$, how does it relate to $t$?\u2028\n\t-\tWhat are the simplex projections being used in line 4, algorithm 1? The text mentions \u201cfor various projections \u2026 spanning softmax and sparsemax\u201d \u2014 Which projections specifically? \u2028\n\t-\tNow, in order to compute the margin $M_2$, I believe one needs to know the lipschitz constant of the classifier $s$ composed with $f$, how is this computed for each simplex projection chosen?\u2028\n\t-\tExperiments: How is the 1-Lipschitz backbone constructed. This should be detailed as it relates to the main contribution. \u2028\n\t-\tExperiments: The effects of the change in the $\\alpha$ calculation vs the change in the certificate are unclear. In particular, what are the results when the $\\alpha$ calculation in this paper is used on top of Cohen et. al.? Similarly, what are the results when the Clopper-Pearson $\\alpha$ calculation are used on top of this paper\u2019s certificate?"
            },
            "questions": {
                "value": "At a high level it would be good to,\n\n(1) clarify and augment the paper with additional technical details to complete the proofs and correct the typos (details above),\n\n(2) explain the precise mathematical details of the certification procedure from input to certificate (see details in weaknesses above), and\n\n(2) empirically ablate the two components of the proposed approach (see details above)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6520/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6520/Reviewer_12zu",
                    "ICLR.cc/2024/Conference/Submission6520/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6520/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698792863999,
        "cdate": 1698792863999,
        "tmdate": 1700563128240,
        "mdate": 1700563128240,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "nuQpqxyes2",
        "forum": "C36v8541Ns",
        "replyto": "C36v8541Ns",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6520/Reviewer_2wVc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6520/Reviewer_2wVc"
        ],
        "content": {
            "summary": {
                "value": "This paper identifies a circular dependency between the regularity of the smoothed classifier, the Lipschitz of the base classifier and the variance of gaussian noise in randomized smoothing procedure. Then they propose the LVM-RS framework, given either a Lipschitz constant or variance, one can select the complementary variance or Lipschitiz constant to maximize the synergistic effect of RS and Lipschitz continuity. Experiments show superior performance of the proposed method comparing with RS or deterministic Lipschitz training alone."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper identifies an interesting circular dependency among each ingredient in the RS procedure, and propose a principled way to improve current certification radius in a zero-shot manner.\n\nIn general, the paper is well written and each statement seems to be well supported by proof, empirical evaluation and intuitive explanation. Especially Figure 1 did a great job in summarizing the contribution of this paper.\n\nExperiment results shows superior performance to the current state-of-the-art methods."
            },
            "weaknesses": {
                "value": "1. In terms of presentation, I feel the authors could add an overview paragraph for section 3. Sometimes I fail to connect each sub-sections. It may be good to map the flow of section 3 to Figure 1.\n\n2. It will be good to discuss the computational cost of LVM-RS comparing to RS and deterministic Lipschitz."
            },
            "questions": {
                "value": "1. Is there a typo in Eq 4 where \\leq should be \\geq? Currently it contradicts table 1.\n\n2. In algorithm 1, how many temperature do we need? And how to find those temperature?\n\n3. How much computational cost that is induced by using multiple temperature? How sensitive the certified accuracy is with respect to the corrected margin?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6520/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698945705198,
        "cdate": 1698945705198,
        "tmdate": 1699636732624,
        "mdate": 1699636732624,
        "license": "CC BY 4.0",
        "version": 2
    }
]