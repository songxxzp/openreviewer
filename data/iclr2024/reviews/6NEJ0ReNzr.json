[
    {
        "id": "9gHOjlkF8W",
        "forum": "6NEJ0ReNzr",
        "replyto": "6NEJ0ReNzr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2736/Reviewer_eZwk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2736/Reviewer_eZwk"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a set of questions related to the input as an intermediate step for long-form QA and summarization tasks. The authors experiment with different ways to generate the intermediate set of questions and show that the model performs better than similar works in the literature in summary quality and attribution/citations in the summary. They further test the generalizability of the model on a new task and comparing it with other LLM pipelines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Within the proposed work the authors have performed comparisons on different variations.\nOn attribution, the proposed work performs better than larger models on new domains."
            },
            "weaknesses": {
                "value": "1. The description in the problem formulation is incomplete. I had some struggles understanding the difference btw abstractive and extractive models. Figure 1 is good, the authors would use the figure to clearly explain the input, output, and how is the blueprint being used/generated.\n2. There is no apple-apples comparison in Table 1. \n- The authors could test their model without Retriever to make it comparable with Narayan et al easily. \n- I think the proposed model can be extended to a setting with QA pairs similar to Narayan et al.\nThese two experiments will help us understand the exact contribution of the difference in blueprint style and the modifications proposed compared to the one in the literature (Narayan et al.)\n3. A simple/traditional baseline for attribution quality will better ground our understanding.\n4. Table 5 with (-blueprint + attributions) will make for better comparison"
            },
            "questions": {
                "value": "No questions"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2736/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2736/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2736/Reviewer_eZwk"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2736/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698704030338,
        "cdate": 1698704030338,
        "tmdate": 1700779168477,
        "mdate": 1700779168477,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2gazB8NtGk",
        "forum": "6NEJ0ReNzr",
        "replyto": "6NEJ0ReNzr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2736/Reviewer_8rJ6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2736/Reviewer_8rJ6"
        ],
        "content": {
            "summary": {
                "value": "This paper explores the attribution capabilities (generating citations) of the plan based methods for long-form Question Answering where plans are framed as an intermediate step of generating sequence of questions aka blueprints. Two kinds of models (extractive and abstractive) are proposed to generate these blueprints and empirical results show improved performance on both AQuAMuSe and ALCE datasets using LongT5 as the base model."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper introduces an interesting approach to incorporating attribution (i.e generating citations) into text generation models through blueprints, which can potentially improve the trustworthiness and quality of generated content.\n- This work also evaluates model performance in transfer settings, demonstrating the robustness and generalization capabilities of the blueprint models."
            },
            "weaknesses": {
                "value": "- The paper is hard to follow, referencing other papers and models without providing detailed contextual information, making it challenging for readers unfamiliar with these works to fully understand the content. \n- It is also not clear what are the main contributions of the model wrt Narayan et al (2023). It seems that the main differences are just the formulation of the blueprints as a set of questions rather than the Question-Answer pairs and re-ranking of passages before generation. It would help to clearly specify this in the beginning of the text. \n- It is not clear if the passages used for grounding are retrieved or just re-ranked from the given 10 passages for each instance.  \n- Table 1 suddenly shows ablations with attribution while it is not explicitly explained in the earlier sections. Does the attribution here refer to \u201cgenerating text with citations\u201d only?  Was this originally not part of the task formulation? If not, then do the authors post-process the generated summary to remove the citations while performing evaluation? It would help to clarify this in the starting of the text. \n- It is not clear as to how extractive models learn to copy. Is there any utilization of the forced copy mechanism? The authors mention that a question generation model is not available at transfer time to ALCE dataset. It is not clear why this is not the case for an abstractive model? \n- Pictorial depiction of the models could aid in understanding the proposed approach. \n- It seems the relevant baselines haven\u2019t been used like GPT-3.5/4 or open-sourced LLMs like LLaMA-2 where the field has evolved a lot in recent years. \n- Human evaluation is not provided. Reliability of automatic eval metrics for NLG is a known issue in the community. \n- Table 5 shows consistent decreased performance of the proposed models for correctness compared to the baselines (especially on QAMPARI and ELI5 datasets). It indicates that the model learns to hallucinate, producing more citations but incorrect results. \n\n\nEven though the results are promising at the initial level, the paper can be improved in terms of writing clarity and structure. It would be interesting to see a revised version of the paper in future conferences."
            },
            "questions": {
                "value": "- From Table 1, it seems that vanilla seq2seq (line 1) performs better than all the models of Narayan et al. which uses blueprints. This seems to oppose the earlier observations of Narayan et al. Do the authors have an intuition behind this? Also how would this model differ from the base LongT5 model of Narayan et al?  \n- Are the number of contextual passages 10 for all the query examples? It would be nice to see an ablation as to how the models perform with increasing complexity of the context. \n- Could the authors comment on the individual performance of Question Generator and Answerability classifier? What is the overall accuracy of the classifier? How many generated questions were actually determined to be answerable by the classifier? \n- In section 5.2, the authors note that \u201cThe plot reveals that the extractive model generates the most abstractive responses\u201d. Do authors have an intuition of this counterintuitive observation? \n- It is mentioned that blueprint plans improve attribution quality. Could the authors quantify this? It is hard to distinguish between attribution and blueprint. \n- Fig 1 shows the same summaries for the baseline as well as extractive, abstractive models which seem to be scripted and less informative. Actual outputs of different models would be interesting.  \n\n\nSuggestions/Comments:\nPage 5, Section 3.2:  sentencss -> sentences"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2736/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698799953812,
        "cdate": 1698799953812,
        "tmdate": 1699636216274,
        "mdate": 1699636216274,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DDVHl51ChD",
        "forum": "6NEJ0ReNzr",
        "replyto": "6NEJ0ReNzr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2736/Reviewer_LX3E"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2736/Reviewer_LX3E"
        ],
        "content": {
            "summary": {
                "value": "This paper develops and motivates a method for long-form question answering with citations using a variant of the blueprint method of Narayan et al. 2023. In the present paper, blueprint models take as input a query and a set of retrieved passages for that query, and then the model predicts a blueprint (sequence of quetions) and a long-for response to the questions (with citations) in a single decoding step. The paper reports on trained blueprint models that do extractive answering and abstractive answering. The main experiments are with the AQuAMuSe dataset, and both extractive and abstractive blueprint models perform extremely well. The models also perform exceptionally well at the citation step, and a separate set of experiments on the ALCE dataset show that the AQuAMuSe-trained models are competitive with ChatGPT and other LLMs."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "This is an interesting paper that addresses a really important and challenging problem -- answering complex questions with citations. The citation piece is an excellent step towards enabling people to verify for themselves what LLMs seem to be telling them.\n\nThe paper itself is easy to read, and the experimental results are rich and interesting."
            },
            "weaknesses": {
                "value": "I don't have weaknesses to point out per se. I am supportive of publishing the paper."
            },
            "questions": {
                "value": "The ALCE experiments are interesting and informative. It looks like a trained blueprint model is effective here. My question is about whether the blueprint strategy might be effective with pure in-context learning, with a model like GPT-3.5. This would be similar to SelfAsk, I suppose. My apologies of this was already done; the description at the bottom of page 8 is very high-level and so I can't really tell what the prompting strategies were here."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2736/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698806394087,
        "cdate": 1698806394087,
        "tmdate": 1699636216204,
        "mdate": 1699636216204,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NiAlODLMyB",
        "forum": "6NEJ0ReNzr",
        "replyto": "6NEJ0ReNzr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2736/Reviewer_vp7e"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2736/Reviewer_vp7e"
        ],
        "content": {
            "summary": {
                "value": "This paper studies query-based summarization with citations. The authors propose to create blueprints - generating a set of questions to guide the generation of summaries with citations. The authors propose two methods for blueprint generation: the abstractive model and the extractive model. \n\nExperimental results demonstrate the effectiveness of the proposed framework on both the summary quality and the citation quality - the blueprint generation and the attribution generation mutually enhance each other as well as the summary generation quality. \n\nThe authors also conduct experiments on the ALCE benchmark. The proposed method achieved better performance for citation generation while falling behind for correctness. Since the ALCE benchmark does not have training data, the proposed method cannot surpass the LLM-based in-context learning methods in terms of correctness, but surpass citation generation since it's explicitly fine-tuned."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposed method is decent and achieves good performance, and the proposed abstraction and extractive modules are demonstrated performant."
            },
            "weaknesses": {
                "value": "1. The major concern is the lack of novelty in the proposed method. The question generation modules are not new. \n2. In the experiments on the ALCE benchmark, the correctness score falls far behind the LLM-based few-shot prompting methods. \n3. The writing of the paper should be significantly improved. There are too many redundant narratives but less focus on the overall technical contribution."
            },
            "questions": {
                "value": "Are you going to release the code for better reproducibility?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2736/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2736/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2736/Reviewer_vp7e"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2736/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699152094503,
        "cdate": 1699152094503,
        "tmdate": 1700777117407,
        "mdate": 1700777117407,
        "license": "CC BY 4.0",
        "version": 2
    }
]