[
    {
        "id": "8OPx2cjIB7",
        "forum": "vMBNUCZ4cS",
        "replyto": "vMBNUCZ4cS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8114/Reviewer_UHJu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8114/Reviewer_UHJu"
        ],
        "content": {
            "summary": {
                "value": "This paper presents explicit formulas for computing NNGP and NTK for some GNN variants including those with skip-concatenation and attention mechanism in node classification/regression tasks. Experimental results show kernel ridge regression w.r.t. these kernels has competitive performance on some node classification and regression tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The closed form expression for NTK and NNGP for GNNs with skip-concatenation and attentions seems interesting and potentially useful for understanding the efficacy of different GNN architectures.\n2. The structure of the paper is clear and easy to follow."
            },
            "weaknesses": {
                "value": "1. The novelty and technical contribution is limited. GNTK has been studied in graph-level tasks, and also extended to node-level ones in some published papers that are neglected in this work, e.g. [1-2] (though they did not explicitly claim that as a contribution). While I understand considering other architectures might be novel, regrettably there is no discussions on how they might help theoretical understandings or become practically useful (incorporating which can greatly strengthen the paper).\n2. Theorem 3.2 seems incorrect. $A^\\top$ should be outside the bracket in $\\mathbf \\Theta$ otherwise the kernel matrix is not symmetric and thus invalid. (This could be a typo rather than technical flaw.)\n3. The method seems not scalable or efficient, though it is a common limitation for applying NTKs of other types of neural networks. PS, the datasets in this paper are not \u201clarge\u201d (or even \u201cmedium\u201d) as has been claimed in 4.1. For large datasets, the authors can consider those in [3]. \n4. For experiments, the dataset split is not standard, and the standard deviation is not reported. Whether the results are statistically significant is unknown. \n5. Many formatting issues and nonstandard notations impair the readability. To name a few: abuse of capital letters in writing; unnumbered equations; wrap lines in theorems (3.2, 3.3, 3.5); precision is not consistent for Crocodile dataset. Many expressions throughout the paper are also informal for an academic paper.\n\n[1] Graph Neural Networks are Inherently Good Generalizers: Insights by Bridging GNNs and MLPs, in ICLR 2023\n[2] Graph Neural Tangent Kernel: Convergence on Large Graphs, in ICML 2023\n[3] Open Graph Benchmark: Datasets for Machine Learning on Graphs, in NeurIPS 2020"
            },
            "questions": {
                "value": "1. Are there any theoretical insights we can draw from the explicit formula of NTK for GNNs with skip-concatenation and attentions about, e.g. why these architectures work?\n2. Why GAT\\*GP and GAT\\*NTK perform worse than other kernels?\n3. How exactly is the efficiency of KRR-based methods compared with standard GNNs?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8114/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8114/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8114/Reviewer_UHJu"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8114/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698130828825,
        "cdate": 1698130828825,
        "tmdate": 1700664998077,
        "mdate": 1700664998077,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "c1QFtlFPNt",
        "forum": "vMBNUCZ4cS",
        "replyto": "vMBNUCZ4cS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8114/Reviewer_V34A"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8114/Reviewer_V34A"
        ],
        "content": {
            "summary": {
                "value": "In principle, the main contribution of this paper is to revisit the theory on expressive function space generated by several typical graph neural network architectures when the width of layer features goes infinity. The research question was partially answered in the previous research where the answers are not entirely correct."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Identify the issue in the previous literature such as Sabanayagam et al 2022 and show its incorrectness by looking at the special cases (without graph structure).\n\n2. Clearly provide and prove the new conclusion on the graph neural tangent kernels for a number of widely used graph neural network architectures."
            },
            "weaknesses": {
                "value": "Although the main conclusions are correct (based on my reading), the paper does show certain weakness in its current format. For example, the notation used sometimes is quite confusing.  Here is an example, in the two lines before equation (1), the authors use $X\\in \\mathbb{R}^{d_0\\times n}$ and $X\\in \\mathbb{R}^{n\\times d_0}$, also a typo in (1) where shapes do not match with each other.    I also note that the paper on this page of openreview is different from that in pdf version.\n\nThe paper has been greatly improved.  I believe it satisfies the publication standard."
            },
            "questions": {
                "value": "Nil"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8114/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8114/Reviewer_V34A",
                    "ICLR.cc/2024/Conference/Submission8114/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8114/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698535765614,
        "cdate": 1698535765614,
        "tmdate": 1700686758704,
        "mdate": 1700686758704,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "21CodcIOcC",
        "forum": "vMBNUCZ4cS",
        "replyto": "vMBNUCZ4cS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8114/Reviewer_s1WM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8114/Reviewer_s1WM"
        ],
        "content": {
            "summary": {
                "value": "The paper builds Kernels (neural tangent kernels) and Gaussian Processes for graph neural networks in the regime of infinite width networks. The Kernel and Gaussian Process closed forms are derived for a variety of architectures, namely the standard Graph Neural Network, the Graph Neural Network with Skip-Concatenate Connections and the Graph Attention Neural Network. The proposed methods are evaluated on transductive node classification and regression."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper builds on prior work on the analysis of graph neural networks in the infinite width regime and extends the current theory to the Skip-Concatenate Graph Neural Network and the Graph Attention Neural Network. Close form solutions are developed, which is a step forward to the development of the underlying theory.\n\nThe derived Gaussian Processes (GNNGP) and the Kernels (GNTK) are evaluated on six node classification/regression datasets. The results show that the new models perform better than the previous models that do not consider infinite width."
            },
            "weaknesses": {
                "value": "The new theory is with respect to GAT and skip-concatenate models. As the experimental results are showing, they are not the best models. And the theory for the nest model, GCN, is already developed. This lessens the impact of the work.\n\nI would have liked to see an analysis of the results. Why is GNTK consistently doing better than the other models, especially with respect to GNN?\n\nI would have liked to see the experimental results on the kernels developed by others (Du et al and Niu et al)."
            },
            "questions": {
                "value": "Please see the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8114/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698959938861,
        "cdate": 1698959938861,
        "tmdate": 1699637004772,
        "mdate": 1699637004772,
        "license": "CC BY 4.0",
        "version": 2
    }
]