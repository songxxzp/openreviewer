[
    {
        "id": "BbNsbmFARo",
        "forum": "qiduMcw3CU",
        "replyto": "qiduMcw3CU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3429/Reviewer_38Qb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3429/Reviewer_38Qb"
        ],
        "content": {
            "summary": {
                "value": "This work proposes skill machines to encode skill primitives of all high-level sub-tasks. Then zero-shot and few-shot learning is used to help train a policy which aims to satisfy specifications composing sub-tasks temporally and logically."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Proposed a principled way to transfer learned task policies to related temporal logic specification.\n2. Conducted thorough experiments to validate the approach."
            },
            "weaknesses": {
                "value": "1. The definition of (linear) temporal logic is not provided.\n\n2. I would suggest the authors to briefly mention how to convert LTL to reward machines, e.g., through buchi automata, to make it more clear.\n\n3. This work only considers some simple temporal operations such as \"until\" and \"eventually\", but no continuous time interval. Extension to other temporal logic like signal temporal logic (STL) can be more interesting and useful practically -- also more challenging for sure.\n\n4. There are some related work to compare. For instance, I find the following two papers relevant to this paper, the authors are encouraged to compare with them.\n[1]. Luo, X. and Zavlanos, M.M., 2019, December. Transfer planning for temporal logic tasks. In 2019 IEEE 58th Conference on Decision and Control (CDC) (pp. 5306-5311). IEEE.\n[2]. Le\u00f3n, B.G., Shanahan, M. and Belardinelli, F., Systematic Generalisation of Temporal Tasks through Deep Reinforcement Learning."
            },
            "questions": {
                "value": "1. In Sec 2.2, the authors say \"Consider the multitask setting, where all tasks share the same state and action space, transition\ndynamics and discount factor, but differ in reward function. The agent is required to reach a set of desired terminal states in some goal space G \u2286 S.\" Does this mean all subtasks can be represented as reaching a some goal states? If yes, this should be remarked more clearly."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3429/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698635912755,
        "cdate": 1698635912755,
        "tmdate": 1699636294811,
        "mdate": 1699636294811,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BrYRvfiKxT",
        "forum": "qiduMcw3CU",
        "replyto": "qiduMcw3CU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3429/Reviewer_pDWg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3429/Reviewer_pDWg"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces skill machines, which are finite state machines derived from reward machines, enabling agents to tackle complex tasks involving temporal and spatial composition. These skills are encoded in a specialized goal-oriented value function. By combining these learned skills with the value functions, downstream tasks can be solved without additional learning. Importantly, this method guarantees that the resulting policy aligns with the logical task specification. The behavior generated is provably satisficing, with empirical evidence indicating performance close to optimality. The paper suggests that further fine-tuning can enhance sample efficiency if optimal performance is required."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper showcases the development of an agent with the ability to flexibly compose skills both logically and temporally to provably achieve specifications in the regular fragment of linear temporal logic (LTL). This approach empowers the agent to sequence and order goal-reaching skills to satisfy temporal logic specifications. Zero-shot generalization is important in this context is important due to the vastness of the task space associated with LTL objectives,  rendering training across every conceivable scenario intractable."
            },
            "weaknesses": {
                "value": "The paper raises concerns about the optimality of the policy resulting from composition, as the task planning algorithm lacks consideration for the cost of sub-tasks within a skill machine. Consequently, the generated behavior can be suboptimal due to this oversight. Although the paper presents a method for few-shot learning to create new skills, it remains unclear why learning a new skill is preferred over recomposing existing ones, raising questions about the approach's efficiency. Furthermore, it does not seem a learned new skill in few-shot learning is consistent with Definition 3.1.\n\nAn empirical comparison between the paper's few-shot learning strategy and the Logical Options Framework (LQF) baseline is lacking. The LQF baseline employs a meta-policy for choosing options to achieve subgoals within the finite state automaton representation of an LTL property. This approach integrates value iteration over the product of the finite automaton and the environment, ensuring that options can be recombined to fulfill new tasks without necessitating the learning of entirely new options. \n\nFurthermore, the absence of a comparison with LTL2Action in the continuous safety gym environment raises questions. The paper concludes that the zero-shot agent's performance is nearly optimal. It is crucial to assess this claim by comparing the zero-shot agent's performance in terms of discounted rewards with that of the LTL2Action agent in the safety gym environment.\n\nThere is room for improvement in the execution of the paper, particularly in the clarity of its formalization. Following the formalization proved to be challenging, and below, I have outlined my specific concerns regarding this aspect.\n\nBrandon Araki, Xiao Li, Kiran Vodrahalli, Jonathan A. DeCastro, Micah J. Fry, and Daniela Rus. The logical\noptions framework. CoRR, abs/2102.12571, 2021"
            },
            "questions": {
                "value": "* In Sec 2.2, $\\pi^\\ast(s) \\in arg max_a max_g \\bar{Q^\\ast}(s, g, a)$.  I would appreciate clarification regarding the reward function associated with the policy $\\pi^\\ast$. Does this imply that the policy is considered successful whenever it reaches any goal $g$ within the goal space?\n\n* In Definition 3.1, what is the purpose of tracking $c$ as a history of safety violation to constraints in $\\mathcal{C}$?\n\n* In the work by Nangue Tasse et al. (2020), the reward function for a skill specifies that if the agent enters a terminal state corresponding to a different task, it should incur the most substantial penalty possible. I am curious why this aspect is omitted in Definition 3.1.\n\n* At state $u$ in a skill machine, it computes a skill $\\delta Q(u)(\\langle s, c\\rangle,\\langle a, 0 \\rangle) \\mapsto max_g \n\\bar{Q}^\\ast_u (\\langle s, c \\rangle, g, \\langle a, 0 \\rangle)$ that an agent can\nuse to take an action $a$.  Is it necessary for this skill to enumerate the entire goal space? At every state within the skill machine, you have already derived a Boolean combination of primitive Q functions concerning specific goals. For instance, in $u_0$ from Fig. 1, the relevant goals include the green button and the blue region. Could we potentially use the green button directly as the goal for Q_button and the blue region as the goal for Q_blue, thereby bypassing the need to exhaustively enumerate all possible goals?\n\n* Relatedly, in the scenario where the goal space is continuous, how does the algorithm determine the optimal value for $g$?\n\n* As per Definition 3.1, each skill is associated with a sub-goal. However, in the context of few-shot learning, what constitutes the goal for a newly acquired skill?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3429/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3429/Reviewer_pDWg",
                    "ICLR.cc/2024/Conference/Submission3429/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3429/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698728284351,
        "cdate": 1698728284351,
        "tmdate": 1700684791585,
        "mdate": 1700684791585,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qcIOVR7BiW",
        "forum": "qiduMcw3CU",
        "replyto": "qiduMcw3CU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3429/Reviewer_vdA3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3429/Reviewer_vdA3"
        ],
        "content": {
            "summary": {
                "value": "This paper points out that a large number of combinations of high-level goals can lead to the curse of dimensionality. To address this issue, this paper presents skill machines, which flexibly compose a set of skill primitives both logically and temporally to obtain near-optimal behaviors. This paper demonstrates that skill machines can map complex temporal logic task specifications to near-optimal behaviors, which are sufficient for solving subsequent tasks without further learning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The motivation for this paper is quite sound. The agent begins by converting the LTL task specification into a reward machine (RM) and proceeds to determine the suitable spatial skill for each temporal node through value iteration. It also composes its skill primitives into spatial skills, effectively forming a skill machine. Finally, the agent applies these skills to solve the task without requiring additional learning."
            },
            "weaknesses": {
                "value": "One weakness of the paper lies in its relatively complex non-end-to-end training process. More precisely, it necessitates reinforcement learning training for acquiring skill primitives and additional value iteration for the selection of appropriate spatial skills. Another potential limitation is its applicability, which may be more suited to navigation scenarios. It requires different linear temporal logic (LTL) task specifications tailored to specific application scenarios, potentially leading to challenges in generalization across various scenarios. What\u2019s more, the descriptions of definitions 3.1 and 3.2 are not clear enough and lack some corresponding explanations."
            },
            "questions": {
                "value": "1. How are different tasks specifically set in each benchmark? Why does the agent achieve zero-shot performance?\n\n2. The policy generated by Skill Machine (SM) may be locally optimal, so Theorem 3.4 assumes global reachability and the policy is satisficing. There may be enough combinations of skills to approach this assumption, but it will limit the agent's exploration ability. And what is the approximate performance gap when certain states are unreachable?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3429/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3429/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3429/Reviewer_vdA3"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3429/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698826868649,
        "cdate": 1698826868649,
        "tmdate": 1700701813182,
        "mdate": 1700701813182,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1opaO6tCk8",
        "forum": "qiduMcw3CU",
        "replyto": "qiduMcw3CU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3429/Reviewer_uyfa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3429/Reviewer_uyfa"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces the skill machines formalism. This is a variant of the reward machine formalisms where the agent learns Q-functions to satisfy individual task propositions, and then plans a temporal sequence of proposition states to be achieved through value iteration over the skill machine. Once the optimal path through the reward machine is found, the authors propose to use logical Q-function approximation from prior work to initialize the global semi-markov decision process Q-function. The initial policy can serve as a good zero-shot approximation to a satisficing policy. While the Q-function can also be optimized through exploration to generate a hierarchically optimal solution for the overall problem. \n\nThe key assumption in this problem is that all reward machine transitions are possible from all given reward machine states. Such as assumption is usually satsified when the domain involved proposition encoded as occupying certain regions of the state-space. It is definitely true if the propositions cover non-overlapping regions, but I am less certain if the assumption holds when there are overlapping regions as well. Nevertheless, the authors do clearly state the requisite assumption for the validity of their proposed approach."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "**Originality, and significance**: The paper makes a significant contribution by providing an approach to compose skills both through proposition logic and temporal operators. This is quite a unique capability, and many works have proposed partial solutions for the same. The ideas, and evaluations presented in this work are quite compelling, and are theoretically sound. There are some claims that need to be examined in a greater detail as listed in the weakness and limitations sections, but nonetheless the paper is a significant advance.\n\n**Evaluations**: The authors decision to push the complexity of the task specification must be commended. This evaluation explicitly tests for upward generalization capabilities. This can and should be strengthened by randomly sampling LTL goals from predefined distributions over specifications with a rejection sampling approach for unsatisfiable specifications, however, I am convinced regarding the validity of the approach from the theoretical arguments presented in the paper, and the scrupulous evaluations over varying complexity formulas.\n\n**Comparison to prior LTL-based transfer approaches**: The comparison with prior approaches and the explanations of the key differences in capabilities is much appreciated."
            },
            "weaknesses": {
                "value": "**Environment validity:** The assumption of task satisfiability, and proposition transition reachability are quire strong. It is unclear what the test for whether an environment satisfies the assumptions required by the approach. The authors can ameliorate this by either providing domains where these assumptions always hold (environments where propositions encode visiting specific regions, and there being no overlap between propositions is one such environment), and they can also specifically search for a violating environment specification pair, and demonstrate what their approach outputs in that case. This information will be valuable to practitioners who would like to utilize skill machines.\n\n**Relative expressivity of RMs vs LTL**: I believe that only a fragment of LTL can be translated into a reward machines, and there exist reward machines that cannot be expressed through an LTL formula. I would suggest the authors to appropriately restrict the LTL formula class they allow as input to the model to a fragment of LTL that is known to be expressible as a reward machine. (I believe the obligations segment)"
            },
            "questions": {
                "value": "Please refer to the weakness section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3429/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698885102315,
        "cdate": 1698885102315,
        "tmdate": 1699636294472,
        "mdate": 1699636294472,
        "license": "CC BY 4.0",
        "version": 2
    }
]