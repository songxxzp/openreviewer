[
    {
        "id": "soeIhxPpej",
        "forum": "vngVydDWft",
        "replyto": "vngVydDWft",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8194/Reviewer_pGUF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8194/Reviewer_pGUF"
        ],
        "content": {
            "summary": {
                "value": "The paper is about enhancing the relative representation. Relative representation is determined with dissimilarity measure between target data and anchor that is invariant to angle transformation. The former work of Moschella et al. (2022) uses cosine angle as this dissimilarity, but in this paper, it aggregates other dissmilarity to enhance latent communication. The results of this aggregation is assessed by accuracy of zero-shot classification using stiching models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper gives evidences that why the relative representation only using cosine angle can be inappropriate."
            },
            "weaknesses": {
                "value": "1. The definition of the RR framework is strange. It is stated that RR is concatenation of $d(z, a_i)$, but $z$ and $a_i$ should be in different domain $(\\mathcal{Z},$ and $\\mathcal{X})$. I am assuming that the anchors are also encoded with $E_\\theta$ so that $a_i$'s in the latent space $\\mathcal{Z}$\n\n2. The experiments setting in the section 4 is unclear. I am having trouble figuring out what is a stiching model for this downstream task and how it is trained. I am assuming it is the same definition as the stiching model defined in Moschella et al. (2022), but I am having trouble how the decoder for this down-stream task is (pre-)tained.\n\n3. The enhancement of relative representation through aggregating is not convincing for me. In Table 2, the aggreagated accuracies closely matches with using $L_1$ encoder. Using MLP or SelfAttention in aggreagation does not seems to be fair in that it requires an addtional training to get the additional parameters for these layers (correct me if I am wrong.)"
            },
            "questions": {
                "value": "1. In experiment in Section 4.3 ~ 4.4, is the MLP and self-attention aggregation trained (fine-tuned) in end-to-end fashion? \n\n2. Does the downstream task with relative representation presented also enhance the performance of other tasks? (e.g. generation)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics review needed"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8194/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8194/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8194/Reviewer_pGUF"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8194/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698599692625,
        "cdate": 1698599692625,
        "tmdate": 1700668627509,
        "mdate": 1700668627509,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9rWxldKVkd",
        "forum": "vngVydDWft",
        "replyto": "vngVydDWft",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8194/Reviewer_8YvH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8194/Reviewer_8YvH"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a product projection mechanism to generalize the framework of relative representation. In particular, the authors incorporate a set of invariances into the representation by constructing a production space of invariances. The findings are intuitive that multiple projections behave differently across different choices on initialization, model architecture, etc. Experimental results proved the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(1) The motivation is well presented of infusing multiple invariances into relative representation.\n\n(2) The explanations and illustrations are mostly clear and intuitive of the manifold assumption and the product projection mechanism."
            },
            "weaknesses": {
                "value": "(1) On Page 5, the result analysis presents the discovery challenges the assumption in Moschella et al. (2022). More explanations are required to make this point clear. Besides, wondering if the experimental results are just a normal fluctuation due to different runs.\n\n(2) On Page 6, the authors used 1280 randomly selected but fixed anchors. This is also a kind of randomness that is not explained away. In fact, for different choice of anchors, the sensitivity is different of the projection and measure function.\n\n(3) On the experiments, the employed datasets and models are in small-scale and probably prone to overfitting issues. Do the analysis conclusions hold for large-scale models such as Stable Diffusion and GPT? There should be large-scale results to support the findings."
            },
            "questions": {
                "value": "No."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8194/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8194/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8194/Reviewer_8YvH"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8194/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698740438593,
        "cdate": 1698740438593,
        "tmdate": 1699637016448,
        "mdate": 1699637016448,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZyzP7Cvpb3",
        "forum": "vngVydDWft",
        "replyto": "vngVydDWft",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8194/Reviewer_NofE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8194/Reviewer_NofE"
        ],
        "content": {
            "summary": {
                "value": "This paper extends the work on Relative Representation by ensembling multiple relative representations obtained by different distances. The combination of four distances cos, Euc, L1, $L_\\infty$ and three ensemble methods concat, sum, and attention are explored in the text. Extensive experiments across text, graph, and vision domains demonstrated that the ensembled version can improve the performance of zero-shot stitching."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Ensembling multiple relative representations is a reasonable idea and it enhances the power of the original cosine relative representation.\n2. The experiments are extensive. There are 28 tables including the appendix.\n3. The writing style is formal."
            },
            "weaknesses": {
                "value": "1. The selection of distances seems arbitrary.\n    - (a) While the Euclidean distance is invariant under the Euclidean isometries and is a reasonable candidate beyond the Cosine distance. What is the rationale of the rest of the distances? Any geometric intuitions?\n    - (b) The Euclidean isometry is a special case of conformal (angle-preserving) map. For experiments that show better performance on Euclidean distances than on Cosine distances, what can we say about the underlying symmetries of the neural representation? Does it mean that that latent space contain less invarinace? I am asking this question because I want to see what extra understanding on neural representations we can get from this new formulation.\n    - (c) Page 2 \"... which, combined, can capture arbitrary complex transformations of the latent space\". It seems an overstatement to claim that the four chosen distances can capture \"arbitrary complex transformations\".\n2. The Assumption in page 3 does not read smoothly. \n    - (a) The equivalence class of encoders is defined as the set of E such that $ \\pi_\\mathcal{M}TE=\\pi_M E, \\forall T\\in\\mathcal{T}$. This definition is confusing. I fail to see why it is an equivalence class. For example, say, $\\mathcal{T}_1$ is scalings, and $\\mathcal{T}_2$ is rotations, and $E$ is a constant mapping to the origin. Does $\\mathcal{T}_1$ and  $\\mathcal{T}_2$ induces two different equivalence classes of transformations? But clearly $E$ belongs to both classes of transformations.\n    - (b) Suppose $\\mathcal{M}$ is a single point. Then $\\pi_{\\mathcal{M}}TE=\\pi_{\\mathcal{M}}E$ for all $E$ and all $T$. This definition does not contain any useful info then.\n3. Page 5 \"it is not possible to connect latent spaces of models with different initializations ...\" It seems that the Pearson correlation for Cosine is higher than 0.94 in the left subfigure of Fig. 3 and higher than 0.8 in the right subfigure. What is the criterion for the statement of \"no connection\"? Any reference for the choice of criterion? I do not see this as \"challenges the assumption in Moschella et al.\".\n4. Please clarify the aggregation used in Tab. 1 to Tab. 3, since there are multiple possibilities.\n5. Sec. 4.4 leads confusion. What is the difference between SelfAttention and SelfAttention + MLP opt? Isn't the SelfAttention trained (finetuned)? If not, what is the exact computation formula for the SelfAttention aggregation? Where is the initial values of the attention weights come from? Also, the numbers in Tab. 5 does not match that of in Tab. 15."
            },
            "questions": {
                "value": "See weakness section.\n\ntypo:\n\n1. page 4, invriances\n2. page 9, fourth row -> fifth row"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8194/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8194/Reviewer_NofE",
                    "ICLR.cc/2024/Conference/Submission8194/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8194/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698835798191,
        "cdate": 1698835798191,
        "tmdate": 1700652345904,
        "mdate": 1700652345904,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "W8uXzoSDcb",
        "forum": "vngVydDWft",
        "replyto": "vngVydDWft",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8194/Reviewer_ay1h"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8194/Reviewer_ay1h"
        ],
        "content": {
            "summary": {
                "value": "This paper expands on Relative Representations by allowing several distances to be combined, which allows incorporating additional invariances in the resulting representation.\n\nThey first present evidence that single distances aren\u2019t sufficient as they are data/model dependent (the original work used Cosine distance), they then explore adding 3 new distances (Euclidean, L1, L_\\infty) in Text, Image and Graph domains.\n\nThis is a rather simple yet very clear and well-executed paper, which brought back Relative Representations to my attention, a nice idea which got washed up in the recent wave of LLM excitement. It might have limited scope, but currently I lean favorably."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper has a clear focus, presents the problem well, and is overall extremely well executed. \n2. It was easy to follow and the extensions to the math were very well brought up.\n3. It explores appropriate choices of distances and aggregations. Good details and interpretations were provided for what one should expect from them (e.g. Table 6 in the Appendix was exactly what I was looking for)\n4. Results are clear and do improve in predictable fashion over baselines."
            },
            "weaknesses": {
                "value": "1. I feel like too much of the paper is spent on presenting evidence for the sub-optimality of single distance relative representations. I did not really understand why that point was made so repeatedly (Figure 1, Figure 3, Figure 4, Appendix Figure 8 and 9), instead of spending more time presenting different *combinations* of distances and their benefits/implications. In effect Table 1 is the first time a clear combination of distances is shown, and it is clearly better than the rest, so I would have wanted more of that.\n2. Equally, as a result, less emphasis and space was spent explaining the results in 4.2, 4.3 and 4.4. I had to go back to the original paper to remember/understand what \u201czero-shot stitching\u201d meant and how it was implemented.\n3. Details were lacking in a few places, for example which aggregation function was used for most of the results. I assume MLP+sum given it performed the best in Section 4.3, but this isn\u2019t spelt out?\n4. Section 4.4 is also lacking in details and could benefit from some improvements, see below.\n5. It is potentially of limited scope, but I would defer to the majority vote to see if that is a blocker or not."
            },
            "questions": {
                "value": "1. Do you really need to spend that much space and energy on presenting the failures of single distance Relative Representations? \n   1. Figure 1, 3 and 4 are all making a similar point, and Section 4.1 does not feel as crucial as its length suggests it.\n   2. I would probably recommend re-balancing this down and using the extra space to expand on the other Results sections.\n   3. I would recommend keeping either 3 or 4 in the main text but not both.\n   4. I am not sure that Figure 1 is the best framing figure to open the paper with, I might prefer to start with Figure 2.\n2. The aggregations functions are presented well in Section 3, but it would have been useful to present implications for the choices of Sum and SelfAttention, in a similar manner to Concat (\u201cgiving to M the structure of a cartesian product space\u201d).\n   1. The Sum aggregation is actually a DeepSet by implementation. I would have liked having this spelt out explicitly and discussed?\n3. The choice of Anchor points A_X and their implications on the invariances or properties of the relative representations are not discussed.\n   1. Section 4.2 mentions using 1280 randomly selected fixed anchors. Did you try changing it? Does it affect distances differently?\n4. I could not find which aggregation function was used for results in Table 1, 2 and 3. This should be specified clearly.\n5. It feels like showing other combinations of distances (instead of \u201csingle\u201d vs \u201call\u201d) would have been helpful, especially if different domains require different distances.\n   1. Section 4.4 tries to go in that direction, but the Transformer aggregation is not the best one and combined with my issue 4, I wasn\u2019t sure what you used, so it muddles the results.\n6. Section 4.4 would benefit from being extended, I do not think it contains enough details currently.\n   1. The experimental setup needs more details, there is no description of the transformer aggregation anywhere I could find.\n   2. Table 5 should contain the value for the best other aggregation (e.g. MLP+sum?), as currently it makes it harder to see if QKV opt is sufficiently accurate or not.\n   3. It is unfortunate that the Transformer aggregation performs poorly. It would be good to bring the MLP+Transformer one to the main text, or at least present more clearly what model is used. It is not my expectation that a DeepSet should outperform a Transformer if it has enough layers?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8194/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698858607788,
        "cdate": 1698858607788,
        "tmdate": 1699637016185,
        "mdate": 1699637016185,
        "license": "CC BY 4.0",
        "version": 2
    }
]