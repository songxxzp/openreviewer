[
    {
        "id": "rk3BWzn1HG",
        "forum": "ftKqt3Di3H",
        "replyto": "ftKqt3Di3H",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4428/Reviewer_PazT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4428/Reviewer_PazT"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the challenges in Federated Learning (FL) for NLP tasks, specifically the complications arising from using GANs and auxiliary data. By leveraging the embedding structure of Transformers, the authors propose a novel method to generate pseudo data inspired by soft prompts. This approach sidesteps the need for GANs, reduces computational overhead, and outperforms auxiliary data methods on the SuperGLUE Benchmark."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "This paper has a clear presentation."
            },
            "weaknesses": {
                "value": "* **Motivation.** The motivation of this paper did not convince me. It seems that the target problem is ambiguous and meaningless. The authors seem to just make a minor modification to replace GAN in FL's knowledge distillation for NLP tasks and it lacks motivations and scenarios. GAN is actually rarely used in NLP and NLP is also less studied in FL before. Not using GAN in NLP is trivial and common, which cannot be the main motivation. An appropriate motivation is the problems raised in actual scenarios and previous works, not the \"a + b\" pattern. Also, the authors think GAN will leak privacy and the proposed method can protect privacy, but the authors didn't provide evidence to support that point.\n* **Novelty.** I think the proposed method is not novel. First, knowledge distillation is not a novel thing in FL. Second, such a design in Transformers is also not novel. \n* **Baselines.** The authors missed some important baselines in the experimental part, which weakens the validity of the proposed method. Specifically, the authors should compare the following methods in the experiments: [1] [2] [3].\n\n----\n\n[1] Zhang L, Shen L, Ding L, et al. Fine-tuning global model via data-free knowledge distillation for non-iid federated learning[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022: 10174-10183.\n\n[2] Zhu Z, Hong J, Zhou J. Data-free knowledge distillation for heterogeneous federated learning[C]//International conference on machine learning. PMLR, 2021: 12878-12889.\n\n[3] Wang H, Li Y, Xu W, et al. DaFKD: Domain-aware Federated Knowledge Distillation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 20412-20421."
            },
            "questions": {
                "value": "See the weakness above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4428/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698231351349,
        "cdate": 1698231351349,
        "tmdate": 1699636417439,
        "mdate": 1699636417439,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lBImfAbSQH",
        "forum": "ftKqt3Di3H",
        "replyto": "ftKqt3Di3H",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4428/Reviewer_hSV1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4428/Reviewer_hSV1"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a lightweight approach for knowledge distillation in federated learning (FL), particularly in the context of Transformer models. The authors address the challenges posed by Generative Adversarial Networks (GANs) and auxiliary data in FL by sampling from the embedding structure of Transformers and learning a set of pseudo data for the distillation process. This approach, called FedDRS, draws inspiration from the concept of soft prompts and does not require GANs or auxiliary data. It incurs no communication overhead and yields improved model performance with relatively lower computational costs on the server side.\n\nThe authors propose three methods for sampling from embeddings: random sampling, target sampling, and adversary sampling. They demonstrate that their approach outperforms methods relying on auxiliary data on complex NLP tasks such as the SuperGLUE Benchmark. The paper also presents ablation experiments that elucidate the unique advantages of models equipped with embeddings over those without embeddings, showcasing the efficiency and quality of sampling in embedding-enhanced models.\n\nIn summary, the paper introduces a novel text-free approach for knowledge distillation in federated learning, specifically for Transformer models. The proposed FedDRS method addresses the challenges posed by GANs and auxiliary data and yields improved model performance with lower computational costs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "### **Originality:**\n\nThe paper presents a novel approach for knowledge distillation in federated learning, particularly focusing on Transformer models. The proposed FedDRS method is unique in its text-free approach, which samples from the embedding structure of Transformers and learns pseudo data for the distillation process. This approach addresses the challenges posed by GANs and auxiliary data in FL, offering a creative combination of existing ideas.\n\n### **Quality:**\n\nThe paper is well-written and provides a clear explanation of the proposed method. The authors demonstrate the effectiveness of FedDRS through experiments on the SuperGLUE benchmark, showing improved performance compared to methods relying on auxiliary data. The paper also includes ablation studies that elucidate the advantages of models equipped with embeddings.\n\n### **Clarity:**\n\nThe paper is well-organized and presents its ideas in a clear and coherent manner. The authors provide a thorough explanation of the proposed method, its components, and the experimental setup. The results are presented in a clear and concise manner, making it easy for readers to understand the contributions of the paper.\n\n### **Significance:**\n\nThe proposed FedDRS method addresses an important problem in federated learning, particularly in the context of Transformer models. By offering a lightweight approach that does not require GANs or auxiliary data, the method has the potential to advance the field of federated learning and improve the performance of Transformer models in FL settings. The paper also contributes to the understanding of the challenges posed by GANs and auxiliary data in FL, providing valuable insights for future research.\n\nOverall, the paper presents a novel and creative approach to knowledge distillation in federated learning, focusing on Transformer models. The proposed FedDRS method demonstrates improved performance compared to existing methods and addresses the challenges posed by GANs and auxiliary data. The paper is well-written clear, and significantly contributes to the field of federated learning."
            },
            "weaknesses": {
                "value": "1. Privacy concerns (important): The paper does not address the potential privacy concerns arising from sampling from the model. Incorporating privacy-preserving measures, such as differential privacy, could help ensure the privacy of the pseudo-samples and enhance the overall robustness of the proposed method. \n\n2. Limited exploration of sampling methods: The paper focuses on three sampling methods (random, target, and adversary sampling) but does not explore other potential sampling strategies. Investigating alternative sampling techniques could lead to further improvements in the performance of the proposed method. \n\n3. Limited exploration of model architectures: The paper focuses on two Transformer models (RoBERTa and T5) but does not explore other popular Transformer architectures, such as BERT or GPT. Investigating the performance of the proposed method on a broader range of Transformer models could provide more insights into its applicability and effectiveness. \n\n4. The illustration of Figure 1 seems chaotic."
            },
            "questions": {
                "value": "1. Although the authors mentioned about this weakness in the conclusion, it still requires some interpretation of how likely a generative model could leak private data. Therefore, I suggest authors add text inference attack experiments to show this risk. \n2. In Table 3, I am curious about the performance of Fedavg + random sample + adv. sample. I suspect that the improvement of including a target sample in MixSample is negelactble."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4428/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698579615641,
        "cdate": 1698579615641,
        "tmdate": 1699636417366,
        "mdate": 1699636417366,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "crB3U8crJE",
        "forum": "ftKqt3Di3H",
        "replyto": "ftKqt3Di3H",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4428/Reviewer_DJtk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4428/Reviewer_DJtk"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a lightweight approach for knowledge distillation in federated learning without using GANs or auxiliary data. The approach samples from the embedding structure of Transformers and learns a set of pseudo data for the distillation process, resulting in improved model performance with relatively lower computational cost. The paper suggests that this approach can be applied to other large-scale NLP tasks beyond Transformers."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The approach does not require GANs or auxiliary data, incurs no communication overhead, and yields improved model performance with relatively lower computational costs on the server side.\n* The experiments conducted in the paper show that the proposed approach yields superior results compared to methods that rely on auxiliary data on complex NLP tasks such as the SuperGLUE Benchmark."
            },
            "weaknesses": {
                "value": "* The challenge addressed in this paper may not be comprehensive. Although some papers utilize GANs to generate data for model distillation, it's important to note that GANs are not the sole method for data generation. Therefore, the scope of this paper appears to be limited.\n* The assertion that \"The GANs approach faces numerous challenges in recent popular large-scale Transformer-based NLP tasks\" prompts the question: Were the models employed in the experiments considered large-scale?\n* This paper does not specifically address the challenges associated with GAN-based methods for Federated Learning (FL) in its experimental section.\n* Is this method applicable to other NLP tasks aside from text classification?"
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4428/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4428/Reviewer_DJtk",
                    "ICLR.cc/2024/Conference/Submission4428/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4428/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698762428464,
        "cdate": 1698762428464,
        "tmdate": 1700663226752,
        "mdate": 1700663226752,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BdLVF9ZicC",
        "forum": "ftKqt3Di3H",
        "replyto": "ftKqt3Di3H",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4428/Reviewer_PY1Q"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4428/Reviewer_PY1Q"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the author propose a method to sample the embedding layer of transformer models and use for knowledge distillation in Federated Learning. The paper provides a good motivation to come-up with privacy preserving methods for knowledge distillation and identifies the gaps in GAN based methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper provides an interesting method to sample the embeddings of the transformer models for knowledge distillation in federated learning and thereby reducing the communication overhead and improving the accuracy."
            },
            "weaknesses": {
                "value": "The paper lack some important details about the proposed method and hence very difficult to read. In the abstract, it is mentioned, \"This lightweight approach does not require GANs or auxiliary data, incurs no communication overhead, and yields improved model performance with relatively lower computational costs on the server side.\". However, I don't see any discussion of the saving in communication cost later in the paper. Since the difference in accuracy is quite moderate as compared to FedAUX for various values of \\alpha in Dirichlet distribution, we need to see what's the saving in communication cost and trade-off with additional computation cost at server. \n\nFurther, in the Ablation study, it's not clear that what numbers in Table 1 should be compared with the accuracy numbers given in Table 3.\n\nWhy do we see decaying performance difference between FedDRS and other techniques in Table 1 with increasing value of \\alpha?"
            },
            "questions": {
                "value": "please see above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4428/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699238950731,
        "cdate": 1699238950731,
        "tmdate": 1699636417209,
        "mdate": 1699636417209,
        "license": "CC BY 4.0",
        "version": 2
    }
]