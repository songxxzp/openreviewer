[
    {
        "id": "0oh4tzfgBH",
        "forum": "ic1Z7Qe9xH",
        "replyto": "ic1Z7Qe9xH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5256/Reviewer_fgMF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5256/Reviewer_fgMF"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces DynPipe, a system to support LLM training tasks with running time dynamics, where such methods include neural network pruning, layer frozen training, etc. A system is implemented to support the load balancing in such training tasks. The balancing policies include centralized partitioning based on parameters and decentralized partitioning based on workloads. Empirical study was conducted to verify the effectiveness and efficiency of the proposed design."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- To build an efficient system to support LLM training tasks with runtime dynamics is interesting research from the system perspective. \n\n- The paper leveraged a production-level cluster for some demonstration of the deployment of the system."
            },
            "weaknesses": {
                "value": "- The paper is pool written and hard to understand:\n\n  - The paper makes some exaggerated statements about its contribution. For example, \"Research on dynamic models will not deliver practical impact unless there is a platform from which those models can be made efficient.\" -- This is untrue from the machine learning algorithm's perspective. As long as such an algorithm shows statistical efficiency or better generalization performance, it has a significant practical impact w/wo a platform/framework, right?\n\n  - The technique session is confusing and not self-explained; for example, in Lemma 1 and 2, the term \"bubble\" is referred to without a formal definition. \n \n  - The experimental section is poorly organized; there is even a lack of formalization of the central hypothesis of the evaluation.   \n\n- There is also some issue w.r.t the baseline selection, Megatron is designed to support standard non-sparse LLM training, DeepSpeed is similar where the additional effort is made for MOE. Those are not strong baselines for such training tasks. As far as I know, systems like PEFT from huggingface include some relevant functionality.  Some more advanced baselines should be considered for evaluation."
            },
            "questions": {
                "value": "Would it be possible to provide an empirical study with state-of-the-art baselines?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5256/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698745170710,
        "cdate": 1698745170710,
        "tmdate": 1699636524467,
        "mdate": 1699636524467,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9p7FREzNhh",
        "forum": "ic1Z7Qe9xH",
        "replyto": "ic1Z7Qe9xH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5256/Reviewer_cgYk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5256/Reviewer_cgYk"
        ],
        "content": {
            "summary": {
                "value": "DYNPIPE is introduced as a tool that enables the exploration of dynamic models, substantially enhancing the end-to-end training efficiency and making their practical application more viable. It operates independently of the underlying pipeline parallelism, pruning, and freezing schemes, ensuring compatibility with various compute optimization and reduction strategies. To mitigate the adverse effects of dynamic models on pipeline utilization, load balancing algorithms are proposed, which are proven to converge to optimal balancing. The framework's benefits are showcased through gradual pruning training and layer freezing scenarios across both single-node and multi-node settings, including a strategy to reduce GPU usage by consolidating work onto fewer GPUs. DYNPIPE demonstrates significant speed improvements over existing solutions, achieving up to 1.3x speedup on a single-node with 8 A100 GPUs, over 2.5x speedup in multi-node settings with up to 720 A100 GPUs, and an average of 2.3x speedup over the state-of-the-art layer freezing solution, all while effectively reducing GPU requirements by half without compromising performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Load balancing algorithms for adaptive equalization of compute workloads among different workers"
            },
            "weaknesses": {
                "value": "Unclear presentation of the solution strategy and how the problem and solution is different from prior works.\nIncomplete analysis because the communication aspects are not discussed."
            },
            "questions": {
                "value": "1) In Figure 1, how is the Imbalanced pipelines in dynamic models lead to additional stalling in data parallelism showed in the figure. It seems like the idleness is not as high as pipeline in each dense level. \n2) In Figure 2, what is P2P layer transfer? As far as pipeline is concerned, what is GPU1 doing when GPU 0 is occupied. For example, in stage, L5 send data to L4 after finishing L1, L2, L3, L5. What is GPU 1 doing? Is it idle?\n3) How is the prediction result influenced by the pruning?\n4) During the measurement of the throughput, how to reflect that profiling time and other pruning time will not influence the total performance of the framework?\n5) Why y-axis in Figure 4 and 5 use token/# GPU. Is it same as token per device? Meanwhile, is the communication bandwidth between GPU sufficient to not influence the experiment?\n6) In figure 4, why is the throughput of the model with different number of GPUs roughly the same. Shouldn\u2019t we expecting the throughput will increase accordingly?\n7) In section 4.1, how is the prune region and iteration being selected. What is the affect of changing the settings?\n8) In the description of Diffusion by Param and Diffusion by Time, how do they iteratively minimize load variances among accelerators? Do they communicate or they select parameters based on the variance of after gradient decent?\n9) In figure 4, why only partition by param is included?\n10) In figure 5(right), it seems that there is not a clear relationship between number of GPU and throughput/# GPUs. Why is that? The goal is the increase the throughput after pruning in order to use less computing resources. Moreover, if we multiply # of GPU with throughput/# GPUs, the total throughput of the system will decrease. Why is that?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5256/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5256/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5256/Reviewer_cgYk"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5256/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698819857011,
        "cdate": 1698819857011,
        "tmdate": 1700007050127,
        "mdate": 1700007050127,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WABU1zkkOM",
        "forum": "ic1Z7Qe9xH",
        "replyto": "ic1Z7Qe9xH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5256/Reviewer_4Zma"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5256/Reviewer_4Zma"
        ],
        "content": {
            "summary": {
                "value": "Given the significant computational and memory costs involved in training Large Language Models (LLMs), recent studies have proposed various techniques to curtail these expenses. These techniques include dynamically pruning or freezing portions of the model. However, these methods can lead to an imbalance in the workloads within pipeline parallelism, as some workers will have fewer tasks after the pruning or freezing processes. To address this issue, the paper proposes a solution that dynamically rebalances workloads among distributed training workers. The results indicate that this approach surpasses static load balancing baselines in achieving higher training throughput."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. **Addressing a Timely Issue**: The paper addresses a timely problem associated with combining pipeline parallelism with dynamic pruning and freezing. The clear motivation behind this problem is well-illustrated in Figure 1.\n\n2. **Enhancing Reproducibility**: The authors have contributed to reproducibility by providing a source code repository. This repository not only allows for easy reproduction of the results but also enables other users to utilize the system to improve training time using dynamic schemes.\n\n3. **Extensive Evaluation**: The paper undertakes a large-scale evaluation with a substantial GPU cluster comprising 720 A100 GPUs. This evaluation substantiates the scalability of the proposed solution."
            },
            "weaknesses": {
                "value": "1. **Limited Novelty**: The paper's technical contribution is limited due to its use of existing techniques in a new context, rather than introducing entirely new concepts. The proposed load balancing solutions seem to involve applications of DeepSpeed's workload partitioning algorithms at the end of each pruning, and the diffusion-based algorithm also employs a similar partitioning strategy, albeit more akin to work stealing.\n\n2. **Lack of Overhead Discussion**: The paper does not adequately discuss the overhead of different strategies, which could have provided an interesting perspective on these solutions. It remains unclear whether the throughput reported in the evaluation takes into account all overheads.\n\n3. **Absence of Key Training Metrics in Evaluation**: The evaluation does not include important training metrics such as time-to-accuracy or learning curves. Even though model accuracy is not the primary focus of the proposed solution, including this information would have added to the completeness of the paper. For instance, it might be the case that improvements in throughput during large-scale training do not translate into time-to-accuracy gains because the pruning/freezing techniques reduce training quality. This potential issue could limit the usefulness of such solutions.\n\n**Minor**: The paper does not adhere to the proper citation format (it should use \\citep instead of \\citet). This oversight can hinder readability and understanding at certain points in the paper."
            },
            "questions": {
                "value": "1. Could the authors provide a detailed discussion on the overheads associated with the different strategies and clarify whether these overheads were factored into the reported throughput in the evaluation?\n\n2. Would it be possible for the authors to include key training metrics, such as time-to-accuracy and learning curves, in the evaluation? This inclusion could provide a more comprehensive understanding of the impact of the proposed solutions on training quality."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5256/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5256/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5256/Reviewer_4Zma"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5256/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699596924594,
        "cdate": 1699596924594,
        "tmdate": 1699636524297,
        "mdate": 1699636524297,
        "license": "CC BY 4.0",
        "version": 2
    }
]