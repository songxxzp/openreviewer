[
    {
        "id": "mhgl0z4oWL",
        "forum": "noe76eRcPC",
        "replyto": "noe76eRcPC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission750/Reviewer_XwFE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission750/Reviewer_XwFE"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel method for lifting a few unposed images directly into a 3D representation as well as estimating the corresponding camera poses. The key contributions are a novel object-centric pose estimation pipeline based on the Perspective-n-Point (PnP) algorithm, scaling up training to achieve strong cross-dataset generalization, and instant estimation of the underlying NeRF (as opposed to training offline via gradient descent). The authors demonstrate impressive results on object-centric scenes across datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "-The motivation for the method is clear - reconstructing object-centric 3D scenes without knowledge of the camera poses.\n-The method is simple (which is a good thing) and straightforwardly scalable.\n-The approach for camera pose estimation is neat, containing an attractive 3D inductive bias and mostly outperforms common object-centric pose estimation methods.\n-Generated results are strong and aesthetically pleasing, with significant cross-dataset generalization demonstrated.\n-Ablations are informative."
            },
            "weaknesses": {
                "value": "-I have a suspicion that the baselines - RelPose++ and Forge - are not trained on the same datasets, but that the authors are rather using the pre-trained methods and just evaluate them on these datasets. Could the authors clarify?\n- One baseline is very clearly missing - the Scene Representation Transformer, Sajjadi et al, which can similarly perform novel view synthesis from unposed images at test time while requiring camera poses at training time. \n- The prior point is even more critical since the authors don't always outperform RelPose++ on Co3D. If RelPose++ wasn't trained at a similar scale, this is concerning.\n- In the evaluation section, the authors state that they evaluate PSNR against the input images. That is questionable - they should evaluate the PSNR against held-out test views?\n- The authors should clarify - already in the introduction and the abstract - that their method still requires ground-truth poses at training time. I.e., it is only pose-free at test time!\n- Why do the authors choose to report results on Co3D in the appendix as opposed to the main paper? I don't see a principled difference here. Just as the other results, this result should be reported in the main paper. The fact that their method requires training on an additional real-world dataset is important and relevant and should not be hidden in the appendix.\n- The authors should demonstrate their method on some real-world captures instead on only synthetic examples.\n\nRelevant references missed:\n- Triplanes for neural fields were first proposed in \"Convolutional Occupancy Networks\", Peng et al. and should be discussed.\n- The paper \"FlowCam\" also incorporates 3D shape information during the camera pose prediction process and should be discussed.\n\nMinor concerns:\n- In all the tables, the best-performing method should be bolded.\n- \"output-performing\" --> outperforming (last paragraph before related work)\n- I would recommend to avoid phrases like \u201cpretty well\u201d - sounds a bit casual."
            },
            "questions": {
                "value": "See weaknesses - in summary, I think this paper could be strong, but if the authors want to claim that their method outperforms prior pose-estimation methods, then they have to provide a fair benchmark and train these other methods on the same datasets and with a comparable level of compute as well, or at least provide a clear ablation study where they uncover that their model scales better with data than prior methods.\n\nIf the authors address this issue, I'm happy to raise my score significantly."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission750/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission750/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission750/Reviewer_XwFE"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission750/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698617823066,
        "cdate": 1698617823066,
        "tmdate": 1700665721245,
        "mdate": 1700665721245,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ogPyPJsh0D",
        "forum": "noe76eRcPC",
        "replyto": "noe76eRcPC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission750/Reviewer_T3us"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission750/Reviewer_T3us"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a 3D reconstruction method that focuses on shape and pose predictions given a sparse set of un-posed input images. The proposed method consists of two major components: 1) a transformer that aggregates DINO-based image tokens from multi-view images and 2) a DSAC-fashion pose estimator that predicts 3D points coordinates from aggregated features followed by a differentiable PnP refinement. While aggregating multi-view features, the transformer also enables shape reconstruction by predicting NeRF parameters in terms of Triplane features."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. The paper is generally well written and easy to follow.\n2. The method looks convincing and the paper is addressing a critical challenge in this field.\n3. The anonymous website provides extra demos."
            },
            "weaknesses": {
                "value": "1. In Figure 3 and Figure 4, I am confused about why the results are comparing input images with their predictions? Shouldn\u2019t it be comparing GT novel view images with rendered novel views, as in Figure 5? Given input images, it\u2019s not surprising that the reconstruction of input images should be almost perfect right?\n2. The first issue leads to further confusion when reading Figure 4. I am not sure I understand correctly, but it seems like pose prediction makes limited difference.\n3. Missing related works. Section 3.3 is about pose predictions and PnP. This approach is closely related to the line of work in *scene coordinate regression* [a][b], which is not mentioned at all in the related work section.\n\n**Reference**\n\n[a] Shotton, Jamie, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, and Andrew Fitzgibbon. \"Scene coordinate regression forests for camera relocalization in RGB-D images.\" In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2930-2937. 2013.\n\n[b] Brachmann, Eric, Alexander Krull, Sebastian Nowozin, Jamie Shotton, Frank Michel, Stefan Gumhold, and Carsten Rother. \"Dsac-differentiable ransac for camera localization.\" In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 6684-6692. 2017."
            },
            "questions": {
                "value": "Refer to points 1 and 2 in the \"Weaknesses\" section.\n\nWhile the paper presents a novel idea and convincing quantitative results, these two points create confusions and prevent me from making a definitive evaluation. Once these issues are resolved, I am willing to increase my evaluation to accept."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission750/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission750/Reviewer_T3us",
                    "ICLR.cc/2024/Conference/Submission750/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission750/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698707548668,
        "cdate": 1698707548668,
        "tmdate": 1700644592432,
        "mdate": 1700644592432,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SvU7Rxy9t9",
        "forum": "noe76eRcPC",
        "replyto": "noe76eRcPC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission750/Reviewer_Pk6q"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission750/Reviewer_Pk6q"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method to reconstruct 3d models from unposed sparse-view images. To enable this goal, the authors use a single-stream transformer to simultaneously process image patch tokens and nerf tokens, resulting in a simultaneously estimation of triplane-nerf and camera poses. The estimation of camera poses utilizes predicted per-view coarse geometry and differentiable pnp. This method shows strong cross-dataset generalization ability and outperforms baselines on various datasets."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. The authors conducted rich experiments to prove the effectiveness of their method. \n2. Reconstructing 3d models from sparse-view images is an important topic in the AIGC era. \n3. Employing differentiable pnp into nerf reconstruction is a good idea to handle camera pose uncentainty."
            },
            "weaknesses": {
                "value": "The authors mainly handled the cases of 2-4 views (e.g. Figure 1). Is there any criteria for the authors to select image views? It seems that the tested camera views have suitable (or informative) relative angles (about 30 degrees to about 120 degrees). What would happen if extreme/degenerated views are provided (e.g. the front and back views that are parallel, or views with small angles, or views with translational displacement only)?   I think explaing this would make the study stronger."
            },
            "questions": {
                "value": "No further questions. I think the paper provides enough  details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission750/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission750/Reviewer_Pk6q",
                    "ICLR.cc/2024/Conference/Submission750/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission750/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698742299085,
        "cdate": 1698742299085,
        "tmdate": 1700651403476,
        "mdate": 1700651403476,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "67tYB7aF03",
        "forum": "noe76eRcPC",
        "replyto": "noe76eRcPC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission750/Reviewer_5PYC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission750/Reviewer_5PYC"
        ],
        "content": {
            "summary": {
                "value": "The paper presents an amortized inference model for a fast NeRF reconstruction of 3D objects from a few source views. These sources are either obtained from a view-aware diffusion model such as zero123 or give a ground truth. The work trains a large transformer model i.e. 0.59B parameter model for fast (1.3s) inference on A100GPUs. The transformer model outputs coarse pointclouds as well as triplanes for differentiable PNP based pose estimation and NeRF reconstruction respectively. The model is trained on Objaverse dataset and shows generalization capability on other datasets such as ABO, GSO, Omni Objects3D etc."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper proposes a technically sound amortized inference approach for 3D object reconstruction in a fast manner from a few unposed images. The strnegths of this paper are as follows:\n\n1. Clearly better quantitative results against relevant pose predictor baselines\n2. Good ablations showing how various factors affect the model performance, especially mask noise\n3. A good downstream application of the method showing text-to-3D results which is an important application for this work"
            },
            "weaknesses": {
                "value": "Although the paper is nicely written and the experiments are good, I have the following questions:\n\n1. How much of the improvement comes from the data? The pipeline is not that novel since all of which the authors have presented have been shown before, even the differentiable PnP solver comes from prior work. I would have liked to see scaling laws of the performance on the increased number of data points.\n\n2. No comparison with sparse NeRF methods[1, 2, 3] is shown. Is this because they require accurate camera poses? I would have liked to see some comparison or discussion with these methods, some of them already show sparse novel view synthesis using triplane formulations[1]\n\n3. Relating to point 1, how much does the network learn training data distribution? Is it possible that due to the massive scale of the objaverse dataset, some of the generalizable evaluation, might be in questions, since we don't know if the network has seen similar data before. Can the authors comment on that with some analysis/discussion?\n\n[1] Irshad et al, NeO 360: Neural Fields for Sparse View Synthesis of Outdoor Scenes\n[2] Niemeyer et al, RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from Sparse Inputs\n[3] Truong et al, SPARF: Neural Radiance Fields from Sparse and Noisy Poses"
            },
            "questions": {
                "value": "Please see my questions in the weakness section above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission750/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission750/Reviewer_5PYC",
                    "ICLR.cc/2024/Conference/Submission750/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission750/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699207337242,
        "cdate": 1699207337242,
        "tmdate": 1700656374045,
        "mdate": 1700656374045,
        "license": "CC BY 4.0",
        "version": 2
    }
]