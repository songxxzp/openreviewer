[
    {
        "id": "I6CScj5bYF",
        "forum": "9w3iw8wDuE",
        "replyto": "9w3iw8wDuE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1097/Reviewer_NrgV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1097/Reviewer_NrgV"
        ],
        "content": {
            "summary": {
                "value": "TTA approaches select good samples for TTA using entropy as a metric (some of the approaches, and not all as I point in my review). However, a sample can have low entropy but using spurious features for prediction. Including such samples for TTA can be hurtful as they use spurious features for low entropy. The authors propose patch shuffle augmentation to identify such samples and remove them. \nThe paper has missing important baseline for some core experiments (Table 2 and Table 3), and it is not clear why the approach helps in WILD distribution shift (label shift, batch size 1)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Strengths:\n- The problem is well motivated i.e. spurious features can cause entropy to be low."
            },
            "weaknesses": {
                "value": "Weakness:\n- Unclear what the authors mean by \u201cdisentangled latent vector\u201d exactly for each image. No proper reference has been made as well to get details. It would be better to write more explicitly how does Equation 6 arise for fluency of the reading flow. Authors mention $x^{T>>C}$, please describe this notation (and might be not even required to introduce this as it is clear authors mean second term is more pronounced).\n- It is not a great practice to include the whole of related works in the appendix. Even the related works section on TTA in appendix is extremely sparse (a single paragraph!) and should be much more broader in it\u2019s scope.\n- I understand that authors aim is to reject samples which use spurious features for prediction as doing TTA on them might be harmful. However, past theoretical work (https://proceedings.neurips.cc/paper/2020/file/f1298750ed09618717f9c10ea8d1d3b0-Paper.pdf) shows that self training avoids using spurious features. \n- There are many methods which assess augmentation + entropy (see MEMO, SENTRY https://arxiv.org/pdf/2012.11460.pdf). SENTRY does sample filtering based on consistency of predictions over various augmentations. It is a crucial and missing baseline in this work, especially in Table 2 and Table 3. I know SENTRY operates in UDA setting, but the consistency over augmentations part is pretty standard. \n- The authors mention existing works use entropy for selective minimization. There should be a baseline where we do selection based on average entropy over multiple augmentations. It is a trivial extension of previous works and should have been a baseline to asses the effectiveness of pseudo-label confidence drop proposed in this work.\n- Why is the MEMO baseline missing in Table 2 and Table 3, where I would guess MEMO to be most effective as it also does entropy average over multiple transformations. It is quite possible that the average entropy over augmentations is low for spurious correlations domainted prediction, making those samples contribute much less to the loss anyways.\n- Can the authors give any reason why their proposed approach should work better under label shift or for batch size 1 as shown in Table 5? I do not see any reason why augmentation based sample selection should help for label shift TTA (makes sense for spurious TTA). Similar is the question for Batch size 1 setting. Are the authors sure they did proper hyperparameter tuning for the baselines?\n- Further, the effect of such spurious correlations can be removed by incorporating such transforms (cutmix, random cropping, or the used patch shuffling) during pretraining itself. Will the proposed approach help in those cases as well?\n\n\nOverall, the paper does consider an important question. However, misses some very important baselines. Further, some of the results on some benchmarks have been left unexplained beyond simply stating the numbers. The writing also lacks heavily, with a complete miss on the related works section.\n\n\n_______post rebuttal_____\nAuthors addressed some of my major concerns with respect to the baselines. I am increasing my rating from 5 to 6."
            },
            "questions": {
                "value": "See the weakness section for questions to answer during rebuttal"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1097/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1097/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1097/Reviewer_NrgV"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1097/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698617142749,
        "cdate": 1698617142749,
        "tmdate": 1700690535865,
        "mdate": 1700690535865,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "R16OniJLoe",
        "forum": "9w3iw8wDuE",
        "replyto": "9w3iw8wDuE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1097/Reviewer_7TAy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1097/Reviewer_7TAy"
        ],
        "content": {
            "summary": {
                "value": "The paper works on test-time adaptation with entropy minimization. While the online updates with entropy minimization can lead to error accumulation, the paper proposed to do sample selection and weighting by the proposed DeYO, which combines entropy and pseudo-label probability difference. Experiments on several datasets with various distribution shifts show the effectiveness of the method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The observation and theoretical demonstration of \"entropy is not enough\" on the spurious correlation shifts is interesting and motivating to the method.\n\n2. The results of the proposed method on several datasets with different distribution shifts are good, demonstrating the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1. The motivating observations and theoretical support are conducted on spurious correlation shifts, which is mainly on the semantic level. Can this also be found and theoretically proof for the other distribution shifts like covariate shifts?\n\n2. In 2.3, the paper theoretically demonstrates that entropy is not enough and it is better to incorporate the CPR factors for sample selection and reweighting in test time adaptation, which is done by the proposed PLPD. However, PLPD is then combined with the common entropy method in the experiments and implementations. Then what role does entropy play in sample selection? and how it helps  PLPD to incorporate the CPR factors?"
            },
            "questions": {
                "value": "1. Did the authors try some other methods like data augmentation methods to replace the transformed one? Will they also work to incorporate CPR factors?\n\n2. How do the thresholds in eq. (9) defined?\n\n3. What are the numbers and sizes of the patches for patch shuffling? Will these also influence the adaptation?\n\n4. Why PLPD and entropy sample selections behave differently for different distribution shifts in Table 6? How to select different methods for different distribution shifts? Is there any theoretical support for this problem?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1097/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1097/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1097/Reviewer_7TAy"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1097/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698700718399,
        "cdate": 1698700718399,
        "tmdate": 1700661015955,
        "mdate": 1700661015955,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WuW1DZ4IGy",
        "forum": "9w3iw8wDuE",
        "replyto": "9w3iw8wDuE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1097/Reviewer_xMUg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1097/Reviewer_xMUg"
        ],
        "content": {
            "summary": {
                "value": "The authors found that using entropy as metrics is not enough in some biased scenarios. To address this, the authors devise a new metric namely Pseudo-Label Probability Difference. The experimental results demonstrate the effectiveness of the proposed metric."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThe authors empirically and theoretically analysis the entropy metric for TTA.\n2.\tThe authors devise a metric namely Pseudo-Label Probability Difference that further improves the entropy metric.\n3.\tThe proposed method is easy to understand and implement. I believe it can be applied to real-world applications. In addition, the proposed metric only requires negligible computational cost to compute, which would not introduce obvious latency compared with EATA or SAR."
            },
            "weaknesses": {
                "value": "1.\tCould the authors give simple explanation TRAP factors and CPR factors? With this, the readers may easily to capture the motivation of the proposed metric.\n2.\tCould the authors explain more about the motivation of the choice of patch shuffled input as x\u2019?"
            },
            "questions": {
                "value": "It would be better if the authors could explain the motivations more clearly."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1097/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698981135344,
        "cdate": 1698981135344,
        "tmdate": 1699636035995,
        "mdate": 1699636035995,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LlXYQCwQPI",
        "forum": "9w3iw8wDuE",
        "replyto": "9w3iw8wDuE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1097/Reviewer_M673"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1097/Reviewer_M673"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a new method for test-time adaptation (TTA) called Destroy Your Object (DeYO) that uses a novel confidence metric called Pseudo-Label Probability Difference (PLPD) to improve the adaptation performance and stability of test-time adaptation methods. The authors demonstrate the limitations of entropy as a confidence metric and compare the performance of DeYO with other TTA methods on the ImageNet-C and ImageNet-R benchmarks under various mild and wild test scenarios."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposed DeYO method is simple yet effective for improving the stability and performance of entropy-based TTA.\n\nThe idea and motivation behind Pseudo-Label Probability Difference (PLPD) are novel and interesting, providing new insights for the community.\n\nThe paper is strong on the empirical side. Extensive experiments with various model architectures, datasets, and mild/wild test scenarios are thorough."
            },
            "weaknesses": {
                "value": "The proposed terms \u201cTRAP\u201d and \u201cCRP\u201d are a bit hard to understand. The authors could refine the name and give more high-level/easy-understanding explanations about them in the Introduction. \n\nFor parameter sensitivity analyses in Figure 7, could the authors report more results under different model architectures, datasets and test scenarios? This helps demonstrate the hyperparameters\u2019 generality. \n\nAblation studies in Table 6 are also highly encouraged to be conducted on different models, datasets and test scenarios."
            },
            "questions": {
                "value": "Pls refer to Weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1097/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699110706559,
        "cdate": 1699110706559,
        "tmdate": 1699636035924,
        "mdate": 1699636035924,
        "license": "CC BY 4.0",
        "version": 2
    }
]