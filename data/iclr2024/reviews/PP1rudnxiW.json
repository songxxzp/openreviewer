[
    {
        "id": "xb89MEUPaf",
        "forum": "PP1rudnxiW",
        "replyto": "PP1rudnxiW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1846/Reviewer_ndxa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1846/Reviewer_ndxa"
        ],
        "content": {
            "summary": {
                "value": "This paper present a framework to perform variational inference with a score-based algorithm. Specifically, it introduces HJB-regulariser to the diffusion normalizing flow process and provide some justification for the framework."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This connection between score-based algorithm and variational inference is an interesting topic. It remains open to make these score-based model really works in VI literature."
            },
            "weaknesses": {
                "value": "The idea is relatively direct and the whole motivation looks incremental.\n\nThe empirical results are not strong enough from both VI/transport side.\n\nThe ablation study of HJB-regulariser is weak."
            },
            "questions": {
                "value": "In Variational inference literature, the hidden dimension $z$ is a low dimensional manifold that can be interpretable factors? The latent of VAE uses smaller latent dimension than the data. However, in diffusion model/transport-based algorithms. The space of $x$ and $z$ are in the same dimension, which makes $z$ encode all the information of $x$ (and loses the ability to perform inference). How can you justify this compensation? What are the benefits of the (almost) lossless VI?\nMoreover, it is not clear that how good is this \u201cVI\u201d compared to other VAEs, such as image generation, latent variable inference. The current setting is relatively toy.\n\n\nFrom the transport side, although it is understandable that Figure 2 is more well behaved than non-regularized transport, we still found that Figure 3 is good enough in real practice. Can you find an example that the non-regularized way fail?\n\nIn general, I do believe that the HJB-regulariser is useful in some cases, but given the current justifications and evidence, the significance is not clear at this moment."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1846/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1846/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1846/Reviewer_ndxa"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1846/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698727654803,
        "cdate": 1698727654803,
        "tmdate": 1700547137436,
        "mdate": 1700547137436,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "q8UHyQlsnM",
        "forum": "PP1rudnxiW",
        "replyto": "PP1rudnxiW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1846/Reviewer_nqRv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1846/Reviewer_nqRv"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a novel framework for generative modelling and Bayesian computation that bridges variational inference and optimal transport using the theory of diffusion models. Starting from the Schr\u00f6dinger problem and iterative proportional fitting, the authors draw a connection to the EM-algorithm, and finally present a novel regularized diffusion objective with a unique minimizer. They validate their method on several experimental benchmarks where it outperforms competing state-of-the-art methods."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The paper is as far as I (the reviewer) can judge highly original and will be greatly valued by the community. \n- The paper is well written and generally clear. The contextualization of their work with respect to other frameworks is well done. It unifies and generalizes several previously introduced frameworks, such as Monte Carlo diffusions or unadjusted Langevin annealing. The motivation and lead-up to their final results is in my opinion very convincing.\n- The proposed method outperforms other state-of-the-art-methods on a variety of experimental evaluations. The experimental section is sufficiently exhaustive wrt compared data sets and competing methods."
            },
            "weaknesses": {
                "value": "I don't have major comments regarding weaknesses other than that the source code for the experimental validation needs to be provided in the supplement and not only via some possibly non-permanent web link."
            },
            "questions": {
                "value": "- In my opinion the paper could benefit from a shorter introduction and motivation and instead an extended experimental section with method limitations, experimental details, computational trade-offs, etc. for more applied audiences. Since this is likely not possible due to page limits, an extended treatment in the appendix would be very welcome.\n- What are the training times of the method, for instance, in comparison to running SMC?\n- How were the neural networks architectures chosen and how is the hyperparameter selection and other details impacting the experimental results (e.g., see Table F1)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1846/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1846/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1846/Reviewer_nqRv"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1846/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698749812411,
        "cdate": 1698749812411,
        "tmdate": 1699636114714,
        "mdate": 1699636114714,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PrsY5UsWIr",
        "forum": "PP1rudnxiW",
        "replyto": "PP1rudnxiW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1846/Reviewer_2i1s"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1846/Reviewer_2i1s"
        ],
        "content": {
            "summary": {
                "value": "The authors propose to revisit sampling through the lens of generative modeling. In particular, they show that the EM algorithm can be formulated as an Iterative Proportional Fitting when there is no restriction on the kernels underlying the generative models. Based on their presentation and observations, they propose a score-based annealing (Controlled Monte Carlo Diffusion) that betters the state of the art on two datasets: funnel and GMM"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I find the paper very clear and the presentation very interesting. I believe the main novelty to be in the score-based annealing CMCD. I am unsure how novel the restricted correspondence between EM and IPF is; however, presenting such a relation is very interesting."
            },
            "weaknesses": {
                "value": "I am unsure how novel the restricted correspondence between EM and IPF is; in particular, it seems to me that the correspondence only holds when there is no restriction on $p^\\theta(x\\vert z)$; in the general setting, EM does not allow for the forward generative model $\\pi^{2n+1}(x,z)$ to have as a marginal $\\pi_x(x) = \\mu(x)$ and the backward to have as a marginal $\\pi_z(z) = \\nu(z)$. If I am not mistaken, I think it would be very helpful to have some comments on this point in the paper."
            },
            "questions": {
                "value": "I would be very happy to have more information on the relation between the correspondence between EM and IPF as discussed in limitations."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1846/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1846/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1846/Reviewer_2i1s"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1846/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698971168165,
        "cdate": 1698971168165,
        "tmdate": 1699636114650,
        "mdate": 1699636114650,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iFVGoJzxBK",
        "forum": "PP1rudnxiW",
        "replyto": "PP1rudnxiW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1846/Reviewer_LiYc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1846/Reviewer_LiYc"
        ],
        "content": {
            "summary": {
                "value": "This paper is divided in two parts. In the first part, the authors provide a unifying framework for VAEs and diffusion models using stochastic differential equations. In the second part connect the iterative proportional fitting procedure with the expectation maximization algorithm and provide a new objective for solving IPF that avoids the mode forgetting phenomenon that happens when one implements the diffusion schrodinger bridge. The added regularization term is null iff the HJB equation holds. Finally, the authors introduce the MCMD algorithm which makes uses of forward and backward SDEs that admit as marginals a prescribed sequence of distributions $(\\pi_t)_t$."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- I enjoyed reading this paper; it is highly pedagogical and sheds light on interesting connections between variational inference, diffusion models, stochastic control as well as optimal transport. The connection with the expectation maximization algorithm is particularly interesting. \n- The proposed algorithm CMCD generalizes existing methods such as Monte Carlo VAE aswell as MCD. I believe that the adopted point of view here is more rigourous and elucidates what the backward process should \"look like\", which was not clear in the Monte Carlo VAE paper for example. \n- Extensive experiments show that the CMCD method outperforms existing methods in relatively high dimensional examples."
            },
            "weaknesses": {
                "value": "- It is quite unfortunate that there are no experiments for the loss proposed in Proposition 3.2 to back the fact that the proposed loss does not suffer from mode forgetting. Furthermore, it is not clear how this is implemented in practice since a Laplacian term is involved."
            },
            "questions": {
                "value": "i have no further questions"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1846/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1846/Reviewer_LiYc",
                    "ICLR.cc/2024/Conference/Submission1846/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1846/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699214105001,
        "cdate": 1699214105001,
        "tmdate": 1700693493375,
        "mdate": 1700693493375,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "De8pik4v9w",
        "forum": "PP1rudnxiW",
        "replyto": "PP1rudnxiW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1846/Reviewer_yH6G"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1846/Reviewer_yH6G"
        ],
        "content": {
            "summary": {
                "value": "This paper presents the controlled Monte Carlo diffusion sampler (CMCD). CMCD builds upon previous work in the literature by adapting both the forward and the backward dynamics. The authors also posit a regularised iterative proportional fitting for schrodinger bridges that improve upon standard IPF. Finally, the authors link CMCD to Jarzinsky identity and demonstrate its performance through numerical experiments."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The technical contributions of this paper is unquestionable. The authors have managed to link key and fundamental ideas from various fields. I especially liked the connection between regularised IPF and EM algorithm, which is illuminating. I think the paper has good contribution and would be helpful to researchers in the field if presented correctly."
            },
            "weaknesses": {
                "value": "Unfortunately, I think the presentation of the paper needs to be revised extensively. The paper is confusing, and posits ideas without forewarning or motivation. The introduction is especially difficult to read, and not because of its technicality. In general, I believe the authors' math is quite readable. It is the writing and motivation that needs improvements. As an example, nowhere in the introduction the authors explicitly state the problem they are tackling (CMCD). It takes until page 7, eq. (22) for the authors to state the explicit problem that they address. \n\nAs another example, the authors never explicitly state the necessity of various connections they draw. For example, the text above Proposition 3.1 explains why IPF fails to perform well. However, there is no explanation afterwards to say why going through the lens of EM solves this issue. Rather, the text proceeds to describe optimality conditions in what seemed to me to be like a tangent. In summary, although I could follow the math, it was extremely hard for me to follow the context of the paper.\n\nIt pains me to say that I cannot suggest the authors any single thing that could drastically improve the paper. I appreciate the technical contribution. However, given how difficult it is to follow its context, I don't believe that the paper would be helpful to the wider community in its current state.\n\nAt the very least, I think the first two pages of the introduction should be thoroughly revised to focus more on the problem addressed in the paper, rather than overloading on related technical terms and connections. I also think the propositions which are not key contributions of the paper (for instance, 2.1) could be better placed in the supplements. \n\nI also list some minor typos:\n\n1. A bracket has not been closed in page 2, paragraph 3, after \"(Section 2.2\"\n2. Page 7, last paragraph, \"Proposition2.2\" is missing a white-space.\n\nPost Rebuttal: I thank the authors for the extensive revision. I think it addresses most of my comments and the authors truly went above and beyond. I have revised my scores to reflect this. I think that in its current form the paper can definitely be useful and has interesting insights which are presented clearly. I believe, if accepted this will be an important contribution to the community."
            },
            "questions": {
                "value": "Is eq. (10) the same as eq (1)? I failed to find the difference."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1846/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1846/Reviewer_yH6G",
                    "ICLR.cc/2024/Conference/Submission1846/Senior_Area_Chairs"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1846/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699304460999,
        "cdate": 1699304460999,
        "tmdate": 1699988735517,
        "mdate": 1699988735517,
        "license": "CC BY 4.0",
        "version": 2
    }
]