[
    {
        "id": "A3s8mWNctJ",
        "forum": "JsnR0YO4Fq",
        "replyto": "JsnR0YO4Fq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7286/Reviewer_2xQy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7286/Reviewer_2xQy"
        ],
        "content": {
            "summary": {
                "value": "The paper aims to analyze weight balancing by examining neural collapse and the cone effect at each training stage. The analysis reveals that weight balancing can be broken down into an increase in Fisher's discriminant ratio of the feature extractor due to weight decay and cross entropy loss, as well as implicit logit adjustment caused by weight decay and class-balanced loss. This analysis allows for a simplified training method with only one training stage, while improving accuracy."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. As an experimental and analytical paper, the logical flow of the entire article is smooth, providing a good reading experience.\n2. Weight Decay, as a simple yet effective model, is thoroughly explained in this paper with targeted explorations and explanations at each step. The argumentation is well-grounded and convincing.\n3. The feasibility of single-stage training is explored based on the analysis of the original method, which represents a certain breakthrough."
            },
            "weaknesses": {
                "value": "1. The analysis solely based on one particular model method has certain limitations, as it lacks consideration of other methods. Exploring why Weight Decay performs exceptionally well indeed raises a thought-provoking question in the long-tail domain. However, the favorable properties of Weight Decay have already been extensively explored in balancing datasets, and its effectiveness can be considered widely recognized."
            },
            "questions": {
                "value": "1. Besides the analysis metrics mentioned in the paper, what other commonly used metrics exist? Why was the choice of metrics in the paper considered?\n2. If we consider balanced datasets, the analysis in the paper can still hold true. The only difference lies in the performance based on the sota models. What distinguishes this type of analysis from conventional methods when dealing with long-tailed data distributions? What are the innovative aspects of this paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7286/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7286/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7286/Reviewer_2xQy"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7286/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697725969547,
        "cdate": 1697725969547,
        "tmdate": 1699636870367,
        "mdate": 1699636870367,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "u5480T8NJj",
        "forum": "JsnR0YO4Fq",
        "replyto": "JsnR0YO4Fq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7286/Reviewer_GF5W"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7286/Reviewer_GF5W"
        ],
        "content": {
            "summary": {
                "value": "This paper primarily investigates why the two-stage WD method could perform well in long-tailed tasks. It analyzes the WB by focusing on\nneural collapse and the cone effect at each training stage and found that it can be decomposed into an increase in Fisher\u2019s discriminant ratio of the feature extractor caused by weight decay and cross-entropy loss and implicit logit adjustment caused by weight decay and class-balanced loss. Then the paper proposes the simplify the WD by reducing the number of training stages into one with the combination of WD, FR, and ETF."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper provides an in-depth analysis of the reasons behind the success of WD in long-tail scenarios, demonstrating thoughtful insights. From the perspective of neural collapse and the cone effect, it explains the WD well.\n2. This paper has a well-organized structure which makes it easy for readers to understand the research.\n3. Extensive experimental results confirm the validity of the analysis."
            },
            "weaknesses": {
                "value": "1. The paper only discusses the related work of NC and WD but the related work of the long-tail is also necessary.\n2. Some concerns which I will mention in the following section."
            },
            "questions": {
                "value": "1. What's the meaning of O in Eq.3 and could the author explain more about Theorem 2?\n2. Could the author explain why the WD&FR&ETF performs worse than the WD&ETF on the ImageNet-LT dataset in Table 13? And are there any experiments conducted on large-scale datasets, such as iNaturalist 2018?\n3. Existing long-tail solutions often rely on expert systems to improve the performance of tail classes, such as RIDE[1] and SADE[2]. Is the proposed method in this paper compatible with them?\n\n[1] Wang, Xudong, et al. \"Long-tailed recognition by routing diverse distribution-aware experts.\" arXiv preprint arXiv:2010.01809 (2020).\n[2] Zhang, Yifan, et al. \"Test-agnostic long-tailed recognition by test-time aggregating diverse experts with self-supervision.\" arXiv e-prints (2021): arXiv-2107."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7286/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7286/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7286/Reviewer_GF5W"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7286/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698390603387,
        "cdate": 1698390603387,
        "tmdate": 1699636870235,
        "mdate": 1699636870235,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "n7pyPTapGH",
        "forum": "JsnR0YO4Fq",
        "replyto": "JsnR0YO4Fq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7286/Reviewer_7mhz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7286/Reviewer_7mhz"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the problem of long-tailed recognition (LTR) and presents theoretical analysis regarding the two-stage training of LTR. The main findings include two theorems showing 1) how neural collapse and the cone effect are affected by weight balancing at each training stage; 2) how weight decay contributes to an increased in Fisher's discriminant ratio of the feature extractor and implicit logit adjustment. In addition to those theoretical results, authors also report extensive experimental results as supporting evidence. The paper is well-written and easy to follow. The technical contributions of this work are expected to sharpen our understanding of the LTR problem, which might inspire other attacks to LTR than weight balancing."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. The problem formulation is well motivated and sensible. Developing a theory for weight balancing in LTR has been under-researched in the literature. This work makes a timely contribution to this important topic.\n2. The technical contributions in Sec. 4 and 5 are solid. Both theorems 1 and 2 are well presented and their rigorous proof have been included in the Appendix. The generalized result of Theorem 2 (Theorem 3 in Appendix) is commendable. \n3. In addition to the theoretical analysis, this paper also reported extensive experimental results as supporting evidence. Those figures and tables have greatly facilitated the understanding of the underlying theory."
            },
            "weaknesses": {
                "value": "1. The difference between weight balancing (WB) and weight decay (WD) needs to be make clearer. Sec. 3 only reviews WB and overlooks WD. Historically, WD was proposed much earlier than WB. It will be a good idea to include some review of WD in Sec. 3, I think. Note that WD is already present in Table 1 on page 4 (right after Sec. 3).\n2. For those who are less familiar with two-stage training of LTR, it might be a good idea to include a concise review of two-stage training methods in the Appendix. Note that CVPR2022 and ICLR2020 have different formulation of two-stage training. Please clarify that the model analyzed in this paper is based on the CVPR2022 work even though it cited the ICLR2020 as the original source of two-stage training.\n3. There are many acronyms in this paper. It might be a good idea to provide a table summarizing them in the Appendix A.1 (Notation and Acronym)."
            },
            "questions": {
                "value": "1. What do blue and red colors in Table 4 and Table 9 highlight? Some explanations can be added to the caption of those tables. \n2. Table 1 includes experimental results for WD without and with fixed batch normalization (BN). Any plausible explanation for these results? Why does BN further improve the performance of WD?\n3. In Table 5, the accuracy performance of LA (N/A) is noticeably higher than add/mult for the category of \"many\". Why does LA only work for the Medium and Few classes?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7286/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698414725363,
        "cdate": 1698414725363,
        "tmdate": 1699636870086,
        "mdate": 1699636870086,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hw84OMhPof",
        "forum": "JsnR0YO4Fq",
        "replyto": "JsnR0YO4Fq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7286/Reviewer_dLbF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7286/Reviewer_dLbF"
        ],
        "content": {
            "summary": {
                "value": "The author analyzed the weight balancing method for long-tailed classification problems from the perspectives of neural collapse and the cone effect, and provided some insights."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The problem of imbalanced classification is undeniably a highly practical and crucial research issue in the field of machine learning.\n2. The authors provided an analysis of weight balancing to a certain extent and offered insightful perspectives on the topic."
            },
            "weaknesses": {
                "value": "1. This paper appears to resemble an appendix on Weight Balancing to some extent and the technical innovation is rather limited.\n2. Given that Weight Balancing is not the best-performing method in the field of imbalanced learning, the significance of this paper in the field remains debatable.\n3. Considering that Weight Balancing involves implicit constraints at the parameter level (compared to direct correction in other long-tail classification methods), its extension to address broader distribution shift issues should hold greater value.\n4. Sec 5.1\"the second stage of WB is equivalent to multiplicative LA\". Why not just use explicit LA?\n\n\nupdate: After reading the authors' response and other reviewers' comments, I would like to increase my score to weak accept."
            },
            "questions": {
                "value": "See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7286/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7286/Reviewer_dLbF",
                    "ICLR.cc/2024/Conference/Submission7286/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7286/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698648561127,
        "cdate": 1698648561127,
        "tmdate": 1699969564930,
        "mdate": 1699969564930,
        "license": "CC BY 4.0",
        "version": 2
    }
]