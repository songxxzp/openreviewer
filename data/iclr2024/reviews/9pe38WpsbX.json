[
    {
        "id": "JnRHPjWdtX",
        "forum": "9pe38WpsbX",
        "replyto": "9pe38WpsbX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission650/Reviewer_QumB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission650/Reviewer_QumB"
        ],
        "content": {
            "summary": {
                "value": "This paper combines the methods from DreamerV3 and MuZero, and presents a new model named MuDreamer for visual reinforcement learning. The key contribution is to introduce a new world model architecture that involves the prediction of environment rewards, value functions, continuation flags, and inverse dynamics. The proposed model showcases a comparable performance to DreamerV3 in multiple domains, including DeepMind Control Suite and the Atari100k benchmark."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper is well-organized and easy to follow. \n2. The proposed model is extensively evaluated on widely used visual control benchmarks. It also provides comprehensive ablation studies to explore the effectiveness of each model component.\n3. The model achieves performance comparable to DreamerV3, and as claimed by the authors, it is more efficient in the training time."
            },
            "weaknesses": {
                "value": "1. As stated by the authors, 'MuDreamer solves tasks without the need for a reconstruction loss.' However, this seems to be in contrast with the loss function described in Eq. (3), which still involves optimizing the image decoder with a reconstruction loss. If my understanding is accurate, the distinction from DreamerV3 lies in the fact that the gradient from the reconstruction loss doesn't back-propagate to the dynamics module. In light of this, I recommend that the authors consider revising the paper's title.\n2. The proposed model offers limited novelty when compared to DreamerV3. The introduction of inverse dynamics and continuation prediction loss is not a novel contribution in the field of model-based RL.\n3. While MuDreamer trains faster than DreamerV3, the difference in training time is relatively modest (4 hours vs. 4 hours and 20 minutes).\n4. The outperformance of the proposed model compared with DreamerV3 is observed in only 3 out of 26 games on the Atari100k, which may not be sufficient to establish its overall effectiveness."
            },
            "questions": {
                "value": "My main concerns are about the technical novelty and the experimental results. Please see my comments above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission650/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698638956831,
        "cdate": 1698638956831,
        "tmdate": 1699635992462,
        "mdate": 1699635992462,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VfFlFNzI2r",
        "forum": "9pe38WpsbX",
        "replyto": "9pe38WpsbX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission650/Reviewer_5xED"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission650/Reviewer_5xED"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces MuDreamer, an enhanced version of the DreamerV3 algorithm. MuDreamer eliminates the requirement of reconstructing input signals by learning a predictive world model that predicts the environment value function and previously selected actions. The importance of batch normalization in preventing learning collapse is highlighted, and the impact of KL balancing on convergence speed and learning stability is examined."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The motivation is sound, as this paper combines the strengths of dreamerV2 and MuZero to tackle tasks from image inputs with both continuous and discrete action spaces, without the need for input signal reconstruction."
            },
            "weaknesses": {
                "value": "1. Value predictors in this research are inspired by MuZero, and the inclusion of action prediction is a common practice in various model-based approaches. As a result, the novelty may be relatively constrained.\n\n2. The comparison is unfair as it only considers Dreamerv3. It would be more equitable to include more model-based methods for comparison, such as Dreamerpro[1] and denoised MDPs[2]. Dreamerpro, in particular, is a highly relevant method within the domain of Reconstruction-free model-based reinforcement learning.\n\n[1] Dreamerpro: Reconstruction-free model-based reinforcement learning with prototypical representations. ICML 2022.\n\n[2] Denoised mdps: Learning world models better than the world itself. ICML 2022.\n\n3. The experimental results are not satisfactory, as it appears that DreamerV3 performs better. I am aware that MuDreamer has fewer parameters, but it is important to analyze specifically where the differences lie. Please provide an analysis and remove the corresponding parts from DreamerV3 to assess the performance. Additionally, since the authors were inspired by MuZero in several aspects, it would be beneficial to compare this approach as well.\n\n4. Why is batch normalization used instead of other normalization techniques such as layer normalization? Can layer normalization achieve similar effects?\n\n5. Many explanations are not sufficiently in-depth. For example, in KL balancing, why does using a slight regularization of the representations toward the prior with \u03b2rep = 0.05 solve both of these issues?"
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission650/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698836045529,
        "cdate": 1698836045529,
        "tmdate": 1699635992385,
        "mdate": 1699635992385,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WMgbPYxM78",
        "forum": "9pe38WpsbX",
        "replyto": "9pe38WpsbX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission650/Reviewer_8AeP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission650/Reviewer_8AeP"
        ],
        "content": {
            "summary": {
                "value": "This paper presents an exploration of 4 modifications to DreamerV3, and presents promising results on the Control Suite and more limited results on Atari100k.\n\nThey assess 4 changes: removing the observation reconstruction loss, adding a previous action prediction head, replacing LayerNorm with batch normalization and changing the weights of the L_dyn and L_rep losses.\n\nOverall, this is a clear and well-executed piece of work, but it has quite limited scope and the results (although promising) aren\u2019t clearly demonstrating a strong benefit over DreamerV3 or EfficientZero."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The paper is clear, presents the scope, problem it wants to tackle and related work well.\n2. The presentation of the method is clear and the modifications are easy to follow (although quite directly inspired from the Dreamer papers)\n3. Results are complete and well presented, with a good coverage of Control Suite results experiments as well as Atari100k. Baselines choices are good.\n4. Ablation study of 5.2 is clean and well executed once again."
            },
            "weaknesses": {
                "value": "1. The similarities between MuDreamer and DreamerV3 are potentially too strong to make this work significant enough in this state. The paper looks like yet another version of Dreamer, with the exact same math and text dangerously close to a copy, with only a few extra ablations and modifications.\n2. Results aren\u2019t as clear-cut as I\u2019d like. There have been a lot of MBRL papers in recent years which explored many combinations of losses, models, actors, but it is quite hard to find which components really matter. \n   1. The Control Suite results are slightly better, but not groundbreakingly so\n   2. The Atari100k results aren\u2019t that competitive, especially compared to EfficientZero which would be the clear baseline if one would take the strict desire of not learning to reconstruct observations.\n   3. Figure 5 and 6 in the Appendix demonstrates this well, where all curves are fairly similar and do not show a strong enough signal for me.\n3. Despite the removal of the observation reconstruction loss, Figure 2 and others indicate that image reconstruction is still done nearly perfectly, which goes counter to the original motivation. It is unclear why that is the case, but it does feel like the model is not as different in what the latent space capture compared to DreamerV3 as it could be?"
            },
            "questions": {
                "value": "1. Why is Figure 2 so good at reconstructing the observation?\n   1. One would have expected to only capture what mattered for the task if the assumptions from the abstract/introduction were true?\n   2. Are there games where you have examples of \u201cDreamer failing to perceive crucial elements\u201d, which MuDreamer does capture?\n   3. As it stands, it is unclear to me that the latent space is any different and more abstract than DreamerV3.\n2. It would have been interesting to point to specific games where this effect should arise, and make a clear comparison between DreamerV3, MuDreamer and EfficientZero.\n   1. For example, having a good score on Frostbite seemed interesting (as it does contain quite a lot of hard details to model well), but looking in the Appendix Figure 6, this seems to be more about 1 seed of DreamerV3 doing badly\u2026\n3. Did you explore using the Action predictor network directly for acting, instead of having another Actor network?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission650/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698858776530,
        "cdate": 1698858776530,
        "tmdate": 1699635992302,
        "mdate": 1699635992302,
        "license": "CC BY 4.0",
        "version": 2
    }
]