[
    {
        "id": "3YHMMd4IYS",
        "forum": "xVBXz7wD2m",
        "replyto": "xVBXz7wD2m",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9256/Reviewer_7nfe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9256/Reviewer_7nfe"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel multi-task learning (MTL) framework, GatedMTL, to address the fundamental challenges of task interference and computational constraints in MTL. GatedMTL learns the optimal balance between shared and specialized representations by leveraging a learnable gating mechanism to allow each task to select and combine channels from its task-specific features and a shared memory bank of features. Moreover, a regularization term is used to learn the optimal balance between allocating additional task-specific parameters and the model\u2019s computational costs. Extensive empirical evaluations are conducted."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper proposes a novel multi-task learning (MTL) framework to address the fundamental challenges of task interference and computational constraints in MTL.\n2. Extensive empirical evaluations are conducted."
            },
            "weaknesses": {
                "value": "1. The code is not provided.\n2. The description of the proposed method in Section 3 and the overall framework in Figure 1 are confusing. If my understanding is correct, the proposed method is very similar to the existing MoE-base MTL methods. However, this paper does not discuss and compare with MoE-based MTL methods.\n3. The proposed method uses the single-task weights for initialization, which means it needs to train $T$ single-task models before training the proposed method, and it is unfair to compare with the baselines which do not use the information from single-task models.\n\nSee the next Questions part for details."
            },
            "questions": {
                "value": "**Major Concerns**:\n1. The description in Section 3 and Figure 1 are confusing. Is the encoder in Figure 1 shared among different tasks? Does $\\Psi$ denote the shared encoder in Figure 1? If so, which part in Figure 1 is $\\Phi_t$, and how can we obtain the shared and task-specific features at each layer? Are there $T+1$ encoders where one is $\\Psi$ shared among different tasks and the others are task-specific $\\Phi_t$? If so, what is the difference between the proposed GatedMTL and MoE-based MTL methods like [1, 2, 3, 4, 5]?\n2. How to choose $\\omega_t$ in Eq. (1)?\n3. In the last paragraph of Section 4.1: \"the task-specific branches are with their corresponding single-task weights\". It means we need to train $T$ single-task models before training the proposed GatedMTL, which causes a huge computational cost in the training process. \n4. \"for a given computational budget\" in the abstract and \"matching the desired target computational cost\" in the second contribution. What is the \"given computational budget\" or \"desired target computational cost\"? Is it $\\tau_t$ in Eq. (4)? However, $\\tau_t$ represents neither parameter size nor flops. Besides, although both $\\lambda_s$ and $\\tau_t$ can control the trade-off between performance and computational cost, the sparsity regularization cannot be guaranteed to be optimized to $0$.\n5. Why not report and compare the parameter size? It is very important in multi-task learning.\n6. Many recent or important baselines are missing. For example, MoE-based MTL methods like [1, 2, 3, 4, 5] and MTO approaches like [6, 7, 8].\n\n\n**Minor Concerns**:\n1. $\\odot$ in Eqs. (2), (3), (6), (7), and (8) is not defined.\n2. Next line of Eq. (3): $R$ should be $\\mathbb{R}$.\n3. $\\beta^l$ is a learnable parameter, but it does not appear in the overall training objective Eq. (5).\n4. $(\\tau_t)\\_{t=1}^T$ should be $\\\\{\\tau_t\\\\}\\_{t=1}^T$. \n5. Some references appear twice, such as \"Multi-task learning using uncertainty to weigh losses for scene geometry and semantics.\", \"End-to-end multi-task learning with attention.\", \"Auto-lambda: Disentangling\ndynamic task relationships.\", and \"Attentive single-tasking of multiple tasks.\".\n\n----\n[1] Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts. KDD, 2018.\n\n[2] Progressive Layered Extraction (PLE): A Novel Multi-Task Learning (MTL) Model for Personalized Recommendations. RecSys, 2020.\n\n[3] DSelect-k: Differentiable Selection in the Mixture of Experts with Applications to Multi-Task Learning. NeurIPS, 2021.\n\n[4] Deep Safe Multi-Task Learning. arXiv:2111.10601v2.\n\n[5] MSSM: A Multiple-level Sparse Sharing Model for Efficient Multi-Task Learning. SIGIR, 2021.\n\n[6] Multi-Task Learning as Multi-Objective Optimization. NeurIPS, 2018.\n\n[7] Conflict-Averse Gradient Descent for Multi-task Learning. NeurIPS, 2021.\n\n[8] Reasonable Effectiveness of Random Weighting: A Litmus Test for Multi-Task Learning. TMLR, 2022."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9256/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698324389570,
        "cdate": 1698324389570,
        "tmdate": 1699637165560,
        "mdate": 1699637165560,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7Fo9lIPv9f",
        "forum": "xVBXz7wD2m",
        "replyto": "xVBXz7wD2m",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9256/Reviewer_DXQP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9256/Reviewer_DXQP"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a GatedMTL framework for MTL.  GatedMTL aims to address the fundamental challenges of task interference and computational constraints in MTL. Specifically, a learnable gating mechanism is used to select and combine channels from its task-specific features and a shared memory bank of features. In addition, the gates are regularized to learn the optimal balance between allocating additional task-specific parameters and the model\u2019s computational costs. The proposed method is evaluated on datasets and the experiment results also achieve comparable performance. However, the contribution of this GatedMTL seems marginal and the results are not very strong."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1) The proposed GatedMTL method to assign features to either a task-specific or shared branch, until reaching an adjustable target computational budget.\n2) Experiment results demonstrate competitive performance.\n3) Easy to understand."
            },
            "weaknesses": {
                "value": "1) The core idea of this paper is to find a parameter to control the ratio of task-specific features to task-shared features. The motivation of the gating design for MTL is not clear. The gating mechanism is not a new story in MTL.\n2) The gating module to balance task-specific features and the shared features in the decoder seems a bit more reasonable. Since the encoder is responsible for encoding out the shared features across all tasks, it doesn't seem to make sense to split out the task-specific features in the encoder. \n3) The proposed gating mechanism seems similar to a simplified variant of smooth Gating in DSelect-k[R1]. It is not possible to observe from Eqs. 2 and 6 that there is a point of novelty in the gating of this paper.\n4) The authors are encouraged to show comparisons of feature changes before and after the addition of gating through visualization. In addition, how to show task-specific features and shared features. Can these two features be displayed through visualization?\n5) The results in Table 1 were confusing to the reviewers, who could not see directly from the table how the five GatedMTLs are differentiated. The other tables have the same confusion.\n6) Why are the results for Auto-\u03bb not shown in Tables 3 and 4?\n[R1] DSelect-k: Differentiable Selection in the Mixture of Experts with Applications to Multi-Task Learning, NeurIPS, 2021."
            },
            "questions": {
                "value": "1) The gating module to balance task-specific features and the shared features in the decoder seems a bit more reasonable. Since the encoder is responsible for encoding out the shared features across all tasks, it doesn't seem to make sense to split out the task-specific features in the encoder. Have the authors considered this?\n2) Minor error:\n$\\Delta_{MTL}$ and $\\Delta$ denote the same metric. The authors are encouraged to keep them consistent."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9256/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9256/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9256/Reviewer_DXQP"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9256/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698691580234,
        "cdate": 1698691580234,
        "tmdate": 1699637165431,
        "mdate": 1699637165431,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xwpVlnqHtf",
        "forum": "xVBXz7wD2m",
        "replyto": "xVBXz7wD2m",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9256/Reviewer_PvDp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9256/Reviewer_PvDp"
        ],
        "content": {
            "summary": {
                "value": "The manuscript proposes a new Multi-Task Learning (MTL) framework called *GatedMTL* that learns the optimal balance between shared and task-specific representations for a given computational budget. It uses a gating mechanism to learn a combination of shared and task-specific features for each task in each layer. Unused features and weights are pruned during inference to improve sparsity and efficiency. The framework generalizes to convolutional backbone and transformer-based backbone. Experiments on CelebA, NYUD-v2, and PASCAL-Context datasets demonstrate the proposed method maintains a favorable balance between compute costs and multi-task performance across computational budgets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "*Originality*: This work introduces a multi-head gating mechanism into feature transformation, solving the challenge of multi-task learning with an emphasis on computational efficiency. \n\n*Quality*: The experiments are extensive. \n\n*Clarity*: The paper is written clearly, and the figures are easy to understand.\n\n*Significance*: The problem that this work attempts to address is important. Given the computational budget, the performance improvement is obvious."
            },
            "weaknesses": {
                "value": "W1: No source code is provided. Although the experimental setup is detailed and the results are extensive, it is still necessary to provide the code for reference and reproducibility checking.\n\nW2: Since a shared feature branch acts like a memory bank where task-specific features can communicate, a task-specific gate still learns features from other tasks, which can cause task interference.\n\nW3: The reported performance in each table is based on a single run. The standard deviation based on multiple random runs is highly encouraged to be provided."
            },
            "questions": {
                "value": "Q1: What is the purpose of the \"convolution block\" in forming the shared feature map of the next layer (line 1, page 4)? \n\nQ2: A more detailed description of the changes made to the backbone is needed for the implementation of the gated MLT layer."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics review needed."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9256/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9256/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9256/Reviewer_PvDp"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9256/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698801633362,
        "cdate": 1698801633362,
        "tmdate": 1699637165306,
        "mdate": 1699637165306,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CnWtY9QlaI",
        "forum": "xVBXz7wD2m",
        "replyto": "xVBXz7wD2m",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9256/Reviewer_wdHJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9256/Reviewer_wdHJ"
        ],
        "content": {
            "summary": {
                "value": "The focus of this work is to address the problem of task interference in multitask learning (MTL), which manifests as the negative effect that learning a task may have on another one when trained together. To this end, the authors propose a new soft parameter-sharing framework coined GatedMTL, which effectively consists of an automatic mechanism by which a series of identical task-specific architectures learn to share a mixture of their features during training, while retaining task-specific parameters when needed. The authors also propose to use sparsity regularization to encourage sharing parameters and reduce compute. Finally, empirical results on convolutional and transformer based models show that the proposed architecture is able to successfully explore the performance vs. compute trade-off, outperforming the chosen baselines in that matter."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is well-written, and the proposed solution is super intuitive and easy to understand.\n- The emphasis on performance vs. flops (or size) is rather refreshing to read.\n- The number of experiments variety is impressive for what is usual in the field, and it is nice to see a discussion and empirical evaluation of negative transfer and backbone size.\n- The authors propose GatedMTL for two fairly widespread architectures, and the empirical results are quite positive."
            },
            "weaknesses": {
                "value": "**Limitations**\n- W1. The biggest problem I have with the manuscript is that it does not discuss or show the limitations of the proposed approach _at all_, which can really easily mislead the readers (and thus, the reviewers). For example, to my understanding, the proposed approach at training time is $T$ individual models that are trained altogether. However, this is a _huge_ setback as it scales poorly with $T$ in memory and time (for example, the usual CelebA setting in MTL is to do a 40-task binary classification, but the authors reduce it to 3 tasks). The authors should discuss it in the manuscript and show training times for each of the experiments.\n- W2. The hyperparameters $\\tau_t$ are hardly intuitive, and the recommendation is to i) use the gap between STL and MTL models, and to ii) study the distribution of the gating patterns wrt the shared branch. The former requires tuning and training $T+1$ models, whereas the latter requires carefully looking into the model parameters. I am afraid that this can really hurt the adoption of the model by practitioners.\n\n**Presentation**\n- W3. Citations should properly use `\\citet` and `\\citep`. Even worse, the bibliography is a mess and I cannot comprehend how it happened (and I am going to assume, in good faith, that LLMs have nothing to do). The ones I spotted:\n\t- Kendall's citation is doubled (and with different years).\n\t- The citations **in the same paragraph of the manuscript** for DWA and MTAN (proposed in the same paper) are different. And again, different years. This is mind-blowing to me.\n\t- The paper by Maninis is also doubled.\n\t- The paper by Javaloy & Valera is from ICLR 2022, not 2021.\n\t- GradNorm is cited as arxiv 2017 when it is published at ICML 2018.\n\t- Adashare's paper has no venue.\n\t- Most urls point to semanticscholar instead of the official venue.\n\n**Experiments**\n- W3. I find $\\Delta_{\\text{MTL}}$ a brittle metric, as it is sensitive to low-magnitude metrics and task metrics are not comparable. I would add a more robust metric like the rank mean (see, e.g., [1]). \n- W4. The chosen baselines are inconsistent across experiments and mostly outdated. From the MTO side, DWA and Uncertainty are quite old and weak in comparison with other methods like PCGrad, CAGrad, or NashMTL. From the side of adaptive architectures, more modern approaches like Adashare should be included.\n\n[1] Navon, A., Shamsian, A., Achituve, I., Maron, H., Kawaguchi, K., Chechik, G., & Fetaya, E. (2022). Multi-task learning as a bargaining game. arXiv preprint arXiv:2202.01017."
            },
            "questions": {
                "value": "- Q1. Do you use a different Lagrange multiplier for each task when using L1 regularization? Otherwise, I don't see how it is comparable to the hinge loss in Eq. 4."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9256/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9256/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9256/Reviewer_wdHJ"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9256/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698830759261,
        "cdate": 1698830759261,
        "tmdate": 1700739227086,
        "mdate": 1700739227086,
        "license": "CC BY 4.0",
        "version": 2
    }
]