[
    {
        "id": "wRdBzZoupu",
        "forum": "UmMa3UNDAz",
        "replyto": "UmMa3UNDAz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3007/Reviewer_SYoS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3007/Reviewer_SYoS"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new quantization scheme for diffusion models. In particular, the paper notes that post-training quantization (PTQ) may be efficient but brings relatively low performance compared to quantization-aware training (QAT). Conversely, QAT brings higher performance but requires heavy computational resources. To combine the advantages of these two main quantization approaches, the paper introduces a quantization counterpart of low-rank adapter (LoRA). The paper also proposes to quantize the model in a data-free manner through distillation from the original full-precision model."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper introduces a new quantization method that brings substantial efficiency improvement without incurring extra computational overhead and performance degradation.\n\n- The paper demonstrates strong performance.\n\n- The paper is clearly written."
            },
            "weaknesses": {
                "value": "- Comparisons: What happens if other quantization models also employ LoRA and distillation, which are common techniques to use?\n\n- Novelty concern: I think the paper presents a combination of existing works (scale-aware optimization from LSQ, common distillation technique, and LoRA). Can the authors clarify the difference in contribution from the combination of existing works? If there is a difference, how does the performance differ compared to the combination?"
            },
            "questions": {
                "value": "Please refer to the weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3007/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698838473541,
        "cdate": 1698838473541,
        "tmdate": 1699636245239,
        "mdate": 1699636245239,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iEjIvf2S6p",
        "forum": "UmMa3UNDAz",
        "replyto": "UmMa3UNDAz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3007/Reviewer_BLfX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3007/Reviewer_BLfX"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a data-free and efficient Quantization-Aware Training (QAT) method for diffusion models. For efficient QAT, it introduces the Quantization-Aware Low-Rank Adapter (QALoRA), which combines LoRA and QAT. The paper extends the LSQ QAT method a little to the Temporal LSQ method, which learns different scale factors for different time steps to handle the activation distribution difference across steps. The experimental of image diffusion and latent diffusion models on CIFAR-10, LSUN, and ImageNet demonstrates that this method can significantly outperform previous PTQ methods when doing W4A4 and W2 quantization."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Applying QAT to diffusion models to achieve better quantization performance is reasonable.\n- The proposed QALoRA is data-free and efficient (cost about 10 GPU hours).\n- The experimental results are promising.\n- Actual speedup with CUTLASS is reported."
            },
            "weaknesses": {
                "value": "There exist some formulas and details that are not clear enough. Some additional ablation and analysis experiments are needed to make the overall method more convincing. Check the questions section."
            },
            "questions": {
                "value": "- How to get $\\bf{x}_t$ in equation (7) is not described properly. Do the authors sample $\\bf{x}_T \\sim \\mathcal{N}$, and run several solver steps using the FP model to get $\\bf{x}_t$, or otherwise? If it is the case, the equation (7) is not written properly.\n- The proposed Temporal LSQ (TLSQ) method uses a different activation quantization scale for different time steps. Can the authors show the learned scales and analyze how the scale factors of certain layers change w.r.t. the time steps on different datasets? \n- Can the authors compare TLSQ with deciding the time-step-wise activation quantization scale using some calibration data or even run-time dynamic quantization? This can help illustrate the necessity of LSQ.\n- The paper mentioned that \"we interpolate the learned temporal quantization scales to deal with the gap of sampling steps between fine-tuning and inference\". I found steps=100 experiments on CIFAR-10 and LSUN in Table 1 and Appendix A, I wonder if the authors experimented with using a different schedule with fewer steps? Does this scale-deciding technique work well in the fewer-step regime?\n- Is QALoRA applied for all the weights, including the convolutions and the attention layers?\n- For the good of future efficient diffusion, can the authors discuss more relevant limitations and raise questions worth future studying? The current discussion is not specific."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3007/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699370520933,
        "cdate": 1699370520933,
        "tmdate": 1699636245144,
        "mdate": 1699636245144,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WMVclxuBq0",
        "forum": "UmMa3UNDAz",
        "replyto": "UmMa3UNDAz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3007/Reviewer_5Wot"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3007/Reviewer_5Wot"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a data-free fine-tuning framework tailored for low-bit diffusion models. The key approach involves freezing the pretrained diffusion model and fine-tuning a set of quantization-aware LoRA variants (QALoRA) by employing knowledge distillation to capture the denoising capabilities in the full-precision model. The paper also introduces two techniques, namely scale-aware optimization and learned step-size quantization, to address challenges related to ineffective learning of QALoRA and variations in activation distributions. Extensive experiments highlight that EfficientDM achieves performance levels comparable to QAT methods while preserving the data and time efficiency advantages of PTQ methods."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1.\tAchieving QAT-level performance with PTQ-level efficiency is significant and promising for low-bit diffusion models.\n2.\tThe idea of the QALoRA is novel. Compared to QLoRA, it avoids extra floating-point calculations during inference.\n3.\tThe results are encouraging and demonstrate the strong performance of EfficientDM under various bit-widths.\n4.\tThe paper is well-organized and easy to follow. The supplementary material provides additional experimental results and comprehensive visualization results, which enhance the overall credibility of the work."
            },
            "weaknesses": {
                "value": "1.\tIt would be beneficial to evaluate EfficientDM over recent text-to-image diffusion models, such as Stable Diffusion.\n2.\tRecent work TDQ [1] also introduces a quantization method that adjusts the quantization scale at various denoising steps. The differences should be discussed.\n3.\tFormulating the gradient of LoRA weights can help elucidate the reasons for ineffective learning of QALoRA.\n4.\tFigure 2: the notation for scale-aware optimization is inconsistent with Eq. (8), please fix it.\n\n[1] So, Junhyuk, et al. \u201cTemporal Dynamic Quantization for Diffusion Models.\u201d arxiv 2023."
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3007/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3007/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3007/Reviewer_5Wot"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3007/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699496913558,
        "cdate": 1699496913558,
        "tmdate": 1699636245087,
        "mdate": 1699636245087,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xwL7I36Hg3",
        "forum": "UmMa3UNDAz",
        "replyto": "UmMa3UNDAz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3007/Reviewer_ec6g"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3007/Reviewer_ec6g"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a quantization-aware variant of low rank adapter and a data-free training scheme  for fine-tuning quantized diffusion models. It introduces scale-aware techniques to optimize the weight quantization parameters. For activation quantization, this paper employs a separate activation quantization step-size parameter for each denoising time step. With tuning the low rank weight parameter adapters, this method can achieve  image generation performance comparable to QAT based methods with much lower fine-tuning cost. It firstly achieves FID score as low as 6.17 on conditional image generation on ImageNet 256x256 dataset with 4bit-weight, 4bit-activation diffusion model."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* This paper is the first to achieve very good image generation performance with W4A4 diffusion models and W2A8 diffusion models.\n\n* This paper introduces **low rank adapter** and **distillation loss** to fine-tune quantized diffusion models and achieve good results with relatively low cost than QAT methods."
            },
            "weaknesses": {
                "value": "* The experimental results listed in this paper are confusing and do not align well. The effectiveness is not very well proved. \n\n* This paper proposes **TLSQ**, which is quite similar to TDQ in [1]. TDQ is applicable to diffusion models with both continuous time and arbitrary discrete time steps . The paper should clarify the number of time steps used in TLSQ and discuss the settings.  \n\n[1] Temporal dynamic quantization for diffusion models."
            },
            "questions": {
                "value": "* In Table 2, the FID score of W4A4 model (6.17) is much lower than FP model (11.28) and W8A8 model (11.38) is comparable to FP model. Is there any possible explanation for that? And in the paper \"High-Resolution Image Synthesis with Latent Diffusion Models\", FID score of conditional generation on ImageNet 256x256 is 3.60, which is much lower than 11.28, why is there a gap? In the Appendix.A Table.A, the unconditional image generation on LSUN dataset, the FID score of W4A4 model is much worse than the FP model, how to explain the gap in these two set of experiments?\n\n* Table 3 shows the ablation study results. Does the **QALoRA** use LSQ algorithm to fine-tune the low rank adapter parameters and quantization step-size parameters?  \n\n* Are there any results on using LSQ method on quantized diffusion models on dataset other than Cifar10?\n\n* In **Data-free fine-tuning for diffusion models** part, is $\\mathbf{x}_t$ in Eq(7) sampled from Gaussion noise with an FP model?\n\n* In **Variation of activation distribution across steps** part, it proposes to assign a separate step size parameter for activation quantization in each denoising time step, and the results shown in Table 2 are obtained from 20-step sampling. Is the total time steps fixed to 20 for the fine-tuning. Is the data-free fine-tuning in Eq(7) fixed for 20 steps?\n\n* In Sec3.2 Eq(3), the quantization scheme has three parameters, $l, u, s$, are they all trainable? If so, is it optimized with LSQ [2] or LSQ+ [3] algorithm?\n\n\n[1] High-Resolution Image Synthesis with Latent Diffusion Models.\n\n[2] Learned step size quantization.\n\n[3] LSQ+: Improving low-bit quantization through learnable offsets and better initialization."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3007/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699535446289,
        "cdate": 1699535446289,
        "tmdate": 1699636245020,
        "mdate": 1699636245020,
        "license": "CC BY 4.0",
        "version": 2
    }
]