[
    {
        "id": "0YOEFCMBXL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9012/Reviewer_gxVT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9012/Reviewer_gxVT"
        ],
        "forum": "vmlwllg7DJ",
        "replyto": "vmlwllg7DJ",
        "content": {
            "summary": {
                "value": "> **TL;DR:** The proposed GrowLength method progressively increases the LLM training length throughout the pre-training phase, thereby mitigating computational costs and enhancing efficiency. However, I find the paper lacking comparison to the common BERT two phase pre-training approach which increases the context window in the second phase. Addressing my concerns and questions would improve my score, specifically W.1 and W.2.\n\nThe paper proposes the GrowLength method to reduce the computational cost of training LLMs. The high computational cost of LLMs is an ongoing challenge with plenty of recent research discoveries. Contrary to the fixed sequence length in the pretraining, the proposed GrowLength method utilizes a dynamic, progressively growing training sentence length. The superiority of this method lies in its adaptability and its capacity to significantly optimize the utilization of computational resources, enabling models to process more tokens in a constrained time frame."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* **S.1.** The proposed GrowLength algorithm tackles an important problem in the computational costs of training LLMs.\n* **S.2.** The experiments show that the GrowLength method outperforms the common constant context length approach.\n* **S.3.** The paper provides results on models of different sizes."
            },
            "weaknesses": {
                "value": "* **W.1.** The paper lacks comparison to the BERT [1] pre-training, which used a two-step pre-trianing approach with a growing context window length.\n* **W.2.** The figures are confusing with different arrows pointing at the lines with same colors (Figure 3 & 5).\n* **W.3.** The experiments are conducted on a single neural architecture and the provided architecture sizes are considerably small compared to existing LLMs.\n\n[1] Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. \"Bert: Pre-training of deep bidirectional transformers for language understanding.\" arXiv preprint arXiv:1810.04805 (2018)."
            },
            "questions": {
                "value": "* **Q.1.** Where are GrowLength-1 and GrowLength-2 defined?\n* **Q.2.** How would the GrowLength method work with substantially larger model 7B+?\n* **Q.3.** How would the GrowLength method work with substantially context windows?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9012/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697123203611,
        "cdate": 1697123203611,
        "tmdate": 1699637135438,
        "mdate": 1699637135438,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6FXNlehMhe",
        "forum": "vmlwllg7DJ",
        "replyto": "vmlwllg7DJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9012/Reviewer_w2Jc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9012/Reviewer_w2Jc"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a simple but effective training strategy for LLMs, namely GrowLength, which changes the data loader, by progressively feeding longer training data during the pre-training phase. In this way, the model training process can be accelerated. GrowLength is motivated by the context windows extension methods for fine-tuning, which indicates that model trained with shorter sequence can also benefit tasks with longer inputs, and employ the direct positional extrapolation for implementation. Many experiments are presented in this paper, showing that the motivation is reasonable, and the proposed GrowLength is effective."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper targets a little-explored aspect for LLMs, which is adjusting training data to accelerate training process. Since the training costs for LLMs are huge, I think this paper targets a very important research question.\n- The presentation in this paper is clear. This paper is well-written, and results are clearly demonstrated with figures or tables.\n- Plenty of experiments in this paper make the proposed idea convincing. The motivation of the paper comes from some experimental observations, regarding the computational complexity of LLMs with varying lengths of sequences. And the proposed GrowLength is evaluated and analyzed from multiple aspects, including training time, training loss, model size, and so on."
            },
            "weaknesses": {
                "value": "- Some work related with general language processing but not LLMs is not discussed/compared, such as Curriculum Learning for Natural Language Understanding (https://aclanthology.org/2020.acl-main.542.pdf ). The proposed method is quite related to curriculum learning, where an easy-to-difficult curriculum is arranged for model training. This paper also has a baseline, which uses question length/paragraph length as difficulty metrics. \n- How to determine the growing context window size during training (like 128, 256, 512, \u2026) is not rigorously studies. Will this exponential growth be too fast, especially for the latter stage, or even longer sequence (e.g., 1B tokens)? And maybe direct positional extrapolation will not work well when growing too fast.\n- The unique challenge when applying content window extension to pretraining stage is not very clear. It seems that simply using the technique proposed for fine-tuning also works well.\n- No final testing results are presented. Although this paper is working on optimization but not generalization. But final performance on testing set of various tasks is the thing that matters a lot. Besides loss, these results should also be included and analyzed."
            },
            "questions": {
                "value": "- Will this method be sensitive to the length distribution of the training data? If the pre-training dataset has few samples with short sequences, will GrowLength still be effective?\n- The first observation presented before section 3 is not very rigor. Context window extension methods are proposed because LLMs are pre-trained with a fixed context window, but we would like to apply them to tasks with longer sequences. So, these methods just tell us that context length can be extended, but not say \u201ctrained with shorter sequence lengths has proven to be more effective than training with long sequences\u201d. Not sure what the author is trying to say here.\n- What LLM is used in all experiments? I am curious about how this method works in experiments when applied to LLMs which already have some techniques for accelerating and handling long context, such as GQA in llama2 and FastAttention, given that the authors have explained that they are orthogonal, but experiments are more convincing here.\n- Table 1 2 3 might be better presented with figures.\n- Typos: At the end of the 3rd line in Sec. 4.4, the full stop should be removed. Also the final sentence in this paragraph, the latter one should be GrowLength-2."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9012/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698629502412,
        "cdate": 1698629502412,
        "tmdate": 1699637135306,
        "mdate": 1699637135306,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1oGVqZ32k3",
        "forum": "vmlwllg7DJ",
        "replyto": "vmlwllg7DJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9012/Reviewer_jdbk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9012/Reviewer_jdbk"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to progressively increase input length to accelerate training. By utilizing the functionality of RoPE embedding, the longer sequence is able to adapt the model trained with shorter sequences."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Good writing;\n- The experiments show that progressively growing sequence length can accelerate the training process."
            },
            "weaknesses": {
                "value": "- My largest concern lies in the validation of the experiment. The model is trained around 20000s (5.56h) at most, which is too short to validate the pretraining process since the model is far away from convergence. Notice that [1] trains model for 300B tokens, and a 160M LLM usually needs several days to converge on 16 V100s, to my knowledge. I completely understand that resource demand is high for researchers; however, it is hard for me to agree with the conclusion from the present experiment.\n- A shorter sequence surely leads to a smaller computation complexity. Therefore, this insight is not really original.\n- Typos:\n\t- \"while GrowLength-1 is trained with more tokens.\" -> while GrowLength-2 is trained with more tokens;\n\t- Lacks the conference of [1] in the paper;\n\t- etc.\n\n\n[1] Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hal- lahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing large language models across training and scaling, ICML 2023."
            },
            "questions": {
                "value": "- In Sec 4.4, how many extra tokens are used by GrowLength-2?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9012/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698677775149,
        "cdate": 1698677775149,
        "tmdate": 1699637135176,
        "mdate": 1699637135176,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iTVXGtZXkF",
        "forum": "vmlwllg7DJ",
        "replyto": "vmlwllg7DJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9012/Reviewer_TKo2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9012/Reviewer_TKo2"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes GrowLength, a pre-training strategy to progressively increase the sequence length of the training data in stages. The authors propose to use position interpolation or extrapolation to use the trained model to unseen sequence lengths. The models use relative position embeddings (ROPE) and the authors discuss the utility of the embeddings for such progressive training. On multiple model scales, the authors show the efficacy of their method with training time compared to baseline training."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The main strength of the paper lies in its easy-to-understand logic to use progressive sequence length training with ROPE embeddings. The authors take insights from position interpolation and extrapolation works and ROPE embeddings to develop the GrowLength algorithm. Furthermore, the authors clearly point out the memory and training time benefits of different input sequence lengths. Furthermore, with ablations, the authors show the algorithm's limited dependence on different progressive training schedules."
            },
            "weaknesses": {
                "value": "There are a few details that are unclear from the paper's presentation.\n\n(a) How do the authors transition between stages? Do the authors use position interpolation or extrapolation to provide a smooth transition when the sequence length increases? Furthermore, a comparison study to a different position embedding would highlight the importance of ROPE embeddings for a smooth transition across the stages.\n\n(b) How many sequence batches do the authors use for each stage of training? Is it proportionally set to the sequence length at each stage? \n\n(c) Related to my second question, if the batch sizes have been changed at each stage, have the hyperparameters (Learning rate, batch size, etc.) of the baseline experiments been optimally tuned for fair comparisons?\n\n(d) How does the improvement in perplexity relate to improvements in downstream performance? Any fine-tuned or zero-shot performance will show the general efficacy of the proposed method.\n\nThere are multiple works on efficient training that haven't been mentioned by the authors. It would be good to incorporate them to give the readers a complete view of the literature.\n\n(1) Stacking and Layerdrop: This is a procedure to progressively increase or drop the size of the model across multiple dimensions during the course of training. [1, 2, 3, 4]. [4] had also proposed a GrowLength algorithm to incorporate into their Stacking framework.\n\n(2) Optimization algorithm: This is a line of work that attempts to tweak the optimization algorithm to get faster pre-training. [5, 6]\n\n\n1: Efficient training of bert by progressively stacking. Gong et al. 2019\n\n2: On the transformer growth for progressive bert training. Gu et al. 2020\n\n3: Accelerating training of transformer-based language models with progressive layer dropping. Zhang et al. 2020\n\n4: Efficient training of language models using few-shot learning. Reddi et al. 2023.\n\n5:  Symbolic discovery of optimization algorithms.  Chen et al. 2023\n\n6:  A Scalable Stochastic Second-order Optimizer for Language Model Pre-training.  Liu et al. 2023"
            },
            "questions": {
                "value": "Please see my questions in my previous section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9012/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698752393850,
        "cdate": 1698752393850,
        "tmdate": 1699637135048,
        "mdate": 1699637135048,
        "license": "CC BY 4.0",
        "version": 2
    }
]