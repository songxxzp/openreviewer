[
    {
        "id": "YPHp7WV0xq",
        "forum": "hNhwSmtXRh",
        "replyto": "hNhwSmtXRh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3747/Reviewer_QCzC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3747/Reviewer_QCzC"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces Lemur and Lemur-Chat, openly-accessible large language models that harmonize natural language with code capabilities. The Lemur models are trained on the basis of Llama-2, with a code-centric pre-training stage with a code-to-text ratio of 10:1 for code-text harmonization, and a supervised instruction fine-tuning stage. The authors conduct systematic and comprehensive evaluations of Lemur models on diverse benchmarks, consisting of fundamental code/language benchmarks and pratical scenarios that connect LLMs to environments. The paper categorizes the capabilities of LLM agents in four aspects: agument with tools, self-debug, following feedback, and exploring environments. Over extensive benchmarks, the experimental results demonstrate harmonized capabilties between natural language and codes, and show that the Lemur models consistently outperform their counterparts on a wide range of tasks."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The idea of harmonizing the natural language and coding capabilities of LLMs is nice. With carefully designed code-to-text ratio and the selection of training data, the resulting Lemur models achieve a harmonious blend of language and coding capabilities.\n- The resulting Lemur models achieve competitive performance on language-coding tasks against gpt-3.5-turbo. The open-sourced Lemur models will be useful for the research community, and would be foundation models to develop agents.\n- The experiments are solid and evaluations are systematically organized. The Lemur models are evaluated in a clear and comprehensive evaluation process. The evaluation consists of the evaluations in each domain of code or language, and diverse code-language tasks that are grouped into 4 types of skills, establishing a good evaluation procedure for language-code LLM agents.\n- The paper is clear and concise with well-structured evaluations."
            },
            "weaknesses": {
                "value": "- As mentioned in Introduction, the paper has offered valuable insights on synergy, but it is unclear what the insights exactly are. I would suggest clearly presenting the insights instead of letting readers find where is the insights across the paper.\n- Minor: In Figure 2, the capitalizations are not consistent. (Use->,  run->); Section 4.5: mapp -> map; Section 4.5: intermm."
            },
            "questions": {
                "value": "- Why is a large proportion of the pre-training data is in Python?\n- Is the harmonization controlled by the text-to-code ratio? How did you come up with the idea of setting a ratio of 10:1?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3747/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3747/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3747/Reviewer_QCzC"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3747/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698870248093,
        "cdate": 1698870248093,
        "tmdate": 1699636330969,
        "mdate": 1699636330969,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5htrvDabam",
        "forum": "hNhwSmtXRh",
        "replyto": "hNhwSmtXRh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3747/Reviewer_JHRb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3747/Reviewer_JHRb"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes Lemur and Lemur-Chat language models, emphasizing their combined proficiency in both natural language understanding and coding capabilities. These models are designed to bridge the gap between understanding human interactions and manipulating code, aiming to serve as versatile language agents.\n\nwhat contributions does it make:\n1.The proposed models Lemur and Lemur-Chat narrow the gap with proprietary models in terms of agent abilities, leveraging its harmonization of both natural language and programming languages.\n2.Provide comprehensive evaluations of language and coding abilities.\n3.These models are open-source, providing a valuable resource for the community and potentially contributing to the development of advanced open-source agents that can be seamlessly reasoned, planned, and run in a variety of environments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.It improves the coding ability while maintaining the reasoning ability of Llama-2.\n2.The Lemur is pre-trained and fine-tuned using a rich dataset that includes text and code, ensuring a balance of performance across a variety of text and coding benchmarks.\n3.The model showcases proficiency in agent tasks, encompassing human communication, tool usage, and interaction across observable environments."
            },
            "weaknesses": {
                "value": "1.It seems that pre-training takes the responsibility to gain the coding ability, and the supervised fine-tuning takes the responsibility to gain the natural language ability, while it is vague how the proposed model balance these two abilities. \n2.As shown in Tables 4, 5, and 7, the performance of the proposed model Lemur-70B-Chat falls short when compared to GPT-4 and this discrepancy in performance lacks an explanatory or discussion.\n3.Table 3 lists three baseline models\u2014StarCoder-15B, StarCoderPlus-15B, and WizardCoder-15B\u2014without corresponding explanations or references in the provided context.\n4.Table7 does not have references and analysis."
            },
            "questions": {
                "value": "1.GPT4 also integrates text and code capabilities, what are the advantages of this paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3747/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698943830939,
        "cdate": 1698943830939,
        "tmdate": 1699636330887,
        "mdate": 1699636330887,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rcoenPg9JM",
        "forum": "hNhwSmtXRh",
        "replyto": "hNhwSmtXRh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3747/Reviewer_nTpx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3747/Reviewer_nTpx"
        ],
        "content": {
            "summary": {
                "value": "This paper presents Lemur and Lemur-Chat, large language models that exhibit balanced proficiency in both language and coding. The paper further trains LLAMA on a corpus with a code-to-text ratio of 10:1 and fine-tunes the model on four instruction datasets. The paper evaluates these two models across a broad spectrum of tasks, which includes text benchmarks (such as MMLU, BBH, etc.) and code benchmarks (such as HumanEval, MBPP, MultiPL-E, etc.). Moreover, the paper demonstrates that these models perform exceptionally well in language agent scenarios, such as augmenting with tools, self-debugging with environment feedback, adhering to natural language feedback, and exploring in partially observable environments."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This article validates the effectiveness of the Lemur model on a large number of benchmarks and verifies the importance of balanced language and coding capabilities for language agent scenarios."
            },
            "weaknesses": {
                "value": "1) The technical contribution of this article is quite limited, it merely continues training the LLAMA model on a mixture of text and code data and instruction tuning on four datasets.\n2) When comparing performance on the code benchmark, the authors use a large 70B model, but the code-specific models they compare with are mostly 15-30B in size, which makes the comparison somewhat unequal."
            },
            "questions": {
                "value": "Why you use 10:1 text-to-code ratio in your pretraining data?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3747/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3747/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3747/Reviewer_nTpx"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3747/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698981906717,
        "cdate": 1698981906717,
        "tmdate": 1700464241039,
        "mdate": 1700464241039,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "amgtShZj6Z",
        "forum": "hNhwSmtXRh",
        "replyto": "hNhwSmtXRh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3747/Reviewer_Ycpy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3747/Reviewer_Ycpy"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes two models Lemur and Lemur-Chat by training on a combined data of natural language and programming languages. Comprehensive experiments show that the proposed models show superior performance on 12 agent benchmarks."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "**Originality:** This paper proposes a novel way of training LLMs with code + text data to design language agents. \n\n**Quality:** There are detailed studies included in the paper about how training LLMs can be beneficial to solve both the language and agent tasks. \n\n**Clarity:** The paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "**Ambiguous Motivation:** I am fully not convinced with the sentence \"for the construction of language agents, it is imperative for language models to possess harmonized capabilities in both natural language and programming languages.\" Its unclear how programming languages correlate with language understanding. In fact in the context of linguistics (morphology, syntax and semantics), programming languages might not satisfy any of them. I believe the authors should provide more context for it. Although the experimental results show that Lemur-Chat outperforms on majority of the datasets, correlation does not imply causation."
            },
            "questions": {
                "value": "1. Is there any reason to choosing scripting languages?\n2. Is the performance replicable for base models other than Llama?\n3. How much does the size of Llama matter for experiments? Can the same pipeline be replicated for smaller Llama versions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3747/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3747/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3747/Reviewer_Ycpy"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3747/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699069461833,
        "cdate": 1699069461833,
        "tmdate": 1699636330743,
        "mdate": 1699636330743,
        "license": "CC BY 4.0",
        "version": 2
    }
]