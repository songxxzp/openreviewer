[
    {
        "id": "kcznp290kN",
        "forum": "6IjN7oxjXt",
        "replyto": "6IjN7oxjXt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7445/Reviewer_PH92"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7445/Reviewer_PH92"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new method to improve the trade-off between robustness and generalization in adversarial training.\nFirst, this paper investigates the difference between adversarial and clean representations layer-wisely,\nand next, it investigates overfitting in adversarial training when parameters of only some layers are selectively updated.\nBased on the observation that some layers tend to suffer from overfitting, this paper proposes CURE that leverages a gradient prominence criterion to perform selective conservation, updating, and revision of weights.\nTo evaluate the trade-off, this paper establishes a new metric: Natural-Robustness Ratio which is calculated by using accuracy against C&W and natural accuracy.\nCURE is evaluated in terms of this metric, robustness against several attacks including AutoAttack, and robustness against natural corruption."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper addresses an important problem in adversarial training: Overfitting and the trade-off between natural accuracy and robustness.\n- The detailed investigation of layer-wise learning phenomena in adversarial learning is novel and provides interesting insights.\nRevealed layer-wise properties might inspire researchers in this area and might cause new defense methods.\n- Gradient-based selective update for adversarial training is a new and interesting idea. \nThe figure of gradients in the training (Fig. 6) intuitively shows how the proposed method works by using the information of gradients well.\n- CURE is evaluated by using various attacks and architectures. However, baselines are not consistent and results might be cherry-picked."
            },
            "weaknesses": {
                "value": "- This paper lacks an ablation study. Although layer-wise analyses are interesting, the proposed method contains several components besides selective updating. How is the performance if we use only RGP? \nIf it is not good, are equations (4), (5), (7), and (8) relevant to the layer-wise analysis?\nIf other parts than the selective updating contribute to the performance, the layer-wise analysis may not be very worthwhile.\n\n- This paper does not present a fair and honest evaluation and presentation of the results of the experiments.\nThe trade-off metric is intentionally designed to make the proposed method look overly good. Is there a rational explanation as to why the C&W is used in the evaluation of trade-off, even though AutoAttack performs better than C&W in terms of attack success rate? I suspect that C&W is chosen because the numbers of metrics are not better in the case of AutoAttack. In fact, robustness against AutoAttack of the proposed method is not always greater than baselines.\nAdditionally, the vertical and horizontal scales in Figure 1 are not aligned, which can be misleading.\nNatural corruptions are selectively used from CIFAR10C. Their results may be cherry-picked. I would like to see the results against all natural corruptions in CIFAR10C. Baseline methods are not consistent over expreiments.\nWhy does Table 1 not contain the result of HAT, and does Table 2 not contain ACT, ARD, LAD, and LAS-AT?\n\n- The boundary between the proposed and existing methods is described ambiguously. Eqs. (4) and (5) seem to be an objective function of TRADES. Why are these equations written in the section of the proposed method?\nAdditionally, SMU seems to be exponential moving average (EMA) with a stochastic parameter. SEAT (Wang & Wang, 2021) and other recent methods also use EMA, which is sometimes called weight averaging. Unlike them, the proposed method uses averaged parameters in the regularization term. I would like to see the comparison between averaging weights directly and using averaged weights for regularization.\n- Minor issues\n    - [a] might be related work that addresses the trade-off and focuses on the difference between the representations of clean data and adversarial examples. The method in [a] outperforms LAS-AT in terms of the trade-off. Since it seems to be concurrent work, I think that it is not necessary to compare.  \n    [a] Suzuki S et al. \"Adversarial Finetuning with Latent Representation Constraint to Mitigate Accuracy-Robustness Tradeoff.\" ICCV 2023."
            },
            "questions": {
                "value": "- How is the performance if we use only RGP? If it is not good, are equations (4), (5), (7), and (8) relevant to the layer-wise analysis?\n- Is there a rational explanation as to why the C&W is used in the evaluation of trade-off? Is there any reasonable explanation for Natural-Robusthess Ratio? Why is eq.(9) suit to evaluate the trade-off?\n- How does NRR become if using AutoAttack?\n- What is the difference between TRADES and eqs.(4) and (5)?\n- Why does Table 1 not contain the result of HAT, and does Table 2 not contain ACT, ARD, LAD, LAS-AT?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7445/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7445/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7445/Reviewer_PH92"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7445/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698494506303,
        "cdate": 1698494506303,
        "tmdate": 1701047785218,
        "mdate": 1701047785218,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lsjYtFJ6iu",
        "forum": "6IjN7oxjXt",
        "replyto": "6IjN7oxjXt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7445/Reviewer_i2U9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7445/Reviewer_i2U9"
        ],
        "content": {
            "summary": {
                "value": "This paper unveils the underlying factors of the trade-off between standard and robust generalization in adversarial training by examining the layer-wise learning capabilities of neural networks during the transition from a standard to an adversarial setting. The paper demonstrates that selectively updating specific layers while preserving others can substantially enhance the network's learning capacity empirically, and proposes a method to leverage a gradient prominence criterion to perform selective conservation, updating, and revision of weights named CURE. The paper verified the effectiveness of CURE on various dataset and architecture, which verifying the effect in enhancing the trade-off between robustness and generalization and alleviating robust overfitting empirically."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper has good originality, high quality and clear expression. The paper unveils the trade-off between standard and robust generalization in adversarial training in the perspective of layer-wise learning capabilities of neural networks and proposes a new method to alleviate robust overfitting."
            },
            "weaknesses": {
                "value": "The analysis of selective adversarial training are empirically not theoretically.It's better to provide theoretically analysis of selective adversarial training."
            },
            "questions": {
                "value": "1.Is the proposed method still works well on larger dataset, for example ImageNet?\n2.There are so many hyperparameters such as \u03b1,\u03b2,\u03b3,r,d, how to mediate so many hyperpapameters effectively?\n3.For deeper neural networks, such as resnet-101, is the proposed method still works well?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7445/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698721715689,
        "cdate": 1698721715689,
        "tmdate": 1699636894093,
        "mdate": 1699636894093,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZXnyh7akQA",
        "forum": "6IjN7oxjXt",
        "replyto": "6IjN7oxjXt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7445/Reviewer_HvAu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7445/Reviewer_HvAu"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a novel approach to improving the adversarial robustness of DNNs while maintaining the performance on natural samples. The authors first conduct empirical studies to discover that updating weights in all layers in AT may be not good for the generalization of DNNs in both natural and adversarial samples. Thus, they propose an adaptive method to selectively update a subset of weights in the DNN. The proposed method is simple but empirically effective."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "This paper presents a simple yet effective method of improving both robustness and generalization of DNNs.\n\nThis study discovers some interesting phenomena, e.g., updating middle layers is beneficial to standard and robustness generalization, and adversarial training increases the similarity between features of different layers.\n\nThe paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "- The authors repeatedly use \u201coverwritten\u201d and \u201clearning\u201d, but I cannot grasp the essential difference between them. What kind of update of weights is referred to as \u201coverwritten\u201d and \u201clearning\u201d, respectively? Can you provide a clearer definition for the two terms? Besides, how do the authors come to the conclusion about \u201coverwritten\u201d and \u201clearning\u201d from the accuracy drop in Figure 2(a)? \n- In Section 2, the authors update the weights of selected layers by re-initializing and updating them. Have you ever tried not re-initializing but directly finetuning them with adversarial examples? Learning weights from random noises, especially weights in shallow layers may lead to a significant drop in performance, but finetuning them will not.\n- I\u2019m a little confused about Figure 3. If the weights of deep layers (e.g. U-34) are updated while shallow layers (e.g. blocks 1 and 2) are fixed, the features in shallow layers are also supposed to be frozen. Thus, the similarity between shallow layers in U-34 should be the same as in the ST model. However, the similarities between shallow layers in Figure 3 are all different.\n- Although this paper presents some differences in performance between updating different layers, it does not clearly \u201cdisentangle clean and adversarial representations\u201d or \u201cdisentangle robust and non-robust features\u201d. Also, this paper does not strictly define and extract layers with \u201cgreater learning capacity\u201d. I suggest the authors use such claims carefully and seriously.\n- Figure 4 indicates that updating high layers may cause robustness overfitting, but Figure 6 (b) shows a large ratio of updated gradients in high layers. I expect the authors to discuss more about such a misalignment.\n- How about the representation similarity between different layers in the DNNs trained by CURE?"
            },
            "questions": {
                "value": "- Is CURE used by training the model from scratch with the loss function in equation (5) or finetuning a pre-trained network? If the CURE is adopted on a pre-trained model, I am concerned that it may be unfair to compare it with other training-from-scratch methods. Besides, it also increases the computational cost to train the model twice (pre-train with ST and then finetune with CURE). \n- How stable is CURE when using different hyper-parameters ($\\alpha,\\beta$ and $p$)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7445/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698752988861,
        "cdate": 1698752988861,
        "tmdate": 1699636893981,
        "mdate": 1699636893981,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tMT4tXPLQv",
        "forum": "6IjN7oxjXt",
        "replyto": "6IjN7oxjXt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7445/Reviewer_DhtN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7445/Reviewer_DhtN"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on the trade-off between standard and robust generalization. To this end, this paper proposes CURE that leverages a gradient prominence criterion to perform selective conservation, updating, and revision of weights, which can tackle both memorization and overfitting issues."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The problem authors focused on is very interesting."
            },
            "weaknesses": {
                "value": "1. Experimental results in Fig. 2 cannot support authors' claim that \".. cause reduced performance on both data due to overwriting of learned information..\" Specifically, just a comparison of accuracy in Fig. 2  cannot reflect the overwriting of learned information. Authors should design new solid experiments to support this conclusion.\n2. Authors did not clarify how to disentangle robust and non-robust features, which still presents a significant challenge. Hence, Fig.3 is in doubt.\n3. I wonder why \"a subset with the most significant impact on accuracy\" equals to the subset of weights that \"contribute more to the joint distribution of both natural and adversarial accuracy.\" Can you prove it or explain it?\n4. A algorithm flowchart will help readers better understand how weights are updated in each epoch. In each epoch, are different or same subsets of weights updated?\n5. What does \"sample\u223cU(0,1)<r\" in Eq. 7 mean?\n6. Experimental results cannot verify the effectiveness of the proposed CURE method, since authors just conducted experiments on resnet18 and resnet34. Please conduct more experiments on more classic DNNs."
            },
            "questions": {
                "value": "In this paper, many conclusions are not supported authors' experimental results, i.e., we cannot infer these conclusions just based on existing experimental results.\nThus, many conclusions in Section 3 are over-claimed.\nDetails are stated in weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7445/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698822159095,
        "cdate": 1698822159095,
        "tmdate": 1699636893867,
        "mdate": 1699636893867,
        "license": "CC BY 4.0",
        "version": 2
    }
]