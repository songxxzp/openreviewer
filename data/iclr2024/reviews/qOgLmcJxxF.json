[
    {
        "id": "aF9bG4b6Iy",
        "forum": "qOgLmcJxxF",
        "replyto": "qOgLmcJxxF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6653/Reviewer_yv9g"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6653/Reviewer_yv9g"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the sample complexity associated with training score-based diffusion models. It introduces an essential concept: the $1-\\delta$ error, a more robust measure compared to the conventional L2 error metric. It is demonstrated that using the $1-\\delta$ error, efficient training can be achieved with a sample complexity of poly($log (m^2/\\gamma)$) when employing score matching.This new measure proves sufficient for enabling efficient sampling through reverse SDE."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper introduces the concept of the $1-\\delta$ error as a more robust measure for diffusion models, which is a novel and innovative contribution to the field. This new measure offers an alternative approach to assessing score estimation precision;\n\n2. The paper effectively addresses the crucial issue of sample complexity in training diffusion models. It recognizes the challenge of balancing score estimation accuracy with the number of required samples, providing insights into how to achieve efficient training.\n\n3. By introducing the $1-\\delta$ error as a foundational measure for future work, the paper provides a valuable reference point for researchers looking to advance the field of diffusion models."
            },
            "weaknesses": {
                "value": "I believe the presentation of this paper can be improved and there are several typos:\n1. After equation (1) on page 1, it writes 'for Brownian motion $dB_t$', while $dB_t$ is NOT the Brownian motion (actually $B_t$ is);\n2. On page 1, it writes $x_t\\sim e^{-t} x_0 + N(0, \\sigma_t^2), which seems like $x_t$ is restricted in $\\mathbb{R}$, but after equation (2) it looks like $x_t$ is a process in $\\mathbb{R}^d$ before $d$ is even defined;\n3. Also on page 1, it writes 'logarithmic in $m_2/\\epsilon$', where neither $m_2$ or $\\epsilon$ is defined before. Actually $m_2$ is defined in Theorem 1.2 for the first time;\n4. Equation (4) seems to refer to the minimizer, then it should be 'argmin ...' instead of 'min ...';\n5. Before equation (5), I can understand what you mean by 'with norm bounded by R', but I think it's not a commonly used expression. Maybe you can try something like 'supported in B(0,R)' instead?\n6. Before equation (6), we see $m_2$ again, still undefined;\n7. After equation (6), 'to satisfy' should be 'to be satisfied' instead;\n8. In Lemma 4.1, the parameter $\\eta$ depends on $m$, while $m$ is defined in the proof;\n9. In Lemma 4.1, I guess $\\hat{\\mathbb{E}}$ means the empirical expectation, but it is not defined in the paper.\n10. In Theorem A.1, the notation of $s_r$, $s^*_r$, $\\hat{s}_r$ and $\\tilde{s}_r$ is a bit messy.\n11. A typo! The title of Appendix D should be 'Utility Results' instead of 'Utility Resuts' I think."
            },
            "questions": {
                "value": "1. In the part named Our results, what's the intuition of 'one would like to show ... with poly($log \\frac{m_2}{\\gamma}$)'? Where does this poly($log \\frac{m_2}{\\gamma}$) come from?\n2. In the remark after Corollary 1.1, it states that 'Corollary 1.1 doesn\u2019t depend on the domain size at all'. But H contains sufficiently accurate approximations to each score. Does it mean that $|H|$ should be related to the domain size?\n3. How is the discussion in section 4 related to that in Appendix D of [1]?\n4. On page 5, it states that 'If x is bounded by some value $R$, then the score would be bounded by $R/\\sigma^2$. Can you prove it or provide a reference? \n5. In Theorem A.1, what does it mean by 'r-smoothed version'?\n6. In Theorem A.1, are $\\hat{s}_r$ and $\\tilde{s}_r$ different? It seems that in the proof you assume they are equal (but actually they are not I think).\n\n[1] Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence for score-based generative modeling with polynomial complexity. arXiv preprint arXiv:2206.06227, 2022."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6653/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6653/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6653/Reviewer_yv9g"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6653/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698627347148,
        "cdate": 1698627347148,
        "tmdate": 1699636760743,
        "mdate": 1699636760743,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "R66KHSkxlx",
        "forum": "qOgLmcJxxF",
        "replyto": "qOgLmcJxxF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6653/Reviewer_KNkT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6653/Reviewer_KNkT"
        ],
        "content": {
            "summary": {
                "value": "This paper shows that estimating the score in L2 requires this polynomial dependence, but polylogarithmic samples actually do suffice for sampling."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The investigation of sample complexity is a core issue in statistics literature. For modern generative models, it is quite interesting to see the sample dependency of these systems.\n\nThis paper find a good angle that it is not necessary to learn the score accurately \nWhich is particularly challenging. This idea can help to justify the success of current diffusion models"
            },
            "weaknesses": {
                "value": "The writing of the paper should be improved. It contains too much previous work and technical details. The contribution of this work are scattered.\n\nToo many informal results, which makes readers hard to determine which parts are not rigorous.\n\nThe setting looks artificial compared to true score-based model."
            },
            "questions": {
                "value": "Take Figure 1 for example, in a real score-based model, we would not evaluate score near 0, because we construct the whole process from p to N(0,I). Then we only evaluate the score p_T near 0 rather than p_0. Since the p_T is highly smoothed, and the importance can also be reflected by the number of samples, the score is generally well-estimated in L2 sense. Otherwise the mixing of the algorithm is incorrect. In [1], it is shown that the score-based algorithm has a good mixing property. From this view, the proposed metric looks a bug, not feature?\nCan you explain this?\n\nI understand the challenges and hardness of learning L2, but there are still more examples needed to justify the new metric."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6653/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698821149415,
        "cdate": 1698821149415,
        "tmdate": 1699636760634,
        "mdate": 1699636760634,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "asihYPaAOB",
        "forum": "qOgLmcJxxF",
        "replyto": "qOgLmcJxxF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6653/Reviewer_uE9j"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6653/Reviewer_uE9j"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the complexity of training and sampling using score-based diffusion models. The authors focus on the sample complexity, i.e., the number of samples needed to reach a given accuracy measured e.g. in TV or Wasserstein-2 distance. The key result is an improvement from ${\\rm poly}(R/\\gamma)$ to ${\\rm poly}\\log(m_2/\\gamma)$ samples, where $R$ is the bound on the norm of the distribution, $\\gamma$ the required accuracy in Wasserstein-2 distance, and $m_2$ the second moment. The scaling with respect to the other parameters (input dimension, 1/accuracy in TV distance) remains polynomial.\n\nThe idea is to consider a less restrictive measure for the estimation of the score: instead of the L2 norm, the authors propose a form of quantile error in which regions with low probability are not considered. In fact, they authors show that the estimation of the score in L2 requires polynomially many samples in $1/\\gamma$ via an explicit example."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "* The improvement in the sample complexity in terms of the accuracy in Wasserstein distance is significant: from polynomial to logarithmic.\n\n* The idea of using a different notion of distance to evaluate the error in the score estimation is new and could be useful more broadly when analysing score-based diffusion models. \n\n* I also appreciated the explicit counterexample on the difficulty of learning the score in L2."
            },
            "weaknesses": {
                "value": "* The scope of the paper is quite limited. While the improvement in terms of accuracy in Wasserstein distance is remarkable, the dependency on the accuracy in TV remains polynomial. What's the point of improving drastically the accuracy in W2, if the accuracy in TV remains bad? For this reason, while I like the idea of using the quantile measure, the benefit of doing so in the context of score-based diffusion remains unclear.\n\n* The complexity is also polynomial in $d$. Recent papers analysing diffusion models (see e.g. Table 1 in \"Linear convergence bounds for diffusion models via stochastic localization\") show that this dependency is linear or quadratic in $d$. I appreciate that the setting of this paper is different (most works assume access to a good L2 score, which is shown to be impossible if one sticks with a logarithmic dependency in $1/\\gamma$). Nonetheless, the authors should track how the bound scales in $d$ and compare to existing work.\n\n* It would also add value to the paper to track the dependency on something more explicit than the cardinality of the hypothesis class $|\\mathcal H|$. Along the same lines, for the result on neural networks (Theorem 3.1), one needs to assume the existence of a network that approximates the score well enough.\n\n* The novelty in terms of proof is not high. Basically the idea is to exclude a region with low probability and then use existing analyses (mostly, Benton et al., 2023). This is admittedly a minor point (simple ideas can be very useful!). The two key weaknesses above are the main reason of my score."
            },
            "questions": {
                "value": "Can the authors comment on the points raised above?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6653/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698832758829,
        "cdate": 1698832758829,
        "tmdate": 1699636760528,
        "mdate": 1699636760528,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "i9azaSwM58",
        "forum": "qOgLmcJxxF",
        "replyto": "qOgLmcJxxF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6653/Reviewer_3jfL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6653/Reviewer_3jfL"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the sample complexity for estimating the score functions in the diffusion model sampling process. They establish a polynomial sample complexity under a robust measure they proposed in the paper. They also apply their results to the sampling procedure by incorporating the convergence rate of Benton et al 2023. Their technique might be helpful to improve the sample complexity bound for diffusion based algorithms."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper studies the sample complexity of estimating the score functions in diffusion model. To evaluate the estimation error, they propose a robust measure of distance. For their main result, they show that if the true score function can be approximated well using the function class $H$, and $H$ is finite, then $m = poly(d, 1 / \\epsilon, 1 / \\delta, N, \\log |H| / \\delta_{train})$ samples is sufficient for getting $\\epsilon$ accuract simultaneously for all score functions. This rate does not require the target distribution is bounded, hence generalizes that of Block et al 2020."
            },
            "weaknesses": {
                "value": "1. I feel the presentation is not clear enough for readers to fully understand the merit of this paper. For more details see my question list. \n2. Only informal theorems are presented in the main text. What prevents the authors from adding formal theorems? \n3. Corollary 1.1 does not seem very surprising as $H$ is finite. The authors contain proof in their paper. But maybe it would be helpful to explain intuitively why it is interesting to establish Corollary 1.1, and why it is not a straightforward consequence of the uniform law of large numbers."
            },
            "questions": {
                "value": "1. I think $m_2$ appears a lot before the authors define it as the second moment. \n2. I think the statement \"approximates $q_0$ up to $\\eps$ TV error and $\\gamma$ Wasserstein-2 error\" in introduction section is not accurate. As far as I am concerned, the right expression is \"there exists a distributions $q$, such that $W(q, q_0) \\leq \\gamma$ and $TV(q, \\hat q) \\leq \\eps$, where $\\hat q$ is outputted by the diffusion model\" (at least this is the case in Benton et al 2023). The authors might want to clearly state that to avoid causing confusion.  \n3. What does it mean by \"$H$ contains sufficiently accurate approximations to each score\" in Corollary 1.1? \n4. What is the relation between the last sentence on page 3 and Theorem 1.2? Theorem 1.2 seems to be independent of training, why can we see from this theorem that our outlier-robust approximation suffices for sampling? \n5. Maybe the authors can comment a little bit on the polynomial dependency? Like what is the order of the polynomial. Block et al 2020 has an explicit polynomial dependency, and it is not clear if the results presented here is indeed better if the form of polynomial is not presented. \n6.  It is a little bit restrictive to assume $H$ is finite."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6653/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6653/Reviewer_3jfL",
                    "ICLR.cc/2024/Conference/Submission6653/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6653/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698873965353,
        "cdate": 1698873965353,
        "tmdate": 1700668043386,
        "mdate": 1700668043386,
        "license": "CC BY 4.0",
        "version": 2
    }
]