[
    {
        "id": "R87zQcHGTo",
        "forum": "cLIvvqf3Wk",
        "replyto": "cLIvvqf3Wk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5033/Reviewer_N8n1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5033/Reviewer_N8n1"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an unsupervised few-shot font generation method using diffusion models. Specifically, the authors use a character attribute encoder to extract content and style vectors of glyphs. Then, the encoded vectors are used to condition the diffusion model to generate fonts with desired content and style."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The manuscript is well-organized and easy to follow.\n2. From the qualitative results, the proposed method is superior to other few-shot font generation methods."
            },
            "weaknesses": {
                "value": "1. The novelty of this paper is limited. Currently, Diff-Font [1] has utilized the diffusion model to achieve few-shot font generation task. In addition, some techniques (such as classifier free guidance) and networks have been widely applied in the field of image generation.\n\n2. From Table 1, it can be seen that the proposed method is not superior to the existing few-shot font generation methods in quantitative evaluation. In addition, I suggest that the author add some comparative methods in the experiment, such as Diff-Font [1], MF-Net [2] and VQ-font [3].\n\n\n[1] He, H., Chen, X., Wang, C., Liu, J., Du, B., Tao, D., & Qiao, Y. (2022). Diff-Font: Diffusion Model for Robust One-Shot Font Generation. arXiv preprint arXiv:2212.05895.\n\n[2] Zhang, Y., Man, J., & Sun, P. (2022, October). MF-Net: a novel few-shot stylized multilingual font generation method. In Proceedings of the 30th ACM International Conference on Multimedia (pp. 2088-2096).\n\n[3] Pan, W., Zhu, A., Zhou, X., Iwana, B. K., & Li, S. (2023). Few shot font generation via transferring similarity guided global style and quantization local style. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 19506-19516)."
            },
            "questions": {
                "value": "This work seems to be a combination of some existing work. Are there any special designs made for scenarios with few samples?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5033/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698632383706,
        "cdate": 1698632383706,
        "tmdate": 1699636492641,
        "mdate": 1699636492641,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EJGJoN8YyH",
        "forum": "cLIvvqf3Wk",
        "replyto": "cLIvvqf3Wk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5033/Reviewer_28JC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5033/Reviewer_28JC"
        ],
        "content": {
            "summary": {
                "value": "The paper offers an approach to font generation, using the diffusion model to address the issues of training instability and model collapse commonly associated with traditional GAN methods. The method demonstrates stable training on large datasets and impressive qualitative and quantitative results. It is considered a significant contribution to the field, given the importance of the font generation problem. However, the paper needs to provide more detailed explanations for the addressed problems, better describe where innovation lies within its components, and clarify the implications of comparative results, particularly the substantial decrease in FID."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper introduces a novel generative model to font generation field, the diffusion model, which effectively addresses the issues of training instability and model collapse that traditional GAN methods often face.\n\n2. The proposed method demonstrates the ability to achieve stable training on large-scale datasets, which is a significant achievement in the field of font generation.\n\n3. The paper showcases impressive results in both qualitative and quantitative analyses, highlighting the potential effectiveness of the method.\n\n4. The paper tackles an important problem in font generation, and its \"Attribute-Guided Diffusion\" approach is considered a significant contribution to the field."
            },
            "weaknesses": {
                "value": "1. Lack of Detailed Explanation: The paper lacks a comprehensive explanation of the root causes of training instability and model collapse in traditional GANs and how the proposed diffusion model-based framework effectively addresses these issues. More detailed explanations and experiments are needed.\n\n2. Insufficient Innovation Description: The three components of the method appear to build on existing models, but the paper doesn't sufficiently clarify where innovation lies within these components or how they improve upon existing approaches.\n\n3. Inconclusive Comparisons: Comparative analysis with other methods, especially FUNIT, does not clearly demonstrate a substantial advantage in terms of visual perception. The paper compares its method to the FUNIT method in 2019, but the advantages of the proposed approach over FUNIT do not seem very pronounced in terms of visual perception. Similar issues exist when comparing the method to other approaches. While improvements are noted in terms of RMSE, SSIM, and LPIPS, there is a significant decrease in FID. It would be helpful to explain the reasons behind this change and whether it signifies the effectiveness of the proposed method."
            },
            "questions": {
                "value": "1. What are the root causes of training instability and model collapse in traditional GANs, and how does the proposed diffusion model-based framework mitigate these issues? Can this be supported with additional experimental evidence?\n\n2. Where specifically does the paper introduce innovation within the three method components (Character Attribute Encoder, Diffusion Process, and Classifier-Free Guidance Diffusion)?\n\n3. Could the paper provide more context and explanation regarding the noticeable decrease in FID when compared to other methods? What does this imply for the overall effectiveness of the proposed approach?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5033/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5033/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5033/Reviewer_28JC"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5033/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698655991727,
        "cdate": 1698655991727,
        "tmdate": 1699636492530,
        "mdate": 1699636492530,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gdjbaa1XYb",
        "forum": "cLIvvqf3Wk",
        "replyto": "cLIvvqf3Wk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5033/Reviewer_BnxP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5033/Reviewer_BnxP"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an attribute-guided diffusion model for font generation. \nThe network takes both content font and style font as input and encodes them into two latent codes, which are taken as the diffusion model to generate font with input content in a new style."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper is clear and easy to follow.\nBoth qualitative and quantitative results are shown to prove the effectiveness of the proposed method.\nIt is a good attempt to apply the diffusion model to the font generation task. \nUsing few shot might increase the stability of the font style transfer results."
            },
            "weaknesses": {
                "value": "Miss comparison with recent `Diff-Font: Diffusion Model for Robust One-Shot Font Generation`.\nThe idea of Diff-Font is very close to the proposed method though it only uses one style image as a reference. \n\nThe performance does not overpass the SOTA.\nI do not see any advantage of the proposed method in quantitative results in Table 1 (compared to CF-Font).\nIs there any reason for that?\n\nOne important thing about the font is usually vector graphics. \nIs there any possibility to convert the image into a vector representation?"
            },
            "questions": {
                "value": "As mentioned in the weakness part, my first question is on the performance level. \nCompared to CF-Font, it does not provide any benefit to the font generation task. \nIs there any reason for that?\n\nTo my understanding, the difference between Diff-Font and this paper is that this paper uses multiple style font references. \nI am wondering if this paper can show some single-style reference test results.\n\nThe quantitative results are worse than the current SOTA, is there any reason for this?\nIs it possible to show more qualitative results to better evaluate this pipeline?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5033/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698848336650,
        "cdate": 1698848336650,
        "tmdate": 1699636492443,
        "mdate": 1699636492443,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9Wu2vxw4f5",
        "forum": "cLIvvqf3Wk",
        "replyto": "cLIvvqf3Wk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5033/Reviewer_xmu4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5033/Reviewer_xmu4"
        ],
        "content": {
            "summary": {
                "value": "This paper describes an unsupervised few-shot font generation method based on diffusion model. Qualitative and quantitative results show the usefulness of the approach."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is overall well-written and easy to follow."
            },
            "weaknesses": {
                "value": "1. The paper, while offering insights, seems to lack substantial novelty. Its resemblance to the Diff-Font framework is notable, yet the paper does not adequately delineate the differences between its approach and that of Diff-Font. \n2. The experimental evaluation appears to be somewhat limited. The paper mentions over 60,000 Chinese characters, yet the evaluation only encompasses 1,000 characters. This selection might not fully represent the entire scope, potentially affecting the convincingness of the results. Moreover, there seems to be an absence of a significant evaluation metric: user studying. This metric is notably present in prior research, for instance in works like MX-Font, CG-GAN, CF-font, etc. Additionally, the evaluation focuses solely on Chinese characters. Although the paper acknowledges the intricacy of other languages such as Korean and Japanese, which also possess rich characters and complex structures, these languages have not been included in the evaluation. This exclusion could limit the comprehensiveness of the study's findings.\n3. The method's performance in quantitative comparison appears to be less effective than that of CF-Font."
            },
            "questions": {
                "value": "This paper appears to have limited novelty due to a lack of substantial original design elements. To enhance its innovative aspects, the authors might consider improvements in model design, particularly focusing on the interaction between the attribute encoder and the diffusion model. As it stands, the current version falls short of meeting the standards set by ICLR.\n\nFurthermore, while the authors effectively highlight the limitations of other methods, they do not provide examples of failure cases for their own approach. This omission is particularly noticeable given that the qualitative results presented seem superior. It is surprising that these promising qualitative outcomes are not reflected in superior quantitative results when compared to other methods. The authors are encouraged to identify and discuss the weaknesses of their proposed method. Doing so could offer valuable insights and direct attention towards potential areas for future improvement."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5033/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698915467704,
        "cdate": 1698915467704,
        "tmdate": 1699636492340,
        "mdate": 1699636492340,
        "license": "CC BY 4.0",
        "version": 2
    }
]