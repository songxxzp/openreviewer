[
    {
        "id": "7wpwhMqeGQ",
        "forum": "gU58d5QeGv",
        "replyto": "gU58d5QeGv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4208/Reviewer_MvDK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4208/Reviewer_MvDK"
        ],
        "content": {
            "summary": {
                "value": "This study presents an architecture designed for efficient text-to-image generation. The first text-conditional LDM produces a low-resolution latent map (Stage C), which is used for the second LDM for a high-resolution latent map (Stage B). This map is fed into a VQGAN-based decoder to produce a final image (Stage A), as performed in other LDM and SD models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The key distinction of this work from previous LDM and SD lies in the introduction of a two-stage latent diffusion process, facilitated by the Semantic Compressor. The authors argue that the additional guidance from low-resolution latent maps (Stage C) can help yield good results under a smaller training budget, compared to the conventional LDM framework's Stage B and Stage A.\n- I appreciate the efforts put into designing and training the Semantic Compressor and Stage C. This appears to be far from straightforward, representing methodological and empirical contributions."
            },
            "weaknesses": {
                "value": "- I'm uncertain about the inference efficiency of this approach, as it appears to add an \"extra\" computation (Stage C) on top of the conventional LDM and SD (Stage B and Stage A). In particular, how could the proposed method achieve better inference time than SD-v2.1 in Figure 4? A detailed computational comparison would be beneficial for different components in the system (the text encoder, LDM(s), and image decoder) instead of just an overall process. \n- I think the Baseline LDM (trained for 25,000 GPU-hours (same as Stage C)) needs to be trained for GPU hours of Stage B + Stage C, given that both stages contribute to the final latent representation of the proposed method. More importantly, a baseline with the same architecture of the upper part in Stage B, Figure 3 (i.e., a conventional LDM obtained by just removing Stage C and the below part of Stage B, Figure 3) seems necessary to show the benefit of the proposed approach.\n- The parameter values in Table 2 might confuse readers due to inconsistencies in their presentation. For some models, like LDM, the table seems to consider all the parameters, including the text encoder. Yet, for other models such as the proposed method and SD, only the diffusion parameters are listed. I strongly suggest presenting the \"total\" parameters (because several components work together for a single text-to-image system) or, preferably, detailing both the \"total\" and diffusion parameters separately.\n- The popular MS-COCO benchmark has been conducted at the resolution of 256x256. Why did the authors change the resolution for IS in Table 2? In my experience, the resolution affects the metric scores. Furthermore, for some models (LDM, DALL-E, CogView), the IS results at 256x256 were reported. I also highly recommend including CLIP score.\n- I think the description \u201cBy conditioning Stage B on low-dimensional latent representations, we can effectively decode images from a 16x24x24 latent space to a resolution of 3x1024x1024, resulting in a total spatial compression of 42:1\u201d in page 5 looks incorrect or overclaimed, because Stage B also takes a high-resolution latent map, 4x256x256, as input.\n- The behavior of the proposed model seems less explored. The representative analysis with different classifier-free guidance scales to show the tradeoff between FID-CLIP score [SD, GLIDE, Imagen] is missing. Furthermore, it would be interesting to analyze the tradeoff between the number of sampling steps and generation quality.\n- Minors: The paper is fairly easy to follow, but I think a careful proofreading is necessary: many typos exist.\n  - x -> \u00d7 (in many parts)\n  - In stage B, we utilize a -> Stage B\n  - Inception Score (IC) -> IS"
            },
            "questions": {
                "value": "Please refer to the Weaknesses in the above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4208/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4208/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4208/Reviewer_MvDK"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4208/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698568355891,
        "cdate": 1698568355891,
        "tmdate": 1700282070717,
        "mdate": 1700282070717,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "KsQL14YXIH",
        "forum": "gU58d5QeGv",
        "replyto": "gU58d5QeGv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4208/Reviewer_ix2S"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4208/Reviewer_ix2S"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new text-to-image diffusion model architecture in which a base diffusion model is conditioned on a highly compressed 2D latent space obtained from a second diffusion model. Concretely, the \"main\" diffusion model denoises a higher-resolution image (e.g., 256x256 latent or pixel space) but is being conditioned on 24x24 feature map of the image that is to be generated. The 24x24 feature map is obtained by another diffusion model that is trained on that feature space. The resulting model is faster to train and faster to sample from, since both training and sampling of the 24x24 diffusion model is cheap and the large diffusion model at higher resolution benefits from the additional conditioning of the first diffusion model."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The model architecture seems novel and based on the evaluation it seems to be faster to sample from while also being faster to train than other baseline models.\n\nThe paper builds on top of the latent diffusion architecture and outperforms similarly sized LDMs (and even Stable Diffusion 1 and 2) based on quantitative metrics and human user studies. Importantly, it does so while being faster to train and faster to sample from.\nThe evaluation is well done and compares against several strong baselines, performs severfal human user studies, and also highlights some weaknesses of the current model compared to other models (e.g. fewer high-frequency details).\n\nFurthermore, the model and code to reproduce will be released."
            },
            "weaknesses": {
                "value": "The approach is only tested on latent diffusion models. While there is no reason to believe it wouldn't work on pixel diffusion models it would be nice to verify this."
            },
            "questions": {
                "value": "Since the Semantic Compressor is one of the main novelties I wonder if you tested other feature extractors (e.g., could also use CLIP or Dino) and how that would affect training and quality. Or by simply training an autoencoder with strong compression rate instead of using a pretrained feature extractor?\nAlso, did you try other model architectures for the Stage C model (e.g., transformer based models) instead of only the ConvNext blocks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4208/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698799110557,
        "cdate": 1698799110557,
        "tmdate": 1699636387798,
        "mdate": 1699636387798,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Tq2Za8aPzx",
        "forum": "gU58d5QeGv",
        "replyto": "gU58d5QeGv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4208/Reviewer_b5h8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4208/Reviewer_b5h8"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a new latent representation for images that can serve as compact semantic guidance for the current denoising diffusion process. Specifically, the proposed Wurstchen framework employs three stages of decoupling text-conditional image generation from high-resolution spaces. This supports an efficient optimization, which significantly reduces computational requirements for large-scale training. This architecture also enables faster inference."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "+ This paper is well-written and easy to follow.\n+ The field of efficient training is less discussed than inference, which makes this draft more valuable.\n+ The Wurstchen framework can reduce ~9X GPU training hours yet maintain competitive T2I performance.\n+ They provide comprehensive qualitative examples in the supplementary. The released code and checkpoint can benefit generative AI research."
            },
            "weaknesses": {
                "value": "I am satisfied with the current draft. As it targets robust latent visual representations, there should be a detailed analysis (e.g., the quality of the latent features / the distribution of the compression space). This can make its claim more convincing."
            },
            "questions": {
                "value": "Please see the Weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Discrimination / bias / fairness concerns",
                    "Yes, Potentially harmful insights, methodologies and applications"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Wurstchen is trained on LAION-5B, which may contain potentially harmful data and influence the trained T2I model."
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4208/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699044710623,
        "cdate": 1699044710623,
        "tmdate": 1699636387714,
        "mdate": 1699636387714,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QE5sUOsWuv",
        "forum": "gU58d5QeGv",
        "replyto": "gU58d5QeGv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4208/Reviewer_CAkw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4208/Reviewer_CAkw"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an efficient architecture for large-scale text-to-image diffusion models. It presents a novel text-to-image generation model that utilizes a three-stage process for improved efficiency and superior output quality. With its unique ability to separate text-conditional generation from high-resolution projection, this model demonstrates superior performance over existing models, requiring fewer computational resources without compromising image quality. Evaluations with both automated metrics and human assessments substantiate its effectiveness."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(1) This study tackles an important topic of reducing the computational cost of text-to-image diffusion models.\n(2) The method introduced in the study is both innovative and efficient, offering clear results and validating its effectiveness through extensive evaluations.\n(3) The paper is well written, and one can quickly grasp the main idea and technical designs."
            },
            "weaknesses": {
                "value": "(1) Ablation study is missing. An understanding of the impact of different model components on the final results is desired.\n(2) For automatic evaluation metrics in Section 4.1, only FID and Inception score are evaluated, and there are no metrics evaluating how well the generated images are aligned with the input text instructions, such as CLIPScore.\n(3) The paper does not elaborate on the possible limitations or potential failure cases of the proposed method. Could the authors clarify this aspect?"
            },
            "questions": {
                "value": "Please refer to the weakness section. I expect the authors to clarify the questions about the ablation study and evaluation metrics in the rebuttal."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4208/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699260231681,
        "cdate": 1699260231681,
        "tmdate": 1699636387648,
        "mdate": 1699636387648,
        "license": "CC BY 4.0",
        "version": 2
    }
]