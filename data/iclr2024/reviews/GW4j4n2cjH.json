[
    {
        "id": "0IzM7akLsa",
        "forum": "GW4j4n2cjH",
        "replyto": "GW4j4n2cjH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2432/Reviewer_JwGL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2432/Reviewer_JwGL"
        ],
        "content": {
            "summary": {
                "value": "This work introduces a novel task in 3D dance generation called dance accompaniment. The task involves generating responsive movements from a dance partner, or follower, synchronized with the lead dancer's movements and the underlying musical rhythm. The authors also introduce a large-scale and diverse duet interactive dance dataset called DD100, recorded from professional dancers' performances. A GPT-based model named Duolando is proposed, which predicts motion sequences based on coordinated information from music, leader movements, and previous follower sequences. The model is further enhanced with an off-policy reinforcement learning strategy to generate stable results in unseen conditions."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "(1)\tThe paper introduces a new task called dance accompaniment.\n\n(2)\tThe DD100 dataset containing 115.4 minutes is collected for task training and evaluation. \n\n(3)\tThe proposed Duolando model utilizes a GPT-based approach to predict follower movements based on coordinated information from music, leader movements, and previous follower sequences.\n\n(4)\tThe incorporation of an off-policy reinforcement learning strategy enhances the model's ability to generate stable results on unseen conditions."
            },
            "weaknesses": {
                "value": "This work introduces a new dance accompaniment task, which is interesting and promising for dance generation community. I have some questions and suggestions. \n\n(1)\tTo gain a better understanding of the differences between various dance styles within the dataset and across datasets, it would be beneficial to provide more data statistics, such as movement speed and action distribution. More details on data processing would be helpful to understand it. \n\n(2)\tIn Equation 1, it's stated that both p and M are inputs, but in Figure 3, only p is shown as an input. \n\n(3)\tIt would be interesting to explore which signal, between music and leader motion, has a more dominant influence. For instance, when swapping the music between two test cases in the test set, and using music(B) with leader motion(A) and music(A) with leader motion(B) as inputs, how does this affect the results?\n\n(4)\tThere is some ambiguity in the terminology between \"duet dance generation\" and \"dance accompaniment.\" It's important to differentiate them more clearly in the draft. Additionally, it's worth noting that this dataset potentially holds promise for duet dance generation, where only music serves as input to generate movements for two individuals."
            },
            "questions": {
                "value": "See weaknesses. \n\n(1)\tMore details on data processing and Figure 3.\n\n(2)\tExperiment to explore the importance of signal. \n\n(3)\tQuestions in writing."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2432/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698657212245,
        "cdate": 1698657212245,
        "tmdate": 1699636178813,
        "mdate": 1699636178813,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QzlPUgPj2e",
        "forum": "GW4j4n2cjH",
        "replyto": "GW4j4n2cjH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2432/Reviewer_CR8y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2432/Reviewer_CR8y"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a method for generating dance accompaniment with respect to a leader dancer dancing under a music piece. The method consists of a series of VQVAEs for encoding body motions, music encoder, and a GPT for autoregressive modeling the conditions and follower tokens. In addition, an off-policy RL method is employed to improve the generation quality. In terms of the dataset, the authors collect a duet dance dataset from professional dancers along with the music."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The paper presents a novel task, generation of a dance follower conditioned on the other dance along with the music.\n\n2. The author contributes a dataset and the motion part is accurately captured using MoCap sensor."
            },
            "weaknesses": {
                "value": "1. How is music represented? Midi or spectrogram or raw waveform? I didn\u2019t see where the paper defined the representation of it.\n\n2. An important ablation study is missing, what if the music is not fed as input. I think it still makes sense to generate a follower dancer as the rhythm of music is already embedded in the leader dancer?\n\n3. The RL setup aims to handle the OOD data for unseen dance motions, and the authors present an off-policy learning approach. However, I didn\u2019t see ablations against on-policy RL nor ablations on the reward design, which looks very ad-hoc to me.\nThe supplementary material provides videos for original dataset and generated samples. However, I find some samples where actions are not well-aligned with music beats at all, I doubt whether it is something performed by professional dancers. Also, from the generated samples, I saw that sometimes part of a body model passing through part of another body model in an unnatural manner."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2432/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2432/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2432/Reviewer_CR8y"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2432/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698711215604,
        "cdate": 1698711215604,
        "tmdate": 1699636178737,
        "mdate": 1699636178737,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "h9VA3kLwNn",
        "forum": "GW4j4n2cjH",
        "replyto": "GW4j4n2cjH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2432/Reviewer_otBs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2432/Reviewer_otBs"
        ],
        "content": {
            "summary": {
                "value": "- This paper proposed an interesting human-human interaction task called dance accompaniment generation, which aims to generate human dancing motions conditional on a leader dance and background music. \n- Identifying the need for a dataset for this new task, the author proposed a mo-caped duet dancing dataset, containing 1.92 hours of duet dancing, with an average time for each clip over 1 minute. The dataset is collected with 20 120-FPS optical cameras and meta gloves for motion capture, and it contains 10 distinct genres of duet dances. SMPL-X is fitted for this dataset.\n- This paper proposed a pure kinematics-based method for conditional dance generation. It first tokenizes the dancing motion with 5 VQ-VAE, then trains a GPT-like arch to generate dancing motions autoregressive. To reduce the misalignment of lower-body motion and global root translation, the author used off-policy RL to finetune the model.\n- For experimental evaluations, the author proposed a set of simple metrics for evaluating the follower's interaction with the leader and the background music. The metric is inspired by the Beat-Align Score, where the beat time of dance is defined as the local minimum of velocity. Besides metrics, a user study is also conducted. The author showed better performance of its proposed algorithm than other adopted baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The algorithm proposed does not change the whole landscape of the kinematics-based motion generation, but it has some novelty, especially the second-stage off-policy tuning. Additionally, the authors also collected a high-quality duet dancing dataset, making this paper more sound. \n- For the evaluation part, it is thorough to design three separate metrics and also a user-study.\n- The writing, tables, and figures are clear and easy to follow. For example, table-1 cleary summerize the unique of the collected dataset. Figure-4 shows the autoregressive generator quite clearly. The details of the methods are well-documented in the supplementary parts."
            },
            "weaknesses": {
                "value": "I only have some minor comments on the weakness part.\n\n- I found a major weakness in the off-policy RL part. For a pure kinematics-based method, there might exist a couple of artifacts, such as floor-skating, penetration between bodies and ground, and other physical-infeasible dynamics. The videos in the supplementary materials also showed such artifacts. The paper only addresses the problem of misalignment of lower-body motion and full-body motion; why the author just stopped here? it seems that other artifacts might also be alleviated through similar RL tuning. Has the author tried to address such artifacts? Does the author believe such off-policy tuning combined with some heuristic reward function can reduce most artifacts?\n- The author mentioned using off-policy RL to reduce the artifacts of misalignment of lower and full-body motion. It would be beneficial to add some new metrics to evaluate this specific improvement."
            },
            "questions": {
                "value": "- Why use 2D skeletons for user study? Why not use 3D-rendered motions, like the one in the demo video? \n- What is the training time for each of the stages? \n- A general question is how to evaluate the quality of the motion-capture? I watched the video, and it seems there are some flickering frames and some exaggerated hand motions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2432/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698784018167,
        "cdate": 1698784018167,
        "tmdate": 1699636178668,
        "mdate": 1699636178668,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3dNmnM5nJO",
        "forum": "GW4j4n2cjH",
        "replyto": "GW4j4n2cjH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2432/Reviewer_cNc4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2432/Reviewer_cNc4"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new task and motion captured dataset related to dance generation, specifically, the task of generating dance \u201caccompaniments\u201d. In this task, the model\u2019s job is to generate the follower\u2019s motion from that of the leader, potentially enabling VR/AR experiences where users dance with virtual avatars. This paper also proposes a GPT-based method to address this task which treats dance accompaniment as a sequence-to-sequence language modeling task to convert leader dance tokens and musical context into follower dance tokens."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper has several strengths including **introducing a novel task and dataset**, **proposing well-designed methods with solid evaluation**, and **it is well-written**.\n\n**Novel task and dataset**. The idea of generating dance accompaniments is interesting, and the authors expend extraordinary effort and expense to create a novel dataset for this task. Based on the supplementary material, this dataset appears to my eyes to be of extremely high quality thanks to the use of fine-grained motion capture. I have no doubt that this dataset will constitute a valuable resource to the growing dance generation research community.\n\n**Well-designed methods and evaluation**. This paper poses dance accompaniment as a sequence-to-sequence \u201ctext generation\u201d problem by learned tokenizations of dance motion. Despite the inevitable methodological complexity that comes with dealing with high-dimensional dance data, this approach is _overall_ satisfyingly straightforward and reasonable. Moreover, the authors construct reasonable quantitative evaluation metrics for their method and also conduct a user study, achieving impressive performance relative to baselines (but less impressive than the ground truth) and also effectively ablating additional elements of their approach (e.g. RL).\n\n**Well-written**. This paper is also very well-written, with remarkable clarity both in its conceptual presentation and formalized notation. I was able to follow the details quite well despite being a newcomer to working with dance and motion capture data."
            },
            "weaknesses": {
                "value": "This paper also has some weaknesses including **output quality issues**, **potential copyright issues with the dataset**, and **limited reusability of insights**.\n\n**Output quality issues**. While reasonable in design, the proposed method produces fairly rigid dance accompaniments that, subjectively speaking, vaguely resemble someone dancing with a lifeless mannequin (especially in contrast to extraordinary richness of the ground truth accompaniments). While the authors make a valiant effort to improve results w/ RL, the best system still falls far short. It seems like progress here is more likely to be driven by large-scale pre-training from noisier dance datasets followed by adaptation to small high-quality mocap datasets, rather than training from scratch on mocap datasets.\n\n**Potential copyright issues with the dataset**. Perhaps moreso than the proposed methods, the DD100 dataset may be the most valuable aspect of this work to the dance generation community. This paper promises to release \u201cMP3s\u201d associated with the dataset but fails to report details about the copyright status of the music in the dataset. Listening to the demo videos, the dataset appears to feature copyrighted material (e.g. \u201cCharlie Puth - LA Girls\u201d). It is likely that, even if the authors release the audio, they will likely be forced to take it down eventually, compromising the value of the dataset to the research community.\n\n**Limited reusability of insights**. There is not a lot of information in this paper that would be of interest to researchers outside of the dance generation community. Perhaps there is something reusable happening in the use of RL to refine GPT models, but this is only explored within the context of dance generation. Though ICLR does occasionally have dance generation work in its proceedings, I suspect that this paper overall will not be particularly interesting to the broader ICLR community."
            },
            "questions": {
                "value": "- Why use copyrighted music for capturing this valuable dataset as opposed to copyright-free audio?\n- Why not pre-train models on noisy dance data?\n- There doesn\u2019t appear to be any details about the music tokenization strategy used in this work - can the authors clarify how music features are represented?\n- I was confused by the use of lookahead - would the lookahead model actually work in a real-time dance setting?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Dataset appears to use copyrighted MP3s. From an _ethical_ point of view, my personal stance is that this is fine: the goal of this dataset is clearly to provide realistic dance data set to realistic music moreso than a subversive effort to distribute a few dozen copyrighted songs. However, from a _legal_ point of view, it is likely that the music in the dataset will have to be taken offline at some point, harming its usefulness as a research tool.\n\nAnother mild ethical concern revolves around the gendered nature of the application goal of generating *follower* dancers, conventionally the role of female dance partners. The authors could have also studied generating leader motions, a similarly interesting task that would allow users to dance with leaders. I would like to see the authors briefly address this in their ethical statement as well."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2432/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698790964564,
        "cdate": 1698790964564,
        "tmdate": 1699636178575,
        "mdate": 1699636178575,
        "license": "CC BY 4.0",
        "version": 2
    }
]