[
    {
        "id": "kZXfhUmEo3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7032/Reviewer_LxB9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7032/Reviewer_LxB9"
        ],
        "forum": "JpyWPfzu0b",
        "replyto": "JpyWPfzu0b",
        "content": {
            "summary": {
                "value": "This paper introduces PaLI-3, a new vision language model. One important modification is using SigLIP as the vision module encoder. The final model is small but effective. It performs comparably with many larger models on various benchmarks and also achieves better results in localization and text understanding than prior works."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1) Very good results in terms of cost-effectiveness trade-off.Comprehensive evaluation on various benchmarks.\n2) The paper is very easy to read and understand.\n3) The approach is simple and easy to implement.\n4) The effectiveness of SigLIP is very insightful. It seems that such a simple modification can give a significant improvement. It shows the potential of the importance of designing a smarter training objective that aligns better with the language models."
            },
            "weaknesses": {
                "value": "1) I strongly encourage authors to provide more comparisons between CLIP and SigLIP under this paper's setting. The current ablation only includes the comparison between SigLIP and vanilla classification.\n2) I understand the paper mainly focuses on a smaller and cheaper model, as stated in the title. However, I think it is important to study the scaling results to check the effectiveness on a larger scale. Can the SigLIP still be so effective when using a larger vision encoder and language models? It would also be interesting to further scale down the model and see what would happen."
            },
            "questions": {
                "value": "1) Any plan to release the code? UL2 and SigLIP are both open-sourced. I think it would be nice to have an open-sourced version (would be better to use open-sourced data to pretrain) and will be easy to compare and use this model as a baseline. This model is small. If you can provide a fully reproducable version, I believe more folks from GPT-poor institutes would be motivated to follow.\n2) I understand this paper's setting is not a huge model with a very strong zero-shot ability like GPT-4V. However, I'm highly interested in what do authors think about the potential of using SigLIP as the vision encoder for real LLMs (maybe > 100B or so).\n3) Could you provide a detailed discussion about the difference between PaLI-3 and the previous versions? It would be better to have a table and show the differences directly."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7032/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697160818754,
        "cdate": 1697160818754,
        "tmdate": 1699636825537,
        "mdate": 1699636825537,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GMT8rc6ddb",
        "forum": "JpyWPfzu0b",
        "replyto": "JpyWPfzu0b",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7032/Reviewer_Cwas"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7032/Reviewer_Cwas"
        ],
        "content": {
            "summary": {
                "value": "This paper presents the latest improvement in the so-called PALI series of VL models. The main contributions are the replacement of the Visual Encoder with a SigLIP model and the increase of resolution. The authors present many experimental results which show the effectiveness of their pipeline."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The main strength of the paper is the numerous experiments the authors have carried out and the good results presented. Moreover, the paper is fairly easy to follow."
            },
            "weaknesses": {
                "value": "Unfortunately I don't believe that the claimed contributions (used of SigLIP and increase of resolution) are enough for ICLR. The finding that contrastively pre-trained visual backbone with language supervision works better than training for classification doesn't seem very surprising. Moreover, training follows previous PALI training pipelines so no particular novelty in this regard either. Actually incorporating these improvements could probably benefit any other model compared with the proposed one."
            },
            "questions": {
                "value": "No questions"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7032/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698774124384,
        "cdate": 1698774124384,
        "tmdate": 1699636825401,
        "mdate": 1699636825401,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ANYarYDE7k",
        "forum": "JpyWPfzu0b",
        "replyto": "JpyWPfzu0b",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7032/Reviewer_fT87"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7032/Reviewer_fT87"
        ],
        "content": {
            "summary": {
                "value": "This paper presents PaLI-3, a vision-language model with only 5B parameters but achieves state-of-the-art results across several benchmarks. The authors begin by comparing contrastively pretrained visual encoders and classification-pretrained ViT models, drawing the conclusion that the contrastively pretrained visual encoder demonstrates better performance on vision-language tasks, especially grounding tasks. Compared to previous state-of-the-art models (SOTAs), PaLI-3 can achieve competitive scores with significantly fewer overall parameters. Despite the training process being conducted without any video inputs, PaLI-3 is still capable of accomplishing video-based tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The conclusion that a contrastively pretrained visual encoder can outperform a classification-pretrained encoder in vision-language tasks, particularly in grounding, is valuable and beneficial to the vision-language community.\n- Strong performance with much less parameters.\n- Sufficient in-depth analysis on general tasks and fairness, bias and potential issues are performed to better model understanding."
            },
            "weaknesses": {
                "value": "- The main weakness of PaLI-3, from my perspective, is the way the authors used to draw their conclusion. Specifically, the authors claim that because SigLIP shows better performance than the classification-pretrained visual encoder used by PaLI and PaLI-X, they conclude that a contrastively pretrained visual encoder is superior to a classification-pretrained one. However, it's worth noting that most of the accessible contrastively pretrained visual encoders for the vision and vision-language community are members of the OpenCLIP family. Have you ever attempted to utilize OpenCLIP as a vision encoder?\n- The results in Section 4 are per-benchmark finetuned. What's the performance of PaLI-3 without task-specific fine-tuning (zero-shot)? Is it possible to generate target answers with few-shot demonstrations by prompting (in-context learing)?\n- As mentioned in Section 3.2, during stage 0 of PaLI-3's training process, the contrastive visual encoder is pretrained with a 3B UL2 as a text encoder-decoder. Subsequently, the same 3B UL2 model is employed as the language model for PaLI-3. Is this consistency in using the same language model for both contrastive visual pre-training and generative vision-language pre-training crucial or not? Have any experiments been conducted on this?"
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7032/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698892118649,
        "cdate": 1698892118649,
        "tmdate": 1699636825238,
        "mdate": 1699636825238,
        "license": "CC BY 4.0",
        "version": 2
    }
]