[
    {
        "id": "qKYtiQMiIq",
        "forum": "DxM73sxtna",
        "replyto": "DxM73sxtna",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7718/Reviewer_2YVT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7718/Reviewer_2YVT"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a certain data-adaptive DP training method for the linear regression. It improves upon the recent utility bounds by Varshney et al. (COLT 2022) by getting rid of the explicit dependency on the feature space dimension. This is obtained by utilizing the covariance matrix $H$ of the features in several ways: by adding non-isotropic noise with a covariance that is of the form $\\lambda I + H$ where $ \\lambda > 0$ and by using an adaptive clipping that also takes into account $H$ and the magnitudes of the residuals $x^T \\theta_k - y$, where $\\theta_k$ denotes the model parameters at iteration $k$. Also, instead of using the shuffling mechanism as in (Varshney et al., 2022), the paper uses the DP-FTRL tree aggregation mechanism. By using the non-isotropic noise with the data-covariance matrix, the method can be thought of as a certain second-order version of DP-SGD (or DP-FTRL) for the linear regression. The covariance matrix $H$ is assumed to be obtained from public data, i.e., no private algorithm for its estimation is given, and the dimension-independency is obtained using an analysis where the trace of $H$ (i.e. the sum of the singular values) appears (by using a norm weighted with $H$ in the analysis, as far as I see). The analysis gives the dimension-independency even in case $H$ is not used for the noise addition and clipping ."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is really well written, and seems to address an important problem: based on my knowledge and looking at the references, this way of taking into account the spectrum of the feature covariances has not been addressed before in this detail and accuracy. Even if the analysis is limited to the linear regression, I get the impression this is an important contribution.\n\n- Generality: the results could be used for various models of data. It seems that the techniques pave the way for follow up work, where one could try to get rid of the need for public data, for example."
            },
            "weaknesses": {
                "value": "- The assumption that we know the covariance of the features $H$ or that we can somehow get an approximation from the public data is quite strong. I think it would make the paper much stronger if there was some version that makes a DP approximation of $H$.\n\n- I think one argument against the baseline method of Varshney et al. (2022) is weak: namely that you can get rid of the restriction on $\\varepsilon$ by using DP-FTRL instead of the shuffling approach. This argument is mentioned few times. As far as I see, the condition on $\\varepsilon$ for the shuffling approach of Varshney et al. comes from the analysis of Feldman et al. (2022) for shufflers, where the $(\\varepsilon,\\delta)$-bounds are obtained by bounding the $(\\varepsilon,\\delta)$-distance between certain binomial distributions. And the condition on $\\varepsilon$ then comes from the analysis that they use to obtain convenient analytical bounds. I believe one could have for the shuffler $(\\varepsilon,\\delta)$-bounds with arbitrarily small $\\varepsilon$'s, just the way you have DP bounds for DP-FTRL. My point is that this restriction is not inherent limitation of the shuffler approach, just of the particular analytical bounds, and it would be possible to get bounds with small $\\varepsilon$'s."
            },
            "questions": {
                "value": "Do you think the current techniques could be used in case where you release $H$ privately?\n\nSmall remarks:\n\n- There are typos in summation indices in several places, e.g. p. 3: \n$$\n\\sum_{k_1 < k \\leq k_2} \\mu_i v_i v_i^T\n$$\nand\n$$\n\\sum_{j=0}^T g_i\n$$\nin several places at least on p. 5. \n\n- In the condition on the fourth-order momentum on p.4, shouldn't there be $ \\preccurlyeq $ (or something similar) instead of $ \\leq $ ?\n\n- Beginning of P. 6: the condition $\\varepsilon \\leq \\sqrt{N}$ must be a typo. It does not seem restrictive nor to correspond to results of Feldman et al.\n\n- Check the bibliography. Example: you give the Arxiv versions of Varshney et al. (2022) and Karwa and Vadhan (2018) as references. They have appeared in COLT and ITCS, respectively."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7718/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698743471839,
        "cdate": 1698743471839,
        "tmdate": 1699636940770,
        "mdate": 1699636940770,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WzCEd9gTjs",
        "forum": "DxM73sxtna",
        "replyto": "DxM73sxtna",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7718/Reviewer_BWiw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7718/Reviewer_BWiw"
        ],
        "content": {
            "summary": {
                "value": "This paper analyzes a variant of DP-FTRL for linear regression in the overparameterized regime. Here the goal is to produce a parameter that predicts well on new data. There are many existing works on DP linear regression but all existing analyses have sample complexities that grow with the dimension. Investigation into the behavior of nonprivate algorithms in the overparameterized setting is relatively recent as well, and this paper builds heavily on recent work of Zou et al., who analyze one-pass SGD in a nearly identical setting.\n\nThis work makes two modifications to standard DP-FTRL. The first is adaptive clipping, using a subroutine of Liu et al. The second modification allows the use of non-isotropic noise, with the noise covariance set via some prior knowledge. Ideally, the noise covariance would depend on the covariance of the $x$'s. Privately estimating this requires $N=\\Omega(d^{3/2})$ examples, even for Gaussian distributions [see 1, 2, 3; these aren't cited but should be]. This is outside the regime of interest. However, one might have access to unlabeled public data points, which would yield useful prior knowledge.\n\nThe main result, a theorem about the algorithm's accuracy, is rather inscrutable. We get two main corollaries: one with isotropic noise and one with noise based on this large public data set. The key feature about these bounds is that they are expressed in terms of the spectrum of the covariance of the $x$'s and do not directly depend on the dimension. We get further corollaries simplifying the results when the eigenvalues of the covariance matrix decay sufficiently quickly.\n\n[1] Dwork, Cynthia, et al. \"Analyze gauss: optimal bounds for privacy-preserving principal component analysis.\" Proceedings of the forty-sixth annual ACM symposium on Theory of computing. 2014.\n\n[2] Kamath, Gautam, Argyris Mouzakis, and Vikrant Singhal. \"New lower bounds for private estimation and a generalized fingerprinting lemma.\" Advances in Neural Information Processing Systems 35 (2022): 24405-24418.\n\n[3] Narayanan, Shyam. \"Better and Simpler Lower Bounds for Differentially Private Statistical Estimation.\" arXiv preprint arXiv:2310.06289 (2023)."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Linear regression is a fundamental statistical task. It is interesting that we can achieve nontrivial guarantees in the overparameterized setting. The results are original.\n\nThe paper is structured well, making it easy to understand how the results fit together."
            },
            "weaknesses": {
                "value": "I feel the submission has central weaknesses around the discussion of results and technical contribution. I advocate for rejection but remain open-minded.\n\nI worry that the technical contribution represents a limited increment, combining standard ideas from privacy with the analysis of Zou et al. The paper (pages 2-3) lists as a contribution that \"We develop new analytical tools... which can be of independent interest.\" After reading the paper, I am not sure what these tools are.\n\nI observe two issues with the discussion of results. First are comparisons with the work of Varshney et al. and Liu et al. The abstract says \"Our proposed method significantly improves upon existing differentially private methods for linear regression.\" It does not qualify this statement by saying \"in some regimes,\" so it seems reasonable to interpret this claim as saying the analysis dominates that of prior work. I am not sure this is the case, especially because the analysis here depends poorly on the bound $\\lVert w_0 - w^* \\rVert_2$ and the magnitude of the initial residuals (via $\\ell$).\n\nThe second issue with the discussion of results is around lower bounds. The paper claims, for example on page 2, that the algorithm achieves \"sharp excess risk for private overparameterized linear regression.\" Unless I missed it, this submission contains no lower bounds. It appears the discussion of \"sharpness\" comes from Zou et al., but (i) they do not consider privacy and (ii) they prove that their *analysis* of SGD is sharp. They do not prove a lower bound for the task. I believe it is likely that a casual reader might be misled. It is possible I misunderstand the results and, if so, I would happily change my mind."
            },
            "questions": {
                "value": "What do you mean when you say that, for example, Corollary 4.3 gives a sharp excess risk bound?\n\nCan you point to and discuss the technical hurdles your analysis had to overcome? What are the analytical tools you developed?\n\nAre there parameter regimes where any of the algorithms of Varshney et al. or Liu et al. have asymptotically lower error than your algorithm with isotropic noise?\n\nBefore reading this paper, my impression was that overparameterized linear regression was a theoretical proving ground for understanding implicit bias. Is the problem of overparameterized linear regression (without, for example, sparsity assumptions) of practical interest?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7718/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7718/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7718/Reviewer_BWiw"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7718/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698818149348,
        "cdate": 1698818149348,
        "tmdate": 1699636940647,
        "mdate": 1699636940647,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1fGYPqNLIf",
        "forum": "DxM73sxtna",
        "replyto": "DxM73sxtna",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7718/Reviewer_jHEC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7718/Reviewer_jHEC"
        ],
        "content": {
            "summary": {
                "value": "This submission proposes algorithms and analysis for differentially private linear regression, with focus on dimension-independent guarantees as to have guarantees in the over-parameterized regime (that is, when the dimension is significantly larger than the number of data points). The authors propose an algorithm based on DP-FTRL with adaptive gradient clipping and Gaussian mechanism with non-identity covariance matrix estimated from public data when available to improve the error. Based on techniques from related work, the authors show that the price we pay for privacy is not too large and show dimension-independent rates for a few distributions of the data."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "**Interesting combination of recent topics**: The submission joins a few different interesting directions of recent work: work on DP linear regression together with recent efforts of showing \"benign overfitting\" in the data. Although we should intuitively expect that DP-fying linear regression should not ruin benign overfitting, fleshing out the details is interesting and a contribution up to the standards of most large ML conferences. In fact, by skimming the appendix I could see that there are a few challenges in extending the benign overfitting results to the private case, and the authors do a good job on overcoming these obstacles;\n\n**Interesting use of public data**: One of the contributions of the submission is allowing the use of public data to improve on the Gaussian mechanism by using a covariance matrix that better matches (or, in other words, preconditions) the distribution of the data. Probably the most interesting part is showing how this change can positively affect the dimension-independent rates derived by the end of the submission."
            },
            "weaknesses": {
                "value": "**Missing context for results in the introduction**: One aspect of the submission that is unfortunate is that in the abstract and introduction the authors claim that their convergence rates \"improve on previous work on DP linear regression\", even more so when using general covariance matrices in the Gaussian mechanism. However, these claims should be toned down a bit since there is some context missing. For example:\n- (Major problem) the covariance matrix used in the Gaussian mechanism to achieve the best rates requires access to (unlabeled) public data. This is not ever mentioned in the abstract or introduction. As far as one can tell, the best rates stated in the introduction are achieved only based on the original data, but this is not the case and definitely should be explicitly mentioned in the abstract and in the introduction;\n- (Minor problem) Although it is true that the authors improve on rates of previous work, it seems to me that there is some nuance missing. For example, the work of Liu et al. (2023) had as a goal to have algorithms that are robust to a small portion of adversarial corruptions, right? Moreover, if I am not mistake both Varshney et al. (2022) and Liu et al. (2023) assume less about the data distribution (maybe except for Theorem 4.2, but in that case it is not clear if one can compare the bounds in general). Finally, I believe it is true that the constants hidden make it so that beyond requiring $d > N$, one needs $N$ to be significantly large for the rates to actually improve on the ones with dependency on $d$. I do not think these are points that invalidate the contribution of the submission, but I do believe they should be discussed for a fair placement of the contributions of the submission in the literature.\n\n**Apparently incremental work**: Although the results are interesting, they are somewhat incremental since they rely quite heavily on previous results and it is not clear what are the extra technical difficulties. In fact, I believe a great way to make the submission much stronger is to better delineate what are the technical challenges when trying to combine the several techniques the authors use. More specifically, if I understood it correctly, the main contribution is extending the benign overfitting rates from Zou et al. (2021b) to the private setting by \"practically\" using DP-SGD with the adaptive gradient clipping also used in Liu et al. (2023). I say practically since the authors instead use DP-FTRL (Kairouz et al., 2021) to prevent the need to use shuffling/random sampling to get good dependency on privacy parameters, but they resort on the fact that DP-FTRL can be written as a gradient update (Sec B.2.2) to actually more closely follow the analysis from Zou et al (2021b). So it becomes hard to separate what are the technical difficulties that the authors needed to overcome to make the analysis work well. \n\nA great example if the beginning of the proof of Theorem 4.1 in Sec B.1. The first part of the algorithm is only to show that tree aggregation is private. However, Tree Aggregation to privately release partial sums is already well studied and it is not clear why the analysis needs to be redone. The use of a general $\\Sigma$ matrix in the Gaussian mechanism should not warrant, I believe, an entire new analysis of tree aggregation: adding noise with distribution $\\mathcal{N}(0, \\Sigma)$ to $x$ is equivalent to adding noise with distribution $\\mathcal{N}(0, I)$ to $\\Sigma^{-1/2} x$ and then post-processing by multiplying everthing by $\\Sigma^{1/2}$. This confusion extends to other parts of the analysis since I cannot untangle how much follows from previous work and what are the key technical insights given by the authors. I believe the discussion period will be a great opportunity for the authors to elucidate a bit some of this confusion (which might be simply due to a lack of background from my part or limited time to dedicate to the paper)."
            },
            "questions": {
                "value": "Ultimately, I believe this paper is mostly correct and that its contribution is interesting to the DP community. Yet, I believe better placing the contributions in the literature (including stating use or not of public data) and clarifying the key technical contributions would strengthen the submission by an order of magnitude. Removing the burden of untangling the key technical contributions from previous work and the key technical challenges overcome by the current paper would certainly make us appreciate the results a lot more. Here are a few points I hope the authors can help me get some clarity on:\n\n1) Do you believe my assessment on the minor problem I mentioned on the lack of context is fair? Feel free to disagree, but even if I am wrong, this already tells that the authors should try to clarify the placement of their contributions in the literature.\n2) On my second part, would the authors be able to briefly mention some of the technical challenges they had to overcome? I believe the analysis is not simply following the analysis of benign overfitting from 2021 modulo carrying extra terms due to privacy, but from the appendix it is hard to untangle the key technical contributions (as mentioned before, for example, I am not sure the privacy analysis of tree aggregation needs to be proven again). The tree aggregation example might be a good point to clarify whether a new analysis was actually required or not.\n3) (Minor) A more technical question: Theorem 4.2 specifies a few constants that depend on $\\mathrm{Tr}(H)$, which we assume we do not know, right? Is it standard to have the algorithm parameters in DP linear regression depend on the unknown matrix $H$? \n4) (Minor) This is certainly a minor question, but the authors assume that $\\mathrm{Tr}(H)$ is no bigger than a constant, if I understand correctly. Although this seems to hold for the \n\nFinally, here are direct suggestions that do not necessarily require comments from the authors (I tried to sort them from more important to less important):\n- I think it is paramount that the authors make it clear that the tighter convergence rates depend on a matrix estimated from **public data**. This should not be left to be mentioned only half-way through the paper;\n- I believe the text neves defines explitly from which matrix the eigenvalues $\\lambda_i$ mentioned are from, but this notion is used throughout the entire text (except for a parenthesis in page 7). This should be clearly stated when you introduce the matrix $H$, since this is extremely important for the readers to understand many of the results;\n- At some point the authors call FTRL \"follow-the-perturbed-leader\" but it should be \"follow-the-regularized-leader\". The former is a term for a very different algorithm;\n- Theorem 4.2 is not very readable in its current form, and uses a lot of space in the main paper. Either a discussion should be added on the relevance/meaning of some of these terms, and/or the statement should be simplified. Using space in the main paper with so much notation that does not communicate much would probably be better off being moved to the appendix. Of course, the authors should feel free to disagree with me on this point, but the current discussion after the theorem does not seem to require all the notation used;\n- Right after Assumption 2.2 the text references Assumption 2.3, but I think it should reference Assumption 2.2;\n- Define a few of the terms you use early on. It takes a few paragraphs for the text to explicitly say that overparameterized setting simply means $N < d$. I am not sure what \"sharp risk\" means in the bold text in the introduction."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7718/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7718/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7718/Reviewer_jHEC"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7718/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698896303920,
        "cdate": 1698896303920,
        "tmdate": 1699636940547,
        "mdate": 1699636940547,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5KqxsMRdmo",
        "forum": "DxM73sxtna",
        "replyto": "DxM73sxtna",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7718/Reviewer_mnfV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7718/Reviewer_mnfV"
        ],
        "content": {
            "summary": {
                "value": "The paper proposed a new variant of the DP-FTRL algorithm with a novel noise, a general covariance matrix instead of traditional identical matrix. Specifically, the tree aggregation protocol and designed noise matrix are employed to obtain better trade-offs between privacy and utility; The residual estimator is to use the private empirical variance estimator of the residual to control the gradient."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper introduces a novel designed noise covariance matrix, which could provide improved privacy and utility trade-offs compared to using isotropic noise.\n- The paper is well-organized and the use of pseudocode for the algorithms helps in understanding the technical details."
            },
            "weaknesses": {
                "value": "- The proposed designed noise covariance matrix is also limited and may be unpractical since it needs the unlabeled data first to generate an estimation of $\\mathbf{H}$.\n- The statement of Algorithm 3 is not clear. Binary representation $b_i$ is not clear in line 13. Also, Line 7 in algorithm 3 shows \"$n_j$ is the last node in $\\mathbf{p}$ that is a left child\" and add noise when $n_i \\in \\mathbf{p}_j$, while Page 15 states \"adding Gaussian noise to the nodes along this path from m to the first left child\", which is contradict. $\\mathbf{p}_j$ is a set and first left child is a node. Hope the author give more explanations.\n- typo: Page 5, update rule should be \\sum_{i=0}^t g_i instead of \\sum_{j=0}^t."
            },
            "questions": {
                "value": "-The authors should add experiments to support their theory\n-The authors should show the disadvantage of DP-AMBSSGD: DP-Adaptive-Mini-Batch-Shuffled-SGD, I believe it can also handle the overparameterized case\n-The author should add lower bounds"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7718/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699607934418,
        "cdate": 1699607934418,
        "tmdate": 1699636940441,
        "mdate": 1699636940441,
        "license": "CC BY 4.0",
        "version": 2
    }
]