[
    {
        "id": "SvkkDoGjcQ",
        "forum": "vfEqSWpMfj",
        "replyto": "vfEqSWpMfj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6077/Reviewer_79zL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6077/Reviewer_79zL"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method that focuses on varying prompt words to understand their statistical impact on model outputs. Unlike classical attention, this method measures the importance of words based on their impact on user-defined text scores, allowing for the decomposition of word importance into specific measures like bias, reading level, and verbosity. To validate the effectiveness of this approach, the study investigates the effect of adding different suffixes to various system prompts and compares the resulting generations with GPT-3.5. The results demonstrate a close relationship between word importance scores and the expected suffix importance across multiple scoring functions."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The method that focuses on varying prompt words to uncover their statistical impact on model outputs is novel. The adaptation of this concept to LLMs and the specific measures of interest represent an original contribution to the field.\n\n2. The study provides a clear description of the proposed approach, including the masking of prompt words and the evaluation of their impact on the outputs. The comparison with GPT-3.5 and the demonstration of the relationship between word importance scores and expected suffix importance validate the fidelity of the method.\n\n3. The paper effectively communicates the objectives, methodology, and results of the research. The introduction clearly establishes the problem of explainability in LLMs and the need for a novel approach. The description of the method is presented in a clear and concise manner. The experimental results are well-explained, and the significance of the findings is effectively conveyed."
            },
            "weaknesses": {
                "value": "1. The rationale for selecting a specific model, such as the FlagEmbedding model \"BAAI/bge-large-en,\" is not adequately explained. It is crucial to provide a clear justification for choosing this particular model over others, highlighting its relevant features, performance, or suitability for the research objectives. By providing a comprehensive rationale, readers can better understand the motivations behind the model selection and its implications for the study.\n\n2. The explanation of the Scoring and Impact Calculation method lacks clarity. It is essential to provide a detailed and step-by-step description of how the scoring and impact calculation process works. This should include the specific metrics used, the mathematical formulas or algorithms employed, and any relevant considerations or assumptions. A clear and explicit explanation of this methodology will ensure that readers can comprehend and replicate the calculations performed.\n\n3. The dataset used in the study is generated by GPT4. Merely relying on a dataset generated by GPT4 may not sufficiently capture the range of subjective opinions on explainability. Including a user study or evaluation process would provide valuable insights into the perceptions and interpretations of explainability, enhancing the robustness and validity of the research findings.\n\n4. The algorithm chart provided in the paper is blurry and difficult to read. It is essential to ensure that all visual elements, such as charts or diagrams, are of sufficient quality and clarity to convey the intended information effectively."
            },
            "questions": {
                "value": "Could you give more details or an example of how the score is calculated in the Scoring and Impact Calculation part of the proposed method?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6077/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6077/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6077/Reviewer_79zL"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6077/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698298157036,
        "cdate": 1698298157036,
        "tmdate": 1699636655019,
        "mdate": 1699636655019,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "01yvrno0HM",
        "forum": "vfEqSWpMfj",
        "replyto": "vfEqSWpMfj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6077/Reviewer_pTAs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6077/Reviewer_pTAs"
        ],
        "content": {
            "summary": {
                "value": "The paper describes work on evaluating ChatGPT for word importance with respect to prompts. The author/s motivate the study by claiming that \u201crecognizing the impact of specific words or linguistic structures on LLM outputs can offer a granular understanding of model behavior, providing valuable insights into how information is processed and weighted across different layers of the model.\u201d For the experiments, the author/s propose a simple method for approximating a word\u2019s importance value fro the prompt that is \u201cinspired by permutation importance\u201d in tabular data analysis. The method requires iterating through the prompts while masking each word and evaluating the resulting response from the model to approximate the masked word\u2019s importance. There are no mentions or discussions whatsoever of the limitations and adaptability of the proposed method. The author/s use readability, embedding similarity, and simple word count for scoring. The prompt choice used for the experiment setup has not been properly discussed, which is confusing. Overall, the task presented itself is framed as explainability but is more closely similar to prompt engineering as the method itself optimizes for word importance in prompts."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper explores and interesting concept of word importance which I do find essential in further understanding how large language models like ChatGPT works. The proposed method has some potential provided that it carefully addresses some of the very obvious limitations discussed below and further improve its algorithmic features to consider scale, flexibility, and efficiency."
            },
            "weaknesses": {
                "value": "The depth of the experiments conducted in the study is extremely limited as only three metrics which cover Flesch Ease, word count, and topic similarity (cosine embedding) have been explored. The model variation is also very limited, with only one model used for experimentation, GPT-3.5-Turbo (ChatGPT), despite the diverse publicly available models in Hugginface such as Llama, FlanT5, BLOOMZ. This implies that the study essentially optimizes for OpenAI products instead of prioritizing diverse results from open-sourced models. There is no ablation or in-depth exploration. This form of limitation needs to be addressed for inclusion to ICLR.\n\nThere are several obvious limitations of the proposed methodology involving masking each word in the prompt. The method seems to be not practical for prompts that are considerably long, which is realistically common in most interdisciplinary fields. This should be discussed thoroughly in the paper. Moreover, there are given words that are obviously non-important (ex. stopwords), it would be computationally expensive and impractical to still iterate and and compute the importance of these words in the prompt. The proposed methodology seems to have no workaround for optimization and compression.\n\nWhile the authors are correct that the proposed method is text score agnostic, it is worth exploring what linguistic scoring features are better than others. This begs more in-depth exploration/ablation of an extensive set of features (which is expected for an ICLR paper).\n\nThe paper is basically prompt engineering as it optimizes the quality of generation based on some measure of word importance. The author should explicitly mention this as it directly aligns with the task covered by the paper. It would also help other researchers discover similarities with works on optimizing prompts / explainable prompts in general.\n\nMinor comments:\n\n1. The aesthetics of the paper, including figure quality, structure of sections, proper captioning, and layout, should be greatly improved for readability. The algorithm figure has no number, the tables are too wide instead of compact.\n\n2. The tables are confusing and are not presented properly. For example, Table 2 could have been represented much better as it is confusing what the author/s mean in parallel with the discussion on suffixes. In terms of the suffix configuration, the examples on bullet points are not well presented. Instead, show an actual diagram instead of how the suffixes are added with respect to each evaluation metric used."
            },
            "questions": {
                "value": "1. Is there even a need to mask all words, including stop words (ex. \u201cand\u201d, \u201cis\u201d)? These words might already be obviously unimportant for the user, and the proposed methodology seems to be static and not adaptive.\n\n2. How does using embeddings capture topics? The method only captures semantic relatedness as it only uses cosine similarity. Also, why the FlagEmbedding model? What\u2019s the justification for using this specifically?\n\n3. One thing that is very confusing is that the choice of the prompts used, as evidenced by some instances shown in the paper in the Appendix, for querying responses is unusual and unmotivated. Why should the prompts look like these? If word importance is being measured, I would have expected prompts in qualitative question form (with an absolute gold standard answer on hand) where important entities in a sentence are iteratively being masked, and the goal of the language model is to answer the question. The author/s can then evaluate the correctness of the generated responses by the model with the gold standard to see if there are some negative effects with some entities removed or masked in the prompts. In the paper, I do not understand the motivation and importance of using phrases like \u201cYou answer like David Attenborough.\u201d or \u201cYou are a surgeon.\u201d in the prompts."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6077/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698356635582,
        "cdate": 1698356635582,
        "tmdate": 1699636654921,
        "mdate": 1699636654921,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bD8ZOppj0x",
        "forum": "vfEqSWpMfj",
        "replyto": "vfEqSWpMfj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6077/Reviewer_FVbK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6077/Reviewer_FVbK"
        ],
        "content": {
            "summary": {
                "value": "This paper presents an approach to measure word importances in system prompts for LLM generations. The method specifically investigates how perturbations (i.e., replacing individual input words with an underscore) of system prompts affect the structure and content of LLM output generations. The authors evaluate their method on a synthetic dataset consisting of LLM generations (using GPT-4). Using three evaluation metrics (topic similarity, Flesch reading-ease, word count), the authors compare individual word importances to the importance of instruction suffixes which are appended to model inputs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The paper utilizes a common technique in NLP (word saliencies) and applies the concept of word importances to a recent LLM. Doing so can lead to informative insights into model interpretability as pointed out in the paper."
            },
            "weaknesses": {
                "value": "* The dataset used for the experiment has been generated with an LLM. This is problematic since the dataset is biased towards generations from another LLM and does not necessarily reflect a distribution of human inputs. As such, the reported results do not necessarily hold true for human inputs. It would therefore be important to conduct experiments on a human-written dataset as well.\n* The paper focuses substantially on an importance comparison between individual words and an instruction suffix which is appended to the model\u2019s input. I find the setup of such an experiment confusing in this context. Did the authors consider computing word importances for individual words in a dataset and ranking individual words based on their importance across examples? Such an analysis would give explicit insights into individual words used to query a model. Currently, the analysis is limited to a few suffixes which were defined for the study.\n* The paper introduces \u201cword count\u201d as a measure of deviation. It is unclear to me how this is motivated, i.e., how a change in word count related to an LLM generation reflects the importance of a word that has been removed in its input.\n* To measure word importance, the paper uses absolute values of Flesch reading-ease and topic similarity. However, both metrics are directional in that an increase or decrease after perturbing the input is informative. Absolute values of such deviations should therefore not be used.\n* The presentation can be improved. For example, there is a Figure in page 4 with a very low resolution and no caption. Page 5 states \u201crefer to the appendix\u201d without explicitly stating which section/paragraph is meant."
            },
            "questions": {
                "value": "* What was the motivation for using an LLM-generated dataset as opposed to one consisting of human-written texts?\n* Have you thought about extending the analysis to additional LLMs, to investigate whether the observed patterns emerge with respect to other models as well?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6077/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698766863188,
        "cdate": 1698766863188,
        "tmdate": 1699636654799,
        "mdate": 1699636654799,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HZ0UERftNs",
        "forum": "vfEqSWpMfj",
        "replyto": "vfEqSWpMfj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6077/Reviewer_Jiyw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6077/Reviewer_Jiyw"
        ],
        "content": {
            "summary": {
                "value": "This study proposes a method to enhance the explainability of Large Language Models (LLMs) by examining the statistical impact of prompt words on model outputs. The approach involves masking each word in the system prompt and evaluating its effect on the outputs using aggregated text scores from multiple user inputs. Unlike traditional attention mechanisms, word importance measures the influence of prompt words on user-defined text scores, allowing for the decomposition of word importance into specific measures of interest, such as bias, reading level, and verbosity. This method is also applicable when attention is not available. The fidelity of the approach is tested by adding different suffixes to various system prompts and comparing the subsequent generations with GPT-3.5 Turbo. The results demonstrate a close relationship between word importance scores and expected suffix importance across multiple scoring functions. Additionally, the study provides a Python project for computing these scores and discusses its potential applications in developing generative AI use cases in various industries. Overall, this research offers a valuable method to improve the explainability of LLMs by assessing the impact of prompt words on model outputs and opens avenues for diverse industry applications."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThis paper presents a method to masks each word in the system prompt and evaluates its effect on the outputs based on the available text scores aggregated over multiple user inputs."
            },
            "weaknesses": {
                "value": "1.\tThe contribution of the paper is limited, similar topics have been investigated before while this paper didn\u2019t pose any more valuable conclusions. \n\n2.\tThe experiment section is terribly organized. No quantitative results are provided. The experiment design is very confusing and too specific.\n\n3.\tThe presentation is really bad\n\n     a.\tAll the figures are poorly illustrated. There is even an untitled algorithm diagram before Section 4.\n\n     b.\tAll the tables are also hasty and careless.\n\n     c.\tThe term LLM lacks its full name in the abstract part.\n\n     d.\tThe font of the template is also not correct.\n\n4.\tMissing references:\n\n     a.\t\u201cDid You Read the Instructions? Rethinking the Effectiveness of Task Definitions in Instruction Learning\u201d\n\n     b.\tIt discusses a very similar topic to this paper, the authors need to cite and distinguish their differences."
            },
            "questions": {
                "value": "See the Weakness part for reference."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6077/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6077/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6077/Reviewer_Jiyw"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6077/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698802282972,
        "cdate": 1698802282972,
        "tmdate": 1699636654692,
        "mdate": 1699636654692,
        "license": "CC BY 4.0",
        "version": 2
    }
]