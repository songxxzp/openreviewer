[
    {
        "id": "83DEm5KW6x",
        "forum": "OGtnhKQJms",
        "replyto": "OGtnhKQJms",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7393/Reviewer_BPer"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7393/Reviewer_BPer"
        ],
        "content": {
            "summary": {
                "value": "This paper generalizes existing theories on causal representation learning, multi-view learning by allowing multi-views, partial observability and dependency between latents. It provides identifiability theory for content variables and so-called identifiability algebra to combine content variables."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The literature review is extensive, although there are some recent works on CRL including identification under the nonparametric settings that might be relevant as well.\n\n[1] Liang, Wendong, et al. \"Causal Component Analysis.\" arXiv preprint arXiv:2305.17225 (2023).\n[2] Jiang, Yibo, and Bryon Aragam. \"Learning nonparametric latent causal graphs with unknown interventions.\" arXiv preprint arXiv:2306.02899 (2023).\n[3] Buchholz, Simon, et al. \"Learning Linear Causal Representations from Interventions under General Nonlinear Mixing.\" arXiv preprint arXiv:2306.02235 (2023).\n\n2. I like how the paper is structured with an intuition behind definitions and theorems for readability."
            },
            "weaknesses": {
                "value": "1. Although the setting is different, the theoretical contributions and proof techiniques seem to be mostly derived from [1] and [2].\n\n[1] Roland S. Zimmermann, Yash Sharma, Steffen Schneider, Matthias Bethge, and Wieland Brendel. Contrastive learning inverts the data generating process. Proceedings of Machine Learning Research, 139:12979\u201312990, 2021\n\n[2] Julius von K\u00fcgelgen, Yash Sharma, Luigi Gresele, Wieland Brendel, Bernhard Sch\u00f6lkopf, Michel Besserve, and Francesco Locatello. Self-supervised learning with data augmentations provably isolates content from style. Advances in neural information processing systems, 34:16451\u201316467, 2021"
            },
            "questions": {
                "value": "1. For definition 5.3, why assume l0 to be the same and not the two vectors are the same? For a given view, the content variables are fixed, right?\n\n2. Could you provide a little more motivation as to why view-specific encoders are needed? I understand the theoretical reason. But in practice, one would expect to train with all views available, right?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7393/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7393/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7393/Reviewer_BPer"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7393/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698723371064,
        "cdate": 1698723371064,
        "tmdate": 1699636885516,
        "mdate": 1699636885516,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OT0PxP37y4",
        "forum": "OGtnhKQJms",
        "replyto": "OGtnhKQJms",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7393/Reviewer_QCXK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7393/Reviewer_QCXK"
        ],
        "content": {
            "summary": {
                "value": "This work studies identifying latent causal representations given multi-view data. This work shows that the latent subspaces if shared by more than one view, can be identified up to bijjective mappings. Further, a view-specific encoder is devised to make learning a large number of shared blocks tractable. Experiments on multi-modality/task data are provided to verify the theoretical insights."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "\u2022 The paper is well-written and communicates clearly motivations, formulation, technical details, and theoretical implications.\n\n\u2022 This paper extends previous content-identification results (mainly von Kugelgen et al. 2021, Daunhawer et al. 2023)  to multi-view distributions and further (identifiability algebra).\n\n\u2022 The empirical evaluation is thorough (over multi-view/modality/task datasets) and well-designed to substantiate the theoretical findings in the paper."
            },
            "weaknesses": {
                "value": "\u2022 My primary concern is the novelty of the work. In comparison with prior work (von Kugelgen et al. 2021; Daunhawer et al. 2023), the technical contribution appears somewhat incremental.\n\n\u2022 The assumption that each view-specific generating function is invertible is strong, especially over multiple modalities. Intuitively, this requires the shared blocks to be duplicated multiple times and thus simplifies the identification problem. A recent work [1] has attempted to weaken this assumption.\n\n[1] https://arxiv.org/abs/2306.07916"
            },
            "questions": {
                "value": "I would like to learn about the authors' response to the weaknesses listed above, which may give me a clearer perspective on the paper's contribution."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7393/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7393/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7393/Reviewer_QCXK"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7393/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698759963799,
        "cdate": 1698759963799,
        "tmdate": 1699636885388,
        "mdate": 1699636885388,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QtQSrJz8El",
        "forum": "OGtnhKQJms",
        "replyto": "OGtnhKQJms",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7393/Reviewer_rWVP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7393/Reviewer_rWVP"
        ],
        "content": {
            "summary": {
                "value": "This work studies causal representation learning under partial observability. Causal representation learning is the emerging field of learning generative representations of data that could potentially have causal relationships among them. Identifiability refers to the existence of a unique generative model and is an important aspect of this field. Many works on CRL usually make certain simplifying functional assumptions or access to interventions, in order to exhibit identifiability. In this work, the authors provide a unified framework for identifiability in multi-view CRL, under various assumptions.\n\nThe data is assumed to be split into multiple views which are observed, where each view is a non-linear mixing function of a subset of the latent variables. Standard assumptions are made, including that the prior density is smooth and mixing functions are diffeomorphic. Under these assumptions, block-identifiability results are derived, when we have access to multiple partial views of the data. Because of the generality of the result, various prior results in this field can be viewed as special cases. The conceptual idea is that if we have a set of views, then we can identify a set of content encoders using a mix of an alignment and a regularization term. However, this leads to a potentially exponential number of encoders, which the authors handle via a single view-specific encoder. Experiments on synthetic data and visual/text data (Causal3DIdent and Multimodal 3DIdent) validate their theoretical ideas. The R^2 metric is reported, which measures how well the latents have been recovered. Overall, the paper is technically well-executed and fits the conference.\n\n### References:\n\n- [1] Learning Linear Causal Representations from Interventions under General Nonlinear Mixing\n\n- [2] Nonparametric Identifiability of Causal Representations from Unknown Interventions\n\n- [3] Identifiability of deep generative models without auxiliary information\n\n- [4] Identifying Weight-Variant Latent Causal Models"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- Causal representation learning has gotten a lot of attention recently, due to the promises it holds. This work is another important step in this direction.\n\n- The general identifiability result captures a variety of prior works on CRL, therefore it serves as a neat unifying contribution.\n\n- The approach to avoid using exponential set of encoders for each subset of views is very neat and is crucial for this work, for efficiency purposes."
            },
            "weaknesses": {
                "value": "- R^2 metric is reported for validating identifiability, however as the authors also note, there are instances when R^2 scores can be inflated. Why didn't the authors also cite other standard identifiability metrics like MCC (such as the ones reported in [1], [2])?"
            },
            "questions": {
                "value": "Some questions were raised above. Additional suggestions:\n\n- The recent work [1] also uses contrastive learning in CRL. Are there any resemblances to their approach, i.e. how do their loss function compare to content alignment?\n\n- The works [3], [4] also study nonlinear ICA without auxiliary variables. I believe they use variants of VaDEs in their experiments, can this also be viewed as a soft alignment inductive bias?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7393/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7393/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7393/Reviewer_rWVP"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7393/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698789694470,
        "cdate": 1698789694470,
        "tmdate": 1699636885250,
        "mdate": 1699636885250,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "v7pyVn2vTr",
        "forum": "OGtnhKQJms",
        "replyto": "OGtnhKQJms",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7393/Reviewer_RoXZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7393/Reviewer_RoXZ"
        ],
        "content": {
            "summary": {
                "value": "The authors study latent variable identifiability in the setting where there are multiple observations (views), each being a nonlinear mixture of a subset of latent components. Existing work identifies the shared latents for a given set of views $V$. This work generalizes the existing results, where the shared latents are identified for any subset of views $V_i \\subseteq V$. Moreover, this is done simultaneously for all possible $V_i$ in a single training run. Many related identifiability results can be framed as a special case of the authors\u2019 general framework."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper is written extremely well, and reads like a chapter from a textbook. The authors frequently refer to a running example and include \u201cintuition\u201d paragraphs to make it easier to understand their definitions and results. The loss that leads to identifiability (Eq. 3.1) is simple and intuitive, which makes me optimistic about the (eventual) practical applicability of this approach. The authors\u2019 framework unifies many existing theoretical results in nonlinear ICA and causal representation learning, and also explains the empirical success of certain existing methods. This is a very strong paper that made me want to learn more about the related literature."
            },
            "weaknesses": {
                "value": "The main weakness of this work is its impracticality and lack of experimental results on realistic datasets. However, this is also acknowledged by the authors, and can also be said regarding most papers in this research area."
            },
            "questions": {
                "value": "I understand that it is more general to be able to simultaneously identify the content blocks for all possible $V_i$, but what practical benefit does this generality bring? Was this necessary in order to unify a wider range of existing identifiability results? If so, can you give a  specific example how this generality helped?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7393/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7393/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7393/Reviewer_RoXZ"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7393/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699070985914,
        "cdate": 1699070985914,
        "tmdate": 1699636885131,
        "mdate": 1699636885131,
        "license": "CC BY 4.0",
        "version": 2
    }
]