[
    {
        "id": "xsPjjb4mSR",
        "forum": "bjlTHVAkHS",
        "replyto": "bjlTHVAkHS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5551/Reviewer_8SLG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5551/Reviewer_8SLG"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the robustness of large language models (LLMs) to conflicting prompts that may contain contrasting information in real-world applications. The paper proposes a quantitative benchmarking framework and conducts role-playing interventions to control LLMs' preferences. The paper also evaluates seven open-source and closed-source LLMs on a knowledge robustness evaluation (KRE) dataset, revealing their varying performance and behavior."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The topic is interesting and novel, and the paper makes several valuable contributions, including:\n\n1. Establishing a comprehensive benchmarking framework that comprises a KRE dataset, a robustness evaluation pipeline, and corresponding metrics.\n2. Defining two types of factual robustness (vulnerable and resilient) and three types of decision styles (intuitive, dependent, and rational) to measure LLMs' behavior.\n3. Implementing role-play interventions to alter LLMs' robustness and adaptivity by instructing a specific role style.\n4. Constructing a robustness leaderboard to compare the performance and capabilities of different LLMs.\n5. The experiment results are sufficient enough to support the claims and the presentation of experiment results are clearly."
            },
            "weaknesses": {
                "value": "However, the paper also has some limitations and weaknesses that could be improved in future work, such as:\n\n1. The instructions for the KRE dataset are generated by 12 hand-crafted prompts written by 4 individuals and then rephrased using ChatGPT. But, it's unclear for me what guidance was provided for generating these instructions. Additionally, the small number of human evaluators (only 4) for human evaluation and of instruction annotators may introduce biases or noise in the data quality, which could affect the robustness evaluation results.\n2. The organization of the introduction is challenging to follow, as it introduces several new issues, such as decision style and instructing a role, without clear intuitive connections to the main theme of the paper. Maybe provide some examples to explain these issues would be better. \n3. The problem setting appears novel but complex to understand, particularly in terms of motivation. The framework involving negative and positive context, in-context learning, and decision-making (as shown in Figure 1) needs further clarification regarding its motivation. In addition, are a large amount of human-written works required to implement such framework?\n4. The evaluation concepts are comprehensive, covering factual robustness and decision-making style scores, but the technical novelty of the evaluation metrics appears to be lacking, primarily relying on frequency-based assessments."
            },
            "questions": {
                "value": "1. What are role instructions? What the effects of these? And dose role instructions are reasonable and useful in read-world application?\n2. How are positive and negative contexts constructed, and what are some application scenarios for factual robustness? Can you provide examples?\n\nMinor issues:\nThere are some typos, such as \"After unraveling the preference.\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5551/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5551/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5551/Reviewer_8SLG"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5551/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698668573197,
        "cdate": 1698668573197,
        "tmdate": 1699636570382,
        "mdate": 1699636570382,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fdPbiyPxcT",
        "forum": "bjlTHVAkHS",
        "replyto": "bjlTHVAkHS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5551/Reviewer_WVwf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5551/Reviewer_WVwf"
        ],
        "content": {
            "summary": {
                "value": "The paper investigates the robustness of Large Language Models (LLMs) when faced with prompts that conflict with their internal memory or present noisy information. The authors introduce a quantitative benchmarking framework to assess this robustness and perform role-playing interventions to determine how LLMs prioritize information. They explore two types of robustness: \"factual robustness,\" which is the model's ability to discern correct facts from prompts or memory, and \"decision style,\" which classifies how LLMs make choices (intuitive, dependent, or rational) in the absence of a clear correct answer.\n\nThe study, which tested both open-source and closed-source LLMs, found that these models are prone to being misled by incorrect prompts, particularly regarding commonsense knowledge. The results also suggest that while detailed instructions can reduce the number of misleading answers, they may lead to an increase in invalid responses. By implementing role-play interventions, the authors were able to assess the limits of robustness and adaptability across LLMs of different sizes."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper undertakes comprehensive experimental analysis to assess the robustness of Large Language Models (LLMs) when encountering prompts with conflicting information.\n- Several conclusions drawn by the paper are perceptive and provide valuable contributions to the community."
            },
            "weaknesses": {
                "value": "- Certain conclusions presented in the paper appear to necessitate additional discourse and empirical validation.\n- The manuscript's writing could substantially benefit from enhancements, as the current draft gives the impression of being hastily prepared."
            },
            "questions": {
                "value": "- My primary reservation regarding the paper's conclusions pertains to the variability in scores such as DMSS, VR, and RR for LLMs, as depicted in Figure 5. This variability leads to fluctuations in the ranking of models; for instance, Bard's DMSS score falls below GPT-4 in the dependent and original roles but surpasses it in the intuitive role. Similar patterns are observed with VR and RR scores. Given this, one must question whether the paper's conclusions are contingent upon the specific roles and prompts used in the study. Could it be possible that the findings are not universally applicable but rather prompt-specific?\n\n- The use of role-play interventions in the paper does not appear to constitute a novel contribution. This method, which involves instructing LLMs to either consider or disregard context by modifying the prompt, is a common approach in querying LLMs. This leads to a pertinent inquiry: if the intention is for the LLM to overlook the context information, what is the rationale behind including it in the prompt? Moreover, if the context is omitted, would that not represent a more 'intuitive role' than the one delineated in the study?\n\n- The manuscript would greatly benefit from improvements in its presentation and clarity. For instance, the font size in Figures 3, 4, 7 and 10 is too small to be legible. There are inconsistencies in capitalization, as seen with the word \"robust\" in the title of Section 4.1, and in Sections 4.4 and 4.5, all the letters in the titles are lowercase. Furthermore, there are formatting issues with the citation styles in the related work section, where \"\\cite\" is used instead of the correct \"\\citep\". These, coupled with numerous grammatical mistakes\u2014including in the abstract\u2014suggest a lack of thorough proofreading and give the impression that the paper may have been hastily composed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5551/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5551/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5551/Reviewer_WVwf"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5551/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698945901496,
        "cdate": 1698945901496,
        "tmdate": 1699636570297,
        "mdate": 1699636570297,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pPcaholGJI",
        "forum": "bjlTHVAkHS",
        "replyto": "bjlTHVAkHS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5551/Reviewer_WrsR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5551/Reviewer_WrsR"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the robustness of LLM on two different aspects vulnerable robustness (VR) and resilient robustness (RR). VR measures if the LLM can neglect wrong info in the prompts and respond the correct answers. RR measures if the LLM can respond correctly using the info provided in the prompt. The author proposes a benchmark based on question answering datasets. The author inject some wrong context (i.e., negative contexts) and examples to the prompts in order to measure robustness. They benchmark in several open-sourced and closed-sourced LLMs. The results show that large models are more robust."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Robustness of LLM is an important problem. The proposed benchmark seems to be a viable first step towards evaluating the robustness. Another strength is that the study is compreshensive."
            },
            "weaknesses": {
                "value": "- Notations are confusing. \n- What does \"high-quality\" mean in Section 2?\n- What is the definition of commonsense? This term has 14 occurrence in the paper but never been formally defined. This harms the clarity of this paper\n-  In Section 4.1, \"However, their robustness against negative context introduced by conflicting prompts remains suboptimal. Consequently, as the field progresses, enhancing robustness against adversarial negative context is likely to emerge as a paramount research focus\". I agree the result shows that LLM remains suboptimal when negative context are presented. However, I'm not convinced that \"adversarial prompts\" is a concern since the users have no intention to design adversarial prompts when using LLMs. I can be wrong since how and when adversarial prompts impact LLM users are not specified.\n-  In Section 4.1, this sentence is not clear: \"This result shows that LLMs prioritize the prompts with factual knowledge more than with commonsense knowledge.\". I cannot see why the above result leads to this conclusion. Maybe I don't understand the definition of factual knowledge and commonsense knowledge.\n- Figures 2 and 3 are very hard to parse. I suggest splitting them to different subfigures.\n- In Section 4.4, \"Interestingly, LLaMA the only one that aligns with the Rational Style,\" From Table 2, it seems that LLaMA is not rational right? Did I read anything wrong?\n- What is \u201dGod\u2019s-eye view\" instructions?\n\nOverall, the main weakness of this paper is presentation. At its current form, it's very hard to follow and parse the experimental results. Another weakness is that some of analysis are not well-motivated. For example, I don't get the idea of doing decision style analysis."
            },
            "questions": {
                "value": "- Typo: Section 4.2, \"Dose\" -> \"Does\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5551/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699070373581,
        "cdate": 1699070373581,
        "tmdate": 1699636570213,
        "mdate": 1699636570213,
        "license": "CC BY 4.0",
        "version": 2
    }
]