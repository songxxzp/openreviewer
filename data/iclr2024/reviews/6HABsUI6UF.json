[
    {
        "id": "r5G3hsLHB6",
        "forum": "6HABsUI6UF",
        "replyto": "6HABsUI6UF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5680/Reviewer_BtF4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5680/Reviewer_BtF4"
        ],
        "content": {
            "summary": {
                "value": "This paper analyzes the catastrophic forgetting issue in continual learning.\nThe authors focus on forgetting in the *representation*, which is narrowly defined as the last activation right before the final output.\nThe main findings are summarized as follows.\n1. Catastrophic forgetting does occur in the representation.\n2. This forgetting harms the final performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The text is easy to follow.\nThe main findings are clearly stated at the beginning."
            },
            "weaknesses": {
                "value": "### Novelty\n\nThe major weakness of this work is that the main findings are not new.\nI think the two main findings are the most basic assumptions of continual learning.\nPersonally, the most surprising part of this paper was that there are several works claiming that forgetting is minimal in the representation.\nBut if forgetting in the representation is negligible, why would the entire field of continual learning exist?\n\n\n### Limited Scope of Analysis\n\nThe analyses in this work were conducted in quite narrow settings.\nThe authors focused exclusively on the last activation of a network for classification tasks.\nFurthermore, they concentrated solely on the offline continual learning scenarios, excluding online continual learning from their scope.\n\n### Writing\n\nWhile I didn't have much trouble understanding the paper, there is room for improvement in the overall writing, particularly in terms of grammar."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5680/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698591871222,
        "cdate": 1698591871222,
        "tmdate": 1699636593182,
        "mdate": 1699636593182,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Yv0ihgOY8K",
        "forum": "6HABsUI6UF",
        "replyto": "6HABsUI6UF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5680/Reviewer_gcjs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5680/Reviewer_gcjs"
        ],
        "content": {
            "summary": {
                "value": "This paper tries to explore whether neural networks suffer from catastrophic forgetting at the level of representations. This paper focuses on two questions: Do continually trained representations forget catastrophically, and Does it matter that these representations are forgotten. To answer these questions, the main contributions of this paper are summarized as\"\n\na. This paper shows that continually learned respresentations do forget catastrophically.\n\nb. The respresentation forgetting negatively affects knowledge accumulation.\n\nc. This paper also consider feature forgetting and knowledge accumulation in continual learning methods.\n\nd. This paper explores the feature forgetting with self-supervised and contrastive losses."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "a. This work is novel and explores whether neural networks suffer from catastrophic forgetting at the level of representations. \n\nb. This paper is well-written and easy to follow.\n\nc. Extensive experiments. I appreciate that this paper provides extensive experiments to show the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "Although the experimental phenomenon presented in this paper is very interesting, it is essentially an experimental work. It is hard to determine whether the conclusions drawn in this paper are widespread or only based on the experimental settings (models, datasets) used in this work. However, this type of work is OK and interesting."
            },
            "questions": {
                "value": "In Section 3, this paper argues that before learning task $t$, the model contains different level of information of task $t$ in different scenarios (red and blue lines). It is noteworthy that all tasks in CL are disjoint (mentioned in Section 2) and the model starts from scratch (mentioned in Appendix A). How does the model achieve the knowledge of task $t$ before learning task $t$ and why are you confirmed that this knowledge achievement is valid? Please discuss more about it and it is better to provide theoretical and experimental support."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5680/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698764905053,
        "cdate": 1698764905053,
        "tmdate": 1699636593085,
        "mdate": 1699636593085,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Dw1Y5zRSkp",
        "forum": "6HABsUI6UF",
        "replyto": "6HABsUI6UF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5680/Reviewer_acbx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5680/Reviewer_acbx"
        ],
        "content": {
            "summary": {
                "value": "The paper investigates the relationship between representation feature forgetting and knowledge accumulation during continual learning. The paper suggests relative forgetting that is very similar to Backward Transfer, yet dividing it with the performance improvement obtained by training the target task. To demonstrate the authors' claims, the paper provides multiple empirical results. In summary, they find that the continual learning models consistently forget the features severely, and this interferes with knowledge accumulation when learning new tasks. Additionally, this feature forgetting can be alleviated through adopting various continual learning approaches."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The authors scrab various claims on catastrophic forgetting during continual learning from multiple literatures. And suggests a new relative forgetting metric."
            },
            "weaknesses": {
                "value": "- Regarding feature forgetting, the paper simply repeats the observations of prior/conventional literature on continual learning: forgetting occurs, and it matters the performance of the model. In that sense, the suggested relative forgetting metric does not show any distinguished observations on existing metrics like Backward Transfer and Averaged Forgetting.\n\n- Limited contribution: Although the paper is dedicated to studying well-known and sufficiently analyzed challenges in continual learning fields, the evaluation tasks, domains, models, method types (e.g., rehearsal-/architecture-/regularization-/prompt-based approaches), ..., are limited, and it is hard to catch 'new'/'novel' insights. - Most observations resort to image-based benchmark classification tasks. There are various continual learning approaches in vision/language/multimodal domains with diverse tasks, segmentation/object detection/generation/text classification/(visual) question answering, etc. \n\n- Presentation/writing can be further improved. It seems to include repeated claims and unnecessary sentences. For example, the first paragraph in Section 3 is about what is 'catastrophically', but this paragraph is not aligned with the overall flow and arguments of the paper. the word 'catastrophic' simply indicates critically bad, or severe, and no more implication. \n\n- The faithfulness/benefit of relative forgetting is not clearly described. Regardless of initial performance on target tasks in continual learning, the model contains the most beneficial representations of the task when its performance is the highest during continual learning, and the degenerated performance can be considered as knowledge loss, i.e., forgetting. As shown in the paper, this new metric shows a similar tendency to existing forgetting/backward transfer metrics without new insights, It is not clear why we need to care about the 'relative' forgetting.\n\n- In section 4, the suggested ensemble baseline violates the conventional continual learning setting and is clearly different from the typical continual learning model. Let us store N backbone models by training N past tasks sequentially, the authors concatenate all features on evaluation data from these models and propagate the concatenated features to the classifier. Here, the input dimension of the classifier is different from the base continual learning model (proportional to the number of past tasks (i.e., stored models)), and this means the trainable parameters are N times larger. This is totally different model, and evaluation analyses with the assumption that 'the base continual learning model and ensemble models learn the continual learning tasks in the same way' may not be correct.\n\n- Aligned with the second weakness, I strongly recommend that the authors provide further clear contributions against earlier works that extensively study representational forgetting and transferability in continual learning methods [1,2]. In particular, [2] also observed different behaviors among supervised, self-supervised, and contrastive continual learning in view of representation forgetting and knowledge accumulation.\n\n[1] Chen et al., \"Is forgetting less a good inductive bias for forward transfer?\" ICLR 2023.  \n[2] Yoon et al., \"Continual Learners are Incremental Model Generalizers\", ICML 2023."
            },
            "questions": {
                "value": "."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5680/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698913065620,
        "cdate": 1698913065620,
        "tmdate": 1699636592968,
        "mdate": 1699636592968,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "p8YXxVmbN3",
        "forum": "6HABsUI6UF",
        "replyto": "6HABsUI6UF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5680/Reviewer_1Fn7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5680/Reviewer_1Fn7"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the quality of learned representations in continual learning. With the help of two new metrics -- linear probe accuracy and relative forgetting, it is shown that representation learning also suffers from catastrophic forgetting in both continual supervised learning and continual self-supervised learning, and thus reduces the overall task performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The two proposed metrics --- linear probe accuracy and relative forgetting --- are useful for the community.\n- Experiments are performed in both continual supervised learning and continual self-supervised learning."
            },
            "weaknesses": {
                "value": "- The writing and presentation can be significantly improved. For example, Figure 4 is quite confusing. What does each square mean? What does each color represent? How about the sizes of the squares?\n- Lack of deep analysis. This is my major concern. Since this paper does not propose new methods or new theories, I would expect to see more insights about continual representation learning, which the paper does not provide much. The main conclusions are within expectation and well known in the continual learning community. Knowledge accumulation and feature forgetting are another expression of the stability-plasticity dilemma. I would encourage authors to improve this work by providing deeper analysis. For example,\n  - In the abstract, it is mentioned that \"Some studies ascribe a certain level of innate robustness to representations, that they only forget minimally and no critical information, while others claim that representations are also severely affected by forgetting.\" Why does the contradiction exist? With two proposed metrics, can we explain and verify this contradiction effectively and directly? \n  - Different methods are tested to prevent representation forgetting. However, there is a lack of analysis or discussion to explain why method A is better than method B."
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5680/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699395937087,
        "cdate": 1699395937087,
        "tmdate": 1699636592859,
        "mdate": 1699636592859,
        "license": "CC BY 4.0",
        "version": 2
    }
]