[
    {
        "id": "GTDQOzzQCb",
        "forum": "Tigr1kMDZy",
        "replyto": "Tigr1kMDZy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8426/Reviewer_Lx2y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8426/Reviewer_Lx2y"
        ],
        "content": {
            "summary": {
                "value": "This paper studies inner mechanism of LLMs when facing false demonstations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I like this paper. This work provides interesting findings."
            },
            "weaknesses": {
                "value": "1. Since the authors conduct experiments under in-context learning setting, we may not be sure these findings are from models themself or implicit tuning (in-context learning). Could the authors provide explanations?\n\n2. Can we expand tasks to include generation-based tasks like QA?"
            },
            "questions": {
                "value": "Please see Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8426/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697879978243,
        "cdate": 1697879978243,
        "tmdate": 1699637050092,
        "mdate": 1699637050092,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wvYkaq3a7U",
        "forum": "Tigr1kMDZy",
        "replyto": "Tigr1kMDZy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8426/Reviewer_bUf7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8426/Reviewer_bUf7"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on understanding how language models process correct versus incorrect inputs in few-shot learning contexts, emphasizing harmful imitation. Two phenomena are identified: overthinking and false induction heads. Overthinking occurs when models, given incorrect demonstrations, tend to have reduced accuracy when predictions are decoded from later layers; stopping the model early, before all layers have processed the input, can increase accuracy. False induction heads are specific components in the model that contribute to overthinking by attending to and replicating incorrect information from previous demonstrations. The study's findings advocate for a deeper examination of intermediate model computations to understand and mitigate harmful behaviors in language models\u200b."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The paper explores new concepts like \"overthinking\" (previously introduced) and \"false induction heads\" (novel), offering a fresh lens to study transformers, particularly in handling harmful imitation.\n\n- Solid research execution is evident. Specific model components, like attention heads causing errors, are identified and well analyzed.\n\n- The paper is well-structured and very readable. Concepts are clearly and consistently articulated.\n\n- The methodologies provided comprise a strong set of interpretability methods directed specifically at in-context learning and the role of the examples included.\n  - For instance, interpreting outputs from intermediate layers in such ways may prove useful for understanding what the optimal sets of example to provide to the LLM is, in addition to what is attempted in this paper, which is to understand how bad examples affect overall accuracy.\n\n- The paper studies the important issue of harmful content (which, as outlined above, may extend eventually to non-useful or even non-optimal content within the context). Overall, the findings are practically significant and offer insights for controlling model outputs against harmful examples and wrongful imitation."
            },
            "weaknesses": {
                "value": "- The paper details the phenomena of overthinking and false induction heads, but it doesn\u2019t fully articulate how these insights could be applied practically to improve model behavior.\n  - For instance, while head ablations are discussed as a method to reduce overthinking, the practical impact of these modifications on model functionality and broader applicability is not thoroughly explored in real-world scenarios\u200b i.e. how does one know when the input examples are likely to be false, and that heads ought to be ablated?\n- The methodology involves decoding from intermediate layers and identifying false induction heads, but the paper could provide a more explicit explanation of these processes i.e. how predictions were extracted and analyzed at each layer, and how the heads contributing to overthinking were identified and analyzed.\n- The paper could better acknowledge that the 'incorrectness' of permuted labels can vary based on the dataset and what each label signifies. Some permuted labels might be clearly wrong, while others could be more ambiguous, and providing specific examples for each dataset could clarify this aspect and help to understand the results.\n  - It's not clear in the example from Figure 1 that demonstrations are \"incorrect\", and that all \"imitation\" is wrongful- there is still an argument that context is important, and in this example (at least in the example where all labels are permuted), could the LLM not understand the label permutation and simply interpret that negative sentiments require a positive classification, and vice versa? \n- The reference and justification around the use of the logit lens near the start of the paper could be strengthened, though the authors do come back to discuss it in more detail towards the end."
            },
            "questions": {
                "value": "Please see weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8426/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8426/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8426/Reviewer_bUf7"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8426/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698654093170,
        "cdate": 1698654093170,
        "tmdate": 1699637049967,
        "mdate": 1699637049967,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6fvkGKfrX6",
        "forum": "Tigr1kMDZy",
        "replyto": "Tigr1kMDZy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8426/Reviewer_qL6Z"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8426/Reviewer_qL6Z"
        ],
        "content": {
            "summary": {
                "value": "The paper investigates how language models can learn to generate harmful outputs from few-shot prompts containing inaccuracies or biases. \n\nThe key findings are:\n\nModels tend to \"overthink\" and decrease in accuracy on incorrect prompts after a certain \"critical layer\", while accuracy keeps improving on correct prompts. This suggests incorrect information mainly affects later processing stages.\nAttention heads in later layers preferentially attend to and reproduce incorrect labels from the prompt demonstrations. Ablating a small number of these \"false induction heads\" significantly reduces the accuracy gap between correct and incorrect prompts."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper:\n- Provides novel insights into the internals of in-context learning through layerwise decoding and attention analysis. Links model behavior to specific components.\n\n- Extensive experiments across models, datasets and prompt variations. Head ablation results generalize across tasks.\n\n- Connects to related ideas like overthinking and induction heads. Builds understanding of model internals."
            },
            "weaknesses": {
                "value": "The paper lacks in the following ways:\n- Focuses on a simplified setting of permutations of class labels. \n- Does not cover more subtle inaccuracies or biases.\n- Only studies text classification tasks. Unclear if findings apply to more open-ended generative tasks.\n- No modification of model training process itself to mitigate issues."
            },
            "questions": {
                "value": "- If the prompt contained factual inaccuracies or harmful content, would you expect to see similar overthinking and false induction effects?\n- Could you train with additional regularization to discourage attention to incorrect prompt content?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8426/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8426/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8426/Reviewer_qL6Z"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8426/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698735013640,
        "cdate": 1698735013640,
        "tmdate": 1699637049840,
        "mdate": 1699637049840,
        "license": "CC BY 4.0",
        "version": 2
    }
]