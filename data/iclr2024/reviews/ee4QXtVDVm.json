[
    {
        "id": "e4ld2ebzzF",
        "forum": "ee4QXtVDVm",
        "replyto": "ee4QXtVDVm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5648/Reviewer_zsjW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5648/Reviewer_zsjW"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a method, denoted as Subword Embedding of Bytes, aimed at generating subword embeddings based on their byte representations, thereby improving the security of private training data within the context of federated learning. The approach initially involves the segmentation of an input text into a subword sequence using a well-established tokenization procedure. Subsequently, each subword is represented by a distinct byte sequence of uniform length, with each byte linked to an associated embedding. The byte embeddings used to represent a subword are concatenated and transformed to the subword embedding by a feed-forward network. The main idea behind this method is that the same byte is most likely used to produce many subwords, rendering the process of recovering input texts through the gradients of byte embeddings substantially more challenging compared to traditional subword embeddings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(1) This study introduces a method for constructing subword embeddings from their byte representations. The experimental findings in the domains of machine translation and sentiment analysis validate the efficacy of the proposed method, demonstrating its ability to impede the retrieval of client training data by adversarial models in federated learning.\n\n(2) The paper is generally well-written and easy to follow."
            },
            "weaknesses": {
                "value": "(1) The potential applicability of the proposed method in the context of large language models (LLMs) needs further investigation to substantiate its relevance and effectiveness in such contexts.\n\n(2) The utilization of randomly sampled byte sequences for subword representation, with subsequent subword embeddings derived through linear transformations, raises concerns about the extent to which these embeddings genuinely encapsulate word meanings. The effectiveness of subword embeddings in capturing semantic similarities, where words that are closer in the vector space correspond to similar meanings, remains uncertain. This ambiguity could have implications for the method's generalization capacity.\n\n(3) he paper lacks a comparative analysis against other gradient-based attack methods and does not account for diverse federated learning settings, including variations in the number of clients, participation rates, and heterogeneity. A more comprehensive evaluation in various scenarios could enhance the paper's robustness and practical relevance."
            },
            "questions": {
                "value": "(1) Could you include a comparative analysis of the results with the defense method as described in the FILM attack (Gupta et al., 2022)?\n\n(2) Can the byte embeddings be pre-trained using unlabeled text corpora? If yes, were the byte embeddings pre-trained?\n\n(3) Could you provide a comparative analysis of the results obtained through different methods of incorporating positional embeddings, specifically through addition and concatenation?\n\n(4) Is the embedding method proposed in this study compatible with the \"pre-training + fine-tuning\" paradigm?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5648/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698556109471,
        "cdate": 1698556109471,
        "tmdate": 1699636587230,
        "mdate": 1699636587230,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1mVCKfCCPo",
        "forum": "ee4QXtVDVm",
        "replyto": "ee4QXtVDVm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5648/Reviewer_ybMb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5648/Reviewer_ybMb"
        ],
        "content": {
            "summary": {
                "value": "This paper presents SEB, a novel text representation method that can protect NLP models agaisnt data leaking attacks in federated learning. Experimental results show that SEB can achieve comparable performance in various NLP tasks with better privacy perservation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.Mitigating the data leakage of NLP models in federated learning is an important research problem. This paper presents a novel and practical method to solve this issue.\n2.SEB can achieve comparable and even superior performance in original NLP tasks compared to subword baselines with less space complexity and better privacy preservation."
            },
            "weaknesses": {
                "value": "SEB is proposed to mitigate the privacy leakage in Federated Learning. However, it seems that the evaluation experiments are conducted in the traditional centralized-learning environment. It would be nice if the authors provide the experimental results about the effectiveness of SEB in distributed learning."
            },
            "questions": {
                "value": "Can SEB remain effective on large datasets and language models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5648/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698574817653,
        "cdate": 1698574817653,
        "tmdate": 1699636587117,
        "mdate": 1699636587117,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Cyy3w18O7t",
        "forum": "ee4QXtVDVm",
        "replyto": "ee4QXtVDVm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5648/Reviewer_Crbt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5648/Reviewer_Crbt"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new method called Subword Embedding from Bytes (SEB) to protect privacy in NLP models while maintaining efficiency and accuracy. \nSEB is designed to defend against embedding-based attacks in federated learning by encoding subword information directly from byte-level representations. The authors demonstrate that SEB outperforms traditional subword embedding methods in machine translation, sentiment analysis, and language modeling tasks. \nAdditionally, SEB is shown to be effective in defending against a specific attack model introduced in previous work, which aims to reconstruct sentences from the victim's training batches. \nOverall, this paper argues that SEB is a promising solution for practical applications that require privacy-preserving NLP models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposal of a novel method called Subword Embedding from Bytes (SEB) to protect privacy in NLP models while maintaining efficiency and accuracy. The demonstration that SEB effectively defends against embedding-based attacks in federated learning, making it a valuable tool for practical applications.\n2. In the experimental section, the verification that SEB outperforms traditional subword embedding methods in machine translation, sentiment analysis, and language modeling tasks."
            },
            "weaknesses": {
                "value": "I'm not very familiar with federated learning research. So please correct me if I have any misunderstanding of the paper. \nI identify the following potential limitations in this paper:\n1. The proposed SEB approach can only defend against embedding-based attacks, which recover the sentences in a batch of text data, based on the gradients in federated learning, which may be limited. Does this kind of embedding-based attack serve as the core of privacy attacks in federated learning? It would be great if the authors could provide solid motivations to justify the significance of the considered research problem. \n2. I'm not very excited about the experimental settings in this paper because only some outdated models/datasets are considered. For example, the tasks include the sentiment analysis with the IMDB dataset, which is already solved in NLP.  Is this still an interesting task in the federated learning research in the era of large language models?  Also, the LSTM&BERT is considered the backbone model, which is rarely used for NLP research. \n3. It would be more exciting to see some in-depth analysis of the approach. For example, given the results that SEB can obtain better results over traditional subword embedding methods in some NLP tasks, it would be great if the authors could look into this amazing finding and provide some concrete insights."
            },
            "questions": {
                "value": "Please refer to the weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5648/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5648/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5648/Reviewer_Crbt"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5648/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698618111309,
        "cdate": 1698618111309,
        "tmdate": 1699636587004,
        "mdate": 1699636587004,
        "license": "CC BY 4.0",
        "version": 2
    }
]