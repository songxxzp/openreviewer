[
    {
        "id": "xQYVCREpL8",
        "forum": "WjRPZsfeBO",
        "replyto": "WjRPZsfeBO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1034/Reviewer_e5jX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1034/Reviewer_e5jX"
        ],
        "content": {
            "summary": {
                "value": "The paper summarizes the statistical error bounds for Wasserstein Autoencoders (WAE). Assuming the existence of the true smooth autoencoder model and (intrinsic) low-dimensionality of data distribution, this paper derives the order of the errors of the estimated model with respect to sample number $n$ and other assumed quantities. The resulting corollaries are also given."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The asymptotical error bounds for the estimated WAE is clearly presented with thorough proofs. This guarantess the asymptotical convergence of the WAE model if the target distribution and the true encoder/decoder are well-regularized."
            },
            "weaknesses": {
                "value": "The assumption of the existence of true generator and the true encoder seems to be a strong assumption. Practically, the low-dimensional prior distribution is chosen to be Gaussian, and this distribution may not be able to represent the intrinsic structure of the data distribution.\n\nAlso, it is common to use GAN loss as the dissimilarity measure for the penalty in the objective function, but it seems that GAN loss is not considered."
            },
            "questions": {
                "value": "Can you show a numerical experiment that supports up the asymptotical rate of the presented error bound? Figure 1 may be close to this, but the comparison with $O(n^\\alpha)$ or quantity of the whole objective function should be presented.\n\nAlso, is the theorem applicable for WAE-GAN?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics review needed."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1034/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698502686235,
        "cdate": 1698502686235,
        "tmdate": 1699636029748,
        "mdate": 1699636029748,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zbtspOEFJi",
        "forum": "WjRPZsfeBO",
        "replyto": "WjRPZsfeBO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1034/Reviewer_JGBa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1034/Reviewer_JGBa"
        ],
        "content": {
            "summary": {
                "value": "The paper proves statistical sample complexity for Wasserstein autoencoders on low-dimensional data. The provided sample complexity depends on the data intrinsic dimension, indicating that autoencoders are adaptive to the data intrinsic structures. The analysis also includes MMD divergence. This study is relatively new in the literature, as it is parallel to the GANs analyses and compared to \"Deep nonparametric estimation of intrinsic data structures by chart autoencoders: Generalization error and robustness\", this paper includes the regularization using Wasserstein distance or MMD."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is quite theoretical, but the presentation is relatively easy to follow. The error decomposition in Lemma 7 is helpful in understanding the rate in later sections. As far as I can tell, the theoretical claims are correct.\n\nThe paper also provides some numerical results suggesting that the performance of autoencoders depends on data intrinsic dimension. This is good to have as a motivation for the study."
            },
            "weaknesses": {
                "value": "The theoretical analysis is based on a strong assumption that the low-dimensional data has a global parameterization. This can limit the application of the results.\n\nThe analysis in the paper appears to be standard, and the majority of the idea is adapted from existing research on GANs. However, I would like to emphasize that putting all the details together is not a straightforward task. Moreover, the analysis to MMD is relatively not very common in existing literature. Therefore, I am not criticizing the contribution of the paper, but just pointing out that the technical novelty might not be significant.\n\nI would suggest writing a corollary in Remark 11.\n\nIn Section 4, it might be helpful to provide some examples, such as a smooth $d$-dimensional manifold has an upper Minkwoski dimension bounded by $d$. More discussions can be found in \"Sharp asymptotic and finite-sample rates of convergence of empirical measures in Wasserstein distance\"."
            },
            "questions": {
                "value": "See weakness above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1034/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698809860490,
        "cdate": 1698809860490,
        "tmdate": 1699636029633,
        "mdate": 1699636029633,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fy5fWYZvgg",
        "forum": "WjRPZsfeBO",
        "replyto": "WjRPZsfeBO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1034/Reviewer_GHGf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1034/Reviewer_GHGf"
        ],
        "content": {
            "summary": {
                "value": "The paper provides a statistical analysis for Wasserstein Autoencoder (WAE) method. Specifically, the paper gives an upper bound for the excess risk for the empirical estimator of the encoder and decoder when both are approximated by neural networks. Under certain assumptions, the proposed method obtains an approximate of the optimal en/decoders, which is shown to have a converging excess risk. The analysis is applied to both Wasserstein and MMD dissimilarities. Furthermore, the adequate latent dimension is used in the bounds, showing that lower dimensional data complies with the corresponding rate."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is overall well written and presented, and the the ideas are original to the knowledge of the reviewer. The discussion of all results seem plenty and extensive. Some strengths:\n1. The idea of analyzing WAE statistically seems to be an interesting question, as most of such study seem to be on other generative models. The paper provides solid convergence guarantees, which seems expected but still crucial. \n2. The paper provides thorough discussion on the error and convergence of the model, including both sample complexity and asymptotics. The error bound accounts for sample size, neural network size, optimization oracle, and dimensionality dependence, which is rather complete, as these includes almost all aspects of concern in practice. The limitations are also adequately addressed."
            },
            "weaknesses": {
                "value": "1. Though the paper provides the analysis all the way to containing both sample complexity and misspecification, the single question of sample complexity seems to be an interesting question on its own, i.e. population v.s. empirical over the same Holder class, not neural networks. In this setting, optimization is out of the picture, thus questions such as the convergence rate of $\\|id - G\\circ E\\|$ or $\\|\\hat{G}-G\\|$ can be asked without ambiguity. The reviewer understands that this is additional work, but encourages the authors to provide a discussion, rather than stuffing all ingredients all at once. This could possibly be related to the tightness (minimaxity) of the proposed bound.\n\n2. In Remark 11, the construction of $E^*$ does not seem correct, as repeating $E_1$ several times will only result in a graph supported on the diagonal, thus $E^{*}_\\sharp\\mu$ is not the uniform distribution over the higher dimensional cube. Similarly, the next claim in the remark over the use of $\\nu$ with independent marginals seems unjustified."
            },
            "questions": {
                "value": "Please see above (section Weaknesses) for details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1034/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1034/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1034/Reviewer_GHGf"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1034/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698819988513,
        "cdate": 1698819988513,
        "tmdate": 1699636029560,
        "mdate": 1699636029560,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NBqAjYm31V",
        "forum": "WjRPZsfeBO",
        "replyto": "WjRPZsfeBO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1034/Reviewer_oqXa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1034/Reviewer_oqXa"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a comprehensive framework for analyzing error rates in learning unknown distributions through Wasserstein Autoencoders (WAEs), particularly in scenarios where data points exhibit an intrinsic low-dimensional structure within a high-dimensional feature space. By characterizing this dimensionality using the Minkowski dimension of the target distribution's support, the paper establishes an oracle inequality that effectively balances model-misspecification and stochastic errors, offering insights into the trade-offs achievable through appropriate network architectures. Furthermore, the framework allows for a thorough examination of the accuracy of encoding and decoding guarantees, as well as the approximation of the target distribution by the generated push-forward measure. \nWhile the paper offers valuable insights into the theoretical underpinnings of WAEs, it acknowledges the challenges associated with accurately estimating the complete error due to the complexities of the optimization process. Notably, the paper emphasizes the need for further analysis, especially in contexts involving non-deterministic network outputs and Gaussian-based latent distributions, which are commonly employed in practical applications."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper demonstrates several strengths, particularly in its provision of theoretical analyses and implications regarding the selection of network sizes for encoders and decoders based on the number of training data samples. Notably, the theoretical results, particularly Theorem 8, provide valuable insights that ensure encoding and decoding guarantees, highlighting the paper's ability to establish a close relationship between the encoded distribution and the target latent distribution. Furthermore, the paper emphasizes the effectiveness of the generator in accurately mapping the encoded points back to their original positions, showcasing its robust understanding of the intricacies involved in data transformation and representation within the context of the study."
            },
            "weaknesses": {
                "value": "The paper exhibits certain weaknesses that should be addressed to strengthen its overall contribution. Firstly, while the paper presents intriguing theoretical results, the absence of an experimental demonstration, even with synthetic datasets, limits its practical applicability and hinders the validation of the theoretical findings in real-world scenarios. The lack of empirical evidence to support the theoretical claims diminishes the paper's credibility and inhibits a comprehensive understanding of the practical implications of the proposed concepts. Additionally, the paper falls short in elucidating how the theoretical results can be practically employed to guide the selection or design of encoder/decoder networks, particularly for enhancing the generation of sharper and more realistic images within the context of Wasserstein Autoencoders (WAEs). The absence of clear guidelines or methodologies for leveraging the theoretical findings to improve the image generation process highlights a critical gap in the paper's applicability and its potential impact on practical applications within the field. Addressing these limitations would significantly enhance the paper's overall credibility and ensure its relevance and practical significance within the research domain."
            },
            "questions": {
                "value": "No question"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1034/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699273556090,
        "cdate": 1699273556090,
        "tmdate": 1699636029498,
        "mdate": 1699636029498,
        "license": "CC BY 4.0",
        "version": 2
    }
]