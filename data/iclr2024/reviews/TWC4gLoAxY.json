[
    {
        "id": "FYeqVbqGD6",
        "forum": "TWC4gLoAxY",
        "replyto": "TWC4gLoAxY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1051/Reviewer_dixh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1051/Reviewer_dixh"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a framework to engage human-AI collaboration by incorporating logical reasoning aiming to develop a machine to help humans work together. In the framework, trajectories of human activities are given, and logic rules that map spatial-temporal states to goals. Then, the agents are deployed to new environments where they need to work together with humans and are trained to help humans predict their goals from the sequence of actions and states. In the experiments, the proposed framework outperformed other SOTA approaches, e.g., in the success rate. Different datasets/tasks and evaluation metrics are used in the evaluation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper addresses an important problem of integrating robots to work with humans. The idea of integrating logical reasoning for generalization in this problem setting seems novel. \nThe paper provides good empirical evaluations showing the proposed framework\u2019s advantages against baselines.\nThe experiments are conducted on practical datasets and environments, they give a great insight into the applications of neuro-symbolic methods, where symbolic logic and neural networks are integrated. Limitations are properly discussed."
            },
            "weaknesses": {
                "value": "Although I appreciate the paper\u2019s ideas and evaluations, I found some concerns about the paper.\n\nFirst, more discussions comparing the proposed approach to related studies need to be provided. Otherwise, it is not easy to understand how we can distinguish the proposed method from existing frameworks, and providing them would help readers understand the literature clearly.\n\nMoreover, some parts of the method explanation are somewhat hard to follow. In Sec 2.3, the query is introduced as $\\mathbf{v} = (\\mathbf{a}$, $\\mathbf{s})$, but there is no specification for these variables. I suppose this is a tuple of a sequence of actions and a sequence of states, but that should be noted explicitly. An intuitive explanation of what $\\mathbf{v}$ stands for would help readers.\n\nI list some minor comments:\n- In Eq. (5). it is written as $g \\in f$, but I\u2019m not sure this is a standard notation because here $f$ is a rule, not a set.\n- Typo: double periods in the caption of Fig. 3. \n-  I would kindly suggest including the literature on neuro-symbolic research (e.g. [1,2], but not necessary; it is the author\u2019s choice), since the proposed approach is significantly related to the field, and the community would benefit from this work. The readability of the method explanation could be improved.\n\n[1] Artur d'Avila Garcez, Lu\u00eds C. Lamb: Neurosymbolic AI: the 3rd wave. Artif. Intell. Rev. 56(11): 12387-12406 (2023)\n\n[2] Henry Kautz. The Third AI Summer: AAAI Robert S. Engelmore Memorial Lecture. AI Magazine, 43(1), 105-125 (2022)."
            },
            "questions": {
                "value": "What are the most related studies? How is the proposed approach compared and superior to them?\n\nIncluding the time and location in the predicate (in Eq. (4)) would generate a large number of ground atoms (atoms without variables), and typically, reasoners (including probabilistic ones e.g. ProbLog and Markov Logic Networks) need to compute ground atoms to perform reasoning.\nDoes the proposed framework suffer from the large number of logic representations to be generated in the inference? If not, how does it avoid this problem? Are there any restrictions over the considered language/environments?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1051/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1051/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1051/Reviewer_dixh"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1051/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698512535304,
        "cdate": 1698512535304,
        "tmdate": 1700655621873,
        "mdate": 1700655621873,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uFLYiPgIBN",
        "forum": "TWC4gLoAxY",
        "replyto": "TWC4gLoAxY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1051/Reviewer_WpfX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1051/Reviewer_WpfX"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a framework aimed at improving human-robot perception and collaboration by integrating logical rules and Theory of Mind (ToM). Logical rules provide interpretable predictions and generalize well across diverse tasks, making them valuable for learning and decision-making. Leveraging ToM to understand the mental states of others enhances effective collaboration. \n\nIn this approach, the authors use logical rules derived from observational data to infer human goals and guide human-like agents. These rules are treated as latent variables, and a rule generator is trained alongside a multi-agent system within the robot's cognitive framework. The process involves two stages: first, assessing the posterior distribution of latent rules using learned embeddings to represent entities and relations, with confidence scores indicating consistency with observed data. Second, a joint optimization of the rule generator and model parameters is performed, maximizing the expected log-likelihood."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "To assist humans, a hierarchical reinforcement learning model with ToM is employed to plan robot actions.   \nMultiple experiments validate each component of the framework, and the results on multiple benchmarks demonstrate that this model outperforms the majority of existing approaches.   \nThe combination of logical rules, Theory of Mind, and hierarchical reinforcement learning creates a comprehensive framework for enhancing human-robot collaboration and perception."
            },
            "weaknesses": {
                "value": "The rule generator and the reasoning evaluator are important but don't show the particular design.  \nThe Iterative Goal Inference seems that a process of conditional learning, so how to implement this part is not clear enough."
            },
            "questions": {
                "value": "Refer to the above comments."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1051/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1051/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1051/Reviewer_WpfX"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1051/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698821559771,
        "cdate": 1698821559771,
        "tmdate": 1701035334319,
        "mdate": 1701035334319,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3W2dYlT4Jw",
        "forum": "TWC4gLoAxY",
        "replyto": "TWC4gLoAxY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1051/Reviewer_iG1y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1051/Reviewer_iG1y"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on exploiting logic rules to guide human-like agents. They design a rule generator and rule evaluator to obtain useful rules given entities and relations, and apply hierarchical reinforcement learning with ToM to plan actions. The results show that the model achieves SOTA performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The method to construct and utilize knowledge graphs to plan action is novel and interesting.\n- The experiments show that the proposed method significantly outperforms baselines in various metrics. With the provided standard deviation, the table is more convincible.\n\nI am not familiar with reinforcement learning and am unable to assess this part."
            },
            "weaknesses": {
                "value": "- The definitions of entity, relation, and logic rule are inconsistent with the widely agreed definition in the knowledge graph academia. It seems that the users redefine them in the context of their task, while re-using these terminologies of knowledge graph. This makes the paper a little confusing, especially for audiences with a knowledge graph background.\n- Different examples of rules are inconsistent. In (3), an item of a logic rule is `Walk_to(person, bedroom)`, but the examples of rule 1 in Fig.3 contain an item `Walk_to(plate)`. It is inconsistent in whether `Walk_to` needs a person as the first entity argument. I wonder which setting is actually used in the method.\n- (Minor) In tables, the best results are not easy for audiences to discover. Please use bold text for the best results, and show \u2191 or \u2193 for each metric to indicate whether greater or less is better.\n- (Minor) There are a few confusing wordings. What is the meaning of \"hardness level\" in the caption of Tab.1 and Tab.2? Its meaning is more like \"non-softness\" than \"difficulty\"."
            },
            "questions": {
                "value": "In \"Weaknesses\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1051/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698824404061,
        "cdate": 1698824404061,
        "tmdate": 1699636031424,
        "mdate": 1699636031424,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "f1H917h1sk",
        "forum": "TWC4gLoAxY",
        "replyto": "TWC4gLoAxY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1051/Reviewer_wN2Q"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1051/Reviewer_wN2Q"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a novel framework aimed at improving human-AI collaboration by integrating logic-guided reasoning and theory of mind. The key contributions include a method for generating and evaluating logic rules from observational data to infer human intentions and actions, a hierarchical reinforcement learning model incorporating theory of mind for planning robot actions to assist humans, and comprehensive experiments on two datasets demonstrating the effectiveness and generalizability of the proposed model."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper proposes a novel method that utilizes logical reasoning to generate a broad understanding of the agent\u2019s objectives in a new environment and thus strengthen social perception and collaboration between humans and AI.\n2. The detailed experiment analysis shows the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1. The experimental tasks seem somewhat weak, as even Seq2Seq (Sutskever et al., 2014) can achieve reasonably good results (Table 1). Could the authors provide some more robust human-AI collaboration tasks for reference? (I apologize for not being familiar with the relevant field, so I can only provide a general reference [1]). Additionally, as a human-AI collaboration method, it would be more convincing if a human study could be included if possible.\n2. The authors should provide the related work in the main text rather than in the appendix. The related work can help readers who are not familiar with this field quickly gain background information about this work and its positioning. Furthermore, I recommend adding a section in the related work that focuses on human-AI collaboration, as it is highly relevant to the topic of this paper.\n3. Presentation: (a) I suggest that the authors highlight the best results in Table 1 and clarify in the caption whether each metric is better when higher (success rate) or lower (average number of moves). (b) Watch-and-help is cited twice in references. (c) Citation format like WAH Puig et al. (2020) should be WAH (Puig et al., 2020). Use `\\citep` command in latex.\n\n[1] Building Cooperative Embodied Agents Modularly with Large Language Models"
            },
            "questions": {
                "value": "Although the author has demonstrated in the experiments that the method in the paper is indeed effective, one point still puzzles me: I understand how theory of mind can assist in improving the effectiveness of human-AI collaboration, but why does logical reasoning help enhance human-AI collaboration? Is it merely because logical reasoning is effective in all similar tasks (i.e. reasoning) rather than just theory of mind or human-AI collaboration? If it is effective in both, why not use this framework for accomplishing more reasoning tasks? I hope the author can provide some analysis or experimental data to elucidate the relationship between logical reasoning and human-AI collaboration."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1051/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1051/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1051/Reviewer_wN2Q"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1051/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698828515914,
        "cdate": 1698828515914,
        "tmdate": 1699636031350,
        "mdate": 1699636031350,
        "license": "CC BY 4.0",
        "version": 2
    }
]