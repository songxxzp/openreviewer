[
    {
        "id": "0EPFSux53K",
        "forum": "tFpqGk5hR5",
        "replyto": "tFpqGk5hR5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2552/Reviewer_bfuq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2552/Reviewer_bfuq"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a simple baseline for locomotion tasks usually used to evaluate RL algorithms. This simple baseline employs oscillators to generate periodic joint motions, providing a open-loop method with ease of reproducibility, a fracion of the parameters of neural network-based approaches, and little use of computational resources."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- One of the paper's notable strengths is the simplicity of the proposed approach. The use of open-loop oscillators to solve locomotion tasks offers an elegant and straightforward solution. This simplicity is in contrast to the increasing complexity of many contemporary deep reinforcement learning (DRL) methods.\n\n- This approach is easily reproducible. It provides a minimal standalone code for solving the swimmer task and includes comprehensive details on the optimization of oscillator parameters.\n\n- I really appreciate the thorough analysis of the proposed approach with a few different RL methods. This analysis provides valuable insights into the performance, efficiency, and robustness of this simple baseline in relation to existing more complex methodologies."
            },
            "weaknesses": {
                "value": "- Although the method requires minimal parameters, a lot of trial-and-error is needed in determining the number of oscillators to use for each environment, which is in direct contrast to RL methods which are general across multiple environments. The need for fine-tuning in each context may limit the method's scalability.\n\n- While this result is really nice, this is also very expected. When maximizing the default reward of these environments, the resulting policy always corresponds to cyclical behavior which suggests that a simple cyclical controller could solve the task. Furthermore, these environments are very simple, and this work further intesifies the importance of moving on from this simple environments and focus on harder tasks.\n\n- While the simplicity and practicality of the approach are strengths, the contribution is minimal. I see this work as better suited for the blog post track rather than a conference paper."
            },
            "questions": {
                "value": "No questions. I address my opinions in the fields above. I think this work is somewhat valuable to the community, but I also think that it could easily be more appreciated as a simple blog post as the contribution itself is minimal."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2552/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698383501154,
        "cdate": 1698383501154,
        "tmdate": 1699636191920,
        "mdate": 1699636191920,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0TKBHvNAyl",
        "forum": "tFpqGk5hR5",
        "replyto": "tFpqGk5hR5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2552/Reviewer_w6U7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2552/Reviewer_w6U7"
        ],
        "content": {
            "summary": {
                "value": "The presented paper introduces a Central Pattern Generators (CPG) controller as a rudimentary open-loop baseline for reinforcement learning (RL) in locomotion tasks. In addition to detailing the method, the authors have exhaustively evaluated their proposed baseline on Mujoco-v4 environments and deployed it in a simple real-world setting."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The paper offers a comprehensible open-loop RL baseline, with the technical aspects meticulously described and supplementary implementation details provided in the appendix.\n2. The research does not limit itself to simulation; it extends its evaluation of the open-loop controller to a genuine real-world environment.\n3. The authors emphasize the significance of leveraging prior knowledge in task formulation, suggesting its equal importance to algorithmic enhancement.\n4. Common pitfalls of existing RL techniques are explored, accompanied by results that demonstrate scenarios in which contemporary RL methods are outperformed by the simple open-loop baseline."
            },
            "weaknesses": {
                "value": "1. In terms of novelty of the work, central pattern generators have been applied to legged locomotion in different ways in much more realistic and complex setting, for example it\u2019s been used to build a better action space for RL as described in Bellegarda, et al [1].  In this case, it\u2019s hard to see the value of applying CPG to many simple locomotion tasks (Mujoco-v4 in gymnasium). Though Gym contains simple locomotion tasks, no matter whether it\u2019s hard to solve or not, it\u2019s mainly designed for evaluating different RL methods providing a specific manually designed controller for these tasks can hardly address issues in current RL methods. \n2. Though this work is compared with several RL methods trying to study the robustness of current RL methods, no recent works are compared. The most recent work compared in this work is SAC.\n3. Though the proposed simple open-loop baseline controller is robust and provide good sample efficiency, this open-loop baseline can only be applied to locomotion tasks, where current RL method can solve much more complex locomotion tasks (like traverse complex terrain) within one hour [2, 3]. \n\nReference: \n[1] Bellegarda, et al. CPG-RL: Learning central pattern generators for quadruped loco- motion.\n\n[2] Rudin, et al. Learning to Walk in Minutes Using Massively Parallel Deep Reinforcement Learning\n\n[3] Smith, et al. A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning"
            },
            "questions": {
                "value": "1. It would be great if authors could further address the contribution of the proposed work.\n2. It would be great if this work can provide more comparison with recent RL works if this work is trying to address the issue of existing RL methods, like AWR [1], V-MPO[2] (just example), or some model-based RL methods[3].\n3. When it comes to sim2real, many rewards like minimizing the energy consumption, penalize the high frequency action has been well-studied, as well as the domain randomization techniques, when studying sim2real performance, these techniques should be applied to see the actual performance different. It would be great to apply these techniques if this work is trying to claim the performance of proposed method in sim2real transfer. \n\nReference:\n[1] Peng, et al. Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning\n\n[2] Song, et al. V-MPO: On-Policy Maximum a Posteriori Policy Optimization for Discrete and Continuous Control\n\n[3] Hansen, et al. Temporal Difference Learning for Model Predictive Control"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2552/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698557527699,
        "cdate": 1698557527699,
        "tmdate": 1699636191850,
        "mdate": 1699636191850,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2gP2MWF9YC",
        "forum": "tFpqGk5hR5",
        "replyto": "tFpqGk5hR5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2552/Reviewer_vPWP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2552/Reviewer_vPWP"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an open-loop control algorithm to solve five different locomotion control tasks. The experiments and video show that a simple control algorithm can achieve better results than RL policy."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The paper contains the real robot experiments. It shows the real effectiveness of the proposed algorithm.\n- The paper considers the advantages of the method from the perspective of runtime."
            },
            "weaknesses": {
                "value": "- The method is not innovative. In fact, the baseline of many previous RL work was a simple open-loop controller. However, whether those works only used RL [1] or considered the combination of RL and classical control algorithms [2][3], they all produced much more impressive locomotion control results.\n- I think the training results of RL methods in this paper are much worse than the level of other RL for locomotion papers, and they have not considered clearly what problems RL for control policy solves (like robustness in unseen environments).\n\n- The paper has limited inspiration for the field of machine learning, which is the main topic of this conference.\n\n[1] Ashish Kumar, Zipeng Fu, Deepak Pathak, Jitendra Malik, \"RMA: Rapid Motor Adaptation for Legged Robots\"\n\n[2] Atil Iscen, Ken Caluwaerts, Jie Tan, Tingnan Zhang, Erwin Coumans, Vikas Sindhwani, Vincent Vanhoucke, \"Policies Modulating Trajectory Generators\"\n\n[3] Takahiro Miki, Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, Marco Hutter, \"Learning robust perceptive locomotion for quadrupedal robots in the wild\""
            },
            "questions": {
                "value": "- In experiments, why the authers choose speed as the metric of the locomotion task? Can you consider more realistic metrics like stability under the noises or some uneven terrains, as well as show more videos beyond walking in a straight line?\n- The method works because the tasks only depend on cyclic movements for joints. Can the proposed method still be used if there are more obstacles in the environment, which need some complex behavior (turn left/right)?\n- Is it possible to try more to adjust the parameters of the pd controller and the range of the joint target position generated by the RL policy to improve the results of the RL method? The demo shown by the authors is indeed worse than other RL-based locomotion works recently."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2552/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698575456974,
        "cdate": 1698575456974,
        "tmdate": 1699636191782,
        "mdate": 1699636191782,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CMrDFfQn02",
        "forum": "tFpqGk5hR5",
        "replyto": "tFpqGk5hR5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2552/Reviewer_nvZX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2552/Reviewer_nvZX"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an open-loop controller baseline that has decent performance on a number of control tasks. The baseline is shown to be robust to noise and can be transferred to a real robot."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The primary strength of the approach is in its simplicity. The authors are careful to argue that the proposed method achieves \u2018satisfactory\u2019 performance on a number of tasks without requiring complex models. As the authors point out, there are natural advantages to using simpler pattern generators for robotics which avoid issues of bang-bang control and wear-and-tear that might arise from learnt methods. I also appreciate that the authors are careful with their claims and acknowledge that SAC outperforms their proposed approach in simulation without noise.\n\nThe paper is also written in clear and simple language and is easy to follow."
            },
            "weaknesses": {
                "value": "I generally support the argument for simplicity that is presented in the paper. The baseline requires far fewer parameters and less computation to train. However, I think the paper is missing a discussion on how RL might still play a role when applied to robotics. \n\nThere are solutions to the issues presented in Section 4.3 for Robustness to sensor noise that RL practitioners would likely implement. For example, noise can be added in simulation during the RL training which would result in a conservative but more robust policy. It would be interesting to see how that compares to an open loop baseline. As the paper argues, domain specific knowledge could help improve algorithm design albeit for the RL algorithms in this case. More generally, the results seem to indicate that if final performance is the key driver being optimised, RL may still be the tool of choice.\n\nFinally, while the paper does a good job of implementing baselines from RL and evolutionary algorithms, there are no pattern generating baselines being compared to. Central pattern generators have been studied for some time so it would be important to know how the proposed open-loop baseline compares against existing ideas in the field."
            },
            "questions": {
                "value": "1. The `Contributions` list that the baseline can handle sparse rewards. As far as I know none of the environments used for the evaluation use sparse rewards though. Could the authors clarify this point?\n2. Can the authors include another non-learning baseline to understand how much is gained with the specific implementation proposed?\n3. For the robustness to sensor noise, can the authors include a baseline where SAC is trained with a subset of the noisy parameters? For instance, if SAC were trained to withstand random noise of say 3N, it seems more likely that it would be able to withstand the 5N noise tested in the paper.\n4. For Figure 3, shouldn\u2019t the dot for `Open Loop` sit exactly at 1? It seems that the plot is slightly below 1 which seems off to me.\n5. While I like the general evaluation in the Experiments section, I found Table 6 in the Appendix with the actual scores informative. Since there is still space, I would suggest adding that table to the main text - if the authors think it would not affect the narrative flow too much."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2552/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2552/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2552/Reviewer_nvZX"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2552/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698677151409,
        "cdate": 1698677151409,
        "tmdate": 1700559307523,
        "mdate": 1700559307523,
        "license": "CC BY 4.0",
        "version": 2
    }
]