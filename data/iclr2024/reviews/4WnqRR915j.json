[
    {
        "id": "1VP9171N7g",
        "forum": "4WnqRR915j",
        "replyto": "4WnqRR915j",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8243/Reviewer_Ch2c"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8243/Reviewer_Ch2c"
        ],
        "content": {
            "summary": {
                "value": "The paper proposed continual training with math/code data and achieved very competitive results on the related tasks. The evaluation is comprehensive and ablation studies are sound."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper showcases that if we continue pretraining an LLM on a specific domain, we are able to get good performance. The authors established a good way to do domain adaptation.\n2. Data and training code are open-sourced, expect to have high reproducibility \n3. Ablation study is comprehensive.\n4. Writing is well-organized"
            },
            "weaknesses": {
                "value": "- The authors are comparing their results with `Minerva`, however since the datasets and its mixture, model architecture, training methods are different, we don't know which part contributes to the good performance. \n- We don't know if there training from scratch or starting from other LLM (like llama2 base) could be as impactful as well"
            },
            "questions": {
                "value": "- One thing would be interesting to know is that if pretrained from scratch, will it be better or not? \n- Do we still need fine-tuning in this case"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8243/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8243/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8243/Reviewer_Ch2c"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8243/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697666126079,
        "cdate": 1697666126079,
        "tmdate": 1699637024819,
        "mdate": 1699637024819,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "teLi1vqfuh",
        "forum": "4WnqRR915j",
        "replyto": "4WnqRR915j",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8243/Reviewer_JQiP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8243/Reviewer_JQiP"
        ],
        "content": {
            "summary": {
                "value": "In this submission, the authors released a new large-scale dataset for training mathematically-specified\nlarge language models. Additionally, based on the dataset, a new base model called LLemma is trained and released, which outperforms existing models like Code LLaMA and Minerva. Moreover, the dataset and the baseline are openly released, which helps to boost the research in AI4Math."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is well-written and easy to follow. The details of data collection and training instructions are provided. Moreover, both data and model are released. \n\n2. The experimental part is solid. I especially like the ablation study of data mixture components given in Table 4, which helps to reveal the contribution of different data sources."
            },
            "weaknesses": {
                "value": "1. Technically, this submission is not so interesting, and I cannot learn anything new in the aspect of methodology. The claims and experimental results are natural \u2014 fine-tuning Code Llama on a larger mathematically specific dataset helps to improve its performance. Note that this is a very personal opinion:) It is OK if this work targets the topic of datasets and benchmarks.\n2. I hope that the authors can discuss more about their future plans in the section of Conclusion. In the field of AI4Math, the performance of the current LLM is still limited. What will the authors do in the next step? Will they continuously enlarge the dataset and/or model? Is this the final solution to AI4Math?\n3. The comparison between Code Llama and the proposed Llemma is not fair. It demonstrates the usefulness of the Proof-Pile-2 dataset. It would be nice if the authors could finetune Llama2 directly based on the Proof-Pile-2 dataset and show the model performance."
            },
            "questions": {
                "value": "Why did the authors train the 7B model for 200B tokens and the 34B (the larger) model for 50B (the fewer) tokens?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8243/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698768414880,
        "cdate": 1698768414880,
        "tmdate": 1699637024708,
        "mdate": 1699637024708,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aAZ6crTFct",
        "forum": "4WnqRR915j",
        "replyto": "4WnqRR915j",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8243/Reviewer_HYfd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8243/Reviewer_HYfd"
        ],
        "content": {
            "summary": {
                "value": "This paper present a new model, namely LLEMMA, an open language model for mathematics. The model is used to solve math-related problems such as proving or calculators. The major contribution of the paper is opensourcing the model, as well as the training data and codes. The evaluation results show that the performance is encouraging."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The model and code are opensourcing.\n2. It has proposed the Proof-Pile-2 data and will be released as well.\n3. It has shown a training method that works well by fine-tuning a llama2 based pretrained model."
            },
            "weaknesses": {
                "value": "1. It doesn't have much ablation study or analysis about the data configuration. For example, why we should use which part of data.\n2. It doesn't have much novelties, other than a model that experimentally looks ok."
            },
            "questions": {
                "value": "1. Have you experimented with different combination of training data?\n2. How did you perform instruction fine-tuning?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8243/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8243/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8243/Reviewer_HYfd"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8243/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699346697986,
        "cdate": 1699346697986,
        "tmdate": 1699637024603,
        "mdate": 1699637024603,
        "license": "CC BY 4.0",
        "version": 2
    }
]