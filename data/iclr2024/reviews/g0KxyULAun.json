[
    {
        "id": "A2V1Cu6kQr",
        "forum": "g0KxyULAun",
        "replyto": "g0KxyULAun",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1973/Reviewer_WKXM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1973/Reviewer_WKXM"
        ],
        "content": {
            "summary": {
                "value": "This paper provides a new supervised contrastive-learning method via key-point masking to explore richer features from inactivated key-points. The results demonstrate its better performance compared to SoTA skeleton-based action recognition approaches, especially in the robustness."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "### Main strength points:\n- Better performance, especially in the robustness, which makes the skeleton-based action recognition algorithms more deployable.\n- Comprehensive ablations and experiments for the masking method.\n- Good writing.\n- Codes are available."
            },
            "weaknesses": {
                "value": "### Main weak point:\n- The paper combine several **existing** methods to achieve better performance. Specifically, an additional path is added to improve the performance and extract more features. However, this duplicates the computational cost. How much performance gain can we get if we **simply use the upper path and make the ST-MSHA block larger**? Fairer comparison should be added under the scenario of **similar computational cost**.\n\nI will be very willing to raise the score if the authors can address my concerns."
            },
            "questions": {
                "value": "Similar methods can also be applied to non-skeleton based methods. We know the skeleton-based methods are more robust than non-skeleton-based methods. But will that conclusion still hold when we add masking as mentioned in the paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1973/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1973/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1973/Reviewer_WKXM"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1973/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697493566057,
        "cdate": 1697493566057,
        "tmdate": 1699636128928,
        "mdate": 1699636128928,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dCicmL2xNd",
        "forum": "g0KxyULAun",
        "replyto": "g0KxyULAun",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1973/Reviewer_vH9p"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1973/Reviewer_vH9p"
        ],
        "content": {
            "summary": {
                "value": "This paper tackles the task of skeleton action recognition. Through an analysis of the attention weights of MotionBERT, the authors highlight the model's concentration on a limited set of discriminative joints for action recognition. As a response to this observation, they introduce a novel approach, which involves masking out the most highly activated joint to encourage the model to explore a broader array of informative joints. The proposed method leverages two distinct contrastive loss functions at both sample and class levels. To assess its efficacy, the approach is evaluated on three widely recognized benchmark datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well organized and easy to follow.\n2. The paper presents a commendable analysis and visualization of the attention weights of MotionBERT, offering a deeper insight into the model's inner workings.\n3. Comprehensive experiments are conducted on three benchmark datasets, also with the results on the robustness against skeleton perturbations."
            },
            "weaknesses": {
                "value": "1. In regard to motivation, the inspiration for this paper stemmed from an in-depth examination of the attention weights employed by MotionBERT. This analysis revealed that MotionBERT focuses on a restricted set of joints for recognition. However, this gave rise to two questions:\n\n(a) The paper primarily centers its analysis and experiments on MotionBERT, leading to doubts about whether the identified issues are prevalent in various backbone architectures rooted in self-attention mechanisms.\n\n(b) It is noteworthy that the proposed method can exclusively be applied to models built upon self-attention. This constraint, in turn, restricts the method's applicability and generalizability to non-transformer backbones.\n\n2. In terms of novelty and contributions, the primary innovation presented in this paper involves the process of masking out the most highly activated joint and employing a contrastive loss function to reduce the dissimilarity between the original joint embedding and the masked counterpart. Although the idea is conceptually straightforward, in my view, its novelty might be somewhat limited for publication at ICLR.\n\n3. About literature review: it's worth noting that certain recent works on skeleton action recognition have been omitted, such as [1-5].\n\n4. About experimental evaluation: it's notable that the authors only present the results for the joint modality. However, it is a standard practice in the field to present results for the multi-stream fusion approach, which encompasses joint, bone, joint motion, and bone motion modalities, as demonstrated in previous studies [1-5]. Furthermore, it's essential to acknowledge that the performance of the proposed method falls short when compared to PoseConv3D on NTU120-XSet and Kinetics400 datasets. On NTU60-XView, the improvement is only marginal.\n\n[1] Huang, et al. \"Graph contrastive learning for skeleton-based action recognition.\" ICLR 2023.\n\n[2] Lee, et al. \"Hierarchically decomposed graph convolutional networks for skeleton-based action recognition.\" ICCV 2023.\n\n[3] Lee, et al. \"Leveraging spatio-temporal dependency for skeleton-based action recognition.\" ICCV 2023.\n\n[4] Wang, et al. \"Neural Koopman Pooling: Control-Inspired Temporal Dynamics Encoding for Skeleton-Based Action Recognition.\" CVPR 2023.\n\n[5] Foo, et al. \"Unified pose sequence modeling.\" CVPR 2023."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1973/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698297571930,
        "cdate": 1698297571930,
        "tmdate": 1699636128863,
        "mdate": 1699636128863,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zleaNAhTrR",
        "forum": "g0KxyULAun",
        "replyto": "g0KxyULAun",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1973/Reviewer_8Cc6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1973/Reviewer_8Cc6"
        ],
        "content": {
            "summary": {
                "value": "The manuscript observes that current transformer-based skeletal action recognition methods tend to focus on a limited set of joints and low-level motion patterns, potentially resulting in performance degradation because of the action perturbation or ambiguousness shown in Fig.1. Therefore, the authors propose a Targeted Masking strategy to occlude the joints with highest activations, and a Multi-Level Contrastive Learning framework to push the original and altered feature embeddings (positive pair) together. Extensive experiments are conducted on NTU60, NTU120, and Kinetics datasets to verify the effectiveness, generalization, and robustness."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "a. The motivation is well justified. \nb. Experiments are sufficient to demonstrate the effectiveness and robustness. \nc. The proposed MaskCLR reaches a new state-of-the-art on the benchmarks in use."
            },
            "weaknesses": {
                "value": "a.There is potential room for improvement in the writing skills. For example, in the first sentence of the Abstract, it may be more suitable to use \"tend to focus on\" instead of \"focus on\" when discussing existing methods. Given that the critique of previous methods largely stems from qualitative investigations presented in Fig. 1, it would be advantageous to acknowledge the potential influence of individual bias on these findings. Addressing this issue has the potential to enhance the overall quality of the paper.\nb. Activation-guided augmentation is a concept relatively familiar within the vision community. However, it would be valuable to provide a more direct and explicit explanation of this point. Moreover, it may be better to probe some related works in this field in Sec.2, such as [1-2].\n\n[1] He J, Li P, Geng Y, et al. FastInst: A Simple Query-Based Model for Real-Time Instance Segmentation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 23663-23672.\n[2] Choe J, Shim H. Attention-based dropout layer for weakly supervised object localization[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019: 2219-2228."
            },
            "questions": {
                "value": "Consider replacing the term 'multi-level' in the title with 'activation-guided' or similar terminology to better align with the core theme of this manuscript.\nAdditionally, it's worth noting that most related works in the field of contrastive action recognition are self-supervised and primarily evaluated using unsupervised metrics. However, it's important to point out that despite the reviewer's familiarity with supervised contrastive learning, there was still some confusion experienced until the end of Section 2. This suggests that there may be a need for the authors to dedicate more time to refining and clarifying their paper.\nIf the reviewer's understanding is generally accurate, it underscores the importance of improving the paper's clarity and readability."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1973/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1973/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1973/Reviewer_8Cc6"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1973/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698324760888,
        "cdate": 1698324760888,
        "tmdate": 1699636128801,
        "mdate": 1699636128801,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AfC7YAcoYf",
        "forum": "g0KxyULAun",
        "replyto": "g0KxyULAun",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1973/Reviewer_VBCi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1973/Reviewer_VBCi"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a masked contrastive learning approach for skeleton-based action recognition.\n\nThe approach employs a targeted masking strategy to obscure significant joints, thereby encouraging the model to learn a broader set of discriminative joints.\n\nAdditionally, a multi-level contrastive learning framework is proposed to enhance the feature embeddings of skeletons. This serves the dual purpose of ensuring compactness within each class and increasing dispersion among different classes.\n\nExtensive experiments are conducted on various benchmarks to demonstrate the improvements over existing methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ Overall the paper is technical sound.\n\n+ Some nice visualizations presented in the paper.\n\n+ Extensive experimental results and comparisons, ablation studies are well presented."
            },
            "weaknesses": {
                "value": "Major:\n\n- The novelty of this work is limited.\n\n(i) Transformer-based models are not thoroughly discussed and analyzed (which are closely related to the argument 'seamlessly incorporated into transformer-based models' as mentioned in abstract section. For example, [A].\n\n[A] Focal and Global Spatial-Temporal Transformer for Skeleton-based Action Recognition, ACCV'22\n\n(ii) Why only choose MotionBERT? The reviewer noticed that recent works, e.g., [B] and [C], are not discussed in the paper. There are only 3 works referenced from 2023, why?\n\n[B] 3Mformer: Multi-Order Multi-Mode Transformer for Skeletal Action Recognition, CVPR'23\n\n[C] SkeletonMAE: Graph-based Masked Autoencoder for Skeleton Sequence Pre-training, ICCV'23\n\n(iii) The reviewer noticed that [B] also uses a very similar two pathways setup, the pipeline looks quite similar? (why it is not discussed and it seems that they also use the same dataset for evaluation, and the differences are (1) they use skeletal hypergraph whereas this work uses skeletal graph (2) they use standard classification loss whereas this work applies contrastive learning, so the novelty is the introduced targeted masking strategy?) Also it is a transformer-based model that uses hypergraph? The reviewer noticed that [B] also considers unactivated but informative joints?\n\n(iv) Furthermore, [B] is based on a transformer architecture that utilizes hypergraphs. The question arises as to whether the proposed strategy can be seamlessly integrated into their transformer-based model. \n\n- The justifications of design choice/principle and rationale are not properly illustrated. Why the model are arranged/designed in this way, and its links/relationships to related works are not provided and discussed.\n\n- The maths presented in the paper is confusing and some terms are not explained clearly and properly.\n\n(i) It is suggested to have a notation section for the maths symbols. Why the $_b$ in Eq (1) and (2) sometimes in bold face, it is quite confusing.\n\n(ii) What is the relationship among $\\textbf{A}$, $\\textbf{A}\\_b$, $\\textbf{A}\\_{\\mathcal{S}}$ and $\\textbf{A}\\_{\\mathcal{T}}$. Why $\\textbf{A} = 1/2 * (\\textbf{A}\\_{\\mathcal{S}} + \\textbf{A}\\_{\\mathcal{T}})$, and '*' denotes multiplication? The design choice/principle and rationale are not properly illustrated.\n\n(iii) In Eq. (4) $\\bigotimes$ is not explained. The reviewer noticed that in Fig. 2, it is mentioned but it would be much clearer to have that in main texts.\n\n- Table 2, the comparisons w.r.t. transformer-based (only 1 method is compared) are very limited. Table 4, the red highlighted results are not discussed and analyzed.\n\n- The authors show Fig. 3 and 4, but the discussions and analysis w.r.t. different perspectives / new insights are very limited. It is suggested to add more discussions and analysis.\n\n- The experiments w.r.t. whether the proposed strategy can be applied to transformer-based models are very limited (only 1 model is experimented). It is suggested to explore more transformer-based models.\n\n\nMinor:\n\n- Figure 1 is not very clear to reviewer. What dataset does these videos come from? The color is also not very visible and clearer enough to reviewer. The explanations are very limited.\n\n- Figure 2 some fonts are too smaller to read."
            },
            "questions": {
                "value": "Please refer to weakness section.\n\nMore questions:\n\n- Page 2, 'multi-level contrastive learning' part, what does this 'class-averaged features' mean?\n\n- In contribution section of Introduction, how 'unactivated' and 'informative' joints are determined and to what extent?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1973/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698592637263,
        "cdate": 1698592637263,
        "tmdate": 1699636128716,
        "mdate": 1699636128716,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AURppmtjwI",
        "forum": "g0KxyULAun",
        "replyto": "g0KxyULAun",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1973/Reviewer_jVKN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1973/Reviewer_jVKN"
        ],
        "content": {
            "summary": {
                "value": "This manuscript introduces MaskCLR, a training paradigm tailored for skeleton-based action recognition. This approach applies targeted masking to intentionally occlude predominantly activated joints in skeletal sequences. The authors claim that such a strategy facilitates a more comprehensive extraction of pertinent information from input skeleton joints. Complementing this, a multi-tiered contrastive learning architecture is articulated, contrasting skeleton representations at individual sample and class stratifications, culminating in a class-dissociated feature space. The implications of such a design are posited to be enhancements in model accuracy, resilience to noise perturbations, and adaptability across disparate pose estimators. Empirical results suggest that the MaskCLR paradigm exhibits superior performance metrics relative to extant methodologies on specified benchmarks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The introduction of MaskCLR offers a distinct approach in skeleton-based action recognition. This work differentiates itself by emphasizing targeted masking, as opposed to the random masking strategies referenced in prior research by Zhu et al., 2023 and Lin et al., 2023. Ablation studies on the NTU60-XSub dataset support the claims, with an in-depth exploration of hyperparameters and a comprehensive evaluation of both Lsc and Lcc losses under varying masking strategies. The approach adopted by MaskCLR, which encodes detailed representations from input skeleton joints, demonstrates robustness against perturbations, contributing to advancements in skeleton-based action recognition."
            },
            "weaknesses": {
                "value": "The paper's comparative analysis with existing random masking methodologies seems limited in depth, particularly when referencing works like Zhu et al., 2023 and Lin et al., 2023; A more detailed juxtaposition detailing operational and performance nuances would be insightful. Another constraining factor is the exclusive reliance on attention weights from the final ST blocks, with Chefer et al., 2021 suggesting potential benefits from multi-layer attention scores. Furthermore, while the paper claims robustness and generalization, expanded empirical validation across diverse datasets might solidify these assertions. The current exploration of hyperparameters, especially the effects of Lsc weight \u03b1 and Lcc weight \u03b2, could be augmented by examining their combined effects. A deeper theoretical exposition on choices made, especially regarding contrastive losses, could enhance comprehension. Lastly, a rigorous analysis detailing the model's performance under varying noise conditions would strengthen claims about its robustness against noise."
            },
            "questions": {
                "value": "1.The paper mentions a \"class-dissociated feature space\" as a result of the multi-level contrastive learning framework. Could the authors delve deeper into the tangible benefits of this feature space in comparison to more traditional feature spaces, especially in terms of model interpretability and robustness?\n2.Given the paper's focus on robustness to perturbations, could the authors provide insights into specific types of perturbations where the model excels and where it might face challenges?\n3.The robustness of MaskCLR, especially against varying degrees of noise, is paramount for real-world applicability. How does the model's performance degrade or vary with incremental noise levels in the input data?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "I have no Ethics Concerns"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1973/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1973/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1973/Reviewer_jVKN"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1973/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698737987026,
        "cdate": 1698737987026,
        "tmdate": 1699636128647,
        "mdate": 1699636128647,
        "license": "CC BY 4.0",
        "version": 2
    }
]