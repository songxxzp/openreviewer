[
    {
        "id": "lwVk21rdtI",
        "forum": "RR70yWYenC",
        "replyto": "RR70yWYenC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7163/Reviewer_FDce"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7163/Reviewer_FDce"
        ],
        "content": {
            "summary": {
                "value": "This paper considers the instance-optimal finite-sum minimization problem, where the setting is we have a sequence of functions $f_1, \\cdots, f_n$, and we need to output a sequence of points $x_1^*, \\cdots, x_n^*$, with $x_i^*$ being the minimizer of $\\sum_{j=1}^i f_j(x)/i$. This problem is inline with the modern machine learning setting as usually the training set is went through for only once, but we hope to make sure the current solution is the optimal for the current situation. \n\nFor this setting, the existing SGD or variance reduced methods are not optimal, because naively we need to call these methods $n$ times in order to find these $n$ minimizers. This paper proposed a new algorithm which finds the minimizer iteratively, which incurs $\\tilde{O}(n/\\epsilon^{1/3} + \\log n /\\sqrt{\\epsilon})$ FO complexity. The authors also show that the lower bound of this problem is $\\tilde{O}(n/\\epsilon^{1/4})$, which means the newly proposed algorithm is close to optimal."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Originality: the setting is interesting, and the proposed algorithm is novel as far as I know. In their algorithm, they propose to switch between compute full gradient with $i$ FOs and compute a simple modification based on the previous gradient with 1 FO for full gradient estimation, which is a special trick. \n\nQuality: The results are nice, because the authors not only provide the convergence guarantees, but also compare it with the existing methods, and provide a very close lower bound. \n\nClarity: The overall writing of this paper is excellent. \n\nSignificance: I think the results are interesting, because this setting is an important setting to explore, and they have presented a full story about this setting."
            },
            "weaknesses": {
                "value": "I guess the main weakness of this paper is about the strong convexity assumption of f. This is kind of old-school assumption, because nowadays we always use deep neural networks to fit the data, and these models are almost never strongly convex.  In fact, as far as I know, variance reduced techniques are not so useful for neural networks, so I am afraid that the proposed method are not very practical for real world models."
            },
            "questions": {
                "value": "I do not have additional questions. I have went through the proofs in the main paper, and they look correct to me. However, I did not get time to verify the long proofs in the appendix."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7163/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698491443599,
        "cdate": 1698491443599,
        "tmdate": 1699636849248,
        "mdate": 1699636849248,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6ZqAg4d0yk",
        "forum": "RR70yWYenC",
        "replyto": "RR70yWYenC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7163/Reviewer_zvPG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7163/Reviewer_zvPG"
        ],
        "content": {
            "summary": {
                "value": "This paper studies a new problem of efficient instance-optimal finite-sum minimization, where given a sequence of functions, the goal is to find a sequence of points $x_1^\\star, \\ldots, x_n^\\star $ such that for each $i \\in [n]$, point $x_i^\\star$ approximately minimizes $\\sum_{j=1}^{i} f_j(x) / i $, with the number of first-order oracle(FO) calls as least as possible. Given the condition that each function $f_j(\\cdot)$ is strongly convex, the proposed method SIOPT-Grad produces an $\\epsilon$-optimal sequence within $\\tilde{O}(n/ \\epsilon^{1/3} + 1/\\sqrt{\\epsilon})$ FOs. In addition, they prove a lower bound that, there is no \\emph{natural} FO methods that completes this task within $O(n/\\epsilon^{1/4}) queries.\n\nIn the regime motivated by the empirical risk minimization in the strongly convex case where statistical error is $\\epsilon = O(1/n)$,  SIOPT-Grad requires only $O(n^{4/3})$ FOs compared to all previous FO solutions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The studied problem is well-motivated, both from the literature review on incremental learning, and from empirical estimation.\n- The theoretical analysis is complete, an upper bound and lower bound is provided for this problem, as well as compared to state of the art as in table 1.\n- The logic of this paper is easy to follow, and the assumptions/notations are presented in a clear way.\n- The paper has additional experiments on the ridge regression problem."
            },
            "weaknesses": {
                "value": "- The upper bound provided by the algorithm is not tight compared to the lower bound. \n- The algorithm only work in the strongly convex case. \n\nMinor Issue:\n- there is no input in the algorithm 2.\n- the value of $\\alpha$ needs to be in line 2 of algorithm 1. \n- additional \"the\" in the second line of the first paragraph of section 3.1\n- the VR is never defined. suggestion: \"variance reduction(VR)\" and then use VR afterwards"
            },
            "questions": {
                "value": "- The solution proposed in this paper, although is better in the number of FOs, may result in much added overall running time. Is there any practical regime where the number of FOs is expensive, compared to the efficiency of the algorithm measured by the running time?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7163/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7163/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7163/Reviewer_zvPG"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7163/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698741822098,
        "cdate": 1698741822098,
        "tmdate": 1699636849127,
        "mdate": 1699636849127,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uiRMiCn04B",
        "forum": "RR70yWYenC",
        "replyto": "RR70yWYenC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7163/Reviewer_s2LS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7163/Reviewer_s2LS"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces the following natural problem: given a sequence of n convex functions. Find a sequence of inputs that minimizes the partial sum of the functions. Standard methods such as SGD require $O(n/\\epsilon)$ first order gradient information about the functions to get error up to additive $\\epsilon$. That paper introduces a novel algorithm which obtains the same accuracy using substantially fewer number of first order gradient information. In particular, the new algorithm only requires $O(n/\\epsilon^{1/3})$  gradient calls. For small $\\epsilon$, this is a big improvement. For instance, the authors note that $\\epsilon=1/n$ and is a very common setting, in which case their algorithm gives a polynomial improvement. The authors also introduce the notion of a 'natural algorithm' and prove a lower bound of $\\Omega(n/\\epsilon^{1/4})$ gradient calls under the assumption."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The introduction of the problem is a nice conceptual contribution. The new algorithm they proposed could also have other applications. In the notion of a \"natural algorithm\" is a nice contribution since it allows the analysis of algorithms and lower bounds."
            },
            "weaknesses": {
                "value": "A weakness is the lack of intuition about their algorithm. I mostly follow the math, however conceptually I do not know why exactly they can get an improvement in the epsilon power. It seems like the high level idea is only to compute a gradient update if we have not had an update for a long time. \n\nEstablishing that the gradient is unbiased seems fairly straightforward: it just uses the fact that the gradient at the next step is a linear combination of the new function and the previous gradient. Analyzing the variance seems like some technical work. However I did not understand the conceptual improvement. \n\nCan the authors comment more about the intuition of their algorithm?\n\nAnother weakness is the experimental setting. In the experiments, the authors analyze a version of ridge regression. They divide by $i$ for every step which seems a bit unnatural to me. It would make more sense if the $i$ term was not there. But in that case, the problem becomes very easy. By using standard rank one update formulas, the *exact* optimum can be found in $O(nd^2)$ time. Perhaps the authors divided by $i$ to make the problem `artificially' harder? In any case, testing the algorithm on other natural problems which cannot be solved by other methods would be more convincing. The authors also do not list the parameters of the dataset and the $\\epsilon$ used in the algorithm."
            },
            "questions": {
                "value": "Please see above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7163/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7163/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7163/Reviewer_s2LS"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7163/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698799750602,
        "cdate": 1698799750602,
        "tmdate": 1700698652953,
        "mdate": 1700698652953,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qtbBtNJgaf",
        "forum": "RR70yWYenC",
        "replyto": "RR70yWYenC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7163/Reviewer_jqUd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7163/Reviewer_jqUd"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the following problem: given $n$ functions, the goal is to find $n$ points such that the $i$-th point minimizes the average of the first $i$ functions. For this problem, the paper presents a lower bound on the number of iterations for a certain class of algorithms, called natural first-order methods. They present an algorithm that achieves first-order oracle complexity which is close to the lower bound."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "A clearly written paper with sound results."
            },
            "weaknesses": {
                "value": "I have the following concerns about the paper:\n\n-- I can\u2019t connect Problem 2 with its motivation. For example, you say that \u201cit is important that a model is constantly updated so as to perform equally well both on the past and the new data\u201d, but:\n1) Problem 1 achieves precisely that;\n2) in Problem 2, you train *multiple* models, with later models performing well on all data, and with older models not taking into account new data at all.\n\nTo conclude, I don\u2019t see a motivation for the problem.\n\n-- You give lower bounds only for natural algorithms. If you don\u2019t restrict yourself to natural algorithms (and it\u2019s not clear to me why you consider only natural algorithms), I believe that $O(\\frac{\\log n}{\\epsilon})$ bound is possible, which outperforms all suggested methods. The idea is to run the following simple algorithm:\n\nfind $\\epsilon$-minimizer $x_n$ of $g_n$ using SGD\nfor i = n-1, n-2, \u2026, 1:\n\tstarting from $x_{i+1}$, find $\\epsilon$-minimizer $x_i$ of $g_i$ using SGD\n\nI didn\u2019t check the math in detail, but I believe that the proof goes like this:\n1) If $x_{i+1}$ is $\\epsilon$-minimizer of $g_{i+1}$, then $x_{i+1}$ is $(1 + 1/i) \\epsilon$-minimizer of $g_i$.\n2) The number of iterations required to find $x_i$ starting from $x_{i+1}$ is $O(\\frac{\\log (1 + 1/i)}{\\epsilon})$\n3) Summing this over all $i$, we get $O(\\frac{\\log n}{\\epsilon})$ iterations\n\n-- Experiments are only conducted in simple regression settings. Dataset statistics (e.g. the number of points) are not shown. It is unclear how exactly you execute the baselines."
            },
            "questions": {
                "value": "Minor issues:\n\n-- Commas are missing in many places\n\n-- You mention $O(n \\log \\frac{1}{\\epsilon})$ complexity for VR methods, but isn\u2019t the same bound achievable by trivial deterministic gradient descent, by sampling all $n$ functions at every iteration?\n\n-- Page 2: \u201cfor Problem 1 using $O(1/\\epsilon)$.\u201d - \u201cFOs\u201d is missing."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7163/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699315148655,
        "cdate": 1699315148655,
        "tmdate": 1699636848847,
        "mdate": 1699636848847,
        "license": "CC BY 4.0",
        "version": 2
    }
]