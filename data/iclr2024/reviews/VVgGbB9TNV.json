[
    {
        "id": "KfYD4kGD5o",
        "forum": "VVgGbB9TNV",
        "replyto": "VVgGbB9TNV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission540/Reviewer_tZSe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission540/Reviewer_tZSe"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a new adversarial attack scheme against LLM using prompt engineering. The authors propose PromptAttack which includes three key components: original inputs, attack objective, and attack guidance. To enhance the attack efficacy, the authors also investigate the ensembling methods. Results show that PromptAttack can achieve high attack success rates (ASR) compared to AdvGLUE."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The authors show an effective adversarial attack against LLMs using prompt engineering. Particularly, PromptAttack designs fine-grained instructions to guide the victim LLM itself to generate adversarial samples that can fool itself. \n\n+ The authors investigate the efficacy of PromptAttack using the few-shot strategy and ensembling strategy. \n\n+ Empirical results show that PromptAttack achieves higher ASR on various benchmarks compared to AdvGLUE and AdvGLUE++."
            },
            "weaknesses": {
                "value": "The paper has the following weaknesses: \n- The novelty of the proposed attack scheme is not clear. Although PromptAttack is shown to be effective, the working mechanism is straightforward and simple. It's not clear what is the challenge of designing an effective adversarial attack against LLMs. \n- The contributions of the paper do not seem to be enough. The authors put together multiple existing techniques, including few-shot prompt engineering, ensembling, and adversarial attacks against the text. The contributions seem incremental and not substantial."
            },
            "questions": {
                "value": "Please consider addressing the weak points above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission540/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission540/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission540/Reviewer_tZSe"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission540/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698623048110,
        "cdate": 1698623048110,
        "tmdate": 1699635981141,
        "mdate": 1699635981141,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "v87i20m4vU",
        "forum": "VVgGbB9TNV",
        "replyto": "VVgGbB9TNV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission540/Reviewer_zRU1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission540/Reviewer_zRU1"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on adversarial attacks on large language models. To be more specific, this paper considers the black-box attacks and designs a structure of prompts to lead the model to generate adversarial prompts by themselves. Extensive experiments are conducted to illustrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The organization is good and the paper is easy to follow. The proposed method is simple and the effectiveness is promising via experiments."
            },
            "weaknesses": {
                "value": "1. Task description in section 4 is confusing. Please provide backgrounds in the appendix, showing what they are and why they matter.\n2. This work is partially motivated by the lack of efficiency and effectiveness of existing adversarial attacks, but there is no illustration of efficiency in the experiment part.\n3. The experiments are only conducted on 2 models, which is not enough, especially when Llama is an open-source model. I would recommend testing on more recent black-box models such as Bard, Claude, Palm."
            },
            "questions": {
                "value": "1. From Table 4, the ASR for each perturbation type is very low but the ASR in Table 3 is much higher (3-5 times higher). Why does this happen?\n2. Could you provide some understandings of why PromptAttack works? Also, are there any defenses against adversarial attacks in LLM? If there exists, please evaluate those defenses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission540/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698696657139,
        "cdate": 1698696657139,
        "tmdate": 1699635981077,
        "mdate": 1699635981077,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2cQQAI0yFW",
        "forum": "VVgGbB9TNV",
        "replyto": "VVgGbB9TNV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission540/Reviewer_2JDp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission540/Reviewer_2JDp"
        ],
        "content": {
            "summary": {
                "value": "The paper presents \"PromptAttack\", an adversarial attack method for evaluating the robustness of LLMs. It introduces a prompt-based adversarial attack that manipulates the victim LLM to generate adversarial samples by itself. The attack is composed of three elements: original input, attack objective, and attack guidance. A fidelity filter is employed to ensure that the adversarial samples maintain semantic meaning. The paper evaluates PromptAttack on Llama2 and GPT-3.5 that outperforms existing benchmarks like AdvGLUE and AdvGLUE++ in ASR. It raises important questions about the reliability and safety of deploying LLMs in critical applications."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is easy to follow.\n2. The study of adversarial attacks in LLM is important and interesting."
            },
            "weaknesses": {
                "value": "1. The paper methodology technical approach lacks novelty, which is essentially an application of known techniques, e.g. the design of the perturbation instruction, fidelity filter, few-shot inference and ensemble attack. Can the authors explain more what is the unique contribution?\n2. As the authors mentioned, the scale of LLMs may impact attack performance. If so, a more comprehensive evaluation of PromptAttack across a range of LLM scales, along with an analysis of computational overhead, would strengthen the paper.\n3. The paper would benefit from a discussion on potential countermeasures or mitigation strategies that could enhance the robustness of LLMs against such attacks like PromptAttack."
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission540/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission540/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission540/Reviewer_2JDp"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission540/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698827095500,
        "cdate": 1698827095500,
        "tmdate": 1699635980982,
        "mdate": 1699635980982,
        "license": "CC BY 4.0",
        "version": 2
    }
]