[
    {
        "id": "bdD9HM6kaM",
        "forum": "RqUMWdDg52",
        "replyto": "RqUMWdDg52",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6842/Reviewer_3cXG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6842/Reviewer_3cXG"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces FireAct, a language agent that involves fine-tuning. The proposed method introduces how to leverage a strong LM with few-shot prompting to generate trajectories for fine-tuning. Experimental results demonstrate that the proposed method can improve the performance of language agents and reduce the gap between open-source LLMs and ChatGPT / GPT-4."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper highlights the importance of fine-tuning to obtain better language agents. Massive experimental results also explore different aspects to improve agents, including scaling effects, robustness, generalization, efficiency and cost."
            },
            "weaknesses": {
                "value": "1. Generally, I think the main contribution of this paper can be considered as a data augmentation, which aims to utilize strong LLMs to generate trajectories for language models and mix multiple training tasks for fine-tuning. The idea of mixing multiple training tasks is not interesting, and previous works like T5 or instruction tuning also adopted such an experience for tuning.\n2. Although the authors have provided massive experiments to verify different fine-tuning factors in this paper, most of the conclusions are obvious and not inspired. For example like the experiments in sec 5.3, many conclusions are straightforward and it is evident that full-model training is better than LoRA. How to design experiments to prove the innovation of the proposed method (FireAct) is more important, rather than analyzing these factors.\n3. Most of the experiments are only employed on three datasets (HotpotQA, StrategyQA, and MMLU). To verify the effectiveness of agents, more real-world scenarios like interactive environments are required.\n\nOther suggestions:\n1. Too many fine-tuning details and settings are given in the appendix, not the main paper. And many useless experimental results cost too many pages in the whole paper. However, these experiments do not bring any insights. On the contrary, some fine-tuning details are more important for us to know the contribution of this paper."
            },
            "questions": {
                "value": "1. The papers conduct experiments to validate the robustness to noisy tools. However, previous works (e.g., HuggingGPT, Chameleon) have adopted retrieval-based methods to utilize tools based on ChatGPT or GPT-4 and achieve some performance. So how fine-tuning-based methods compare with retrieval-based methods?\n2. What is the content of Appendix A.2?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6842/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698038718638,
        "cdate": 1698038718638,
        "tmdate": 1699636792501,
        "mdate": 1699636792501,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "v7QyE9N3hr",
        "forum": "RqUMWdDg52",
        "replyto": "RqUMWdDg52",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6842/Reviewer_iJYa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6842/Reviewer_iJYa"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes FireAct in the ReAct framework to finetune LMs with agent trajectories generated by GPT-4. FireAct enhances the training process by combining ReAct with COT and Reflexion prompting techniques, resulting in the generation of more diverse and improved training samples. Notably, FireAct does not require a few-shot prompting example during inference. The study presents several intriguing findings and highlights unexplored research questions, including the intricate relationship between the base LM and fine-tuning trajectory data, assessing the robustness of language agents, optimizing their task-solving strategies, and evaluating the effectiveness of strategy selection. In the experimental evaluation, the authors demonstrate the effectiveness and efficiency of the proposed method on various QA tasks, such as HotpotQA, Bamboogle, StrategyQA, and MMLU."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The motivation of this paper to systematically investigate the effect of finetuning language agents is of importance.\n2. The authors have performed fine-tuning on various backbone models, including LLaMA and GPT-3.5, thereby enhancing the soundness of this study.\n3. This paper uncovers valuable insights, such as the increased robustness of fine-tuned agents compared to zero-shot ones, and the ability of LLMs fine-tuned on multi-method training data to implicitly select reasoning methods."
            },
            "weaknesses": {
                "value": "1. The FiReACT method is not novel today. Several works have proposed fine-tuning Language Model Models (LLMs) on ReACT, COT, or Reflection data. Examples include \"Large Language Models Are Reasoning Teachers\" (Ho et al., 2022) and toolLLaMA (Qin et al., 2023).\n\n2. The authors have not conducted enough experiments on the setting of full fine-tuning. I wonder whether the conclusions of this paper still hold when the method is applied to fully fine-tune an agent."
            },
            "questions": {
                "value": "1. What is the reason behind the observation that the fine-tuned agents exhibit more robustness compared to the zero-shot ones, and that the LLMs fine-tuned on the multi-method training data can implicitly select reasoning methods?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6842/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698714185980,
        "cdate": 1698714185980,
        "tmdate": 1699636792391,
        "mdate": 1699636792391,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WirmLdkegE",
        "forum": "RqUMWdDg52",
        "replyto": "RqUMWdDg52",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6842/Reviewer_6Re8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6842/Reviewer_6Re8"
        ],
        "content": {
            "summary": {
                "value": "This study delves into the relatively under-researched area of refining language models (LMs) to create linguistic agents. By employing a straightforward, regulated framework that utilizes a Google search API for question answering (QA), this paper conducts a thorough examination across a range of foundational LMs, agent strategies, data used for fine-tuning, and QA challenges. This investigative work uncovers new understanding regarding how the size of the base LM and the fine-tuning dataset influences outcomes, the integration of path data from diverse tasks and agent techniques, and the resilience of these systems to various forms of data disturbances."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Pros:\n1. This paper claims that they study a new direction: Agent fine-tuning.\n2. Comprehensive experiments show the effectiveness of the proposed Agent fine-tuning.\n3. Detailed analyses are given to illustrate Agent fine-tuning."
            },
            "weaknesses": {
                "value": "Cons:\n1. It is unclear the advantage of FiReAc compared to ReAct considering FiReAc is fine-tuned based on ReAct trajectories. The authors claimed that \u201cFiReAct agents benefit from the diversity of learning support, thus become more robust to external noises, more generalizable to novel tasks, and more flexible to choose various agent methods and task solving strategies adaptive to the problem at hand.\u201d However, intuitively, it is hard to understand the reasons why fine-tuning can make the agents more robust and generalizable. It is suggested that authors could provide more explanations.\n2. It is known that fine-tuning can hurt the generalization ability of LLMs. But the authors claimed that the proposed \u201cagent fine-tuning\u201d method is \u201cmore generalizable to novel tasks\u201d, which is hard to understand.\n3. The method seems to be very trivial. It seems this paper just renames the conventional \u201cfine-tuning\u201d as \u201cagent fine-tuning\u201d.\n4. The technical contribution is limited because It is unclear what is technical challenges of \u201cagent fine-tuning\u201d. It Is suggested the authors provide more explanations.\n5. It is suggested the authors summarize their contributions more clearly. It is very vague to say \u201cTo sum up our contributions, we advocate for the overlooked direction of language agent fine-tuning, and propose novel methodologies, systematic experiment designs, practical insights and guidelines, as well as new research questions for this direction.\""
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6842/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6842/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6842/Reviewer_6Re8"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6842/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698839884957,
        "cdate": 1698839884957,
        "tmdate": 1699636792278,
        "mdate": 1699636792278,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ajtMkjlkVa",
        "forum": "RqUMWdDg52",
        "replyto": "RqUMWdDg52",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6842/Reviewer_GhVh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6842/Reviewer_GhVh"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces \"FiReAct,\" a method for fine-tuning language models to function as language agents capable of reasoning and interacting with external environments. It challenges the conventional few-shot prompting approach, suggesting that fine-tuning with agent trajectories is more robust and efficient. The study uses a controlled environment with a Google search API for question answering to demonstrate the benefits of FiReAct. Key contributions include a novel fine-tuning methodology, a set of empirical guidelines for implementing such agents, and the release of code and model checkpoints for future research. The authors argue for fine-tuning over prompting when data and task understanding permit, setting new directions for language agent development."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The strengths of the paper, as identified through the experimental results, include the following:\n\n1. FiReAct agents eliminate the need for few-shot prompting examples during inference, which results in cost savings, faster operation, and increased convenience compared to prompting-based agents.\n2. These agents benefit from a diverse learning support, which enhances their robustness against external noise and improves their generalizability to novel tasks.\n3. FiReAct provides flexibility for agents to choose from various methods and strategies for solving tasks, allowing them to adapt to the problem at hand more effectively.\n\nMoreover, the paper opens up new avenues for research by highlighting previously unexplored questions related to the interactions between the base language model (LM) and the fine-tuning data, evaluating the robustness and strategic decision-making of language agents, and the systematic analysis of fine-tuning data design for language agents."
            },
            "weaknesses": {
                "value": "Task and Tool Limitation: The research is limited to question-answering (QA) tasks and relies on a single tool (Google search) for evaluation. This limitation questions the generalizability of FiReAct to other types of tasks and tools, which is a critical aspect for language agents intended for broader applications. To make the conclusions more convincing, it would be great if you could evaluate on other types of interactive tasks such as ScienceWorld, Mind2Web, InterCode, etc.\n\nMethodology Limitations: The paper focuses on three methods that maintain a single autoregressive trajectory context, which might not be representative of more complex agent behaviors. The ability to handle multiple prompts, roles, and contexts is a significant aspect of agent design that is not addressed in the current research.\n\nMulti-Task Constraints: The multi-task setup is confined to only three QA tasks, and the most advanced language model (LM) fine-tuned is GPT-3.5. The limited scope in both the variety of tasks and the LM capabilities may restrict the insights into the potential of FiReAct for scaling up to more complex multi-task environments."
            },
            "questions": {
                "value": "1. How do the authors envision the application of FiReAct to tasks beyond QA and the use of tools other than Google search? Can FiReAct work on other types of interactive tasks such as ScienceWorld, Mind2Web, InterCode, etc? Why do you choose to focus on QA only?\n2. Can the authors discuss potential approaches for fine-tuning more advanced agents involving multiple prompts, roles, and contexts?\n3. How might prompting and fine-tuning be optimally combined in a complex agent system?\n4. What are the anticipated difficulties in scaling FiReAct to a large-scale multi-task environment?\n5. How might newer or more powerful models like GPT-4 affect the outcomes, and what would be the implications for the FiReAct methodology?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6842/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698915697051,
        "cdate": 1698915697051,
        "tmdate": 1699636792168,
        "mdate": 1699636792168,
        "license": "CC BY 4.0",
        "version": 2
    }
]