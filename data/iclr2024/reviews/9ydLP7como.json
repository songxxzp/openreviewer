[
    {
        "id": "TAeo2L90KV",
        "forum": "9ydLP7como",
        "replyto": "9ydLP7como",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6026/Reviewer_RjcT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6026/Reviewer_RjcT"
        ],
        "content": {
            "summary": {
                "value": "This paper demonstrates to use the ReLU as an activation function for inference. While more complex activation functions are beneficial during training, they incur overhead during inference. With knowledge distillation, it is possible to reduce the accuracy loss when using the ReLU. The results implemented on different benchmarks show tradeoffs between accuracy and efficiency."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed method consists of a simple but effective idea.\n\n2. The experiments have been conducted on a wide set of benchmarks."
            },
            "weaknesses": {
                "value": "There are some suggestions for improvements. Please refer to the questions below."
            },
            "questions": {
                "value": "1. Please clarify what is the state-of-the-art of prior work that use ReLU for inference on networks that were trained with more complex activation functions. What are the limitations and challenges of the related work? What are the innovations made in these paper?\n\n2. It is recommended to compare the proposed solution using ReLU with approximate implementation of the complex original activation functions to explore the tradeoffs between accuracy and efficiency.\n\n3. Besides FPS and latency, it would be interesting to compare the results in terms of energy consumption between different activation functions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6026/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698149222933,
        "cdate": 1698149222933,
        "tmdate": 1699636647388,
        "mdate": 1699636647388,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JboN6ctHGg",
        "forum": "9ydLP7como",
        "replyto": "9ydLP7como",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6026/Reviewer_jbaK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6026/Reviewer_jbaK"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to use ReLU, instead of other complicated activation functions, in different transformer models for higher efficiency during inference. During training, this method uses knowledge distillation to mitigate the accuracy drop and can reduce computational costs with minimal accuracy degradation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper demonstrates the effectiveness of the proposed on different models, covering different tasks such as classification, object detection, and language modeling."
            },
            "weaknesses": {
                "value": "- The technical novelty of this paper is limited. The ReLU is more efficient than GeLU/SiLU is well known. And in ConvNext[1], they have discussed the model accuracy with ReLU and GeLU. Only replacing the activation function to ReLU, and without more theoretical analysis, does not contribute much.\n- The knowledge methods used in this paper are all existing methods.\n- This method need to train more with knowledge methods, which is not efficient for model training.\n\n[1] Zhuang Liu, et al. A ConvNet for the 2020s. CVPR, 2022"
            },
            "questions": {
                "value": "Typo\n- In abstract, \u201clanguage modelling\u201d -> \u201clanguage modeling\u201d"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6026/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698757405972,
        "cdate": 1698757405972,
        "tmdate": 1699636647256,
        "mdate": 1699636647256,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dPglDJQe7p",
        "forum": "9ydLP7como",
        "replyto": "9ydLP7como",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6026/Reviewer_9ug7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6026/Reviewer_9ug7"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a series of methods in order to replace modern activation functions with ReLU after training while minimizing quality loss. Their method is based on a combination of knowledge distillation and smooth interpolation from the original function to ReLU."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper was well written and clear aside from a few limited places I note below."
            },
            "weaknesses": {
                "value": "I think there are two primary weaknesses in this paper. First, I did not find the motivation for using ReLU to be very compelling. The author\u2019s case seems to be primarily predicated on an \u201cAI accelerator\u201d which remains unnamed. The paper provides essentially no detail on this alternative hardware accelerator other than some cycle count costs of matrix multiplication and SiLU in the introduction. The authors refer to this accelerator as \u201cour\u201d hardware in multiple places, which suggests to me that they may have omitted details to maintain anonymity. I think the paper would be much stronger if the authors re-wrote to include details on the accelerator of interest and avoided claiming it as \u201cours\u201d to maintain anonymity (although it is too late for this change in this review cycle, obviously).\n\nSecond, the results on Nvidia GPUs mixed and confusing in places. The quality loss/runtime improvements in Table 1 seem to suggest this is not a very good tradeoff. For example, 1.5% lower AP with DamoYolo for a 3% improvement in runtime? 1.5% accuracy loss is somewhat significant and I would expect that training a slightly smaller model would yield larger runtime improvements for the same quality. I also found the results in Table 2 somewhat contradictory to the paper up to that point. Switching from GeLU to ReLU appears to make the model slower? But it also increases the model quality?"
            },
            "questions": {
                "value": "In section 4.2.1, what does \u201cwe theorize the performance on one of hardware\u201d mean?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6026/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698764854024,
        "cdate": 1698764854024,
        "tmdate": 1699636647156,
        "mdate": 1699636647156,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uIL5QafhTQ",
        "forum": "9ydLP7como",
        "replyto": "9ydLP7como",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6026/Reviewer_SAkP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6026/Reviewer_SAkP"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes using the ReLU activation function in place of more complex activations like SiLU during inference to improve speed and efficiency. The key idea is that while smooth activations like SiLU are important for training, ReLU can be used during inference with minimal accuracy loss if knowledge distillation is used during training. The method is evaluated on image classification, object detection, and language models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The proposed method is simple but effective - replacing activation functions with ReLU improves inference speed across models and tasks with small accuracy drops.\n* The paper thoroughly evaluates the approach on major model types - CNNs, detectors like YOLO, and transformers. The consistent gains show the broad applicability.\n* Detailed experiments quantify the tradeoff between accuracy and speed. Up to 10% higher FPS is achieved on GPUs and even larger gains on specialized hardware.\n* The method can be combined with other optimizations like quantization for greater efficiency."
            },
            "weaknesses": {
                "value": "* The rationale behind initializing the student model (which has a ReLU activation function) with the teacher's weights is unclear. Does it aid in optimization, or does it provide the student model with a higher initial accuracy?\n* While all the components used in this study are well-established and the proposed method appears to be a straightforward amalgamation of techniques, it would be interesting to know the generality of the method. In which scenarios might the method falter and in which might it produce excellent performance?\n* The results pertaining to specialized hardware, though promising, are merely theoretical estimations. Practical implementation could potentially uncover additional trade-offs."
            },
            "questions": {
                "value": "see weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6026/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698813391773,
        "cdate": 1698813391773,
        "tmdate": 1699636647047,
        "mdate": 1699636647047,
        "license": "CC BY 4.0",
        "version": 2
    }
]