[
    {
        "id": "6i8nljJO5A",
        "forum": "dxI1HLatWw",
        "replyto": "dxI1HLatWw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7205/Reviewer_WyFN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7205/Reviewer_WyFN"
        ],
        "content": {
            "summary": {
                "value": "The authors describe how to model the data sampling/generation\nprocess, in a supervised learning framework, as a Markov reward\nprocess. This contrasts with the established i.i.d. data assumption\nused in most classifiers.\nThe authors consider that the data points are originated from a Markov\nreward process, where the policy is fixed and the goal is to perform a\npolicy evaluation.\nThey introduce a generalized temporal-difference (TD) algorithm for this.\n\nThey proposed a mapping between the training features (X) and the\nstates in RL, and between the target variable (y) and the state value\nfunction. Under this framework, the reward function can be estimated\nfrom the differences between the output variables from successive times.\nThey show that under a TD framework, the updating rule can bootstrap\nwith the output variable of the next time.\n\nThey also provide a generalization to different types of data\nby incorporating logits. They provide convergence proofs and several\nempirical studies showing different aspects of the proposed approach."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Relate two different learning paradigms \n- Proposes a theoretical framework\n- Present several theoretical results"
            },
            "weaknesses": {
                "value": "- The paper is not easy to read"
            },
            "questions": {
                "value": "The proposed approach seems only applicable to supervised\nlearning. How could it be extended to unsupervised learning where the\nis not a clear target output."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7205/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698614488649,
        "cdate": 1698614488649,
        "tmdate": 1699636855812,
        "mdate": 1699636855812,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rBiOFHQhNr",
        "forum": "dxI1HLatWw",
        "replyto": "dxI1HLatWw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7205/Reviewer_ZxKx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7205/Reviewer_ZxKx"
        ],
        "content": {
            "summary": {
                "value": "The paper applies reinforcement learning to solve conventional regression/supervised learning problems. In particular, the author(s) focused on classical generalized linear models and reformulated the parameter estimation with a reinforcement learning (RL) framework. This allows to apply the classical on-policy evaluation algorithm in RL to solve supervised learning. In theory, the author(s) established the convergence properties of the estimated parameters. Empirically, they further extended their methodology to deep learning."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "As far as I can see, the strengths of the paper can be summarized as follows:\n\n**Originality**: The idea to apply RL to solve supervised learning is relatively less explored in the literature. As the author(s) commented in the paper, existing algorithms are particularly designed for specific types of problems. To my knowledge, similar ideas have not been proposed in the literature. \n\n**Quality**: Compared to the existing RL algorithms to solve supervised learning problems, the solution the author(s) proposed is general and is applicable to a wide range of regression problems covering parameter estimation in both classical generalized linear models and deep learning. \n\n**Clarity**: The paper is easy to follow. The writing is generally clear."
            },
            "weaknesses": {
                "value": "In my assessment, the primary limitation of this paper stems from the effectiveness of the proposed method. From my interpretation, the recommended RL algorithm appears to be less efficient than traditional supervised learning algorithms, particularly from a theoretical standpoint. This issue considerably constrains the practicality of the presented methodology, and I will provide a comprehensive explanation below.\n\nLet's initiate the discussion with the ordinary least square (OLS) regression problem, which is introduced at the onset of Section 3. The conventional OLS method calculates the following estimator $\\widehat{\\omega}_{\\textrm{SL}}$ for $\\omega^*$, the oracle value in the linear model,  \n\n$$\\widehat{\\omega}_{\\textrm{SL}}=(\\sum_i x_i x_i^\\top)^{-1} (\\sum_i x_i y_i).$$\n\nThe mean squared error (MSE) of this estimator can be shown to be equivalent to\n\n$$n\\textrm{MSE}(\\widehat{\\omega}_{\\textrm{SL}})=\\sigma^2 \\textrm{trace}(\\Sigma^{-1}),$$ \n\nwhere $n$ denotes the sample size, $\\Sigma= \\mathbb{E} x_i x_i^\\top$ and $\\sigma^2$ denote the error variance $\\mathbb{E} (y_i-x_i^\\top \\omega^*)^2$. It is widely recognized that the OLS estimator is the Best Linear Unbiased Estimator (BLUE), minimizing the MSE among all unbiased estimators. \n\nMoving on to the RL estimator, as depicted in Equation (4), the resulting estimator $\\widehat{\\omega}_{\\textrm{RL}}$ can be expressed as\n\n$$\\Big[\\sum_i x_i (x_i-\\gamma x_{i+1})^\\top\\Big]^{-1} \\Big[\\sum_i x_i (y_i - \\gamma y_{i+1})\\Big].$$\n\nAssuming both $x_i$ and $y_i$ are of mean zero, its MSE can be shown to equal to\n\n$$n\\textrm{MSE}(\\widehat{\\omega}_{\\textrm{RL}})=(1+\\gamma^2)\\sigma^2 \\textrm{trace}(\\Sigma),$$ \n\nwhich is strictly larger than the SL-based estimator as long as $\\gamma>0$. Additionally, the larger the discounted factor $\\gamma$, the larger the MSE. This observation demonstrates that when tailored to linear models, the RL-based estimator exhibits statistical inefficiency in comparison to the SL-based estimator, particularly from a theoretical standpoint. A thorough examination of the closed-form expression for the RL estimator\u200b sheds light on the root cause of this inefficiency. In the RL estimator, the target variable encompasses both the current outcome $y_i$ (which embeds the true signal) and the future outcome $y_{i+1}$. It is crucial to note that the latter is independent of \n$x_i$ and can be perceived as an additional source of error. By adopting the RL framework, we inadvertently introduce extra noise to the outcome, ultimately compromising the efficiency of the resulting estimator.\n\nFurther considering the more extensive Generalized Linear Model (GLM), it is established that the SL-based estimator, when computed via Newton-type algorithms such as Fisher scoring, achieves efficiency, attaining the Cramer-Rao (CR) lower bound. I suspect that similar disparities are present in Generalized Linear Models (GMLs), where the RL-based estimator may not reach the CR lower bound, exhibiting reduced efficiency, due to the additional noise introduced in the target variable.\n\nIt is crucial to note that the above analysis predominantly pertains to estimators derived through Newton-type algorithms. When gradient-type algorithms are employed in conjunction with the Polyak-Ruppert averaging scheme, the resulting estimators exhibit the same asymptotic distributions as those computed via Newton's method. This leads us to the same conclusion."
            },
            "questions": {
                "value": "Please refer to the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Not applicable"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7205/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698617729591,
        "cdate": 1698617729591,
        "tmdate": 1699636855693,
        "mdate": 1699636855693,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "c2fmty1NkA",
        "forum": "dxI1HLatWw",
        "replyto": "dxI1HLatWw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7205/Reviewer_oS7Z"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7205/Reviewer_oS7Z"
        ],
        "content": {
            "summary": {
                "value": "This paper generalizes the temporal difference (TD) learning models for supervised learning with dataset treated as interconnected. To this end, this paper introduces a generalized TD learning algorithm and establishes theoretical convergence guarantees under both expected update and sample-based update. In addition, this paper also conducts experiments, which shows the generalization performance of the TD learning algorithm."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper proposes a new generalized TD learning algorithm and establishes theoretical convergence guarantees.\n\n2. This paper conducts experiments which shows the generalization performance of the TD learning algorithm."
            },
            "weaknesses": {
                "value": "1. Assumption 2 seems to be a little strong to require $f$ is invertible. In addition, how will this assumption be when the logit $z$ is a vector? For example, when $f$ is a softmax function?\n\n2. The algorithm 1 requires the knowledgement of transition $P$, which seems not realistic.\n\nMinors:\n\nIn assumption 1, $D(s)$ comes without definitions and explanation."
            },
            "questions": {
                "value": "1. I wonder how is lemma 1 derived? I think lemma 1 requires that the function $f$ is $L$-Lipschitz continuous. It seems that this can not be implied by assumption 1 and 2. \n\n2. I want to understand whether the transition kernel $P$ will influence the theoretical results. I do not find any term related to $P$ in theorems."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7205/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7205/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7205/Reviewer_oS7Z"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7205/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698663910911,
        "cdate": 1698663910911,
        "tmdate": 1699636855588,
        "mdate": 1699636855588,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3HUILmgRq1",
        "forum": "dxI1HLatWw",
        "replyto": "dxI1HLatWw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7205/Reviewer_9RLa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7205/Reviewer_9RLa"
        ],
        "content": {
            "summary": {
                "value": "In the traditional statistical learning setting, data is assumed to be independently and identically distributed (i.i.d.) according to some unknown probability distribution. Supervised algorithms such as generalized linear models are constructed by making assumptions about the distribution of the response variable given the independent variables. The authors propose an assumption where the data is interconnected and the sampling process is a Markov reward process. This turns the supervised learning problem into an on-policy evaluation problem in reinforcement learning, which the authors address by utilizing a generalized temporal difference (TD)\nlearning algorithm. They theoretically prove the convergence of these algorithms under linear function approximation and explore the relationship between the generalized TD solution and the original linear regression solution. Through their experiments, they compare the generalized TD algorithm with other baseline models in regression, binary classification, and image classification tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I appreciate that the authors provided clear background information on supervised learning and temporal difference learning before outlining the theoretical proofs. I also appreciate that the authors also provided limitations of their work within the main text."
            },
            "weaknesses": {
                "value": "I believe the authors could have provided a few more datasets for the regression problem and the binary classification class imbalance problem."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7205/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7205/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7205/Reviewer_9RLa"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7205/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698723460504,
        "cdate": 1698723460504,
        "tmdate": 1699636855466,
        "mdate": 1699636855466,
        "license": "CC BY 4.0",
        "version": 2
    }
]