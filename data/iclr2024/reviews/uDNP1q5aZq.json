[
    {
        "id": "Fyt0ZGp7Ty",
        "forum": "uDNP1q5aZq",
        "replyto": "uDNP1q5aZq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission249/Reviewer_i4rE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission249/Reviewer_i4rE"
        ],
        "content": {
            "summary": {
                "value": "This paper points out a common weakness of most existing backdoor attacks, which is randomly choosing samples from the benign dataset to poison without considering how important different samples are. Instead of random selection, this paper proposes a novel strategy to choose poisoned data more efficiently by formulating it as a min-max optimization problem, called learnable poisoning sample selection strategy (LPS). Extensive experiments show that this strategy can improve data poisoning attacks with low poisoning rates."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The idea of improving backdoor attacks' efficiency by having better sample selection to inject backdoors is intriguing. \n2. The proposed method is quite interesting. By effectively choosing highly influential poisoned samples, it allows different types of backdoor attack to reach high ASRs with very few number of poisoned samples. It can also overcome the limitations of previous related work (FUS).\n3. The paper includes vastly extensive experiments to show the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1. I find the results of this method with SSBA are quite underwhelming: in most experiments with SSBA, LPS could not reach high ASRs and seem to not have much improvement compared to the baselines. Therefore, I am not sure about LPS's flexibility, i.e., whether it can work with any type of backdoor trigger. \n2. Although there are experiments with different model architectures for the surrogate model, only ResNet is used for the target model. In the paper's settings, the adversary have full control of the process of generating poisoned samples but no control in the victim model's training procedure, so I think it would make more sense if there were experiments with different victim model's architectures.\n3. The experimental results of LPS's resistance against backdoor defenses are quite unsatisfying: in the cases of FP, ABL, NAD, and I-BAU, the ASRs are greatly degraded and/or have merely limited improvement compared to other baselines.  Also, there are experiments with 6 backdoor defenses, but none of them are data filtering defense, such as [1], [2], [3], [4]. Since the adversary in this paper acts as data provider, I think there should be also evaluations with data filtering defenses. \n\n\n[1] Chen, Bryant, et al. \"Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering.\" (AAAI 2019)  \n[2] Tran, Brandon, Jerry Li, and Aleksander Madry. \"Spectral signatures in backdoor attacks.\" (NeurIPS 2018)   \n[3] Hayase, Jonathan, et al. \"Spectre: Defending against backdoor attacks using robust statistics.\" (ICML 2021)   \n[4] Zeng, Yi, et al. \"Rethinking the backdoor attacks' triggers: A frequency perspective.\" (ICCV 2021)"
            },
            "questions": {
                "value": "1. Could the authors provide any insights/explanations for LPS's low performances with SSBA? Could LPS work with any attack method, or the adversary should carefully choose a suitable type of trigger? \n2. Regarding my concerns about victim model's architecture and data filtering defenses, I would recommend adding aforementioned experiments.\n3. All datasets used in this work have relatively low resolution. Could this method also work with high resolution datasets, such as CelebA, PubFig, ImageNet?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission249/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698210944815,
        "cdate": 1698210944815,
        "tmdate": 1699635950699,
        "mdate": 1699635950699,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "T5Q40pScJ1",
        "forum": "uDNP1q5aZq",
        "replyto": "uDNP1q5aZq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission249/Reviewer_mh2m"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission249/Reviewer_mh2m"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors study the data-poisoning-based backdoor attack. They propose to select the hard backdoor samples with the larger training loss on the surrogate model and formulate it as a min-max optimization problem. Extensive experiments show that it's better than the random selection and existing baseline."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. It's an interesting idea to exploit hard examples to inject backdoors.\n2. Experiments show it can improve existing backdoor attacks.\n3. Code is provided."
            },
            "weaknesses": {
                "value": "1. It seems not robust against existing defenses according to Table 4. Although it improves the robustness of existing attacks, it's still defeated by existing defenses.\n2. Some other attack and defense methods may be evaluated as well, such as [FTrojan](https://dl.acm.org/doi/abs/10.1007/978-3-031-19778-9_23), [ABS](https://dl.acm.org/doi/10.1145/3319535.3363216), [Unicorn](https://arxiv.org/abs/2304.02786).\n3. Because it selects training samples with larger losses, it may be easily detected by scanning the dataset.\n4. It would be better to show how to formulate the strategy for all-to-all and clean labels in the appendix.\n5. It's good to show the performance with different poisoning rates. However, in the tables, most of the ASRs are lower than 90%. That means they are all unsuccessful attacks and not that useful. One may want to show a different ratio range."
            },
            "questions": {
                "value": "1. Can it be detected by scanning the dataset and selecting the outliers with respect to the training loss?\n2. How does the robustness look like if the ASRs are above 90%? Currently, most of the ASRs in Table 4 are very low.\n3. Are the losses for the selected samples aligned well on the surrogate model and the target model? That is if the target model also considers them as hard examples. Also, how does the loss change for those hard backdoor samples? Because existing research shows the backdoor features are usually easier to learn and thus have smaller losses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission249/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission249/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission249/Reviewer_mh2m"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission249/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698720700319,
        "cdate": 1698720700319,
        "tmdate": 1699635950617,
        "mdate": 1699635950617,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "S4s4GIJrPe",
        "forum": "uDNP1q5aZq",
        "replyto": "uDNP1q5aZq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission249/Reviewer_n1Ao"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission249/Reviewer_n1Ao"
        ],
        "content": {
            "summary": {
                "value": "The authors present a sample selection method for data poisoning aimed at enhancing backdoor attacks. A min-max optimization technique is employed to learn a poisoning mask for selecting the appropriate samples."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Pros:\n- The manuscript is well-organized and easy to follow.\n- Although the idea of sample selection for poisoning is conceptually similar to the FUS method, the two approaches diverge in their perspectives. While FUS focuses on local optimization, the proposed method aims for global sample selection.\n- The empirical results are robust and substantiate the paper's claims effectively."
            },
            "weaknesses": {
                "value": "Cons:\n- The code for replication is not provided, limiting the paper's reproducibility.\n- The significant training loss gap between poisoned and clean samples might make the attack easily detectable by potential victims.\n- Ethic statement is missing."
            },
            "questions": {
                "value": "- Does the threshold \"T\" vary across different datasets and model architectures?\n- Is the proposed approach effective for the combination of CNN-based surrogate models and attention-based target models?\n- Could the authors clarify why the method underperforms when the poisoning rate is low?\n- For the ablation study, could the authors provide results of LPS\\PC?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety",
                    "Yes, Potentially harmful insights, methodologies and applications"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Given that this paper focuses on boosting backdoor attacks, an ethical statement should be included."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission249/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698734785833,
        "cdate": 1698734785833,
        "tmdate": 1699635950537,
        "mdate": 1699635950537,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BWm48WYvPv",
        "forum": "uDNP1q5aZq",
        "replyto": "uDNP1q5aZq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission249/Reviewer_CSPw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission249/Reviewer_CSPw"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the inefficiency in existing data-poisoning based backdoor attacks which arbitrarily select samples from a benign dataset to poison, overlooking the varying significance of different samples.\n\nThe paper proposes a Learnable Poisoning sample Selection (LPS) strategy. The strategy employs a min-max optimization approach to understand which samples are most crucial for poisoning.\n\nThe paper sets up a two-player adversarial game: The inner optimization focuses on maximizing the loss concerning the mask, to pinpoint hard-to-poison samples. The outer optimization aims to minimize the loss with respect to the model's weight, to train the surrogate model.\nThrough multiple iterations of this adversarial training, the system selects samples that have a higher contribution to the poisoning process.\n\nComprehensive experiments on established datasets are conducted. Results showcase that the LPS strategy significantly enhances the efficiency and effectiveness of several data-poisoning based backdoor attacks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper proposes a novel approach to select samples for poisoning. The approach is intuitive and effective.\n- The paper provides a comprehensive evaluation and solution proof."
            },
            "weaknesses": {
                "value": "- The problem makes some sense to me, but I am not sure how practical it is. A practical scenario will better motivate the problem."
            },
            "questions": {
                "value": "1. What is the practical scenario that the proposed boosted attack can be used in?\n\n2. What is the scalability of the proposed attack? For example, the ImageNet has 1000 classes. Is the inner maximization still effective?\n\n3. Some term usages are confusing. For example, under the context of backdoor attack, the term 'm' mask usually refers to the mask of trigger. However, in this paper, the term 'm' mask refers to the mask of sample selection. It would be better to use a different term to avoid confusion. Another example, in page 3, 'K' refers to class numbers. But Section 6 says 'K' is epoch numbers. It would be better to clarify such inconsistency."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission249/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission249/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission249/Reviewer_CSPw"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission249/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698781021741,
        "cdate": 1698781021741,
        "tmdate": 1699635950465,
        "mdate": 1699635950465,
        "license": "CC BY 4.0",
        "version": 2
    }
]