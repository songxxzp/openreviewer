[
    {
        "id": "N5obwzEuPy",
        "forum": "gSB6IfUEGz",
        "replyto": "gSB6IfUEGz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3241/Reviewer_cVSz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3241/Reviewer_cVSz"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a speaker creation method that uses a pre-trained speaker embedding model together with Gaussian sampling to generate new speakers."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed model performs better than Tacospawn in generating unseen and diverse speakers.\n2. Ablation studies are performed to understand the impact of different design choices.\n3. The paper is in general written clearly with a good amount of illustrations and maths."
            },
            "weaknesses": {
                "value": "1. I think the novelty is somewhat limited. To me, the proposed method is quite similar to a VAE-based speaker encoder but with the encoder changed to a pre-trained speaker embedding module. The mapping network is almost the same as in StyleGAN and is quite similar in principle to AdaSpeech. My main takeaway is that a pre-trained speaker embedding model as the encoder of a VAE works well for generating unseen speakers, and NFA is a promising speaker embedding model compared to other pre-trained speaker embedding model.\n2. The baselines compared are quite old by deep learning standards. There are a few follow-up works to TacoSpawn. For zero-shot VC there are many more recent works (e.g. ControlVC). The authors state VITS is the \"the state of the art\" Multi-Speaker TTS but it is by no means true in 2023."
            },
            "questions": {
                "value": "For Table 1, it is not clear to me for each baseline, what is the exact procedure and hyper-params involved to sample speaker embeddings. I hope the authors can clarify."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3241/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698352618471,
        "cdate": 1698352618471,
        "tmdate": 1699636272623,
        "mdate": 1699636272623,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MN90G7Va6S",
        "forum": "gSB6IfUEGz",
        "replyto": "gSB6IfUEGz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3241/Reviewer_fp4x"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3241/Reviewer_fp4x"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a neural factor analysis (NFA)-based speech synthesis model for tts, vc, and voice generation. They simply disentangle a HuBERT representation for controlling the semantic guided voice representations."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "They utilize a neural factor analysis (NFA) for speech representation disentanglement, and adapt the semantic representation with style conditions. This simple modification could improve the TTS and VC performance by adopting it to speech resynthesis frameworks."
            },
            "weaknesses": {
                "value": "1.\tThe authors should have conducted more comparisons to evaluate the model performance. They only compare it with speech resynthesis, and the result is a little incremental.\n\n2.\tThey utilize a NFA which was proposed in ICML 2023. The contribution is weak.\n\n3.\tIt would be better if you could compare the self-supervised speech representation model for the robustness of your methods by replacing HuBert with any other SSL models.\n\n4.\tUsing HuBERT representation may induce a high WER. Replacing it with ContentVec may improve the pronunciation.\n\n5.\tThe semantic conditioned Transformation is utilized in many works. NANSY++ utilizes a time-varying timbre embedding and this is almost the same with this part."
            },
            "questions": {
                "value": "1.\tThe authors cited LibriTTS and LibriTTS-R together. Which dataset do you use to train the models? In my personal experience, using LibriTTS-R decreases the sample diversity. Do you have an experience with this?\n\n2.\tAccording to ICLR policy, when using human subjects such as MOS, you may include the evaluation details in your paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3241/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3241/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3241/Reviewer_fp4x"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3241/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698653867888,
        "cdate": 1698653867888,
        "tmdate": 1700624087165,
        "mdate": 1700624087165,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DSRVuTcdym",
        "forum": "gSB6IfUEGz",
        "replyto": "gSB6IfUEGz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3241/Reviewer_dpEi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3241/Reviewer_dpEi"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an unsupervised voice generation model called VoxGenesis by transforming Gaussian distribution to speech distribution. The proposed VoxGenesis can discover a latent speaker manifold and meaningful voice editing directions without supervision, and the latent space uncovers human-interpretable directions associated with specific speaker characteristics such as gender attributes, pitch, tone, and emotion, allowing for voice editing by manipulating the latent codes along these identified directions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. It is interesting to utilize Gaussian distribution transformation for unsupervised voice (speech) synthesis so that the model is able to generate realistic speakers with distinct characteristics like pitch, tone, and emotion."
            },
            "weaknesses": {
                "value": "1. While the idea is promising, the experimental results seem to be limited. Most of the performances are from the ablation studies. The proposed model should compare the performances with the previous works like SLMGAN [1], StyleTTS [2], and LVC-VC [3]. Moreover, the paper only utilizes one dataset, LibriTTS-R. More extensive experiments on different dataset might be necessary.\n2. The paper can be more curated. While it is well written paper, it slightly lacks in structure. Since the idea is interesting enough, I would consider adjusting the rating if the paper is more well structured and the additional experiments are conducted.\n\n[1] Li, Yinghao Aaron, Cong Han, and Nima Mesgarani. \"SLMGAN: Exploiting Speech Language Model Representations for Unsupervised Zero-Shot Voice Conversion in GANs.\" 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA). IEEE, 2023.\n\n[2] Li, Yinghao Aaron, Cong Han, and Nima Mesgarani. \"Styletts: A style-based generative model for natural and diverse text-to-speech synthesis.\" arXiv preprint arXiv:2205.15439 (2022).\n\n[3] Kang, Wonjune, Mark Hasegawa-Johnson, and Deb Roy. \"End-to-End Zero-Shot Voice Conversion with Location-Variable Convolutions.\" Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH. Vol. 2023. 2023."
            },
            "questions": {
                "value": "Please refer to the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3241/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3241/Reviewer_dpEi",
                    "ICLR.cc/2024/Conference/Submission3241/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3241/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698752220080,
        "cdate": 1698752220080,
        "tmdate": 1700638217681,
        "mdate": 1700638217681,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cozv4tAC4E",
        "forum": "gSB6IfUEGz",
        "replyto": "gSB6IfUEGz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3241/Reviewer_dHpK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3241/Reviewer_dHpK"
        ],
        "content": {
            "summary": {
                "value": "This paper discusses on an approach on modeling speaker's voice in speech generation models, as well as its application in voice conversion and multispeaker TTS."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "Not clear to me."
            },
            "weaknesses": {
                "value": "* The clarity of the presentation and writing needs improvement. Overall, it's difficult to read this paper. Some basic writing styles, like missing necessary parentheses on citations, is very bothersome for reading. The description of the method seems over complicated than what the method actually is.\n\n* A lot of technical incorrectness. Just a few examples:\n   - Sec 3.1: \"GAN has been the de facto choice for vocoders.\" This is a false claim and ignores a large arrays of active and important works in the community. There are other popular vocoders choices like WaveNet, WaveRnn, diffusion-based approaches etc.\n   - Sec 3.1: \"A notable limitation in these models is ... learn to replicate the voices of the training or reference speakers rather than creating new voices.\" Another false claim. It improper to state for such an \"limitation\" because that is not the goal of the task of vocoders.\n   - Sec 3.1 \"consequently, a conditional GAN is employed to transform a Gaussian distribution, rather than mapping speech features to waveforms\". This is another improper comparison as what GAN does is to transfer the conditioning features (e.g. speech features) , rather than plain Gaussian noise, to waveforms.\n   - Sec 3.1 \"It is crucial, in the absence of Mel-spectrogram loss, for the discriminator to receive semantic tokens; otherwise, the generator could deceive the discriminator with intelligible speech.\" I don't see the connection.\n   - Sec 4.2 says speaker similarity is evaluated in a 3-point scale from 0 to 1, but Table 3 shows speaker similarity as 4.x and 3.x values."
            },
            "questions": {
                "value": "None."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3241/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698885209547,
        "cdate": 1698885209547,
        "tmdate": 1699636272341,
        "mdate": 1699636272341,
        "license": "CC BY 4.0",
        "version": 2
    }
]