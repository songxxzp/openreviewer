[
    {
        "id": "K0CSRP4WA9",
        "forum": "wZWTHU7AsQ",
        "replyto": "wZWTHU7AsQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5179/Reviewer_GZCq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5179/Reviewer_GZCq"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the challenge of deploying reinforcement learning (RL) systems that can withstand uncertainties, particularly those that are temporally coupled. Recognizing that conventional robust RL methods may falter against such temporally-linked perturbations, the authors introduce a game-theoretic approach called GRAD (Game-theoretic Response approach for Adversarial Defense). GRAD conceptualizes the robust RL problem as a partially observable two-player zero-sum game and uses Policy Space Response Oracles (PSRO) to achieve adaptive robustness against evolving adversarial strategies. The study's experiments confirm GRAD's performance in ensuring RL robustness against both temporally coupled and standard adversarial perturbations."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors formulate the robust RL objective as a zero-sum games and demonstrating the efficacy of game-theoretic RL in tackling this objective."
            },
            "weaknesses": {
                "value": "1. This paper does not have a clear mathematical representation of the problem it intends to address. \n\n2. The article claims its primary contribution lies in using zero-sum games to formulate the robust RL problem. However, employing zero-sum games to account for uncertainties, whether in single-agent or multi-agent RL, is well-established, as seen in works like Robust Adversarial Reinforcement Learning, Robust Reinforcement Learning as a Stackelberg Game via Adaptively-Regularized Adversarial Training, and Robust Multi-Agent Reinforcement Learning with State Uncertainty. Given this widespread application, the paper's stated novelty becomes questionable.\n\n3. In terms of algorithmic design, the proposed method is largely an application of the Policy-Space Response Oracles (PSRO). The novelty seems limited, and it's unclear how PSRO uniquely addresses the issue of temporally-coupled perturbations.\n\n4. Considering that the PSRO algorithm converges to an NE in two-player, zero-sum games and has seen recent extensions to other equilibria types [1, 2], the paper's proposed method, essentially a reiteration of PSRO, makes the convergence proof for GRAD appear somewhat lackluster in its contribution.\n\n5. This paper has ample room for improvement in writing, problem formulation, and the method itself. For instance, by simply using the triangle inequality, we could give the range of $\\bar{\\epsilon}$ that break the temporally-coupled property, rather than merely stating, \"By setting \\epsilon \u0304 to a large value, it converges to the non-coupled attack scenario.\" Additionally, the motivation behind temporally-coupled perturbations lacks clarity and persuasiveness, leaving me unconvinced of its pressing relevance.\n\n[1] Lanctot, Marc, et al. \"A unified game-theoretic approach to multiagent reinforcement learning.\" Advances in neural information processing systems 30 (2017).\n\n[2] McAleer, Stephen, et al. \"Pipeline psro: A scalable approach for finding approximate nash equilibria in large games.\" Advances in neural information processing systems 33 (2020): 20238-20248."
            },
            "questions": {
                "value": "Please see the Weaknesses, I will decide the final rating after the rebuttal."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Non."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5179/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5179/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5179/Reviewer_GZCq"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5179/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698068444408,
        "cdate": 1698068444408,
        "tmdate": 1700516652610,
        "mdate": 1700516652610,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jGRwI2L0qv",
        "forum": "wZWTHU7AsQ",
        "replyto": "wZWTHU7AsQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5179/Reviewer_JmX3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5179/Reviewer_JmX3"
        ],
        "content": {
            "summary": {
                "value": "This work introduces a novel-framework of temporally-coupled robust RL problem that is closer to the real-world setting. This work proposes GRAD, a game-theoretic approach to provide robust policies played against an adversary which attacks states and actions fitting in the temporally-coupled robust RL problem setting. This work also gives extensive complementing experiment results."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I think this work really pushes the robust RL community research efforts further by answering:\n\n> can we design robust RL algorithms for realistic nature attacks?\n\nThe main contribution of game-theoretic algorithm with temporal-based nature attacks (robust RL problem) is a really nice idea worthy for publication. But the score reflects my weakness section."
            },
            "weaknesses": {
                "value": "I have only a few weakness for this work as follows:\n\n> The current framework considers robustness against state and action uncertainty. More closer work [1] and thereafter are not included. Model uncertainty is justified in the framework mentioning the evolution of the environment depends on the perturbed actions. Model uncertainty in robust RL is defined in more generality [2-10]. So it will be better to include more detailed Related Works including [2-10] and more relevant works in the revision. I agree this work includes experiments with model uncertainty, but the baselines are also only action robust algorithms. I'd rather see more extensive writing and experiments for model-uncertainty OR the current work just focusing on state-action uncertainty is a big step forwards in itself. I've also stopped at '10' since you get the idea of inadequate related work discussion.\n\n> GRAD shares similar idea of RARL algorithm (Pinto et al., 2017), that is, zero-sum structure to get the robust policy against the nature adversary. More details than below must be added to point out key differences (like state-action uncertainty inbuilt) and due references need to be given.\n` Pinto et al. (Pinto et al., 2017) model the competition between the agent and the attacker as a zero-sum two-player game, and train the agent under a learned attacker to tolerate both environment shifts and adversarial disturbances `\n\nI am open to discussions with the authors and reviewers to increase/maintain (already reflects the positive impact) my score. All the best for future decisions!\n\n[1] Robust Multi-Agent Reinforcement Learning with State Uncertainty Sihong He, Songyang Han, Sanbao Su, Shuo Han, Shaofeng Zou, and Fei Miao. Transactions on Machine Learning Research, June 2023.\n\n[2] Xu. Z, Panaganti. K, Kalathil. D, Improved Sample Complexity Bounds for Distributionally Robust Reinforcement Learning. Artificial Intelligence and Statistics, 2023.\n\n[3] Nilim, A. and El Ghaoui, L. (2005). Robust control of Markov decision processes with uncertain transition matrices. Operations Research, 53(5):780\u2013798\n\n[4] Iyengar, G. N. (2005). Robust dynamic programming. Mathematics of Operations Research, 30(2):257\u2013280.\n\n[5] Panaganti, K. and Kalathil, D. (2021). Robust reinforcement learning using least squares policy iteration with provable performance guarantees. In Proceedings of the 38th International Conference on Machine Learning, pages 511\u2013520.\n\n[6] Panaganti, K. and Kalathil, D. (2022). Sample complexity of robust reinforcement learning with a generative model. In Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, pages 9582\u20139602.\n\n[7] Roy, A., Xu, H., and Pokutta, S. (2017). Reinforcement learning under model mismatch. In Advances in Neural Information Processing Systems, pages 3043\u20133052.\n\n[8] Panaganti, K., Xu, Z., Kalathil, D., and Ghavamzadeh, M. (2022). Robust reinforcement learning using offline data. Advances in Neural Information Processing Systems (NeurIPS).\n\n[9] Shi, L. and Chi, Y. (2022). Distributionally robust model-based offline reinforcement learning with near-optimal sample complexity. arXiv preprint arXiv:2208.05767\n\n[10] L Shi, G Li, Y Wei, Y Chen, M Geist, Y Chi  (2023) The Curious Price of Distributional Robustness in Reinforcement Learning with a Generative Model, NeurIPS 2023"
            },
            "questions": {
                "value": "-n/a-"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5179/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698448884350,
        "cdate": 1698448884350,
        "tmdate": 1699636513677,
        "mdate": 1699636513677,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5rDurpcmTn",
        "forum": "wZWTHU7AsQ",
        "replyto": "wZWTHU7AsQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5179/Reviewer_mjop"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5179/Reviewer_mjop"
        ],
        "content": {
            "summary": {
                "value": "The classic robust RL focuses on worst-case scenarios, which may result in an overly conservative policy. Instead, this paper introduces temporally-coupled perturbations. Additionally, this paper proposed an adversarial training approach named the game-theoretic response approach for adversarial defense. Finally, the authors show the robust performance of the proposed methods in several MuJoCo tasks compared with several baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* This paper is easy to follow.\n* This paper does thorough experiments for state attacks and action attacks and compares the proposed method with several baselines. \n* The temporally-coupled adversarial perturbation seems new."
            },
            "weaknesses": {
                "value": "* Although the temporally-coupled adversarial perturbation seems new, it is quite limited. Definition 3.2 only considers the temporally-coupled perturbation from the last time step. Even if the authors don't consider the general partially observable MDP, they should consider a more general case, e.g., m-order MDP [Efroni et al. 2022, Provable Reinforcement Learning with a short-term memory].\n* The zero-sum game-based approach is not new for robust training in RL, e.g., [Tessler et al., 2019].\n* This paper misses one classic setting of robust MDP (i.e., transition adversaries) [e.g., Iyengar'05, Robust Dynamic Programming] as well as related baselines [e.g., Zhou et al. 2023, Natural Actor-Critic for Robust Reinforcement Learning with Function Approximation].\n* Figure 1 doesn't make sense to me. The robust baseline considers the worst-case scenario, which should be more stable for different kinds of attacks compared with the less conservative model that is proposed in this work."
            },
            "questions": {
                "value": "Please refer to the \"weakness\" section for further information."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5179/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5179/Reviewer_mjop",
                    "ICLR.cc/2024/Conference/Submission5179/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5179/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698644320239,
        "cdate": 1698644320239,
        "tmdate": 1700435595691,
        "mdate": 1700435595691,
        "license": "CC BY 4.0",
        "version": 2
    }
]