[
    {
        "id": "mFpCh8mbdg",
        "forum": "Unz9zYdjTt",
        "replyto": "Unz9zYdjTt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5479/Reviewer_hsUT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5479/Reviewer_hsUT"
        ],
        "content": {
            "summary": {
                "value": "This paper explores novel class discovery and learning within the framework of federated learning, addressing the challenge of evolving data on local client devices. The study introduces a Global Alignment learning framework aimed at estimating the number of global novel classes and providing guidance for local training. Specifically, this framework initially estimates the number of novel classes by identifying high-density regions within the representation space. Then, it captures all potential correlations between prototypes and training data to alleviate issues related to data heterogeneity. The proposed approach demonstrates advanced performance across a range of benchmark datasets"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is well-written and easy to read. \n\n2. It introduces an effective approach for discovering novel classes with Global Alignment learning, specifically targeting the federated learning setting with dynamic change in data distribution.  \n\n3. The authors conducted thorough experiments to validate the effectiveness of their proposed approach. Through empirical evidence, they support their claims and offer insights into the performance and advantages of novel class learning under the constraint of non-iid data, and privacy protection."
            },
            "weaknesses": {
                "value": "1. The method lacks sufficient elaboration and requires additional effort to comprehend the underlying technique. For instance, it encompasses multiple design elements, yet Figure 1 fails to offer adequate details to facilitate a clear understanding of the approach. And it is difficult to visualize the algorithm. It would be better if the paper provided an algorithm block. (Q1, Q2, Q4)\n\n2. Some of the assertions made in the paper need additional justifications. (Q3)"
            },
            "questions": {
                "value": "1. What is the training process? \n\n(a) In section 3.2, the known classes depend on Equation (2) to converge to model $m^L$. Then Section 3.3.1 includes a modified PCL to enhance training. Is the modified PCL included during the training for $m^L$ or after the global server achieved $m^L$?\n\n(b) From Figure 1, the local prototypes are only uploaded once to the server, which contradicts my understanding of FL: the local models should communicate with the central server for several rounds until converge. And as the local model keeps updating, the local prototypes should also evolve, how does one-time uploading transmit every local information?\n\n2. Section 3.2 mentions the unlabeled testing datasets \"belongs to a unified novel label space\", does it assume the number of classes of novel samples is fixed? If this number is not fixed, then how to decide the number of clusters? If this number is fixed, how does it apply to the setting where the novel data are continually emerging?\n\n3. Section 3.3.2 chooses neuron weights as prototypes since the data from other clients is unavailable in the FL setting. However, there is still a lot of difference between weights and features. Any justification to support that using weights as prototypes is as effective as data/features?\n\n4. The \"anchor sample\" first appears in Section 3.3.1. How are they selected, or what is their definition? \n\n5. In Section 3.3.2, there is a data memory storing filtered-out data. Are these data filtered out because they are known class data?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5479/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5479/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5479/Reviewer_hsUT"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5479/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698633779986,
        "cdate": 1698633779986,
        "tmdate": 1699636559237,
        "mdate": 1699636559237,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dzdxp8vKmg",
        "forum": "Unz9zYdjTt",
        "replyto": "Unz9zYdjTt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5479/Reviewer_RSBY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5479/Reviewer_RSBY"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a prototype-based class number estimation method for Federated Novel Class Discovery, where the model is required to merge and align novel classes that are discovered and learned by different clients under privacy constraint. Extensive experimental results demonstrate the effectiveness of proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1.\tThis paper is well-written, well-organized and easy to follow.\n2.\tThe performance of the proposed method is impressive.\n3.\tAblation studies are comprehensive and demonstrate the effectiveness of proposed method."
            },
            "weaknesses": {
                "value": "1.\tLacks of crucial reference literatures [A][B]. Thus, Federated New Class Discovery/Learning is not a new research problem. In contribution, authors say,\u201d we are the first to focus on this problem and propose an effective solution\u201d. It might be somewhat overclaiming.\n2.\tInsufficient comparison and discussion. From my understanding, the proposed method is similar to commonly-used semi-supervised learning methods. \n3.\tLimited novelty. From the perspective of NCD methods, prototype-based contrastive learning and low confidence sample rejection have explored in [C]. Why is the proposed method superior to [C]? From the perspective of semi-supervised federated learning methods, the federated prototype learning has been studied in [D][E]. What\u2019s the novelty of the proposed methods compared with [D][E]?\n4.\tFrom my understanding, [A][B] can also be used in federated new class learning. It is better to discuss and compare the proposed method with [A][B].\n5.\tLack of theoretical proof why the estimation method is better than other competitors.\n6.\tThe reasonableness of the experiment setting still needs to be considered. As authors claimed in the paper, they use Dirichlet Distribution to control data heterogeneity. (1) It makes client labeled and unlabeled data highly-unbalanced. However, in [F], the labeled and unlabeled data are kept fixed partitioning. It is better to discuss the relationship between proposed method and data distribution. (2) It might lead clients to have some sharing categories or have non-overlapping categories. How does the proposed method solve both situations?\n\n[A] Towards Unbiased Training in Federated Open-world Semi-supervised Learning. ICML, 2023.\n[B] Federated Generalized Category Discovery, Arxiv, 2023.\n[C] Opencon: Open-world contrastive learning. TMLR, 2022.\t\n[D] Fedproto: Federated prototype learning across heterogeneous clients, AAAI,2022.\n[E]Federated Semi-Supervised Learning with Prototypical Networks, Arxiv,2022.\n[F] Generalized Category Discovery, CVPR, 2022."
            },
            "questions": {
                "value": "Please see Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5479/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698722333225,
        "cdate": 1698722333225,
        "tmdate": 1699636559151,
        "mdate": 1699636559151,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iZJu8mBa8L",
        "forum": "Unz9zYdjTt",
        "replyto": "Unz9zYdjTt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5479/Reviewer_a8Cy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5479/Reviewer_a8Cy"
        ],
        "content": {
            "summary": {
                "value": "This paper considers a novel FL scenario, where the data distribution involves dynamic and continual changes. Instead of naively integrating FL and conventional novel class discovery methods,  the authors propose a Global Alignment Learning (GAL) framework to estimate the number of novel classes and optimize the local training process in a semantic similarity-empowered reweighting manner. Extensive experiments have been conducted to demonstrate the efficiency of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "[+] The whole paper is easy to understand, and well-written.\n\n[+] The problem statement is very nice and clean. It also has some applications in practice.\n\n[+] There is enough empirical evidence to support the main claims of the paper."
            },
            "weaknesses": {
                "value": "[-] It seems that open-world semi-supervised learning [1][2] also considers classifying both seen and unseen classes during the testing phase. Please compare it in related work.\n\n[1] Open-world semisupervised learning. In International Conference on Learning Representations, 2022.\n\n[2] Robust semi-supervised learning when not all classes have labels. In Advances in Neural Information Processing Systems, 2022.\n\n[-] In section 4, the baselines on federated self-supervised learning methods are insufficient. It is suggested to add more FedSSL methods in the experimental part.\n\n[3] Semifl: Semi-supervised federated learning for unlabeled clients with alternate training. Advances in Neural Information Processing Systems, 35:17871\u201317884, 2022."
            },
            "questions": {
                "value": "It is unclear why the value of $n_{size}$ is set as 2. A more detailed explanation should be added."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5479/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698746424660,
        "cdate": 1698746424660,
        "tmdate": 1699636559037,
        "mdate": 1699636559037,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PXHw110V4p",
        "forum": "Unz9zYdjTt",
        "replyto": "Unz9zYdjTt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5479/Reviewer_skcH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5479/Reviewer_skcH"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel method to learn novel classes in federated learning with emerging unknown classes."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. (Originality) The proposed method is novel by known-class representation learning and adaptive class merging without access to clients' data.\n2. (Clarity) The paper is clear in techniques. Methods are well formulated and motivated. Sufficient details are provided for the experiments.\n3. (Significance) The proposed method is sufficiently evaluated in multiple datasets, models including small-scale sets (like Cifar10, or Cifar100) and large-scale sets (ImageNet). In all of these experiments, the proposed methods outperform the baselines in both known class and novel class evaluations.\n4. (Quality) Extensive experiments evaluate the method in multiple dimensions. Importantly, multiple federated learning is demonstrated to be integrable with the proposed method."
            },
            "weaknesses": {
                "value": "1. The authors claim their contribution as a federated novel-class learning without compromising privacy. However, it is unclear how the existing federated novel-class learning methods compromise privacy. Importantly, the definition of private information is vague. It seems that the number of novel classes is thought to be private, which however is not necessarily true. Without specification on the privacy definition, there also lacks sufficient justification for how the proposed method will protect privacy. Though I appreciate the empirical results of privacy evaluation, the authors should clarify the meaning of privacy and adjust the claim of privacy."
            },
            "questions": {
                "value": "* What is the definition of private information in the paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5479/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5479/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5479/Reviewer_skcH"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5479/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698902382091,
        "cdate": 1698902382091,
        "tmdate": 1700690473003,
        "mdate": 1700690473003,
        "license": "CC BY 4.0",
        "version": 2
    }
]