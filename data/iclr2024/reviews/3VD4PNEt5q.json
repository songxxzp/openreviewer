[
    {
        "id": "NdDu3iyiHy",
        "forum": "3VD4PNEt5q",
        "replyto": "3VD4PNEt5q",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1194/Reviewer_nx8r"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1194/Reviewer_nx8r"
        ],
        "content": {
            "summary": {
                "value": "The author proposes a two-stage optimization strategy for camera-LiDAR fusion models via using adversarial patches in camera images. The paper explores single-modal attack on fusion models, an interesting yet under-explored problem."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper is well-written and explores an interesting topic. \n* The proposed method can decrease the model performance by a large margin, showing the possibility of dramatically deteriorating the system performance by only attacking single-modality."
            },
            "weaknesses": {
                "value": "* The effectiveness of the proposed two-stage optimization approach needs further justifications. Only showing the performance drop on fusion models is not enough. Comparisons with other single-stage attacks are also needed to demonstrate the effectiveness. Without proper benchmarks and comparisons with other SOTA algorithms, it is hard to justify the effectiveness of the technical contributions. \n* How to ensure the feasibility of the adversarial patches? Since the gradient optimization may find patches in the undeployable areas e.g., sky, can the proposed approach ensure the attack is feasible in the real physical world? Also in the paper, the author assumes the lidar data would not be changed. Since the patch may influence the lidar intensity or introduce extra points, please provide justifications for this assumption."
            },
            "questions": {
                "value": "* What is the sensitivity of single-modal methods vs multi-modal (LiDAR/camera) methods?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1194/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1194/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1194/Reviewer_nx8r"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1194/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698432586813,
        "cdate": 1698432586813,
        "tmdate": 1700626554084,
        "mdate": 1700626554084,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Ji7vbx0VqJ",
        "forum": "3VD4PNEt5q",
        "replyto": "3VD4PNEt5q",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1194/Reviewer_DXNR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1194/Reviewer_DXNR"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an adversarial attack method, which could attack multi-modal 3D object detection methods from the image input, making the attack easy to implement.\nIt first recognizes the proper attack type and proposes different methods for each type.\nExperiments are conducted with multiple popular multi-modal detectors, showing promising performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The motivation that attacks from image input is reasonable, which is practical real world.\n- The proposed method overall makes sense.\n- Authors provide a demo, which makes the application of the proposed method more clear.\n- Authors conducted extensive experiments with many SOTA the art detectors, making the results convincing."
            },
            "weaknesses": {
                "value": "- Attack from only the image side has limited application. In particular, some methods do not conduct feature-level fusion between image and LiDAR like [1][2].  The two modalities are decoupled in these methods. Even if the image modality is totally failed, they can output reasonable results.\n- The writing is not clear, especially in page 4 and page 5. There are very long paragraphs and many notations without clear organization, making it hard to follow the detailed method. I list some detailed questions in the following question box. \n- The method is not well-motivated. The proposed method seems to be a general attacking method. Are there any special designs to solve problems in image-only attacking or autonomous driving scenes? What is the difficulty of image-only attacks?\n\n[1] Frustum PointNets for 3D Object Detection from RGB-D Data \\\n[2] Fully Sparse Fusion for 3D Object Detection"
            },
            "questions": {
                "value": "- In attack strategies, is the noise patch shared by the whole dataset? I saw the patch keeping changing in object-level attacks but remaining unchanged in scene-level attacks.\n- Is the mask in Sensitivity Distribution Recognition shared by the whole dataset?\n- I do not understand the form of the proj_x in Eq. 6 and why it is necessary."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1194/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1194/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1194/Reviewer_DXNR"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1194/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698486626747,
        "cdate": 1698486626747,
        "tmdate": 1700632730933,
        "mdate": 1700632730933,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hMaycWLJmJ",
        "forum": "3VD4PNEt5q",
        "replyto": "3VD4PNEt5q",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1194/Reviewer_QA2X"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1194/Reviewer_QA2X"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new approach to attacking fusion models from the camera modality, which successfully compromises advanced camera-LiDAR fusion models and demonstrates the weaknesses of relying solely on fusion for defence against adversarial attacks. The proposed attack framework is based on a novel PointAug method that perturbs the point cloud data and generates realistic-looking adversarial examples. The experiments conducted in both simulated and physical-world environments show the practicality and effectiveness of the proposed approach."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.Sophisticated Camera-LiDAR Fusion Model: The proposed methodology exemplifies a commendable synthesis of advanced camera-LiDAR fusion models. This amalgamation not only taps into the inherent strengths of individual modalities but also crafts a synergistic fusion, ensuring that the combined system is more robust and efficient than its constituent parts in isolation.\n2.Efficacy of PointAug in Generating Adversarial Samples: An intrinsic highlight of the paper is the PointAug method, which adeptly fabricates realistic-looking adversarial examples. Such capability is pivotal, particularly in the realm of robust machine learning, as it enables researchers to thoroughly evaluate the resilience of models against potential adversarial threats.\n3.Rigorous Experimental Validation in Diverse Environments: The rigorousness and diversity of experiments set this work apart. By conducting evaluations in both simulated and real-world environments, the paper fortifies the assertion of the proposed approach's practicality and efficacy. This dual-pronged validation underscores the method's adaptability and reliability in a wide range of scenarios."
            },
            "weaknesses": {
                "value": "1.Limited Fusion Model Efficacy: The methodology, while promising, seems to be narrowly tailored for a specific set of fusion models. This raises concerns about its universality. An in-depth exploration into its effectiveness against a broader spectrum of fusion models would have provided a more comprehensive perspective, allowing for a holistic understanding of its potential and pitfalls.\n2.Data Dependency and Generalizability Concerns: The experiments, predominantly based on a circumscribed dataset, cast doubts on the model's capacity to generalize across diverse scenarios. The exclusive reliance on a limited dataset can inadvertently introduce biases, thereby undermining the robustness of the approach when deployed in novel, real-world situations.\n3.Inadequate Security Analysis and Potential Resource Constraints: While the paper delves into several aspects of the proposed approach, it seems to sidestep a comprehensive analysis of its security implications. Given the pivotal role of security in such contexts, a detailed discourse would have been invaluable. Furthermore, the potentially substantial computational overhead required to generate adversarial samples may render the approach untenable for resource-constrained environments. The paper's omission of a thorough dissection of its inherent limitations further obscures the potential challenges one might encounter in its adoption."
            },
            "questions": {
                "value": "1.Comparison with Pre-existing Methodologies: Given the emergence and evolution of methodologies targeting fusion models, how does the proposed approach position itself relative to these existing strategies? An analytical juxtaposition against established techniques would elucidate its uniqueness, advantages, and potential shortcomings.\n2.Defensive Countermeasures against the Approach: While the paper sheds light on an innovative adversarial approach, it begs the question: What are the viable defensive strategies that can be deployed to counteract its effects? Unveiling potential countermeasures not only underscores the resilience of the approach but also aids in the development of more robust fusion models.\n3.Extension and Scalability Concerns: Fusion models are diverse and multifaceted. How malleable is the proposed approach in its application to other fusion model variants? A deeper dive into its adaptability would provide insights into its scalability and flexibility across various fusion paradigms.\n4.Implications for Autonomous Vehicle Security: Given the pivotal role of fusion models in autonomous vehicle systems, the proposed adversarial approach inevitably raises safety and security concerns. How might these attacks compromise the integrity and reliability of autonomous driving systems? A comprehensive discussion on this would be crucial for stakeholders in the autonomous vehicle domain."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1194/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698738028785,
        "cdate": 1698738028785,
        "tmdate": 1699636045678,
        "mdate": 1699636045678,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rcMCkdVz9Y",
        "forum": "3VD4PNEt5q",
        "replyto": "3VD4PNEt5q",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1194/Reviewer_qqbx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1194/Reviewer_qqbx"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the vulnerability of multi-sensor fusion to adversarial attacks in autonomous driving. The authors propose to leverage the adversarial patch to attack the camera modality in 3D object detection. Specifically, they propose an attack framework employing a two-stage optimization-based strategy that first evaluates vulnerable image areas under adversarial attacks, and then applies dedicated attack strategies for different fusion models to generate deployable patches."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This paper studies an important concern in autonomous driving, i.e., the vulnerability of multi-sensor fusion to adversarial attacks.\n\n- Multiple feature-level fusion models are considered in this paper.\n\n- The performance of the proposed framework is evaluated through both simulated and real-world experiments."
            },
            "weaknesses": {
                "value": "- This paper does not provide the threat model. What information is available to the attacker during the attack? How feasible is it for the attacker to access this information in a real-world setting? What are the attacker's capabilities?\n\n- The practicality and generalizability of the proposed attack are limited due to its ineffectiveness on decision-level fusion models, which are widely used in many autonomous driving systems, such as Baidu Apollo. Although the proposed attack can alter camera inputs, it fails to affect the outputs of decision-level fusion models. Moreover, these models tend to depend more on LiDAR detection results, further diminishing the practicality of the proposed attack.\n\n- I found it hard to understand the positioning of this paper. There are many existing works studying the attacks against camera-LiDAR fusion models [1,2]. These methods can be used to attack all three types of sensor fusion models including data-level fusion, feature-level fusion, and decision-level fusion. However, the method proposed in this paper can only be used to attack the first two types of fusion models. So, what is the major advantage of this work compared to those existing works? The authors should compare their method with existing attacks to demonstrate superiority of the proposed attack. \n\n[1] Exploring adversarial robustness of multi-sensor perception systems in self driving.\n[2] Invisible for both camera and lidar: Security of multi-sensor fusion based perception in autonomous driving under physical-world attacks.\n\n- The practicability of the adopted adversarial patch is questionable. Table 7 shows that the minimum dimensions of the patch are 1 meter by 1 meter, and the patch is too large to be practical. How to place such a large patch on a pedestrian in the real world? In addition, it's impractical to place such a large patch on the back of a vehicle. The patch may hide the license plate of the vehicle, which is prohibited by the traffic law. \n\n- The real-world evaluation is weak. The authors propose to use a patch with a special color pattern to conduct the attack. Such a color pattern can be affected by many factors in the physical world such as light condition, the distance between the camera and the patch, as well as the view angle of the camera. However, the authors do not evaluate the impact of these factors in the real-world setting. \n\n- The impact of the proposed attack on the vehicle's motion remains unclear. Is the perception system of the vehicle consistently deceived by the attack? Is the vehicle's trajectory affected by the attack?"
            },
            "questions": {
                "value": "- What is the threat model. What information is available to the attacker during the attack? How feasible is it for the attacker to access this information in a real-world setting? What are the attacker's capabilities?\n\n- What is the major advantage of this work compared to existing attacks against camera-LiDAR fusion models?\n\n- How to place the adversarial patch on a pedestrian in the real world (as shown in Figure 10)? How to place the patch on the back of a vehicle? Does the patch hide the license plate of the vehicle?\n\n- Does the proposed attack maintain its effectiveness under varying light conditions in the real world?\n\n- The impact of the proposed attack on the vehicle's motion remains unclear. Is the perception system of the vehicle consistently deceived by the attack? Is the vehicle's trajectory affected by the attack?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Responsible research practice (e.g., human subjects, data release)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The experiment in this paper involves human subjects, but the authors do not report ethical approvals from an appropriate ethical review board."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1194/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698806947109,
        "cdate": 1698806947109,
        "tmdate": 1699636045608,
        "mdate": 1699636045608,
        "license": "CC BY 4.0",
        "version": 2
    }
]