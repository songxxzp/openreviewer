[
    {
        "id": "lJwlZm0trs",
        "forum": "0akLDTFR9x",
        "replyto": "0akLDTFR9x",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7601/Reviewer_Jmo8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7601/Reviewer_Jmo8"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the representation learning for goal-conditioned reinforcement learning problems. Built upon InfoNCE objective, this paper proposes a temporal difference estimator of InfoNCE objective and applied it to goal-conditioned RL algorithm.\n\nIn the experiment section, in the online RL setting, the proposed method is compared with prior goal-conditioned RL algorithms, including quasimetric RL, contrastive RL (Monte Carlo estimator of NCE objective), and hindsight experience relabelling. The proposed method achieved a higher average return in the comparison. Also, in the offline RL setting, The proposed method is compared with quasimetric RL, contrastive RL, and SOTA offline RL algorithms. The proposed algorithm generally outperforms baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper is well-written and easy to read. The proposed method is explained and presented clearly.\n\nThis paper provides clear derivation and a solid theoretical foundation for the proposed method in Section 3. I \n\nThe proposed method is supported by extensive experiments comparing with many baseline approaches.\n\nThe analysis in Section 4.3 and 4.4 validate the advantages of the proposed method in comparison with other representation learning approaches. It is impressive to see that the proposed method can stitch together pieces of data."
            },
            "weaknesses": {
                "value": "Some statements, especially some in the introduction part, seem not fully supported by evidence provided in the paper.\n\nFor example, it is claimed that the proposed method \"enables us to perform counterfactual reasoning\". However, this point is not clear in the following section. Could you please explain it in detail?"
            },
            "questions": {
                "value": "How is the proposed method sensitive to hyper-parameters? Do we need careful hyper-parameter tuning to make it work? Is there any intuitive guidance about how to adjust the hyper-parameters?\n\nIn Algorithm 1, many notations are introduced for the first time without any definition. Could you please clarify them?\n\nOne important baseline, contrastive RL, is the Monte Carlo estimator of the NCE loss. Could you please also compare with the algorithm using Monte Carlo estimator of InfoNCE loss, since it is already introduced in the prior work Eysenbach et al., 2022"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7601/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698294059203,
        "cdate": 1698294059203,
        "tmdate": 1699636921829,
        "mdate": 1699636921829,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Tn58GxBzxM",
        "forum": "0akLDTFR9x",
        "replyto": "0akLDTFR9x",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7601/Reviewer_a2XQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7601/Reviewer_a2XQ"
        ],
        "content": {
            "summary": {
                "value": "This paper extends previous works on contrastive RL by using a temporal difference estimator. Similar to contrastive RL, the paper proposes to use contrastive learning, InfoNCE in particular, for estimating the discounted state occupancy measure. Unlike the previous contrastive RL approach, which averages over the goal distribution, the proposed method uses a temporal difference estimator and results in a Bellman-like update rule (TDInfoNCE). Comparing the Bellman update for value function, TDInfoNCE requires taking expectation over the future states from a potentially different goal. It turns out that this can be estimated using importance weight. Then, the paper shows how the estimated state occupancy measure can be used in conjuction with goal-conditioned RL to form a full-fledged RL agent. Experimental results are shown on both online and offline settings and compared to a range of existing methods. In both settings, TDInfoNCE outperforms in most of the environments compared to previous approaches."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is mostly well written, apart from some details (see questions section). \n\nThe derivations are sound. \n\nExperimental results show strong performance comparing to previous methods. The paper also presents some analysis and insights to explain the performance."
            },
            "weaknesses": {
                "value": "The novelty is slightly limited. The idea of using InfoNCE to estimate the state occupancy measure has been presented in contrastive RL; the Bellman-like update and the use of importance weight has been presented in C-Learning."
            },
            "questions": {
                "value": "1. Questions regarding the algorithm:\n - It's not clear to me what's the goal distribution is used? Is it a random goal? Does the different goal distribution affect data efficiency?\n- How is $s_{t+}$ sampled? Is it the same as previous approaches - sample t from a geometric distribution?\n\n2. The notation in Figure 1 is not very clear to me. Is it suppose to visualize Eq. 4?\n\n3. In Eq. 7 and the one above, shoud it be $p^{\\pi}(s_{t+}^{(1)}|s^{'}, a^{'}, g)$ instead of  $p^{\\pi}(s_{t+}^{(1)}|s^{'}, a^{'})$?\n\n4. In Table 1, result for contrastive RL on large-diverse-v2 should not be bold; result for TDInfoNCE on unmaze-v2 should not be bold."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7601/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7601/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7601/Reviewer_a2XQ"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7601/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698766442062,
        "cdate": 1698766442062,
        "tmdate": 1699636921709,
        "mdate": 1699636921709,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cIhT3oTCV1",
        "forum": "0akLDTFR9x",
        "replyto": "0akLDTFR9x",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7601/Reviewer_2Mg8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7601/Reviewer_2Mg8"
        ],
        "content": {
            "summary": {
                "value": "This paper derives a TD variant of the InfoNCE objective function, relating this to some of the work in reinforcement learning using future distributions of state as an objective (i.e. successor representations/features). This algorithm is then applied to goal-conditioned reinforcement learning, showing competitive performance among several baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The derived method fits nicely within the literature and seems to fill a nice gap between contrastive objectives from self-supervised objectives and more online focused temporal-difference updates."
            },
            "weaknesses": {
                "value": "After the rebuttal, I'm raising my score. While I believe there are issues with empirical section still, these are issues the rest of the literature are also facing. I don't think rejecting this paper is a way to a solution. I also appreciate the fix to some of the inaccurate statements that were overlooked! \n\nGreat job authors!\n\n--------before edit-------\n\nThis paper struggles with clarity and accuracy in some of the ancillary statements made about the literature surrounding the paper and in the main content of the paper itself. There are also several issues with the experimental section that should be addressed.\n\n## Accuracy and Clarity:\n1. On page 1, in the paragraph starting with \u201cthe key aim\u2026\u201d, the language on motivating why a TD version of InfoNCE is oddly phrased. I think a fix should be easy here by removing phrases such as \u201cmay allow\u201d, \u201cmay enable\u201d and be more actionable in your language. This could also be replaced by actual hypotheses of what you expect to see from your objective and stated more formally.\n2. In the same paragraph, and further throughout, you make a statement which suggests TD can do counter factual reasoning (or in the parlance of RL make use of off-policy updates) while Monte Carlo estimates cannot.  This is not true as Monte Carlo estimates can be made with off-policy corrections (using Importance Sampling like in TD). While this induces a more variant estimate as compared to TD (there is a classic bias-variance trade-off between MC and TD updates) and a TD update enables the use of incomplete trajectories (because you are using an estimate to inform your update rather than a full trajectory), I came out of the paper with the feeling the paper was suggesting Monte Carlo estimates couldn\u2019t be off-policy. \n    - This comes up very strongly on page 5 (the first paragraph) \u201cThis is, we cannot share\u2026\u201d. We should be able to derive an off-policy version of the monte-carlo update for InfoNCE. This doesn\u2019t mean it would be an estimator we would want to use in this setting, but it should be definable. If this is not the case, the paper should show that this can\u2019t be done using importance sampling or cite a reference which shows monte carlo estimates can\u2019t be off-policy.\n\n3. **Notation clarity issues:** \n   - In your expectations above equation 7, I\u2019m not sure what s\u2019, a\u2019 are here. Are you using these instead of s_{t+1} as used in equation 4? This notation should be unified. \n   - Equation (1) and following uses, I\u2019m not sure you explain what the superscript is signifying. I think it is time, but it is not clear from the writing.\n   - Shouldn\u2019t the expectation in the middle of page 4 on the RHS (i.e. after applying the importance weight) be selecting actions a\u2019 from the behavior policy? Or is the importance weight only correcting for the state distribution? Shouldn\u2019t we also correct for the action distribution as well?\n\n\n\n## Empirical Results:\n\nThere are two major issues with the empirical results as presented. \n\n### Major issues:\n\n4. 3 seeds is too few to get any statistical confidence, especially without doing independent hyperparameter sweeps for each baseline. While in the past this has been standard, as a field we continually have shown that the statistical power of our experiments are laughably poor, even if the bounds of our results show statistical significance. This continually misleads the community, and needs to be addressed. This doesn\u2019t include the issue with not running hyperparameter studies on the methods independently. See https://sites.ualberta.ca/~amw8/cookbook.pdf for a reference. While this paper is a draft it does a good job going through these issues and providing context from the literature.\n\n5. In the appendix it is mentioned \u201cWe increase the capacity of the goal-conditioned state-action encoder\u2026\u201d. This suggests the model you are using may have more parameters than your counterparts. Is this true? Also did you use a larger batch size for all baselines or just your algorithm? If this was done for your method, this makes the results difficult to trust. If not, it likely means the hyperparameters of your baselines are now invalid. If both methods have the same hyperparameters then I\u2019m usually ok with re-using old hyperparameter studies, unfortunately when any of the hypers change OR the new method has additional hypers this begins to weaken the validity of re-using the same hypers.\n\n### Minor Issues:\n- You should include all hyperparameters you used, even for baselines. \n- You include only success rate as a metric to compare. While this is reasonable, I think there is a lot that could be learned from more traditional metrics (i.e. return or something similar). This is especially the case when the success metric doesn\u2019t clearly separate the methods (for instance reach, Push, slide (state) in firgure 2a). \n\n\n\n# Edits/Suggestions \n- I don\u2019t like the notation $s_{t+}$. I think it could be replaced with something that is more understandable on first read (and looks less like a mistake). This is a preference though.\n- Page 4: \u201cOur proposed method (Sec 3)\u2026\u201d Should say Sec 3.2.\n- Page 4: \u201cwe conjecture that our\u2026\u201d -> You should state that you test this in the empirical section (I think you do at least).\n- Page 7: \u201cWe will to evaluate\u201d -> \u201cwe will evaluate\u201d\n- If you reference a result in the main paper you should include this in the main paper. \u201cTD InfoNCE achieves a 2x median improvement etc\u2026\u201d references figure 6 (I believe)."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7601/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7601/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7601/Reviewer_2Mg8"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7601/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698789364159,
        "cdate": 1698789364159,
        "tmdate": 1700586181515,
        "mdate": 1700586181515,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7ITt49PRjA",
        "forum": "0akLDTFR9x",
        "replyto": "0akLDTFR9x",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7601/Reviewer_yDjK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7601/Reviewer_yDjK"
        ],
        "content": {
            "summary": {
                "value": "Predicting future states is crucial for many time-series tasks, including goal-conditioned reinforcement learning (RL). While contrastive predictive coding (CPC) has been used to model time series data, learning representations that capture long-term dependencies often requires large datasets. This paper introduces a temporal difference (TD) version of CPC that combines segments of different time series, reducing the data needed to predict future events. This representation learning method is applied to derive an off-policy algorithm for goal-conditioned RL. Experiments show that the proposed method achieves higher success rates with less data and better handles stochastic environments compared to previous RL methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper proposes a new temporal difference (TD) estimator for the InfoNCE loss, which is shown to be more efficient than the standard (Monte Carlo) estimator.\n\n- The proposed goal-conditioned reinforcement learning (RL) algorithm outperforms prior methods in both online and offline settings.\n\n- The proposed algorithm is capable of handling stochasticity in the environment dynamics.\n\n- In stochastic tasks, there is an excellent improvement in performance versus the baseline of Quasimetric RL, with some healthy gains on non stochastic tasks versus other baselines, although this is not the primary target of the paper.\n\n- The paper provides a clear and concise explanation of the proposed algorithm.\n\n- The paper is well-written and easy to understand.\n\n- The paper is well-supported by experiments.\n\n- The proposed algorithm is evaluated on a variety of tasks, including the Fetch robotics benchmark.\n\n- TD InfoNCE learns on image-based pick & place and slide, while baselines fail to make any progress.\n\n- TD InfoNCE maintains high success rates on more challenging tasks with observation corruption, while the performance of QRL decreases significantly."
            },
            "weaknesses": {
                "value": "- The paper focuses on fairly trivial environments, it would be nice to see these methods working on more challenging higher dimensional goal conditioned RL tasks, as its not a given that these gains will carry over to tasks that matter a lot more.\n\n- The proposed TD estimator is more complex than the standard (Monte Carlo) estimator and its implementation requires more hyperparameters.\n\n- The performance of the proposed goal-conditioned RL algorithm on the most challenging tasks was less than 50%.\n\n- QRL assumes deterministic dynamic of the environment, while TD InfoNCE learns without such assumption.\n\nLoss Function Composition: The loss function L(\u03b8) is composed of two cross-entropy (CE) loss terms, one for predicting the next state and one for predicting the future distribution of states. The \u03b3 hyperparameter is used to weight these two terms, but the choice of \u03b3 and its impact on the algorithm's performance are not discussed in detail."
            },
            "questions": {
                "value": "Can you explain how you selected the hyperparameters for the proposed algorithm?\n\nCan you provide more details about the observation that TD InfoNCE learns on image-based pick & place and slide, while baselines fail to make any progress?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7601/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699530774902,
        "cdate": 1699530774902,
        "tmdate": 1699636921449,
        "mdate": 1699636921449,
        "license": "CC BY 4.0",
        "version": 2
    }
]