[
    {
        "id": "3kBDX4HL9o",
        "forum": "N23A4ybMJr",
        "replyto": "N23A4ybMJr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5045/Reviewer_aJaX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5045/Reviewer_aJaX"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors tackle the challenges in training high-resolution vision transformers. ViTs have quadratic complexity with respect to the input resolution. Hence, reducing the number of tokens during training is helpful. To this end, the authors proposed Win-Win, an approach that optimizes token selection during training, balancing local and long-range interactions, and is adaptable to various computer vision tasks, including semantic segmentation and optical flow estimation. The Win-Win approach reduces training complexity by using a subset of tokens, denoted as P', during training. This involves masking (discarding) the remaining tokens. This reduces the training complexity from O(|P|^2) to O(|P'|^2), where the size of P' is much smaller than P."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well-written and easy to follow.\n\n- The proposed idea is interesting. The authors demonstrated that partial supervision at training time can also work for pixelwise prediction tasks in ViT. \n\n- Win-Win leads to measured training speedup for high-resolution ViTs.\n\n- Validation results on BDD100K and MPI-Sintel indicates that partial supervision at training time does not significantly impact the performance at test time."
            },
            "weaknesses": {
                "value": "- The evaluation appears to be somewhat constrained. High-resolution Vision Transformers (ViTs) possess a wide range of potential applications, including object detection, semantic segmentation, instance segmentation, and serving as a backbone in diffusion models. However, the authors have restricted their analysis to just two representative applications and a limited set of datasets. This limitation raises questions about the general applicability of the proposed method. Specifically, I am keen to ascertain the viability of applying this method to object detection and diffusion models.\n\n- The ablation study regarding window selection is currently lacking completeness. Notably, an essential baseline - random and non-contiguous window selection - has been omitted for semantic segmentation tasks. I am curious whether Win-Win works better than random masking in single-frame applications, though Table 8 demonstrated its effectiveness for optical flow prediction.\n\n- The paper brings to mind GridMask, a well-known data augmentation technique widely applicable in various vision-related tasks. It would be beneficial if the author could incorporate a discussion on GridMask, highlighting the similarities and differences with the Win-Win method. Furthermore, the paper lacks any discussion regarding inference acceleration techniques for Vision Transformers (ViTs) through token pruning, such as DynamicViT, A-ViT, IA-RED^2, and SparseViT. Incorporating a brief comparison or analysis of Win-Win in the context of these methods would enhance the completeness of the paper."
            },
            "questions": {
                "value": "Please respond to my concerns in the \"Weaknesses\" section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5045/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5045/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5045/Reviewer_aJaX"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5045/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698693457358,
        "cdate": 1698693457358,
        "tmdate": 1700678963567,
        "mdate": 1700678963567,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "47WSqG9Ji3",
        "forum": "N23A4ybMJr",
        "replyto": "N23A4ybMJr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5045/Reviewer_yL3d"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5045/Reviewer_yL3d"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the challenge of training vision transformers for high-resolution pixelwise tasks, which is computationally expensive. The authors propose a novel strategy called Win-Win, which involves masking out most of the high-resolution inputs during training and keeping only a few random windows~(2 windows are enough). The model can learn local and global interactions efficiently. The proposed strategy is faster to train, straightforward to use at test time, and achieves comparable performance on semantic segmentation and optical flow tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Easy to implement: The method is simple and easy to integrate into the dense prediction tasks like semantic segmentation and binocular tasks like optical flow estimation.\n2. Efficient Training: The Win-Win strategy reduces training time by a factor of 4 compared to full-resolution networks. It achieves this by focusing on random windows instead of processing the entire high-resolution input."
            },
            "weaknesses": {
                "value": "1. Lack of Comparative Analysis: It does not provide a comprehensive comparative analysis with a wide range of existing methods for training high-resolution vision transformers.\n2. Lack of Novelty: Masking out image patches is not a new approach in the literature, and it is simple data augmentation tuning to mask out most of them. I highly recommend the authors to do in-depth exploration and analysis of this method."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5045/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5045/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5045/Reviewer_yL3d"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5045/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698758291852,
        "cdate": 1698758291852,
        "tmdate": 1700704394818,
        "mdate": 1700704394818,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "55y2QGe36e",
        "forum": "N23A4ybMJr",
        "replyto": "N23A4ybMJr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5045/Reviewer_Jtip"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5045/Reviewer_Jtip"
        ],
        "content": {
            "summary": {
                "value": "The authors introduce a novel strategy for efficient high-resolution vision transformer training and inference. This approach involves masking most high-resolution inputs during training, retaining only N random windows. It enables the model to learn local and global token interactions, allowing direct processing of high-resolution input during testing without special techniques. Notably, this method is four times faster to train than full-resolution networks and straightforward to implement during testing. The authors apply this approach to semantic segmentation and optical flow tasks, achieving state-of-the-art performance on the Spring benchmark with significantly reduced inference times"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Strength:\n1. The proposed model provides competitive performance while reducing the training time\n2. The proposed model can be generalized and applied to various tasks like segmentation and binocular task of optical flow.\n3. It is easy to apply the proposed strategy."
            },
            "weaknesses": {
                "value": "1. The paper presents semantic segmentation result for a single train and test resolution (1280x720). Does the performance hold for the solution exceeding 1280x720?\n2. Win-Win is better than ViT-Det by a mere 0.3%, suggesting a marginal enhancement. Can 0.3% deemed as a substantial improvement?"
            },
            "questions": {
                "value": "Please refer to the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5045/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5045/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5045/Reviewer_Jtip"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5045/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698882987720,
        "cdate": 1698882987720,
        "tmdate": 1699636494099,
        "mdate": 1699636494099,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vYUqP9jRm7",
        "forum": "N23A4ybMJr",
        "replyto": "N23A4ybMJr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5045/Reviewer_wXRr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5045/Reviewer_wXRr"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a ViT training strategy that allows plain-ViT to train with high-resolution images with masking, and inference with high-resolution images with a single forward. Specifically, it proposes to keep only tokens from two randomly selected non-overlapping windows at training time for ViT to process. The paper empirically shows the proposed way (win-win) of training ViT achieves better training and inference cost trade-off with comparable performance of training on full-resolution. Experiments are conducted on both monocular task of semantic segmentation on BDD-100k and binocular task of option flow estimation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposes method enables a simple single forward inference process on high-resolution images without performance drop. In comparison to existing approach, most requires extra effort of aligning train and test resolution difference, e.g. aggregating predictions from multiple small patches. \t\n\nThe ablation study regarding the window generation strategy is quite extensive, including various ways of choosing windows, how many windows, window size, square or non-square window, etc. \n\nThe paper also extends the approach to optical flow, which requires more specific design of choosing window as two images involved, and the experiments results listed shows its potentials compared to other methods."
            },
            "weaknesses": {
                "value": "While the extensive experiments show the two window strategy is the best one with its simple and good performance, some analysis of such strategy/experiment results is missing. For example, in Table 1 right, why using 1021 tokens will drop 0.6 performance to using 1009 is not clear. \n\nThe results from Table 3 indicates the select of window strategy affects a lot for optical flow task, which suggests difficulties of generating to other data/task (search of window is needed)."
            },
            "questions": {
                "value": "1.\tFor results in Table 1, row 1 (968 tokens) compare with row 2 (988), using non-square window has a large drop of 1 mIOU, do authors have any insights on that? And for row 5 (1021 tokens), why a 0.6 mIOU drop compared to 1009 tokens, it is because of min window size? \n2.\tTable  3 shows larger variance when using different window strategy compared to table 1, any insights for this? \n3.\tFrom Table 3, the best setting is 2 win. 10x10, 4 win. 7x7, then as state \u2018using the best settings found previously), to eval on MPI-Sintel\u2019, however, the setting used have different window size, how did the author choose the window size on MPI-Sintel? Does this mean each time, a search of window size is needed when applying to different data?\n4.\tIn table 5, compared to CroCoFlow, which is also the paper\u2019s base method, it has a performance drop, authors states using different backbone (vit-l v.s. vit-base), the comparison of using the same backbone is missing."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5045/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698902219552,
        "cdate": 1698902219552,
        "tmdate": 1699636493860,
        "mdate": 1699636493860,
        "license": "CC BY 4.0",
        "version": 2
    }
]