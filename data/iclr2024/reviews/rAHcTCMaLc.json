[
    {
        "id": "o8QuK9iJ7d",
        "forum": "rAHcTCMaLc",
        "replyto": "rAHcTCMaLc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5997/Reviewer_FpMW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5997/Reviewer_FpMW"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a new algorithm for MaxEnt RL that can find a policy with better entropy. The main benefit comes from an explicit evaluation of the entropy term thanks to the closed-form entropy of the based measure and invertiability of the transformation. Experimental results demonstrate the effectiveness of the proposed algorithm on finding maximum entropy policy."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The presentation is easy to follow.\n* The demonstration can clearly show the differences between SQL, SAC and the proposed method.\n* The computation for the invertibility of SVGD can be of independent interest."
            },
            "weaknesses": {
                "value": "* I believe that the authors missed an important related work [1]. There are lots of similarities between the proposed method and [1], e.g. using SVGD to approximate the energy-based policy parameterized by the $Q$-function. In fact, if [1] does not amortize the policy, I believe it is nearly identical to the proposed methods.\n\n* I feel there are several modifications that make the proposed method alleviating from the concept of SVGD, e.g. the truncation of the particle update. Furthermore, as there will be some discretization error from the gradient flow of KL on Wasserstein space (which is the motivation of SVGD), I'm thinking if the entropy of the discretized SVGD is a proper estimate of the KL on energy-based policy. I'm also wondering that given the $Q$-function, is it possible to directly estimate the entropy of the energy-based policy, which potentially gets rid of the other issues.\n\n* The experimental results in fact do not beat the baseline a lot. \n\n[1] Liu, Yang, et al. \"Stein variational policy gradient.\" 33rd Conference on Uncertainty in Artificial Intelligence, UAI 2017"
            },
            "questions": {
                "value": "* The relationship between the proposed methods and the references I mentioned in the weakness part.\n* Is it possible to directly estimate the entropy of the energy-based policy?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5997/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698637361458,
        "cdate": 1698637361458,
        "tmdate": 1699636642823,
        "mdate": 1699636642823,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ABVPi1kFtD",
        "forum": "rAHcTCMaLc",
        "replyto": "rAHcTCMaLc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5997/Reviewer_4Fze"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5997/Reviewer_4Fze"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new algorithm, named Stein Soft Actor Critic (S$^2$AC), for policy learning with entropy regularization. The goal is to encourage sufficient reward maximization as well as a reasonable coverage of actions, i.e., return a stochastic policy instead of greedy deterministic one. The algorithm is related to existing methods like SAC and SQL algorithms, however, with improved performance demonstrated in empirical results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is mostly well written and easy to follow. The graphical demonstrations help a lot in understanding key concepts. To my knowledge, the proposed algorithm is new, and the empirical results corroborate the improved performance.\n\nI like the idea behind the S$^2$AC algorithm, which is not complicated but to the point."
            },
            "weaknesses": {
                "value": "1. Technical issues\n\nEquation (2) seems to be inconsistent with the setting in SQL, as there is no discount factor in Equation (2). Is this a typo (as well as in Equation (3))?\n\nTheorem 3.1 is not rigorous. My main concern is around the condition $\\epsilon \\lVert \\nabla h \\rVert_{\\infty} \\ll 1$. First, this is not a precise statement. Second, under this condition, Equation (10) cannot hold with equality (as already displayed in the appendix, there is an approximately equal argument). My suggestion is to either make a precise statement articulating the relationship between the condition and the final assertion to respect the rigor of a theorem, or make it casual by changing it to be a claim with approximate equality. Afterall, this theorem serves as a benign consequence of preserving the invertibility and as a motivation to properly choose $\\sigma$.\n\nIn Proposition 3.2, it is better to recall the definition of $\\sigma$, as there is an ambiguity of $\\sigma$ being the variance of Gaussian or kernel function.\n\n2. Claims in empirical results\n\nI am not confident in the claim that \"This empirically confirms that SGLD, DLD, and HMC update rules are not invertible\" (beginning of page 7) can be obtained from the entropy is not accurately estimated. Can authors provide more context around it? Moreover, why the target distribution is chosen so, as the mean and covariance are not natural (mean $[-0.69, 0.8]$, and variance ...).\n\nI would suggest to avoid the claim of \"maximizing the future reward and maximizing the future entropy\". The objective in Equation (2) only maximizes the sum of the future reward and future entropy, which is not to maximize both terms."
            },
            "questions": {
                "value": "Please see the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5997/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698800136403,
        "cdate": 1698800136403,
        "tmdate": 1699636642714,
        "mdate": 1699636642714,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vGf1Wv8U0Q",
        "forum": "rAHcTCMaLc",
        "replyto": "rAHcTCMaLc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5997/Reviewer_4hdR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5997/Reviewer_4hdR"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a method that combines soft actor-critic and Stein variational gradient descent to solve the reinforcement learning problem with continuous state and action space. The critic is a common Q-learning method. For the actor, the authors learn an initial distribution for the action that is close to the desired one, in order to improve the sufficiency of the algorithm. The authors provided detailed numerical tests to validate their algorithm."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The combination of soft actor-critic and Stein variational gradient descent is a good idea. The presentation of the is clear. The numerical results are comprehensive."
            },
            "weaknesses": {
                "value": "Certain sections of the article, particularly the basic setting and theorems, are somewhat unclear and could benefit from additional clarification. A more rigorous treatment from the authors in these areas would greatly enhance the overall quality of the paper. Further details can be found in the \"Questions\" section."
            },
            "questions": {
                "value": "1.\tPage 3. The notation \\rho_\\pi. Is it the stationary distribution of the state under \\pi, or the state distribution s_t, which depends on the initial distribution? If it is the former, please clarify. If it is the latter, then \\rho_\\pi should depend on t.\n2.\tPage 3 basic setting. It seems that the authors are considering an RL problem with infinite horizon and without discount. With such a setting, it is not a trivial issue to guarantee the total reward is finite. However, the authors make no comments on this issue.\n3.\tPage 3 eqn (3). The equivalence between MaxEnt RL and (3) is not clear to me. Can you also provide a short proof in the appendix? It is also surprising to me that (3) does not involve \\alpha. (Is there any typo here?) \n4.\tPage 3 eqn (4). -\\alpha should be +\\alpha?\n5.\tPage 4 bottom. In the change of variable formula, there should be an epsilon after I+?\n6.\tPage 5 equation (9). The motivation for the actor update is to minimize the KL divergence between the policy distribution and EBM of Q-values, which is clear to me. But it is not obvious that minimizing this KL divergence is equivalent to (9). Can you also add a short proof in the appendix. Also, I think \\mathcal{D} is used without definition. Q(a|s) looks like a typo, is it Q(a,s) or q(a|s)?\n7.\tPage 5 theorem 3.1. Eqn (10) should not be equal, but an approximation. Maybe it is better to add the order of the approximation error. I think it is O(\\epsilon^2 * L), provided sufficient regularity.\n8.\tPage 6. Prop 3.5 says HMC is not invertible, but the following paragraph says, \u201cWhile the HMC update is invertible\u201d. Is it a contradiction?\n9.\tRelated work. I think the paper \u201cSingle Timescale Actor-Critic Method to Solve the Linear Quadratic Regulator with Convergence Guarantees\u201d (JMLR 2023) could also be added to the related work. The LQR setting also has continuous state and action space, and the actor is a soft policy.\n10.\tPage 17. The first \\epsilon should not appear in the second last line."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5997/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5997/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5997/Reviewer_4hdR"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5997/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699032955037,
        "cdate": 1699032955037,
        "tmdate": 1699636642618,
        "mdate": 1699636642618,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1Roarewq4h",
        "forum": "rAHcTCMaLc",
        "replyto": "rAHcTCMaLc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5997/Reviewer_4Btw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5997/Reviewer_4Btw"
        ],
        "content": {
            "summary": {
                "value": "This paper argues that in MaxEnt RL, when policy is based on EBMs, estimating the entropy of policies could be an issue. The authors discussed related work, such as SQL and SAC, which have issues to not learn optimal regularized policies. They then proposed S2AC, which learns a more optimal solution to the MaxEnt RL objective. This is achieved by modelling the policy as a Stein Variational Gradient Descent (SVGD) sampler. They show that both SQL and SAC can be recovered with small modifications over S2AC. The authors then conducted experiments to verify the capability of S2AC in estimating entropy, in learning multimodal policies and maximizing entropy, as well as its performance in MuJoCo tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Estimating entropy in learning MaxEnt RL is a quite interesting question and the authors showed that this actually matters.\n2. The idea of using the invertibility of samplers and methods are novel to my knowledge.\n3. The experimental results are promising, and they support the proposed methods.\n4. The backbone of S2AC, i.e., the new variational inference algorithm, could be of independent interest.\n5. The paper is well written. I can easily follow the logic."
            },
            "weaknesses": {
                "value": "I understand that the paper mostly focuses on discussions with SQL and SAC. However, from reading the paper it seems calculating/estimating entropy of high dimensional policies is an essential part. It could be worth discussing existing literature on entropy estimation and compare the proposed estimation method with them in terms of accurately estimating entropy and computational efficiency."
            },
            "questions": {
                "value": "1. Does the result apply to other regularizers other than standard entropy?\n2. There are also variants of SAC for discrete settings. Do you think the S2AC would benefit in those scenarios?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5997/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699112179941,
        "cdate": 1699112179941,
        "tmdate": 1699636642517,
        "mdate": 1699636642517,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YGCPYaHsqA",
        "forum": "rAHcTCMaLc",
        "replyto": "rAHcTCMaLc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5997/Reviewer_ThdK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5997/Reviewer_ThdK"
        ],
        "content": {
            "summary": {
                "value": "The author present an algorithm to use steins gradient descent to learn stochastic policies using soft-actor critic. The specific advantage is to not rely on variational approaches  (such as Gaussian-like) for the stochasticity of the policy."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well written\n- Methodology appears sound and novel\n- Related literature is checked and carefully introduced  (e.g. Relations to SQL and SAC) \n- Design choices (such as Parameterized initialization and amortized inference) are valid and thoroughly discussed\n- Theoretical insights, such as a closed form expression of the entropy and invertibility of SVGD  are given"
            },
            "weaknesses": {
                "value": "Significance:  My biggest reservation for acceptance is significance.  How much does the increased expressiveness of the stochastic policy  (at the cost of computation) matter? The authors could only show significant differences to existing methods on hand-crafted toy problems. For the traditional benchmarks, i.e. Mujoco the method does not appear to perform better than standard approaches such as PPO. The problem here, is not the fact that it not performs better, but that it appears better experiments could be designed to show how large/if there is a significant advantage.\n\nThe general arguments for stochastic policies are, to my knowledge e.g.  \"multi-modality also has application in real robot tasks, as demonstrated in (Daniel et al., 2012) Quote taken from SAC paper. Additionally, as the authors write themselves: \"this enables better explo-\nration during training and eventually better robustness to environmental perturbations at test time, i.e., the agent learns multimodal action space distributions which enables picking the next best action in case a perturbation prevents the execution of the optimal one.\"\n\nIf that is the argumentation I would expect the authors to design experiments under these scenarios that are not toy.  This could have been achieved for instance by  augmenting the MuJoCo tasks with perturbation events that prevent execution of certain actions in certain situations. Then a more expressive stochastic policy could perform better.  For now it makes it hard to asses if the increase time- and algorithmic complexity is worth the cost of obtaining better performing policies."
            },
            "questions": {
                "value": "- How does the armotized version of the method perform on all benchmarks?  Figure 9 (right) only shows one performance curve with no indication which benchmark it is\n- Because the armotized version is much faster and performs similarly I wonder why it is not the main method, but it details are hidden in the appendix. \n- Also I would be curious how this version performs on the toy problems  (Fig. 5-7)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5997/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699113068871,
        "cdate": 1699113068871,
        "tmdate": 1699636642413,
        "mdate": 1699636642413,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Iow2XnRqgi",
        "forum": "rAHcTCMaLc",
        "replyto": "rAHcTCMaLc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5997/Reviewer_1y8M"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5997/Reviewer_1y8M"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel maximum entropy reinforcement learning algorithm that leverages Stein Variational Gradient Descent (SVGD) that allows the analytic computation of entropy."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The exposition of the paper is clear. The paper explicitly presents its problem, and the proposed solution explicitly addresses the problem.\n* The proposed method is theoretically sound and enjoys interesting connections to existing algorithms.\n* The paper's claims are well supported by the toy experiments. Experiment results on Mujoco environment seem promising."
            },
            "weaknesses": {
                "value": "* The main limitation seems to be the slow inference of SVGD. However, the paper argues that this limitation can be addressed by amortization. It would be better if the main manuscript provided more details on how amortization is performed.\n* It would be great if the paper mentions the scalability of SVGD with respect to dimensionality. How large the method can be scaled in terms of the dimensionality of the problem? Scaling SVGD to higher dimensional spaces seems to be challenging because we need exponentially more particles to represent a distribution."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 6,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5997/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699262282281,
        "cdate": 1699262282281,
        "tmdate": 1699636642271,
        "mdate": 1699636642271,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Z717eQAvH7",
        "forum": "rAHcTCMaLc",
        "replyto": "rAHcTCMaLc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5997/Reviewer_mSs2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5997/Reviewer_mSs2"
        ],
        "content": {
            "summary": {
                "value": "This paper deals with creating an SAC like algorithm, but while allowing\nmultimodality of the policy. This is achieved by using a Stein Variational\nGradient Descent sampler. A neural network parameterizes the mean and variance\nof the initial distribution of the sampler. Moreover, the samples produced\nby the SVGD method are restricted to lie within 3 standard deviations of the\ninitial distribution. The policies entropy can be computed thanks to the\nSVGD sampling process being invertible, similar to how a normalizing flow\nworks.\n\nThere are experiments to check the correctness of the formulation in simple 2D landscape fitting tasks. Moreover, they perform experiments\non 5 MuJoCo benchmark tasks. The performance seems similar to SAC in\n4/5 tasks, and better in one of the tasks.\n\n**Update**\n____________________________\n\nThanks for the extensive update.\n\nMany things improved, but I also still have concerns.\n\nMainly, the rebuttal claims: \"SSPG ... Specifically, the middle plot in Fig. 4(A) is an identical setup to the Multi-goal environment in Fig. 2 in our paper.\" But this does not seem to be true. The task in the SSPG is a bandit problem with only 1 environment step (and the action selects the location on the landscape); hence, there is no \"future entropy\" as there is just one step in the environment. Whereas in the current paper, there is an agent that moves around on the landscape, so the tasks are different. I see no reason why SSPG would also not be able to maximize the future entropy, as it uses a soft critic that includes the future entropy similarly to this paper.\n\nAnother concern is that your re-implementation of SAC-NF does not improve over SAC, whereas it did in the references, both in the SAC and REDQ cases, so perhaps your implementation is not well-performing.\n\nWhile the use of the rliable library was a good step to improve the analysis, there is a fairly large overlap in the error bars for SAC and your newly proposed method. As the number of random number seed experiments per environment was 5, I think this should be increased to reduce the error bars and receive a more reliable result.\n\nBased on the above, I decided to keep my score. What may have changed my assessment would have been performing more experiments so that the statistical significance of the results is clear (this would have increased my score to 5), not making claims that the setup is the same as the one in SSPG, better results for SAC-NF, etc. However, I will increase the contribution rating by 1 point, as I think there are also other contributions in the work.\n\n[3] SSPG: \"Policy Gradient With Serial Markov Chain Reasoning\""
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The correctness of the method is properly checked with experiments."
            },
            "weaknesses": {
                "value": "- There are many works looking at multimodality in MaxEnt RL that were\nnot discussed or cited. Moreover several of these obtain better\nresults than the current work (although they include additional features).\nFor example one earlier work is \"Boosting trust region policy optimization with\nnormalizing flows policy\" (https://arxiv.org/pdf/1809.10326.pdf). Their equation\n(5) seems similar to equations (10) and (11) in the current paper (it's more\nclear from the proof sketch). Moreover, there are the recent works:\n\"Reparameterized Policy Learning for Multimodal Trajectory Optimization\"\n(https://arxiv.org/pdf/2307.10710.pdf), although it is model-based, and\nalso the work: \"Policy Gradient With Serial Markov Chain Reasoning\"\n(https://openreview.net/forum?id=5VHK0q6Oo4M) that is model-free and obtains\ngood performance (it's also based on the MaxEnt principle); this work\nalso included a comparison with a normalizing flow in their appendix,\nand showed that their method achieves better performance; the experiments\nin this work were also more substantial than the current work.\nFinally, there are some other works that can be found in the related\nworks section of this paper: \"Leveraging exploration in off-policy algorithms via\nnormalizing flows\" (https://arxiv.org/pdf/1905.06893.pdf),\n\"Iterative Amortized Policy Optimization\" (https://arxiv.org/pdf/2010.10670.pdf).\nNone of these works were cited, so I think the literature review was insufficient\n(they cite many works, but miss the most relevant ones).\n\n- The experimental results are not very strong. The improvement over\nSAC is marginal. The other works I referenced above have more substantial experiments,\nand show greater improvements."
            },
            "questions": {
                "value": "How does your method compare to other methods that create multi-modal\npolicies in RL? Please include a comparison.\n\nHow is it related to normalizing flows?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5997/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5997/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5997/Reviewer_mSs2"
                ]
            }
        },
        "number": 7,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5997/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699283632520,
        "cdate": 1699283632520,
        "tmdate": 1700780145406,
        "mdate": 1700780145406,
        "license": "CC BY 4.0",
        "version": 2
    }
]