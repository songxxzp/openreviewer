[
    {
        "id": "AbcquhHjP4",
        "forum": "w4abltTZ2f",
        "replyto": "w4abltTZ2f",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6581/Reviewer_daKT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6581/Reviewer_daKT"
        ],
        "content": {
            "summary": {
                "value": "LoRA, a widely used technique for fine-tuning a small number of parameters in foundation models, exhibits a weakness in batched inference settings where each request in the batch requires a unique adapter.\nIn such a scenario, batched inference using LoRA becomes sequential and inefficient. This paper proposes a variant of LoRA, called fast LoRA (FLoRA), which utilizes a parameterization that enables minibatch computations to be performed using matrix multiplications. This makes it efficient to perform batched inferences with distinct adapters per request. \nThe paper presents a computational analysis demonstrating that FLoRA can achieve improvements in both throughput and latency compared to LoRA for scenarios involving low-rank and small model dimensions. \nThe paper presents empirical results demonstrating the advantages of FLoRA over LoRA when using StarCoder (Li et al., 2023) as the foundation model. On multilingual code generation and speech recognition tasks, FLoRA achieves similar performance to LoRA and outperforms IA3."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "* Proposes an alternative to the LoRA approach that is efficient for batched inference with distinct adapters per request.\n* Presents an analysis demonstrating the conditions under which the proposed approach can outperform LoRA.\n* Demonstrates using the StarCoder 15B LLM that FLoRA can double the throughput (halve the latency) in a low-rank setting when diverse adapters are required for incoming examples.\n* Shows that FLoRA yields similar results as LoRA on multilingual code generation and speech recognition tasks."
            },
            "weaknesses": {
                "value": "* Some parts of the paper are not clear (see comments below)."
            },
            "questions": {
                "value": "* The transition from Eqn 4 to 5 is not immediately clear. It would be helpful to provide intermediate steps.\n* P5: In the sentence, \"Secondly, in configurations where the model has fewer hidden units but an increased number of layers, FLORA tends to outperform LORA due to the smaller value of d in the denominator of Eq. (7).\" How is the increased number of layers important given that Eq (7) contains only the dimensionality of the hidden units d and the rank r?\n* Table 2: Is there any reason why FLoRA underperforms LoRA for Marathi? What is the amount of fine-tuning data for each language?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6581/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6581/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6581/Reviewer_daKT"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6581/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698205541573,
        "cdate": 1698205541573,
        "tmdate": 1699636747949,
        "mdate": 1699636747949,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AU8D0GsfC0",
        "forum": "w4abltTZ2f",
        "replyto": "w4abltTZ2f",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6581/Reviewer_46uC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6581/Reviewer_46uC"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes FLoRA, which allows each example in a minibatch to own unique low-rank adapters. FLoRA encourages efficient batching of serving various requests, retaining performances of LoRA with throughput improvement and latency reduction in low-rank settings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The orientation is clear. It can important to equip language models with various task-specific adapters for diverse requests. The overall idea is well-motivated.\n2. The formulation is clear and analysis of computational consumption is in detailed."
            },
            "weaknesses": {
                "value": "1. If each example in a minibatch has its own adapters, the overall performance is expected to overcome LoRA, however, it's almost the same as LoRA. So the \"performance bottleneck in scenarios requiring personalized, task-specific adaptations for each incoming request\" isn't largely solved.\n2. The whole mechanism and the algorithm isn't mentioned clearly. e.g., how to choose the batch size for real situations, how to make each example corresponding to its appropriate adapters during inference. The paper over-concentrates on Fomulation and Computational Efficiency, while the high-level algorithm--the whole process is not quite clear."
            },
            "questions": {
                "value": "1. What's the memory comsumption of FLoRA compared with other methods?\n2. Can you further explain \"FLORA has the same expressive power as LORA by its construction\"?\n3. The reason for changing \"addition\" of low-rank adapters in LoRA to \"multiplication\" in FLoRA is only for computational efficiency or for something else?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6581/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6581/Reviewer_46uC",
                    "ICLR.cc/2024/Conference/Submission6581/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6581/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698752550859,
        "cdate": 1698752550859,
        "tmdate": 1700545188195,
        "mdate": 1700545188195,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MGcwmpHPFx",
        "forum": "w4abltTZ2f",
        "replyto": "w4abltTZ2f",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6581/Reviewer_Uf9f"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6581/Reviewer_Uf9f"
        ],
        "content": {
            "summary": {
                "value": "The paper builds up on the Low-Rank Adaptation (LoRA) framework to fine-tune foundation models, by introducing fLoRA, which allows distinct adapters for different task-specific requests within the same batch. The authors empirically demonstrate that their approach preserves the advantages of LoRA in terms of accuracy on multilingual code generation and speech recognition tasks, while facilitating a higher throughput and lower latency."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper clearly introduces the problem and the contributions compared to the state of the art. The contribution is significant to cope with practical challenges of using foundation models in real-time serving scenarios, especially when considering world-wide incoming requests.\nThe paper looks theoretically and technically sound and the presentation is clear, well framed in the context, and easy to follow."
            },
            "weaknesses": {
                "value": "I don\u2019t find major weaknesses. Minor comments are indicated in the following section."
            },
            "questions": {
                "value": "-\tI suggest removing references from the abstract.\n-\tCould you explicitly clarify the definition of \u201cexpressive power\u201d in the paper? \n-\tAbout contribution 3 (Introduction): since fLoRA allows task-specific adapters for fine-tuning, wouldn\u2019t you expect a higher, rather than simply equivalent, accuracy compared to fLoRA? In which scenarios do you expect that fLoRA could have sacrificed accuracy compared to LoRA?\n-\tFig 1: The figure is useful, but framing the different sections (1,2,3,4), or at least avoid overlapping among them would help clarity. Also, 4 task descriptions are indicated at point 1, and the corresponding 4 results are shown at point 4, while only 2 adapters and weights computations are shown at point 2 and 3. In my view, it would be clearer to keep the number of examples consistent across the sub-figures.\n-\tIn LoRA the weight matrix of the adapted foundation model is expressed by the SUM of W0 and DeltaW, while in fLoRA the weight matrix specific for each example is calculated as the element-wise MULTIPLICATION of W0 and DeltaWi. Is this correct?\n-\tOn paragraph 3.2 you say that \u201cfLoRA exhibits a lower computational cost than bmm LoRA whenever the above inequality holds true\u201d. Could you elaborate more about scenarios when you expect (7) to be lower than 1?\n-\tPlease insert references to Table 1 and 2 when you comment results in Section 4.\n-\tTable 1: I suggest to highlight (e.g. bold text) the best improvement for each row.\n-\tI would move Section 5 (Related work) after the Introduction, since it provides some useful context to the presented approach."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6581/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6581/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6581/Reviewer_Uf9f"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6581/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698770631088,
        "cdate": 1698770631088,
        "tmdate": 1699636747709,
        "mdate": 1699636747709,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dKuIytUycX",
        "forum": "w4abltTZ2f",
        "replyto": "w4abltTZ2f",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6581/Reviewer_1RDh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6581/Reviewer_1RDh"
        ],
        "content": {
            "summary": {
                "value": "The paper propose a new low rank adaptation technique based on a generalization of IA3.\nEssentially the adaption changes from LORA: W = W0 + BA to FLORA: W = W0.*BA\nThis allows to pack in a batch many different adaptors per input or even per chunk efficiently."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper presents several strong points.\nThe proposed approach improves latency and throughput as well as a theoretical cost estimation.\nSeveral model sizes from starCorder and LLama 2 are considered for throughput and latency estimation.\nThe accuracy of the proposed method is similar or better to that of LORA and IA3 and report improvements/checks on several models such as Llama2, whisper or starCoder."
            },
            "weaknesses": {
                "value": "The approach requires re-adapting the models that have already been adapted with LORA to leverage the improvements.\nThere is a breaking point where FLORA doesn't improve over LORA effectively. Intuitively, there is at least 4 factors for this: the model, the gpu architecture, the rank of the adaptation and the batch size . The rank is taken into account but it is not very clear how the other elements will come into play in practice. Eq 7 claims only important factors are the dimension of the multiplication the constants for MM and BMM and the rank. However, it is difficult to understand why this should be the case for batch size 1 in contrast to a larger batch size.\nComputing some plots in this area would have been very helpful to grasp how the theoretical analysis transfer to the practical scenarios..\nAnother example of this would be computing per token and example adapters , which is the extreme case. It would have been interesting for latency and throughput curves to see such an extreme case, even though there is no such a real task. \nThe section 3.1 is confusing in its current form and a rewrite paying attention to the Matrix and elementwise operations would improve readability. \nGiven the constrains of the approach regarding the low-rank dimension, the applicability of the approach is limited to some specific scenarios which could have already been taken care on the base LLM pretraining. For instance, for the multilingual case the models could have already specific sparsely activated components given the language category or the programing language from the beginning."
            },
            "questions": {
                "value": "How does the batch size affects the improvements of the proposed FLORA ? \nHow does the picture change if we use per token and per batch adapter ?\nWhich other scenarios are the authors considering further from fixing lack of conditional inputs on the models ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6581/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698889255742,
        "cdate": 1698889255742,
        "tmdate": 1699636747528,
        "mdate": 1699636747528,
        "license": "CC BY 4.0",
        "version": 2
    }
]