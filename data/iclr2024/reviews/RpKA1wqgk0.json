[
    {
        "id": "349NJ7uaZu",
        "forum": "RpKA1wqgk0",
        "replyto": "RpKA1wqgk0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9345/Reviewer_xXZf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9345/Reviewer_xXZf"
        ],
        "content": {
            "summary": {
                "value": "A coherent and lightweight framework MetaFormer is proposed to improve the Vision Transformer performance in meta-learning. The framework contains a sample-level attention module (SAM) and a task-level attention module (TAM). The SAM enables the consistency of attended features across samples in a task whilst the TAM regularizes the learning of features for the current task by attending to a specific task in the pool. Extensive experiments are conducted on four commonly used FSL benchmark datasets and the new SOTA is achieved with a marginal increase in computational cost."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "-- The paper adds inter-sample and inter-task attention modules to the original Vision Transformer for meta-learning. These ideas are not novel whilst the implementation of them in such a way is somewhat novel.\n\n-- The presentation is generally good but lacks clarity for some details (see weaknesses).\n\n-- Thorough experiments on the in-domain and cross-domain settings provide valuable references to the community of FSL."
            },
            "weaknesses": {
                "value": "-- The authors use the terms context and target sets instead of support and query sets which are usually employed in FSL literature. This makes the presentation harder to understand.\n\n-- The main concern is the use of test samples during learning which makes it a transductive learning method. It might be that I misunderstand the training details of the model but it is unclear to me if M unlabelled query samples are used in any way before final prediction using Eq(7).\n\n-- The autoregressive inference setting employs the information from the test data. It is unfair to compare with those under the true inductive learning setting.\n\n-- It is unclear what properties of the learned task embedding have. A suggestion would be to analyze the task embedding space somehow to give intuitive insights for better understanding. For example, one can sample N tasks from the meta-testing data and M tasks from the meta-training data and compute the NxM distance matrix and take a closer look at some exemplar rows/columns to see what kind of tasks are similar in the learned task embedding space."
            },
            "questions": {
                "value": "1. In tables 1 and 2,  why are the bottom two groups of methods separated whilst they use the same backbone ViT-Small?\n\n2. If I misunderstand the method regarding the transductive setting, is it possible to adapt the framework to the transductive setting and how?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9345/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698221199414,
        "cdate": 1698221199414,
        "tmdate": 1699637175826,
        "mdate": 1699637175826,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sWQVfdwBjg",
        "forum": "RpKA1wqgk0",
        "replyto": "RpKA1wqgk0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9345/Reviewer_xYfp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9345/Reviewer_xYfp"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed MetaFormer, a new Transformer-based method for meta-learning. To improve the efficiency of self-attention in meta-learning, the authors propose to distengle the computation into three different dimensions: Task Attention, Spatial Attention and Sample Attention. Compared to existing state-of-the-arts, the proposed method achieves better performance across different datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.  The motivation of this paper is clear: most exisiting meta-learning frameworks only show effectiveness in convolutional neural networks, as Transformer is prevailing these days, it is meaningful to validate and adapt this architecture in meta-learning as well.\n2. The proposed method achieves clearly better performance than SOTA methods.\n3. This paper is easy to follow. The figures well illustrate the framework of the proposed MetaFormer."
            },
            "weaknesses": {
                "value": "1. Recent large-scale pretrained vision foundation models, such as CLIP and SAM, have demonstrated superior zero-shot performance on image classification and visual grounding. In this context, one of my primary concerns is that the problem setting in this paper is not sufficiently significant. For example, datasets such as miniImageNet and CIFAR-FS may provide insights into the performance of meta-learning frameworks on small datasets, but they cannot accurately reflect performance on large-scale open-vocabulary datasets.\n\n2. From a technical perspective, the proposed holistic attention mechanism is novel in meta-learning. However, in general, the decoupling of self-attention computation into multiple dimensions is not new in the literature [1, 2]. For example, in video processing, TimeSformer [2]  has proposed to separate the spatial and temporal attention in a single block.\n\n3. The paper does not demonstrate any efficiency gains from the proposed attention design. This is unconvincing, as one of the primary motivations described in the introduction is to reduce the computational cost of attention in ViTs when adapting for Meta-learning.\n\n[1] Ho, Jonathan, et al. \"Axial attention in multidimensional transformers.\" arXiv preprint arXiv:1912.12180 (2019).\n\n[2] Bertasius, Gedas, Heng Wang, and Lorenzo Torresani. \"Is space-time attention all you need for video understanding?.\" ICML. Vol. 2. No. 3. 2021."
            },
            "questions": {
                "value": "Can the authors report the FLOPs and inference speed, memory cost in Table 1, 2, 3? Or at least the Table 3."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9345/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9345/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9345/Reviewer_xYfp"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9345/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698488965799,
        "cdate": 1698488965799,
        "tmdate": 1699637175719,
        "mdate": 1699637175719,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7ZwERzHyKC",
        "forum": "RpKA1wqgk0",
        "replyto": "RpKA1wqgk0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9345/Reviewer_5apc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9345/Reviewer_5apc"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose a ViT-based few-shot learning framework namely MetaFormer. Starting from vanilla ViT, the authors first propose Sample-leel Attention Module to reduce the computation cost and better intra-task interaction, and then propose Task-level Attention Module to enhance inter-task interactions for better feature representation. MetaFormer achieves promising performance on various few-shot learning benchmarks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The motivation of \"decoupling space-level and sample-level attention\" is intuitive. Since this design can effectively reduce the computation cost meanwhile reduce sufficient unrelated attention values to ensure the quality of output features. \n\n2. The performance of MetaFormer is promising. And the visualization results demonstrate the effectiveness of proposed methods. \n\n3. This paper is well written and easy to reproduce."
            },
            "weaknesses": {
                "value": "1. Some critical ablation studies are lacked. e.g., the nubmer of task probe vectors (why choosing 1 for ViTs) and the size of knowledge pool (which number is better). Besides, for Table 3, the authors could introduce more variants (e.g., vanilla ViT with more layers) to support that: the performance improvement is from SAM / TAM but not more parameters. \n\n2. Though the authors claim that using proposed holistic attention mechanism can significantly reduce the computation complexity, the authors still need to provide essential FLOPS / latency statistic to support the merit. For example, reporting baseline method and MetaFormer under 5-way 5-shot setting. \n\n3. More questions regrading the details of the paper, please see Question section for detail."
            },
            "questions": {
                "value": "1. In line 1 of page 5, the authors claim that the complexity of O((NK + M)^2 + L^2). Nevertheless, both NK+M and L cannot be omitted in the decoupled attention, therefore it mighte be O(L(NK+M)^2 + (NK+M)L^2). The authors could recheck the complexity and ensure the correctness of the manuscript. \n\n2. As shown in Eqn. 5, the tokens in knowledge pool are updated by direct addition without averaging. The authors could discuss the performance between with and without averaging during pool consolidation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9345/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9345/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9345/Reviewer_5apc"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9345/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698605881015,
        "cdate": 1698605881015,
        "tmdate": 1699637175609,
        "mdate": 1699637175609,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8Tn06lJB6l",
        "forum": "RpKA1wqgk0",
        "replyto": "RpKA1wqgk0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9345/Reviewer_kzi2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9345/Reviewer_kzi2"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces MetaFormer, a ViT-based framework, designed to excel in the domain of few-shot image classification. It splits attention mechanisms into two key phases: intra-task and inter-task interactions. Intra-task interactions are handled by the Sample-level Attention Module (SAM), which models sample relationships within tasks. For inter-task interactions, the Task-level Attention Module (TAM) is introduced to learn task-specific probe vectors and retrieve relevant semantic features from previous tasks, building a dynamic knowledge pool. MetaFormer demonstrates good performance across a wide range of benchmarks, including those related to few-shot learning and cross-domain tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1)  The concept behind the proposed Metaformer is very simple and straightforward.\n2)  The proposed Metaformer delivers superior quantitative results on extensive few-shot learning benchmarks."
            },
            "weaknesses": {
                "value": "1) Fundamentally, the technical contributions concerning sample-level attention and task-level attention presented in this work are not groundbreaking. For instance, the approach of decoupling attention (as detailed in section 3.2) to alleviate computational complexity is a well-established practice, particularly within the domain of video transformers.\n2) A more thorough examination of related research is warranted. For example, it would be insightful to delve into the distinctions between the inter-task attention module in [1] and the proposed TAM module, even if [1] is rooted in the continual learning community.\n3) It would enhance the clarity of Tables 1 and 2 to incorporate columns displaying the number of parameters for each backbone model, as opposed to segregating this information in the ablation study section. Such an adjustment would facilitate a more straightforward assessment of whether the observed improvements in numerical performance can be attributed to an augmented parameter count.\n\n[1] Continual learning with lifelong vision transformer, CVPR 2022"
            },
            "questions": {
                "value": "It would be beneficial to incorporate more in-depth discussions concerning prior research in the realm of meta-learning that incorporates vision transformers as their foundational architecture. For instance, when the authors highlight that \"the majority of existing methods are specially tailored for CNNs and thus fail to translate effectively to vision transformers,\" it would be valuable to provide a more comprehensive explanation of the limitations of existing approaches when they are applied in conjunction with transformers."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9345/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9345/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9345/Reviewer_kzi2"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9345/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698733594412,
        "cdate": 1698733594412,
        "tmdate": 1699637175497,
        "mdate": 1699637175497,
        "license": "CC BY 4.0",
        "version": 2
    }
]