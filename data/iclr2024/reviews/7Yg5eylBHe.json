[
    {
        "id": "Wc9KnjyYDv",
        "forum": "7Yg5eylBHe",
        "replyto": "7Yg5eylBHe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9310/Reviewer_BqAx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9310/Reviewer_BqAx"
        ],
        "content": {
            "summary": {
                "value": "The authors study the problem of distributed BO, in which multiple agents wish to optimize the same black-box objective function in a sample-efficient manner but cannot share their data due to privacy concerns. The authors propose ZGS-ED-DLCB, an algorithm that circumvents the problem by sharing the parameters of a random Fourier features model instead of the data directly. The algorithm builds off an event-triggered zero-gradient sum algorithm in the distributed learning literature. The authors provide theoretical results to show the convergence of the parameters of all agents in each iteration. The authors compare ZGS-ED-DLCB to previous algorithms in the distributed learning literature."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Applying the event-triggered zero-gradient-sum algorithm to BO may be of interest to BO practitioners interested in distributed BO with privacy concerns.\n2. The authors show that their algorithm outperforms previous algorithms in the distributed learning literature, suggesting that their algorithm has empirical advantages within the BO setting as compared to these other algorithms."
            },
            "weaknesses": {
                "value": "The paper has several major weaknesses:\n\n1. **Technical misconception of random Fourier features (RFF) in a BO context.** Section 3.3, Stage 2 says \"Based on the obtained $W_t^{i*}$, each agent calculates the aforementioned mean and prediction variance at the iteration step t. Using the calculated $m_t(\\mathbf x^*)$ and $\\hat \\sigma_t^2(\\mathbf x^*)$, the RFF-based DLCB acquisition function is employed...\" where DLCB is a UCB-type acquisition (lower bound version). $W_t^{i*}$ are the parameters to the linear model that arises from a RFF approximation of the GP ($\\omega$ in [1]); specifically, the parameters that minimize the linear model's regularized L2 loss with respect to all agents' data. The problem arises as $W_t^{i*}$ has nothing to do with the computation of $m_t(\\mathbf x)$ and $\\hat \\sigma_t^2(\\mathbf x)$. In a RFF context, $m_t(\\mathbf x)$ and $\\hat \\sigma_t^2(\\mathbf x)$ are computed only using $s$ ($\\phi$ in [1]), i.e., the RFF approximation to the kernel $k$, along with the data (see Appendix B in [1]). This begs the question, what is the use of computing $W_t^{i*}$ if a UCB algorithm is to be used subsequently? Computing $W_t^{i*}$ makes sense in a Thompson sampling context (as was done in [1]), but not with UCB algorithms. This renders the significance of most of the technical content in this paper questionable since most of the paper is devoted to the ZGS-based algorithm for computing $W_t^{i*}$.\n\n2. **Experiments do not compare to the most relevant baselines, and results are not presented well.** The paper makes mention of [1] and [2], and the idea of using RFF to share information in a parametric manner is borrowed from [1]. In a BO context, [1] is clearly the closest work. However, the experiments do not compare against the algorithms from [1] and [2], and instead compare against existing algorithms in the distributed learning (DL) literature. This is a problematic choice given that, in the introduction, the authors claim that \"the existing DL works ... do not consider the expensive black-box optimization problems utilizing limited data\", so why only use them for the experiments and exclude the directly relevant algorithms from [1] and [2]? This weakens the empirical support for the proposed algorithm. Furthermore, the authors present the number $R_T/T$ without error bars in tables instead of graphs of the per iteration cumulative regret as is standard for BO works in order to compare the convergence over time.\n\n3. **Significance of theory is questionable.** From a BO perspective, this paper does not contribute anything theory-wise. All the results are about the convergence of the parameters $W$ in each iteration due to the ZGS algorithm, and it is not clear how this suggests sublinear regret. There is a cryptic paragraph about a regret bound in Appendix M (not referenced from the main paper) which handwaves the issue to the results from another paper. From a distributed learning perspective, I am not sure that this paper has significant contribution, since the BO problem is turned into a linear regression problem upon which the ZGS algorithm is applied, but the ZGS algorithm builds upon work in [3] which is for continuously differentiable strongly convex problems, a more general class of problems.\n\n4. **Severe clarity issues**:\n    \n    a. Problems with notation. Some variables are used before they are defined e.g. $W(k)$, and some are never defined at all e.g. $\\hat \\sigma_2(\\mathbf x)$. Some notation is inconsistent e.g. $\\mathbf x$ and $x$ are both for inputs; vectors are sometimes denoted in lower case, sometimes they are in bold lower case, sometimes they are in upper case. Some notation is just wrong, e.g., in Appendix B, an input output pair is written as $(\\mathbf x_1, f(\\mathbf x_1))$, and then the output is later referred to as $y_1$, but $f(\\mathbf x_1)$ and $y_1$ are separate quantities, the latter is the noise-corrupted version of the former. \n\n    b. Extensive typos and grammatical errors, including but not limited to: \"addressing these significant gaps, we aim to design a DBO algorithm that solves costly-to-evaluate black-box optimization problems in a fully distributed multi-agent system (MAS) is nontrivial and promising\"; \"It is the first time to provide\"; \"2.1 ALTERNATIVE TO A GAUSSIAN PORCESS\"; \"As aforementioned discussions\".\n\n\n[1] Dai et. al., 2020. \"Federated Bayesian optimization via Thompson sampling\".\n\n[2] Dai et. al., 2020. \"Differentially private federated Bayesian optimization with distributed exploration\".\n\n[3] Chen and Ren, 2016. \"Event-triggered zero-gradient-sum distributed consensus optimization over directed networks\"."
            },
            "questions": {
                "value": "No questions other than those listed in the Weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9310/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697901508640,
        "cdate": 1697901508640,
        "tmdate": 1699637172794,
        "mdate": 1699637172794,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gRuK8pw5KC",
        "forum": "7Yg5eylBHe",
        "replyto": "7Yg5eylBHe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9310/Reviewer_EeJ5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9310/Reviewer_EeJ5"
        ],
        "content": {
            "summary": {
                "value": "***** I do not have enough background knowledge to provide a fair review of this paper, since it is totally out of my scope. I sincerely request the AC to introduce another reviewer. I have set my confidence score to be 2 to indicate this.*******\n\n\nThis paper studies Bayesian optimization method for the fully distributed multi-agent systems. A new algorithm is proposed and the convergence is proved."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The problem setting is quite clear, and the authors describe their method straightforwardly. \n\nThe theoretical results are provided, an upper bound of the regret is given.\n\nThe experiment part is strong and shows improved results."
            },
            "weaknesses": {
                "value": "I am quite confuse about the comparison of the results to other papers. For example, is the regret bound in (15) matching the SOTA? I think some discussion on the related works and results are missing."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9310/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698709519772,
        "cdate": 1698709519772,
        "tmdate": 1699637172663,
        "mdate": 1699637172663,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xyXdmCsb7n",
        "forum": "7Yg5eylBHe",
        "replyto": "7Yg5eylBHe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9310/Reviewer_ugDi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9310/Reviewer_ugDi"
        ],
        "content": {
            "summary": {
                "value": "This paper studies distributed Bayesian optimization in a multi-agent setting where private raw data cannot be shared among neighboring agents. The authors introduce the ZGS-ED-DLCB algorithm to address the challenge of expensive black-box optimization over a network. In this context, agents possess local data and must collaborate to achieve a global objective. The algorithm employs an event-driven mechanism to trigger communication between agents when necessary, thereby reducing the overall communication burden within the network. The paper includes theoretical and experimental results to demonstrate the efficiency and effectiveness of the proposed algorithm."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The problem-setting appears interesting and worthy of exploration. The authors provide some theoretical analysis for their algorithm, ZGS-ED-DLCB."
            },
            "weaknesses": {
                "value": "- I have to say the paper is not well written. It suffers from a lack of clarity in terms of writing, notation, and explanation, making it challenging to follow. From the problem formulation to the theoretical results, several essential details are omitted. Understanding the context is difficult, necessitating multiple visits to the appendices. I would suggest the authors maintain the core message within the main paper while relocating unnecessary sections to the appendix.\n\n- Furthermore, the main technical contributions remain poorly explained, with insufficient references to existing literature. High-level intuitions are well presented. It is hard for me to evaluate the theoretical contribution.\n\n- Regarding the contributions you claim, it is unclear how your algorithm tracks expensive-to-evaluate black-box optimization. Additionally, the paper does not adequately demonstrate how your algorithm handles limited data and how it compares to existing approaches. The privacy preservation aspects are mentioned, but the extent and specific mechanisms are not detailed.\n\nSome minor comments:\n- The line immediately following equation (11) introduces $\\hat{W}_t^i$, but this term is not defined.\n- The introduction of the notion of \"regret\" raises questions as the paper does not present the associated results."
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9310/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698711977638,
        "cdate": 1698711977638,
        "tmdate": 1699637172560,
        "mdate": 1699637172560,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bNpjdDJBLt",
        "forum": "7Yg5eylBHe",
        "replyto": "7Yg5eylBHe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9310/Reviewer_N4Gg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9310/Reviewer_N4Gg"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a distributed optimization algorithm for multi-agent systems, in which the agents use random Fourier features to model the objective function as a linear function in the random features, and collaboratively estimate the parameter for the linear function."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper derives theoretical guarantees for the parameter estimation.\n- The experimental results do achieve improvements over previous methods."
            },
            "weaknesses": {
                "value": "- The most important concern I have is regarding the writing and presentation of the paper. I'm relatively familiar with the field of Bayesian optimization, but I find the paper not easy to understand and I am somewhat confused as to what is the connection between the proposed method and Bayesian optimization. If I understand correctly, the paper models the objective function $f$ as a linear function in the random features (equation 4) with parameters $W$, and the proposed algorithm aims to estimate the parameters $W$ (using the methods from equations 12 and 13) and the theoretical guarantees in Section 4 also guarantee the estimation quality for the parameters $W$. However, Bayesian optimization aims to optimize (instead of estimate) the function $f$. Therefore, it's unclear to me why the proposed method is named distributed Bayesian optimization. Also, I'm also confused by equation (7), i.e., why do we need to extend W to be time-varying? Also, none of the baselines compared in the experiments are federated/distributed BO.\n- Below equation (1), the regret bound shown here is for time-varying Bayesian optimization, I wonder why is that the case?\n- Section 3.2, I think the presentation of this section can be further polished. Below equation (11), what is $\\hat{W}$? The estimated $W$ since the last information exchange? In the next paragraph, what is a \"sub-iteration\"? It is never mentioned that the proposed method has an inner loop with sub-iterations. Remark 2: again it's confusing to me why the parameters are time-varying.\n- Section 3.3: I find this section not easy to understand. Perhaps it will help understanding if more intuitions/descriptions are given in addition to the equations.\n- Stage 2 in Section 3.3: it says here that based on the $W^{i^*}_t$, each agent can calculate the Gaussian process posterior mean and variance. But in my understanding, with a single parameter vector $W$, we can only calculate a single function value at an input (see equation 4). To be able to calculate the GP posterior mean and variance, we need the entire distribution of $W$ (see Dai et al., (2020)). Please give the exact equations using which the GP posterior mean and variance are calculated.\n- Section 4.1: I also find this section not easy to understand."
            },
            "questions": {
                "value": "Please see the questions I listed above under Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9310/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698822445994,
        "cdate": 1698822445994,
        "tmdate": 1699637172445,
        "mdate": 1699637172445,
        "license": "CC BY 4.0",
        "version": 2
    }
]