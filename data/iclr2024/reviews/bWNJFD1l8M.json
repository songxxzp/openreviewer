[
    {
        "id": "AGSxEKTmez",
        "forum": "bWNJFD1l8M",
        "replyto": "bWNJFD1l8M",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4798/Reviewer_EPpK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4798/Reviewer_EPpK"
        ],
        "content": {
            "summary": {
                "value": "The computational cost of training the neural network is high. \nTo reduce these computational costs, this paper also intends to utilize a well-trained neural network. \nContrast to previous studies, this paper proposes to use the training trajectory of neural networks for model training because it contains a lot of information. \nThe authors dubbed this problem \"learning transfer problem,\" which is the transfer of trajectories from one initial parameter to another. \n\nTo this end, the paper proposes an algorithm that matches the gradient continuously along the trajectory through permutation symmetry. \nThe authors demonstrate efficiency of the algorithm in two ways to evaluate the validity of the proposed algorithm. \n\nThe first is the initialization scenarios that consists of random or pre-trained initialization to assess whether the transferred parameters via \u201clearning transfer\u201d without finetuning can enhance the accuracy. \nThe other is the fine-tuning scenario to validate whether the transferred parameters can improve the learning speed.\n\nWith two scenarios, this paper empirically demonstrates the proposed algorithm can train the model quickly and efficiently."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The task, \u201clearning transfer problem\u201d the authors proposed is novel to me.\n\nTo address this problem, the authors proposed an algorithm to match the trajectories between source and target, which is seemly convincing. To evaluate the validity of the proposed algorithm, the authors, without any training, conducted an experiment that transfers the calculated parameter to match the trajectory, which performed somewhat successfully. \n\nIn addition, as a result of fine-tuning after transferring the parameters, it is revealed that the performance increased very quickly."
            },
            "weaknesses": {
                "value": "1. There is a lack of motivation albeit the promising results. It is a lack of the evidence whether having the same trajectory between tasks is always good. \n2. To experimentally prove that an initialization or architecture affects the similarity more than the dataset, it is necessary to verify it on more datasets."
            },
            "questions": {
                "value": "1. Does it show the good performance to make the model to have the same trajectory in tasks that are not related to each other?\n2. Are these transferred models' ensembles better than scratch ensembles? What is it like from an ECE perspective?\n3. In Sec. 3.4, there are an explanation why the transfer of the linear trajectory is more stable and has less variance than the transfer of the actual one. The authors explain that it may be because the actual trajectory contains noisy information. I think the theoretical or empirical evidence is necessary to support the explanation. \n4. I think we need to do the work shown in Fig. 5 to select the optimal parameter. Then shouldn't we put this process into the pseudo-code?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4798/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4798/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4798/Reviewer_EPpK"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4798/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698551158497,
        "cdate": 1698551158497,
        "tmdate": 1699636462681,
        "mdate": 1699636462681,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XHSJysb1UN",
        "forum": "bWNJFD1l8M",
        "replyto": "bWNJFD1l8M",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4798/Reviewer_rULw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4798/Reviewer_rULw"
        ],
        "content": {
            "summary": {
                "value": "### Problem Statement\n\nThe paper introduces a novel problem called the \"learning transfer problem\", aimed at reducing training costs for seemingly duplicated training runs on the same dataset by transferring a learning trajectory from one initial parameter to another without actual training.\n\n### Main Contribution\n\nThe paper's contributions include: (1) formulating the new problem of learning transfer with theoretical backing, (2) deriving the first algorithm to solve it, (3) empirically demonstrating that transferred parameters can accelerate convergence in subsequent training, and (4) investigating the benefits and inheritances from target initializations. Through these contributions, the paper presents a promising avenue to significantly reduce the computational cost and time required to train DNNs, especially in scenarios involving model ensembles or fine-tuning pre-trained models.\n\n### Methodology\n\nThe authors approximate the solution to the learning transfer problem by matching gradients successively along the trajectory via a permutation symmetry technique. The updates along the \"source trajectory\" are applied to update a target network with a different random initialization after being permutated such that the gradients of the two networks at the same \"time step\" are best matched under the permutation, resulting in the Gradient Matching along Trajectory (GMT) algorithm. To further optimize the space and time complexity of the algorithm, the authors propose to use linear interpolation of the initial and final parameters of the source network in place of the acutal training trajectory, and to re-use the mini-batch gradients evaluated along the trajectories to search for the best-matching permutations. The optimized verison of the algorithm is named \"Fast Gradient Matching along Trajectory\" (FGMT). The best permutations for parameter alignment at each time step are solved with a coordinate descent algorithm, iteratively optimizing for the permutation in each layer of the network by solving a linear assignment problem.\n\n### Experiments\n\nThe learning transfer methods are evaluated on standard vision datasets with various architectures including CNN and MLP. Both random initializations and pre-trained initializations are used to evaluate and demonstrate the effect of GMT and FGMT in terms of 1) the performance after transfer; 2) the fine-tuning efficiency and performance after transfer.\n\nEmpirical evaluations reveal that the transferred parameters achieve non-trivial accuracy before any direct training and can be trained significantly faster than training from scratch, while inheriting the properties (e.g. generalization ability) from the parameter initialization."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "### Originality and significance\n\nThe proposed task of learning transfer problem is novel and very interesting, with potentially wide applications, as the foundation-model paradigm prevails in many AI / DL fields. The proposed method is to progressively merge the target network with the source network using Git Re-basin, which is straightforward and efficient.\n\n### Quality\n\nTheoretical analysis is performed to justify the adopted method, in addition to a series insightful experiments. The experimental details are in general well documented. However, I find the experiments are not enough to support some of the claims, and will expand on this in the Weakness section.\n\n### Writing\n\nThe writing is overall good, despite minor grammar problems. I find the mathematics in the paper is clear with consistent and self-explanatory notations. The problem is well motivated and formulated, and the main thesis is well conveyed."
            },
            "weaknesses": {
                "value": "I am open to change my score if the authors can address the following concerns:\n\n### Lack of experiments more closely demonstrating the actual usage of the proposed method\n\n1. One potential usage of the proposed method suggested by the authors is to transfer the update of a foundation model to its fine-tuned versions. However, all experiments are limited to network architectures of relatively smaller scale, and to the cases where fine-tuning task shares exactly the same number of classes as the pre-training task, which differs from the realistic use-case of foundation models, which are of typically larger scale, and are used for various down-stream tasks with task-specific heads.\n\n2. The authors claim that method can accelerate the training of an ensemble of neural networks. Although the computational cost of the proposed method is briefly described in the appendix, there is no figure or table systematically comparing the cost with traditional training / fine-tuning approaches. More crucially, no experiment compares the performance of the ensemble obtained with GMT / FGMT and traditional approaches. One concern is that, transfering one (or a limited number of) source trajectory, the diversity of the resulted target networks is limited (this is partially endorsed by the landscape visualization in Figure 7), which could hurt the performance of the ensemble.\n\n### Lack of experiments verifying the Assumption (P), Theorem 3.1, and Lemma 3.2\n\nAlthough the theoretical analysis abounds, it would be much more convincing to show these statements hold in real experiments.\n\n### Lack of experiments and discussion on the choice of $T$\n\nIt is not clear how $T$, the numebr of time steps, or rather, the number of samples taken from the source trajectory, influences the performance of the transfer result. It seems that $T$ does not really matter in the Na\u00efve baseline and Oracle baseline, where the parameter-aligning permutation $\\pi$ remains the same across time steps, which 1) would be good to be verified by the authors in the main text and 2) makes it interesting to explore the value of $T$ that GMT / FGMT requires to have good performance, because the computational cost is proportional to $T$.\n\n### The \"Generalization ability\" part in section 4.3 is not clear\n\nIn the \"Generalization ability\" part in section 4.3, the experiments and figures should be further clarified. From Figure 7, I assume that the task is to fit the CUB or Cars dataset (for the previously chosen 10-class subset), and the two \"Standard\" curves are for the target trajectories, which are transferred through FGMT to initialization pretrained with ImageNet-10%, leading to the green curves, but the explanations are really not clear. However, the main text explicitly states that the target initialization $\\theta_2^0$ is \"pre-trained on full ImageNet\", which implies only *1* (instead of *2*) possible combination for FGMT for each fine-tuning dataset: starting from ImageNet-10%-pretrained, transferring the finetuning trajectory which starts from the ImageNet-Full-pretrained initialization). Another example is, I am not sure what validation the authors refer to when they mention that the ImageNet-Full pretrained initialization has \"validation accuracy $\\approx$ 72%\" and the ImageNet-10% one has \"validation accuracy $\\approx$ 50%\".\n\n\n### Minor\n- In the second last line of the second paragraph in Introduction: strength -> strengthen\n- On Page 5, right below Lemma 3.2: please make it explicit in which Appendix the proof is\n- On Page 6, in the \"Linear trajectory\" section, \"such a linearly interpolated trajectory *satisfies* ...\"\n- On Page 9, in the \"Generalization ability\" section, \"generalizes poorly *than* ...\" -> \"generalizes poorly *compared to* ...\""
            },
            "questions": {
                "value": "1. Data augmentation is widely used for the training of DNNs, which essentially (often randomly) modifies the dataset. Does this violate the assumption that the dataset is the same in the problem statement?\n\n2. Even gradient-based, many optimization methods for DL can have updates very different from SGD (for example, Adam). Considering that the parameter alignment is done with gradient matching in GMT and FGMT, does the choice of optimization method changes the effect of GMT / FGMT? Ablation study regarding this could be worthwhile to add.\n\n3. Why does the performance deteriorate in Figure 5d-5f? It is simply acknowledged without analysis and discussion.\n\n4. In Figure 7, to better understand the effect of FT and FGMT, where would the FGMT-only parameters be in the landscape? What about permuted-source + FT?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4798/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4798/Reviewer_rULw",
                    "ICLR.cc/2024/Conference/Submission4798/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4798/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698605454464,
        "cdate": 1698605454464,
        "tmdate": 1700892646088,
        "mdate": 1700892646088,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qwvAAnE3BH",
        "forum": "bWNJFD1l8M",
        "replyto": "bWNJFD1l8M",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4798/Reviewer_gCAh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4798/Reviewer_gCAh"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel algorithm for transferring a learning trajectory from one initial parameter to another, which can significantly reduce the computational cost of training deep neural networks. The algorithm formulates the learning transfer problem as a non-linear optimization problem for the policy function and matches gradients successively along the trajectory via permutation symmetry to approximately solve it. The empirical results show that the transferred parameters achieve non-trivial accuracy before any direct training and can be trained significantly faster than training from scratch. However, the algorithm's limitations include the assumption that the source and target tasks are related, and the lack of a detailed analysis of the computational cost of the algorithm."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed algorithm is a novel approach to the problem of transferring a learning trajectory from one initial parameter to another. The idea is interesting.\n2. The algorithm is theoretically grounded and can be solved efficiently with only several tens of gradient computations and lightweight linear optimization.\n3. The empirical results show that the transferred parameters achieve non-trivial accuracy before any direct training and can be trained significantly faster than training from scratch."
            },
            "weaknesses": {
                "value": "1. The empirical evaluation of the algorithm is conducted on a limited set of benchmark datasets, and it is unclear how well the algorithm would perform on other types of datasets or in real-world scenarios.\n2. The paper assumes that the source and target tasks are related, and it is unclear how well the algorithm would perform when the tasks are not such related.\n3. The paper does not provide a detailed analysis of the computational cost of the algorithm, which may be a concern for large-scale neural networks.\n4."
            },
            "questions": {
                "value": "1. How sensitive is the algorithm's performance to the assumption that the source and target tasks are related, and how well does it perform when the tasks are unrelated?\n2. How does the proposed algorithm compare to other methods for transferring learning trajectories, such as fine-tuning or transfer learning?\n3. How well does the algorithm perform on datasets that are not included in the empirical evaluation, and how does its performance compare to other methods on these datasets?\n4. Can the algorithm be extended to handle more complex neural network architectures?\n5. How does the computational cost of the algorithm compare to other methods for transferring learning trajectories, and how does it scale with the size of the neural network and the number of training epochs?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4798/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4798/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4798/Reviewer_gCAh"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4798/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698809410995,
        "cdate": 1698809410995,
        "tmdate": 1699636462461,
        "mdate": 1699636462461,
        "license": "CC BY 4.0",
        "version": 2
    }
]