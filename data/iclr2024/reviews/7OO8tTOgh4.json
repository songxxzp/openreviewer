[
    {
        "id": "i76yR3AHDc",
        "forum": "7OO8tTOgh4",
        "replyto": "7OO8tTOgh4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2947/Reviewer_fBK2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2947/Reviewer_fBK2"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a novel untargeted attack on large visual-language models. To achieve this, the paper attempts to perturb the input image in order to maximize the entropy of the logits, attention, and the intermediate embeddings. Subsequently, the semantic meaning of the output texts is disrupted. The experiments demonstrate the effectiveness of this method and show that visual-language models can be attacked from the vision side."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper explores an important problem. Currently, a lot of work on adversarial examples has been done on vision models, while less attention has been paid to large VLMs. With the increasing application of large VLMs, I believe that the exploration of the robustness of large VLMs is a valuable and important step.\n- This paper presents the first untargeted attack against large VLMs. The untargeted goal is realized by maximizing the entropy of the intermediate or output values of the LLM.\n- This paper is well-written and well-presented. I enjoyed reading this paper."
            },
            "weaknesses": {
                "value": "- While I like the idea of this paper, the technical contributions concern me. Since this paper is not the first attack against large VLMs, I believe the main contribution of this paper is the design of an untargeted attack that explores information entropy. If this is the case, I think the contributions, in their current state, lack depth. I believe this paper can improve in the following directions.\n    - For example, we now have three methods of MIE, but which one is better? What are their strength and weaknesses? Can we have an analysis and an in-depth discussion?\n    - What factors can influence the performance of this method?\n    - Is this attack robust against simple defensive methods like robust training?\n    - Comprehensive evaluation on parameter different settings. For now the evaluation is limited, e.g., only one attack strength is evaluated."
            },
            "questions": {
                "value": "I generally appreciate the direction and idea of this paper. However, the current state of this paper can only be considered a simple proof of concept and lacks depth. I would consider accepting this paper if the authors provide a comprehensive study of this method."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2947/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698332727443,
        "cdate": 1698332727443,
        "tmdate": 1699636238527,
        "mdate": 1699636238527,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ibwL700CLc",
        "forum": "7OO8tTOgh4",
        "replyto": "7OO8tTOgh4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2947/Reviewer_CGXF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2947/Reviewer_CGXF"
        ],
        "content": {
            "summary": {
                "value": "The study achieves non-targeted attacks on VLMs by maximizing the entropy of network output, attention, and features."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "* The content of the paper is very easy to understand."
            },
            "weaknesses": {
                "value": "* Lack of comparative methods. Many methods have already been proposed for attacking VLMs[1], but the authors did not compare with these methods during the experimental phase, choosing only simple Gaussian noise for comparison.\n\n* The approach in the paper is somewhat ad-hoc. There are various ways to disrupt the expressions in network layers, such as maximizing the norm of mid-layer features. Why did the authors choose to maximize entropy for the attack? The rationale behind this was not clarified in the paper.\n\n* Absence of ablation studies. The final attack method in the paper is composed of three losses, but the authors did not discuss the impact of different loss coefficients on the results within the article.\n\n* There is a typo below Equation (5), where the second instance of $\\lambda_1$ should be $\\lambda_3$.\n\n[1] Zhao, Yunqing, et al. \"On evaluating adversarial robustness of large vision-language models.\" arXiv preprint arXiv:2305.16934 (2023)."
            },
            "questions": {
                "value": "Please see weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2947/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2947/Reviewer_CGXF",
                    "ICLR.cc/2024/Conference/Submission2947/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2947/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698547060195,
        "cdate": 1698547060195,
        "tmdate": 1700750616707,
        "mdate": 1700750616707,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3qGPX1C5z1",
        "forum": "7OO8tTOgh4",
        "replyto": "7OO8tTOgh4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2947/Reviewer_qFBU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2947/Reviewer_qFBU"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new attack on VLM. They utilize the PGD optimization algorithm to maximize the entropy of the predicted token entropy, attention weights, and normalized hidden layers values. They show empirically that their attack is effective in attacking sevral open-source VLMs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The topic of robustness of VLMs is relevant. \n- The experimental results show the effectiveness of the attack."
            },
            "weaknesses": {
                "value": "- The technical contribution is on the moderate side. However, to the best of my knowledge using maximum entropy to adversarial attack is novel.   \n- the paper lacks comparison to related work. There are several attacks on image captioning in the literature. The paper lacks a comparison against them (see for example, [1] and [2]). The authors mention [2] in the related work but did not empirically compare against it, they justify it as this method uses the original caption. This, however, is a minor requirement as it can be mitigated by treating the caption on the clean image as the ground-truth caption.   \n\n\n[1] Aafaq, Nayyer, et al. \"Language model agnostic gray-box adversarial attack on image captioning.\" IEEE Transactions on Information Forensics and Security 18 (2022): 626-638.\n\n[2] Schlarmann, Christian, and Matthias Hein. \"On the adversarial robustness of multi-modal foundation models.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023."
            },
            "questions": {
                "value": "- There are hyper-parameters for the attacks $\\lambda_1,\\lambda_2,\\lambda_3$. Are these selected based on the same validation set?  If so I believe the experiment might lack statistical integrity."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2947/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2947/Reviewer_qFBU",
                    "ICLR.cc/2024/Conference/Submission2947/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2947/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698879501898,
        "cdate": 1698879501898,
        "tmdate": 1700659810759,
        "mdate": 1700659810759,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PVI5B43Fpv",
        "forum": "7OO8tTOgh4",
        "replyto": "7OO8tTOgh4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2947/Reviewer_r3mc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2947/Reviewer_r3mc"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes  adversarial attacks on Visual Language Models (VLMs), as illustrated in Figure 1. A clean image is perturbed slightly so that a VLM generates a wrong description of the image. In this paper, the attacks are \"non-targeted\" in that the goal is to cause the description to change to anything, and not something specific. \n\nA concern with the paper is that prior work, e.g., (Carlini 2023), which is cited in the paper, also solves a pretty similar problem, but the contribution over that work  is not clearly stated. For instance, see page 20 of (Carlini 2023) in which the image of Mona Lisa was adversarially perturbed to cause a completely incorrect description to be output. Why couldn't the same techniques be used here and what precisely is the contribution over (Carlini 2023) or similar work? I would have liked to see the difference with the closest works highlighted in the Intro. \n\nIn the Related Work section, it seems that the main difference claimed is that prior adversarial attacks on VLMs are targeted, whereas authors propose an untargeted attack (as an aside, if this is the crucial contribution -- I think that should have been stated in the Intro clearly). But, even accepting the author's premise that untargeted attacks weren't addressed by prior work, why (1) is an untargeted attack important; (2) not a special case of a targeted attack where a target is picked at random from the desired domain, thus turning an untargeted attack into a targeted attack and using a prior solution."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The specific algorithm for doing an untargeted attack seems to be different from prior work in VLMs. The authors propose injecting adversarial noise that maximizes the entropy  -- thus effectively causing the resulting image to produce essentially a random caption. They propose three different ways of maximizing entropy and use a weighted combination of the three methods as the objective function (as shown in Algorithm 1).  They find that the first method (equation 2) dominates overall (weight ended up as 0.8). An ablation study would  have been nice to show how each way (equation 2, 3, or 4) would have performed by itself versus the  weighted combination of the three."
            },
            "weaknesses": {
                "value": "An alternate way would be to simply feed some random image that is clearly maximum entropy to the VLM, get a few words of text from it (essentially a random phrase) and then use a targeted attack on the VLM to generate that particular or similar text. I would have liked to see a comparison with such an approach in the paper.\n\nAs an example, couldn't \"a pair of flip flops sitting on a pile of garbage\" in Appendix A.1 first row  be set as the target caption in Carlini 2023 and then a perturbation found that achieves that? Or any other random caption for that matter? \n\nWouldn't the captions just become random sequence of words as the attack progresses? The figures in Appendix A should probably illustrate that, if that is the case. And if that is the case, is such an attack considered successful? Or should the caption generally make sense to a human?"
            },
            "questions": {
                "value": "Can the Intro be revised to identify the closest work to the paper and identify the key contribution over that work (or minor variants of prior work)? \n\nWhy can't untargeted attacks use a method for targeted attacks as a subroutine to achieve an untargeted attack? Note that there is precedence for this in adversarial ML. In the OPT method for blackbox adversarial attacks, the fundamental method is for a targeted attack. The OPT paper discusses how to do an untargeted attack by wrapping a small amount of code around a targeted attack. An example strategy would be to choose a random target and then attempt a targeted attack.\n\nWhy are untargeted attacks on VLMs particularly interesting, given that we already know that targeted attacks are possible and how to do them?\n\nCan an ablation study be presented to show how each entropy method performs on its own (e.g., lambda_1 = 1, others 0, etc.), versus the chosen setting for lambdas of (0.8, 0.1., 0.1)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2947/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699239009909,
        "cdate": 1699239009909,
        "tmdate": 1699636238270,
        "mdate": 1699636238270,
        "license": "CC BY 4.0",
        "version": 2
    }
]