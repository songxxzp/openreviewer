[
    {
        "id": "aYRvrqJMqA",
        "forum": "V8aD5pUcVX",
        "replyto": "V8aD5pUcVX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5808/Reviewer_NrqM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5808/Reviewer_NrqM"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates how to improve the visual encoders for Multimodal Large Language Models (MLLM). \n\n* The authors start by curating a more comprehensive benchmark for evaluating MLLMs. The new benchmark incorporates the tasks of object counting and multi-class identification for fine-grained perceptual abilities.\n* The authors tried a wide range of visual tokenizers: DeiT, CLIP, MAE, DINO, analyzed their behaviors on the datasets, and reached several insights, including (1) supervision leads to a better understanding of semantics; (2) patch-level supervision improves fine-grained perceptual abilities.\n* Based on the above insights, the authors combined the best of both worlds by introducing \"feature distillation\" to apply patch-level supervision to pretrained models. This improves the fine-grained perceptual abilities significantly."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper shows the following strengths:\n\n1. The authors have conveyed their design choices and insights via extensive experiments. For example, the insights of the visual tokenizers are derived from the experiments of various models and datasets.\n2. The benchmark constructed could be useful for future studies on multi-modal models with LLMs.\n3. The final approach of feature distillation is straightforward and clearly improves the performance on the additional datasets of object counting and multi-class identification."
            },
            "weaknesses": {
                "value": "I think the authors may want to pay attention to the following weaknesses. I will include the specific questions in the next section.\n1. I think the authors need to **better define the important notions** and consequently **clarify the difference to previous works**. \n* Describing the difference between \"overall semantics\" and \"fine-grained perception\" is especially critical. Both terms seem vague to the readers but are the foundation of the evaluation benchmarks and insights in the paper.\n* I understand the intuition of adding object counting and multi-class identification. However, their separation from VQA is unclear. I suggest improving the current clarification in Sec. 2.1 according to my questions below.\n2. The method of this paper is quite simple (in a good way) by combining the techniques of feature distillation with patch-level supervision. However, I found the following things need improvement:\n* Writing. Since the method is simple, it is always better to clearly explain the method. Sadly, I could hardly find a grounded description of the method in both Sec. 2.1 and Sec. 3.2. Adding proper equations and detailed clarifications will help the reader to fully understand the approach.\n3. I personally think that the title of the paper is a little **overclaiming**. I will give two perspectives:\n* I know that it is hard to define \"good,\" but what the paper achieves is only **making MLLMs better at object counting multi-class identification** using patch-level feature distillation, without improving the overall semantic understanding. So concentrating on what is achieved is more precise, in my own opinion.\n* When we talk about visual tokenizers, a lot of variables also need to be covered, such as the architecture, training techniques, etc. The paper mainly focuses on the types of supervision without investigating the other variables, which might not be sufficient for a grand title or claim. For example, will switching the claim of the paper to concentrate on \"supervision\" be better?"
            },
            "questions": {
                "value": "Here are my questions. The authors are also welcome to reply to the points raised in the \"Weaknesses\" section here.\n\n1. **Difference between VQA and the additional introduced tasks.** I think this is my most important concern of the paper. Specifically, I am not convinced by the author's explanation in Sec. 2.1 and would like more details. According to my observation on VQAv2, there are plenty of questions asking about the *number of objects* and *small objects in the image*. In my opinion, these are already sufficient for fine-grained perception. As the evaluation is the foundation for the points in this paper, I have the following questions:\n* A clearer definition of \"overall semantics\" and \"fine-grained perception\" with grounded examples and statistics.\n* Quantitatively compare VQAv2, Image captioning, and the datasets used for object counting and multi-class identification to show how the new tasks truly correspond to what this paper claims.\n2. **Is semantic understanding and fine-grained understanding really the key?** Echoing the question above, I notice that the additional datasets rely on MS-COCO as their basis, which naturally has a distribution shift. I acknowledge that the different performances reflected in these datasets are intriguing, but I want quantitative evidence or a reasonable explanation attributing the reason to fine-grained visual understanding.\n3. The part related to masked image modeling confuses me. In Sec. 2.2, the authors discovered that MAE is particularly better at fine-grained visual understanding, but found that masked image modeling hurts the performance in Sec. 3.1 and Table 3. These claims are conflicting and need better clarification. For example, the authors mentioned the train-test difference in introducing the mask tokens: (a) are there any ways to quantify this? (b) Does MAE exhibit the same issue? (c) If using a mask token can be made further tuning to function properly, does the claim \"masked pertaining may not lead to ...\" need to be revised?\n4. I expect **more informative qualitative results**. I appreciate the authors picking qualitative comparison with previous methods (e.g. BLIP-2, in this case). However, these visualizations do not provide sufficient insights into *how the fine-grained abilities are improved?* In the worst case, the readers would consider cherry-picking or writing a program to filter several examples that your method outperforms another approach straightforward. Therefore, I have a suggestion/question:\n* Is it doable to find some ways to visualize the attention/feature activation in such vision-language models? For example, if the feature activation in the 5-th example \"dies bike exists in the image\" shows better focus on the bike for your GVT model, it would be really convincing. Some papers have done similar analysis for your reference ([Shi et al, Fig. 1 and Fig. 6; Pang et al., Fig. B]), but your case might be complicated by the perceiver architecture. Finally, having such qualitative results will greatly add to your simple method and better convince me.\n\nShi et al. Top-Down Visual Attention from Analysis by Synthesis\nPang et al. Frozen Transformers in Language Models Are Effective Visual Encoder Layers"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5808/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5808/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5808/Reviewer_NrqM"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5808/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698532084031,
        "cdate": 1698532084031,
        "tmdate": 1699636612276,
        "mdate": 1699636612276,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FQLw00f2UA",
        "forum": "V8aD5pUcVX",
        "replyto": "V8aD5pUcVX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5808/Reviewer_v493"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5808/Reviewer_v493"
        ],
        "content": {
            "summary": {
                "value": "This paper empirically investigates different visual tokenizers that can be used for multimodal large language models. Based on a set of experiments, authors utilize a visual tokenizer pretrained on large-scale datasets and properly integrate it with patch-level supervision. Extensive experiments on the new benchmark (GVTBench) demonstrate the effectiveness of the proposed model."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The analysis of different visual tokenizers (e.g., Table 1) is very informative. \n\n2. The writing is clear and easy to follow. \n\n3. Compared to the baselines, there are consistent improvements on OC and MCI tasks."
            },
            "weaknesses": {
                "value": "1. The main concern is about the experimental setup. Besides CC3M and SBU, authors use Visual Genome, MS-COCO, Object365 and OpenImages V6 to train the visual tokenizer. Based on the descriptions in Section 2.1, questions in OC and MCI tasks are object-centric. Such pretraining datasets provide a privilege for the proposed model. As shown in table 4, most of the improvements come from OC and MCI.\n\n2. In table 1 and 2, authors use CLIP for ablation study but switch to EVA-CLIP for the remaining experiments. Why is there a inconsistency?\n\n3. Based on table 5 and 6, it seems that there are consistent improvements by explicitly utilizing object-level annotations during pretraining.  This does not necessarily mean that the proposed tokenizer is a good visual tokenizer, especially considering the performance drop on VQA and image captioning tasks."
            },
            "questions": {
                "value": "See the weakness. The main concern is that the pretraining image-text pairs contain a significant amount of object-level annotations. This leads an unfair comparison when the downstream tasks are most object-centric questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5808/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5808/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5808/Reviewer_v493"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5808/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698795256095,
        "cdate": 1698795256095,
        "tmdate": 1699636612143,
        "mdate": 1699636612143,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EOMw1H1eJy",
        "forum": "V8aD5pUcVX",
        "replyto": "V8aD5pUcVX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5808/Reviewer_p5cW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5808/Reviewer_p5cW"
        ],
        "content": {
            "summary": {
                "value": "This work studies an intriguing problem about how the visual tokenizer affects the final performance of LMMs. The authors started with a comprehensive evaluation on various vision encoders, obtained from supervised learning, self-supervised, and weakly-supervised learning. The results demonstrate that a visual tokenizer that possesses both semantic and fine-grained representations achieves the best performance. The author also studied some fine-grained CLIP models and found that it usually tend to loss generic semantic representations due to task overfitting. In the end, the authors proposed their own visual tokenizer training strategy by distilling the CLIP knowledge from one model to another. It turns out that such a simple distillation strategy can preserve a good quality of semantics while enhancing the spatial representations as well. The experimental results on the proposed GVTBench and other question-answering tasks show the superiority to previous models in general."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. I appreciate the authors for conducting a good experimental study on the effect of different visual tokenizers for large multimodal models. It has been a routine that almost all LMMs use CLIP vision encoder as the visual tokenizer. Few people pay enough attention to the vision encoder itself, though it is a very important factor in LMMs.\n\n2. The study on different visual tokenizers suggests that CLIP is still one of the best visual tokenizers for LMMs, despite it is not necessarily able to provide fine-grained representations. This is somehow surprising but also expected because all of the other used visual tokenizers were trained either with much fewer image-text pairs or purely image data at similar scales. \n\n3. The authors further proposed a new benchmark and a simple distillation method to investigate and enhance the fine-grained visual understanding capability for LMMs. The experimental results demonstrate the superiority of the proposed method to previous ones."
            },
            "weaknesses": {
                "value": "1. As mentioned earlier, I like the first part of this work studying the effectiveness of different visual tokenizers for LMMs. The argument of LMMs needing both semantic-rich and fine-grained representations is valid and shared by many but has never been demonstrated experimentally before. However, I do not see a clear motivation to propose a distillation method to address the problem derived from the observation at the first part. The authors simply distill the visual representations from EVA-CLIP to another randomly initialized vision encoder and presume that the new model has better semantic and fine-grained representations.\n\n2. I do not buy that the proposed method learns better fine-grained representations in that the input images for distillation are still 224x224, and the distillation is simply copying one network to another based on a limited number of images from ImageNet-1K. Despite the better performance in Table 4, I can see the unfairness when comparing the proposed method with previous ones -- e.g. different models have different settings and pretraining data. In the ablation study, the authors did show some improvements after distillation on the new benchmarks. Two question marks arise. First, why the authors use EVA-CLIP which proved to be defective in Table 3. Second, is it because the distillation pulls the feature space closer to the target domain, e.g., web images->imagenet->COCO? In Table 6, the gap actually is very marginal (seems the averaged gap does not match the individual gaps for Flant5-xxl).\n\n3. The experimental study is not enough to demonstrate the effectiveness of the distilled visual tokenizer. The authors should examine the visual tokenizers in a wider range of vision and vision-language tasks. That being said, the authors attempt to find a more generic visual tokenizer that can provide semantic-rich and fine-grained representations but fail to demonstrate the generality of the proposed visual tokenizer by itself."
            },
            "questions": {
                "value": "1. I think the authors should explain a bit more about how to derive the proposed distillation methods to improve the visual tokenizer and why the new tokenizer is supposed to learn more fine-grained representations.\n\n2. The empirical study on the effectiveness of the proposed visual tokenizer is insufficient, not to mention some unfairness in the comparisons with previous works. To solidly prove the generality of the proposed distillation technique, the authors should provide more benchmarking results across a wide range of tasks. I doubt distilling on ImageNet-1k is enough to achieve the goal.\n\n3. Why the authors did not use CLIP released by OpenAI as the teacher, but rather used EVA-CLIP as the teacher model, despite knowing that it is inferior to CLIP and other models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5808/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698894429427,
        "cdate": 1698894429427,
        "tmdate": 1699636612025,
        "mdate": 1699636612025,
        "license": "CC BY 4.0",
        "version": 2
    }
]