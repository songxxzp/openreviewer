[
    {
        "id": "Sgv4JKyw7p",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5502/Reviewer_uHfk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5502/Reviewer_uHfk"
        ],
        "forum": "pE6gWrASQm",
        "replyto": "pE6gWrASQm",
        "content": {
            "summary": {
                "value": "In this paper, the authors study the generalization of adversarial robustness from class-wise and sample-wise aspects."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. There are two different settings, i.e., CSAT and ESAT. For each of them, there are comprehensive experiments based on the proposed entropy metrics. On the other hand, the authors consider both $L_2$ and $L_\\infty$ attacks. All settings are studied on multiple datasets, which makes the results more convincing.\n\n2. Downstream task transferability is an interesting topic. The results indicate that the learned features can be transferred to other classes, which is aligned with the observation from CSAT.\n\n3. This paper is easy to follow. The writing is clear."
            },
            "weaknesses": {
                "value": "1. This paper mainly contains various experiments and their results, but lack the important analysis. Specifically, there are no analysis of the transferability observed from CSAT and ESAT. I cannot get any insightful information after reading this paper, although the results are informative.\n\n2. There is no theoretical analysis to rethink the observation as a special property of adversarial training. Additionally, from Figure 4, we can find that the clean accuracy has a similar tendency as the robust accuracy, therefore, it is possible that CSAT and ESAT are just because of the generalizability of the deep learning models.\n\n3. Only PGD-AT is considered. More advanced methods, like TRADES and AWP, should be evaluated under the same settings.\n\n4. For downstream task transferability, it is similar, but not exactly the same, as contrastive adversarial training. The authors should discuss the similarity and difference between these two methods in this case.\n\n\nMinor:\n\na. some figures' labels are blocked."
            },
            "questions": {
                "value": "1. I notice that the authors use stronger data augmentation than usual. For example, when training on CIFAR-10, we usually only use random crop and flip. I hope the authors can provide ablation studies on these data augmentation methods."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5502/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5502/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5502/Reviewer_uHfk"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5502/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697369629090,
        "cdate": 1697369629090,
        "tmdate": 1700628705393,
        "mdate": 1700628705393,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fiD7KTfX4Q",
        "forum": "pE6gWrASQm",
        "replyto": "pE6gWrASQm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5502/Reviewer_3h75"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5502/Reviewer_3h75"
        ],
        "content": {
            "summary": {
                "value": "To investigate the transferability of adversarial robustness across different classes and tasks, the authors proposed Subset Adversarial Training (SAT), which splits the training data into A and B and constructs adversarial examples (AEs) on A only.  Using SAT, this paper shows that training on AEs of just one class (e.g., cars in CIFAR-10) can transfer a certain level of robustness to other classes. Hard-to-classify classes (like cats) tend to provide greater robustness transfer compared to easier ones. Moreover, using AEs generated from half of the training data can match the performance of full AT. These findings also apply to downstream tasks. This paper distinguishes itself from others by only creating AEs on a pre-defined subset of the training set, independent of the model's architecture or the specifics of the training process."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper provides valuable insights into the transfer properties of adversarial robustness. The observation that adversarial robustness can transfer to classes that have never been adversarially attacked during training is intriguing.\n2. The finding that generating AEs on merely 50% of the data can recover most of the baseline AT performance, especially on large datasets like ImageNet is insightful. This could potentially lead to significant computational savings without compromising the robustness of the model.\n3. The paper's findings are not limited to a single task or dataset. The authors have undertaken a thorough experimental evaluation across multiple datasets."
            },
            "weaknesses": {
                "value": "1. While this paper presents intriguing empirical results on the SAT approach, it falls short of providing a clear explanation for the observed transferability of adversarial robustness from subset A to subset B."
            },
            "questions": {
                "value": "1. Can you provide more theoretical justification or intuitive explanations for the observed efficacy of constructing AEs on only a subset of the training data? Specifically, what underpins the phenomenon where harder examples offer better robustness transfer?\n2. You mentioned that as dataset complexity increases, the trend of harder examples providing better robustness transfer diminishes. Can you explain the reasons behind this observation? Are there specific characteristics or properties of complex datasets that might be influencing this behavior?\n3. Could you explain more on why the robustness transfer notably increases for smaller $\\epsilon$ and decreases for larger $\\epsilon$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5502/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5502/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5502/Reviewer_3h75"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5502/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698733591175,
        "cdate": 1698733591175,
        "tmdate": 1699636562605,
        "mdate": 1699636562605,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JE3vz5YBWE",
        "forum": "pE6gWrASQm",
        "replyto": "pE6gWrASQm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5502/Reviewer_gZFF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5502/Reviewer_gZFF"
        ],
        "content": {
            "summary": {
                "value": "This paper conducts extensive experiments to see how only perturbing a subset of adversarial examples in training impacts adversarial robustness.  The authors find that using adversarial examples from certain classes can lead to nontrivial gains in robustness in other classes (despite not training with those classes).  The authors find that the most useful examples/classes to train with are correlated with their difficulty which they measure using entropy."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- paper is very clear\n- great scope in experiments which encompass multiple datasets and model architectures\n- experiments clearly demonstrate correlation between entropy and robust accuracy on the subset\n- experiments suggest that in certain settings, we can train with a smaller subset of adversarial examples instead of all adversarial examples which can reduce the runtime of adversarial training, making it more feasible in practice.  Robustness also transfers to other datasets as well suggesting that this approach can be used with pretraining models."
            },
            "weaknesses": {
                "value": "- while it's clear that training with smaller subsets of adversarial examples can be beneficial, are there guidelines for how to determine the size of this subset to use SAT in practice?"
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5502/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698794083529,
        "cdate": 1698794083529,
        "tmdate": 1699636562491,
        "mdate": 1699636562491,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oUvCbrL4Sr",
        "forum": "pE6gWrASQm",
        "replyto": "pE6gWrASQm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5502/Reviewer_QzqJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5502/Reviewer_QzqJ"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the impact of adversarial training with a limited subset of data on the robust accuracy of computer vision models. The empirical study shows that a limited amount (30%) of carefully selected data is sufficient to achieve 90% of the robustness of the models. Additionally, the paper explores the transferability to other models of the robustness acquired with their method. Empirically, the paper shows that model robustness is best preserved with a custom-balanced loss and with hard-to-classify examples."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper addresses the relevant problem of the generalization of the acquired robustness to unseen classes, examples, and tasks.\n- The paper is well-written and easy to grasp.\n- The experimental protocol is well described and allows to reproduce the study.\n- The paper provides an in-depth empirical study to support its claim.\n- The paper proposes a method to estimate the transferability of the acquired robustness which can be useful for testing ML models, as new classes and examples can appear after adversarial training in real-world systems."
            },
            "weaknesses": {
                "value": "I have a single but potentially critical concern regarding the significance of this work. In particular, I am unsure what are the benefits of considering a subset of the adversarial examples during adversarial training to improve the efficiency of the process. \n\nAs I understand, the key objective of this paper is to limit the size of the set of examples used for adversarial training, to achieve a similar robust accuracy than with full adversarial training. The reason is that full adversarial training is computationally expensive. Considering the settings described in the experimental protocol, I fail to understand why the proposed process is more efficient than full adversarial training. In both cases, the full datasets need to be labeled and the same amount of computational resources is needed, as only the set of available examples for adversarial training differs, not the total number of examples used during adversarial training (since the subset of adversarial examples is completed with the clean examples not used for generating adversarial examples. One could argue that we save 70% of the time to generate the adversarial examples, but the largest cost still come from model training. Considering the empirical results that demonstrate that the proposed method does not lead to better robust accuracy than full adversarial training, I do not see the added value for model robustness efficiency/effectiveness. It would be good if the authors could clarify the benefits of using only a subset of examples in the adversarial training process."
            },
            "questions": {
                "value": "- What is the exact process of adversarial training used in the experiments? Are all examples adversarially perturbed? Are some examples perturbed and others clean? In what proportion? In total is the number of perturbation executions the same for adversarial training as subset adversarial training? \n\n- What is the objective of transferring the robustness across classes and tasks? We need to retrain models to adapt to these new classes and tasks, isn\u2019t it simpler to adversarial train them at retraining, therefore empirically obtaining a more robust model ?\n\nOther comments:\n\n- On page 6,  lines 6 and 11, of the paragraph left to fig 3, describe the results for the class \u201ccar\u201d while the numerical value corresponds to the line \u201cplane\u201d on fig 2.\n- Figures are not readable when printed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5502/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5502/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5502/Reviewer_QzqJ"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5502/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698828849246,
        "cdate": 1698828849246,
        "tmdate": 1699636562384,
        "mdate": 1699636562384,
        "license": "CC BY 4.0",
        "version": 2
    }
]