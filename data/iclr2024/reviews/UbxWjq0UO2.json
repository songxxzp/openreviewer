[
    {
        "id": "rRtCllL1hJ",
        "forum": "UbxWjq0UO2",
        "replyto": "UbxWjq0UO2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2347/Reviewer_s3AW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2347/Reviewer_s3AW"
        ],
        "content": {
            "summary": {
                "value": "The paper investigates the geometry quality of SDS-based methods. By using an additional control net module that is aware of the sparse depth from point-e/mcc, the model grounds the 3D generation with rough geometry and therefore reduces the janus issues. The proposed pipeline can be integrated with SDS and VSD."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ The depth-aware controlnet module alleviates the janus problem and improves the geometry quality of 3D generation greatly.\n\n+ LoRA is utilized for parameter-efficient fine-tuning when improving the semantic consistency. \n\n+ I played with the code of the proposed framework, and it works well."
            },
            "weaknesses": {
                "value": "- Missing references on image-to-3D generation (CVPR'23 papers, both released on arxiv before the original release of this submission). [1] proposes textual inversion + dreambooth method for improving semantic consistency, similar to Imagic. [2] uses textual inversion for semantic consistency. Would be nice if these approaches were discussed in the related works section, as they are relevant to the semantic consistency component in this work.\n\n[1] Xu D, Jiang Y, Wang P, et al. NeuralLift-360: Lifting an In-the-Wild 2D Photo to a 3D Object With 360deg Views[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 4479-4489.\n\n[2] Melas-Kyriazi L, Laina I, Rupprecht C, et al. Realfusion: 360deg reconstruction of any object from a single image[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 8446-8455."
            },
            "questions": {
                "value": "What are the major differences between the proposed method and the above [1][2] on improving semantic consistency in SDS?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2347/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2347/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2347/Reviewer_s3AW"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2347/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698553635624,
        "cdate": 1698553635624,
        "tmdate": 1700710323864,
        "mdate": 1700710323864,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wErz6hPtk4",
        "forum": "UbxWjq0UO2",
        "replyto": "UbxWjq0UO2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2347/Reviewer_MdgG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2347/Reviewer_MdgG"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces 3DFuse, a new method to integrate 3D consistency information into the SDS loss optimization pipeline of text-to-3D generation. The authors first fine-tune the Stable Diffusion model to understand sparse depth conditions. \nDuring SDS optimization, 3DFuse reconstructs a coarse point cloud from an image using off-the-shell methods and then renders sparse depth maps from the point clouds for depth-conditioned SDS loss supervision.\nBoth qualitative and quantitative analyses underscore the efficacy of conditioning based on coarse point clouds, resulting in enhancements in the text-to-3D generation results. Additionally, the authors have unveiled a semantic coding method to tackle inherent semantic ambiguities."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. 3DFuse offers a robust solution to fine-tune a sparse depth-conditioned Stable diffusion model, employing the Co3D dataset. The result in Fig.3 provides a compelling argument for the performance of the depth-conditioned diffusion model. Furthermore, depth-conditioned SDS loss augments the 3D consistency of generated results, as depicted in Fig.4.\n2. The authors explore the semantic ambiguity problem, which has often been overlooked in prior studies. They introduced semantic code sampling to mitigate this challenge, and Fig.12 convincingly showcases the effectiveness of this strategy.\n3. The authors conduct comprehensive ablation studies and extensive experimental validation. Most of the proposed components are well-ablated. In addition, the authors also provide a user study in Fig.8.\n4. This paper boasts of a coherent and lucid narrative structure, facilitating easy comprehension."
            },
            "weaknesses": {
                "value": "1. 3DFuse relies heavily on off-the-shelf point cloud reconstruction or generation methods like Point-E, to obtain coarse point clouds. These point clouds subsequently serve as conditional information in the SDS optimization process. However, sparse-depth renderings are low-quality and may have many artifacts, resulting in degenerated and ambiguous results as shown in Fig.4.\n2. The authors provide the novel view synthesis results compared with Zero-123 in Fig.10. However, 3DFuse fails to fit the reference image and generates a different object from it. Furthermore, the predicted novel views are also blurry.\n3. The quality of 3DFuse's results lags behind contemporaneous methods. Additionally, it is noteworthy that the results for benchmark methods, such as Dreamfusion and ProlificDreamer, presented in this paper, appear to be less robust than those delineated in their original publications and open-source implementations. It would be beneficial for the authors to recalibrate their methodology based on more formidable baseline models."
            },
            "questions": {
                "value": "NA"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2347/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698837952045,
        "cdate": 1698837952045,
        "tmdate": 1699636166941,
        "mdate": 1699636166941,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ogBw9GTIgo",
        "forum": "UbxWjq0UO2",
        "replyto": "UbxWjq0UO2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2347/Reviewer_ftV7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2347/Reviewer_ftV7"
        ],
        "content": {
            "summary": {
                "value": "This work introduces a new framework called 3DFuse that enhances the robustness and 3D consistency of score distillation-based methods for text-to-3D generation. The framework incorporates 3D awareness into the pretrained 2D diffusion model, resulting in geometrically consistent and coherent 3D scenes. The authors also introduce a new technique called semantic coding for improved results. The effectiveness of the framework is demonstrated through qualitative analyses and ablation studies, and it is shown to outperform previous prompt-based methods in terms of precision and controllability of injected 3D awareness. The 3DFuse framework and semantic coding technique have the potential to improve the quality and controllability of generated 3D scenes, which could have applications in various fields such as virtual reality, gaming, and architecture."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The authors introduce a novel framework called 3DFuse that enhances the robustness and 3D consistency of score distillation-based methods for text-to-3D generation. This framework incorporates 3D awareness into the pretrained 2D diffusion model, resulting in geometrically consistent and coherent 3D scenes.\n\n- The authors introduce a new technique called semantic coding that involves generating an initial image based on the text prompt and optimizing the corresponding text prompt embedding. This technique improves the quality and controllability of generated 3D scenes.\n\n- The authors demonstrate the effectiveness of the 3DFuse framework through qualitative analyses, which show that it outperforms previous prompt-based methods in terms of precision and controllability of injected 3D awareness.\n\n- The 3DFuse framework and semantic coding technique have the potential to improve the quality and controllability of generated 3D scenes"
            },
            "weaknesses": {
                "value": "- My biggest concern is the marginal improvement. It seems that the proposed model only shows a very limited improvement compared to each baseline method.\n\n- My understanding is that the video from supp is generated by render well-optimized NeRF, which should be inherently. But why does the video itself look aliasing?\n\n- The proposed framework breaks the text-to-3D problem into text-to-image + image-to-3D tasks. Therefore, it would be better to compare it with other image-to-3D methods [a, b] too.\n\nReferences:\n\n[a] Zero-1-to-3: Zero-shot One Image to 3D Object\n\n[b] Magic123: One Image to High-Quality 3D Object Generation Using Both 2D and 3D Diffusion Priors"
            },
            "questions": {
                "value": "- How stable is the 3D point cloud generative model? Any failure cases of predicted 3D point cloud? How to handle it?\n\n- Why should it predict a point cloud and then convert it to a depth map? What if the proposed module directly predicts a monocular depth map?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2347/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699342640052,
        "cdate": 1699342640052,
        "tmdate": 1699636166875,
        "mdate": 1699636166875,
        "license": "CC BY 4.0",
        "version": 2
    }
]