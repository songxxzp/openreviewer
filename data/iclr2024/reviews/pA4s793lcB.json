[
    {
        "id": "SHsQTuahh7",
        "forum": "pA4s793lcB",
        "replyto": "pA4s793lcB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5545/Reviewer_apnD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5545/Reviewer_apnD"
        ],
        "content": {
            "summary": {
                "value": "This paper designs replicable algorithms for multi-armed bandits and linear bandits, i.e., with a fixed inner random variable, the vector of the pulled arms will remain the same with high probability, even if the outcomes of arms are also random. The authors adapt the RASMAB algorithm framework in Esfandiari et al. (2023), and modify the way of eliminating arms to get new algorithms REC and RSE. In REC, the algorithm only tries to eliminate all the arms in one single phase. This leads to an $O(\\sum_i {\\Delta_i \\log T \\over \\Delta_{\\min}^2 \\rho^2})$ regret upper bound, which is better than existing algorithms when $\\Delta_{\\min} \\ge \\Delta_i/K$ for all arm $i$. Then the authors mix REC and RASMAB to get RSE, which achieves a regret upper bound that equals the minimum of the regret upper bounds of REC and RASUCB. As for the linear bandits case, the authors also provide an RLSE algorithm. Finally, they use some simulation results to demonstrate the effectiveness of their algorithms."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is clearly written, and there are some new regret upper/lower bounds."
            },
            "weaknesses": {
                "value": "My main concern is that the writing of proofs is not good enough. In fact, I only check one proof in the appendix (Theorem 5), but I find it hard to understand some of the steps.\n\nSpecifically, Eq. (75) and Eq. (76) only say that $P_{-\\Delta, U}[N_2(T) \\ge N_C] \\ge {1\\over 4}$ and $P_{\\Delta, U}[N_2(T) \\ge N_C] \\ge {3\\over 4}$. This cannot ensure that there exists some $r \\in [-\\Delta, \\Delta]$ such that $P_{r, U}[N_2(T) \\ge N_C] = {1\\over 2}$. For example, perhaps $P_{r, U}[N_2(T) \\ge N_C] = 1$ for all $r \\in [-\\Delta, \\Delta]$.\n\nThere are also many typos in the proof (which makes it more complicated to understand the steps), e.g., in the line after Eq. (75), I think it should be $Regret_{-\\Delta,U}(T) \\ge N_2(T)\\Delta$ but not $Regret_{-\\Delta,U}(T) \\ge N_2(T)\\Delta/4$, and it should be $N_C \\le {T\\over 2}$ but not $N_2(T) \\le {T\\over 2}$. \n\nI really suggest the authors revise their proofs to make sure that they are correct and easy to understand. \n\nAlso, I think some of the proof sketches should be given in the main text.\n\nBesides, both the analysis and experiments show that REC (or RSE) does not dominate RASMAB, and I am also wondering the experiments of performance comparison between RLSE and existing benchmarks. \n\n=============================\n\nI would like to raise my score if my concern on the proof is addressed.\n\n\n=============================\n\nThe proof after revision looks clear now. I change my score to 6."
            },
            "questions": {
                "value": "See the above \"Weaknesses\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5545/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5545/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5545/Reviewer_apnD"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5545/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698209031623,
        "cdate": 1698209031623,
        "tmdate": 1700451628214,
        "mdate": 1700451628214,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CIuSi9xFRG",
        "forum": "pA4s793lcB",
        "replyto": "pA4s793lcB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5545/Reviewer_Hx4P"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5545/Reviewer_Hx4P"
        ],
        "content": {
            "summary": {
                "value": "This work analyzed the replicable bandits. The replicable bandit algorithms were first analyzed by Esfandiari et al. (2023). This work proposed the REC and RSE algorithm to solve the regret minimization problem in the standard replicable bandits and also studied the linear setting. It provided theoretical analysis on for the proposed algorithm and numerical comparison among algorithms.\n\n\n==========\n\nThanks for response from author(s). I prefer to keep my score so far."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is generally well organized and easy to follow.\n1. The problem is clearly formulated.\n2. Numerical experiments are conducted to evaluate the performance of algorithms.\n1. This work provides extensive study on the replicable bandits."
            },
            "weaknesses": {
                "value": "Title:\n1. The title of this work starts with 'Improved algorithms'. I doubt whether it is an appropriate title. I feel that this work is more likely to be an extensive study on replicable bandits.\n    1. According to Table 1, the REC and RSE algorithms are only better than the existing RASMAB in the instances where $K^2/\\Delta^i$ is larger than $\\Delta^2_i/\\Delta$. In other cases, the order of distribution-dependent upper bounds of these three algorithm seem to be the same.\n    1. Numerical results imply that RASMAB algorithm outperforms the REC and RSE algorithms in Model 1. I appreciate some explanation regrading this point.\n\nApplication:\n1. I understand it is a follow-up work on the topic of replicable bandits. However, if the aim is to handle situations where we have simply vague data, I think we can also apply differentially private stochastic bandits (https://proceedings.mlr.press/v180/hu22a/hu22a.pd). May you clarify the difference between application of these two settings? What are their individual strengths/weaknesses?\n\nContributions:\n1. The distribution-independent bound is only provided for the RSE algorithm. However, it is usually not so difficult to derive a distribution-independent bound for the other algorithms when the distribution-dependent ones already exist. I suggest the author(s) to provide distribution-independent bounds for other algorithms and a comparison or explain the analytical challenge.\n1. As the lower bound for the standard setting is provided, may the author(s) discuss the gap between upper and lower bounds?\n1. I appreciate the efforts to study the linear setting, may the author(s) also provide a lower bound (or explain the difficulty)?\n1. As the linear algorithm is proposed for an instance with a large number of arms, may the author(s) compare(s) the performance of linear and nonlinear algorithms in a numerical experiment with a large $K$?\n1. Section 1.2 claimed that 'we introduce the Replicable Successive Elimination (RSE) algorithm whose regret bound is the minimum\nof those of REC and the existing algorithms.'\n    1. From Table 1, I cannot tell RSE is obviously better than REC.\n    1. According to experiments, REC seems to be always slightly better than RSE.\n    1. If RSE is better than REC, why are we interested in the REC algorithm?\n\nMinor suggestion:\n1. I appreciate the comparison among bounds in Table 1 but have a minor suggestion: it would be better to include the name of algorithms( and the theorems index), and rearrange the table."
            },
            "questions": {
                "value": "Please refer to the **Weaknesses** section for questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5545/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5545/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5545/Reviewer_Hx4P"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5545/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698723365230,
        "cdate": 1698723365230,
        "tmdate": 1700622056983,
        "mdate": 1700622056983,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0Os0VzHetI",
        "forum": "pA4s793lcB",
        "replyto": "pA4s793lcB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5545/Reviewer_a2Sd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5545/Reviewer_a2Sd"
        ],
        "content": {
            "summary": {
                "value": "This paper considers the problem of replicability in the context of stochastic bandits. Following the definition of Impagliazzo et al. '22, and its adaptation to the bandit setting of Esfandiari et al. '23, a bandit algorithm is called replicable if, with probability at least 1-$\\rho$, it pulls the exact same sequence of arms when executed twice in the same bandit environment when its internal randomness is shared but the rewards of the arms are drawn independently across the two executions.\n\nIn the multi-armed bandit setting, the authors provide algorithms with an instance-dependent bound of $O(K)$, where $K$ is the number of different arms, and instance-independent bound of $O(K^{1.5})$. Moreover, they provide lower bounds which show that the dependence on the replicability parameter $\\rho$ is tight. \n\nThen, the authors consider the linear bandit setting and they provide algorithms whose instance-dependent regret scales as $O(d)$ and the instance-independent regret scales as $O(K^{1.5}d^{0.5})$."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "-The paper presents an algorithm that gets $O(K)$ instance-dependent bound in the multi-armed bandit setting.\n\n-The authors provide a best of both worlds type of algorithm that achieves the best bound between their algorithm and the one in Esfandiari et al. '23.\n\n-An instance-independent regret analysis of the algorithm is provided. \n\n-A lower bound on the regret is given which establishes the optimality on the dependence of the replicability parameter $\\rho$. In my opinion, this is the most technically interesting part of the paper.\n\n-Similar upper bounds are provided in the linear bandit setting. The instance-independent analysis provided in this work uses a different regret decomposition from Esfandiari et al. '23, which allows the authors to get rid of a factor $K/\\rho$ in the regret bound."
            },
            "weaknesses": {
                "value": "After reading the abstract and the introduction of the paper, I was excited to learn more about the results. However, I think there are several issues that make the results less exciting than they appear to be.\n\nLet me start by explaining the main technical challenges in this line of work on replicable algorithms. The bandit algorithms of Esfandiari et al. '23 and of this paper use multiple calls to the replicable mean estimation algorithm of Impagliazzo et al. '22. Let $N$ be the number of times this algorithm is called. Because of a union bound over the probability of non-replication, the sample complexity overhead scales as $O(1/(\\rho/N)^2)$. For this reason, both Esfandiari et al. '23, and this paper using a batching approach and call the replicable mean estimation subroutine (or a randomized thresholding scheme that closely resembles it) at the end of each batch. The reason why Esfandiari et al. '23 incur an extra overhead of $O(K^2)$ in the regret compared to the non-replicable setting is, essentially, a union bound over the number of arms. Let me now move on to pointing specific weaknesses of the paper.\n\n-Abstract: \"Existing algorithms require a regret scale of $O(K^3)$, which increases much faster than the number of actions (or \u201carms\u201d), denoted as K. We introduce an algorithm with a distribution-dependent regret of $O(K)$\" -> both bounds are distribution-dependent, but are incomparable since the bound of Esfandiari et al. has better dependence on the distribution-dependent parameters than the one in the current paper. I think the way it is stated in the abstract is a bit confusing.\n\n-Abstract: \"Additionally, we propose an algorithm for the linear bandit with regret of $O(d)$, which is linear in the dimension of associated features, denoted as d, and it is independent of K.\" -> this bound is distribution-dependent, I think it should be mentioned.\n\n-Abstract: \"Our algorithms exhibit substantial simplicity compared to existing ones\" -> I don't agree with the statement, I will elaborate shortly.\n\n-Page 2: \"Upon closer examination of the problem, we discovered that $K^2$ factor can be eliminated\" -> as I mentioned before, the bounds are incomparable. I think at first read this sentence gives the impression that the regret bound is improved.\n\n-Page 2-3: \"Furthermore, due to the simplicity of the algorithm, we establish the first distribution-independent regret bound for the replicable K-armed bandit problem.\" -> this claim is imprecise. Esfandiari et al.'23 have already established distribution-independent bounds for linear bandits (without any restriction on the relationship between d, K), which is a more general setting than multi-armed bandits. As I said before, I don't agree with the statement that the algorithms are \"simpler\" either.\n\n-Section 3: I believe that the non-replication framework is inspired by the long line of work on batched bandits and bandits with low adaptation, so I think some citations are needed. Moreover, the same batching framework was essentially used in Esfandiari et al. '23 (even though it wasn't stated as such, its essence remains the same). I don't the benefit of stating it as an abstract framework. I think the authors could move some of this discussion to the appendix and elaborate more on their proofs technique in the main body.\n\n-Section 3: The set $|A_{p+1}|$ which I assume denotes the set of active arms is not defined (please correct me if I have missed it).\n\n-Section 4: The explore-then-commit algorithm was mentioned in the warm-up section of Esfandiari et al. for the case where the suboptimality gap $\\Delta$ is given to the designer and a sketch of the regret analysis was also provided. Essentially, Algorithm 2 of the current submission gets rid of the assumption that $\\Delta$ is known by using the doubling-trick approach. I believe some discussion about it would be useful,\n\n-Section 5: Essentially, this algorithm is a best of both worlds combination of Algorithm 2 and the replicable arm elimination algorithm of Esfandiari et al. '23. It is easy to see that if the algorithm detects a large gap between the two best arms, it just eliminates all of them except for the best, otherwise it performs the replicable arm elimination process of Esfandiari et al. '23. I think this comparison should be mentioned in the text. Given this discussion, I don't see where the simplicity of this algorithm compared to Esfandiari et al. '23 lies. \n\n-Section 5: \"Here, eliminating all but one arm is equivalent\nto switching to the exploration period.\" -> exploitation.\n\n-Section 5: \"Moreover,\nthis algorithm is the first replicable algorithm that has a distribution-independent regret bound in the\nK-armed bandit problem.\" -> As I mentioned before, this is not correct. Since the distribution-independent bounds for linear bandits of Esfandiari et al. hold even when $K = d$ they already imply distribution-independent regret bound for the K-armed bandit problem.\n\n-Section 7: \"Next, we consider the linear bandit problem, a special version of the K-armed bandit problem\nwhere associated information is available.\" -> if $d = K$ then the linear bandit problem can express the multi-armed bandit problem, so maybe it should be emphasized that $d << K.$\n\n-Section 7: \"The main innovation here is to use the G-optimal design that\nexplores all dimensions in an efficient way.\" -> I think it should be mentioned that this innovation has already been done in the literature since it is a very well-known technique and is not an innovative element of the current submission.\n\n-Lemma 7: The definition of $\\hat{\\theta}_p$ is missing. I think it's worth discussing that even though $\\hat{\\theta}_p$ is different across two executions, because of the different observed rewards, the algorithm is still replicable. \n\n-Related work: Throughout the draft, the citation to Esfandiari et al. '23 is not the indented one. In fact, the citation to the correct paper is absent from the references. \n\n-Conclusion: \"This represents a significant advancement over existing algorithms.\" -> as I explained before, the bounds are not comparable. \n\n-In general, the authors could try to add some proof sketches in the main body. If need be, they could try to shorten the discussion about the replicability framework, which I don't think adds much to the paper.\n\n-Not a weakness, but relevant the the related works section: Some other references that could be useful:\n\"List and Certificate Complexities in Replicable Learning\", Peter Dixon, A. Pavan, Jason Vander Woude, N. V. Vinodchandran\n\"Replicability and stability in learning\", Zachary Chase, Shay Moran, Amir Yehudayoff\nBoth of these works study a different notion of replicability that has to do with the number of different outputs of a learning algorithm.\n\n\"Replicable Reinforcement Learning\", Eric Eaton, Marcel Hussing, Michael Kearns, Jessica Sorrell\n\"Replicability in Reinforcement Learning\", Amin Karbasi, Grigoris Velegkas, Lin F. Yang, Felix Zhou\nBoth of these papers study replicable algorithms for RL.\n\n\"Statistical Indistinguishability of Learning Algorithms\", Alkis Kalavasis, Amin Karbasi, Shay Moran, Grigoris Velegkas\nThis paper provides a relaxation of the replicability definition and extends some of the equivalences shown in Bun et al. '23 to uncountable domains."
            },
            "questions": {
                "value": "-See weaknesses section.\n\n-Instance-independent regret bound for linear vs. multi-armed bandits: for linear bandits the regret bound is $O(K/\\rho \\sqrt{dKT\\log T})$ whereas for multi-armed bandits it is $O(K/\\rho \\sqrt{KT\\log T})$. Isn't the former always worse even though there is more structure?\n\n-Appendix F.2, proof of the (A) case. Isn't there an additive term O(K) missing in the regret bound?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5545/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5545/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5545/Reviewer_a2Sd"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5545/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698821006749,
        "cdate": 1698821006749,
        "tmdate": 1700709741027,
        "mdate": 1700709741027,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2wSpeSfOs1",
        "forum": "pA4s793lcB",
        "replyto": "pA4s793lcB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5545/Reviewer_tkZN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5545/Reviewer_tkZN"
        ],
        "content": {
            "summary": {
                "value": "This paper considers the problem of replicability in stochastic and linear stochastic bandits. Specifically, the $\\rho$-replicability criterion requires the sequence of actions chosen by the algorithm in two independent runs over the same data generating process be identical with probability at least $1-\\rho$, where $\\rho$ is a given replicability parameter, and the probabilities are taken with respect to the internal randomization of the algorithm and randomness in the data generating process. This work considers this criterion in the context of regret minimization in stochastic, and linear stochastic bandits. This paper provides 3 key results: (a) they provide a natural algorithm that achieves an instance dependent expected cumulative regret of $O\\left(\\rho^{-2}\\log T\\cdot\\min\\left(\\{\\sum_{i\\neq i^*} \\frac{\\Delta_i}{\\Delta^2},\\sum_{i\\neq i^*}\\frac{K^2}{\\Delta_i}\\}\\right)\\right)$, which, depending on the instance improves upon the existing known regret upper bound of $O(\\rho^{-2}\\log T\\cdot\\sum_{i\\neq i^*} \\frac{K^2}{\\Delta_i})$. They additionally provide an instance independent regret upper bound of $O(\\rho^{-1}K\\sqrt{KT\\log T})$ achieved by the same algorithm. (b) They provide a 2-arm regret lower bound of $\\Omega(\\rho^{-2} \\Delta^{-1}\\log^{-1}(1/\\rho\\Delta))$ for any $\\rho$-replicable algorithm for stochastic bandits. (c) Lastly, for the case of linear stochastic bandits, they provide an algorithm that builds upon the replicable algorithm for stochastic bandits that achieves an instance dependent regret upper bound of $O(\\rho^{-2}\\Delta^{-2}d\\log T)$, and an instance independent regret upper bound of $O(\\rho^{-1}K\\sqrt{dKT\\log T})$."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "Overall, the paper is well written and easy to read. The algorithms designed are natural and quite easy to understand."
            },
            "weaknesses": {
                "value": "To be very honest, I am not too enthusiastic about this result. I have serious questions about the motivation for this particular problem - I don't see the practical need for exact replicability in the bandit setting, as it seems too stringent a condition, and existing ideas (which I shall elaborate shortly) already provide \"near\"-replicable algorithms for this particular problem at no additional loss in regret. From a technical perspective, I don't see any new technical ideas being developed in this piece of work. The algorithms provided are pretty much a straightforward application of the batched bandits idea of Perchet et. al. (Annals of Statistics, 2016) + its other follow-ups. This is also an obvious connection because the batched bandits framework already provides near-replicability: very arm $i\\neq i^*$ essentially has a fixed optimal epoch when the number of plays exceeds $\\tilde{O}(\\Delta_i^{-2})$ for the first time so it becomes \"distinguishable\" from the best arm. We have the high probability guarantee that every arm will necessarily be played up until the optimal epoch and will necessarily be discarded at the end of the epoch following the optimal epoch (by slightly strengthening the rejection condition for an arm to be something like the empirical means are separated by 3x the size of the confidence interval in that epoch, and increasing the number of samples obtained from active arms such that the size of the confidence interval shrinks by a factor of 1/5 across successive epochs - this will effectively guarantee that there is only one \"uncertain\" epoch when an arm may or may not be rejected, before which the arm will necessarily be played and after which the arm will necessarily be rejected. This slight modification of the rejection condition affects cumulative regret by only a small constant). Therefore for every arm, the behavior of the algorithm is near-deterministic and hence replicable (with polynomially high probability in $T$) for all epochs except one epoch where anything can happen (the optimal epoch where this arm becomes distinguishable for the first time. In this epoch, it is hard to argue the nature of the overlap/non-overlap in the confidence intervals since the size of the confidence intervals is at the same scale -- up to small constants -- as the gap between the rewards). One might argue that this is already good enough for most practical purposes - the number of times any arm will be played will be within constant factor of each other across any two runs of the algorithm with any desired polynomially large probability (in $1/T$). In fact, across any two runs of the algorithm, we have the additional guarantee that for any arm, the number of times the arm is played takes only 2 possible values! One can add additional randomness by randomizing the width of the confidence interval in this one epoch (you don't need to know the identity of the bad epoch. you can essentially randomize confidence intervals in each epoch and the strong separation or overlap in confidence intervals in other epochs guarantees that this randomization effectively only kicks in at this one potentially bad epoch where it is hard to argue what the algorithm does) to explicitly get a handle on what the probability of non-replicability looks like. The instance-independent regret bounds follow by an identical analysis (essentially the same analysis as batched bandits), the algorithm and analysis for linear bandits follows pretty much the same way. The lower bound is also extremely weak. These are not at all novel ideas, and I find it hard to argue acceptance for these results."
            },
            "questions": {
                "value": "What is novel in this work? I would request the authors to improve their upper bounds (which I am almost certain is possible with a finer analysis exploiting subgaussianity and a cleverer randomization strategy for rejection in an epoch), as well as provide tighter lower bounds."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5545/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5545/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5545/Reviewer_tkZN"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5545/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699127325875,
        "cdate": 1699127325875,
        "tmdate": 1699636569517,
        "mdate": 1699636569517,
        "license": "CC BY 4.0",
        "version": 2
    }
]