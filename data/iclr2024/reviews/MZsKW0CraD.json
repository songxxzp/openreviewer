[
    {
        "id": "OQFJ9WPZ9h",
        "forum": "MZsKW0CraD",
        "replyto": "MZsKW0CraD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5366/Reviewer_ersf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5366/Reviewer_ersf"
        ],
        "content": {
            "summary": {
                "value": "The paper examines model extraction attacks, which aim to steal machine learning models via query access. It argues that the practical threat of these attacks is often overstated, as attackers frequently require prior access to in-distribution data. When such data is available, it is often more cost-effective to train a model from scratch, reducing the incentive for model extraction. The paper introduces a new benchmark to assess attacker knowledge and control the impact of out-of-distribution queries."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper highlights the potential overestimation of the practical threat associated with such attacks.\n- Use of illustrative examples to enhance understanding.\n- Discuss the impact of surrogate datasets on model extraction performance."
            },
            "weaknesses": {
                "value": "- The paper's linear classification example may not align perfectly with the concept of accuracy.\n- The rationale behind the mixed use of original and surrogate datasets requires further clarification."
            },
            "questions": {
                "value": "- The authors utilize linear classification as an illustrative example to convey the idea that in model extraction (ME) attacks, adversaries require either prior knowledge about the data distribution or the ability to make a substantial number of queries to the target model. However, in my interpretation, this example primarily underscores that these conditions are crucial for preserving high fidelity rather than accuracy. The focus of the adversary in this case centers on replicating the decision boundary. While accuracy and fidelity share similarities in certain contexts, distinct decision boundaries may result in comparable performance but differ in fidelity. Consequently, it would be more appropriate for the authors to place greater emphasis on the concept of fidelity in their paper.\n\n- In Section 4.3, the authors discuss the ME performance gains from the use of surrogate datasets. However, they don't just use surrogate data alone; instead, they mix part of the original dataset with the surrogate data. It would be beneficial to understand the rationale behind this mixed approach."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5366/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5366/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5366/Reviewer_ersf"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5366/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698593763011,
        "cdate": 1698593763011,
        "tmdate": 1699636541524,
        "mdate": 1699636541524,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RF5R4xJzbn",
        "forum": "MZsKW0CraD",
        "replyto": "MZsKW0CraD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5366/Reviewer_5YY3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5366/Reviewer_5YY3"
        ],
        "content": {
            "summary": {
                "value": "This paper conducts the measurement on how data access assumptions influence the accuracy of the extracted models, and how this changes adversary's incentives. Evaluation shows that victim labels leak limited information about the decision boundaries. The paper also discusses the possible defense based on the findings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Trendy topic\n- New perspective for ME attacks"
            },
            "weaknesses": {
                "value": "- More evaluation is needed\n- Presentation can be improved\n- Lack of certain theoretical support"
            },
            "questions": {
                "value": "This paper presents the measurement results of model extraction attacks from a novel perspective. The authors argues that \nwith different access to the original training data, the performance of the model extraction attacks can be influenced. The paper \ninspired researchers to think about model extraction attacks from another perspectives. However, it still has some flaws. I list my concerns in the followings:\n\n- Based on my understanding, it is a measurement paper. Therefore, to achieve the conclusion, more experimental settings should be considered. For instance, if the architecture of the target models will influence the attack performance, or whether the target model that is trained under unsupervised loss may exhibit different results. I recommend the authors consider more settings for the comprehensive measurement conclusion.\n\n- The writing needs to be improved. It seems that the warm up part is not necessarily connected to the following evaluation part. Also, in this paper, model stealing attacks and model extraction attacks are used confusingly. In the experimental results part, the main conclusion is also not that obvious.\n\n- Besides the experimental results, a certain degree of theoretical proof is necessary. Note that the proof is not the illustration presented in the warm up part. I would suggest the authors explain the commonality of ME attacks more rigorously."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5366/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698717599863,
        "cdate": 1698717599863,
        "tmdate": 1699636541401,
        "mdate": 1699636541401,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "N37UJbIbsa",
        "forum": "MZsKW0CraD",
        "replyto": "MZsKW0CraD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5366/Reviewer_7cgJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5366/Reviewer_7cgJ"
        ],
        "content": {
            "summary": {
                "value": "This paper studies model extraction attacks that aim to steal trained models with query access. It challenges the assumption that these attacks save on data costs, showing that they often rely on prior knowledge of in-distribution data, limiting their practical utility. In addition, the authors emphasize the need to separate prior knowledge from the attack policy and suggests a benchmark for evaluating attack policies directly."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Studying model extraction attacks is a crucial avenue in the study of adversarial attacks, given the resource-intensive nature of training ML/DL models.\n- The paper is easy to follow."
            },
            "weaknesses": {
                "value": "- While the authors draw conclusions from a diverse set of experiments, there appears to be a need for a principled approach to assess model extraction attacks. It would be valuable if the authors could provide clear and concise definitions that offer a unified perspective on all the attacks and defenses discussed in their paper. Currently, the experimental findings, while intuitive, seem somewhat fragmented and lack a well-organized presentation.\n- Similar challenges are evident in the comparison between OOD and IND. If the authors could provide a clear and concise mathematical definition aligned with the objectives of model extraction attacks, it could offer deeper insights.\n\nIn summary, while the paper presents intriguing empirical findings, there is a need for the authors to establish straightforward and principled concepts to enhance the clarity of their arguments and conclusions."
            },
            "questions": {
                "value": "-"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5366/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698792028496,
        "cdate": 1698792028496,
        "tmdate": 1699636541293,
        "mdate": 1699636541293,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "d78JLpifjY",
        "forum": "MZsKW0CraD",
        "replyto": "MZsKW0CraD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5366/Reviewer_HWBR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5366/Reviewer_HWBR"
        ],
        "content": {
            "summary": {
                "value": "The paper investigates query-based model extraction attacks, where the objective is to train a substitute model that exhibits similar performance to the stolen one. The authors argue about the real advantages of performing this kind of attack, especially considering the requirement of a surrogate dataset from the same distribution of the victim training data. Moreover, they propose an OOD-based approach, that avoids unveiling the actual model decision boundaries to out-of-distribution queries, assuming the attacker mostly relies on them to build its surrogate dataset, as a benchmark to evaluate how the availability of data influences the attack success."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The literature on this topic is scattered, and it is very difficult to have a clear assessment of the real-world impact of model extraction attacks, as the considered settings can be very different in their assumptions (knowledge of model architecture, training data distribution, preprocessing steps, output access, application domain, etc.). Often, even comparing different attacks and defenses is not straightforward. This work has the merit of trying to address some of these issues."
            },
            "weaknesses": {
                "value": "Although the considerations and the findings of the paper are very interesting, its contribution seems limited by the considered case studies, whereas the conclusions drawn by the authors are supposed to be generally applied.\n\nModel stealing attacks can be performed with different purposes and settings, but in this work, the analyzed ones and experimental evaluation include a very tiny set of them: for instance, the attacker might easily obtain some data from the training distribution (or similar ones), a pre-trained model might be already available, in some domains it is not possible to use automatic labeling services, and in general the cost of query the victim model and make a sample labeled by a third-party service might widely vary.\nSeveral attacks cover these or other settings (see [a, b]) and some of them seem very practical in a real-world scenario. In addition, as this research field is relatively recent, some of the authors' claims might become less strong as new more efficient attacks are designed: for instance, a recent work [c] shows that there is still room for improvements in this sense.\n\nRegarding the evaluation using additional data samples from different distributions: I don't expect that simply using them to train the substitute model after being labeled from the victim can give high improvements, whereas several attacks use them as a starting point to build a more efficient surrogate training set with some optimization technique (similar to active learning). The authors should have considered these attacks to evaluate how the availability of these data influences the attack's success. A similar consideration can be made with respect to the knowledge of a very small fraction of the training data.\n\n[a] Oliynyk, D., Mayer, R., & Rauber, A. (2022). I Know What You Trained Last Summer: A Survey on Stealing Machine Learning Models and Defences. ACM Computing Surveys, 55, 1 - 41.\n[b] Yu, H., Yang, K., Zhang, T., Tsai, Y., Ho, T., & Jin, Y. (2020). CloudLeak: Large-Scale Deep Learning Models Stealing Through Adversarial Examples. Network and Distributed System Security Symposium.\n[c] Lin, Z., Xu, K., Fang, C., Zheng, H., Ahmed Jaheezuddin, A., & Shi, J. (2023). QUDA: Query-Limited Data-Free Model Extraction. Proceedings of the 2023 ACM Asia Conference on Computer and Communications Security."
            },
            "questions": {
                "value": "Can you please extend the overall discussion and the experiments to a wider range of settings and attacks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5366/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5366/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5366/Reviewer_HWBR"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5366/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698837982684,
        "cdate": 1698837982684,
        "tmdate": 1699636541185,
        "mdate": 1699636541185,
        "license": "CC BY 4.0",
        "version": 2
    }
]