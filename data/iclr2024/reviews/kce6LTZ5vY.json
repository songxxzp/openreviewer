[
    {
        "id": "OmLy2WZ5Ww",
        "forum": "kce6LTZ5vY",
        "replyto": "kce6LTZ5vY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4082/Reviewer_CNR2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4082/Reviewer_CNR2"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a technique, InstructionMining, to analyse the performance of a set of instruction-tuned models to identify the features that best correlate with model performance (as measured by loss on a validation set). They also apply BlendSearch to find the optimal data size to select. They show that selecting data with InstructMining results in models that perform as well or better than models trained on random data and much larger datasets (e.g. StableBeluga). They also find that dataset size and model performance display double descent, where increasing data helps, then hurts, then helps again, counter to intuition (that more data is always better)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The method is sound, and investigating how to determine the \u2018quality\u2019 of a dataset without training a model is interesting and impactful for the community. The exploration of different indicators (and ablating them in 4.3) is interesting and has some useful takeaways, suggesting that just examining the naturalness and understandability of responses is enough to detect quality data.\n- The GPT -4-based preference results suggest that the model trained on the InstructMining selected data performs better than random selection, and is similar in quality to Vicuna 1.5 7B.\n- The results investigating dataset size and model performance are interesting and provide interesting guidance for future efforts in collecting data, as well as (partially) validating the LIMA hypothesis that fewer, but higher-quality, samples are preferable to a large amount of data."
            },
            "weaknesses": {
                "value": "- The method is computed over loss, but it is unclear how this translates to eval task performance. Appendix C presents a somewhat weak argument - it would be good to e.g. strongly correlate the inference loss with MMLU/MT-Bench/etc results. It\u2019s non-obvious that this is true in instruction tuning setups due to the lack of hard gold labels. There may be many potential outputs that are equally valid, and only one is generated as the gold label by GPT-4. This also makes it unclear to determine what differences in loss are significant - for example, in Table 3, Random 150(Train) gets .0085 higher loss than Selected 22(Rule) + 150(Train). Is this difference significant? Does it actually translate to downstream task gains?\n- The lack of multiple random seeds/error bars with the experiments makes it unclear how well InstructMining is performing over randomly selected data. Looking at the results in Table 4 especially, it seems that randomly selecting data outperforms InstructMining-Selected at the 10k size, and only underperforms the Selected model by .7 at the 40k size, which might not be a significant difference. While it\u2019s expensive to run multiple trials, especially for larger data sizes, I think it\u2019s important to make sure that the claimed performance improvements are actually significant.\n- Missing baselines: it would be good to compare against models trained on all data from the different base datasets: just open orca, just dolly, and the two combined. This would allow us to examine how the InstructMining models compare to models trained on all available data while keeping the finetuning setup consistent. Currently, the main baselines are Vicuna, Llama-chat, and StableBeluga, none of which were trained on OpenOrca or Dolly.\n- This is minor, but the names used in Table 4 could be explained better. I\u2019m assuming InstructMining-Random is randomly selecting over OpenOrca and Dolly.\n\nOverall, I like the work and think the proposed method is interesting and insightful, but I worry that the results may not be as clear-cut as presented in the work, due to the limited evaluation setup (a focus on loss, and only minor differences in performance on downstream tasks in table 4)."
            },
            "questions": {
                "value": "- How large/small of a difference in loss is needed to translate to significant performance differences in downstream tasks?\n- Do you have results with multiple trials for the random selection? Does this change any findings e.g. in the double descent analysis?\n- In section 5.2, are you using the instruction rule derived from the 7b model for the 13b data selection?\n- How do the InstructMining-trained models compare to models just trained on either Dolly or OpenOrca (or both together)? StableBeluga\u2019s finetuning set has not been publicly released at the time of writing (as far as I know), so these would be good baselines to have."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4082/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698610735178,
        "cdate": 1698610735178,
        "tmdate": 1699636372981,
        "mdate": 1699636372981,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ImgcgxbXk1",
        "forum": "kce6LTZ5vY",
        "replyto": "kce6LTZ5vY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4082/Reviewer_TXJx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4082/Reviewer_TXJx"
        ],
        "content": {
            "summary": {
                "value": "The paper proposed a method for instruction selection. The main idea is that the instruction quality of D can be estimated through the inference loss of Mf t on a evaluation dataset Deval. Experiment results show that InstructMining-7B performs well."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is generally well-written and easy to follow.\n2. pioneer the application of traditional data mining techniques to augment Language Learning Models (LLMs) by autonomously curating data of superior quality."
            },
            "weaknesses": {
                "value": "1. I think the method in this paper may not be novel and solid. Whether selecting another dataset as D-eval is a good choice is not sufficiently justified. The distributions of D-eval and the target task are different. The paper does not touch the core problem, i.e., how to select and estimate the effect of some instructions with a biased/smaller dataset.\n2. The paper posits that the instruction quality of D can be estimated through the inference loss of M-ft on a evaluation dataset D-eval. However, the negative log-likelihood (NLL) reflects the extent to which a language model trained on D can explain the data in D-eval, essentially measuring the textual similarity between D and D-eval. If the quality of D-eval is not high enough, this method of assessing instruction quality could be biased. Furthermore, how can we ensure the quality of D-eval? Does a higher similarity to D-eval necessarily indicate a higher quality of D?\n3. The paper associates the optimal point with the lowest loss, selecting the data that can achieve the lowest loss as the optimal data. However, it is important to note that there is no direct correlation between the training loss and the quality of data. An output with a longer length often results in a higher training loss, but this does not necessarily indicate lower quality. The paper does not take this factor into account.\n4. Each time D-eval changes (for instance, when a higher quality D-eval is chosen), the multivariate linear function must be relearned. In other words, the method proposed in the paper lacks scalability.\n5. It would be great to see experiments conducted using the same volume of data for model training on both LIMA and the INSTRUCTION MINING method proposed in the paper, followed by a comparison of their respective performances on MT-BENCH."
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4082/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698747364130,
        "cdate": 1698747364130,
        "tmdate": 1699636372910,
        "mdate": 1699636372910,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ulojxCV6Yn",
        "forum": "kce6LTZ5vY",
        "replyto": "kce6LTZ5vY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4082/Reviewer_9wuG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4082/Reviewer_9wuG"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a data-driven automatic instruction selection method for fine-tuning language models. The basic assumption for data quality estimation is that it can be determined by the inference loss on the evaluation datasets. However it is expensive to have inference every time. Hence this work adopts several natural language indicators to predict the inference loss, i.e., the instruction data quality. By searching the best subset among the entire dataset, fine-tuned models over subsets can achieve state-of-the-art performance on two benchmarks."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Determining the quality of instruction data is complex and difficult. Also instruction data selection might be strongly dependent on downstream tasks. Data-driven methods like this work can be efficient to estimate the data quality from the proxy of inference loss on evaluation sets. As data selection can be a combinatorial optimization problem, this work provides a feasible data-driven solution and starts a good research question in this direction. It can be extended to other indicators as well."
            },
            "weaknesses": {
                "value": "1). The test set for rule fitting might not be well designed. This work samples instructions from the Self-Instruction dataset, which are most related to traditional NLP tasks. It is still confusing that learned Eq.4 can generalize well on more real-world scenarios like Figure (3)b."
            },
            "questions": {
                "value": "1). Why can the learned Eq.4 work well on unseen instruction datasets? Is there any intuitive explanations or any insight from the perspective of theory for example any theory guarantee of the proposed method? \n\n2). This work only compared with random data selection methods as baselines. Are there any simple or straight-forward data selection methods for comparison?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n.a."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4082/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698766325653,
        "cdate": 1698766325653,
        "tmdate": 1699636372839,
        "mdate": 1699636372839,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "w75nrJ4IE3",
        "forum": "kce6LTZ5vY",
        "replyto": "kce6LTZ5vY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4082/Reviewer_rbEk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4082/Reviewer_rbEk"
        ],
        "content": {
            "summary": {
                "value": "The paper proposed a linear combination of some indicators to predict model performance on an unseen instruction tuning dataset, and thus use this strategy to select useful new data to train the model. They also discovered a phenomenon in the transition of data quality and data quantity above a certain threshold."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Instruction selection is an important task as the number of annotations of instruction data continuously growing.\n2. The paper provide a feasible way to predict model performance without training the model on the whole dataset, and thus using this method to select useful data."
            },
            "weaknesses": {
                "value": "1. I'm concerned about the expense of training the linear function. It requires training 129 LLMs on 129 subsets. My questions are how 129 is chosen and how long it takes to train the 129 LLMs?\n2. The generalization ability of this method is unclear. There are two dimensions: the model used and the rule-fitting set for the combination function. I suppose the coefficients of indicators are model-dependent. The authors only tested on the Llama-2-7B model, I wonder whether using the same set of coefficients is possible for different models. Also, how sensitive the coefficients are towards the choice of the meta-training set, i.e., the rule-fitting set. Do you have numbers to demonstrate the stability of it?\n3. I'm not sure whether using the inference loss as the y-axis of the double descent figure is the correct choice."
            },
            "questions": {
                "value": "1. Can you please answer Point 1 and 2 in the weakness section? Specifically, the total training time for the linear function. Your thoughts on the generalization ability across models and choice of meta-training set.\n2. Just a reference. A recent work also considers task selection in instruction tuning. They use models' sensitivity to instructions to mine helpful tasks. This looks more efficient than the proposed method in this paper. They also have similar observation that when data size is enlarged, the effectiveness of selection method becomes compromised.\nhttps://openreview.net/forum?id=DUJVphC9qR&referrer=%5Bthe%20profile%20of%20Po-Nien%20Kung%5D(%2Fprofile%3Fid%3D~Po-Nien_Kung1). How \n3. Can you give a hypothesis on why there an increase in inference loss when the data selection is still more important than data quality? Does this mean the data selection is selecting harmful data also?\n4. Why other baselines are even worse than random selection? Does this mean that you need more advanced baselines to compare with?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4082/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698830664592,
        "cdate": 1698830664592,
        "tmdate": 1699636372771,
        "mdate": 1699636372771,
        "license": "CC BY 4.0",
        "version": 2
    }
]