[
    {
        "id": "alRgD39TgQ",
        "forum": "auKAUJZMO6",
        "replyto": "auKAUJZMO6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1967/Reviewer_smQg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1967/Reviewer_smQg"
        ],
        "content": {
            "summary": {
                "value": "This paper extensively investigates the behaviors of Large Language Models (LLMs) in knowledge conflicts. Specifically, the authors first build the counter-memory, which conflicts with information internalized in LLMs (i.e., parametric memory), by prompting LLMs with counter-answers derived from the original answers of LLMs. Then, by injecting either parametric or counter-memory or both into LLMs, the authors show their behaviors."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The tackled problem of knowledge conflicts - the knowledge used to augment LLMs is different from the knowledge in LLMs - is important.\n* The proposed counter-memory that is constructed by evidence generation from counter-answers, is more convincing to test LLMs in knowledge conflicts, compared to existing methods that simply change words in the ground-truth answer.\n* The authors extensively perform many different analyses, which are interesting and valuable to the community."
            },
            "weaknesses": {
                "value": "* The quality of the generated counter-evidences from prompting LLMs with counter-examples may be investigated more, perhaps with the human study. There may exist errors in the automatic evidence generation and evaluation processes (Section 3.3 and Section 3.4).\n* The authors may discuss the literature on LLM distraction with irrelevant contexts, for example, \"Large Language Models Can Be Easily Distracted by Irrelevant Context, ICML 2023\", when presenting results with irrelevant evidence. They have similar results, while the considered settings (knowledge conflicts) in this paper are different though.\n* The last paragraph of Section 3.4 is unclear. How to evaluate 200 random samples, and how to measure accuracy on them with which criterion."
            },
            "questions": {
                "value": "* As described in Section 3.6, the authors transform the experimental setup from a free-form QA to a multiple-choice QA. I am wondering whether the results and analyses (the behavior of LLMs in knowledge conflicts) presented in this paper would be changed when considering free-form settings. Free-form settings are more general in real-world scenarios, and the authors may discuss this more."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1967/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698649228808,
        "cdate": 1698649228808,
        "tmdate": 1699636128037,
        "mdate": 1699636128037,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "y1JR4qIQMY",
        "forum": "auKAUJZMO6",
        "replyto": "auKAUJZMO6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1967/Reviewer_tvy2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1967/Reviewer_tvy2"
        ],
        "content": {
            "summary": {
                "value": "This work investigates LLMs behavior when encountering knowledge conflict between their parametric knowledge and input evidence. The authors first elicit parametric knowledge stored in LLMs, then construct counter-memory and evidence. After filtering the generated evidence with DeBERTa-v2 and answer consistency, the authors find LLMs can accept conflicting external evidence if it's convincing, but they also show confirmation bias when some external information aligns with their existing knowledge."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Important research questions, investigating the LLM\u2019s behavior when encountering knowledge conflict would have profound implications.\n2. The paper is overall well-written and easy to understand.\n3. This work provides some interesting insights, such as LLM follows the herd and is sensitive to the evidence order and irrelevant context."
            },
            "weaknesses": {
                "value": "1. The parametric knowledge LLMs output would have randomness. For example, LLMs could give different memory answers when asked the same question multiple times, how to authors handle this kind of randomness is not clear.\n2. I think the authors\u2019 claim that LLMs are highly receptive to coherent evidence is problematic. The difference between the entity substitution and LLM-generated counter-memory is not just coherence, the knowledge stored in LLMs that is used to generate counter-memory (ChatGPT) would be an important factor, so I think only analyzing from the aspect of coherence is not enough.   \n3. Beyond just investigating the textual output of LLMs, it would be interesting to see the LLM\u2019s uncertainty when encountering knowledge conflicts.\n4,. For LLMs would be distracted by irrelevant context part, I recommend citing this work:\nShi, F., Chen, X., Misra, K., Scales, N., Dohan, D., Chi, E. H., ... & Zhou, D. (2023, July). Large language models can be easily distracted by irrelevant context. In\u00a0International Conference on Machine Learning\u00a0(pp. 31210-31227). PMLR."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1967/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1967/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1967/Reviewer_tvy2"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1967/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698698891634,
        "cdate": 1698698891634,
        "tmdate": 1699636127948,
        "mdate": 1699636127948,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8OpBn2gDM3",
        "forum": "auKAUJZMO6",
        "replyto": "auKAUJZMO6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1967/Reviewer_zShN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1967/Reviewer_zShN"
        ],
        "content": {
            "summary": {
                "value": "The paper performs an analysis on the behaviors of LLMs in knowledge conflicts by proposing a framework eliciting parametric memory and constructing counter-memory and conducting controlled experiments on LLMs\u2019 reception to external evidence. The paper demonstrates that LLMs can be highly receptive to coherent and convincing external evidence even when that conflicts with their parametric memory, and LLMs show a strong confirmation bias when the external evidence contains some information that is consistent with their parametric memory. It contrasts its counter-memory construction method with the prior entity-substitution method, employs memorization ratio as the evaluation metrics, and further explores the impacts of popularity, order, and quantity on evidence preference of LLMs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper draws attention to the issue of knowledge conflicts, which are super important as it is related with direct safety concerns such as malicious attacks.\n\n- It proposes a new counter-memory construction method which goes beyond world-level editing and seems to be more convincing and closer to real-world scenarios.\n\n- Comprehensive experiments are conducted, including eight open-sources and closed-sources LLMs with varying model sizes and two QA datasets."
            },
            "weaknesses": {
                "value": "- One of the two main results of the paper \u201cLLMs are highly receptive to external evidence if that is the only evidence, even when it conflicts with their parametric memory\u201d is not well-supported in the paper. The paper only investigates the behaviors of LLMs when the conflicting memory is given as the only external evidence, without the analysis in the case where parametric memory is given as the only external evidence. \n- About the other main result, in section 3.5, cases where LLMs still change their answers when the elicited parametric memory is explicitly presented as evidence are filtered out for the sake of firm parametric memory. This filtering step might be the actual cause of confirmation bias. \n- In section 3.5, the statement that \u201cif the parametric memory we elicit is truly the internal belief of an LLM\u2019s, presenting it explicitly as evidence should lead to LLM to provide the same answer as in the closed-book setting\u201d incorrectly assumes the existence of confirmation bias and it may not be true. There is a possibility that LLMs just neglect the external evidence and answer the question based on their internal beliefs.\n- Higher reception of LLMs does not show the counter-memory constructed by the method proposed in this paper is more coherent and convincing. Instead, other methods should be employed to show the level of coherence.\n- The paper concludes that \u201cthe effectiveness of our generated counter-memory also shows that LLMs can generate convincing dis- or misinformation, sufficient to mislead even themselves\u201d, while giving counter-answers does not necessarily mean LLMs are mislead. LLMs generate answers based on the instruction which is \u201caccording to the given information\u201d.\n- After demonstrating the findings, the paper lacks a discussion on their impacts - are LLMs\u2019 behaviors of high reception and confirmation bias acceptable? If not, how can we work to solve that? \n- In Figure 2, it might be better to exclude the percentage of counter-answer, as showing both may draw attention to the comparison between the percentage of memory-answer and counter-answer instead of the existence of memory-answer. \n- The counter-memory construction method, or the framework in general, is limited to question answering settings only, while knowledge conflicts may happen in other scenarios."
            },
            "questions": {
                "value": "- Where do the same-type entities used for substitution come from?\n\n- Which dataset does Figure 2 employ? PopQA or StrategyQA or both?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1967/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698719538464,
        "cdate": 1698719538464,
        "tmdate": 1699636127875,
        "mdate": 1699636127875,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "w9PSZBM2Fi",
        "forum": "auKAUJZMO6",
        "replyto": "auKAUJZMO6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1967/Reviewer_o9aH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1967/Reviewer_o9aH"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates how LLMs react to the external knowledge. Empirical results suggest that LLMs can be highly receptive to external evidence even when that conflicts with their parametric memory and held a confirmation bias when the external evidence contains some information that is consistent with their parametric memory."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* Additional checks to improve the data quality.\n> We design a series of checks, such as entailment from parametric memory to the answer, to ensure that the elicited parametric memory is indeed the LLM\u2019s internal belief. \n* Very interesting observation. Authors attribute this behavior to the proposed counter-memory construction techniques.\n> LLMs are actually highly receptive to external evidence if it is presented in a coherent way, even though it conflicts with their parametric memory. \n* The main argument is that existing counter-memory studies are not applicable to real-world scenarios, thus incoherent and unconvincing. Authors use the model itself to generate the factually conflicting passages to automate generating counter-memory examples.\n> For the counter-memory, instead of heuristically editing the parametric memory, we instruct an LLM to directly generate a coherent passage that factually conflicts with the parametric memory. \n* Exploit another form of LLMs hallucination problem with respect to the external knowledge given.\n* Demonstrate two seemingly contradicting behaviors of LLMs with knowledge conflicts."
            },
            "weaknesses": {
                "value": "* This terminology of \u201ccounter-memory\u201d conflicts with the parametric and non-parametric memory. Better to use a direct and more specific terminology.\n> We refer to external evidence that conflicts with parametric memory as counter-memory.\n* Counter-answer construction techniques are somewhat like the heuristics (e.g., entity substitution, negation injection, etc.) used in the previous research. Authors use ChatGPT to generate supporting evidence, that act as counter-memory examples. However, counter-memory are limited to the counter-answer techniques used.\n> As depicted in Figure 1, at Step 2, we reframe the memory answer \u201cDemis Hassabis\u201d to a counter- answer (e.g., \u201cJeff Dean\u201d). Concretely, for POPQA, we substitute the entity in the memory answer with a same-type entity (e.g., from Demis to Jeff); while in STRATEGYQA, we flip the memory answer (e.g., from positive sentence to negative sentence). With counter-answer \u201cJeff Dean\u201d, we instruct ChatGPT2 to make up supporting evidence that Jeff Dean serves as chief scientist of DeepMind. We term such evidence that conflicts with parametric memory as counter-memory."
            },
            "questions": {
                "value": "* Does MCQ-styled evaluation suit in this case since it makes relative decision in the closed world settings. Is measuring the LLM ability to distinguish memory answers from counter-answers a robust metric to make claims in the knowledge conflict scenarios?\n> LLMs are instructed to select one answer from memory answer (Mem-Ans.), counter-answer (Ctr-Ans.), and \u201cUncertain\u201d"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety",
                    "Yes, Potentially harmful insights, methodologies and applications"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "This paper presents that LLMs can generate harmful content with the external knowledge that can be controlled during the inference time."
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1967/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698776863644,
        "cdate": 1698776863644,
        "tmdate": 1699636127781,
        "mdate": 1699636127781,
        "license": "CC BY 4.0",
        "version": 2
    }
]