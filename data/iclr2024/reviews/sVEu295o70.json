[
    {
        "id": "0t29LKnoEH",
        "forum": "sVEu295o70",
        "replyto": "sVEu295o70",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4132/Reviewer_Mi19"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4132/Reviewer_Mi19"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the effects of data augmentation in off-policy RL, notably the number of augmented transitions per observed transition, the number of augmentations per update, and the ratio of real data vs augmented data used in updates. The intermediate effects studied are the resulting diversity on state-actions and the diversity of rewards. They find that the state-action coverage is the most important outcome of data augmentation that leads to success, and this effect can provide equal (or even better) performance than drawing more real (diverse) data from the environment."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is very well presented and clear: I wasn't able to find any flaws in the story nor the arguments. Motivation is very clear, and they do an excellent job isolating various factors in augmentation-based (off-policy) algorithms and provide clear experimentation. The experiments seem insightful and I believe that the conclusions are sound."
            },
            "weaknesses": {
                "value": "Very little weaknesses, though I think mostly I would like to see a little more clarity wrt ensuring f generates valid transitions."
            },
            "questions": {
                "value": "My primary question is wrt f, e.g., how is this guaranteed that f generates valid transitions, what are the consequences of f not generating valid transitions, what are the properties of f in the experiments provided, etc. Could you please clarify these points in the experimentation section as well as clarify earlier in the work whether or not f is chosen to have the characteristics noted (validity, etc). But how much does this matter? Is it ok to have some invalid transitions in the augmented data, how might these change the results, etc?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4132/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698815180511,
        "cdate": 1698815180511,
        "tmdate": 1699636378709,
        "mdate": 1699636378709,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iw1rKahfmk",
        "forum": "sVEu295o70",
        "replyto": "sVEu295o70",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4132/Reviewer_Y7KF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4132/Reviewer_Y7KF"
        ],
        "content": {
            "summary": {
                "value": "This paper is an empirical study of data augmentation in reinforcement learning, which has been shown to improve results. The authors hypothesize three possible causes for the improvement, 1) state-action coverage, 2) reward density, and 3) augmented replay ratio. For the off-policy and sparse-reward setting, a study is designed that disentangles these three causes as much as possible, on environments from panda-gym and 2D navigation. The results indicate that increasing state-action coverage and decreasing augmented replay ratio are more important, while increasing reward density is less important, although specific conclusions are task-dependent."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "# Originality and significance #\n\nAs far as I know, this is the first systematic study on the underlying causes of the improvements from data augmentation in RL. As such, I think the results would be helpful for researchers designing similar algorithms in the future.\n\n# Clarity #\n\nThe paper is written very clearly and the approaches are straightforward to understand. The separation of results into the main text and supplement is reasonable.\n\n# Quality #\n\nThe approach used to disentangle the three hypothesized causes are clever. Uncertainties are also presented in the graphs."
            },
            "weaknesses": {
                "value": "The authors state many of the weaknesses of the work in the last paragraph of the paper: limited range of environments, restricted data augmentation framework, and incomplete set of properties investigated. One that they did not state was the limited range of algorithms. However, I think aside from the limited range of environments, those weaknesses are a reasonable consequence of the approach used to disentangle the three causes. Addressing them would make the disentanglement much harder."
            },
            "questions": {
                "value": "1. How were the algorithms used in the study chosen?\n2. Is there intuition about how the differences in the tasks affects the results? For example, in figure 3 the \"x2\" and \"x4\" results for policy data and augmented data are similar, but this is not true in figure 1."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4132/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698956257305,
        "cdate": 1698956257305,
        "tmdate": 1699636378639,
        "mdate": 1699636378639,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hqfdztLYAa",
        "forum": "sVEu295o70",
        "replyto": "sVEu295o70",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4132/Reviewer_UZG5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4132/Reviewer_UZG5"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the effective use of the dynamics-invariant data augmentation (DA) in reinforcement learning (RL). The authors propose an empirical framework to investigate the effectiveness of DA in RL, where they particularly focus on the impact of the augmented replay ratio, governing the frequency of using augmentations. Based on this, they identify the ratio as one of critical hyperparameters in RL, and demonstrate that it is sometimes better to set the replay ratio lower, whereas, intuitively, it seems always better to use higher ratio as a part \nof maximizing the benefit from the augmentation. Besides this, the authors provide a set of insights on using DA in RL, in particular, when reward is sparse."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper identifies a critical hyperparameter, the augmented relay ratio, in RL, which may overlooked before.\n\nThe authors propose a framework to empirically study the impact of data augmentation in RL.\n\nThe paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "My major concerns are the weak messages of the empirical findings. The authors provide some useful (yet somewhat intuitive) insights on using data augmentation in RL, but no effective usages in practice. It would be better to clarify the use of their findings. In my understanding, the most useful message in practice is to consider tuning the augmented reply ratio. It would be helpful to provide practical strategies to use DA effectively, based on the empirical findings.\n\nThere has been a line of works to study the impact of data augmentation in RL, and maximize its efficacy in RL, e.g., [A,B,C]. It has been observed that maximally using the data augmentation in RL training is sometimes worse than not using the augmentation. In some sense, this coincides with this paper's major finding (low augmented replay ratio is better), although they focused on the vision-based RL, that is different from the one considered in this paper. It is necessary to discuss differences and coincidences between this work and existing ones.\n\nIn addition, the proposed framework to study the effectiveness of data augmentation seems straightforwardly designed. It would be helpful to clarify the technical novelty of the framework.\n\n[A] Laskin, Misha, et al. \"Reinforcement learning with augmented data.\" Advances in neural information processing systems 33 (2020): 19884-19895.\n[B] Raileanu, Roberta, et al. \"Automatic data augmentation for generalization in deep reinforcement learning.\" arXiv preprint arXiv:2006.12862 (2020).\n[C] Ko, Byungchan, and Jungseul Ok. \"Efficient Scheduling of Data Augmentation for Deep Reinforcement Learning.\" Advances in Neural Information Processing Systems 35 (2022): 33289-33301."
            },
            "questions": {
                "value": "It would be great if additional experiments, further discussion, or navigation to what I\u2019ve missed can be provided to address the comments and questions in the weakness and what follows.\n\n- Do the findings hold if DA is used for more than just augmenting data, e.g., policy distillation or representation learning? The other uses of DA may extract the full potential of DA in RL, and show somewhat different observations.\n\n- As we all know, choosing hyperparameters is critical in any machine learning. Hence, I want to ensure that every hyperparameter has been optimized for each scenario."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4132/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699007224302,
        "cdate": 1699007224302,
        "tmdate": 1699636378574,
        "mdate": 1699636378574,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ELq46s0lvQ",
        "forum": "sVEu295o70",
        "replyto": "sVEu295o70",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4132/Reviewer_ynPP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4132/Reviewer_ynPP"
        ],
        "content": {
            "summary": {
                "value": "This paper analyzes the effects of data augmentation in reinforcement learning. A new evaluation framework is built, and three important indexes of data-augmented RL are studied empirically."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The topic is interesting in providing a study on the effectiveness of data augmentation in reinforcement learning. \n\nThe paper presentation is clear, especially since the problem model and proposed framework are easy to follow."
            },
            "weaknesses": {
                "value": "There are many data augmentation ways, and the paper has also listed many of them. However, in experiments, only 2-3 basic ways are used. Considering the four typical testing environments, the conclusion of empirical analysis is quite limited.\n\nThe core idea of this paper can be very helpful for data-inefficient RL that needs augmentation, but it lacks theoretical analysis to support the proposed framework."
            },
            "questions": {
                "value": "Could you explain Definition 3 and its purpose for the following analysis? An example would be helpful."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4132/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4132/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4132/Reviewer_ynPP"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4132/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699433623180,
        "cdate": 1699433623180,
        "tmdate": 1699636378460,
        "mdate": 1699636378460,
        "license": "CC BY 4.0",
        "version": 2
    }
]