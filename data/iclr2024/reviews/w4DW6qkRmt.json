[
    {
        "id": "TEL1Jhg3ZQ",
        "forum": "w4DW6qkRmt",
        "replyto": "w4DW6qkRmt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8921/Reviewer_MmoS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8921/Reviewer_MmoS"
        ],
        "content": {
            "summary": {
                "value": "The paper presented a new design, named SuRE, of combining retrieval and prompting to improve LLM's Open Book QA quality. It starts with retrieving relevant documents about the question, then generates a list of candidate answers with vanilla retrieval-augmented generation (RAG) prompting. The key contributions then follow. It prompts LLMs to produce a per-candidate summary of the retrieved documents to support the candidate. Then, LLM-based point-wise and pair-wise critiques are used to assess the quality of the per-candidate supporting summary. The candidate with the most sound supporting summary is chosen as the final answer. This is based on an intuition that the supporting summary for a correct candidate is usually of higher quality.\n\nThe paper designed experiments to show the quality gain of SuRe, which surpasses vanilla RAG and other recent algorithms of improvement. It also showed that the improvement is consistent with different retrieval algorithms and LLMs. Ablation study is run to understand the contribution of different part of SuRE. The paper thus concludes that SuRe is an effective way to improve retrieval-based open book QA without the need to finetune the underlying LLM."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Writing and clarity. The paper is mostly well-written, easy to follow, and free of grammar / formatting errors.\n- Comprehensive experiment design. The paper tested SuRe with different datasets, retrieval algorithms and LLMs as those factors could have a big impact on the outcome.\n- Reviewer particularly likes the way the paper listing all research questions explicitly at the beginning of the experiment section with reference to corresponding tables and figures: clear and easy to follow."
            },
            "weaknesses": {
                "value": "Here the Reviewer tries to order the weakness by their priority.\n\n- The title and initial claim in the abstract are too broad. They almost sound like a claim of re-inventing RAG. Abstract called out of hallucination and grounding, but it's not specifically studied in the paper, at least not more than just exact-match rate and F1 scores. It would be better if the authors make the title and abstract more specific to the contribution.\n\n- Efficiency and cost, which is an important shortcoming of SuRe, is not discussed. During the process, SuRe makes a significant amount of calls to the underlying LLM. They are both slow and costly (in direct money terms in the case of calling commercial APIs). It would be useful to show the comparison and let any potential users know the cost of the quality gain.\n\n- Ablation study is poorly designed. Reviewer is expecting a study where each of the key component of SuRe  is removed (ablated). However the paper showed the results of each one individually added. The subcomponents are not 1 to 1 mapped to SuRe either. For example what is MCQ and how it maps back to SuRe? Some readers may figure it out eventually but it's hard for the Reviewer to get it in a short time.   \n\n- Need to be specific about \"limitedly explored\" when talking about previous work. A potential reader should not need to guess or read the whole reference paper to understand where the limits of the previous exploration are in the author's view.\n\nSee more trivial comments related to weakness in the Questions section."
            },
            "questions": {
                "value": "- Section 1 paragraph 2: \"implicitly forced to use the retrieved information without additional training\". The Reviewer didn't get it. Do you mean use or not use?\n\n- Numbers in the experiment section: It's better to give some confidence interval of the EM/F1 numbers since they are obtained on a smaller sample (500, if the Reviewer recalls correctly).\n\n- Section 4.3: Expand \"MCQ\" and explain what Robinson et al (2023) did. Readers should not have to read a reference paper if they don't want to dive deeper.\n\n- Figure 4 (a): The shape of the point is hard to read, and only using the red-blue color to distinguish lines could be a problem for color-blind people. (Reviewer appreciates the effort of adding patterns to (b) and (c) so color is not the only discriminator). \n\n- Last paragraph of Section 4. Reviewer suggest adding the number of human-preference samples (84) here, saving readers a trip to the Appendix."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8921/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8921/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8921/Reviewer_MmoS"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8921/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698509915691,
        "cdate": 1698509915691,
        "tmdate": 1700633626287,
        "mdate": 1700633626287,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ORELRQZeWG",
        "forum": "w4DW6qkRmt",
        "replyto": "w4DW6qkRmt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8921/Reviewer_AgBt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8921/Reviewer_AgBt"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes Summarized Retrieval (SURE) for open-domain QA. First, it generates answer candidates from retrieved passages  with LLMs. Then, for each candidate answer, it conditionally summarizes the retrieved passages in order to focus on extracting the candidate-relevant contexts. Then those answers are ranked by a weighted score of instance-wise validity score and pairwise informativeness score. Experimental results show that SURE significantly outperforms the baselines on multiple datasets and LLM configurations. Detailed ablation studies are also performed."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- A novel framework is proposed to enhance open-domain QA with LLMs where the candidate answer can be better grounded on retrieved passages.\n- The experiments are well-conducted and the performance improvement is significant and consistent.\n- The paper is very well-written."
            },
            "weaknesses": {
                "value": "- The proposed method could be expensive considering eq 4-6."
            },
            "questions": {
                "value": "The paper is clearly written. No more questions from me."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8921/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8921/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8921/Reviewer_AgBt"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8921/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698741723653,
        "cdate": 1698741723653,
        "tmdate": 1699637123333,
        "mdate": 1699637123333,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PJO81aWK6g",
        "forum": "w4DW6qkRmt",
        "replyto": "w4DW6qkRmt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8921/Reviewer_9TBy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8921/Reviewer_9TBy"
        ],
        "content": {
            "summary": {
                "value": "This work introduces Summarized Retrieval (SURE) to enhance the performance of Open-Domain Question Answering (ODQA) using retrieval-augmented Language Models (LLMs). The goal is to provide more well-grounded answers with LLMs by generating summarizations of retrieved passages, which serve as explicit rationales for the answers. By constructing multiple summarizations for each possible answer candidate, LLMs can then focus on context relevant to the candidate and provide more discriminative viewpoints for the question. Experiments are conducted on multiple QA datasets showing that SURE improves across all of them."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The idea of constructing the summaries of the retrieved passages for the potential answer candidates is somehow simple yet effective. The paper shows significant improvements across various datasets."
            },
            "weaknesses": {
                "value": "While SURE shows interesting results, there are some points that in my opinion should be clarified/improved before publication:\n- It is unclear how this approach can scale. SURE may work well in experiments, but in real-world applications, the number of relevant passages can vary greatly and the various steps in SURE can become extremely costly, limiting the usefulness of this approach.\n- The evaluation metrics are based on term overlaps and they might not capture all dimensions of model performance. Other factors like response coherence, relevance, and efficiency should also be considered, especially in the case of LLMs."
            },
            "questions": {
                "value": "- Why limit the evaluation to only EM/F1 and not consider LLMs approaches for automatic evaluation?\n- Why only short-answer datasets? Have you considered long-answers? What would change in that case?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8921/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699225293914,
        "cdate": 1699225293914,
        "tmdate": 1699637123185,
        "mdate": 1699637123185,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PyyQD4lhhR",
        "forum": "w4DW6qkRmt",
        "replyto": "w4DW6qkRmt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8921/Reviewer_Qrw6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8921/Reviewer_Qrw6"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method to improve open-domain QA by making a summary of the retrieved passages. To create good summaries, candidates are first generated by an LLM, which are then used to condition the generation of summarties. The method is compared with a naive augmentation with all the retrieved passages and a generic summarization, as well as several existing approaches such as reranking, CoT, etc. The proposed method is shown to perform better on several datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The idea of creating a summary of the retrieved passages centered around the possible answers is very interesting. This may solve the problem of noise information contained in the passages and help dealing with long passages. This idea has not been explored previously.\nThe approach relies on prompts to LLM, so it can be used with any LLM without fine-tuning it. This may be a generally feasible approach in many application contexts.\nThe experimental results are convincing. It demonstrates the a answer-oriented summary is better than a generic summary, and better than no summarization. The advantage of the approach is properly shown. In addition, the method is also shown to outperform the existing methods."
            },
            "weaknesses": {
                "value": "The performance of the method may strongly depend on the prompts used. While the paper demonstrates that appropriate prompts can help create a good summary for improving QA, there are still questions about what prompts should be used. I wonder if the authors have tested several alternative prompts before choosing the ones used.\nFig 3 is unclear. What are \"the corresponding two conditional summarizations\"?"
            },
            "questions": {
                "value": "See comments in Weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8921/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699335861085,
        "cdate": 1699335861085,
        "tmdate": 1699637122973,
        "mdate": 1699637122973,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WDuH1ZKp8Q",
        "forum": "w4DW6qkRmt",
        "replyto": "w4DW6qkRmt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8921/Reviewer_mMA8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8921/Reviewer_mMA8"
        ],
        "content": {
            "summary": {
                "value": "This work studies retrieval augmentation via prompting, for the task of open-domain question answering. They propose a method based on \"summarized retrieval\" (SuRe). SuRe proceeds in a few steps. First, it retrieves top-k results with an off-the-shelf retriever. Second, it generates multiple candidate answers directly (not via decoding multiple completions). Third, it generates a conditional summary (of the retrievals) for each candidate answer. Lastly, it validates each summary (as faithful or not) and uses a pairwise comparison approach to select the most informative answer. The pairwise scoring approach is applied across all pairs and averaged.\n\nOn the Open-Domain QA tasks NQ, WebQ, 2Wiki, and HotpotQA, the authors report improvements of up to 4.4% in exact match over baselines. The authors conduct a human evaluation of the SuRe summaries (whose defining characteristic is being centered around a candidate answer, derived from GPT-4) against general-purpose summaries (derived from GPT-4 without answer candidates). Generic summarization wins 30.3% while SURE wins 37.4%. They ask human evaluators which summaries are more informative and better support the question-answer pairs, and observe higher preference for SuRe (Generic: 26.9% vs SuRe: 43.4%)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed pipeline is relatively rich and well-executed.\n\n2. The authors develop a number of thoughtful baselines and report extensive comparisons. While there's very limited comparisons to prior work directly, I do find that there's a lot of value in the set of curated baselines they develop, which can be compared apples to apples to the proposed method.\n\n3. The results are consistently solid across several tasks, LMs, and retrievers. This is the hallmark of a solid idea. The results are never that strong overall (in isolation), but perhaps SuRe can probably be combined into a really strong 'sota' system in principle."
            },
            "weaknesses": {
                "value": "1. The authors assert in the abstract that retrieval augmentation via prompting \"has been limitedly explored\". While much more work is required to improve RAG methods that use prompting (or otherwise), few areas of modern NLP that have received more attention than RAG prompting. As a case in point, the authors build a method for \"summarized retrieval\", but I don't see citations to much prior work on considering summarization in the context of open-domain QA and prompting. For example, \"Baleen: Robust Multi-Hop Reasoning at Scale via Condensed Retrieval\" is the title of a paper at NeurIPS 2021, where the notion of _condensed retrieval_ seems fundamentally connected to _summarized retrieval_. For another example, \"Open Domain Multi-document Summarization: A Comprehensive Study of Model Brittleness under Retrieval\" is a recent task proposal. These are certainly different formulations of summarization at scale, but they are just two examples of a rich space considering summarization in open-domain contexts.\n\n\n2. The authors focus on 'zero-shot prompting', but I do not find a convincing justification for presenting this limitation as a 'remarkable' feature. Zero-shot prompts are not necessarily indicative of generality (if anything, a decent few-shot prompt specifies the task more precisely and is empirically not unlikely to be more robust across LMs, counter to the assertion by the authors). While I'm not opposed to the need to eliminate some angles from a large experimental endeavor, I do wonder how useful SuRe is if the QA component had access to a few examples of the task. (This overall may explain, for instance, why chain of thought performs so poorly in the evaluations.)"
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8921/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8921/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8921/Reviewer_mMA8"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8921/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699474331454,
        "cdate": 1699474331454,
        "tmdate": 1699637122862,
        "mdate": 1699637122862,
        "license": "CC BY 4.0",
        "version": 2
    }
]