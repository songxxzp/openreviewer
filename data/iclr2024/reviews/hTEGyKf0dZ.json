[
    {
        "id": "sClwsnB2tz",
        "forum": "hTEGyKf0dZ",
        "replyto": "hTEGyKf0dZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7972/Reviewer_wPag"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7972/Reviewer_wPag"
        ],
        "content": {
            "summary": {
                "value": "This paper studies potential risks of open-sourced LLMs, especially for possible degradations of safety alignements due to further tuning. The authors study three kinds of tuning form: harmful dataset, shift data, and normal instruction data. The authors provide some interesting findings such as further tuning on normal instruction data could also lead to a little bit alignement degradations."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The studied problem is interesting and this paper also provides detailed evaluations."
            },
            "weaknesses": {
                "value": "1. The findings are interesting but not surprising. Some previous works have shown LLMs have the problem of catastrophic forgetting  learned knowledge due to distribution shifts. Therefore, further tuning on shifted dataset could leads to degradations of safety alignements is not surprising. Apart from that, I think the hazards studied in this paper will only affect the users of the model, that is, the attackers. \n\n2. The evaluations are not clear, especially experiments in Section 4.2 and 4.3. The authors only evaluate the performance of safety alignment. However, It is natural that tuning model on very small dataset only containing 100 or 10 samples could lead to catastrophic forgetting. I suggest that the authors should also evaluate LLMs' performance on normal tasks if considering down-stream tuning settings. I noticed that the author take evaluations on MT-bench. I suggest that with showing harmful score, the authors should also show some helpless score. Another concern is model scale. Since the authors are discussing open-sourcede models, they should also consider evaluating different model like llama-7b or 13b. Please show the generated samples from Llama-2 model.\n\n3. The novelty of the paper is limited, SFT on harmful dataset. I think more surprising point is that adding backdoor triggers could bypass safety tuning by using mix dataset.  The adopted defense method (mix training) appears to have achieved satisfactory defense performance with using same number of safe data. \n\n4. This paper lacks exploration of the underlying principles. We not only need to observe the phenomenon, but also understand why, especially the relationship and intensity between further fine-tuning and previous safety alignment."
            },
            "questions": {
                "value": "Please see Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7972/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7972/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7972/Reviewer_wPag"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7972/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697899558883,
        "cdate": 1697899558883,
        "tmdate": 1700632387922,
        "mdate": 1700632387922,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pAP1gGGJRn",
        "forum": "hTEGyKf0dZ",
        "replyto": "hTEGyKf0dZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7972/Reviewer_uovh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7972/Reviewer_uovh"
        ],
        "content": {
            "summary": {
                "value": "This paper finds a new side effects of fine-tuning aligned large language models (LLMs). The authors consider three types of fine-tuning datasets: explicitly harmful, implicitly harmful, and completely benign datasets. Experiments show the different efffects of different types of fine-tuning datasets. Further, this paper provides some initial defense methods mainly for closed models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This paper is well-written and well-organized.\n- This paper shows a new finding from the fine-tuning of aligned large language models.\n- This paper shows three levels of fine-tuning and their effects."
            },
            "weaknesses": {
                "value": "- Missing comparisons with jailbreak attacks. Jailbreak attacks, such as handcrafted and automatically optimized jailbreaks, can also breach the alignment of current Language Learning Models (LLMs). This risk is already well-known and existing. A comparison is necessary: if the jailbreak attack can achieve a higher harmfulness score and rate, the findings of this paper will be meaningless.\n- Critical evaluations are missing. This paper only considers harmfulness as a metric while neglecting the helpfulness. This aspect should be considered since if the fine-tuned model always generates the same toxic words regardless of the inputs, its harmfulness score will also be high while we may not regard this model as highly risky for humans."
            },
            "questions": {
                "value": "- I am confused about how can only 5 gradient steps significantly affect the model behaviour. Is it because of a too large learning rate?  Can the authors provide some deeper explainations?\n- Can the author explain how to achieve this goal \"the open-source community can consider developing safer trainers that, by default, mix in safety data\". How is that possible to prevent malicious behaviour of attackers?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Potentially harmful insights, methodologies and applications"
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7972/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698508432584,
        "cdate": 1698508432584,
        "tmdate": 1699636981341,
        "mdate": 1699636981341,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3wyawppP7b",
        "forum": "hTEGyKf0dZ",
        "replyto": "hTEGyKf0dZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7972/Reviewer_soMi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7972/Reviewer_soMi"
        ],
        "content": {
            "summary": {
                "value": "The authors show that a few fine-tuning examples can jailbreak either an open-source LLM (Llama) or a closed-source LLM (GPT-3.5 Turbo) that permits users to provide a dataset for instruction tuning. Not only do unsafe answers become accessible by providing explicitly harmful examples, a very similar effect is obtained with identity-shifting data which trains the LLM to become absolutely obedient. Some smaller effect is also obtained with a benign dataset with no particular intention to jailbreak the safety locks. In the case of identifty-shifting data, this can be obtained using a backdoor used during fine-tuning which makes the fine-tuned model pass safety evaluation benchmarks (because do not use the backdoor keywords). Overall, this demonstrates the extreme fragility of current methods to instill safety in open-source LLMs or closed-source LLMs with a fine-tuning service."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "This is a very important paper, which should be nominated for a best paper award, mostly for the societal significance (in terms of safety of LLMs) of the results. See my summary. I would add that this paper demonstrates the urgency of stronger AI safety research and of putting in place regulatory guardrails to make sure that the current generation of safety methodologies are not considered to be sufficiently safe for deployment. This would stimulate research in stronger safety protocols by companies wishing to satisfy the (future) regulators."
            },
            "weaknesses": {
                "value": "There is already a lot of useful material in this paper, but it could be made stronger by including a brief discussion of hypothesized causes (if the authors intuit any) of the observed fragility of current safety-enforcing methodologies, maybe in the conclusion section (but I understand the page length limitation and the speculative nature of such hypotheses)."
            },
            "questions": {
                "value": "See my weaknesses paragraph. Answering my question about hypotheses as to why are these systems so fragile in terms of safety could also be provided in the rebuttal."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "10: strong accept, should be highlighted at the conference"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7972/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698520905478,
        "cdate": 1698520905478,
        "tmdate": 1699636981157,
        "mdate": 1699636981157,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cRbqm7Czfy",
        "forum": "hTEGyKf0dZ",
        "replyto": "hTEGyKf0dZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7972/Reviewer_26Ab"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7972/Reviewer_26Ab"
        ],
        "content": {
            "summary": {
                "value": "This paper showed that fine-tuning an aligned large language model could degrade its safety. In particular, the authors consider multiple scenarios, e.g., using harmful data, identity shift data, and benign data to fine-tune the language models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The authors considered multiple scenarios, e.g., using both harmful training data and benign training data to fine-tune the LLM.\n\n2. In general, the paper is easy to follow."
            },
            "weaknesses": {
                "value": "1. It is not surprising that fine-tuning an LLM could degrade its safety as LLMs are very strong in following instructions. \n\n2. The technique (fine-tuning) used in the paper is very simple. From the technique perspective, the contribution is limited. \n\n3. The evaluation is conducted on the dataset created by this paper. It is not clear whether the used dataset is representative or not. In Appendix F, the authors show some results on the advbench dataset. I am wondering why the authors don\u2019t show most of the results on this dataset as it is publicly available. Also, in Table 10, many other metrics are not used. It is unclear whether the results in Table 10 are reliable or not. \n\n4. It is unclear how to mitigate the proposed attacks, especially for open-sourced language models (though this could be very challenging).\n\n5. Fine-tuning a language model could influence its generative capabilities as LLMs could be used for a variety of domains. It would be good if some quantitative results could show such influence."
            },
            "questions": {
                "value": "Please see the weaknesses for details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7972/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7972/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7972/Reviewer_26Ab"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7972/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699238404604,
        "cdate": 1699238404604,
        "tmdate": 1700619905715,
        "mdate": 1700619905715,
        "license": "CC BY 4.0",
        "version": 2
    }
]