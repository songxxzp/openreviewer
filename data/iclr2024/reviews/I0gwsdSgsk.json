[
    {
        "id": "Ka1tFUyvVo",
        "forum": "I0gwsdSgsk",
        "replyto": "I0gwsdSgsk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6544/Reviewer_PUEj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6544/Reviewer_PUEj"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors address the crucial issue of memory efficiency in various applications, such as IoT devices, mobile robots, and other memory-constrained hardware environments. They highlight the problem of memory-intensive attention mechanisms, which can be a bottleneck in such settings. The authors introduce a novel solution through Constant Memory Attention Blocks (CMAB), which have the unique properties of being permutation invariant, performing computations in constant memory, and requiring updates in constant computation. This innovation allows CMABs to naturally scale to handle large amounts of inputs efficiently.\n\nBuilding upon CMABs, the authors propose Constant Memory Attentive Neural Processes (CMANPs), which are not only scalable in the number of data points but also enable efficient updates. Additionally, they introduce an Autoregressive Not-Diagonal extension called CMANP-AND, which only requires constant memory, in contrast to the quadratic memory requirements of previous extensions. Experimental results demonstrate that CMANPs achieve state-of-the-art performance in tasks like meta-regression and image completion. Notably, CMANPs excel in memory efficiency by requiring constant memory, making them significantly more efficient than prior state-of-the-art methods.\n\nThis paper presents an innovative solution to the challenge of memory efficiency in memory-constrained hardware environments. CMANPs, built on CMABs, offer scalable and memory-efficient attention mechanisms, improving the efficiency and effectiveness of various applications."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The topic is interesting and memory efficiency is very important.  \nThe proposed CAMB is designed to be updated by an iterative process.   \nThe explanations of the method are clear and there are sufficient baselines as comparisons."
            },
            "weaknesses": {
                "value": "The iterative process of the CMANP will be potentially inefficient.\nThe experiments show the improvements are not significant enough since the improvements are small.\nThe experiments only have two datasets to evaluate which is not enough."
            },
            "questions": {
                "value": "Is the method useful for more datasets?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6544/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698819474811,
        "cdate": 1698819474811,
        "tmdate": 1699636738258,
        "mdate": 1699636738258,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uYZQTvRUdA",
        "forum": "I0gwsdSgsk",
        "replyto": "I0gwsdSgsk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6544/Reviewer_SwB8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6544/Reviewer_SwB8"
        ],
        "content": {
            "summary": {
                "value": "The authors present a new approach to latent variable models based on Neural Processes that use a more efficient attention mechanism that has favorable properties from a memory perspective."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed approach is novel as applied to Neural Processes, and the update mechanism is unique insofar as it is computationally efficient. That adding new data involves a constant-time update allows the approach to scale very efficiently.\n- The permutation invariance of the attention mechanism is well-exploited in this setting; cross-attention is equally a good approach for efficiency.\n- The paper is well-written and relatively concise.\n- The results as shown are compelling, demonstrating god improvements on prior NP methods, including those leveraging attention."
            },
            "weaknesses": {
                "value": "- The authors might be more explicit about the ways in which memory is a meaningful limitation in conventional Neural Process methods. It may be possible to more-explicitly show both memory and method performance in a plot or table to illustrate the interplay.\n- While the authors demonstrate the relationship between block size and end-to-end time cost (in the appendix), doing so for their proposed approach compared to other latent variable modeling approaches might be helpful to contextualize CMANPs in terms of not only memory, but also computation time."
            },
            "questions": {
                "value": "- The authors may care to briefly explain here and in the manuscript: why is permutation invariance important for latent variable/neural process modeling?\n- Table 3 might be improved by including \"ours\" in relevant rows for readability.\n- The equation at the end of section 3 describing CMAB-AND is rather dense/difficult to read, even given an understanding of each of the components. An explanation of the update mechanism aside from the equation might help clarify.\n- As described in section 4.1, how much does increasing the number of iterations affect the performance of the model?\n- What were the resources used for training, and what qualified as \"too computationally expensive\" for Non-Diagonal variants trained on CelebA?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6544/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6544/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6544/Reviewer_SwB8"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6544/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698900311514,
        "cdate": 1698900311514,
        "tmdate": 1699636738142,
        "mdate": 1699636738142,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "e9KewcGzjS",
        "forum": "I0gwsdSgsk",
        "replyto": "I0gwsdSgsk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6544/Reviewer_GqA3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6544/Reviewer_GqA3"
        ],
        "content": {
            "summary": {
                "value": "The author proposed the constant memory attention block to make memory usage efficient in attentional neural processes."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The overall workflow is clear and the authors provide enough details on the proposed approach. The authors incorporated a rich set of baselines in the comparison."
            },
            "weaknesses": {
                "value": "At the very core of the algorithm, section 3.1.2, it seems the proposed approach basically achieves the constant memory by splitting the batch of input data into numerous slices of constant size, but at the same time, more operations are involved in the form of \"CA(LB,D) = UPDATE(D1,UPDATE(D2, . . .UPDATE(...\". Analysis is needed on if the added computational and operational complexity is worth the saved memory space. For such splitting of input batches, it seems to be more of an engineering design choice to be done by low level programming of GPU drive instead of tackling it at the high level architecture design.\n\nIt is always easy to modify some architectures/operations from current state of the art network structures for it to have approximately the same performance on a specific type of (meta learning) task. In my perspective, to claim concrete contribution of advancing current state of the arts based on network architecture engineering, the authors need to perform experiments on a wider range of tasks to show the versatility of proposed approach. The work seems pretty incremental on top of state of the arts attention models."
            },
            "questions": {
                "value": "is CMAB mechanism applicable to other types of learning models in addition to neural processes? Clarification is needed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6544/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698944027930,
        "cdate": 1698944027930,
        "tmdate": 1699636738022,
        "mdate": 1699636738022,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Uo9pgXpFLG",
        "forum": "I0gwsdSgsk",
        "replyto": "I0gwsdSgsk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6544/Reviewer_wchU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6544/Reviewer_wchU"
        ],
        "content": {
            "summary": {
                "value": "The paper presents the Constant Memory Attention Block (CMAB), a new attention mechanism that facilitates memory-efficient operations, crucial for applications where computational resources are limited. This approach streamlines the modeling process without the high memory demand typical of advanced attention mechanisms.\n\nExpanding on CMAB, the authors propose the Constant Memory Attentive Neural Processes (CMANPs) along with an autoregressive variant, CMANP-AND. This final model iteration maintains the constant memory efficiency characteristic of CMANPs, contrasting with the less scalable Not-Diagonal methods that require exponentially more memory as the number of target data points increases. CMANP-AND's design is essential for efficiently processing a full covariance matrix, a memory-hungry task, thus enabling it to achieve leading performance on Neural Process benchmarks with greater memory efficiency."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- **Clarity**: The paper is well-written and structured, offering a clear explanation of its contributions and comparisons with existing Neural Processes (NP) models, and provides results in a manner that is easy to follow.\n- **Quality**: The model is evaluated against established NP benchmarks. The constant memory advantage is supported by theoretical proofs and empirical evidence,\n- **Originality**: The paper's CMAB proposes an innovative approach to attention mechanisms, employing constant memory to enhance NP, which represents a notable advancement over the memory constraints associated with conventional models.\n- **Significance**: The CMABNP-AND's ability to bring high-performing deep learning to low-resource settings by addressing and overcoming the scalability limitations of contemporary models is a significant stride in widening their applicability.\n- **Applicability**: The paper is particularly notable for its application potential, demonstrating how the model can be scaled to tackle harder problems, such as higher-resolution challenges, a common limitation in existing models."
            },
            "weaknesses": {
                "value": "1. The main weakness that I see in this work lies in the trade-off between constant memory usage and computational efficiency. To maintain constant memory, the input data is divided into batches, requiring multiple iterative updates rather than a single computation. This approach, while memory-efficient, could potentially lead to increased computational time, as the number of updates is inversely proportional to the batch size. The figures provided offer valuable insights into the relationship between block size and time within the CMABNP model itself. However, to fully substantiate the efficiency claims, it would be advantageous to expand the experimental setup to include comparisons of the CMABNP model with traditional update rules. Such an experiment could involve running the same model with and without the new update rule e.g. across CalebA32, 64, and 128. This would not only highlight the benefits of the proposed method in a more diverse context but also guide users in understanding the practical trade-offs when implementing this model in real-world scenarios.\n2. While the paper posits the CMABNP model as particularly useful for streaming data settings like e.g. contextual bandits and Bayesian optimization, it lacks empirical evidence to support these claims. Given the relevance of such applications, it would strengthen the paper to include experiments that demonstrate the model's performance in these specific scenarios. Furthermore, a direct comparison with current state-of-the-art implementations, such as PFNs4BO [1], would not only validate the model's utility in practical settings but also offer a clearer understanding of its position relative to existing methods. \n3. The paper's discussion of implementing CMAB blocks in parallel for efficiency raises questions about the actual execution framework used in their experiments, particularly since the model is intended for low-resource settings where parallel processing might not be an option. It is not entirely clear to me whether the efficient parallel implementation was employed in the experiments or not. To address this, it would be constructive to see a comparison of the CMAB model's performance with parallel processing enabled versus a purely sequential implementation. Such an analysis would not only clarify the model's operational requirements but also help in understanding the practicality and scalability of CMAB in various computational environments.\n4. Lastly, the paper could be enhanced in the provision of implementation code.\n\n[1] PFNs4BO: In-Context Learning for Bayesian Optimization, Samuel M\u00fcller and Matthias Feurer and Noah Hollmann and Frank Hutter, ICML 2023"
            },
            "questions": {
                "value": "1. Could you detail the computational efficiency of the update process and provide comparative runtimes for the models?\n2. Is there empirical evidence to support the model's utility in streaming data settings?\n3. Can you clarify whether parallel computation was used in the experiments? And if so, how does the model perform when processed sequentially, as would be common in low-resource environments?\n4. Could you share the code?\n5. Could you provide in Figure 8, in the Appendix, visualizations that would clearly show out-of-distribution prediction?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6544/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6544/Reviewer_wchU",
                    "ICLR.cc/2024/Conference/Submission6544/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6544/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699565527681,
        "cdate": 1699565527681,
        "tmdate": 1700690027380,
        "mdate": 1700690027380,
        "license": "CC BY 4.0",
        "version": 2
    }
]