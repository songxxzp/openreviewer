[
    {
        "id": "hA1uHdiibM",
        "forum": "BWMFMIad3G",
        "replyto": "BWMFMIad3G",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3706/Reviewer_qx3e"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3706/Reviewer_qx3e"
        ],
        "content": {
            "summary": {
                "value": "The authors proposed a method for the task of weakly-supervised visual grounding. The method consists of four main parts: visual encoder, language encoder, CAM encoder, and multi-modality fusion part. The proposed method is trained with a new proposed loss fucntion. The experimental results on several mainstream datasets show the good performance compared with state-of-the-art methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The performance of the proposed method looks good on several Ref datasets compared with the others. It shows the effect of the weally-supervised on this challenging vision-language task. The paper also verifies the effect of the grad-cam features that can play an important role in the precise boundbing box prediction."
            },
            "weaknesses": {
                "value": "There are several main weakness of this paper, and here follows the details:\n\n1. The novelty of the proposed method. All the four main parts and their architecture are not new in the domian of vision-language. This kin of easy combination does not make a strong techinical contribution to this task.\n\n2. I am really interested in how to make use of the prior attention maps from Grad-CAM. Unfourtunately, I did not find the detailed information  about it. This could be the biggest stregth of this paper.\n\n3. A lot of details are missing in this paper, for example, which version of BERT did the authors adopt? How to address the difference of the feature dimensions when fusing them? \n\n4. Figure 5, Here I do not understand that with a regression predictor, how can the authors relate the predicted ones with the natural language words or nouns? More common cases could be multiple bounding boxes, how can the authors filter that and correspond that?"
            },
            "questions": {
                "value": "1. The authors mention \"focus on the entire object\". How did they do that? And how to measure the difference of entire object and partial attention maps in visual grounding task?\n\n2. The authors pointed out the weakness of transformer-based methods, and claimed the proposed method could addrees that on Page 2. Why and how to measure that?\n\n3. I suggest to introduce Grad-CAM method and the prior works based on it on Page 3.\n\n4. The content of the caption is repeated.\n\n5. Can the authors describe the details of the loss compared with that in GAIN?\n\n6. Have the authors done the ablation study about the loss function? The hyper-parameters are very important in this kind of training procedure.\n\n7. The experimental results did not show quite big difference between various versions. Which part do the authors think the proposed fusion method really helps?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3706/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3706/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3706/Reviewer_qx3e"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3706/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698434993933,
        "cdate": 1698434993933,
        "tmdate": 1699636326754,
        "mdate": 1699636326754,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PzzcYLQ3iW",
        "forum": "BWMFMIad3G",
        "replyto": "BWMFMIad3G",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3706/Reviewer_gxYC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3706/Reviewer_gxYC"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a method for weakly supervised visual grounding. They design a model that consists of an image encoder, a text encoder, a CAM encoder, and a Multi-Modality Fusion Module to fuse features from different encoders and predict bounding boxes for input text phrases. The novel part of the model is the CAM encoder, in which they first obtain a Grad-CAM heatmap from the input image, text and the CNN backbone, and use the heatmap as the input for the CAM encoder. They use the CAM encoder to calculate the attention mining loss to help the CAM features focus on the whole object mentioned in the text. They also adopt a self-taught regression\nloss and a phrase reconstruction loss proposed by Liu et al. to train the model. They compare with some previous methods on both weakly supervised visual grounding and fully supervised visual grounding on several benchmarks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "First, the experiments are comprehensive. They validate their methods on multiple benchmarks including RefCOCO, RefCOCO+, RefCOCOg, Flickr30, and ReferItGame. \nSecond, they conduct a bunch of experiments to show the effectiveness of each component in the model design, including the feature selection for the multi-modality fusion module and for the image encoder."
            },
            "weaknesses": {
                "value": "(1) Missing baselines for comparison. In this work, the author also uses an object detector for generating proposals, which is a usual setting for weakly supervised visual grounding. However, in the experimental section, the authors do not discuss or compare with multiple previous proposed methods such as [1-4]. If the authors add the results from these works, they will not achieve state-of-the-art results for at least Flickr30k and RefCOCO+. \n\n(2) The authors claim \"... Nevertheless, no previous attempts have been made to integrate Grad-CAM with existing weakly supervised visual grounding methods ...\", and \"... Although many methods have been proposed to improve the performance of weakly supervised visual grounding, none have made use of Grad-CAM, which is commonly used in weakly supervised training ...\". However, in [4], they also use Grad-CAM to help the visual grounding. \n\n(3) Missing discussion of previous works in weakly supervised visual grounding. In this paper, the authors still use object detectors to do the weakly supervised visual grounding. Unfortunately, training an object detector requires bounding box annotations (in this case, bounding box annotations in the VG dataset). Therefore, it is not exactly \"weakly supervised\" since the model still requires bounding box annotations to train the object detector. There exist works [5,6] that only use images and text for weakly supervised visual grounding, but the author did not discuss them in Section 2. \n\nReferences:\n\n[1] Dou, Zi-Yi, and Nanyun Peng. \"Improving pre-trained vision-and-language embeddings for phrase grounding.\" Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.\n\n[2] Wang, Qinxin, et al. \"MAF: Multimodal Alignment Framework for Weakly-Supervised Phrase Grounding.\" Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020.\n\n[3] Chen, Keqin, et al. \"Contrastive Learning with Expectation-Maximization for Weakly Supervised Phrase Grounding.\" Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022.\n\n[4] Yang, Ziyan, et al. \"Improving Visual Grounding by Encouraging Consistent Gradient-based Explanations.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n[5] Arbelle, Assaf, et al. \"Detector-free weakly supervised grounding by separation.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.\n\n[6] Shaharabany, Tal, and Lior Wolf. \"Similarity Maps for Self-Training Weakly-Supervised Phrase Grounding.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."
            },
            "questions": {
                "value": "(1) What is the object detector used in the method?\n\n(2) Please polish the figures in the paper. For example, Figure 4 is too small, making the text in the figure unclear. \n\n(3) For the qualitative results, can you provide the results for the same samples from other methods as well? That will make the qualitative results more impressive."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3706/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698769817363,
        "cdate": 1698769817363,
        "tmdate": 1699636326677,
        "mdate": 1699636326677,
        "license": "CC BY 4.0",
        "version": 2
    }
]