[
    {
        "id": "MNAknO0QYV",
        "forum": "t84UBRhhvp",
        "replyto": "t84UBRhhvp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6628/Reviewer_rjB2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6628/Reviewer_rjB2"
        ],
        "content": {
            "summary": {
                "value": "This paper is in line with a recent trend of augmenting CLIP's classification templates with class-specific descriptions generated from LLMs   (eg, GPT-3). The major technical contribution of this work is to concatenate the descriptions of all classes of interest, learn weights to aggregate these descriptions' text features and form new classifiers for each class. The weights are regularized with l1-norm to encourage sparsity. Experiments show that tuning with this classifier outperforms methods that work directly on top of vision features (without texts). Besides, information-theoretical analysis is also provided to show the improved invariance of text features (in CLIP's joint VL space) over raw vision features (output of the vision tower prior to linear projection)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "*Originality*: The technical contribution is relatively limited, which still fits in the scope of description selection. Its combination with other schemes like LP-FT is somehow instrumental. Yet the theoretical analysis is relatively novel and inspiring.\n\n*Clarity*: The paper is overall clearly written. Yet the organization could be improved, and more details could be provided to help understand details of the proposed method.\n\n*Significance*: This paper fits in the scope of visual representation learning under text supervision, and provides analysis on feature invariance and compression, which is helpful for the community."
            },
            "weaknesses": {
                "value": "I put the minor concerns in the section above (which did not harm my rating much), and list the major concerns here:\n\n1\\) The theoretical analysis is not well-aligned with the proposed method:\n- The major conclusion that could be derived from sec 3.3 is that features from CLIP's joint VL space are more compressed and invariant to visual variations (compared with vision features).\n- This is good and helps us understand CLIP itself, but does not explain why using descriptions (no matter w/ or w/o selection) is better than using other texts (eg, the default templates), which is the foundation of the proposed method.\n- From fig. 2 I find $I(H_{avd}; A)$ is almost identical to $I(H_{cp}; A)$, indicating descriptions do not introduce more invariance than template ensemble, which raises concern on why descriptions are needed in this work. $I(H_{avd}; Y)$ is higher than $I(H_{cp}; Y)$, which should indicate better predictions, yet in tab. 1 the gain of ZS-AVD over ZS is just marginal.\n\n2\\)  The experiments also do not support this work (description selection)'s superiority over other template designs:\n- In tab. 2 & 3, SLR's superiority over FT & LP under the few-shot setting is expected, since both WISE-FT's $W_\\text{learned}$ and LP's classifier are trained from scratch given only very few samples, thus could not match the performance of classifiers derived from CLIP's text encoder.\n- Yet this does not help understand SLR's strength over other classifiers. For instance, a\\) what if we drop the selection process and just follow Menon & Vondric's average ensemble, b\\) what about WaffleCLIP [1]'s random descriptors, c\\) how much is SLR's improvement over CLIP's default templates, and d\\) how do k-NN classifiers (use average pooling of the labeled few-shot samples to form classifiers, try both vision features and VL features) perform?\n\n3\\) No ablation study is provided to understand the proposed method (relatively minor)\n\nRef:\n[1] Roth et al., Waffling around for Performance: Visual Classification with Random Words and Broad Concepts, ICCV'23"
            },
            "questions": {
                "value": "One possible cause of ZS-VD's much inferior performance than ZS in tab.1 is the use of class templates. If the templates of ZS are also applied to ZS-VD, the difference in their performance should be marginal (refer to WaffleCLIP). I suggest the authors give it a try."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6628/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6628/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6628/Reviewer_rjB2"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6628/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698672102314,
        "cdate": 1698672102314,
        "tmdate": 1699636757075,
        "mdate": 1699636757075,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LcomefRDmK",
        "forum": "t84UBRhhvp",
        "replyto": "t84UBRhhvp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6628/Reviewer_CDXQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6628/Reviewer_CDXQ"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a method for obtaining more robust CLIP models. First, multiple visual descriptions are generated by an LLM and then used by the CLIP model alongside encodings of the class names. The visual representation is projected into the class + descriptors space and then a sparse logistic regression is trained on this representation.  As shown by experiments estimating mutual information, projecting on the class embeddings and visual descriptions minimizes the mutual-info with the domain, while selecting sparse features follows the bottleneck principle. Experimentally, the method is shown to produce better results than good baselines and can be combined with existing methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* S1. The method is well-motivated.\n\n* S2. The method is sound. Both the visual descriptions and sparsity are good ways to improve robustness."
            },
            "weaknesses": {
                "value": "* W1. The paper already compares against a few methods, but some additional baselines and ablations would also help. First, the full method but with logistic regression instead of sparse logistic regression (this will show if the sparsity is really important). The same method without using the video descriptions (partially shown in Table 1). Second, an MLP with a similar number of parameters as SLR-AVD, with different levels of weight decay. Third, use random projections / learnable matrices instead of the U projections (this will also induce a bottleneck). \n\n* W2. It will be good to have an overall comparison with different methods and an ablation study of the proposed method. I am thinking mainly of using the same base model (e.g. ViT- B/16), maybe just some standard few-shot k (e.g. k=16). A table with all datasets as columns and different methods (ZS, LP, MLP probing, FullFinetunning, CoOp, Wise) and ablations (ZS-AVD, SLR-AVD etc.).\n\n* W3. It will be good to see the performance of *all* models presenting using different number of shots: 1,2,4,8,16, 32. At the moment some ablations contain only up to 4 or 16 shots.\n\n* W4. The paper focuses on few-shot learning and this is an important area. To see the tradeoffs of the proposed method and the baselines, it will be interesting to also see the performance using larger training sets. For example, use k from 64, 256, 1024, etc. This way we can see at which scale of training samples is the proposed method more beneficial.\n\n\n* W5. Comparison with CoOP does not seem to be fair. \u201cSince CoOp injects \u201cclassname\u201d to the prompt during inference directly, this enforces a very strong prior. For a fair comparison, we also inject a strong prior by interpolating our learned linear head\u201d. It is not clear what this means, and why a direct comparison is not fair. Why is the \u201cclassname\u201d injection a strong prior for CoOP? Isn\u2019t SLR-AVD also using the same classname to produce the class prompts (CP)? Combining the proposed method with Wise and comparing to plain CoOp seems unfair.\n\n* W5.2. The comparison with CoOP is made using Resnet-50, which gives poorer performance for CLIP. Comparison using the ViT models should also be made.\n\n* W6. It is not clear how hyperparameter selection is done. Hyperparameter and model selection are crucial for domain generalization, thus this should be made more clear. For Wise models, alpha seems to be selected optimally, using validation OOD data.\n\n* W7. In Figure 4 top, it seems like WISE-FT+LP, which finetunes only the last linear layer, is compared against WISE-FT+SLR-AVD, which finetunes the entire model. Is this correct, or is WISE-FT+SLR-AVD finetunning only the linear classifier? Both should either update the linear layer or full-finetuning. Also, what is the difference between WISE-FT+LP and WISE-SLR?\n\n\n* W8. The paper will benefit from a better presentation. There are multiple acronyms, and sometimes the difference between them is not clear. The section in the appendix explaining the acronyms should be expanded with more details and should contain all acronyms and combinations used (e.g. WISE-FT+SLR-AVD, WISE-FT+SLR-AVD). \n* W8.2 Minor: Figure 4 should be improved, e.g. use consistent symbols, especially for start and end points. Show the optimal checkpoint in the figure."
            },
            "questions": {
                "value": "Q: What is the number of learnable parameters of SLR-AVD, how does it compare to linear probing?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6628/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698776862780,
        "cdate": 1698776862780,
        "tmdate": 1699636756954,
        "mdate": 1699636756954,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OnTIytdML9",
        "forum": "t84UBRhhvp",
        "replyto": "t84UBRhhvp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6628/Reviewer_cpSM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6628/Reviewer_cpSM"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method to produce better zero-shot classifiers for vision-language models, more specifically CLIP.\nTo do so, the set of standard class prompts used to construct zero-shot classifiers is enhanced using GPT-3 with extra textual descriptions.\nMoreover, $\\ell_1$ regularized logistic regression classifier is trained to select certain textual descriptions for each class.\nThis way, slightly better performance is achieved on ImageNet datasets compared to the standard prompts."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "One of the strengths of this paper is the idea of sparse selection of automatically generated prompts for classes (for which we want to learn a zero-shot classifier).\nThere are several prompt templates, which are provided as input to GPT-3 to retrieve textual descriptions for classes.\nThen $\\ell_1$-regularized logistic regression is trained to select the most discriminative prompts for each class.\nThis strategy brings slight performance boost over manually constructing only a few (if not one) class prompts."
            },
            "weaknesses": {
                "value": "There are two main concerns that I would like to raise.\n\n1) The benefit of automatically generating prompts for obtaining zero-shot classifiers is not very clear and significant. Table-1 compares the proposed method of generating textual desciptions from GPT-3 (ZS-AVD) against using only manually defined class prompts (ZS), and we see marginal improvements (max a few decimal points). It would be nice to see the impact of the number and diversity of generated prompts into performance. Section 4.1 mentions that \"...class names are probably one of the strongest prompts... One can certainly try to improve ZS-VD results by more carefully prompting GPT-3, or gathering descriptors from different data sources/search engines.\" this contradicts with the motivation of the paper, no?\n\n2) The benefit of using $\\ell_1$-regularized logistic regression technique is not very clear either. It would be nice to see if simple $\\ell_2$ regularization performs the same, or what kind of prompts selected for certain classes using different regularization criterons. Also, a simple k-NN based approach can also be applied as a baseline both with soft or hard assignment. On the other hand, similar logistic regression can be trained also for ZS (where there can be multiple manually defined class prompts). Maybe the impact is only due to learning weights ($W$) which connect the two modalities (image and text) using some regularization technique.\nSection-5 mentions that \"Applying sparse logistic regression then successfully selects the important features, which turn out to be intuitive\" but we don't see any evidence, right?\n\nMinor comments:\n- 1st sentence of Introduction: \"Self-supervised vision-language models (VLMs) like CLIP...\" is there any reference claiming CLIP to be self-supervised?\n- 2nd sentence of Section-2: \"WLOG...\" what is WLOG?\n- Missing closing paranthesis in Figure-1\n- 4th paragraph of Section-3.1 (\"Denote M ...\") is very confusing. It would be nice to explain all $U$ and $W$ in a diagram/visualization.\n- Figure-3 caption \"the x-axis represents...\" (not y)\n- Figure-3 what is the label for y-axis?"
            },
            "questions": {
                "value": "I would like the authors to address the concerns I listed in the weaknesses part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6628/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698864077995,
        "cdate": 1698864077995,
        "tmdate": 1699636756832,
        "mdate": 1699636756832,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zFNuMLqXAi",
        "forum": "t84UBRhhvp",
        "replyto": "t84UBRhhvp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6628/Reviewer_piqv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6628/Reviewer_piqv"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a model SLR-AVD which first automatically generates visual descriptions of each class via a LLM, then use a VLM to translate these descriptions to a set of viaul feature embeddings of each image. The features are proved to be more invariant to domain shift than traditional image embeddings with information-theory. The SLR-AVD is validated on both in-distribution and out-of-distribution classification."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposed model is novel, which extracts multiple potential visual features of each class, and then uses L1-regularized logistic regression to fit a sparse linear classifier on top of these visual descriptions. The generated descriptive features are proved to retain substantial information about the true labels, making them good invariant representations."
            },
            "weaknesses": {
                "value": "The paper writing and the experiments need improvement. \n\n1. The training and inference process is not clear. \n- What is the loss function used to train the model?\n- The three W matrices W_{vd}, W_{cp}, and W_{avd} are used as zero-shot classifiers. However, the inference process of the zero-shot classifier is not clearly explained.\n- In Section 3.2 paragraph 2, it would be better to mathematically specify how to regularize W_{avd}, and how to pick three features for each class with the largest coefficients.\n2. The experiments show limited performance gain in zero-shot classification.\n- the results in Table 1 indicate that ZS-VD performs worse than ZS. ZS-AVD only provides marginal improvement over ZS, i.e., 0.74 (IN), 0.74 (IN-V2), 0.13 (IN-R), 0.23 (IN-A), 0.27 (ObjectNet).\n3. In Figure 3, it would be interesting to show the performance of the ZS-VD model.\n4. In section 4.2, which dataset is the OOD test set?\n\nI am looking forward to the authors' responses and would like to adjust my rating if the questions are properly addressed."
            },
            "questions": {
                "value": "Figure 4 is unclear and the sub-caption in the bottom-right corner is blocked."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6628/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699099300526,
        "cdate": 1699099300526,
        "tmdate": 1699636756688,
        "mdate": 1699636756688,
        "license": "CC BY 4.0",
        "version": 2
    }
]