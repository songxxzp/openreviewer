[
    {
        "id": "7bYSCSynjf",
        "forum": "T97kxctihq",
        "replyto": "T97kxctihq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1688/Reviewer_TkUX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1688/Reviewer_TkUX"
        ],
        "content": {
            "summary": {
                "value": "This paper offers a timely and rigorous critique of the recent trend of using transformer-based models for long-term time series forecasting, and arrives at several surprising and important empirical conclusions. One of these conclusions is that just a simple linear layer (as in DLinear) combined with RevIN (reversible instance normalization with a learnable affine mapping of statistics) results in superior long-term forecast performance. The second is, even more shockingly, that many popular transformer-based methods with random parameter initializations already outperform their trained counterparts on long-term multivariate point forecasting tasks. The paper further investigates failure modes of single-layer linear forecasters in the multivariate setting. \n\nThe paper is well-written and of interest to the community in terms of the findings it offers. However, the novelty of the paper is limited for both methodology and theory."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is well-written, and its empirical findings are surprising and insightful for time series forecasting practitioners. \n\nThe first such conclusion results in a new and easy-to-implement model: RLinear. RLinear is simply DLinear with RevIN, and offers competitive performance. The second empirical exploration calls into question the use of large transformer architectures for forecasting. When using transformers to \"extract features\" the authors find that the learned mappings are easily confused and fail to capture the most basic signals with periodicity. In fact, this effect of \"overfitting\" appears so pronounced that the random initializations of these models appear to generalize almost better than their fitted counterparts. This is a very suprising finding that I look forward to verifying with the experimental setup the authors will make available.\n\nThe paper then moves to discussing why linear maps suffice to fit periodic signals. They offer an interesting comparative study of how linear models behave with different methods for normalization/disentanglement."
            },
            "weaknesses": {
                "value": "My main critique of the paper is its limited novelty. While the insights offered by the paper are quite useful, the main methodological advance offered simply combines two very recent ideas. Moreover, the validity of this method is also not rigorously tested with an ablation/comparative study. For example, there is no study as in Table 2 for the idea in Figure 8. \n\nThe theoretical results proposed in the paper are hardly novel. For example, Thm 1 barely needs to be denoted so (periodic functions can be reconstructed by shifting, and shifting is a linear operator..). Similarly, Thm 3 produces a somewhat unsurprising conclusion which is not contextualized well in the paper's presentation.\n\nAmong other points the authors may like to consider:\n- For the uninitiated reader, the architecture of RLinear (and its simplicity) are hard to understand, and Figure 1 are hard to understand. Perhaps they could be revisited\n- Footnote 4 and how it relates to the context is unclear.\n- Please cite the DLinear paper with the accepted venue.\n- Please revisit the conclusion section for grammar: e.g.,  \"investigate\" \"where they generally prone\" \"encounter\"\n- Just a suggestion: The recent TiDE paper attacks some of the same problems as the authors. Although that work does not appear to be peer-reviewed yet it may be worthwihle to briefly discuss it in context."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1688/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1688/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1688/Reviewer_TkUX"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1688/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698582795075,
        "cdate": 1698582795075,
        "tmdate": 1699636097138,
        "mdate": 1699636097138,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zDIbHeFuz6",
        "forum": "T97kxctihq",
        "replyto": "T97kxctihq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1688/Reviewer_SH4h"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1688/Reviewer_SH4h"
        ],
        "content": {
            "summary": {
                "value": "The authors of this work study the long-term time series forecasting (LTSF) problem. They demonstrate that a single linear layer can perform competitively for LTSF compared to other complex architectures. Specifically, with theoretical analysis and empirical studies, the authors find that affine mapping is critical in LTSF tasks and inspect its efficacy. While dealing with multivariate time series data, the limitation of linear models is also discussed."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The authors study the fundamental mechanisms that affect the performance of recent LTSF models, which is important for the community. \n2. This work provides theoretical and empirical evidence to support its findings.\n3. The presentation is well-organized and easy to follow."
            },
            "weaknesses": {
                "value": "1. As a research work submitted to a top-tier ML conference, the technical contribution is limited. In particular, novel solutions/algorithms that leverage the important findings are expected. \n2. It is better to investigate more real-world time series datasets with perplexing patterns."
            },
            "questions": {
                "value": "What about the efficacy of affine mapping when the models deal with noisy and low-quality time series data?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1688/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698631886447,
        "cdate": 1698631886447,
        "tmdate": 1699636097060,
        "mdate": 1699636097060,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QEt6IcKzgW",
        "forum": "T97kxctihq",
        "replyto": "T97kxctihq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1688/Reviewer_uhGz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1688/Reviewer_uhGz"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the recent approaches in Long-Term Time Series Forecasting (LTSF) especially around the one-pass prediction over a single linear layer that is seen to achieve competitive performance on par with other complex architectures.\nThe authors state that affine mapping in LTSF models effectively capture periodic patterns and thus dominates in the forecasting performance over the baseline models. However, there are no information about what affine mapping is or how affine mapping is effectively introduced in the LTSF models to improve performance.\nThe authors state that a single linear projection layer with Reversible Normalization (RevIN) outperforms the state-of-the-art models currently because the single layer feature extractor learns weights that are consistent with the projection layer which indeed imply a mapping pattern between input to output series. But this doesn\u2019t effectively prove, affine mapping is the reason for the performance improvement.\nThe authors investigate the disentanglement of seasonal and trend terms of the time series model and state that there is research gap on the advanced models still left to explore. The authors also question the effectiveness of temporal feature extractors on LTSF tasks but there is no investigation around it that leads to the impact of the temporal feature extractors on the model performance other than affine mapping.\nFinally, the authors run a series of experimental evaluation and state that the large models, PatchTST and TimesNet do not exhibit significant improvement over baseline single layer models and the authors are assuming it to be the models\u2019 efficacy to learn periodicity through affine mapping. Also, in multiple channel use-cases, a single layer is observed to fail without modelling each channel independently which again questions the findings of the investigation."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The paper gives a detailed analysis of various use-cases of LTSF on public datasets and makes solid observations.\n2. The authors prove the importance of Reversible Normalization(RevIN) and increasing input horizon on multi-channel on the LTSF model performances over multiple in-depth experiments."
            },
            "weaknesses": {
                "value": "1. The findings of the paper in the abstract are not evidently proved in the experiments.\n2. This being an investigation paper, the experiments around affine mapping don\u2019t include Mean\nBias Error for evaluation which effectively depicts the efficacy of affine transformation.\n3. The conclusion states that the affine mapping dominates in \u201csome\u201d LTSF models but there is no\ndetailed information on this generalized statement.\n4. TYPOS: It\u2019s a well-written paper. I had to point out just one on a high-level overview. Use of \u201cInstead\u201d and \u201cApart from\u201d with \u201cHowever\u201d in the Introduction were slightly confusing. Please consider checking those."
            },
            "questions": {
                "value": "The work in this paper is detailed and had observations from extensive experiments on both simulated and real-world datasets, but the findings are inconsistent. The findings stated in the abstract are not derived across the experiments through the conclusion.\nI believe there are many studies on why and how affine transformations can improve time-series forecasting models. As an investigation paper, I would recommend to consider the pros and cons of the current research to effectively conclude the findings and future work that is required on the topic. For example, in this paper, X. M. Chen, Y. Li, R. Z. Wang; Performance study of affine transformation and the advanced clear-sky model to improve intra-day solar forecasts. J. Renewable Sustainable Energy 1 July 2020; 12 (4): 043703., the results about affine transformation is similar to the findings of our paper. This paper should consider investigating the use-case used in this paper and fetch out the results and challenges faced in this use-case."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1688/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698699620958,
        "cdate": 1698699620958,
        "tmdate": 1699636096955,
        "mdate": 1699636096955,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kbvjtb3BKZ",
        "forum": "T97kxctihq",
        "replyto": "T97kxctihq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1688/Reviewer_NWqo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1688/Reviewer_NWqo"
        ],
        "content": {
            "summary": {
                "value": "Long-term time series forecasting (LTSF) is an important problem. Based on a finding from previous work that a linear layer can achieve forecasting performance comparable to complex models such as Transformers, in this paper, the authors study the effectiveness of recent approaches and provide various findings about their limitations. The authors provide theoretical and experimental explanations to support their findings."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper provides various types of visualization to support their claims and to present the results of experiments.\n2. Simplicity is always appreciated. It is nice to observe simple models can achieve performance similar to or even better than complicated models."
            },
            "weaknesses": {
                "value": "1. The main claim of Section 2 is unclear. Are simple models always better than complex models for LTSF, or is it the case only to the specific framework shown in Figure 1? What if we adopt a different but still complicated framework, such as 1-dimensional CNNs? How do the results change if we include traditional models such as AR (autoregression) or ARIMA?\n2. This paper is not self-contained. The experiments in Section 2 play a crucial role to motivate this work, but there is not enough description about the models and experimental setup. For example, RevIN is mentioned several times throughout the paper, but there is no definition of it. If the page limit is a problem, the authors could have added the details to Appendix.\n3. Theorem 1 and 2, which are the main theoretical contributions of this work, seem trivial. If a time series can be clearly (and linearly) separated into seasonality and trend parts, I think it is obvious that a linear layer (or any linear function) is able to learn such separation."
            },
            "questions": {
                "value": "1. What is the main improvement of this work from (Zeng et al., 2022)? This works seems like a re-verification of the claim made by the previous work.\n2. What do the authors suggest as a result of this work? Should we replace the benchmarks for LTSF with simple linear models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1688/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698822603317,
        "cdate": 1698822603317,
        "tmdate": 1699636096866,
        "mdate": 1699636096866,
        "license": "CC BY 4.0",
        "version": 2
    }
]