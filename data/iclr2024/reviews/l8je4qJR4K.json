[
    {
        "id": "UhjdXe69in",
        "forum": "l8je4qJR4K",
        "replyto": "l8je4qJR4K",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3253/Reviewer_Vftx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3253/Reviewer_Vftx"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to learn a two-level, hierarchical latent space with each layer partitioned into the content and style group. The content group controls the data-invariant features, while the style group controls the data style factors. Such a model aims to address the difficulty in domain generalization that label-specific and domain-related features are not well distinguished."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is in general well-written and well-presented with the illustration figures.\n2. The idea is motivated to address the practical issue in an active research field.\n3. The author conducts various experiment settings, including toy data synthesis and ablation studies."
            },
            "weaknesses": {
                "value": "1. I don't have much experience in this particular research field, so based on my understanding, the main purpose of the paper is to learn a well-defined and smooth latent space that can distinguish the domain features and style features; therefore, the model can perform well when the underlying distribution shift happens. The two-level latent space seems to be related to the hierarchical VAEs, where multi-layer latent variables are used to learn different levels of data features. So, how does such a two-level latent space compare or connect to the hierarchical VAEs?\n\n2. I understand learning a separated latent space for different data features can be beneficial to learning a smoother model manifold. But how does this model improve the performance of DG? The author mentioned that \"The key to achieving DG based on our model is recovering the distribution of content factors and isolating them from style factors.\" An additional and detailed explanation would be good."
            },
            "questions": {
                "value": "Please see the Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3253/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3253/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3253/Reviewer_Vftx"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3253/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698162362672,
        "cdate": 1698162362672,
        "tmdate": 1699636273637,
        "mdate": 1699636273637,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mKNzTx0Fj7",
        "forum": "l8je4qJR4K",
        "replyto": "l8je4qJR4K",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3253/Reviewer_XKs5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3253/Reviewer_XKs5"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to recover the label-specific content factors and isolate them from the style ones utilizing a two-level latent space, and consequently learn a stable predictor applicable to all domains. Specifically, they propose a novel data generation process and then exploit a VAE-based framework to achieve the latent variable identifiability based on some assumptions. Theoretical analysis and various experimental results demonstrate the effectiveness of this method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.This work proposes a two-level latent space that allows for better identifiablility of latent content factors from raw data.\n\n2.This work designs a practical VAE-based framework and also provides sufficient theoretical analysis.\n\n3.Extensive experiments that conducted on both synthetic and real-world datasets show the validity of the proposed framework."
            },
            "weaknesses": {
                "value": "1.It is common to decouple the raw data into domain-invariant and domain-specific parts in feature disentanglement methods and some causality-based methods. Though this work proposes a two-level latent space to assist the isolation of latent variables, the novelty is still limited. \n\n2.The proposed framework requires the use of domain variables, which are not accessible in some cases, limiting the application of this method.\n\n3.To better show the superiority of this work, it is necessary to compare the experimental results of the proposed method with that of the similar works, such as LaCIM, iMSDA, and the works mentioned in Sec 3.1. Besides, the baselines compared in tables are somewhat outdated.\n\n4.The authors are suggested to evaluate the algorithm on Domainnet dataset to verify its effectiveness on large-scale datasets."
            },
            "questions": {
                "value": "See Weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3253/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3253/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3253/Reviewer_XKs5"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3253/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698772319488,
        "cdate": 1698772319488,
        "tmdate": 1699636273538,
        "mdate": 1699636273538,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Sbozyu3K8G",
        "forum": "l8je4qJR4K",
        "replyto": "l8je4qJR4K",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3253/Reviewer_f1Ds"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3253/Reviewer_f1Ds"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel approach for domain generalization by introducing a two-level latent variable model.\nThe key idea is to partition the latent space into invariant content factors and variant style factors across domains. Specifically, the high-level latent space consists of content and style variables.\nThe content variables capture label-related information while the style variables represent domain-specific information.\nTo achieve identifiability, the model introduces a middle-level latent space with the same partition structure.\nThe middle-level content factors are derived from the high-level content factors via label-specific functions.\nSimilarly, the middle-level style factors are obtained by applying component-wise monotonic functions to the high-level style factors, which depend on the label, domain, and variable index.\nThe observation is then generated by applying a mixing function on the middle-level latent factors, which is shared across domains.\nThe key theoretical contribution is providing sufficient conditions to achieve identifiability and isolation of the content factors from the style factors. This relies on assuming the style factors follow an exponential family distribution conditioned on label and domain, plus a domain variability assumption.\nUnder these assumptions, the authors prove the content factors can be block-identified and style factors can be linearly identified.\nBased on the theoretical results, the paper proposes a practical learning algorithm using a VAE framework.\nThe VAE encoder estimates the posterior of the latent factors.\nNormalizing flows are used to transform between the high-level and middle-level latent variables.\nAn invariant classifier is trained solely on the recovered content factors.\nExperiments on synthetic and real-world image datasets demonstrate the approach can effectively identify the latent factors and that training on just the content factors improves domain generalization performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper made contributions: 1) A novel identifiable latent variable model with content/style partition 2) Sufficient conditions for identifiability and isolation of content factors 3) A practical learning algorithm based on VAEs and normalizing flows 4) Strong empirical performance on domain generalization tasks. The proposed approach offers a promising way to learn invariant representations for generalizable models. This paper proposes a novel two-level latent variable model with content/style partitioning to achieve domain generalization. This framework allows dependence between factors while still enabling identifiability. This work introduces sufficient conditions for identifiability and isolation of content factors based on exponential family priors and domain variability assumptions, combining VAEs and normalizing flows in a new way to estimate latent factors for domain generalization.\n- Provides thorough theoretical analysis and identifiability guarantees for the proposed model.\n- Learning invariant representations is an important open problem for building generalizable ML models. Methodology could be applied to other domain generalization areas beyond image classification."
            },
            "weaknesses": {
                "value": "- The method relies on specific assumptions about the latent variable distributions which may not hold universally. For example, the exponential family prior and domain variability assumptions.\n- The theoretical analysis requires infinite data and may not provide guarantees for small sample sizes. More analysis of finite sample behavior would be useful.\n- The model structure imposes some limitations, like only allowing dependence between factors through the label, but more complex relationships may exist in real datasets.\n- While state-of-the-art results are shown, the gains are incremental. More significant jumps in performance may be needed to drive adoption."
            },
            "questions": {
                "value": "Please see above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3253/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698827364450,
        "cdate": 1698827364450,
        "tmdate": 1699636273465,
        "mdate": 1699636273465,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0FgmvXj7BJ",
        "forum": "l8je4qJR4K",
        "replyto": "l8je4qJR4K",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3253/Reviewer_2mKD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3253/Reviewer_2mKD"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes two-level latent variables for improving domain generalization. The key idea is to employ other $z_c$ and $z_s$ to  recover the distribution of content factors and isolating them from style factors."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The idea is innovative and interesting. \n\n- The experimental results are convincing."
            },
            "weaknesses": {
                "value": "- The notions used in this paper are confusing, making it hard to read. The authors can simplify the notions in formal setup because it seems that the paper only uses a few of them. Moreover, in Eq. (1), what is the meaning of the distribution inside $\\phi(.,.)$.\n\n- The generative process or data generation model also confuses me. It is not clear why $f_y(\\hat{z_c})$ can return $z_{c_1}, z_{c_2}, z_{c_3}$. Also, the same question is for the style branch. What are $\\hat{z_{s_1}}, \\hat{z_{s_2}}, \\hat{z_{s_3}}$? Are they the styles of the domains? \n\n- Eq. (7) is also hard to interpret to me. As far as I understand, in the first level you have a single variable $\\hat{z_s}$ for the style and using the map $f_{e,y,i}$ to transform it to $p(z_s \\mid y,e)$. However, why do you need the index $i$ here?"
            },
            "questions": {
                "value": "Please answer my questions in the weakness session. Moreover, how do you design $f_y$ and $f_{e,y,i}$ using distinct flow-based architecture to incorporate the information of e, y, i? I am happy to increase my score if the authors can resolve my unclear points."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3253/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3253/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3253/Reviewer_2mKD"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3253/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699054914945,
        "cdate": 1699054914945,
        "tmdate": 1700686015630,
        "mdate": 1700686015630,
        "license": "CC BY 4.0",
        "version": 2
    }
]