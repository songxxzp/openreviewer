[
    {
        "id": "WsGJOO1mQR",
        "forum": "lgvOSEMEQS",
        "replyto": "lgvOSEMEQS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6655/Reviewer_91xa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6655/Reviewer_91xa"
        ],
        "content": {
            "summary": {
                "value": "This paper uses the CLIP image and text encoder for unsupervised learning in FL. Specifically, it uses the image encoder and text features extracted in the server in the client for training. Besides, it also generates data samples on clients to mitigate data imbalance problems."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The idea is interesting and the Figure 1 illustrates the idea well.\n- The paper the generally well-written and easy to follow.\n- The proposed method achieves significantly better performance than compared counterparts, especially under heterogeneous data distribution.\n- The proposed method only requires a few rounds of communication."
            },
            "weaknesses": {
                "value": "- Some important experimental details are missing. For example, the model architecture used for training the other methods.\n- The paper mentions computation efficiency in Section 4.2.3, but it seems that the computation saving is mainly from the reduction of the communication round. The paper seems to focus on cross-device FL. It would be useful to further investigate whether the device could fit in the model size of CLIP and has enough memory to use it for inference.\n- The compared methods are somewhat weak baselines. Some important unsupervised FL baselines and methods are not discussed or compared in the paper, e.g., [1][2][3][4]\n    - [1] Collaborative Unsupervised Visual Representation Learning from Decentralized Data. ICCV\u201921\n    - [2] Divergence-aware Federated Self-Supervised Learning. ICLR\u201922.\n    - [3] Orchestra: Unsupervised Federated Learning via Globally Consistent Clustering. ICML\u201922\n    - [4] MocoSFL: Enabling Cross-client Collaborative Self-supervised Learning. ICLR\u201923\n- Some papers mentioning adopting CLIP to FL are not discussed. e.g. [5][6]\n    - [5] Fedclip: Fast generalization and personalization for clip in federated learning.\n    - [6] When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions."
            },
            "questions": {
                "value": "- What is the impact of using different types of backbone?\n- What is the impact of training for more local epochs in each round?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6655/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6655/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6655/Reviewer_91xa"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6655/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698552222004,
        "cdate": 1698552222004,
        "tmdate": 1699636760972,
        "mdate": 1699636760972,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FViNP8lNz7",
        "forum": "lgvOSEMEQS",
        "replyto": "lgvOSEMEQS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6655/Reviewer_JRVk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6655/Reviewer_JRVk"
        ],
        "content": {
            "summary": {
                "value": "This work proposed a novel lightweight unsupervised federated learning approach, FSTCBDG, to alleviate the computational and communication costs as well as the human labor of data annotations. The evaluation results illustrated the proposed FSTCBDG significantly outperforms the baseline methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Developed a lightweight unsupervised federated learning approach based on a single linear layer.\n\n2. Designed a self-training objective for the linear classifier\n\n3. Conducted extensive experiments to show the good performance of the proposed method over the baselines."
            },
            "weaknesses": {
                "value": "1. The class prototype augmentation based on Gaussian noise is not novel, since this idea has been used in prior work called PASS [Zhu CVPR 2021]. In addition, some follow-up works like [Zhu CVPR 2022] have pointed out that synthetic data from Gaussian-based augmentation would make some similar classes overlap with each other. Thus, the proposed method in this paper may not work well.\n\n2. It may need to explain why the testing accuracy of baselines drops as the number of communication rounds increases.\n\n3. Why did not compare with the FedUL baseline?\n\n**References:**\n\n[Zhu CVPR 2021] Prototype Augmentation and Self-Supervision for Incremental Learning, CVPR,2021\n\n[Zhu CVPR 2022]. \"Self-sustaining representation expansion for non-exemplar class-incremental learning.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022."
            },
            "questions": {
                "value": "Please see the comments above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6655/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698718747063,
        "cdate": 1698718747063,
        "tmdate": 1699636760828,
        "mdate": 1699636760828,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HyIfR7afzv",
        "forum": "lgvOSEMEQS",
        "replyto": "lgvOSEMEQS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6655/Reviewer_b2QK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6655/Reviewer_b2QK"
        ],
        "content": {
            "summary": {
                "value": "This study leverages a pre-trained vision-language model for unsupervised image classification within a federated learning framework. The authors introduce two strategies to enhance the zero-shot prediction capabilities of CLIP. Experiments show that the proposed framework achieves better results compared to conventional supervised federated learning approaches."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tCompared to other federated learning scenarios, such as supervised and semi-supervised methods, unsupervised federated learning remains a relatively unexplored domain.\n2.\tFrom the ablation study, the proposed two approaches can effectively improve the zero-shot prediction accuracy of CLIP and mitigate the class imbalance problem to some extent."
            },
            "weaknesses": {
                "value": "1.\tThe contributions of this study did not meet the anticipated expectations. The two methods introduced lack novelty. The idea of refining pseudo-labels has been previously explored within the context of self-supervised learning. Additionally, the class-balanced data generation draws parallels with the Synthetic Minority Over-sampling Technique and can be categorized as an oversampling strategy.\n2.\tWhile the authors highlight the lightweight nature of their proposed framework, evidence throughout the paper seems insufficient. The sole indication of its lightweight character is the use of a linear classifier during training. However, this isn't a unique aspect as the baseline methods employ the same classifier. The purported lightweight advantage of the proposed framework isn't adequately substantiated. Furthermore, a comprehensive analysis of both computation and communication costs is essential to truly label the framework as lightweight and communication efficient.\n3.\tMore experiments are required, especially for large scale datasets like ImageNet.\n4.\tThere are some typos, like FedBR (Guo et al., 2023b) FedBR (Guo et al., 2023b), tranfer, etc."
            },
            "questions": {
                "value": "please respond to the weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6655/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698834938831,
        "cdate": 1698834938831,
        "tmdate": 1699636760695,
        "mdate": 1699636760695,
        "license": "CC BY 4.0",
        "version": 2
    }
]