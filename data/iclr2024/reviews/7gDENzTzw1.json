[
    {
        "id": "KOZahWTXUL",
        "forum": "7gDENzTzw1",
        "replyto": "7gDENzTzw1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3965/Reviewer_cDHy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3965/Reviewer_cDHy"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the vulnerability of reinforcement learning agents to adversarial attacks by manipulating state observations. The authors propose a algorithm focusing on developing a pessimistic policy that accounts for uncertainties in state information. This is supplemented by belief state inference and diffusion-based state purification techniques."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Addressed an important problem in Reinforcement Learning, providing fresh perspective and insights.\nThe experimental design and methodology are well-constructed."
            },
            "weaknesses": {
                "value": "The innovative aspects presented over WocaR-DQN appear to be incremental in nature."
            },
            "questions": {
                "value": "How extensive or generalizable are the empirical results presented? Given that the study focuses on the Continuous Gridworld, which is relatively simple, and only includes two Atari games. \n\nConsidering that Atari game screens are depicted using 8-bit (256 levels) RGB, yet typically select colors from a narrower palette, how effective are the different attack budgets (15/255, 3/255, 1/255) tested in this study? Specifically, can these perturbations alter the colors enough to cause confusion with other colors in the game's limited palette? If not, could the observed robustness of the learning method simply be a consequence of learning to filter out specific colors?\n\nIs the proposed method also applicable to environments like Mujoco or Mountain Car, where the input variables are more continuous and less discrete than those in Atari games?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3965/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698650740220,
        "cdate": 1698650740220,
        "tmdate": 1699636357638,
        "mdate": 1699636357638,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UwfCirMikx",
        "forum": "7gDENzTzw1",
        "replyto": "7gDENzTzw1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3965/Reviewer_GsDy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3965/Reviewer_GsDy"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a novel RL algorithm that aims to protect RL agents from adversarial state perturbations. The authors propose a pessimistic DQN algorithm that takes into consideration both the worst-case scenarios and belief about the true states. The algorithm also features a diffusion-based state purification method for applications like Atari games. The paper shows empirical results demonstrating that their approach significantly outperforms existing solutions in robustness against strong adversarial attacks, while maintaining comparable computational complexity."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The empirical results show performance improvement under strong attacks compared to baseline methods. The algorithm works well for both simplistic environments and more complex scenarios like Atari games with raw pixel input.\n2. The algorithm's computational overhead is comparable to existing methods that use regularization terms."
            },
            "weaknesses": {
                "value": "1. The algorithm assumes access to a clean environment during training, which may not always be the case in real-world applications.\n2. While the diffusion model adds robustness, it also adds computational overhead, potentially making it slower at test time."
            },
            "questions": {
                "value": "1. How sensitive is your algorithm to the choice of hyperparameters?\n2. Given the requirement for a clean training environment, how would your method perform in a scenario where such an environment is not readily available?\n3. The diffusion model increases computational complexity during the test stage. Are there ways to optimize this without compromising the robustness?\n4. Why PA-AD is not evaluated on continuous gridworld?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3965/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698730493260,
        "cdate": 1698730493260,
        "tmdate": 1699636357548,
        "mdate": 1699636357548,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7T2OU6u2pH",
        "forum": "7gDENzTzw1",
        "replyto": "7gDENzTzw1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3965/Reviewer_achV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3965/Reviewer_achV"
        ],
        "content": {
            "summary": {
                "value": "The authors study the problem of defense in presence of perceived state attack in Reinforcement learning and propose a method to approximately solve the Stackelberg equilibrium between the agents and the adversary. Their method involves solving pessimistic Q-learning and estimating belief state of the agent and using them for state purification. They propose two algorithms called BP-DQN and DP-DQN to defend against adversarial attacks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Proposes a novel method to defend against adversarial attacks in RL by combining pessimistic Q-learning with belief state estimation and state purification objective.\n    \n2. Authors provided theoretical results to compare the policy found by their algorithm to the optimal policy.\n    \n3. Presented interesting empirical experiments to demonstrate effectiveness of their algorithm on several examples."
            },
            "weaknesses": {
                "value": "1. It has been shown that Stackelberg Equilibrium as defined in Definition 2 need not always exist, refer to theorem 4.3 [https://arxiv.org/pdf/2212.02705.pdf](https://arxiv.org/pdf/2212.02705.pdf). So, finding an approximate solution for them is meaningless. However, non-existence of Stackelberg equilibrium is a worst case phenomenon. Authors should incorporate this in their paper.\n    \n2. It will be great if authors can include a short paragraph before section 3.2 discussing a big picture of their strategies before diving into each one of them. It would also help to include some mathematical details in each section.\n    \n3. Abstract wrongly mentions that past methods either regularize or retrain the policy. However, methods like Bharti et.al. just purify the states directly."
            },
            "questions": {
                "value": "1. It is well known that defense against perceived state attack requires solving a partially observable MDP which is a hard problem to solve in general. Could you clarify how your method is able to avoid these hardness issues?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3965/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698770242823,
        "cdate": 1698770242823,
        "tmdate": 1699636357474,
        "mdate": 1699636357474,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mcRQltYSgH",
        "forum": "7gDENzTzw1",
        "replyto": "7gDENzTzw1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3965/Reviewer_QHWx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3965/Reviewer_QHWx"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes two new algorithms to robustify agent policies against adversarial attacks. It formulates the problem of finding a robust policy as a Stacleberg game (where agents choose policies), and then further incorporates beliefs into the derived algorithm. For pixel-based observation spaces, the game uses a diffusion-based method to derive valid possible states. The paper is very well structured and easy to follow."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- the paper addresses many shortcomings of current works\n- the theoretical algorithms and derivations are insightful\n- the practical implementation of the derived algorithms are well motivated"
            },
            "weaknesses": {
                "value": "- the paper assumes that both the clean MDP and the perturbation budget are known to both the victim and the attacker\n- it would be interesting to run an ablation on these assumptions. How well does the method work if the budget is not known exactly, or if the MDP transition function is not known exactly?"
            },
            "questions": {
                "value": "see weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3965/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698827071366,
        "cdate": 1698827071366,
        "tmdate": 1699636357402,
        "mdate": 1699636357402,
        "license": "CC BY 4.0",
        "version": 2
    }
]