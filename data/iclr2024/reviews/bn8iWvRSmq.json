[
    {
        "id": "bWczBpSC6S",
        "forum": "bn8iWvRSmq",
        "replyto": "bn8iWvRSmq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3085/Reviewer_bNWc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3085/Reviewer_bNWc"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces SF-GEN, which is grounded in two primary concepts: successor features (SFs) to decouple the LLM\u2019s dynamics from task-specific rewards, and language model rectification to proportionally adjust the probability of selecting a token based on the\nlikelihood that the finished text becomes undesired. The result is promising."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The use of successor features is novel for controllable NLG and provides benefits like adding/removing control dimensions efficiently.\n* Requires simpler training than methods like discriminator guides or adapter tuning.\n* Achieves strong performance - on par or better than various baselines.\n* More efficient in memory and computation compared to other methods."
            },
            "weaknesses": {
                "value": "* The linearity of rewards can limit expressiveness for more complex control objectives.\n* Not as performant as state-of-the-art methods like RECT for single dimension control.\n* Limited analysis of how it handles multiple simultaneous control dimensions."
            },
            "questions": {
                "value": "NA"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3085/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698659895171,
        "cdate": 1698659895171,
        "tmdate": 1699636254309,
        "mdate": 1699636254309,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AboCUFLSFr",
        "forum": "bn8iWvRSmq",
        "replyto": "bn8iWvRSmq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3085/Reviewer_NYoH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3085/Reviewer_NYoH"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces successor features (SFs) into controllable text generation (CTG) and proposes an efficient decoding framework for multi-subject CTG from the perspective of reinforcement learning (RL). The experimental results on two CTG tasks primarily demonstrate its great performance when compared to other baselines, while maintaining high efficiency. Specifically, in comparison to other methods, the advantage of introducing SFs in this task is evident in its efficiency due to being retraining-free and having lower inference costs (see Strength.2 for more details). In summary, the contributions of this paper are as follows:\n\n1. Building upon previous research that framed LM's generation within the RL framework (Cao et al., 2023), this paper is the first to explore SFs in this research domain and to design a plausible SF-based CTG generation framework.\n\n2. The method introduced in this paper sheds light on the efficiency in the design of CTG tasks, which represents a valuable contribution to GreenNLP."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. **Originality**: This paper presents original research by exploring the application of SFs in multi-subject CTG, utilizing an RL framework and building upon existing theories. This empirical application is the first work in the field of pre-trained LM.\n2. **Soundness**: The utilization of SFs offers solutions to address challenges present in previous paradigms: \n    - a) The proposed framework leverages SFs to disentangle LM's dynamics from subject rewards, demonstrating flexibility in overcoming the challenges associated with retraining-based methodologies and their associated optimization costs. \n    - b) In comparison to other decoding-based methods, the test-time inference cost is reduced, owing to the decreased computational load on tensors.\n3. **Significance**: This efficient solution for multi-subject CTG provides valuable insights into how to steer the generation of pre-trained (or large) LMs, with the potential to mitigate bias-related issues without introducing substantial inference latency."
            },
            "weaknesses": {
                "value": "1. **Performance**: The performance of the proposed method may not be excellent when compared to existing baselines, especially for RECT. To address this concern, the authors could highlight the efficiency of their work in the experiments, in addition to the inference time (discussed in Section 5.2).\n2. **Claims**: While the authors mention the application of large LMs in this paper, the main experiments and analysis primarily focus on previous pre-trained LM, specifically GPT-2-large, which lacks instruction-following capabilities. Although GPT4ALL-J is used for prompting experiments, the authors might consider exploring more application scenarios for large LMs. I acknowledge that not all of the CTG paper ought to chase popular large LMs, it is essential to ensure that the claims made regarding large LMs, such as those found in the abstract, are adequately supported through experiments involving large LMs.\n3. **Literature review**: Notably, recent parameter-efficient transfer learning methods [1] are used in multi-subject CTG [2]. The authors may consider discussing this paradigm within the paper.\n4. **Clarity**: Several typos and clarity issues are present in the paper:\n   - The abbreviation \"Eq X\" is used alongside \"Equation X\" (page 7) in this paper. It is advisable to standardize the expression.\n   - In Section 3: \"The state $s_t \\in \\mathcal{S}$ consists of the prompt and the concatenation of the previously generated tokens.\" However, it is worth noting that some pre-trained LMs may not take the prompt as their input. The definition of the prompt should be clarified.\n   - In Section 3.3, while \"SARSA\" is a well-known concept, the authors should consider citing relevant literature when mentioning it in this paper for the first time.\n   - In Section 3.4, \"Laroche et al. (2017)).\" should be corrected to \"Laroche et al. (2017).\"\n\n\n**References**:\n\n[1] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for NLP. In International Conference on Machine Learning, pages 2790\u20132799.\n\n[2] Kexin Yang, Dayiheng Liu, Wenqiang Lei, Baosong Yang, Mingfeng Xue, Boxing Chen, and Jun Xie. 2023. Tailor: A Soft-Prompt-Based Approach to Attribute-Based Controlled Text Generation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 410\u2013427, Toronto, Canada. Association for Computational Linguistics."
            },
            "questions": {
                "value": "1. Section 3.2: How can Eq 10 be simplified to match Eq 3? Is the focus of this simplification on $r_w(s, a, s')$ in Eq 2?\n\n2. Section 3.3: $\\phi$ is parameterized by the output of the final layer of pre-trained LMs. Have the differences between various networks been compared? Is there an exploration of whether simpler networks can achieve similar effectiveness?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3085/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3085/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3085/Reviewer_NYoH"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3085/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698826286241,
        "cdate": 1698826286241,
        "tmdate": 1700658306680,
        "mdate": 1700658306680,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "E3hD8b0Ahk",
        "forum": "bn8iWvRSmq",
        "replyto": "bn8iWvRSmq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3085/Reviewer_th5r"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3085/Reviewer_th5r"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces SF-Gen to tackle controlled text generation without finetuning a LM\u2019s parameters. SF-Gen is based on two key concepts: (1) language model rectification and (2) successor features (SF). (1) learns a value function to adjust token selection probability during decoding to avoid undesired discourse. (2) disentangles the computation of value functions and tasks, requiring only two models (LLM and SF model) regardless of the end tasks.\n\nExperiments are conducted on two tasks (1) sentiment control and (2) LM detoxification where SF-Gen is compared with baselines that also do not require LM retraining."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ A light-weight solution to controlled text generation. No LM retraining needed and only one additional model is maintained for multiple tasks.\n\n+ Comprehensive experimental design and analysis."
            },
            "weaknesses": {
                "value": "- Compared with baselines, SF-Gen lags behind some approaches such as DExperts in sentiment control and Rect in both sentiment control and detoxification."
            },
            "questions": {
                "value": "* In the analysis of combining reward parameters in 5.1, at maximum 3 reward parameters are combined. What if more are added? I imagine as the number of subjects get too large, SFs will have insufficient capacity to model them, or there is no interference at all?\n\n* What\u2019s the main rationale for focusing on GPT-2 XL? Would you expect the observation being different when the base LM is switched to a different one from another family (e.g. Llama) or a different scale?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3085/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699098790074,
        "cdate": 1699098790074,
        "tmdate": 1699636254077,
        "mdate": 1699636254077,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Wm5kHvweW9",
        "forum": "bn8iWvRSmq",
        "replyto": "bn8iWvRSmq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3085/Reviewer_WhQL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3085/Reviewer_WhQL"
        ],
        "content": {
            "summary": {
                "value": "Controlled text generation has emerged as a significant area of interest, especially when Large Language Models (LLMs) achieve remarkable results across broad applications. However, a potential issue is the typical requirement for retraining LLMs when there is a shift in the control target. To address this, the authors introduce SF-GEN, a method built upon two primary concepts: successor features (SFs) and language model rectification. SF-GEN, following the reinforcement learning (RL) framework for text generation, employs SFs to reduce the complexity of Q-value calculations. Meanwhile, SF-GEN seeks to address challenges associated with the application of SFs to text generation, such as the derivation of the Bellman equation, the interdependency of the value function and task-specific rewards, and the expansive action space. Besides, SF-GEN facilitates the concurrent control of multiple aspects by integrating various reward parameters. Comparative experiments conducted in text generation for sentiment control and detoxification show the superiority of SF-GEN over baselines and most current methods with respect to performance, memory efficiency, and computational speed. Subsequent analysis verifies the advantages of leveraging the decoupling effect of SFs in text generation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This work appears to be the first application of SFs, traditionally utilized within RL, to the domain of text generation. RL techniques have demonstrated efficacy in addressing specific challenges in NLP, such as RLHF, and this work is one more example. \n\n2. The adaptation of RL techniques for text generation in this paper is convincingly justified. Each component of the proposed method is introduced by articulating the current challenges and limitations, providing a clear reason for the design. \n\n3. The empirical evaluation showcases the superiority of the proposed method, with experiments across two datasets demonstrating enhanced performance, memory efficiency, and inference speed."
            },
            "weaknesses": {
                "value": "1. While the paper presents an application of SFs for controlled text generation, the core novelty seems incremental. The principal contribution lies in adapting SFs for multiple subject control within text generation tasks. Despite adjustments to tailor RL techniques to a new domain, the foundational aspects of the proposed SF-GEN method primarily rely on pre-existing approaches. \n\n2. The claimed superiority of the proposed SF-GEN method over competing approaches is not consistently demonstrated across all experimental settings. While potential explanations, such as the linearity constraint, are briefly touched upon, the paper does not offer substantial discussion or experimental evidence to corroborate these hypotheses or to fully account for the discrepancies. \n\n3. The scope of the experimental evaluation appears limited, with the evaluation on two datasets that share similarities. The choice to employ different LLMs for each task raises questions about the comparability of the results. The analysis focuses on the detoxification outcomes, which might present an incomplete picture. A more holistic evaluation, such as the training time in addition to the inference time, would contribute to a deeper understanding (e.g., time efficiency from an algorithmic perspective) of the proposed method."
            },
            "questions": {
                "value": "1. Could further clarification or empirical evidence be provided regarding the influence of the \"linearity constraint\" on the comparative results with RECT?\n\n2. Similarly, could additional insights be shared about the \"safety conditions\" that were factored into the comparative results with DEXPERTS? \n\n3. Regarding the combination of reward parameters, Table 4 does not clearly demonstrate the claim of \"without affecting the other\". Could the authors expand on this with more details to illustrate this aspect?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3085/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3085/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3085/Reviewer_WhQL"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3085/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699318871181,
        "cdate": 1699318871181,
        "tmdate": 1700506261064,
        "mdate": 1700506261064,
        "license": "CC BY 4.0",
        "version": 2
    }
]