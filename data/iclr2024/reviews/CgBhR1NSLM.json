[
    {
        "id": "W22o3ZnKva",
        "forum": "CgBhR1NSLM",
        "replyto": "CgBhR1NSLM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4360/Reviewer_PtGd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4360/Reviewer_PtGd"
        ],
        "content": {
            "summary": {
                "value": "The paper analyzes the effect of residual connections in MLPs on the loss landscape. The authors challenge the common belief that residual connections lead to smoother loss landscape and present an analysis showing the opposite. To this end, the authors introduce new notions of bumpiness and ruggedness and derive some properties. The paper further analyzes the evolution of these measures throughout training. Small MLPs with 2, 3, and 4 layers are analyzed on three low-dimensional data sets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Better understanding the training dynamics and architectural design choices of modern neural network architectures is an important endeavor. Challenging common assumptions about relevance of certain design choices and procedures such as residual connections can lead to broad impact and improvements across subfields."
            },
            "weaknesses": {
                "value": "- The results are hard to follow. The axis labels and ticks in he figures are barely readable; side-by-side plots have differently scaled axes without giving a rationale for this, which makes it hard to draw conclusions from them. I assume \u201cTrue\u201d/\u201dFalse\u201d refers to networks with and without residual connections? If so, it would be good to make this explicit.\n- Some of the design choices in the experiments are hard to understand for me. For example it is not discussed how using non-smooth ReLU activation functions affect their theory. To get around this, the authors could use e.g. the GELU activation function.\n- Similarly, modern MLPs can have a latent dimension different from the input dimension (replacing the skip connection with a linear map to accommodate the change in width). I feel the latent dimension of 2, 4, and 10 might not be representative for modern MLPs used in practice.\n- The authors use the optimization algorithm from (Bosman et al. 2018a). How does this differ from SGD or ADAM and other more popular algorithms? Why did the authors not use one of those?\n- It is unclear from the results presented in the paper whether the presented networks reach reasonable accuracy on the corresponding tasks."
            },
            "questions": {
                "value": "If I understand correctly, the proposed definition of bumpiness defined in Sec. 3.1 removes the absolute values to make it sensitive to whether the function is locally convex or concave. I\u2019m not convinced that this definition always leads to this outcome, it would be great if the authors could give some additional intuition."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4360/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698515649521,
        "cdate": 1698515649521,
        "tmdate": 1699636408011,
        "mdate": 1699636408011,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5AxBhjxAAw",
        "forum": "CgBhR1NSLM",
        "replyto": "CgBhR1NSLM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4360/Reviewer_TMom"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4360/Reviewer_TMom"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new type of visualization of Hessian clouds, as well as derive the bumpiness and ruggedness metrics. By using the proposed visualization, it empirical compare the loss landscape of MLPs with or without residual connection."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The motivation of the paper is to study the loss landscape via visualization meets general interests."
            },
            "weaknesses": {
                "value": "* It doesn't seem to be a paper that is ready for a machine learning / deep learning conference. It focuses on the visualization and there is no clear conclusion or insights from the results. \n* The visualization figures to follow and read. It would be very helpful if the authors could add more detailed explanations in each caption to show how the displayed patterns related to the insights and conclusions.\n* There is no model performance-related metric (e.g. accuracy) shown in the paper, which makes it hard to know if the proposed metrics and visualization are really meaningful for practical usage."
            },
            "questions": {
                "value": "* What is the motivation for proposing the new sharpness metrics bumpiness and ruggedness, what is the advantage of it compared to the conventional metrics like Hessian trace/top-eigenvalue? \n* What is the computation complexity of the proposed visualization? Is it possible to scale up to larger models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethical concern."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4360/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4360/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4360/Reviewer_TMom"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4360/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698548990086,
        "cdate": 1698548990086,
        "tmdate": 1699636407907,
        "mdate": 1699636407907,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mZg5Dx9xuc",
        "forum": "CgBhR1NSLM",
        "replyto": "CgBhR1NSLM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4360/Reviewer_Me3L"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4360/Reviewer_Me3L"
        ],
        "content": {
            "summary": {
                "value": "The authors extend an existing loss landscape visualisation technique (loss-gradient clouds) to incorporate ruggedness and bumpiness metrics, and study the effects of residual connections on MLPs using their proposed visualisation (Hessian clouds). The main contribution of the paper is the proposition of two new metrics to quantify landscape properties, as well as the extension of the existing visualisation method. Observations regarding the residual connections seem to mostly corroborate with existing insight."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is focused on enhancing existing visualisation methods, which is great: our tools are limited in the visualisation domain, and the topic is well worth the exploration. The metrics proposed by the authors (ruggedness and bumpiness) are mathematically sound. Visualisations are quite striking, illustrating that scatter plots applied to scalar representations of the NN loss landscapes can be a very effective tool to visually study these extremely high-dimensional problems.\n\n**Originality:** the proposed metrics for ruggedness and bumpiness are original, and clearly provide a new angle of view on the loss landscape. The visualisation itself (Hessian cloud) is a derivative of the loss-gradient cloud, and the residual connections seem to exhibit expected behaviour.\n\n**Quality and clarity:** the paper is mostly well-written, although clarity can be improved. See below my comments on explicitly referencing the appendices.\n\n**Significance:** in my opinion, authors could have focused on something more enigmatic than the residual connections - for example, the effect of regularisation methods such as dropout on the landscape. The proposed metrics are new and interesting, but the subject matter (residual connections) does not offer that much new insight."
            },
            "weaknesses": {
                "value": "Related work is a bit haphazard: the authors cite numerous papers, but do not structure the review into a coherent narrative.\n\nAuthors refer to various appendices (Appendix A.1, Appendix A.2, etc.) but I could not find either of the listed appendices in the paper. I realise that the appendices are included as supplementary material, but explicit references to supplementary material simply do not work. Can the authors perhaps add appendices directly to the paper, otherwise refrain from discussing appendix data explicitly? The paper must be self-contained, and in its current form the paper cannot be followed without the supplementary material.\n\nAuthors propose measures for bumpiness and ruggedness, but do not provide a clear intuition for the difference between the two. Adding a toy 1D example just to illustrate the difference between \u201cbumpy\u201d and \u201crugged\u201d can be very beneficial for the narrative.\n\nAuthor\u2019s choice of study subject (MLP with residual connections) is not that interesting: at the end of the day, the paper simply shows that residual connections make gradients steeper (expected). Authors also state that residual connections increase the ruggedness in MLPs, which was shown to not be the case for CNNs, which is an interesting observation, if true. In my opinion, one of the plots is not interpreted correctly in this regard. Either way, authors conclude that residual connections make training easier, which is once again a known fact. Thus, although a new lens to study loss landscapes is offered, the lens falls short of discovering something of importance.\n\nAuthors only make use of their proposed ruggedness/bumpiness metrics, which seems short-sighted. Ruggedness can be measured in terms of the Hessian trace or entropy of the walk - how do these existing metrics correlate with the proposed metric? A visual and numerical comparison would have enhanced the paper significantly. There is only one reason for new metrics: if the old metrics fail."
            },
            "questions": {
                "value": "Authors state that ruggedness is \u201can essential metric in measuring the network performance\u201d. Is this true? Could you provide evidence that ruggedness is strongly correlated with network performance?\n\nFigure 3: What do the \u201cTRUE\u201d and \u201cFALSE\u201d labels in the figure represent? Please discuss explicitly. Authors state that Figure 3 shows that residual connections yield more rugged landscapes - if the right pane corresponds to residual connections, then I disagree with the interpretation. It seems that residual connections yield stronger gradients (expected and desired effect), as well as overall lower ruggedness except for a few outliers.\n\nWhat is the difference between Figures 4, 5, and 6? All the axis (x,y,z) are identical, yet the observed patterns are different. How was the data generated for each of the plots?\n\nAbstract: \u201cbeneficial to generating\u201d -> beneficial for generating\n\nAbstract: \u201cresidual MLPs prefer to generate\u201d -> residual MLPs generate (MLPs do not have preferences, since they do not have agency)\n\n\u201cpeople\u2019s understanding\u201d - the understanding\n\n\u201cand it is a two-dimensional visualization\u201d - which is a two-dimensional visualization\n\n\u201cthe two-dimensional presentation method can capture some characteristics of the loss landscapes, but the results represented are still not clear enough\u201d - what do you mean by \u201cnot clear enough\u201d? This is a very vague statement.\n\n\u201cas discussed in Introduction 1\u201d - as discussed in Section 1."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4360/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698574816900,
        "cdate": 1698574816900,
        "tmdate": 1699636407813,
        "mdate": 1699636407813,
        "license": "CC BY 4.0",
        "version": 2
    }
]