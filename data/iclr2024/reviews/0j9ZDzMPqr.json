[
    {
        "id": "QHHAVfQgXz",
        "forum": "0j9ZDzMPqr",
        "replyto": "0j9ZDzMPqr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8687/Reviewer_WfpS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8687/Reviewer_WfpS"
        ],
        "content": {
            "summary": {
                "value": "The authors introduce a new method to obtain counterfactual (CF) explanations from unsupervised node learning. Their method uses a learned unsupervised node learning method to get embeddings. They then define their method via the importance function which they try to maximize while minimizing the edge alterations to the graph (minimizing the counterfactual explanation). They provide an upper bound on the Importance function. \n\nThey alter a Monte Carlo Tree Search (MCTS) method to get the CF. The MCTS uses the Importance function as a reward which looks for subgraphs that are important but sparse. They show that their MCTS does not degrade the expressiveness of subgraph explanations like the vanilla MCTS would by altering the reward function. They do this by choosing the action where the upper confidence boundary (UCB a term used in their MCTS which dictates which edge to take in traversal) is larger. They also show analysis showing that the UCB for an edge that leads to a new node is greater thus prioritizing exploring new paths. They claim these alterations to the vanilla MCTS leads to more expressive explanations which are partially supported by theoretical claims. \n\nThey also have quite a large and expansive set of experiments. They do experiments on 3 synthetic datasets (BA-Shapes, Tree-Cycles, and Tree-Grid) and 3 real datasets (Cora, CiteSeer, and PubMed), parameter sensitivity study, ablation study and a case study. They conduct their experiments on 8 methods (including theirs) on several metrics: Precision/Recall, Validity, size of the model and Importance. They show promising behaviour of their method in comparison to other methods in many settings with various evaluation metrics. They also show a case study on the NIPS dataset a social network of citations. They show by perturbing explanation graphs on a particular author they can obtain a graph that belongs to a different author that belongs to a different subfield of ML. They also conduct an ablation study showing variants of the MCTS algorithm which is fundamental to their methodology. They show their variant of MCTS can find expressive explanations (high importance score) while being efficient. Finally they also show experiments of parameter sensitivity. They show what effects the choice of the restart parameter, perturbation parameter, and the number of neighbors has on their method. They do this by varying the choice of hyperparameter and evaluating the importance score on the Cora Dataset."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is very well written. The design of the paper from problem definition, to methodology, to experimental evaluation follows clearly and is well designed. The authors also motivate their work by addressing the problem in a well defined manner. The Importance measure is novel and inventive way to quantify the counterfactual explanation. The alterations to the MCTS to construct these counterfactual explanations is reasonable and well grounded by theory to supplement their decisions. The paper also employs theory on the Importance measure to show an upper bound. \n\nThe experimental list is fairly exhaustive and shows superior performance to several other methods in multiple datasets and cases. The case study is a nice touch to display their method\u2019s ability to obtain meaningful counterfactual explanations. The ablation study shows that their variant of MCTS can find expressive subgraphs while being efficient in comparison to other tree search methods. Finally, having a study to show their methods sensitivity/robustness to choices of hyperparameters is important for anyone seeking to employ this method."
            },
            "weaknesses": {
                "value": "There could be more discussion on experiments where the UNR-Explainer underperforms compared to other methods. \n\nAlso further explanation on certain hyperparameter choices could be made more clear for the readers. Such as the choice of k in each experimental setting. A discussion on when to use a particular larger/smaller value of k would be interesting. The authors do have experiments showing the sensitivity of the number of neighbors, they also have a limited discussion on this phenomenon. However this hyperparameter is central to their method (their importance measure is heavily influenced by it) a discussion to explain what settings would require very large k vs very small k would be beneficial to solidify their work although it is not necessary."
            },
            "questions": {
                "value": "Although the authors provided a study of hyperparameter sensitivity, why did they select k=5. Clearly, as seen in the experiments the choice of k does seem to impact the importance score. More discussion of the effect of the choice of k would be beneficial for practitioners. \n\nAlso their method seems to underperform in the synthetic experiments particularly with the precision measure. Significantly smaller methods do better than UNR-Explainer in these settings which is seemingly consistent throughout the synthetic experiments. Any discussion as to why this is would be beneficial to readers and the authors."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8687/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8687/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8687/Reviewer_WfpS"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8687/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698003586963,
        "cdate": 1698003586963,
        "tmdate": 1699637089122,
        "mdate": 1699637089122,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Qi4fBjHFhU",
        "forum": "0j9ZDzMPqr",
        "replyto": "0j9ZDzMPqr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8687/Reviewer_TnSw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8687/Reviewer_TnSw"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a method called UNR-Explainer for generating counterfactual explanations in unsupervised node representation learning models. The goal of these explanations is to provide information for understanding unsupervised downstream tasks. UNR-Explainer performs Monte Carlo Tree Search to find the explanation subgraph. The subgraph importance is measured by the change of the top-k nearest neighboring nodes after perturbation. UNR-Explainer is evaluated on six datasets including both synthetic ones and real-world ones, and UNR-Explainer is shown to outperform existing explanation methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. UNR-Explainer shows the good quantitative performacne, and the case study on NIPS shows UNR-Explainer can select qualititatively meaningful subgraphs.\n\n2. The importance metric proposed in Equation 1 is novelt to me.\n\n3. Time complexity analysis and discussion of limitation are both included in the appendix.\n\n4. Code is provided for reproducibility."
            },
            "weaknesses": {
                "value": "1. Lacking discussions. Some baseline methods considered in the experiment section are very simple but achieve strong performance without discussion or analysis. See question 1 as well.\n\n2. Efficiency. MCTS-based explanation can be slow than other explanation methods, e.g., gradient-based methods, especially on large graphs. This is verified by the time complexity as well.\n\n3. Presentation can be further improved. Some figures have text that is too small to read. For example, embedding labels in figure 2."
            },
            "questions": {
                "value": "1. In Table 1 for the synthetic datasets, the naive random selection baselines 1hop-2N and 1hop-3N achieve the best results in terms of precision. Why? Any discussions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8687/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8687/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8687/Reviewer_TnSw"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8687/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698657905979,
        "cdate": 1698657905979,
        "tmdate": 1699637089001,
        "mdate": 1699637089001,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VeDgHeveH3",
        "forum": "0j9ZDzMPqr",
        "replyto": "0j9ZDzMPqr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8687/Reviewer_8JCL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8687/Reviewer_8JCL"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a novel method for explaining graph neural networks. The authors focus on counterfactual explanations and propose the UNR-Explainer, which aims to identify subgraphs that, when perturbed, lead to significant changes in node embeddings. The paper evaluates various explanation methods in unsupervised settings using synthetic and real-world datasets. The proposed method leverages the Monte Carlo Tree Search (MCTS) for efficient traversal in large search spaces. The paper also provides a theoretical analysis of the upper bound of Importance and discusses the algorithm for calculating Importance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1)\tThe paper tackles CF reasoning in unsupervised settings, a relatively unexplored area potential implications for explainability in graph neural networks and unsupervised learning.\n2)\tThe paper leverages the Monte Carlo Tree Search (MCTS), a technique from reinforcement learning, to efficiently traverse the search space of potential subgraphs. MCTS is known for its effectiveness in large search spaces, making it a suitable choice for this problem.\n3)\tThe paper clearly defines the counterfactual property for unsupervised representation learning models, providing a solid foundation for their method.\n4)\tThe paper includes a theoretical analysis of the upper bound of Importance for GraphSAGE, adding a rigorous foundation to their empirical findings."
            },
            "weaknesses": {
                "value": "1)\tWhile the paper does evaluate on both synthetic and real-world datasets, it might benefit from testing on more diverse datasets, especially those from different domains or with different characteristics. Information on how the method scales with larger datasets or more complex graphs, and its computational efficiency, would be valuable.\n2)\tI believe the paper would greatly benefit from additional visual illustrations or diagrams to depict the proposed method. Visual aids can provide a clearer understanding and offer readers an intuitive grasp of the methodology. Given the complexity and novelty of the approach, diagrams or flowcharts could enhance comprehension and make the content more accessible to a broader audience."
            },
            "questions": {
                "value": "1)\tHow does the method scale with larger and more complex graphs? Are there any computational or memory constraints that might limit its applicability to very large datasets?\n2)\tHow sensitive is the method to the degree of perturbation applied to the subgraph? Would minor changes in perturbation lead to significantly different results in the algorithm of importance?\n3)\tGiven the contrastive approach employed by DGI and the inductive learning capability of GraphSAGE, how might these characteristics influence the types of counterfactual explanations generated? Furthermore, how would the proposed counterfactual explanation method adapt and perform when integrated with generative models such as GraphGAE or S2GAE?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8687/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8687/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8687/Reviewer_8JCL"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8687/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698756015955,
        "cdate": 1698756015955,
        "tmdate": 1699637088885,
        "mdate": 1699637088885,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8qXxxq6QIR",
        "forum": "0j9ZDzMPqr",
        "replyto": "0j9ZDzMPqr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8687/Reviewer_eAe7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8687/Reviewer_eAe7"
        ],
        "content": {
            "summary": {
                "value": "This work explores explanation generation for unsupervised node representation learning.  The authors propose a Monte Carlo Tree Search (MCTS)-based method to generate counterfactual (CF) explanations. Specifically, this method aims to identify the most important subgraphs that cause a significant change in the k-nearest neighbors of a node. The proposed method is incorporated into unsupervised GraphSAGE and DGI, and the performance on six datasets confirms the efficacy of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. It is an interesting research topic to improve the interpretability of unsupervised learning models on graphs. \n2. This can help to find the explanations of  GNN models with unseen downstream tasks.\nThe proposed method is tested on several datasets and shows satisfactory results.\n3. The paper is well-structured and organized."
            },
            "weaknesses": {
                "value": "1. The work is somehow incremental work. SubgraphX proposed a Monte Carlo tree search algorithm to efficiently explore different subgraphs. Compared with SubgraphX, the authors seem to just add a new policy, \u201crestart\u201d, in the Selection step to mitigate the search bias. The design makes sense but results in limited novelty.\n2. The indicators of counterfactual explanations are not rigorous. The perturbations of the input graph not only change the node embedding of interest ($emb_{v} \\neq emb_{v}^{'}$) but also change other node embeddings. It does not match the Figure 1 (b) and (c) illustrated.\n3. The motivation should be further improved. The authors do not state the challenges of generating counterfactual explanations in unsupervised learning compared with supervised methods, such as CF-GNNExplainer, RCExplainer, and CF2.\n4. The authors do not provide real-world applications or pilot studies to support their claim that \"the perturbation strategy of adding edges or nodes has a significant risk in real-world scenarios\"\n5. Minor error: Page 9 The first line is not left-justified; Measures in Table 1 are not arrowed."
            },
            "questions": {
                "value": "1. Why do the authors choose the MCTS-based framework rather than other gradient-based or causal-based interpretable methods? Can you show the relevant analyses?\n2. Can the authors state what new challenges your approach addresses compared to existing counterfactual explanation methods on supervised learning?\n3. Can the author give some applications of real-world scenarios or do some pilot studies to show the benefit of the perturbation strategy of only removing edges?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8687/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8687/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8687/Reviewer_eAe7"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8687/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698781547775,
        "cdate": 1698781547775,
        "tmdate": 1700722158431,
        "mdate": 1700722158431,
        "license": "CC BY 4.0",
        "version": 2
    }
]