[
    {
        "id": "9QSIrY9ThZ",
        "forum": "LiNIIxm545",
        "replyto": "LiNIIxm545",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8868/Reviewer_Bpfu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8868/Reviewer_Bpfu"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a framework called EASE (Explanation-Aware Soft Ensemble) to improve the in-context learning capabilities of large language models (LLMs) by leveraging natural language explanations. The key ideas are:\n\n(1) Explanation-guided ensemble: Assign a score to each prediction generated by the LLM based on the quality of its associated explanation. This is done by using another LLM as an explanation scorer. Higher quality explanations result in higher weights during ensemble. Negative explanations are generated automatically without extra annotations.\n\n(2) Soft probability aggregation: Instead of using hard 0/1 predictions, aggregate the soft probability scores across different label verbalizers. This helps reduce noise and inconsistencies between explanations and predictions.\n\nThe experimental results on several NLU datasets show EASE outperforms baselines for in-context learning with explanations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(1) The idea of using LLM to score explanations and weigh predictions is quite simple and straightforward.\n\n(2) Makes good use of soft probabilities to address prediction noise and inconsistencies with explanations.\n\n(3) Provide empirical results demonstrating consistent improvements across multiple datasets and LLMs. Ablation studies further verify the efficacy of the two proposed techniques.\n\n(4) Well-written paper and thorough analysis and discussions."
            },
            "weaknesses": {
                "value": "(1) The approach increases computational overhead since it requires additional score for each explanation. Could be expensive for real-time applications.\n\n(2) Relies on access to model logits which may not be available with certain blackbox LLMs, e.g. Claude-2 and ChatGPT.\n\n(3) Improvements are not significantly, especially when models become larger.\n\n(4) Experimental results are mainly on multi-choice/classification tasks. Not quite sure these results can hold on asthmatic reasoning tasks, e.g. GSM8K.\n\n(5) Since the proposed techniques introduce additional computations, it will be good to know if adding more samples for self-consistency with similar overall computations can achieve similar performance. That is, adding more samples for self-consistency baselines, for fair comparisons."
            },
            "questions": {
                "value": "Missing references:\n\n(1) Ye et al. Explanation Selection Using Unlabeled Data for Chain-of-Thought Prompting. 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8868/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698556929608,
        "cdate": 1698556929608,
        "tmdate": 1699637116112,
        "mdate": 1699637116112,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QAEie4qXD8",
        "forum": "LiNIIxm545",
        "replyto": "LiNIIxm545",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8868/Reviewer_gauf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8868/Reviewer_gauf"
        ],
        "content": {
            "summary": {
                "value": "This work introduces a novel method to ensemble the explanations using a bootstrapped LLM scorer and a soft probability aggregator to improve the performance of LLMs on classification tasks. The authors mainly focus on natural language understanding tasks of natural language inference and question answering, with possible extensions to generation tasks in the future work. This work builds on top of the self consistency approach, where the prediction is based on majority voting, without taking into account the variance in explanation quality and the noise introduced in the sampling stage leading to predictions which are inconsistent with the explanations. To address these issues the authors use a bootstrapped LLM scorer to score the explanations using verbalizers, prompts and in-context demonstrations of negative and positive explanations. The negative explanations are obtained without any human annotations, by assuming that explanations leading to wrong predictions are in fact negative. Finally these scores are aggregated using the sum of the probabilities associated with each classification label and weighted by the explanation score, thereby reducing the effect of the noise originating from the sampling based algorithms and obtaining a more accurate result. The experiments are conducted on several benchmarks like E-SNLI, ECQA etc. Compared to baselines like in-context learning, predict then explain framework, self-consistency etc, the EASE framework introduced in this work performs the best on all the benchmarks using PALM as the backbone as well as other open-sourced models like FLAN-UL2 and Llama-2. This shows the effectiveness and the generalisability of the proposed method"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "This work produces a simple intuitive approach to improve the prediction of LLMs using the explanations generated. The experiments conducted on various benchmarks and different LLMs shows that the proposed method can be generalised to any problem. All the in-context demonstrations required for the bootstrap LLM scorer are generated automatically without the need for human annotations. The ablations performed shows the effectiveness of using both the bootstrapped LLM scorer and the soft probability aggregations introduced in this work. The study of the inconsistency ratio and their corresponding relation with the improvement in the performance across several benchmarks, thoroughly explains the need for the soft probability aggregation framework. Similarly the need for bootstrapped LLM scorer is explained using human annotations and compared with the previous methods of lexical match and NLI models, showcasing its superior performance. Overall, there is a very detailed explanation of the introduced method and the ablations performed motivating the need for the proposed framework."
            },
            "weaknesses": {
                "value": "There is a limited number of types of tasks studied, confined only to NLI and QA in the English language. There is a possibility of the framework failing in cases where the LLM scorer is wrong at every stage of the framework starting with the bootstrapped LLM scorer assigning a higher weight to the explanation being a positive verbaliser when in fact it could be negative. This error could be carried downstream to the soft probability aggregator where the prediction generated using the explanation could be wrong, leading to a worse performance. Since this method is tested out on popular easy tasks like NLI this effect may not be visible. If the framework could be tested on other tasks like multilingual NLI or uncommon sense reasoning where the LLMs do not have a particularly good performance, this effect may be visible."
            },
            "questions": {
                "value": "1)Were the EP and the PE methods tested using other sampling methods or other greedy sampling, which may have given diverse explanations leading to better results?\n\n2)Are there any insights into why the performance of the EASE framework without bootstrapped LLM scorer works slightly better/on par with the EASE framework for certain benchmarks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8868/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8868/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8868/Reviewer_gauf"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8868/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698980965988,
        "cdate": 1698980965988,
        "tmdate": 1699637116005,
        "mdate": 1699637116005,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bsXDhF7ksU",
        "forum": "LiNIIxm545",
        "replyto": "LiNIIxm545",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8868/Reviewer_HfbG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8868/Reviewer_HfbG"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a framework called EASE (Explanation-Aware Soft Ensemble) to improve in-context learning for large language models (LLMs) using natural language explanations. The authors address the limitations of existing methods by introducing explanation-aware ensemble and soft probability aggregation techniques. They conduct experiments on seven natural language understanding tasks and four LLMs, demonstrating the effectiveness of their proposed framework."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The authors provide extensive experimental results on multiple datasets and LLMs, demonstrating the effectiveness of their proposed framework. They compare their approach with several baselines and achieve superior performance in most cases."
            },
            "weaknesses": {
                "value": "- The paper introduces a framework that leverages explanations to enhance in-context learning for LLMs, which is not quite novel as there are existing works that uses explanations to enhance in-context learning for LLMs as well [1,2]. \n\n- While the experimental results are promising, the paper lacks a theoretical analysis of the proposed framework. A more in-depth analysis of the underlying principles and theoretical guarantees would strengthen the paper.\n\n- The authors briefly mention the limitations of their approach, such as the reliance on LLMs for explanation scoring and the absence of negative explanations. However, they do not provide a thorough discussion of these limitations or potential ways to mitigate them.\n\n- The authors compare their full framework with several baselines, but they do not provide ablation studies to analyze the individual contributions of the explanation-aware ensemble and soft probability aggregation techniques. A more detailed analysis of these techniques would provide a better understanding of their impact.\n\n\nReferences\n\n- [1] Krishna, Satyapriya, et al. \"Post hoc explanations of language models can improve language models.\" arXiv preprint arXiv:2305.11426 (2023).\n\n- [2] Bills, Steven, et al. \"Language models can explain neurons in language models.\" URL https://openaipublic. blob. core. windows. net/neuron-explainer/paper/index. html.(Date accessed: 14.05. 2023) (2023)."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8868/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699014478755,
        "cdate": 1699014478755,
        "tmdate": 1699637115880,
        "mdate": 1699637115880,
        "license": "CC BY 4.0",
        "version": 2
    }
]