[
    {
        "id": "HYzMgpQcot",
        "forum": "LLbHdII8Pg",
        "replyto": "LLbHdII8Pg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3840/Reviewer_a4Uq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3840/Reviewer_a4Uq"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose the first work that mitigates the risk of model leakage by preventing attackers from both unauthorized inference and cross-domain transfer, thus achieving comprehensive protection for DNN models. Experiments demonstrate their design outperforms the state-of-the-art model protection works and exhibits robustness against different fine-tuning methods employed by attackers."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Propose the first work that mitigates the risk of model leakage\n- Experiments demonstrate their design outperforms the state-of-the-art model protection works"
            },
            "weaknesses": {
                "value": "- The threat model is not practical\n\nIn this paper, the authors assume an unpractical threat model for the attacker. Although this can be the worst case for the defender, such unpractical threat model for attack can make that such an attack can never happen in the real world. Thus, developing a valid defense for such unrealistic attack is not very meaningful. Thus, it would be great if the authors can provide more detailed discussion or justification on the threat model side. \n\n- Lack of justification on the representativeness of their evaluation setup\n\nThe evaluation setup for this paper such as dataset and models selection is very ad-hoc. There is no justification on the reason for such selections. For instance, are the selection representative or the state-of-the-art? Without such justification, it is unclear whether their designs and findings can transfer and work well in the most representative setup."
            },
            "questions": {
                "value": "Provide the justification on the threat model and evaluation setup."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3840/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698620488048,
        "cdate": 1698620488048,
        "tmdate": 1699636341882,
        "mdate": 1699636341882,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9epDd7CtXF",
        "forum": "LLbHdII8Pg",
        "replyto": "LLbHdII8Pg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3840/Reviewer_RU2E"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3840/Reviewer_RU2E"
        ],
        "content": {
            "summary": {
                "value": "**Paper Summary**\n\nThis paper presents a method to protect the intellectual property (IP) of a model when an attacker gains access to both the model architecture and parameters. The approach employs a bi-level optimization method to reduce the fine-tuning accuracy of a model on various nearby target domains. The results show that this approach can significantly reduce the model transferability from the source domain to any unauthorized target domain."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "**Strengths:**\n- The paper is well-organized and easy to follow.\n- The algorithm is robust."
            },
            "weaknesses": {
                "value": "**Weaknesses:**\n- I have concerns regarding the threat model and evaluation method which are given in questions.\n- The practical real-world applications of this approach are not clear."
            },
            "questions": {
                "value": "**Comments:**\n- I'm uncertain if this is a mistake, but in your threat model, you mentioned, \"We assume the attacker is able to extract the DNN model, including its architecture and well-trained weights.\" I find this confusing. What exactly is the Trusted Execution Environment (TEE) used to protect in this context? How is it feasible to protect the model against **unauthorized inference** when the attacker has both the model weights and architecture?\n\n- Comparing transferability is challenging. In the evaluation, you present the final test accuracy of a model transferred from the source to the target domain, which is a good starting point. However, it is hard to conclude that the approach reduces model transferability. Many other factors could influence your evaluation, such as model initialization and target domain selection. Is it possible to include learning curves in your analysis?\n\n- Can you show that the bi-level optimization added during the training stage does not compromise model quality in the source domain? What you are trying to show is the existence of distinct local minima for the source domain compared to nearby target domains. This is very hard to prove.\n\nI find the problem and the solution in this paper intriguing. However, I still have some concerns regarding the evaluation and threat model."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3840/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698818092374,
        "cdate": 1698818092374,
        "tmdate": 1699636341813,
        "mdate": 1699636341813,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Yhb1LUMDtJ",
        "forum": "LLbHdII8Pg",
        "replyto": "LLbHdII8Pg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3840/Reviewer_rJhp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3840/Reviewer_rJhp"
        ],
        "content": {
            "summary": {
                "value": "This work presents DeTrans which protects on-device DNN models against unauthorized inference and cross-domain transfer while preserving model performance for users using TEE.  By selectively modifying a small subset of weights in the pre-trained model, DeTrans achieves near-random guess performance on the source domain and transferability reduction for potential target domains."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Threat model covering both unauthorized inference and cross-domain transfer is realistic."
            },
            "weaknesses": {
                "value": "1. Evaluation is limited to small dataset. Can authors provide results on CIFAR-100, Tiny-ImageNet and ImageNet?\n\n2. Lack of comparisons against wider range of prior arts."
            },
            "questions": {
                "value": "Please see the weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3840/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3840/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3840/Reviewer_rJhp"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3840/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698823408723,
        "cdate": 1698823408723,
        "tmdate": 1699636341738,
        "mdate": 1699636341738,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XYOcvaMwru",
        "forum": "LLbHdII8Pg",
        "replyto": "LLbHdII8Pg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3840/Reviewer_Uryd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3840/Reviewer_Uryd"
        ],
        "content": {
            "summary": {
                "value": "The paper  considers the problem of preventing misuse of DNNs from an IP perspective in two ways: \n(1)  prevent unauthorized users or use on unauthorized hardware from using the model; In past work, this problem has been addressed by use of TEEs. The model is protected in some way, with various approaches, in a hardware-based TEE. Any use on unlicensed hardware either results in degraded performance or entirely prevented.\n\n(2)  Even if (1) is ensured, restrict the use of the model to only the source domain (a specified domain) and not allow use on other domains. This is referred to as cross-domain transfer. As the paper on page 1 points out, (Wang et al. 2022/2023) identified this problem.  \n\nUnfortunately, what is not clear is why prior work doesn't already solve the problem.  See Questions below in the review. It appears that (Wang et al. 2022/2023) not only identified the problem, but also proposed a solution to the problem.  Furthermore, Wang et al. 2022 cites to solutions for (1) and was meant to address the gap (2), and thus anticipated the straightforward combination.  In other words, One uses Wang et al. to develop a model that performs well on the source domain but poorly on non-source domains and then uses a solution to (1) to restrict the availability of the resulting model on only authorized hardware.  Thus, the motivation for the work is not clear and the premise appears weak or lacking a clear well-stated motivation.\n\nThe paper  points to Wang et al. 2022 again in Section 2.4,   acknowledging that they solve (2), but then says these methods cannot be extended to \"pre-trained models\".  But, if this limitation of Wang et al. is really important, it seems that should have been introduced in the abstract and Intro and claimed as the contribution. But the paper does not do that.\n\nOverall, it is not clear precisely what problem is being solved in this paper that is not already solved. The abstract and Intro need to explain the contributions and the problem much better. For instance, if the paper is really addressing that the model needs to be extensible to \"pre-trained models\", and that is the distinguishing contribution from prior work, the overall pitch needs to change considerably."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper correctly points out that DNNs can be valuable IP and limiting their use to authorized devices and only on desired domains (source domains) are important. It provides a framework that claims to address both issues."
            },
            "weaknesses": {
                "value": "The paper lacks proper foundation. It seems the problems that the paper claims to solve for the first time are already solved by prior work or easily addressed by combining existing techniques. See Questions below.\n\nI recommend addressing that first (see Questions below). Once that is addressed, I may have further questions on the performance results and the proposed method. But, right now, the contributions are not properly situated with respect to prior work and addressing that is crucial.\n\n The rebuttal suggests that authorized users will not do any attacks and are fully benign. Is that a realistic assumption? Can't an unauthorized user simply become an authorized user, if the device is cheap to buy (it seems the authors assume the devices are relatively modest devices). It would be good to know one scenario where the threat model is realistic. If the distribution of the device is extremely restricted, is the solution needed? \n\n\nIt appears that the scheme is a variant of Zhou et al.'s  2023 scheme, but weakens some of the security assumptions in that paper (e.g., preventing recognition of model parameters that are obfuscated by the attacker).  That should be clearly acknowledged, if some security assumptions are  being given up or have to be given up. Couldn't the same security assumptions be retained and the scheme built along the line of Zhou et al.s 2023, but with a different training objective?  This weakens the soundness of the scheme from a security perspective with respect to (Zhou et al. 2023). The paper does not evaluate that or consider that or even acknowledge that.\n\nAdaptive attacks are not considered or acknowledged."
            },
            "questions": {
                "value": "I would like to see the weaknesses addressed. \n\n I would like to see more clarity on the premise of the paper (motivation).\n\nThe paper should clarify why  (Sun et al. 2023) prevents unauthorized use cannot be combined with Wang et al. (2022, 2023) and achieve the desired goals (or, for that matter, Wang et al. 2022/2023 objective function combined with Zhou et al. 2023's approach of restricting the changes to a small set of weights).\n\n(I have updated the review after an extensive back-and-forth with the authors.)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3840/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3840/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3840/Reviewer_Uryd"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3840/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699236539402,
        "cdate": 1699236539402,
        "tmdate": 1700717902294,
        "mdate": 1700717902294,
        "license": "CC BY 4.0",
        "version": 2
    }
]