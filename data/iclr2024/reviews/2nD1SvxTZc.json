[
    {
        "id": "AIJN7nTwhN",
        "forum": "2nD1SvxTZc",
        "replyto": "2nD1SvxTZc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission348/Reviewer_YfMJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission348/Reviewer_YfMJ"
        ],
        "content": {
            "summary": {
                "value": "The paper presents One-Versus-Others (OvO), a new scalable multimodal attention mechanism. The proposed formulation involves averaging the weights from each modality during training, significantly reducing the computational complexity compared to early fusion through self-attention and cross-attention methods. OvO outperformed self-attention, cross-attention, and concatenation on three diverse real-world datasets and on a simulation dataset that shows the scalability gains in an extremely multimodal setting. The results demonstrate that the proposed approach improves performance compared to state-of-the-art fusion techniques while decreasing computation costs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ The paper presents, OvO, a generalizable multimodal integration scheme that is domain-neutral and does not require modality alignment;\n+ OvO scales linearly with the number of modalities, while also performing competitively to self-attention and cross-attention;\n+ The paper performs robust benchmarking on new simulated and real-world multimodal integration tasks."
            },
            "weaknesses": {
                "value": "- One of the major weaknesses of the paper is that the experiment section is not convincing. The datasets used are simulation dataset and small scale datasets or datasets which are not reported by other compared methods such as VisualBERT, VL-BERT, etc. The main argument of the paper is that, the paper proposes a scalable one-versus-others attention, which is better than cross-attention used in LXMERT and ViLBERT and self-attention used in VisualBERT and VL-BERT. Thus a fair comparison would be conducting experiments on the same datasets reported by these methods.\n\n- The multimodal fusion strategy is also explored in unified content code extraction of multimodal generation [1]. Adding related work in the reference could make the paper more smooth and have bigger impact.\n[1] Multi-Domain Image Completion for Random Missing Input Data, IEEE Transactions on Medical Imaging, 2021."
            },
            "questions": {
                "value": "Please check Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission348/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697824332730,
        "cdate": 1697824332730,
        "tmdate": 1699635961965,
        "mdate": 1699635961965,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vsERGfmUzU",
        "forum": "2nD1SvxTZc",
        "replyto": "2nD1SvxTZc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission348/Reviewer_UUiq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission348/Reviewer_UUiq"
        ],
        "content": {
            "summary": {
                "value": "The current state of multimodal learning, particularly in the medical field, is confronted with challenges stemming from the diverse nature of data inputs, such as X-rays and PET scans. These varied data types necessitate a method for efficient and precise information integration. In response to this, the authors have introduced an innovative attention mechanism, termed \"one-versus-others.\" This mechanism stands out for its ability to scale linearly with the number of input modalities, presenting a significant advantage in handling multimodal data. The effectiveness of this approach has been validated through rigorous testing on three real-world datasets, where it consistently outperformed other existing fusion techniques, showcasing its potential to enhance performance in multimodal learning applications."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The authors have introduced \"one-versus-others,\" a versatile and scalable approach for integrating multimodal data without the need for modality alignment. \n+ Despite its linear scalability with the increasing number of modalities, this method competes effectively with other attention mechanisms in terms of performance. \n+ Demonstrating robust applicability, \"one-versus-others\" has shown promising results on both simulated and real-world multimodal integration tasks, indicating its potential as a reliable tool for handling diverse data inputs."
            },
            "weaknesses": {
                "value": "- The linear time complexity of \"one-versus-others\" is commendable; however, its storage requirements are substantial. This is evident from Equation 6, where outputs from all attention heads are concatenated and subjected to another multihead attention operation, essentially amounting to a sum of all previous module results.\n- The authors highlight the prevalent focus of existing methods on NLP applications, yet their experiments predominantly utilize NLP datasets. To bolster their argument and the generalizability of their method, the inclusion of diverse data inputs from varied domains, such as x-ray or PET scans, would be beneficial.\n- The datasets used, Amazon and hateful memes, feature a limited number of modalities. This scenario does not truly challenge or demonstrate the linearity of the proposed method.\n- The manuscript appears imbalanced, with the methodology section spanning just one page, and a disproportionate amount of content dedicated to dataset settings and experimental configurations. A recalibration of focus towards the methodological aspects of the paper is suggested.\n- The reported improvements in results are modest, with most datasets showing an enhancement of merely 1%. Such marginal gains may not sufficiently underscore the significance of the proposed method."
            },
            "questions": {
                "value": "- The utilization of a simulated dataset is perplexing. The multimodal setting is not clearly articulated, and the necessity of simulations is questionable, especially if the proposed method is as universally applicable as suggested. The availability of multiple real-world datasets should negate the need for simulated scenarios.\n- Equation 3 outlines a weighted averaging approach to calculate attention weights, seemingly derived from prior works. This approach could potentially dilute the informative value of the inputs. It would be beneficial for the authors to delve deeper into this aspect and provide empirical or theoretical insights to address these concerns."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission348/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698765072325,
        "cdate": 1698765072325,
        "tmdate": 1699635961875,
        "mdate": 1699635961875,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NehHrd9hJr",
        "forum": "2nD1SvxTZc",
        "replyto": "2nD1SvxTZc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission348/Reviewer_sMs5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission348/Reviewer_sMs5"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a scalable attention mechanism for multimodal fusion named One-Versus-Others (OvO).  OvO averages the weights from each modality during training, reducing the computational complexity compared to early fusion through self-attention and cross-attention methods. The results demonstrate that the proposed approach outperforms self-attention, cross-attention, and concatenation on three diverse real-world datasets and a simulation dataset."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper proposed a multimodal attention fusion mechanism, scaling linearly with the number of modalities, which is more efficient than self-attention and cross-attention.\n2. The proposed OvO has the potential for a large number of modalities due to the small computational costs and compared performance."
            },
            "weaknesses": {
                "value": "1. The novelty seems limited, as this paper's contribution is only a new design of multimodal attention that averages weights from all other modalities\u2019 encoders.\n2. The introduction of the baseline model is insufficient,  including the architecture details and the references. \n3. In the experimental results of the simulated dataset in Figure 3, it seems that the performance improvement achieved by OvO is marginal compared to concatenation with similar FLOPs.\n4. The compared method is insufficient. There are some different fusion mechanisms in  [1] such as Hierarchical Attention.\n    * [1] Xu P, Zhu X, Clifton D A. Multimodal learning with transformers: A survey[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023."
            },
            "questions": {
                "value": "1. Please compare parameters to demonstrate the efficiency of OvO further.\n2. Is there any reference for the simulation of the 20 simulated modalities? Why is the simulation analogous to a medical setting? Please clarify.\n3. Is there any pre-trained model used for the training? The self-attention and cross-attention are both used to pretrain large models (e.g.,  ALBEF,  VisualBERT) with large datasets (e.g., COCO[1], SBU Captions[2]). However, the scale of datasets in the paper is relatively small, which is unfair. Could the OvO achieve compared performance on the big dataset? Please clarify the model details and the task application to demonstrate the advantages of the proposed method. \n5. Please check the formula  $k^P_2$ in Section 3.3.\n\n    * [1]T.-Y. Lin et al., \u201cMicrosoft COCO: Common objects in context,\u201d in Proc. Eur. Conf. Comput. Vis., 2014, pp. 740\u2013755. \n    * [2] V. Ordonez, G. Kulkarni, and T. Berg, \u201cIm2Text: Describing images using 1 million captioned photographs,\u201d in Proc. Int. Conf. Neural Inf. Process. Syst., 2011, pp. 1143\u20131151."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission348/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698802393738,
        "cdate": 1698802393738,
        "tmdate": 1699635961801,
        "mdate": 1699635961801,
        "license": "CC BY 4.0",
        "version": 2
    }
]