[
    {
        "id": "3jeicghStl",
        "forum": "UqEI76CKgO",
        "replyto": "UqEI76CKgO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6637/Reviewer_kAVs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6637/Reviewer_kAVs"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a continual learning approach called Amphibian, which is based on MAML-style model updates.\nUnlike existing MAML-based approaches (e.g., Gupta et al., 2020), it does not rely on a replay buffer.\nIt assumes that the training examples are provided as a sequence of batches, each with dozens of examples.\n\nThe basic training scheme can be summarized as follows.\nWhen a new batch comes in, it performs gradient descent for each example in the batch one by one (the inner-loop updates), producing a temporary model fitted to the batch.\nThe temporary model is then evaluated on the entire examples in the batch to yield the meta-loss.\nFinally, the original model parameters are updated with the gradient w.r.t. the meta-loss (the outer-loop update), and the training proceeds to the next batch.\n\nThe main novelty of Amphibian is to introduce a gradient scaler $\\lambda_i$ for each parameter $i$.\nWhenever gradient descent is performed, this value is multiplied to the gradient, acting as a per-parameter learning rate.\nThis $\\lambda_i$ is updated every batch by accumulating the products of the outer-loop gradient and the inner-loop gradient."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "This paper dedicated significant effort to ensure reproducibility.\nThe appendix includes experimental details, and the code is provided in the supplementary material."
            },
            "weaknesses": {
                "value": "### Lack of Justification for the Method\n\nOverall, the proposed method does not seem to have a solid theoretical basis.\nThe key idea of this work is to adjust the per-parameter learning rate with the cumulative sum of the products between the inner-loop gradients and the outer-loop gradients.\nIf the inner and outer gradients have the same sign in the current batch, the learning rate for the corresponding parameter is increased, and vice versa.\nHowever, there is no justification for how such learning rate updates can be helpful to continual learning.\n\nInterestingly, if we consider the case where the batch size is reduced to 1, this algorithm seems to become almost the opposite of EWC (Kirkpatrick et al., 2017).\nIn EWC, squared gradients are accumulated for each parameter, and the parameter becomes less flexible as the accumulated value grows.\nIn Amphibian with a batch size of 1, the inner gradient and the outer gradient are both computed with the only example in the batch.\nAssuming the inner gradient does not incur drastic changes in the parameters, their product can be likened to the squared gradient in EWC.\nHowever, Amphibian encourages the changes in the parameters with larger accumulated gradient products, which is the opposite of EWC.\n\n### Confusing Notations\n\nStarting from Eq. (7), $\\ell_{in}$ and $\\ell_{out}$ take only $\\theta_0^j$ as input.\nThis ambiguates the meaning, especially for $\\ell_{in}$.\nAccording to the description under it, the inner loss $\\ell_{in}$ is computed with a single example in a batch, but which example is it?\nAnd why isn't there a summation of multiple $\\ell_{in}$ from each example in the batch?\n\nSimilar confusion continues, even in the appendices.\nFor instance, $g_k$ in Eq. (14) and $g_{k'}$ in Eq. (16) seem to have the same definition with a different index, but their definitions are completely different.\n\nI also do not see any utility in adopting the concept of gradient space.\nSince the authors simply use $e_i$ as basis vectors, all the scaling is independently performed for each individual parameter.\nTherefore, many equations can be simplified without introducing $e_i e_i^T$, which causes unnecessary confusion.\nSimilarly, the scale matrix $\\Lambda$ can be simplified to per-parameter scale values.\n\nAdditionally, there is inconsistency in the subscripts for $\\lambda$ and $e$.\nThe use of $i$ and $m$ is mixed in various instances, as seen in Equation (8) and (9).\n\nI strongly recommend that the authors carefully restructure the overall notation in a systematic manner.\n\n\n### Technically Incorrect Statements\n\n#### Online Setting?\n\nAlthough Amphibian is proposed as an online continual learning approach, one of its key assumptions is that the training examples are provided as a series of batches.\nI think this is far from a truly online setting.\nGenerally, an online learning algorithm should be able to update a model meaningfully, even with a single example.\nHowever, this is not the case for the proposed method.\n\n#### Equivalence between Eq. (6) and (7)\nThe authors argue that Eq. (7) is *equivalent* to minimizing Eq. (6).\nHowever, it seems to be an approximation, according to Appendix A.\n\n---\n\nIn summary, I find limited value in the proposed method, and there is ample room for improvement, even in terms of its presentation. Consequently, I believe this paper does not meet the standards expected for an ICLR publication."
            },
            "questions": {
                "value": "How does Amphibian work in a fully online setting where each example is given individually, i.e., when $|\\mathcal B_i| = 1$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6637/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698563862398,
        "cdate": 1698563862398,
        "tmdate": 1699636758270,
        "mdate": 1699636758270,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ksRkn3QWir",
        "forum": "UqEI76CKgO",
        "replyto": "UqEI76CKgO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6637/Reviewer_esZ7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6637/Reviewer_esZ7"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a rehearsal-free continual learning algorithm, based on La-MAML. It employs bi-level optimization to learn a diagonal scale matrix in each layer, aiming to prevent catastrophic forgetting. Comprehensive experiments and analyses demonstrate its superior experimental performance. However, there may be an unfair comparison setting that needs clarification."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The paper focuses on Task-IL incremental learning and significantly improves performance in the realm of rehearsal-free methods.\n+ I commend the authors for conducting comprehensive analysis experiments to evaluate the proposed Amphibian. These include Task Learning Efficiency, visualization of the loss landscape, and a comparison of few-shot forward transfer."
            },
            "weaknesses": {
                "value": "+ I have concerns about the fairness of the comparable online setting. In both La-MAML and the code provided in your appendix, you have the hyper-parameter 'self.glances', which allows your online training batch to be optimized multiple times. While it's understandable that the early CL work La-MAML adopts this 'single-pass' setting due to the lack of clear definitions for online and offline CL settings, if you're adopting the online CL setting, you need to clearly highlight the differences between your experimental setting and the standard online CL setting where each example can only be seen once. Furthermore, you should provide results of other comparable methods under this setting or set your hyperparameter 'self.glances' to 1 for a fair comparison.\n+ La-MAML, as the most important baseline, also learns the learning rate through bi-level optimization, similar to your learned diagonal scaled matrix. Despite the results provided in Table 3, I'm still unclear if the learned diagonal scaled matrix truly outperforms the learned learning rate for each parameter of La-MAML. The differences between La-MAML and the proposed Amphibian are:\n    - La-MAML uses samples from the memory buffer, while Amphibian does not.\n    - Both La-MAML and Amphibian apply the ReLU operation on the learned learning rate or the diagonal scaled matrix. However, La-MAML only applies this ReLU operation during the outer loop, while Amphibian uses it in both the inner and outer loops. Existing research [1] shows that using the ReLU operation on the learning rate during both inner-loop and outer-loop can effectively improve performance. So it is unclear if your performance gains lies in this different operation.\nIn Table 3, you only show the ablation study on the first point. Therefore, it doesn't convince me that the learned diagonal scaled matrix is truly superior to the learning rate learned by La-MAML.\n    ```\n    Reference: [1] Learning where to learn: Gradient sparsity in meta and continual learning.  NeurIPS, 2021\n    ```\n+ In my view, the learned diagonal scaled matrix is equivalent to learning the important weights for the current task. However, like EWC, it learns the important weights (i.e., the Fisher information matrix) for each task and suppresses the model\u2019s updates in these directions. I'm still unsure how the timely learned diagonal matrix can prevent catastrophic forgetting of previous tasks. I believe the authors need to provide more explanations. Is the proposed method, Amphibian, only applicable in the relatively simple Task-IL setting? Providing the Class-IL online CL performance could be much more convincing."
            },
            "questions": {
                "value": "+ If you're adopting the online CL setting, you need to clearly highlight the differences between your experimental setting and the standard online CL setting where each example can only be seen once.\n+ It's unclear how the timely learned diagonal matrix can prevent catastrophic forgetting of previous tasks.\n+ Is the proposed method, Amphibian, only applicable in the relatively simple Task-IL setting? Could you provide the Class-IL online CL performance?\n\nPlease see the weakness section for more details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6637/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698631759550,
        "cdate": 1698631759550,
        "tmdate": 1699636758141,
        "mdate": 1699636758141,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5RZdGkP77n",
        "forum": "UqEI76CKgO",
        "replyto": "UqEI76CKgO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6637/Reviewer_ebWh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6637/Reviewer_ebWh"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a new algorithm tailored for the online continual learning paradigm, which operates without the need for rehearsal. It also offers a theoretical analysis of the approach. The method is characterized by its learning of a layer-wise diagonal scale matrix that captures the historical trajectory of gradient updates. The paper conducts a comparative evaluation of the proposed algorithm against established methods in the field of continual learning and provides a detailed analysis of the outcomes."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The experimental section of this article is quite comprehensive and theoretical analysis are provided.\n\n2. The authors design a novel rehearsal-free algorithm for continual learning, which achieves commendable results."
            },
            "weaknesses": {
                "value": "1. This work just adds an adaptive diagonal scale matrix in each layer, which seems trivial. The contribution is somewhat limited.\n\n2. The authors allocate a substantial portion to the analysis of experimental results. Although the necessity of the experiment is clear, the analysis could benefit from being more concise to avoid redundancy.\n\n3. The presentation could be improved to get better readability."
            },
            "questions": {
                "value": "1. Could you please provide a more detailed explanation of how the proposed method differs from La-MAML?\n\n2. Could you explain the rationale behind constraining the matrix to a scale matrix?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6637/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6637/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6637/Reviewer_ebWh"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6637/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699414020458,
        "cdate": 1699414020458,
        "tmdate": 1699636758026,
        "mdate": 1699636758026,
        "license": "CC BY 4.0",
        "version": 2
    }
]