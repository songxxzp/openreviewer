[
    {
        "id": "31A6UkFLr8",
        "forum": "0A5o6dCKeK",
        "replyto": "0A5o6dCKeK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3687/Reviewer_mDkx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3687/Reviewer_mDkx"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT, by connecting an LLM with multimodal adaptors and different diffusion decoders."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(1) The paper formulation is good and clear.  \n(2) Introduced lightweight alignment learning techniques.  \n(3) Annotated a high-quality modality-switching instruction tuning dataset."
            },
            "weaknesses": {
                "value": "(1) The model relies on different pretrained models to understand different types of information, like text, images, and audio. The quality of pretraining may directly impact how well the model performs its tasks.  \n(2) What is the size of parameters when tuning the model for each modality?"
            },
            "questions": {
                "value": "(1) Please see the comments above.  \n(2) The work in [1] may be related, can the authors provide a comparison? (not included in rating) \n\n[1] Moon, S., Madotto, A., Lin, Z., Nagarajan, T., Smith, M., Jain, S., Yeh, C., Murugesan, P., Heidari, P., Liu, Y., Srinet, K., Damavandi, B., & Kumar, A. (2023). AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model. ArXiv, abs/2309.16058."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3687/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698440717308,
        "cdate": 1698440717308,
        "tmdate": 1699636325399,
        "mdate": 1699636325399,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wsJmhYE72b",
        "forum": "0A5o6dCKeK",
        "replyto": "0A5o6dCKeK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3687/Reviewer_NwS3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3687/Reviewer_NwS3"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a unified framework that enables any-to-any generation. Specifically, it receives inputs from multiple modalities, such as text, audio and video. Furthermore, it leverages off-the-shelf LLMs and diffusion models, which enables efficient training. They propose to leverage special tokens that encodes the semantics, and then feed this to the diffusion model to act as the conditional input. The paper shows promising results on the evaluated benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper proposes a novel approach to enable any-to-any generation by integrating off-the-shelf diffusion models with LLMs. The proposed approach to align the semantic tokens with outputs from text encoders of diffusion models seems efficient. The results look promising."
            },
            "weaknesses": {
                "value": "The major concern I have regarding this paper is the training object during alignment, which is to align the semantic tokens with outputs from text encoders of diffusion models. This seems reasonable at first, but if the objective is to \"match the semantics token with textual captions' representations from the text encoders of diffusion models\", why not just directly use the diffusion model's text encoder to encode the textual captions? More specifically, why not just let the LLM output a caption according to some fixed format, and extract the caption, then feed it to the diffusion model for generation? I think it would be a more direct approach and probably will enable better performance. Unfortunately, I do not find comparison with this simple alternative in the paper."
            },
            "questions": {
                "value": "Please see weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3687/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698563500435,
        "cdate": 1698563500435,
        "tmdate": 1699636325302,
        "mdate": 1699636325302,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GIKH5Kuzyx",
        "forum": "0A5o6dCKeK",
        "replyto": "0A5o6dCKeK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3687/Reviewer_ijPL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3687/Reviewer_ijPL"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces NExT-GPT, an end-to-end, multi-purpose, multi-modal Language Learning Model (MM-LLM) capable of generating text, images, audio, and video. The system is designed to be efficient, utilizing a small quantity of parameters. Furthermore, a multimodal instruction dataset named MosIT is presented, which facilitates cross-modal understanding and content generation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The system architecture is compact and includes multiple decoders for text, video, audio, and image generation, making it straightforward to implement.\n2. The generation process is end-to-end and does not require initial text generation."
            },
            "weaknesses": {
                "value": "1. The quality of the generation output is primarily dependent on the pre-trained generation modules. If these modules are flawed or produce errors, the system cannot rectify these issues. For instance, if in image generation, stable diffusion struggles with accurately rendering certain elements (e.g., the number of human fingers), NExT-GPT would not be able to produce an accurate output, irrespective of its understanding of the instruction.\n2. The evaluation strategy appears questionable. It seems that the NExT-GPT model used in the evaluation was fine-tuned on individual datasets, which may not accurately reflect the effectiveness of the proposed method.\n3. What would be the results if the model was trained on a mixture of the proposed MosIT dataset and benchmark datasets, and then evaluated on the benchmarks? Additionally, in a multimodal language model, text generation is crucial. It would be interesting to know how the system performs on recent benchmarks such as MME [1], MMBench [2], and SEEDBench [3].\n4. The qualitative comparison provided lacks thoroughness. The paper only presents a few demonstrations and fails to provide comparisons with other MLLMs, including InstructBLIP [4], LLaVA [5], mPLUG-Owl [6] for text generation, and DreamLLM [7] and EMU [8] for conditioned image generation."
            },
            "questions": {
                "value": "see weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3687/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698665385127,
        "cdate": 1698665385127,
        "tmdate": 1699636325219,
        "mdate": 1699636325219,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lFZ2hsGwWM",
        "forum": "0A5o6dCKeK",
        "replyto": "0A5o6dCKeK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3687/Reviewer_7uQJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3687/Reviewer_7uQJ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a multi-modal LLM, called any-to-any MM-LLM, to extend the multi-modality of LLM to a state where there is no limitation on the input and output modality combinations. To achieve this goal, the authors (1) propose a lightweight alignment learning technique to achieve en effective semantic alignment across different modalities with limited trainable parameters and (2) annotate a modality-switching instruction tuning dataset. The displayed results and visualizations suggest the promising performance of the tuned any-to-any MM-LLM."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Extending the multi-modal LLMs free of limitation on the input/output modalities is an important research question that can facilitate a wider range of applications. \n- The introduced dataset, if made publically available, would be a good contribution to the community.\n- Various evaluation benchmarks are used to benchmark the proposed model with existing solutions. \n- The writing is clean and easy to follow"
            },
            "weaknesses": {
                "value": "- The proposed alignment learning technique is a bit naive and does not consider much about the challenge introduced by the any-to-any modality, such as how to balance the performance across different modalities. \n- Although introducing contents from different modalities during tuning is considered to improve the overall performance of the model, in the experiment section, it seems introducing these additional modalities actually leads to worse performance on benchmarking datasets. Does this indicate the alignment technique is not effective enough as expected?"
            },
            "questions": {
                "value": "Will the pretrained model and dataset be released to the public?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Discrimination / bias / fairness concerns",
                    "Yes, Privacy, security and safety",
                    "Yes, Responsible research practice (e.g., human subjects, data release)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The proposed datasets's content may need a deeper look from experts to check its content. And the content generated by the model may need further checking to make sure there are no harmful contents generated."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3687/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699320956741,
        "cdate": 1699320956741,
        "tmdate": 1699636325131,
        "mdate": 1699636325131,
        "license": "CC BY 4.0",
        "version": 2
    }
]