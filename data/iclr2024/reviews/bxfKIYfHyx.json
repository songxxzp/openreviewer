[
    {
        "id": "fvZoXJGYpZ",
        "forum": "bxfKIYfHyx",
        "replyto": "bxfKIYfHyx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4619/Reviewer_pwJk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4619/Reviewer_pwJk"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a diffusion model with RLHF to train an RL agent that follows human preferences and instructions. A attribute strength model is trained on a newly built human feedback datasets, which is leveraged to annotate the behavior dataset. Extensive experiments are conducted on the proposed method in terms of the preference matching, switching, and covering. All achieves superior performance compared to baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The idea of using RLHF to align human preference is reasonable and insightful.  \n2. The experiments are extensive and verify the effectiveness of the proposed method. \n3. The design of the attribute strength and the corresponding datasets could be helpful to many relative future works."
            },
            "weaknesses": {
                "value": "Could the authors offer more clarifications and analysis to demonstrate the extent to which the proposed attribute strength can encompass a broad spectrum of human preferences and instructions? \nHow accurate is the language model to find the correct attribute strength that match user's intent?"
            },
            "questions": {
                "value": "Please refer to the weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4619/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4619/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4619/Reviewer_pwJk"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4619/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698723261646,
        "cdate": 1698723261646,
        "tmdate": 1699636441036,
        "mdate": 1699636441036,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dYkNoRE1yG",
        "forum": "bxfKIYfHyx",
        "replyto": "bxfKIYfHyx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4619/Reviewer_WtmM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4619/Reviewer_WtmM"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on the issue of consistency between agent behavior and human preferences in RLHF, and proposes an alignment method based on the diffusion model. The authors construct a multi-perspective human feedback dataset and train an attribute model, which is then used to relabel the dataset. A diffusion model is utilized as a planner and it's trained on the preference-aligned dataset. In this way, the authors achieve preference aligning between different human. Both quantitative and qualitative experimental results demonstrate the effectiveness of this method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The authors revisit the impact of inherent human annotator preferences on reinforcement learning training, which is illuminating.\n\n2. The proposed method is innovative and achieves relatively good results, which is demonstrated by the experiments.\n\n3. The visualizations and supplementary material provided in the website support the paper and make it easier for readers to understand.\n\n4. The paper is clearly written and well organized."
            },
            "weaknesses": {
                "value": "1. The paper does not include ablation experiments on attribute model training, so the actual effect of attribute alignment is not easy to measure.\n\n2. The explanation of some details of the method is not clear enough. For example, the meaning of equation (5) and \"inpainting manner\" needs further clarification."
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Not applicable"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4619/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4619/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4619/Reviewer_WtmM"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4619/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698758965816,
        "cdate": 1698758965816,
        "tmdate": 1699636440959,
        "mdate": 1699636440959,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8MvFp7XPMM",
        "forum": "bxfKIYfHyx",
        "replyto": "bxfKIYfHyx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4619/Reviewer_t4Ww"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4619/Reviewer_t4Ww"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a method for aligning agent behaviors with human preferences. Firstly, it introduces multi-perspective human feedback datasets. Secondly, it trains an attribute-conditioned diffusion model, referred to as AlignDiff, to act as a director for preference alignment during the inference phase. AlignDiff utilizes Reinforcement Learning from Human Feedback (RLHF) to quantify human preferences, allowing it to match user behaviors and seamlessly transition between different preferences."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed diffusion-based framework demonstrates exceptional performance in decision-making scenarios involving complex dynamics.\n2. The method presented in this paper is capable of effectively matching user-customized behaviors and seamlessly transitioning between different preferences.\n3. Additionally, this paper introduces a valuable contribution in the form of a multi-perspective human feedback dataset. This dataset has the potential to facilitate the wider adoption of human preference aligning techniques.\n4. The proposed method leverages a multi-stage diffusion process, which effectively simulates the reinforcement learning (RL) process."
            },
            "weaknesses": {
                "value": "1. Both the proposed method and RBA utilize RLHF for aligning agent behaviors with human preferences. The novelty is unclear.\n2. The diffusion model usually achieves the best result in the final step. How does the diffusion model guarantee the best human preference at each step? Does the proposed method obtain a plan with T diffusion steps? If so, how about the inference time?\n3. The proposed method only did some ablation studies and has not compared with the state-of-the-art methods, such as RBA."
            },
            "questions": {
                "value": "1. What is the inference time?\n2. How about the comparison with the state-of-the-art methods, such as RBA."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4619/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4619/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4619/Reviewer_t4Ww"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4619/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698797575979,
        "cdate": 1698797575979,
        "tmdate": 1700656045179,
        "mdate": 1700656045179,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sc8F0XakwM",
        "forum": "bxfKIYfHyx",
        "replyto": "bxfKIYfHyx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4619/Reviewer_SGJ2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4619/Reviewer_SGJ2"
        ],
        "content": {
            "summary": {
                "value": "This paper utilizes the diffusion model conditioned on attributes for planning. Inspired by RLHF, this paper first fits a reward model trained to predict preference over trajectories given human preference, followed by using it to predict the attribute strengths given the episode. This is then used to label the unlabelled episodes with the corresponding attributes, which are then used to condition the diffusion model, trained in a classifier-free guidance sense.  \n\nThe paper shows the advantages of the proposed technique over existing baselines for learning the policy including on human preferences, and tests its robustness when the strengths are suddenly changed in between. Lastly, an ablation of label pool size is done to understand its impact on learning the reward model."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is easy to follow in most places, and I like the experiments on the robustness and label efficiency of the reward function."
            },
            "weaknesses": {
                "value": "As someone who is not well versed in the empirical reinforcement community, my weak comments would be high level and mostly some of my confusion throughout the paper. \n\n- Can authors provide the inter-annotator correlation when they're labeling the reward model? Annotator alignment is a problem in the RLHF community when it comes to LLMs and even RLHF for diffusion models in image generation (See [1]) therefore it would be good to see some numbers on annotator agreement. \n\n- Can authors provide the accuracy of the reward model after training it based on human preference? Does the Area metric in Table 5 correspond to that? Moreover, what is the accuracy of random guessing? I am assuming it would be 50%? \n\n- Are there any ablations done in case one does not apply masking in the way current AlignDiff is applying? What if one just used 0s in the strength where it is not needed? I think having that result would further showcase the utility of masking in the current way. \n\n- How exactly is BERT used in the pipeline? How is the mapping from BERT representations to strength and mask learned? \n\n- How is the performance affected by the precision of discretization? \n\n- $\\mathcal{B}(k, p)$ is not defined. \n\n- How is the performance of the reward function (that predicts strengths) affected by the length of episodes, when varied during training and inference time (zero-shot say)? \n\n\n[1] Human Preference Score v2: A Solid Benchmark for Evaluating Human Preferences of Text-to-Image Synthesis."
            },
            "questions": {
                "value": "Refer to the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4619/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699139769826,
        "cdate": 1699139769826,
        "tmdate": 1699636440779,
        "mdate": 1699636440779,
        "license": "CC BY 4.0",
        "version": 2
    }
]