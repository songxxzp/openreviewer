[
    {
        "id": "nuguwa2CFB",
        "forum": "dRel8fuUK4",
        "replyto": "dRel8fuUK4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6925/Reviewer_F7FX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6925/Reviewer_F7FX"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new membership inference attack against machine learning models. Numerical experiments on four data sets demonstrate its superior performance over existing attacks. Some heuristic justifications are given to explain the improvement in performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The new membership inference attack proposed by this paper consistently outperforms existing attacks, particularly when the number of reference models available is small. This advantage significantly reduces the computation burden of membership inference attacks.\n\n* There are some neat heuristics for explaining the improvement in empirical performance. These heuristics are useful for inspiring new attack methods, and/or theoretical research on privacy attacks (and privacy-preserving machine learning in general)."
            },
            "weaknesses": {
                "value": "1. *Theoretical justification of the method*. Section 2.2 attempts to justify the \"test statistic\", equation (4), by arguing that it is a good approximation of the \"true\" likelihood ratio, equation (3). In particular, the paragraph between equations (3) and (4) and the paragraph between equations (4) and (5) makes many assumptions and makes several approximations in succession. It might be easier for readers if these assumptions and approximations are spelled out more directly, preferably in more precise mathematical notations (For a few examples, the \"true\" quantity and the \"estimate\" are both referred to as \"LR\"; the quantity \"MIA\" is first defined as a probability in equation (5), but the actual attack in the paper is an approximation of the \"MIA\". ) \n\n2. *Dependence on reference records*. While the new attack's robustness to few reference models has been clearly demonstrated, I wonder whether the new attack's dependence on many reference records is replacing one type of constraint (abundance of reference models) with another type of constraint (free access to additional samples from the population). While there are some encouraging results in the appendix, 10% of the entire population appears to be a very large amount in most practical applications (sometimes, it is not known in advance what the \"population\" is, or how large the \"population\" might be). \n\n3. *Lack of practical criteria for selecting input parameters $\\gamma$ and $\\beta$*. There is not much discussion on why $\\gamma = 2$ is chosen (besides that it is greater than 1, which makes intuitive sense), and the choice of $\\beta$ in the experiments appears to be the result of picking the best $\\beta$ after having tried many values and looking at the results. Without criteria for selecting these parameters, the new attack's strong performance could be difficult to reproduce/generalize in other settings.\n\n4. *Possibility of \"overfitting\" to image data, neural networks, and/or particular data sets*. On a few occasions, there are signs that the proposed membership inference attack is somewhat tailored to classifying image data using neural networks. Although this task is popular, if not dominant, in the literature, the risk of \"overfitting\" can perhaps be made more explicit. For a few examples: \n    * \"simple transformations of x\" at the end of Section 2.2 sounds straightforward for image data, but may not always make sense for other data types;\n    *  while Sections 1 and 2 discuss membership inference attacks in very general terms, the empirical evidence is predominantly on classification of images by neural networks.\n    * related to the second \"weakness\" above, the finiteness of \"population\" appears to be an artifact of the chosen data sets for empirical evaluation."
            },
            "questions": {
                "value": "The questions correspond to the \"weaknesses\" above.\n\n1. Theoretical justification of the method.\n   * For cancelling the $P(D|\\theta')$ term in equation (3), while the intuition makes sense, can the argument be expressed in mathematical terms?\n   * For $P(x)$, is the summation immediately below equation (4) an exact expression or an approximation? If it is an approximation, how would one sample $\\theta'$ so that the sum is indeed a good approximation? Is any assumption needed for $P(\\theta')$, say, discreteness or finiteness of support?\n\n2. Dependence on reference records. While reference records are not always a direct input in other membership inference attacks, is there any way to assess whether the competing methods would depend more strongly or weakly on the number of reference records, compared to the new attack?\n\n3. Lack of practical criteria for selecting input parameters $\\gamma$ and $\\beta$. Is $\\gamma = 2$ always a good/acceptable choice? For a desired TPR/FPR value, is there a way to select $\\beta$ without trying a range of values? If one has to try many values to make the decision, does it diminish the computation cost advantage over existing methods?\n\n4. Possibility of \"overfitting\". Is there evidence, either theoretical or empirical, that the strong empirical performance observed in this paper can generalize to other models (particularly models other than neural networks)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6925/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697771457989,
        "cdate": 1697771457989,
        "tmdate": 1699636806681,
        "mdate": 1699636806681,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VyGgkdjVyA",
        "forum": "dRel8fuUK4",
        "replyto": "dRel8fuUK4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6925/Reviewer_qcX6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6925/Reviewer_qcX6"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the problem of membership inference attack (MIA). More specifically, the authors propose a new MIA method, RMIA, which can achieve better TPR-FPR tradeoffs comparing previous methods. Empirical results validate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The strength of the paper is as follows:\n1. The authors propose a new MIA method, which is based on a new approximation of the likelihood ratio (LR).\n2. The computation of the proposed LR seems to be easy to implement.\n3. The empirical results across different datasets validate the advantages of the proposed method."
            },
            "weaknesses": {
                "value": "The weakness of the current paper:\n1. The presentation of the paper need to be improved. For example, the comparisons between the proposed method and the previous methods needs to be further clarified, especially for the method proposed by Carlini et at., 2022.\n2. It is unclear why the authors can assume that $Pr(D|\\theta^\\prime)$ to be a constant.\n3. The authors do not test the methods when the model is differentially private."
            },
            "questions": {
                "value": "I find the idea of the paper is interesting and the results seems to be promising. However, I have the following additional questions regarding the current paper:\n1. In Definition 1, you assume a fair coin $b$. What if the probability of the data being a member or not is not $0.5$, and whether your method can be applied to this case?\n2. For the parameters $\\beta$ and $\\gamma$, how sensitive of these parameters and whether it is hard to find the optimal parameters?\n3. What are the standard deviations of your report results? Do you have some confidence intervals in your plots?\n4. For the predication probability functions ($Pr(x|\\theta)$), how sensitive are those hyperparameters?\n5. When you compute $Pr(x)$ for the offline method, what is the computational cost and how the results will be affect by the linear approximations?\n6. If you continue to increase the reference models, how will your methods look like compared to the method proposed by Carlini et at., 2022?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6925/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698774455719,
        "cdate": 1698774455719,
        "tmdate": 1699636806541,
        "mdate": 1699636806541,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MrHDv0YZ7H",
        "forum": "dRel8fuUK4",
        "replyto": "dRel8fuUK4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6925/Reviewer_Ju68"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6925/Reviewer_Ju68"
        ],
        "content": {
            "summary": {
                "value": "This article suggests an improvement to Membership Inference Attacks (MIAs) while ensuring low costs. The authors consider the adversary-challenger model wherein given two samples (target and random), the adversary attempts to find whether a model is trained on the target sample or the random one. By designing a simplified, low-compute likelihood function by allowing access to reference models, the adversary in this paper can identify the presence of the target sample (in the given model's train set) with a higher probability compared to previous SOTA work. Empirical results verify the usefulness of this work. As with all MIAs, the impact of this work is in designing superior ways of testing the privacy claims of a method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The core usefulness of the method is based on the simplification of the likelihood function (by leveraging reference models). The likelihood exhibits superior qualities and requires lower computations compared to previous methods.  \n2. Unlike previous methods, the new method does not require the assumption that the target and random sample have the same predictive probability for any model. Essentially the new method captures that the random and target samples can be quite different. Although this point is highlighted in section 2.3 (para 3) it will be nice to see some basic experiments on how much different two samples can be. For example, consider showing the predictive power differences between the boundary points nearer to the decision boundary v/s interior points.\n3. The suggested adversary can be developed for any given training algorithm/model. More importantly, the method is quite straightforward (except for access to reference models, see weaknesses section).\n4. The method in this work achieves higher True Positive Rates (TPRs) across all False Positive Rates (FPRs) compared to previous methods.\n5. The overall strong empirical results verifies the method's strengths."
            },
            "weaknesses": {
                "value": "1. It is unclear whether the approximation of the likelihood always holds. The idea is that different reference models exhibit similar predictive distributions. Is it assumed that the reference models are trained well and over a large sample size for this assumption to hold? Essentially, having some clarity about the assumptions of the reference models will be useful for judging the adversary's capacity.\n2. Continuing point 1, how hard is it for the adversary to access/train such reference models and is it a standard assumption in literature? A brief discussion about the adversary's strength will be helpful.\n3. The authors mention that they incur lower costs as they do not have to train models including the target sample. However, does the training of reference models not incur additional costs? Providing a simple cost comparison discussion will help."
            },
            "questions": {
                "value": "My main question is regarding the inclusion of reference models as highlighted in the weaknesses section. Answering the questions (in the weaknesses section) will alleviate most of my concerns. Otherwise, the paper is well-written and provides a straightforward method for designing better, low-cost MIAs."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6925/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6925/Reviewer_Ju68",
                    "ICLR.cc/2024/Conference/Submission6925/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6925/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698800865517,
        "cdate": 1698800865517,
        "tmdate": 1700769656975,
        "mdate": 1700769656975,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IRqV7XBBpS",
        "forum": "dRel8fuUK4",
        "replyto": "dRel8fuUK4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6925/Reviewer_HGWk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6925/Reviewer_HGWk"
        ],
        "content": {
            "summary": {
                "value": "This paper suggests a new likelihood ratio loss-based membership inference attack. The authors suggest that their attack differs from SOTA loss based attacks that also rely on the likelihood ratio by additionally incorporating a variety of reference points as opposed to just one reference point in the standard LiRA attack (e.g., Carlini et al (2021)). The authors\u2019 suggested attack clearly outperforms the SOTA attacks by a large margin on standard benchmark datasets like CIFAR10, CIFAR100, Purchase100 and CINIC10.\n\nDespite the test\u2019s strong empirical performance, I am hesitant to provide a more favourable evaluation of the proposed method at this point. This is since there are insufficient details to properly understand how the test is conducted in practice. In particular, the paper does neither provide pseudo code for their attack nor does it describe the step-by-step computation of the test statistic.  If authors could provide clarifications, I may be willing to revise my evaluation."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "**New loss-based attack**: The authors propose a new attack that uses a model\u2019s losses and that seems to outperform SOTA attacks. The attack uses a variety of reference points to calibrate the distinguishability between x and any z when conditioned on $\\theta$. \n\n**Comprehensive empirical evaluation**: The demonstrated empirical evaluation effectively compare the proposed test\u2019s performance against other state-of-the-art attacks that are based on the loss. The results are shown across standard benchmark data sets including CIFAR10, CIFAR 100, Purchase100 and CINICO10 on which the proposed attack seems to outperform SOTA by a large margin."
            },
            "weaknesses": {
                "value": "**Missing details**: The paper does not provide pseudo code for their suggested attack. Neither is the exact computation of the test statistic described. This makes it difficult to fully appreciate the work\u2019s results. Providing further details on this would help to follow the author\u2019s argument more easily. Further, the discussion on the mechanism of the author\u2019s proposed attack in section 2.3 is neither supported by empirical evidence nor accompanied by a theoretical analysis, that would link the discussed probabilities to the power of the likelihood ratio test, and is thus difficult to follow. \n\n**Comparison**: The MIA attack setup described in this work is different from the LiRA attack described in previous work (e.g., Carlini et al (2021)). In the LiRA attack, the attacker does not have the capacity to poison the dataset. Hence, since the attacks run under different threat models the comparison may be misleading."
            },
            "questions": {
                "value": "- How is the indistinguishability game that you propose required for your attack? Why do you require that your game be different from the standard MI attack game proposed by Yeom et al (2018), used in Carlini et al (2021) and recently analysed by Leemann et al (2023)?\n- Aren\u2019t p(z) and p(x) just the prior probabilities of observing x and z, respectively?\n\n----\n**Additional references**\n\nLeemann et al (2023), \u201eGaussian Membership Inference Privacy\u201d, 37th Conference on Neural Information Processing Systems (NeurIPS)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6925/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698867583495,
        "cdate": 1698867583495,
        "tmdate": 1699636806338,
        "mdate": 1699636806338,
        "license": "CC BY 4.0",
        "version": 2
    }
]