[
    {
        "id": "nKyEvBmVWy",
        "forum": "rtzUW1FU3H",
        "replyto": "rtzUW1FU3H",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5230/Reviewer_8ZV2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5230/Reviewer_8ZV2"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a new benchmark called LooGLE for long-context LLMs, with inputs longer than 24k tokes. Some/most examples are human-annotated, and some/most were cross-validated by multiple annotators.\nFurther, some/most of the documents in the benchmark were published after 2022, which is expected to be after the knowledge cutoff of LLMs such as GPT-3.5 and GPT-4, forcing them to rely only on their in-context learning abilities rather than their prior knowledge.\n\nSuch benchmarks are always useful and needed in the community, especially following the growing interest in long-context LLMs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The authors made lots of efforts in collecting and curating the data.\n* Such benchmarks are always useful and needed in the community, especially following the growing interest in long-context LLMs."
            },
            "weaknesses": {
                "value": "- Following the promises in the abstract about the human-annotation and the cross-annotator validation, I was very disappointed to see that a large part of the benchmark's ground-truth output was generated using GPT 3.5 / 4:\n>We utilize the powerful language processing and understanding capability of GPT-3.5-turbo to help generating short QA pairs from the original text.\n\n>we employ GPT-3.5-turbo to generate factual summaries align with the source segment using with constraints\n\nIf this is indeed the case, this is disappointing, and the benchmark may be biased toward \"questions that are easy for ChatGPT to answer\".\n\n- The text is very unclear in many cases. For example when describing the statistics of the benchmark, the paper says:\n>Extra-long realistic documents. It contains 778 latest gathered and extremely long documents\nwith an average of 16.4k words. There are over 6000 test instances without distribution bias for a\nmore generalized assessment, many of which are exceeding 100k words.\n\nSo are there 778 examples or 6000 examples? If \"many of which exceed 100k words\", how many of them? what's the average? Are these two datasets? If not, why are these numbers reported separately?\n\n- The results in Section 4.3.1 are very confusing and unclear. For example:\n>In Table 3, it can be noticed that LlamaIndex obtains from the perspective of GPT4 evalution. Instead\nof memorizing a shortcut of original input with a limited context window, retrieval-based context\ncompression technique augments the LLM by incorporating external memory, allowing relevant\ninformation to be retrieved using a specific query.\n\nI am not sure what such paragraphs are trying to say. What does it mean that \"LlamaIndex obtains from the perspective of GPT4 evalution\"? What do the authors exactly mean by \"memorizing a shortcut\"? Who is memorizing a shortcut?\n- Measuring \"GPT4 score\" on GPT4's outputs is mostly meaningless. It would be better to just completely remove this column, or use another LLM that is not evaluated.\n- Applicability: the paper does not mention anything about its implementation, its ease of use, its availability. As always with benchmarks, the devil is in the details, and the authors have not included the data itself, which makes it hard to really evaluate its quality.\n- Presentation is poor: for example: \n    - the text in Figure 1 is tiny, not allowing to actually understand the overview of the new benchmark. \nThe entire left part of the figure contains barely any information.\nI would prefer an organized and readable list of tasks and data statistics.\n\n    - The text in Table 1 tiny\n    - The text in Table 2 is tiny. Further, it would be helpful if these statistics would include the max/min instead of category, or a more illustrative figure of the characteristics of the examples, as in Figure 1 in the [SCROLLS paper](https://arxiv.org/pdf/2201.03533.pdf)\n    - The text in Figure 3 is tiny. Further, the colors are very similar, and I cannot distinguish between the different models and cannot understand anything from this figure."
            },
            "questions": {
                "value": "### Questions\n1. Section 3.3.1 says that \"we directly use the abstract of each paper as the reference for generating summaries\" - so, the ground-truth summaries where **generated**? are the Abstracts **used** in any part of the process other than for evaluation?\n2. Are the authors going to release the test sets, or keep them \"hidden\"?\n3. Are there training/test spits, or is everything \"test\"?\n\n### Comments\n1. The comparison in Table 1 on \"which tasks are included in each benchmark\" shows that LooGLE contains many tasks that other prior benchmarks do not. However, it is a bit unfair, because these prior benchmarks contain tasks that are not contained in LooGLE, but these are not mentioned. For example, Scrolls, mentioned in the first line, does contain QA (mentioned with \"X\") and NLI (not mentioned at all).\n\n### Summary\nI appreciate the authors's efforts, but as much as good benchmarks are needed in the community, unfinished benchmarks can do harm and drive research in the wrong direction.\nI cannot evaluate the benchmark itself since it was not released, but the paper still feels a bit unclear and unfinished, which makes me worry that the benchmark is too.\nThus, I currently vote for rejection, and hope that the authors would polish both the paper and the benchmark and release them when they are in a more polished state."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5230/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697552850353,
        "cdate": 1697552850353,
        "tmdate": 1699636521500,
        "mdate": 1699636521500,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "x9rmigpTLa",
        "forum": "rtzUW1FU3H",
        "replyto": "rtzUW1FU3H",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5230/Reviewer_oc4r"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5230/Reviewer_oc4r"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a new benchmark for long context understanding, consisting of 24k tokens in average. Firstly, this has the advantage to be more challenging than former benchmarks that have shorter texts compared with current LLMs' context window length (reaching up to 32k tokens). Secondly, it only contains newly created documents (after 2022) which are thus not present in most LLMs' pretraining data, preventing data leakage and enabling fairer evaluation. Experiments on current state-of-the-art LLMs reveal challenges in long-context understanding."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- A curated dataset, thoughtfully designed with great efforts to prevent data leakage and ensure long dependency.\n- Great efforts in assessing current state-of-the-art LLMs' long dependency capabilities such as information retrieval, reading comprehension and reasoning, computation and timeline reordering."
            },
            "weaknesses": {
                "value": "- see questions.\n- the paper is sprinkled with typos (refer to questions for a few)."
            },
            "questions": {
                "value": "- Is the benchmark english-only ? If so, this needs to be mentioned.\n- How will the dataset be released ? For instance ZeroSCROLLS only released the inputs and evaluate through a system of leaderboard, will LooGLE be released the same way ? I'm concerned that revealing input/gold outputs pairs would lead to data leakage for future models.\n- Concerning data collection, were the sourced documents (after 2022) subjected to any machine-generated verification ? I am concerned that ChatGPT-like texts might compromise the fairness of the evaluation.\n- Are the open-source models instruction-tuned ? Commercial closed source models like GPT3.5 rely on RLHF or some instruction tuning techniques that enable them to better follow instructions. If the considered open-source baselines are not instruction-tuned, the comparison might be unfair since the prompt used for evaluation is the same and is instruction-based.\n- Is it fair to have GPT4 both as baseline and evaluator ? I am also concerned about the creation of the dataset of short QAs (generated by GPT3.5). Is it fair to evaluate a model that was used to create the dataset ?\n- In section 4.2, you mention human evaluation (3) but I cannot find any human evaluation both in the paper and the appendix.\n- Section 4.3.2 is confusing, the experiments are based on the recent work from Liu et al. 2023, that accessing information in the middle of the document is more challenging for LLMs. If I understood correctly, you suggest concatenating the head and the tail of the inputs and give it to the LLMs as input. This would mean discarding the whole \"middle\" and leaving the beginning and the end. This puts an immediate limitation on information that can be retrieved by the LLMs. Also did you only use the same model (GPT4-32k) but with various context input length or different variants of GPT4 ? What is the last entry in Table 5 ? (GPT4). For long summarization, arxiv abstracts are highly biased towards the beginning of the article so it is expected that increasing context would result in a higher divergence between the generated summary (which will contain more and more details) and the (gold) abstract.\n \n**Typo**\n- page 2: \"Mannually designed both short and long dependency tasks\"\n- page 3: \"COmputation.\"\n- page 7: \"Retrival\"\n- page 8: \"GPT4 evalution\"\n- table 4: \"Performence\"\n- appendix page 3: \"Dispcrepancy\"\n- not really typos but it is uninformative to report results of the order of \"e-300\" since at this point there is nothing to really compare."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5230/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5230/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5230/Reviewer_oc4r"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5230/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698723649205,
        "cdate": 1698723649205,
        "tmdate": 1699636521416,
        "mdate": 1699636521416,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2x2iXnn4hT",
        "forum": "rtzUW1FU3H",
        "replyto": "rtzUW1FU3H",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5230/Reviewer_SWcJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5230/Reviewer_SWcJ"
        ],
        "content": {
            "summary": {
                "value": "LLMs have shown impressive performance in various NLP tasks. However, the fixed context window length of the transformer architecture limits their ability to understand extremely long inputs. Existing datasets for evaluating LLMs' long context understanding have limitations such as shorter text lengths, outdated documents, and a focus on short dependency tasks. The paper introduces \"LooGLE\"  to evaluate LLMs' ability to understand long contexts. Upon evaluating 8 state-of-the-art LLMs on LooGLE, the authors found:\n1. Commercial models generally outperform open-sourced models.\n2. LLMs excel at short dependency tasks but struggle with real long dependency tasks.\n3. Retrieval-based techniques significantly improve performance on short QA tasks, but many techniques for extending context window length struggle with long context understanding."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- **Up-to-date Documents**: LooGLE contains documents published after 2022, ensuring that modern LLMs have not been pretrained on these documents.\n- **Diverse Tasks**: LooGLE includes both short and long dependency tasks, providing a evaluation of 8 LLMs' capabilities."
            },
            "weaknesses": {
                "value": "**Lack of Experimental Data to Support Some Claims**:\n\n- The paper states that \"by employing scaling techniques like positional interpolation, parallelization, and finetuning on longer texts, open-sourced models have shown improvement in handling longer inputs compared to previous versions.\" However, the article does not provide performance data of previous version models. This omission makes it challenging to ascertain the improvements is brought by modifying position embeddings or instruction-tuning. For example, the comparision between vicuna-2k and vicuna-16k is a better case to validate this claim.\n- The claim that \"GPT4-32k performs better than GPT-8k\" is not consistently supported by the provided metrics. The results between the two models vary across different indicators (automatic metrics v.s. gpt4 score). A more in-depth explanation and analysis are needed to support this claim, including understanding the differences in various metrics. It's unclear why, on Long dependency tasks, the longer window 32k model performs worse than the 8k model. \n\n**Absence of Human Evaluation**: The paper mentions conducting human evaluations, but there's no presentation of the related data. GPT-evals may have some preference in generation length, human evaluation can be a better reference."
            },
            "questions": {
                "value": "- Details of Llamaindex is not clear: 7B or 13B, chat model or regular model?\n- Why not use the chat version of Llama2, which is considered to be skilled at instruction-following and could be potentially better at downstream tasks.\n- Why LlamaIndex is much better than any other open-sourced models? According to your results in Fig3, retrieval+open-sourced model is better than the long-context version of the same base model, and this conclusion is contradict to the conclusion from other long-context benchmarks (Table1).\n- Writing format: it's better to list url as the footnote. It is supposed to leave a black between the main text and citation brackets."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5230/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698857654902,
        "cdate": 1698857654902,
        "tmdate": 1699636521324,
        "mdate": 1699636521324,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Aye4cVYDeh",
        "forum": "rtzUW1FU3H",
        "replyto": "rtzUW1FU3H",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5230/Reviewer_gE1R"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5230/Reviewer_gE1R"
        ],
        "content": {
            "summary": {
                "value": "The authors present a new dataset, called LooGLE, which aims at evaluation of LLMs on long context. Their dataset has documents with longer length compared to previous benchmarks and it is more up to date (2022+). The proposed dataset has task with long dependencies and the authors have made sure that for some of the tasks, the answer needs to be collected from multiple segments of the documents. The evaluate both commercial and open-souse models on the new dataset and provide some insights."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The paper addresses a very important area, i.e., long context evaluation of LLMs\n- Based on the description, the collection method, and evaluation results, the proposed new dataset seems to be of high quality.\n- Authors provide extensive evaluation on different commercial and open-sourced models.\n- The paper is well written and easy to follow."
            },
            "weaknesses": {
                "value": "- The paper presents Human Evaluations as one of the evaluation technique but never present human evaluation results.\n- There are several automatic scores have been presented in Tables 3 through 5 and sometimes. These scores not always in agreement; not all the scores are better for the winning model. I found this confusing especially when some conclusions are drawn in the text."
            },
            "questions": {
                "value": "- For the LlamaIndex, what retriever and chunk size are used?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5230/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5230/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5230/Reviewer_gE1R"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5230/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699235449222,
        "cdate": 1699235449222,
        "tmdate": 1699636521224,
        "mdate": 1699636521224,
        "license": "CC BY 4.0",
        "version": 2
    }
]