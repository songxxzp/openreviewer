[
    {
        "id": "qtDQYoaAQD",
        "forum": "P15CHILQlg",
        "replyto": "P15CHILQlg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3483/Reviewer_qpM8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3483/Reviewer_qpM8"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates generative flow networks (GFlowNets), which sample a composite object via a sequence of constructive steps, with a probability proportional to a reward function. In contrast to prior training objectives for GFlowNets, which mainly focus on learning from complete trajectories, Looking-Forward GFlowNets (FL-GFlowNets, as introduced by Pan et al.) take advantage of the computability of intermediate rewards or energies and employ an additive energy factorization to facilitate learning from incomplete trajectories. However, the authors argue that there are two limitations: 1) intermediate rewards are too expensive to be evaluated, e.g., frequent evaluation over partially-constructed molecules at all intermediate steps can indeed be time-consuming, especially in the context of chemical synthesis or molecular design; 2) the significant variability in intermediate rewards along the entire trajectory, e.g., rewards are low or zero at the beginning, and experience a sudden surge at the end (as illustrated in Figure 1), thus leading to less informative intermediate signals. To this end, inspired by the reward decomposition method in Ren et al., the authors propose to learn a potential function that is additive over transitions. This is done by minimizing the least square loss between $R(s_{n})$ and a summation over potential functions that incorporates a dropout-based technique, i.e., $\\sum_{t=0}^{n-1} z_{t} \\phi_{\\theta} (s_{t} \\rightarrow s_{t+1})$, together with some scaling coefficients. The learned potential function can be directly incorpoated into FL-GFlowNets, thus leading to LED-GFlowNets. Experimental results on several datasets show the effectiveness of LED-GFlowNets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "### Motivation:\n\nA major limitation of GFlowNets is that they suffer from inefficient credit assignment when trajectories are long, as the learning signal only comes from the terminal reward (episodic setting). This is where partial inference comes in - the learning signal is from partial reward signals. To this end, FL-GFlowNets take advantage of intermediate rewards and an additive energy factorization over transitions to facilitate learning from incomplete trajectories. However, there might be two limitations: 1) intermediate rewards are too expensive to be evaluated; 2) the significant variability in intermediate rewards along the entire trajectory. These motivate LED-GFlowNets.\n\n\n### Originality:\n\nThe proposed method builds on two existing works - reward decomposition [Ren et al.] and FL-GFlowNets [Pan et al.]. The authors extend the reward decomposition method by introducing a dropout-based regularizer to reduce variance and promote sparsity. The learnable potential function $\\phi_{\\theta} (s_{t} \\rightarrow s_{t+1})$ can be directly used in FL-DB or FL-SubTB. The combination of existing ideas shows good performance on various tasks. The originality should be ok.\n\n\n### Clarity:\n\nThe paper is well-organized and easy to follow, making it accessible to readers."
            },
            "weaknesses": {
                "value": "Please see the following questions."
            },
            "questions": {
                "value": "### Method:\n\n- The paper considers GFlowNets whose reward function $R(s_{n})$ corresponds to a potential function that is additive over transitions, i.e., $- \\log R(s_{n}) = \\mathcal{E}(s_{n}) = \\sum_{t=0}^{n-1} \\phi_{\\theta} (s_{t} \\rightarrow s_{t+1})$. This makes sense for set GFlowNets, where $s_{n}$ contains information about all the transitions, but not about their order, as $s_{n} = x$ is the set of elements that have been added at each transition. Is it also applicable for the tasks where order might matter?\n\n- In terms of Figure 2, \n   - Why do we want to have (b) for potentials --> approximately same energies to minimize variance (more smooth transitions)? In this case, we just simply set $\\frac{1}{3}$ if $\\mathcal{E} = 1$?\n   - When $e^{- \\mathcal{E}(s)} = e^{- \\mathcal{E}(s^{\\prime})} \\Rightarrow \\mathcal{E}(s \\rightarrow s^{\\prime}) = 0$, it reduces to the DB constraint, such that we cannot take advantage of intermediate rewards or energies to learn from incomplete trajectories? I conjecture this might be a common case in many tasks?\n\n- In GFlowNet settings, we hope to achieve a transition probability distribution: $P_{F}(s_{t+1} | s_{t}) \\propto F(s_{t} \\rightarrow s_{t+1})$. I am curious - since we have $- \\log R(s_{n}) = \\mathcal{E}(s_{n}) = \\sum_{t=0}^{n-1} \\phi_{\\theta}(s_{t} \\rightarrow s_{t+1})$, then we might be able to learn $P_{F}(s_{t+1} | s_{t}) \\propto \\phi_{\\theta}(s_{t} \\rightarrow s_{t+1})$? With such policy, can we achieve - sampling $x$ with probability proportional to $R(x)$? If not, with GFlowNet training objectives, the learned policy would be modified accordingly?\n\n### Experiment:\n\n- In terms of Figure 9(b), how to understand number of calls? Assume we have 16 trajectories now, LED-GFlowNets compute 16 terminal rewards; while FL-GFlowNets need to compute all intermediate and terminal rewards (should be $\\sum_{i=1}^{16} n_{i}$, where $n_{i}$ is the number of states, except $s_{0}$, in the ${i}$-th trajectory?). Thus, 16 calls vs. $\\sum_{i=1}^{16} n_{i}$ calls? But, I think we should have more than 16 calls for LED-GFlowNets, as we need to train the potential function.\n\n- For experimental details, 'We set the dropout probability as 10% for tasks with a trajectory length less than 10 and 20% for others.' -- Did you try other proportions? How does $\\lambda$ affect the performance? Abalation studies on $\\lambda$ for Figure 9 (a) are missing.\n\n\n### Some writing issues, typos and inconsistencies:\n\n1) $\\log$ is missing for $P_B$ regarding the DB loss, as well as in the Appendix.\n\n2) As far as I understand, it should be --> SubTB($\\lambda$) [Madan et al.] is practically useful interpolation between TB and DB losses?\n\n3) Below equation (4), .... by replacing $\\mathcal{E}(s)$, $\\mathcal{E}(s^{\\prime})$ with --> should be $\\sum_{t=0}^{u} \\phi_{\\theta}(s_{t} \\rightarrow s_{t+1})$ and $\\sum_{t=0}^{u+1} \\phi_{\\theta}(s_{t} \\rightarrow s_{t+1})$, respectively.?\n\n4) Figure 4: SubTB-based objectives --> subTB-based objectives\n\n5) For Figure 4, it is clear that DB or subTB is considered. But for Figure 5 & 6, it's just LED-GFN or FL-GFN. Which training objective are you using? (though 'For FL-GFN and LED-GFN, we consider a subTB-based implementation.' is mentioned for molecule generation). Maybe just use LED-subTB for the figures? Try to find a way to avoid such confusion.\n\n---\n\n**Update after rebuttal**\n\nI raised the score: 6 --> 8."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3483/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3483/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3483/Reviewer_qpM8"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3483/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698698078405,
        "cdate": 1698698078405,
        "tmdate": 1700479383564,
        "mdate": 1700479383564,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "o2J4LxnMlI",
        "forum": "P15CHILQlg",
        "replyto": "P15CHILQlg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3483/Reviewer_RMo5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3483/Reviewer_RMo5"
        ],
        "content": {
            "summary": {
                "value": "A method is proposed to improve training of generative flow networks (GFlowNets) using a learned reward shaping scheme. Specifically, one trains an auxiliary model to predict an energy delta for every edge, with the objective that the sum of energy deltas along any trajectory should equal the negative log-reward of the state at which the trajectory terminates. The learned energy delta is then used as a correction to the log-flow difference that appears in the detailed balance training objective. This method is shown to accelerate convergence and improve mode discovery in several very different problems from past work where GFlowNets have been successfully applied."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- Good exposition; I have very few complaints on the presentation and math.\n- Natural and well-carried-out idea that seems to substantially improve GFlowNet training.\n- Experimental validation in very diverse settings from past work. Comparisons to alternative ways of learning the energy decomposition make the paper stronger.\n  - But see questions below."
            },
            "weaknesses": {
                "value": "- I take issue with the use of \"partial inference of GFlowNets\" in the title and \"partial inference\" elsewhere and would strongly suggest for this to be revised.\n  - \"Inference of X\" means, given some information related to X, producing an instance of X or a distribution over X. So \"partial inference of GFlowNets\" should mean learning part of a GFlowNet, or learning it from incomplete information. This is not what is done in this paper: the whole GFlowNet is learned, but some terms in the objective can be computed using partial trajectories.\n  - The third paragraph of the introduction and the second paragraph of 2.2 attributes \"partial inference\" to [Pan et al., 2023a], but in fact that paper does not introduce this term and does not ever use it.\n  - The minimal fix would be to make the title \"partial inference ~of~ **in**/**for** GFlowNets\", which would more accurately describe what is being done.\n- Small errors in exposition on GFlowNets:\n  - Error on the top of p.3: $\\exp(-{\\cal E}(x))$ is the reward, not the energy.\n  - End of 2.1: SubTB as written does not \"interpolate between DB and TB\". It only interpolates if one uses the $\\lambda$ parameter from [Madan et al., 2023], in which case it indeed interpolates between DB ($\\lambda\\to0+$) and TB ($\\lambda\\to+\\infty$).\n  - 2.2: As written, (2) simply does not make sense: $\\cal E$ is defined to be a function with domain $\\cal X$, but then it is applied to nonterminal states $s$! This can be fixed by writing that FL **assumes** an extension of $\\cal E$ to nonterminal states, and that the freedom we have in the choice of this extension is a starting point for this paper (+ briefly discuss possible sources of the partial energies).\n- Limitations/costs/failure modes of the proposed algorithm are not discussed.\n- Missing details on the form of the model that predicts $\\phi$ (I could not find them in the appendix). Does it share some parameters with the flow and policy models? It is an interesting question how simple or complex the energy decomposition model needs to be, relative to the policy model, in order to be useful.\n- Some theoretical characterization of the optimal energy decomposition would be helpful (even in the case of a tree-shaped state space).\n- In all plots with curves, please use markers or line styles, and not just colour, to distinguish the curves.\n\nI am very willing to raise the score to 6 or even 8 if the above weaknesses and below questions are addressed."
            },
            "questions": {
                "value": "- On alternative potential learning:\n  - It is a little surprising to me that learning the $\\phi(s\\rightarrow s')$ works so much better than learning an extension of $\\cal E$ to nonterminal states, just replacing $\\phi(s\\rightarrow s')$ by ${\\cal E}(s')-{\\cal E}(s)$ in the current objective (section 4.5). Such an expression automatically guarantees a cycle consistency property for $\\phi$. \n  - Why do you need to learn a proxy model to predict terminal energies instead of using true rewards?\n  - Additionally, one could regress each $\\phi(s\\to s')$ to $\\frac1T{\\cal E}(s_T)$ (or, using energies, regress ${\\cal E}(s_t)$ to $\\frac tT{\\cal E}(s_T)$). This would also decrease variance and sparsity.\n- I am curious how well the energy decomposition performs with SubTB and how it could be used with TB, which does not require learning state flows (while the current system requires **three** estimators for every edge, in addition to one per state). For example, one could add the predicted energy difference to the logits of the forward policy. Do you have any ideas about this question?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3483/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3483/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3483/Reviewer_RMo5"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3483/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698775463613,
        "cdate": 1698775463613,
        "tmdate": 1700326904201,
        "mdate": 1700326904201,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yRmZ3PtrOU",
        "forum": "P15CHILQlg",
        "replyto": "P15CHILQlg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3483/Reviewer_q5Nd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3483/Reviewer_q5Nd"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to improve GFlowNet training by assigning rewards or credits to the intermediate steps. Previous work like forward-looking(FL) GFlowNet propose to reparametrize the state flow function and make use of the partial reward. However, this evaluation of the reward of intermediate state can be expensive. Therefore, the author proposes an interesting learnable energy function on state transitions and obtain good results in some tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper targets at an interesting and valuable question. Both empirical and theoretical results seem solid and convincing. By learning decomposition of the reward in the terminal stage using potential functions, it can improve both detail-balance based or sub-trajectory balance based GFlownet training."
            },
            "weaknesses": {
                "value": "See questions."
            },
            "questions": {
                "value": "The author(s) use two energy-based decomposition schemes as a base-line, namely, the model-based GFlowNet (Jain et al.) and LSTM-based decomposition (Arjona-Medina et al.). Why not test other base-lines? It seems like a small comparison, and I\u2019m not entirely sure if the LSTM is comparable to a GFlowNet based model given its simplicity.  Another question would be the paper discussed both DB-based and subTB based objectives. What about other objectives?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3483/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3483/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3483/Reviewer_q5Nd"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3483/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698814294316,
        "cdate": 1698814294316,
        "tmdate": 1699636301347,
        "mdate": 1699636301347,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "s01SLdaAjQ",
        "forum": "P15CHILQlg",
        "replyto": "P15CHILQlg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3483/Reviewer_xgzq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3483/Reviewer_xgzq"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the problem of credit assignment in the context of GFlowNets. GFlowNets are amortized samplers which are trained to sample from a target energy function. Typically, GFlowets are only trained using this energy as the terminal reward for the generated object.  Even cases where some intermediate energy signals are available, using them can be computationally expensive. Similar challenges are also studied in the context of reinforcement learning in environments with sparse rewards, which includes approaches such as RUDDER which models the decomposition of returns. This paper proposed LED-GFN a method which learns a decomposition of the energy function to enable partial inference in the context of GFlowNets. The method leverages the flow reparameterization from forward-looking GFlowNets to utilize these partial energy estimates. LED-GFN decomposes the terminal state energy into learnable potential functions defined on state transitions which serve as local credit signals. The potentials are trained to approximate the terminal energy through summation and minimize variance along the action sequence. This provides dense and informative training signals. LED-GFN is evaluated on set generation, bag generation, molecule generation, RNA sequence generation, and maximum independent set problems. It outperforms GFlowNet baselines which do not use the  and achieves similar performance to methods using ideal intermediate energies."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper proposes an interesting approach to tackle the problem of credit assignment and partial inference in GFlowNets. It builds upon the ability of forward-looking GFlowNets and addresses the limitation of having an intermediate potential function by learning it. \n* The learned potential function can also act as an important inductive bias during training: approximating energy through summation and minimizing variance over the trajectory can make the energy landscape easier to model for the sampler. \n* The method enjoys quite strong empirical performance over baselines on a diverse set of fairly complicated tasks.\n* The experiments in the ablations are quite thorough and well designed and provide interesting insights in the method's performance.  \n* Reproducibility: The authors provide code to reproduce the results for the molecule generation experiments and includes most details to reproduce the results."
            },
            "weaknesses": {
                "value": "* Learning the decomposition of the terminal potentials is interesting, but there are no theoretical guarantees that the learned decompositon provides meaningful local credits in all settings. \n* In terms of the empirical results, while the method performs quite well on a variety of tasks - there is an important caveat to note which is the size of the problems. The trajectory length in the problems considered is quite small. There are no experiments on problems with long trajectories which is where the local credit assginment would be critical and thus demonstrate the efficacy of the approach. \n* Another motivation for the approach even in the presence of ideal intermediate signals is that the true intermediate energy function can be expensive to compute. However, all the experiments consider tasks where this is not the case. So it is unclear whether there is a significant computational advantage. \n* Another important caveat of the empirical analysis is that it focuses on discrete problems and does not consider the continous case."
            },
            "questions": {
                "value": "* Can you comment on the scalability of the approach and how much of the benefit does it provide for longer trajectories? \n* Have you considered combining the LED-GFN approach in an active learning setting where the terminal reward is expensive to compute and there is no intermediate energy signal? (This seems to be the actual useful case in the molecule generation case)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3483/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3483/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3483/Reviewer_xgzq"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3483/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698818994265,
        "cdate": 1698818994265,
        "tmdate": 1700596148111,
        "mdate": 1700596148111,
        "license": "CC BY 4.0",
        "version": 2
    }
]