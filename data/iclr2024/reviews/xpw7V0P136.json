[
    {
        "id": "Dv7Mxy1xue",
        "forum": "xpw7V0P136",
        "replyto": "xpw7V0P136",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4157/Reviewer_tStg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4157/Reviewer_tStg"
        ],
        "content": {
            "summary": {
                "value": "The paper talks about how to optimize the postfix of system message to make LLM hallucinate less. By introducing a synthetic task \u2013 the names retrieval task \u2013 the authors create an environment where hallucination occurrences are both frequent and easily traceable. This approach allows for a straightforward and clear identification of hallucinations - when the LLM generates a name not present on the provided list."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Originality: Though previous works have characterized hallucination using synthetic tasks, this paper goes a step further by utilizing synthetic data to actively reduce hallucination. \n\nQuality and Significance: The paper's relevance is evident, considering the persistent challenge of hallucination in LLMs. By offering a tangible method to evaluate and optimize against such hallucinations, the paper contributes a valuable tool to the domain.\n\nClarity: The paper is generally well-written."
            },
            "weaknesses": {
                "value": "The evaluation of SYNTRA is somewhat limited, as it focuses on only 2 models and 3 realistic tasks. This raises concerns about the method's ability to generalize across diverse models and tasks. Also, the effect on Vicuna seems marginal. This weak effect could limit the practical utility of SYNTRA, especially if similar results persist across other models.\n\n\nI believe the way the results are presented can be improved. There are some key takeaway messages that are hidden in the table/figure. Important insights, such as the significant reduction in grounded entities generated by Orca and Vicuna (by 66% and 99% respectively) when using SYNTRA without reference data, are buried within tables and figures. For example, this observation implies that the LLMs, in their attempt to avoid hallucination, compromise by producing limited details. Such significant findings should be more prominently highlighted, especially to stress that this setup may not be a viable solution for hallucination reduction.\n\nI find the axis label \"Hallucination rate\" misleading in Figure 2 - I think the number presented therein should instead be 1-hallucination rate? or did I miss anything?"
            },
            "questions": {
                "value": "1. How is the convergence of the optimization? Is there any constraint on the postfix such as any constraint on the $d\\times n$ matrix? \n\n2. Can SYNTRA be added on top of other classes of methods mentioned in section 2 to further reduce hallucination? \n\n3. The authors mentioned at the end of the Related Work that the direct optimization of a discrete prompt might be viable in SYNTRA as well, can they give more discussions about the pros and cons compared to SYNTRA? Will it be more difficult in terms of the optimization?\n\n4. From Figure 2 and Table 1, it looks like when LLM weights are fine-tuned, they are overly optimized for the synthetic task and as such it might ends up hallucinate more on realistic tasks. Is there any possibility that an early-stopping of the fine-tuning process might actually help in finding better parameters for both synthetic and realistic tasks?\n\nMinor:\n1. Appendix A, Appending the postfix, end of sentence, should it be $d\\times (n+m)$?\n2. Do authors believe reducing hallucination of LLMs solely by optimizing postfix has a limit and is there a way to find that limit?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4157/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4157/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4157/Reviewer_tStg"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4157/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698636419098,
        "cdate": 1698636419098,
        "tmdate": 1700592570884,
        "mdate": 1700592570884,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ixRFbhWNpD",
        "forum": "xpw7V0P136",
        "replyto": "xpw7V0P136",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4157/Reviewer_C5mJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4157/Reviewer_C5mJ"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a method called SynTra to reduce hallucination in LLMs. SynTra designs a synthetic task where hallucination is easy to evaluate -- retrieving names from a given list. The synthetic data was used for soft-prompt tuning or full fine-tuning from Orca or Vicuna. The evaluation of real-world tasks such as MS MARCO, QMSum, and ACI-Bench shows that soft-prompt tuning with both synthetic data + reference data performs the best in general."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The optimized model or system message can be transferred to real-world downstream tasks. Experiments show this reduces hallucination compared to the 13B parameter models Vicuna and Orca on tasks like search-and-retrieve, meeting summarization, and clinical report generation.\n- Novel idea of using a synthetic task to easily isolate and optimize against hallucinations."
            },
            "weaknesses": {
                "value": "- Lack of baselines: The proposed method was applied to existing fine-tuned LLMs like Vicuna and Orca and the experiments only show that they become better than the original LLMs. \nHowever, we do not know how the quality of the SynTra task compared to other datasets.\nFor example, a more fair comparison would be finetuning two LLMs starting from LLaMA, one on the VIcuna sharegpt data, the other on the SynTra data. I wonder if the effects of the SynTra data are not as much as the Vicuna data, in terms of reducing hallucinations.\n- Comparison with the code datasets: [previous work](https://arxiv.org/abs/2210.07128) [1] has found that training the model on code datasets is beneficial for reasoning. I wonder how the quality of the SynTra task compared to directly fine-tuning the model on the code dataset?\n- In general, training on the rule-based synthetic data would help is expected. However, the question that matters is how useful this data could be compared to other existing datasets such as instruction tuning datasets or code datasets. I suspect that the effects of synthetic data would be marginal when compared to others.\n- The transfer from synthetic to downstream tasks could be brittle: Sometimes full fine-tuning is better than soft-prompt tuning in system messages, but sometimes it's not. Sometimes reference data regularization is helping, sometimes it's not. \n\n[1] \"Language Models of Code are Few-Shot Commonsense Learners,\" Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, Graham Neubig. EMNLP 2022"
            },
            "questions": {
                "value": "- In figure 2, the y-axis says hallucination rate (the lower the better), but according to the passages, the y-axis should be the accuracy (the higher is better). Please clarify this.\n- Citation error: (yew Lin & Rey, 2004) -> (Lin & Rey, 2004)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4157/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4157/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4157/Reviewer_C5mJ"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4157/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698704204646,
        "cdate": 1698704204646,
        "tmdate": 1700539405586,
        "mdate": 1700539405586,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "txYlUdJqwZ",
        "forum": "xpw7V0P136",
        "replyto": "xpw7V0P136",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4157/Reviewer_MYvf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4157/Reviewer_MYvf"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a simple method for reducing hallucinations in LLMs: finetuning on a synthetic task where hallucinations are easy to elicit and measure (copying certain entries from a provided list of last names). The authors' best method boils down to optimizing system message via prefix tuning and then using this soft system message generalises well to realistic abstractive summarisation tasks, reducing the hallucination rate (measured in terms of BLEU, number of grounded named entities and GPT-4 judge scores)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper addresses a big open problem (hallucinations in LLMs) and stays up-to-date with state-of-the-art advancements in the field (using benchmarks such as ACI-Bench and models such as Vicuna).\n2. I like the general \u201c[just ask for generalization](https://www.notion.so/Teaching-language-models-to-hallucinate-less-with-synthetic-tasks-128f91205a1d45208d3add1298c95f18?pvs=21)\u201d approach to reducing hallucinations that does not use human feedback or human demonstrations directly. One could hope it would scale well as LLM capabilities grow (though the authors don\u2019t show this) and won\u2019t limited by errors and bias of human-provided, on-domain gold demonstrations.\n3. The paper is written clearly and was easy to follow.\n4. The experiments conducted by the authors are well-designed and comprehensive. For instance, I like how they control for limiting hallucinations by exploiting spurious features in their synthetic task and exploiting the errors of GPT-4 judge and report additional metrics."
            },
            "weaknesses": {
                "value": "1. The authors only experiments with one model size (13B) and it\u2019s hard to say how well their method scales. Will be benefits of SynTra increase or decrease for finetuned LLaMA 30B or 65B? What about LLaMA 6.5B?\n2. The absolute improvements in hallucination rate (Table 1) don\u2019t strike me as very high. I\u2019m not sure they justify the software complexity of soft system message optimization.\n3. Relatedly, it would be good to compare SynTra to some simpler baselines, e.g. finetuning on gold labels for some other summarization dataset (e.g. OpenAI\u2019s cleaned Reddit data). I\u2019d also be curious to see how RLHF\u2019d models (e.g. LLaMA-2-*-chat series) do on those benchmarks."
            },
            "questions": {
                "value": "1. It was a mildly surprising finding for me that soft system message optimisation is so much better than supervised finetuning. Have you tried manipulating the $\\alpha$ hyperparameter to increase regularisation for supervised finetuning? I could imagine this being helpful.\n2. Is the KL in eq. 2 forward (KL(base, new)) or reverse (KL(new, base))?\n3. It\u2019s confusing that \u201challucination rate\u201d in Fig. 2 is actually non-hallucination rate (higher is better) while for \u201challucination rate\u201d in Table 1 lower is better."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4157/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698765579410,
        "cdate": 1698765579410,
        "tmdate": 1699636381478,
        "mdate": 1699636381478,
        "license": "CC BY 4.0",
        "version": 2
    }
]