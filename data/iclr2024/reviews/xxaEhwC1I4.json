[
    {
        "id": "O6tJoJhAIV",
        "forum": "xxaEhwC1I4",
        "replyto": "xxaEhwC1I4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8446/Reviewer_yV1Y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8446/Reviewer_yV1Y"
        ],
        "content": {
            "summary": {
                "value": "This work studies the last-iterate convergence of stochastic gradient methods for convex optimization. The considered algorithm is general enough to include common algorithms as special cases, and the considered setting is general enough to cover general domain/unbounded noise/high probability bound/wide range of losses. This work achieves near-optimal bounds under these general settings, by a simple yet powerful observation made in a previous work Zamani and Glineur 23. The key technique is to leverage the convexity of loss for proving the last-iterate convergence, by carefully designing a sequence $z_t$ to compare the loss values with $x_t$. As a result, the obtained technical lemma yields a unified analysis for the general settings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This work makes a solid contribution to the understanding of the last-iterate convergence of stochastic gradient methods, which is an important problem is convex optimization and particularly gains interest from the ML community, since in practice the theoretically sub-optimal choice of the last iterate is cheaper and thus more popular.\n\nThe technical results are general and cover a wide range of settings, bypassing a few constraints of previous works including assumptions on compact domain and bounded noise.\n\nThe idea of constructing the sequence $z_t$ to make use of convexity is intuitive yet powerful.\n\nBesides recovering the near-optimal convergence results under a more general setting, for smooth functions, this work provides an $\\tilde{O}(1/\\sqrt{T})$ bound which improves upon the previously known best result $O(1/T^{1/3})$."
            },
            "weaknesses": {
                "value": "I do not find any substantial weakness. One constraint of this work is that it lacks a proof sketch or discussion on the main idea in the main-text. I believe the paper can benefit from adding a simplest example of $z_t$, explaining how it's used to utilize convexity. Doing so can improve the readability by giving the readers more intuitions on the design of $z_t$ and how it works."
            },
            "questions": {
                "value": "It looks to me the presentation of results currently takes up a lot of space. Is it possible to make it more compact and add more discussions on the main idea of using $z_t$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8446/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8446/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8446/Reviewer_yV1Y"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8446/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697494920946,
        "cdate": 1697494920946,
        "tmdate": 1700659673440,
        "mdate": 1700659673440,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TJoeGjvHEu",
        "forum": "xxaEhwC1I4",
        "replyto": "xxaEhwC1I4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8446/Reviewer_4SRm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8446/Reviewer_4SRm"
        ],
        "content": {
            "summary": {
                "value": "The work presents the last-iterative convergence of stochastic gradient methods  in expectation and in high probability on general domains not necessarly bounded."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The work presents high-probability and in expectation convergence result for the last iterate of SGD in general domains for convex or strongly convex objectives"
            },
            "weaknesses": {
                "value": "-In find the work incremental compared to the litterature. In fact, the work generelises the convergence results of the last iterate SGD for convex or strongly convex objectives to the general domains not necessarly compact. I find the content of the paper more adapted to be publisehd in a math/optimisation journal than ICLR."
            },
            "questions": {
                "value": "-It will be good to add discussion about your techniques to prove the last iterate SGD convergence for non convex objectives. \n-Bounded variance: This assumption contradicts strong convexity. I understand it is in several works in the literature. Take for instance F(x,y,z) = zx^2 + (1-z)y^2, z follows Bernoulli (1/2), then E_z(norm(\\nabla F(x,y,z) - \\nabla E_z(F(x,y,z)))^2) = (x-y)^2 and this is not bounded for all x and y in R.\n-You mentioned in your criticism to the SOTA in the abstract that \" the existing results are all limited to a single objective..\", as far as I can check you are also considering only a single objective, not multi objectives. Composite optimization problem still a single objective?\n-I did not check the proofs carefully."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8446/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698766616594,
        "cdate": 1698766616594,
        "tmdate": 1699637053443,
        "mdate": 1699637053443,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3EtjuBDQg7",
        "forum": "xxaEhwC1I4",
        "replyto": "xxaEhwC1I4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8446/Reviewer_V1N7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8446/Reviewer_V1N7"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on a fundamental problem on the convergence of SGD. Specifically, the main novelty is high probability, last iterate rates for SGD without bounded domain assumptions for solving convex problems. While doing so, the paper also presents unified results for composite problems / Bregman distances / convex or strongly convex problems / smooth or Lipschitz problems and expectation or high probability guarantees."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "As implicitly stated in my \"Summary\", I do think that the goal of the paper is interesting. A result involving high probability guarantees for the last iterate of SGD for convex problems is interesting in my opinion. The proofs are comprehensive and mostly carefully written (even though there are readability issues I will expand on in the later parts of my review). Having a unified result is also nice to cover different important settings."
            },
            "weaknesses": {
                "value": "Even though I think the main result of high probability rates for last iterate of SGD without bounded domains is interesting and worthy of acceptance (once the correctness is verified) there are many issues with the writing of the paper and proofs that should also be addressed which prevented me to be able to verify the correctness. Right now, the repeating theme in the paper is for the authors to spend way too much time and effort to show their improvements in marginal cases, which confuses reader to miss their main contribution (and also at times misleading comparisons with related works). In addition, the generality and unifications that the authors are trying to achieve (unification is a good thing as I said before but it should be presented in a correct way), makes the proofs unnecessarily complicated and difficult to check. I have suggestions to improve this. \n\nThe main reason for my rating is because I could not verify the proof for the main result of the paper in a reasonable time-span with many questions about technical aspects (given that as reviewers we need to review 4-6 papers in less than 20 days in addition to our regular work load). As a result, I cannot be convinced yet about the correctness. Hence, with my suggestions below, I am giving the authors the opportunity to clarify their main contribution, present their main contribution in a readable way, answer my questions about the proofs and help me verify the correctness which is required for acceptance. Once and if this is done, I will be willing to raise my score.\n\nIn particular, the issue is that the authors are trying too hard in the paper to \"sell the result\" which causes them to write the proofs in such a generality that it makes it hard to verify the result without investing days. The results of the paper should speak for themselves since high probability guarantees for last iterate without bounded domains for such an important algorithm SGD is important. I already invested days to clarify the exact contribution of the paper because the authors are trying to claim novelty in all the presented results, for this they just generalize things so much so that they can claim the novelty. But in some cases, some of these generalizations are straightforward and just degrade readability. Even though the paper definitely cites and states that they use the techniques from Zamani and Glineur, 2023 and Liu et al., 2023, they spent too much effort trying to justify their analysis is still novel. \n\nFor example, before Lemma 4.1, the authors give an effort to describe a high level message of Zamani and Glineur, 2023, but they say \"This brand new analysis is where we different from all the previous works\". It is unclear what are the \"previous work\" or what is \"brand new\". If there is a difference on this part of the analysis from Zamani and Glineur, 2023, please specify what it is, if not, this sentence should be clarified. It is okay to use a tool from another paper, Zamani and Glineur, 2023 in this case, but one should describe clearly what is used directly from that paper and what is modified and what the precise modification is.\n\nBecause of the unclarity in the writing of the paper, I had to spend a considerable amount of time checking the results of Zamani and Glineur, 2023, Liu et al., 2023, Orabona 2020 and Shamir, Zhang 2013 to identify precisely what the contribution is. At the end, writing of the paper felt like an oversell to me and misleading at times even though I still find the main contribution interesting (high probability, last iterate without bounded domains). \n\nAfter checking these 3 papers, I saw that we know (1) expectation rates for last iterate of SGD without bounded domains since Orabona, 2020, (2) we know high probability rates for average iterate of SGD without bounded domains since Liu et al., 2023. The contribution of the paper is to unify these two. However, the paper tries to also claim credit for doing (1) i.e., expectation guarantees for composite problems or Bregman distances or smooth problems: this is not good. \n\nBecause looking at the blog post Orabona, 2020, it is clearly written that Bregman extension and using smoothness instead of Lipschitzness are left as \"exercises\" to the reader. Also, looking at Orabona, 2020 analysis which builds on Shamir, Zhang, 2013, I agree with the blog post that these extensions (composite, Bregman, Lipschitz gradient) should be straightforward. If the authors disagree, they should clearly write why Orabona, 2020's approach does not generalize. Otherwise, the contribution of the paper is still enough without trying to justify every little thing. The authors can still add a remark saying that these were not explicit in Orabona, 2020 and they have these generalizations, but they should not emphasize this and focus on the main contribution of the paper: high probability guarantees for last iterate of SGD for convex problems without bounded domains.\n\n*** As such, what I recommend the authors to do is the following: as the main result of the paper, identify the simplest result that still captures the novelty by forgoing generality. In my view, the authors can just focus on a constrained problem with Euclidean distances, Lipschitz continuous (or smooth) convex function and provide a self-contained proof for high probability convergence of last iterate without bounded domains in this simplified setting. This way, both a reviewer such as myself and the readers of the paper can quickly verify the main result of the paper and appreciate the main contribution. The authors can then provide statements for the general cases with strong convexity/smoothness/composite cases/Bregman distances etc. and show the \"unification\" and provide their generalized proofs in the appendix (they can state the generalized result in the main text if they have the space). But what is needed is to distill the main contribution and give a digestible statement and proof for the reader so I can verify, right now there are many things I could not verify (written below). Right now, I have to go through so many generalized results to verify the correctness of the result which is tedious and time-consuming with the extensive review load."
            },
            "questions": {
                "value": "+ Are the results of Zamani, Glineur 2023 and Liu et al., 2023 work well together to get the high probability rate on the last iterate without bounded domains, or do additional difficulties arise while combining these results? If so, what are these difficulties? To clarify, I think the paper is worthy of acceptance even if the combination of these tools do not create additional difficulties. Identifying 2 interesting techniques and deriving an interesting result (as stated before I think the result of the paper is interesting) is enough for acceptance in my opinion. But this should be clearly written.\n\n+ Before Sec. 4 the authors say \"to the best knowledge, our results are entirely new and the first to prove ....\", there is no need for such repetition or such exaggerated words as \"entirely new\", the sentence meaning is the same if you just say, \"to our knowledge our results are the first to prove ....\". Also the beginning of the sentence should be \"to our best knowledge\". Similarly, before Lemma 4.1 the authors use \"brand new analysis\" which is unnecessary and also unclear if it refers to the analysis of Zamani and Glineur, 2023 or this paper.\n\n+ In my understanding, the analysis of Zamani, Glineur 2023 seems like an alternative to Orabona, 2020 who built on Shamir, Zhang, 2013. Why is the analysis in Orabona, 2020 not sufficient to extend to high probability guarantees? Or do the authors think similar results can be shown with Orabona, 2020's analysis too? Explaining this will help the reader understand what is going on. Put another way, what is the reason that the paper builds on Zamani, Glineur 2023 instead of Orabona, 2020 for the last iterate of SGD - expectation guarantees without bounded domains?\n\n+ Throughout, the authors use notation such as (this one is for example from Lemma 4.1 but it is like this all around the paper and the proofs) $\\eta_{t\\in [T]} \\leq \\frac{1}{2L \\vee \\mu_f}$ which is convoluted, difficult to read and non-standard, whereas they can simply write in a standard way as $\\eta_t \\leq \\frac{1}{2L \\vee \\mu_f}$ for $t \\in [T]$. Things become much more cluttered when the authors start writing things like (again Lemma 4.1) $v_{t\\in \\{ 0 \\}\\cup [T]} > 0$, this is unnecessarily complicated and not very readable, please consider rewriting like $v_t > 0$ for $t \\in \\{ 0, 1, \\dots, T\\}$, which is much easier to read.\n\n+ In Lemma 4.1 and other places, author also use definitions such as $v_t = \\frac{w_T \\gamma_T}{\\sum_{s=1\\vee t}^T w_s\\gamma_s}$, why not just define $v_0$ separately and just write the sum starting from $s=t$ like normal? This is much more readable than seeing $t\\vee 1$ every time for such things.\n\n+ Please describe clearly in which ways Lemma 4.2 improves over the result of Orabona, 2020 since Bregman case or smooth case is just left to readers as exercises in Orabona, 2020.\n\n+ Many places, such as the footnote in page 6 says that \"Harvey et al. (2019a) claims their proof can extend do sub-Gaussian noises ....\" Or in the beginning of page 2: \"whether these two assumptions (referring to bounded noise) remains unclear\" This is rather strange as it gives the impression that the authors think the claim of Harvey et al. (2019a) is not correct. If so, please tell the readers why you believe their proof does not extend to sub-Gaussian noises. Also, it is so easy to write to the authors of Harvey et al. (2019a) an email to ask them the extension for sub-Gaussian case, if you are not convinced. It is not very nice to write it like this in a paper without checking with the authors of the existing paper to clarify their claims. Looking at Harvey et al. (2019a) I also think that their result is extendable to sub-Gaussian as they also described clearly what would change and how the changes would be handled. Again, the current submission does not need to focus on such minor issues, even if Harvey et al. (2019a)'s claim is true (which is in my opinion), the submission's contribution still stands since they get rid of the bounded domain assumption.\n\n+ What are the main technical novelties in addition to Liu et al., 2023? Please describe this in Sec. 1.2. The two works are quite related and this section does not clearly state this.\n\n+ Page 3 mentions \"smoothness with respect to optimum\", what do the authors refer to by this phrase?\n\n+ page 4, you can just use $\\mathcal{X}=\\mathbb{R}^d$ and define the domains of $f,g$ in standard ways to get rid of all the discussion about int(dom) etc. These are standard problem setups, it is better not to complicate things unnecessarily.\n\n+ page 4 and many places of the paper refer to the book Lan, 2020 for some results. Please clearly specify what you refer to in this book so that the readers know where to look at, we cannot expect readers to browse a whole book to understand what the authors are using from this book.\n\n+ Sec 2.1 is unnecessary and should be moved to the appendix. Moreover, the sentence starting with \"We also would like to emphasize that ....\" Reads strange, please consider rewriting. \n\n+ Theorem 3.1 has a step size choice depending on $x^*$, this is not good and should be avoided. The authors say after the statement that things work with any $\\eta$ independent of $x^*$, if so, write things that way in the main text. Of course, a result needing distance $x^*$ to set step size is definitely not desirable.\n\n+ The discussion after Theorem 3.1 says also that \"Orabona 2020 cannot be applied to composite optimization\", why? What breaks in their argument? It is not clear to me why Orabona, 2020 would not extend to composite setting in a straightforward way. If the authors make such claims such \"cannot be applied\" for previous results, they should explain why. Again, whether or not Orabona, 2020 can be extended to composite is not essential for this submission. The main contribution of the submission is independent of this. By putting phrases like this, the authors only confuse their reader. They do not have to claim novelty for every single result contained in the paper (in some cases they can just recover existing ones, this is okay), the authors should clarify their main contribution, which is high probability guarantees for last iterate of SGD for convex problems.\n\n+ Page 5 also claims that the results are the first improvement over Moulines, Bach 2011 for smooth problems, whereas Orabona, 2020 just left it as an exercise to extend their results for smooth problems. If the authors think that Orabona, 2020's analysis does not work with smoothness, they should write why they think so. Otherwise, this is misleading for a reader and only distracts from their main contribution of the submission.\n\n+ Before Theorem 3.2, the authors mention recovering $L/T$ rate in the noiseless case. It seems that this is not true with any $\\eta$ but only the $\\eta$ depending on $x^*$, can the authors clarify?\n\n+ The case of strongly convex is strange since in this case the bounded gradient assumption (that is, Lipschitzness of the convex function) and strongly convex assumption contradict each other. This is well-known, see for example Section 1 of \"SGD and Hogwild! Convergence Without the Bounded Gradients Assumption\" by Lam M. Nguyen, Phuong Ha Nguyen, Marten van Dijk, Peter Richtarik, Katya Scheinberg, Martin Takac. In this case, what is the significance of the strongly convex and bounded gradient case? Of course, the unified result also contains cases where we do not have bounded gradient and strong convexity together, but this should be explained well and be separate from the main contribution of the paper.\n\n+ Paper uses the phrase at many places (including the title) \"last-iterative convergence\", this is not a correct terminology, also probably not a correct grammar, it should be \"last-iterate convergence\". It is used many times and disrupts the flow, please change it throughout.\n\n+ Please explain the 5th line in the chain of equations in the beginning of the proof of Lemma 2.1.\n\n+ Proof of Lemma 4.1. Explain the requirements of $v_k$ in the beginning of the proof. Add the reason why $z_t \\in \\mathcal{X}$, that is: because it is a convex combination of points $x^*, x^1, ..., x^t$ that are in $\\mathcal{X}$.\n\n+ beginning of page 15, fourth line is difficult to follow, please write clearly.\n\n+ After eq. (7), when authors use convexity of $F$ with $z^t$, $z^t$ is not written in a proper way for the reader to understand it is a convex combination of points. Please write the correct representation to improve readability. That is, you need to recall that $v_s$ is monotone and weights sum up to $1$. Zamani, Glineur, 2023 writes this clearly for example.\n\n+ Last inequality chain in page 15, please explain the last step.\n\n+ Page 17, the estimation of $2w_t\\gamma_t \\eta_t v_t$, please explain the first inequality. Also refer here to the definition of $\\bar v$ to improve readability.\n\n+ Beginning of page 18 while using Lemma 2.1, clarify what refers to $\\lambda$, $Z$ in the lemma and why the requirement in Lemma 2.1 on $\\lambda$ is satisfied here.\n\n+ Beginning of page 19: $w_1$ is undefined since $w_t$ contains a sum starting from $2$. Of course it is implicitly clear what it is, please define $w_1$ properly.\n\n+ Page 19, bound of $w_1/w_T$, please explain the inequality.\n\n+ After Lemma 4.2, the authors again say \"first unified inequality for the in-expectation\" and after Lemma 4.3, they say the same for high probability, I understand that the authors want to emphasize the novelties but after a point it gets tedious for the reader and gives the feeling of \"oversell\". Hence, please try to be more concise, state the novelty once or twice in the paper and then tell the readers main contributions clearly."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8446/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8446/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8446/Reviewer_V1N7"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8446/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699316580216,
        "cdate": 1699316580216,
        "tmdate": 1700716363991,
        "mdate": 1700716363991,
        "license": "CC BY 4.0",
        "version": 2
    }
]