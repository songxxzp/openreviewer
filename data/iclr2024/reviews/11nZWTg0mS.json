[
    {
        "id": "0EMcxGvfH7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8903/Reviewer_6LQq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8903/Reviewer_6LQq"
        ],
        "forum": "11nZWTg0mS",
        "replyto": "11nZWTg0mS",
        "content": {
            "summary": {
                "value": "This work seeks to establish a benchmark to establish the morality and business ethics of large language models. They construct their benchmark from two sources: IBM's Business Conduct Guidelines and the Social Chemistry dataset. From these sources, they generate extensive-from games that embody different morals/ethics. They then evaluate a suite of LLMs on their ability to play these games. They additionally showed that a small-LLM fine-tuned on their dataset is able to achieve similar performance as a much larger LLM."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- Investigates an important problem of measuring the value alignment of LLMs."
            },
            "weaknesses": {
                "value": "- Morality and ethics are not well defined in general nor in the context of this paper. This is largely because they are often contextually defined.\n- It's not clear how the concepts sourced to generate games should constitute a benchmark and morality and ethics. \n  - The BCG is just a handbook for IBM. Why is this one handbook the choice? What were the other options? The sections of these things seem really abstract.\n  - Social Chemistry appears to not actually be a benchmark or definition of morality, but instead about compatibility between two agents. \n- How the games are constructed from the source datasets is not described, only the mechanics of playing a game.\n- There's no analysis of the constructed games, only the empirical performance of some LLMs. Recommend following at least a framework like \"Datasheets for Datasets\"."
            },
            "questions": {
                "value": "- Sec 3.1, if the purpose of this work is to build a benchmark why are you filtering down the data?\n  - Why choose to filter specifically the harder examples that might provide better insights into the quality of the models?\n  - Could the authors provide more details as to how they deemed a question was too \"abstract\" and must be filtered? This seems like a point of failure, because authors opinions could overly influence the metrics.\n  - Please provide further details about the number of questions considered, the total number of final questions, and information about the set of withheld question and why they were withheld. \n  - How were the stories created that realized these different criteria? Why only one story? \n- Sec 3.1 and 3.2 are missing basic analysis into the input data and the constructed games. \n- It appears the aliases that were hand-constructed were essential for measuring the quality of current models. This unfairly biases the games to methods that were treated with this extra effort. Therefore, aliases must be further constructed for all future methods to ensure a more fair comparison --- where fair here is hard to quantify or ensure. \n  - For example, consider a new LLM that uses broken English, but has a high morality/ethical position. This method may perform disproportionally poorly. \n- Your results on the Flan models suggest a simple confounder for drawing any results of this dataset: it could simply measure a model's ability to follow instructions rather than its morality. I was hoping the authors might be able to speak more about this. \n \n**Nit**\n- There are issues with using `\\cite`, `\\citet`, and `\\citep`. \n- It seems like the font was modified.\n- Sec 1, Par 2, \"jailbroken\" not defined."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Discrimination / bias / fairness concerns",
                    "Yes, Potentially harmful insights, methodologies and applications"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "This paper attempts to define benchmarks for concepts such as \"morality\" and \"ethics\" to evaluate LLMs. Any issues in these definitions can misalign the incentives of these models resulting in discrimination, bias, and fairness concerns that can potentially harm different groups. This paper does not treat the severity of these definitions with the rigour and care that would be needed to mitigate potential negative outcomes."
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8903/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697223813025,
        "cdate": 1697223813025,
        "tmdate": 1699637120268,
        "mdate": 1699637120268,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Q3HUq5Rqic",
        "forum": "11nZWTg0mS",
        "replyto": "11nZWTg0mS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8903/Reviewer_T1SJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8903/Reviewer_T1SJ"
        ],
        "content": {
            "summary": {
                "value": "The objective of this paper is to evaluate the moral reasoning abilities of large\nlanguage models using text-based games. To begin, the authors develop these\ntext-based games in a conversational manner and generate training datasets in\nthe form of triplets comprising prompts, responses, and scores."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The topic of ethics and large language models is becoming increasingly\nimportant and urgent to address.\n- The idea of discovering moral reasoning through conversation makes sense."
            },
            "weaknesses": {
                "value": "While the primary goal of this paper is\nto introduce a new benchmark, the proposed benchmark does not sufficiently\ndemonstrate its potential for widespread use.\n\n- The paper mentions that the game scores are manually annotated, but\nit lacks an introduction to any specifics regarding human annotation or\nevaluations.\n- The sources of the games are derived from \u2019Business Conduct Guidelines\u2019\nand \u2019Social Chemistry,\u2019 which may not be sufficient sources for defining\ngeneral \u2019moral\u2019 or ethical reasoning, limiting the broader applicability of\nthis benchmark.\n- There are several related works that discuss moral annotation [ZYW + 22,\nJHB + 21], and benchmarks consider moral behavior in text-based games\nwith large language models [PCZ + 23]. However, this paper does not discuss these specific papers.\n\n\n[JHB + 21] Liwei Jiang, Jena D Hwang, Chandra Bhagavatula, Ronan Le Bras,\nMaxwell Forbes, Jon Borchardt, Jenny Liang, Oren Etzioni,\nMaarten Sap, and Yejin Choi. Delphi: Towards machine ethics and\nnorms. arXiv preprint arXiv:2110.07574, 6, 2021.\n\n[PCZ + 23] Alexander Pan, Jun Shern Chan, Andy Zou, Nathaniel Li, Steven\nBasart, Thomas Woodside, Hanlin Zhang, Scott Emmons, and Dan\nHendrycks. Do the rewards justify the means? measuring trade-offs\nbetween rewards and ethical behavior in the machiavelli benchmark.\nIn International Conference on Machine Learning, pages 26837\u2013\n26867. PMLR, 2023.\n\n[ZYW + 22] Caleb Ziems, Jane Yu, Yi-Chia Wang, Alon Halevy, and Diyi Yang.\nThe moral integrity corpus: A benchmark for ethical dialogue sys-\ntems. In Proceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long Papers), pages\n3755\u20133773, Dublin, Ireland, May 2022. Association for Computa-\ntional Linguistics."
            },
            "questions": {
                "value": "- How do you position your work in relation to the above mentioned related work?\n- How do you justify using 'Business Conduct Guidelines' as a basis for moral reasoning?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8903/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698398202698,
        "cdate": 1698398202698,
        "tmdate": 1699637120124,
        "mdate": 1699637120124,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jzgdHMQ54m",
        "forum": "11nZWTg0mS",
        "replyto": "11nZWTg0mS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8903/Reviewer_N9dw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8903/Reviewer_N9dw"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a benchmark featuring a series of text-based games to evaluate the moral reasoning of language models. These games engage the model through multi-turn dialogues. Within a given dialogue context, the model can select from multiple valid actions, each associated with a distinct moral score set manually.\nThe author tested serval leading open-source language models on this benchmark. Among them, Flan-T5 was fine-tuned and its performance was assessed on both the proposed benchmark and other moral corpora. Results suggest that the fine-tuned Flan-T5 excels in moral evaluations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed benchmark consists of a series of text-based games that evaluate the moral reasoning of language models through conversations. The game environment incorporates semantic matching, allowing for better alignment of the language model's output to valid actions.\n- The games were designed with a focus on the moral dimension, a crucial yet under-researched metric for language models. The definitions and considerations regarding moral principles in these games are robust, drawing from established booklists or datasets.\n- The authors conducted comprehensive experiments to assess the moral capabilities of leading open-source models. The evaluation was performed on both the proposed benchmark and other moral corpora."
            },
            "weaknesses": {
                "value": "About the Proposed Benchmark:\n- The authors mention that \"Each game is generally five to eight nodes deep, has between ten to twenty unique conversation states, and about ten unique triggers connecting these nodes.\"  Such a game's state space seems relatively basic to challenge the multi-step reasoning of language models. Perhaps for some leading commercial LLMs, this benchmark with a 5-8 level depth might be too simplistic. While the authors attribute this simplicity to the context window constraints of language models, advances in the field suggest there are methods to overcome this limitation. Introducing games of varied complexity, especially focusing on the number of dialogue rounds, could provide a more rigorous test for the moral reasoning of advanced models.\n- The game's moral scores are manually annotated. It is important to detail the selection and background of the annotators. Are they trained? How is their annotation accuracy evaluated, and are their annotation cross-validated?\n\nAbout the Experiments:\n- The explanation about the loss score (section 7.1) lacks clarity. Could you elucidate how the moral scores were normalized? It's a bit puzzling to see game scores ranging from -100 to 100 being mapped to -3 to 2. The authors mention that \"These scores were normalized and weighted so negative scores, \u22123 \u2264 norm_score \u2264 0, were more impactful than positive ones, 0 \u2264 norm_score \u2264 2.\" raises some questions. For the sake of model improvement, wouldn't it generally be beneficial to give more emphasis to positive samples during the fine-tuning process, leading to more significant weight adjustments?\n- The model was trained and evaluated on the proposed benchmark. This improved its performance on that dataset, but doesn't necessarily demonstrate its ability to generalize well. The authors also conducted experiments on other datasets. However, what is the distribution between these two datasets? Do they share similarities?\n- How was the temperature parameter of the model set in the experiments? Would different settings affect the evaluation results?\n- Would the input order of valid actions have any impact on the evaluation results? I seem to remember a study suggesting that the decisions of GPT-3.5 and GPT-4 might be influenced by the sequence of the options given. Have you found similar observations?\n\nAbout the Writing:\n\nThis paper might benefit from some adjustments:\n- Incorrect citation format: cite/citep.\n- The section on related work could delve deeper into topics like text-based game benchmarks, LM for gameplay, and morality in LMs."
            },
            "questions": {
                "value": "Two minor questions:\n- In Section 3, the title is quite broadly. It might be helpful to specify the specific games, such as \"Moral High Ground\". I noticed that the initial concepts of 'live games' and 'control' aren't revisited later on. Instead, terms like BCG games and social chemistry games are introduced. It would be great to understand their correlation.\n- How was the metric Win Rate determined?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8903/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698965707791,
        "cdate": 1698965707791,
        "tmdate": 1699637120007,
        "mdate": 1699637120007,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VkpmI7NMhH",
        "forum": "11nZWTg0mS",
        "replyto": "11nZWTg0mS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8903/Reviewer_Uvtk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8903/Reviewer_Uvtk"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a text based gaming framework for evaluating LLMs for ethical understanding about the human society. Authors develop Moral High Ground game based on text-based games where an agent encounters various ethical situations while playing the game and it accumulates (positive/negative) points while taking actions in those situations. Each game is modeled as a conversation, where each utterance corresponds to a state (node in a graph) and response (action, an edge in the graph) leads to a new state. The final score in the game is indicative of the moral knowledge of the agent. Authors propose LLM as an agent playing the game to test their ethical knowledge."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The paper is addressing an important problem pertaining to ethical understanding of LLMs.\n2. Authors create a framework for evaluating and comparing existing LLMs regarding moral knowledge.\n3. Authors perform a detailed set of experiments to evaluate various LLMs and show that existing models lack understanding of ethical knowledge."
            },
            "weaknesses": {
                "value": "1. The paper is poorly written and has missing references (see below). The formatting of citations is not proper. There are grammatical errors (e.g., Section 5.1 \"First, we created the games themselves\"). \n2. The proposed framework doesn't involve any type of feedback or interaction between the agent and the environment. The agent provides a series of responses and accumulates score. This is unlike text-based game where an agent interacts and gets rewards and has the opportunity to improve. Authors should not call the current framework as text-based game (this is misleading) and distinguish their work from existing text-based games literature (references below).\n3. The paper has limited novelty as authors are merely creating a new dataset of moral conversations based on existing corpora and use that to evaluate LLMs.\n\nMissing references (these are not weaknesses per se):\n\n1. There has been lot of work where LLMs have been evaluated for ethical and social norms, authors should discuss those in the related work, e.g., \n\n * NormBank: A knowledge bank of situational social norms, Caleb Ziems, et al., ACL 2023\n  * EtiCor: Corpus for Analyzing LLMs for Etiquettes, Ashutosh Dwivedi, et al., EMNLP 2023  \n  * Can Machines Learn Morality? The Delphi Experiment, Liwei Jiang, et al. \n\n2. There has been lot of work on text-based games + LLMs. Various environments, agents and approaches have been proposed, authors should contrast and discuss some of these, e.g., \n\n* Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines., Keerthiram Murugesan et al., AAAI 20\n*  ScriptWorld: Text Based Environment for Learning Procedural Knowledge, Abhinav Joshi, et al., IJCAI, 2023\n*  Interactive Language Learning by Question Answering, Xingdi Yuan, et al., EMNLP 2019"
            },
            "questions": {
                "value": "It is not clear how exactly the conversational graph is created from SocialChem dataset."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8903/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699007972346,
        "cdate": 1699007972346,
        "tmdate": 1699637119882,
        "mdate": 1699637119882,
        "license": "CC BY 4.0",
        "version": 2
    }
]