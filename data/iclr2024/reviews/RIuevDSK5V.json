[
    {
        "id": "s189PXizzW",
        "forum": "RIuevDSK5V",
        "replyto": "RIuevDSK5V",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2927/Reviewer_Yj3F"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2927/Reviewer_Yj3F"
        ],
        "content": {
            "summary": {
                "value": "For imbalanced regression, the authors propose ConR, which uses\nsupervised contrastive loss as part of the loss function.  They define\nSim(label1,label2) > omega, a threshold hyperparameter, as similar\nlabels.  For each of the input instances, it generates two augmented\ninstances.  If the actual labels of two augmented instances are\nsimilar, they form a positive pair.  If the predicted labels of two\ninstances are similar, but the actual labels are not, they form a\nnegative pair.  For an augmented instance, a set of positive samples\nand a set of negative samples are found.  Augmented instances with at\nleast one negative sample are called anchors, which participate in\ncontrastive loss.  For each anchor, the fraction in the regular\ncontrastive loss is summed over all positive samples.  For the\nnegative samples in the denominator, they have a \"pushing weight\" S,\nwhich is a function of the density-based weight of the anchor and the\nSim(anchor_label, negative_sample_label).  L_conR is an average of the\ncontrastive loss of up to 2N anchors. The overall loss is a weighted\nsum of the regular regression loss and L_conR.\n\nConR was evaluated on 4 datasets, one of which has\nmulti-dimensional labels, and compared with 4 recent techniques.\nEmpirical results indicate adding ConR generally improves performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Imbalanced regression is an interesting problem.\nThe proposed technique adapts supervised contrastive loss from\nclassification to regression.  Particularly, they added a pushing\nfactor for the negative samples based on similarity in labels and density, \nwhich is interesting.  Also, negative samples have not just\ndifferent labels but also similar predictions.  Empirical results\nindicate adding ConR generally improves performance."
            },
            "weaknesses": {
                "value": "The proposed contrastive loss, Eq 1, could be further explained and\njustified.  Empirical evaluation could be improved by adding \"ConR\nonly\" and representations of RankSim and Balanced MSE, which are more\nrecent techniques.  The similarity threshold omega seems to be an\nimportant parameter, further insights could be explored.\n\nMore details are in questions below."
            },
            "questions": {
                "value": "1. Eq 1, the outermost summation: Summing over i's implies j's with more\n   positive samples would have more contribution.  Is that desirable?  If so,\n   what is the main reason?\n\n2. L_R is on the original instances, while L_conR is on 2\n   augmentations of each of the original instances.  That is, the\n   original instances are not directly involved in L_conR--is that\n   correct?  If so, what is the main reason?\n\n3. Similarity threshold omega between labels dependent on\n   applications and range of the labels (1-100 vs 1-10^6), any\n   further insights?  It seems to be trial and error as a\n   hyperparameter.\n\n4. How does ConR alone, not inconjunction with another technique,\n   perform?  Including it in Tables 1-3, would be beneficial.\n\n5. How do the representations from Balanced MSE and RankSim, which are\n   more recent, compare with ConR.  Including them in Fig 4 would be\n   important.\n\n6. In Fig 5, why 1/omega, instead of omega, is used?  In the approach\n   section, 1/omega was not discussed. \"Fig. 5b, choosing a higher\n   similarity threshold\"--it seems similarity threshold omega is\n   smaller at 1 (1/omega is higher).\n\n7. p9: Could you further explain: \"sharing feature statistics to an\n   extent that is not in correspondence with the heterogeneous\n   dynamics in the feature space\"?\n\n8. Sec 3.2.1: how are the instances augmented?  It seems to be not\n   discussed in the approach or experiments.\n\nComments:\n\nSec 3.2.2: for completeness, f_S could have more description (it seems\nto be a simple product in the appendix)\n\nminor:\n\nEq 3: 1/2N assumes all 2N augmented instances are anchors, but some\nmight not be anchors (if I understand correctly).\n\nKang et al. 21 and Khosla et al. 20 have duplicated entries in\nReferences."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2927/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697919822760,
        "cdate": 1697919822760,
        "tmdate": 1699636236481,
        "mdate": 1699636236481,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EfRyM7PuIt",
        "forum": "RIuevDSK5V",
        "replyto": "RIuevDSK5V",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2927/Reviewer_M28F"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2927/Reviewer_M28F"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the problem of imbalanced problem in real-world data. To tackle the imbalance in continuous label spaces, the authors introduce a contrastive regularization technique named ConR, which is based on infoNCE. This technique simulates both global and local similarities between labels in the feature space, preventing the features of minority samples from being overshadowed by those of majority samples. ConR primarily focuses on discrepancies between the label space and the feature space and penalizes these differences. Indeed, this is also an augmentation method rather than re-weighted method in DIR.\n\nHowever, this work contains several weakness, which is discussed below."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. ConR presents a novel auggmentation approach to handle the imbalanced problem in continuous label spaces, whose problem is important.\n\n2. The methods of regularizing process of **ConR** by pulling together positive pairs and relatively repelling negative pairs seems solid.\n\n3. The results seem to indicate that the proposed method can be seamlessly integrated with other models and exhibits improvements over existing baselines."
            },
            "weaknesses": {
                "value": "1. While the author claims that **ConR** reduces prediction error, are there any theoretical insights or guarantees supporting the idea that **ConR** can achieve a lower generalization bound? Relying solely on empirical results might not suffice to attest to the superiority of the proposed methods.\n\n2. I am curious about complexity. When performing augmentation on large-scale datasets, sampling might increase the complexity. This leads to a prevalent question: Why not leverage reweighting methods which can attain comparable (or potentially superior) results without the significant memory and time overhead associated with the augmentation step? [1,2] \n\n3. While AgeDB-DIR, IMDB-WIKI-DIR, and NYUD2-DIR are structured in DIR [3], and MPIIGaze-DIR is a creation of the authors, a comprehensive description of each dataset should be included in the Appendix. Moreover, given the variety of metrics in NYUD2-DIR, the rationale behind selecting only two needs clarification.\n\n4. The experiments did not surpass all baselines. For instance, in Table 2, the combination of **LDS + FDS + RankSim** posts the best results in terms of GM in few-shot case.\n\n5. For larger datasets like IMDB and NYUD2, **ConR** doesn't consistently outperform other models. However, it seems to excel with smaller datasets such as AgeDB and MPIIGaze.\n\n6. Examining Table 1, the reported results are as follows:\n\n| model | few |\n| :---------: | :------: |\n|FDS + RankSim|  9.68 |\n|FDS + RankSim + **ConR**| 9.43 |\n|LDS + FDS + RankSim| 9.92 |\n|LDS + FDS + RankSim + **ConR**| 9.21 |\n|||\n\nFurther analysis is warranted. For instance, why does **LDS + FDS+RankSim** underperform compared to **FDS + RankSim**? Yet, with the addition of **ConR**, it achieves superior results. This observation hints that **ConR** might enhance outcomes when paired with **LDS + FDS + RankSim**. However, this theory doesn't hold when assessed on IMDB-WIKI-DIR. A more detailed analysis of the experimental results is recommended.\n\n[1] Wang et al., Variational Imbalanced Regression: Fair Uncertainty Quantification via Probabilistic Smoothing. NeurIPS 2023\n\n[2] Ren et al., Balanced MSE for Imbalanced Visual Regression, CVPR 2022\n\n[3] Yang et al., Delving into Deep Imbalanced Regression. ICML 2021"
            },
            "questions": {
                "value": "See Above. Overall, this work is novel, and interesting. The problem they try to solve is imporant, and results seems good. Nonetheless, I strongly recommend the authors to delve deeper by: \n\n(1) **theoretical insights or guarantees** supporting the efficacy of the proposed method, and \n\n(2) **thorough empirical analysis of the experimental outcomes**."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2927/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2927/Reviewer_M28F",
                    "ICLR.cc/2024/Conference/Submission2927/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2927/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698492415939,
        "cdate": 1698492415939,
        "tmdate": 1700704420029,
        "mdate": 1700704420029,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "t3YMUz95bc",
        "forum": "RIuevDSK5V",
        "replyto": "RIuevDSK5V",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2927/Reviewer_ECg6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2927/Reviewer_ECg6"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the imbalanced regression problems where the label space is continuous. The proposed contrastive regularizer is to model global and local label similarities in feature space and prevent the features of minority samples from being collapsed into their majority neighbours. The proposed ConR consolidates essential considerations into a generic, easy-to-integrate, and efficient method that effectively addresses deep imbalanced regression. The empirical study shows that ConR significantly boosts the performance of SoTA methods on four large-scale deep imbalanced regression benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper is well-written and easy to understand.\n2. The novelty of this paper is good. To my knowledge, applying contrastive learning to imbalance regression is novel.\n3. The authors provide a new dataset with 2-dimentional label space by using MPIIGaze, which could be useful to the imbalance regression community."
            },
            "weaknesses": {
                "value": "1. The proposed objective in Eq. (4) is relative hasty, without the reason of introducing the ordinary regression loss. Also, the definition of the introduced loss should be clearly provided.\n2. No deviation measure in the experimental results.\n3. No experimental result for the ConR-only case. All the results for the proposed ConR are with respect to the combination with existing methods as a regularized. Due to supervised information is already used in the loss of ConR, its preformation should be provided as a baseline.\n4. Some references are missing, e.g., Page 16."
            },
            "questions": {
                "value": "1. In the selection of anchor, if an example without any negative example, it will not be chosen as an anchor. What is the main reason for this selection? Do you consider the contrastive learning method without negative pairs, such as BYOL and SimSiam?\n2. The empirical label distribution is needed to determine the pushing weight for the negative pair. In addition, the authors suggest using the inverse frequency to compute the pushing weight, so that the minority samples will obtain harder force to be repelled from the anchor. How can we determine the continuous weight with the discrete frequency? Do we need any kernel density estimation technique?\n3. Please explain the specific reason for combining the proposed loss with the ordinary regression loss in Eq. (4) with more details and evidence.\n4. I notice some failure case in Table 1, Table 2, and Table 3, when adding the ConR loss as the regularization term. It is strange that you can down-weight of $\\beta$ to avoid this situation. I understand that the main goal is to improve the performance on *Few* case, however, there are still some cases that the performance on *Few* case drops. Also, there are some cases that the performance on *All* case drops. Hence, please provide the explanation for this phenomenon and what is the specific metric for a good imbalanced regression in your paper. I think the *Few* case should be much more important.\n5. Why the results on the *Few* case of Balanced MSE in Figure 3 is inconsistent to that in Table 3?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2927/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2927/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2927/Reviewer_ECg6"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2927/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698584776098,
        "cdate": 1698584776098,
        "tmdate": 1699636236339,
        "mdate": 1699636236339,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2H5qxq2OJ5",
        "forum": "RIuevDSK5V",
        "replyto": "RIuevDSK5V",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2927/Reviewer_viAu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2927/Reviewer_viAu"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a contrastive learning approach to address the issue of imbalanced regression. This method is orthogonal to existing solutions and has demonstrated promising experimental results."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Data imbalance and the fairness of machine learning algorithms are practical issues that warrant significant attention.\n2. The method is reasonably designed and has shown good experimental results."
            },
            "weaknesses": {
                "value": "1. While the paper presents a contrastive learning paradigm adapted for regression, it does not appear to directly address the issue of data imbalance. It would enhance the paper if the authors could clarify how the method specifically tackles this challenge or consider adapting the technique to more explicitly focus on imbalanced datasets.\n2. Comparisons should be made with other contrastive regression learning methods (e.g., [a]).\n3. The manuscript could be strengthened by providing some theoretical analysis and insights to support the empirical findings. \n4. It would be beneficial if the authors could provide the pseudocode for the algorithm to facilitate the readers' comprehension of the algorithm's details.\n\n[a] [Rank-N-Contrast: Learning Continuous Representations for Regression](https://arxiv.org/pdf/2210.01189.pdf)"
            },
            "questions": {
                "value": "Given the distinct advantages of data augmentation under the contrastive learning framework, are the baseline methods utilizing their typical data augmentation strategies, or those consistent with contrastive learning? If it's the former, a comparison showcasing results after aligning the augmentation techniques would be valuable."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2927/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2927/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2927/Reviewer_viAu"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2927/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698731762551,
        "cdate": 1698731762551,
        "tmdate": 1700711256585,
        "mdate": 1700711256585,
        "license": "CC BY 4.0",
        "version": 2
    }
]