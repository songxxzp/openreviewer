[
    {
        "id": "dk7WFxQrgr",
        "forum": "RvmrhrPy7j",
        "replyto": "RvmrhrPy7j",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9268/Reviewer_2Neo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9268/Reviewer_2Neo"
        ],
        "content": {
            "summary": {
                "value": "The authors propose to use LLMs with majority voting to learn a causal order of the random variables in the underlying data generating process represented by directed acyclic graphs from observed data. The learned causal order is then used to orient the undirected edges in the output of the existing causal discovery algorithms. Additionally, the authors claim that causal graphs are not necessary needed for causal effects estimation, rather, the causal order is sufficient by finding a valid backdoor adjustment set. They further argue that using causal orders is preferable in the case when domain expert knowledge is available."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors demonstrate the utility of LLMs in causal discovery through means of causal orders and use that as a background knowledge for the existing causal discovery algorithms.  The paper also shows that causal structures are not necessarily required for causal effect estimation and causal orders are sufficient. It also shows both empirically and theoretically that SHD is not a good metric to measure the accuracy of predicting correct causal orders. The paper is fairly well-written and the proofs are sound."
            },
            "weaknesses": {
                "value": "* Taking outputs from LLMs as inputs to causal discovery algorithms is not uncommon [5]. I find the comparison in the experiment is not quite fair to the existing causal discovery algorithms. There are many existing algorithms that incorporate background knowledge of ordering restrictions [1, 2, 3] and they are not reported on the paper. The authors could have randomly sampled from the ground truth and provided that as background knowledge to other algorithms in the experiment especially for graphs that are less than 20 nodes to compare against methods with LLMs. Given that the theoretical contributions are relatively small, I would expect to see more empirical experiments to show the strong motivation and merits of the approach. \n\n* The experimental result could have been highly affected by the popularity of the datasets and domain knowledge on the internet and using LLMs to guide causal discovery can be very limited to those commonly available data. \n\n* It is not clear what the advantages of using LLMs as a source of domain knowledge are as it may have issues with hallucinations unless there are large-scale experiments that show some domain knowledge are impractical to obtain via domain experts and need LLMs to guide such effort. \n\n* It is also not clear to me why the estimation is not compared against with those estimation methods that use causal graphs or simply a Markov equivalence class of DAGs [4] even if there is only the information of causal orders available to show the merits of using only the causal order for estimation. \n\nReferences\n\n* [1] de Campos, Luis M., and Javier G. Castellano. \"Bayesian network learning algorithms using structural restrictions.\" International Journal of Approximate Reasoning 45.2 (2007): 233-254.\n* [2] Cooper, Gregory F., and Edward Herskovits. \"A Bayesian method for the induction of probabilistic networks from data.\" Machine learning 9 (1992): 309-347.\n* [3] Borboudakis, Giorgos, and Ioannis Tsamardinos. \"Towards robust and versatile causal discovery for business applications.\" Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 2016\n* [4] Jung, Yonghan, Jin Tian, and Elias Bareinboim. \"Estimating identifiable causal effects on markov equivalence class through double machine learning.\" International Conference on Machine Learning. PMLR, 2021.\n* [5] Taiyu Ban, Lyvzhou Chen, Xiangyu Wang, and Huanhuan Chen. From query tools to causal architects: Harnessing large language models for advanced causal discovery from data. arXiv preprint\narXiv:2306.16902, 2023."
            },
            "questions": {
                "value": "1. How does using triples helps avoiding cycles in learning the causal order? \n2. Is it possible that the causal order output by LLMs orient a new unshielded collider in the output of other causal discovery algorithms? \n3. Have the authors tried to provide background knowledge to PC and compare that with PC+LLM? For example, randomly sample from the ground truth and provide such background knowledge to PC or other algorithms."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9268/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9268/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9268/Reviewer_2Neo"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9268/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698389137128,
        "cdate": 1698389137128,
        "tmdate": 1699637167631,
        "mdate": 1699637167631,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DtqW6NrURd",
        "forum": "RvmrhrPy7j",
        "replyto": "RvmrhrPy7j",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9268/Reviewer_cXcH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9268/Reviewer_cXcH"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the question if and how LLMs can be utilized for causal discovery tasks. For this, the authors focus on effect estimation and argue that knowledge about the causal order is sufficient. The paper aims at two contributions: 1) Showing that the causal order is sufficient for effect estimation problems and 2) showing how LLMs can be used in addition to statistical approaches, such as PC, to improve the causal discovery performance. The suggested approach has been evaluated with different experiments."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper addresses a logical step to combine causal discovery approaches with the domain 'knowledge' of LLMs.  \n- Careful consideration of different approaches on using LLMs.\n- Encouraging results in the experiments."
            },
            "weaknesses": {
                "value": "The overall idea is a logical next step seeing the recent success of LLMs in the causal context. However, some of my concerns are:\n- The first contribution regarding the sufficiency about knowing the causal order is not novel and a rather straightforward insight seeing that conditioning on any 'upstream' node of a treatment variable in a DAG results in a valid adjustment set. Therefore, it is certainly good to point this out again, but this is not a new contribution by this work.  \n- The paper overall seems rather incremental, seeing that the paper by Kiciman et al. is already providing some significant prior work in this regard for causal discovery. However, I acknowledge the incorporation of LLM generated knowledge with statistical approaches such as PC.\n\nSee the \"Questions\" section for further points."
            },
            "questions": {
                "value": "My main concern is the rather incremental novelty, especially since the argument that the causal order is sufficient for effect estimation tasks is a well known point. Some other remarks:\n\n- You are focusing on effect estimation tasks, but the general premise of using LLMs for causal discovery can also be helpful for other tasks. Consider formulating it more broadly and then focus only on effect estimation in the experiments as an example.\n- You are arguing that looking at SHD is often the wrong metric. However, these works using SHD typically address the problem of inferring the whole DAG structure without any particular causal task in mind, while you are only concerned with the causal order for effect estimation problems. In that sense, the SHD makes sense as a metric to see how good the inferred DAG structure is.\n- While you reference the work by Kiciman et al., a more direct comparison is missing. In particular, the related open source package https://github.com/py-why/pywhy-llm has several prompting techniques for inferring structural information. That being said, they do not combine it with methods like PC, which is the novel part in your work.\n- Fair discussion of the limitations and potential issues with overfitting."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9268/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9268/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9268/Reviewer_cXcH"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9268/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698709319068,
        "cdate": 1698709319068,
        "tmdate": 1699637167505,
        "mdate": 1699637167505,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4hbSsBXBUI",
        "forum": "RvmrhrPy7j",
        "replyto": "RvmrhrPy7j",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9268/Reviewer_vBFn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9268/Reviewer_vBFn"
        ],
        "content": {
            "summary": {
                "value": "As a method for estimating causal effects, this paper proposes using LLMs as virtual experts to elicit a causal ordering of the variables. With the causal ordering, a valid backdoor set can be determined as the causal effect can be estimated. Different prompting strategies are explored, as well as algorithms that combine these virtual expert judgments with existing causal discovery algorithms."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The results are presented fairly well.\n\nReplacing human experts by LLMs could be considered, though I am not up-to-date on the related work cited for this part of the paper."
            },
            "weaknesses": {
                "value": "The theoretical contribution is trivial, and contains multiple mistakes."
            },
            "questions": {
                "value": "* Assumption 3.3 states there is no latent confounding between treatment and target, but you actually need the stronger assumption that there is no latent confounding between any observed variables. Otherwise for instance proposition 4.2 will fail: Suppose we want to find a valid backdoor set for $X \\to Y$, and there is a third observed variable $Z$ that is not a cause or effect of $X$ or $Y$, but there is a latent variable causing $X$ and $Z$, and another causing $Y$ and $Z$. Then a valid topological ordering of the observed variables is $Z < X < Y$, but adjusting for $Z$ actually opens the backdoor path.\n\n* Proposition 4.2 requires the further assumption that $i < j$.\n\n* Paragraph below proposition 4.2, \"causal effect practitioners tend to include all confounders ...\": Can you provide a reference for this claim? Either way, what you propose goes further than including all *confounders*: you also include variables that cause either the target or the treatment but not both.\n\n* The definitions of $E_m, E_f, $E_d$ for SHD are incorrect: a wrongly oriented edge will add one to each of these three variables. Further, I think you mean to add the cardinalities rather than the sets themselves.\n\n* Algorithm in section 5.2: Steps 2 and 3 and the difference between them are unclear from the text. For algorithms, it may be better to use pseudocode, or at least some mathematical notation.\n\n* In the prompts in the appendix, I noticed that often \"causally effects\" is written when \"causally affects\" was meant."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9268/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698843873598,
        "cdate": 1698843873598,
        "tmdate": 1699637167351,
        "mdate": 1699637167351,
        "license": "CC BY 4.0",
        "version": 2
    }
]