[
    {
        "id": "V9AuiGVcj1",
        "forum": "pbfy04zvcH",
        "replyto": "pbfy04zvcH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission882/Reviewer_5Gh6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission882/Reviewer_5Gh6"
        ],
        "content": {
            "summary": {
                "value": "This paper provides a fine-tuned version of LLaMa 2, OceanGPT, which is meant to be specifically used for ocean studies and related tasks. It was also finetuned by a series of GPT 3.5 derived agents, allowing it to carry out a variety of tasks, such as providing instructions for underwater robots and extracting relevant parts of literature."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The authors aim to train a model specifically for an under-served field of study, that of oceanography\n- They create a benchmark for testing AI models capabilities on ocean-adjacent tasks\n- They create a corpus of open-access literature about ocean studies."
            },
            "weaknesses": {
                "value": "- The 'instruction seed generation' approach described in section 3.2. is fundamentally flawed because it utilizes other LLMs to generate this data, meaning that it is liable to contain hallucinations and not be reliable from a scientific perspective\n- The evaluation carried out has several significant shortcomings (see my questions below) - notably that there is a lack of details regarding how it was carried out, or how reliable GPT4 is as an evaluator.\n- The insistence of the authors on \"embodied intelligence\" is not proven or described in detail, and the whole section about the instructions for underwater robots is unclear to me - how this testing was carried out, how it was evaluated, etc. (see questions below)"
            },
            "questions": {
                "value": "- Do you check licenses of content that you use? \n- \"The seed instruction dataset comes from annotations by ocean science researchers\" - which researchers? what was the annotation procedure?\n- It's unclear to me: you use 'gpt-3.5-turbo' to enrich training samples? what about hallucination issues? (in general, all the agents that you use will have this issue, so for me all the data gathered during this step (described in Section 3.2.) is unverifiable and therefore cannot be trusted\n- You say that you \"leverage GPT-4 as the evaluator for our experimental results\" - that's an unacceptable form of evaluation, because GPT-4 is lacking the specific domain knowledge you seek to represent, and because it suffers from the hallucination issues that all LLMs suffer from.\n- When you compare models and say that \"the result shows that OCEANGPT demonstrates a higher level of knowledge expertise when describing the content of radioactive nuclide research.\", how do you measure this?\n- You state that \"the experimental result suggests that OCEANGPT has the potential to acquire embodied intelligence.\" - without providing any proof of formal evaluation of the model's ability to write instruction code for underwater robots. You need to develop a procedure for evaluating this before you can make such statements."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission882/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698424699083,
        "cdate": 1698424699083,
        "tmdate": 1699636014991,
        "mdate": 1699636014991,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "u7238FenGE",
        "forum": "pbfy04zvcH",
        "replyto": "pbfy04zvcH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission882/Reviewer_bjnA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission882/Reviewer_bjnA"
        ],
        "content": {
            "summary": {
                "value": "This paper builds a large language model (LLM) for ocean science tasks, namely OceanGPT, which is the first attempt of concerning LLM with ocean science. OceanGPT is firstly pre-trained on the collected open-access literature of ocean corpus based on the LLaMA-2 model,  then fine-tuned on the instruction data generated by the proposed DoInstruct framework based on multi-agent collaboration and five specific ocean topics, and lastly evaluated on the constructed ocean-specific benchmark OceanBench."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- It is the first attempt of building a large language model (LLM) for ocean science, which is helpful for the related study.\n- The proposed DoInstruct seems flexible for the LLM model on other fields to generate instruction data."
            },
            "weaknesses": {
                "value": "Major concerns:\n- Although the authors made a great effort on building the OceanGPT, the key contributions seem not strong. Most of the operations on building OceanGPT are general for current LLM models. The authors claim that the DoInstruct is novel, but actually the multi-agent collaboration has already been concerned in the field of LLM, for example, the papers collected in the URL of https://github.com/AGI-Edgerunners/LLM-Agents-Papers, and the authors didn't discuss the essential contributions different from them.\n- The Appendix seems not finished without any content (text) from A.2 to A.6. \n\nMinor concerns: \n- The authors are suggested to pay attention to the writing like typos and grammar errors, for example, the sentence in Page 5: \"We use the retrieved texts are used as high-quality candidate samples\"."
            },
            "questions": {
                "value": "- What are the contributions that can consider that this work is meaningful to the LLM community?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission882/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698640238766,
        "cdate": 1698640238766,
        "tmdate": 1699636014911,
        "mdate": 1699636014911,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Ge4ovAfvde",
        "forum": "pbfy04zvcH",
        "replyto": "pbfy04zvcH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission882/Reviewer_E237"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission882/Reviewer_E237"
        ],
        "content": {
            "summary": {
                "value": "This study introduces a large language model for ocean science tasks to explore the potential of LLMs for ocean science."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This study introduces a large language model for ocean science tasks to explore the potential of LLMs for ocean science. It is a great research topic and beneficial to experts in the ocean science domain."
            },
            "weaknesses": {
                "value": "While the proposed OceanGPT shows great potential for ocean science tasks, certain details appear to be absent from the manuscript, and valuable information seems to be dispersed throughout the text."
            },
            "questions": {
                "value": "1.\tAdditional related work should be included to provide a comprehensive background  and necessity of the proposed work\n2.\tI recommend including \"DoINSTRUCT\" in Figure 2.\n3.\tIn Section 3.1, the authors mention that they collected a raw corpus of 67,633 documents. While the source journals are listed in the Appendix, detailed information is missing, such as the criteria for selecting these journals, the specific volumes chosen, and the types of articles included.\n4.\tIn Section 3.2, the authors state that over 10,000 data entries across 500 sub-categories were provided by ocean science researchers. However, they do not explain how these annotations were collected.\n5.\tIn Section 3.2, the introduction to the fine-tuned agent is unclear. What does it mean to \"automatically generate questions from the unsupervised ocean science corpus\"?\n6.\tIn the title of Figure 4, the authors mention that 15,000 instructions were generated from data seeds. If I understand correctly, they collected 10,000 data seeds. Does this mean that DoINSTRUCT generated an additional 15,000 instructions from the seeds?\n7.\tIn Section 3.2, please provide more details about the quality control steps, such as the number of samples evaluated.\n8.\tIn Section 4, what is meant by \"For each testing question, given two responses from two different LLMs\"?\n9.\tIn Section 5, could you explain how the win rate is calculated?\n10.\tThe content in the appendix should be reviewed for accuracy and completeness, as there are a few issues:\n(1)\tSections A.3, A.4, and A.5 are empty.\n(2)\t Some content is missing or cannot be located. For example, in the title of Figure 6, the authors refer to Table 10 in the Appendix, but I was unable to find Table 10."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission882/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission882/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission882/Reviewer_E237"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission882/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698961990006,
        "cdate": 1698961990006,
        "tmdate": 1699636014809,
        "mdate": 1699636014809,
        "license": "CC BY 4.0",
        "version": 2
    }
]