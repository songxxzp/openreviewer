[
    {
        "id": "DDMfQU2hAx",
        "forum": "yzfi15eVI7",
        "replyto": "yzfi15eVI7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8397/Reviewer_y35Q"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8397/Reviewer_y35Q"
        ],
        "content": {
            "summary": {
                "value": "This paper uses INRs to synthesize time series data. The synthesized time series is an addition of three parts: trend, seasonality and residual components. A hyper network is used to generate parameters of each part given different data. The training of iHyperTime is performed in three stages, in order to improve stability"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The idea is simple and should work well. \n2. Experimental results are promising.\n3. The model supports interpretability."
            },
            "weaknesses": {
                "value": "1. The author claims that \"a single layer SIREN has been shown to be structurally similar to a Fourier mapped\nperception (FMP)\". I am not clear whether the author use SIREN (equation 4) or FMP (equation 5) to implement fr(t) in the paper.\n\n2. If f(t) is multidimensional, does the author model each dimension as ftr(t) + fs(t) + fr(t) independently?\n\n3. How does the proposed model achieve unconditional generation? The proposed model requires {t, f(t)} as input to synthesize time series. However, how can the model synthesize new samples without {t, f(t)}?\n\n4. It would be better to cite Deep Sets when discussing the set encoder."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8397/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697500180343,
        "cdate": 1697500180343,
        "tmdate": 1699637045879,
        "mdate": 1699637045879,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Y5xCGI9fGU",
        "forum": "yzfi15eVI7",
        "replyto": "yzfi15eVI7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8397/Reviewer_3ERW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8397/Reviewer_3ERW"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes to use implicit neural representations / coordinate-based neural networks to model time series data. The proposed method decomposes time series into season, trend, and residual components, and leverages a hypernetwork to predict the coefficients of the season, trend, and residual basis functions. The proposed method is applied on time series generation tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper introduces an interesting new method for time series generation based on implicit neural representations with the added benefit of seasonal-trend-residual decomposition. Empirical and qualitative experiments show significant improvement over baselines."
            },
            "weaknesses": {
                "value": "The writing is currently very unclear and is unable to convey the critical ideas of the paper.\n\n1. How is the model used to perform generation? Based on the writing, I am unable to understand how this formulation, which requires taking data as input with the set encoder, can be used for unconditional generation of new time series.\n2. The paragraph on __Residual blocks__ in Section 3.1.1 does not make sense at all. Why is it describing the seasonal component, \"In this work, we propose to model the seasonal component of the time series as a ...\", which was already addressed in the previous paragraph? The notation is suddenly changed from $t$ to $x$? Why talk about random Fourier features if the paper proposes to use SIREN? I do not understand what is the implemented/proposed model for the residual component after reading this paragraph.\n3. What are the details of the hypernetwork component/iHyperTime architecture? More details with mathematical description needs to be given for this part. How is the Set Encoder and SIREN layers used here? Details of the hypernetwork are not given.\n4. Important details on problem formulation and training are not included. Concretely, what are the time coordinates $t_i$? For long sequences, is the whole time series encoded into a set, or divided into windows? Section 4.2 states \"Performance results for regularly sampled time series are provided in Table 1, for univariate and multivariate datasets of varying lengths.\" -- which datasets are univariate, which are multivariate? What are the varying lengths? How do I interpret 24/72/360 in Table 1?\n\nI would be happy to increase my score to \"marginal accept\" if writing is improved, and to \"accept\" if more evidence is presented on interpretability of real world data."
            },
            "questions": {
                "value": "1. Is the model actually able to disambiguate between season, trend, and residual in an interpretable manner for real world data? Can you visualize the seasonal-trend decomposition on real world datasets with strong seasonality/trend?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8397/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8397/Reviewer_3ERW",
                    "ICLR.cc/2024/Conference/Submission8397/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8397/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698637157834,
        "cdate": 1698637157834,
        "tmdate": 1700474559701,
        "mdate": 1700474559701,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8omifPonld",
        "forum": "yzfi15eVI7",
        "replyto": "yzfi15eVI7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8397/Reviewer_h6ka"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8397/Reviewer_h6ka"
        ],
        "content": {
            "summary": {
                "value": "This paper deals with the generation of time series by employing implicit neural representation (INR) networks. The main advantage of INR lies in handling irregularly sampled grids of different sizes. To avoid training one INR per instance, the INR weights are modulated through a hypernetwork based on a permutation invariant set encoder. The INR consists of three blocks (trend, seasonality, and residual blocks) that are additively recombined to reconstruct the time series. Each block is modulated by a different portion of the latent vector (the output of the set encoder), which is passed through its corresponding decoder. Once the model is trained, a new time series can be generated by linearly interpolating two existing latent vectors (corresponding to two different samples). Then, this new latent vector $z^{(gen)}$ modulates the trained INR to characterize a new time series $x^{(gen)}_{t}$ that can be queried for any timestamp $t$. The approach is tested against several baselines on regular, irregular, and long-time series datasets, and quantitative and qualitative evaluations are presented. In addition, the model allows for a trend-seasonality decomposition analysis."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- S1. IHyperTime can handle irregularly sampled time series allowing the model to learn robust representations across series with different grid measurement. Once trained, iHyperTime can generate new time series,  at any timestamp $t$, opening interesting applications.\n    \n- S2. The model follows a classic time series decomposition, which allows for some degree of interpretability and for some constrained generation on some of the factors, for instance the trend.\n\n- S3. Extensive experiments are conducted in the paper. In addition, plenty of qualitative analysis of the model are available in the appendix, leading to a better comprehension of the model behavior. In addition, the experiments show competitive results compared to the baselines."
            },
            "weaknesses": {
                "value": "Major weaknesses\n\n- W.1. Time series generation is done by interpolating linearly latent codes of two existing series, thus limiting the diversity and expressiveness of the generated time series. Moreover, it would be interesting to understand qualitatively what happened in the latent space when interpolating linearly between two instances (please see Q.1.).\n\n- W.2. The results are good at highlighting that the generated time series is faithful to the original distribution through the \\textit{predictive}, \\textit{discriminative}, and \\textit{marginal} scores, but there is no qualitative or quantitative experiment on the diversity of the generated time series. My concerns comes from the generation procedure. If $z^{(gen)} = 0.9 z^{(1)} + 0.1 z^{(2)}$, the new time series $x^{(gen)}$ would be very likely $x^{(1)}$ leading to good fidelity scores but not really generating novel time series. Moreover it is standard in generation to split the time series into train/test datasets before the generation phase. Then, generate some new $x^{(gen)}$ according to $z^{(i)}$'s from the train dataset and then compute the \\textit{predictive}, \\textit{discriminative}, and \\textit{marginal} scores between the $x^{(gen)}$ and the $x^{(j)}$'s from the test dataset. From the evaluation metrics paragraph, it is unclear if the dataset is split before generation.\n\nMinor weaknesses \n\n- MW1. The point arguing that SIREN outperforms Fourier Features Network (FFN) for learning and representing high frequencies is at my knowledge not true for time series. Indeed, classical Fourier Features where frequencies are sampled in the linear scale can suffer from spectral bias. But if the frequencies are sampled in the logarithm scale or drawn from a Gaussian distribution as in [2] the spectral bias doesn't stand anymore.\n    In the context of DeepTime [3], the authors demonstrate the superiority of FFN over SIREN in time series forecasting. \n- MW2. Figure 4 is not completely clear. More details are needed to understand how this figure was generated. At first glance, even if the method is efficient for different sequence lengths, the training and inference time should increase with the sequence length, at least due to the loading of the data. Moreover, it is quite peculiar to see a method, Fourier Flows, having a convex curve in time with the sequence length. This makes the argument on the independence with respect to the sequence length less convincing, even though the model still is efficient at training time as shown in Table 12.\n- MW3. In Figures 14-20, the distribution of the original data is different for each baseline, this limits the conclusion that can be drawn from these experiments.\n- MW4. The interpolation process for generating new time series is clear when reading the code, however it is not really described in the paper. This would be important for better clarity of the paper to have this description at least in the appendix.\n    \n- W.3. One key feature of the IHyperTime architecture is the permutation invariant set encoder which allows to encode time series with different sampled grids. However the set encoder is known for underfitting [1] because of the naive aggregation mechanism. And there is no metrics in the experiments allowing to understand the quality of the reconstruction. Moreover, the dimension of $Z_{T}, Z_{S}$ and $Z_{R}$ seems to be crucial and are not discussed in the paper (they are set to 10, 15, 15). It would be interesting to understand the trade-off between the quality of reconstruction and the quality of generation according to the dimension of the $Z's$ (please see Q.3).\n\n[1]: Kim, H., Mnih, A., Schwarz, J., Garnelo, M., Eslami, A., Rosenbaum, D., ... Teh, Y. W. (2019). Attentive neural processes. arXiv preprint arXiv:1901.05761.\n\n[2]: Tancik, M., Srinivasan, P., Mildenhall, B., Fridovich-Keil, S., Raghavan, N., Singhal, U., ... Ng, R. (2020). Fourier features let networks learn high frequency functions in low dimensional domains. Advances in Neural Information Processing Systems, 33, 7537-7547.\n\n[3]: Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven Hoi. Deeptime: Deep time-\nindex meta-learning for non-stationary time-series forecasting. arXiv preprint arXiv:2207.06046,\n2022."
            },
            "questions": {
                "value": "- Q.1. It would be interesting for a given latent vector $z^{(1)}$ and another latent vector $z^{(2)}$ to compute  $z_{\u03bb}^{(gen)}$ for $\u03bb$ in $\\{0, 0.1, 0.2, 0.3, ..., 0.9, 1\\}$ and then to visualize $x^{(gen)}_{\u03bb}$ for each $\u03bb$.\n\n- Q.2. Please refer to W.2.\n\n- Q.3. It would also be nice to have an ablation on the modulation mechanism (other than the classical set encoder) for instance the attentive set encoder proposed by Max Horn et al. in [set functions for times series]. In addition, it would be interesting to see the effect of the dimension of $Z_{T}, Z_{S}$ and $Z_{R}$ on the reconstruction loss and on the generation quality.\n\n- Q.4. As describe in MW1, the argument of using SIREN over FFN is not theoretically true. It would then be interesting to see an ablation study on SIREN vs FFN. In the same spirit, no ablation study is performed on $w_0$, which is a crucial and sensitive hyper-parameter of SIREN.\n    \n- Q.5. Could you give some quantitative and qualitative results on the reconstruction quality using IHyperTime?\n    \n- Q.6. Why is the marginal score is used only on the four Monash datasets ?\n    \n- Q.7. Could you comment the results of Table 5 for the Temp Rain dataset? It seems strange that LS4 has such a low marginal score while IHyperTime outperforms greatly LS4 in the other two metrics. Overall this table needs to be more commented. %Why not use the same notations for the metrics in Tables 1-4 and Table 5?\n    \n- Q.8. In Tables 1-2, could you explain why DiffTime discriminative score greatly improves with $30\\%$ missing data ? and then decreases again with $50\\%$  and $70\\%$ missing data. This looks like an anomaly.\n\n- Q9. Why is DiffTime not implemented on the four Monash datasets (cf Table 11)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8397/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8397/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8397/Reviewer_h6ka"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8397/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698677318839,
        "cdate": 1698677318839,
        "tmdate": 1699637045656,
        "mdate": 1699637045656,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4ersSPRieo",
        "forum": "yzfi15eVI7",
        "replyto": "yzfi15eVI7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8397/Reviewer_3NpH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8397/Reviewer_3NpH"
        ],
        "content": {
            "summary": {
                "value": "The proposed work is concerned with time series generation and introduces an approach into encode the time series in the form of implicit neural representation through a TSNET,  a trend-seasonality-residual representation. The framework is applied to several regular and irregular time-series generation tasks with various percentages of missing data and compared with other approaches."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. iHyperTime is able to synthesize time series and decompose the representation into configurable feature sets: trends- slow components of the signal, seasonality - periods of the signal and residuals.\n\n2. Results show more accurate generation ability than compared GAN and other networks.\n\n3. Training and inference times of ihyperTime appear to be more optimal than other compared approaches."
            },
            "weaknesses": {
                "value": "1. It is unclear which data is missing in experiments with irregular timeseries. Is this missing data in training or testing? The procedure of removing data needs to be defined.\n\n2. It is unclear how the method performs in terms of clustering/classification score.\n\n3. The results are not compared to GNN based generation methods."
            },
            "questions": {
                "value": "1. Recent works show that RNN encoder-decoder with various training strategies such as weak decoder or contrastive or attractive losses can both generate clustered interpretable latent representation and generate sequences. How these models compare with iHyperTime?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8397/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8397/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8397/Reviewer_3NpH"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8397/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699604644689,
        "cdate": 1699604644689,
        "tmdate": 1699637045532,
        "mdate": 1699637045532,
        "license": "CC BY 4.0",
        "version": 2
    }
]