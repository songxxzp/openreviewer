[
    {
        "id": "A0JIwQCuNI",
        "forum": "ujX2l7mNX6",
        "replyto": "ujX2l7mNX6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1299/Reviewer_QZRJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1299/Reviewer_QZRJ"
        ],
        "content": {
            "summary": {
                "value": "The authors present an approach to decode semantic textual representations of images from brain activity data recorded with fMRI. An fMRI encoder (ViT) is trained (1) to predict the CLIP-Vision CLS embedding of the seen image given the fMRI activity and (2) as part of a pipeline where the CLIP prediction is fed to each layer of a frozen GPT-2 text generative model through cross-attention layers to predict the next word in the ground truth caption of the image. The pipeline is trained on the DIR dataset containing multiple presentations of 1200 training and 50 test images for three participants. Qualitative and quantitative results suggest the pipeline could predict similar captions to the ground truth. Further analysis showed that using voxels from the higher visual cortex areas leads to better reconstruction than lower or whole visual cortex areas."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Originality: The proposed pipeline (fMRI encoder + GPT-2 with cross-attention layers) appears like a novel application of a SMALLCAP-inspired approach (Ramos et al., 2023) but with CLIP latents predicted from fMRI and no retrieval-based prompting. However, similar fMRI-to-caption work has not been cited or compared to (see Weaknesses).\n\nQuality: The paper is of acceptable quality, with a sound justification of the proposed approach and clear reporting of the results. However, as mentioned above, there is a lack of comparison to the existing literature.\n\nClarity: The paper is overall clear, though some information is missing (see Q1).\n\nSignificance: It is hard to evaluate the significance of the results given the lack of comparison to the existing literature. The analysis of Section 4.3 on decoding from specific ROIs provides interesting evidence into the properties of different cortical areas."
            },
            "weaknesses": {
                "value": "The main weakness to me is the lack of comparison to existing work on the topic of fMRI-to-caption decoding. I am aware of at least two papers proposing a similar fMRI-to-pretrained latent alignment + generative text modeling approach which also report qualitative and quantitative results on an fMRI-to-caption task (Mai & Zhang, 2023; Ferrante et al., 2023). However, the presented results are not compared to any previous baseline. Given the similarity of the approaches, a clear comparison must be made to establish whether the proposed approach provides an improvement over existing approaches. On a similar note, I believe a useful result to include in the analysis of Table 1 would be the performance of the pipeline if GPT-2 receives the ground-truth CLIP latents, instead of the fMRI-based predictions. This would provide an upper-bound for the proposed approach and help situate the reported results. Finally, the DIR dataset contains a small number of examples and categories as compared to the recent NSD dataset. As part of comparing the proposed approach to existing approaches, it would make sense to include results on this larger dataset as well.\n\nMai, Weijian, and Zhijun Zhang. \"Unibrain: Unify image reconstruction and captioning all in one diffusion model from human brain activity.\" arXiv preprint arXiv:2308.07428 (2023).\nFerrante, Matteo, et al. \"Brain Captioning: Decoding human brain activity into images and text.\" arXiv preprint arXiv:2305.11560 (2023)."
            },
            "questions": {
                "value": "1. What is the value of H in the voxel vectors? Is there any lag between the image presentation and the extracted window? How much zero padding does this vectorization scheme lead to? Is there any aggregation of the presentations (at the BOLD or prediction level), or are the reported predictions obtained from single fMRI windows?\n\n2. My understanding is that the input to the fMRI encoder is a sequence of 7 items (each one a vectorized set of ROI voxels) passed through a linear projection to 768 dimensions. Is the linear projection common to all items of the sequence? If so, I am curious to know what kind of operation it ended up learning to do.\n\n3.  In Section 4.1: \u201cOur MindGPT trained on DIR and a subset of ImageNet (Deng et al., 2009), including 150 categories totaling 200.7k images.\u201d How was ImageNet used, and how were these 200.7k images selected? My understanding from Section 3.1 is that DIR contains 1250 unique images.\n\n4. In Section 4.2: \u201cNote that the default training/test split of DIR has no overlapping image categories, we randomly sampled 50 fMRI-image training pairs, and added them to the test set for the few-shot evaluation.\u201d Can you confirm this means that these images were removed from the training set and added to the test set?\n\n5. How do the results vary across subjects? It is not clear whether the results of Table 1 and 2 are across subjects or for a specific subject.\n\n6. How were the examples of Figure 3 selected? What is the proportion of \u201cfailure cases\u201d and high-quality reconstructions? Since there are only 50 test examples (times 3 subjects) my understanding is that all reconstructions could be presented in e.g. a table.\n\n7. The analysis of Section 4.4 is an interesting way to see how the predicted latent shares information with the different image patches. What would this analysis give if you were to use the actual CLIP CLS token instead of the decoder\u2019s prediction to compute the similarity scores? Would the scores look different for examples like the killer whale image (Figure 6, bottom left)? This might be a way to confirm that this analysis tells us about the brain decoding objective and not mostly the CLIP embedding itself."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1299/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1299/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1299/Reviewer_QZRJ"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1299/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698416710698,
        "cdate": 1698416710698,
        "tmdate": 1700686408722,
        "mdate": 1700686408722,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YbLGNG4Eer",
        "forum": "ujX2l7mNX6",
        "replyto": "ujX2l7mNX6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1299/Reviewer_Hx5r"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1299/Reviewer_Hx5r"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces MindGPT, a non-invasive neural decoder that interprets perceived visual stimuli into natural languages from fMRI signals. The model employs a visually guided neural encoder with a cross-attention mechanism that uses the large language model GPT to guide latent neural representations toward a desired language semantic direction."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposed method, MindGPT, is a novel and innovative approach to interpreting visual stimuli using non-invasive brain recordings.\nMindGPT has been shown to generate word sequences that truthfully represent the visual information conveyed in the seen stimuli, with essential details.\nMindGPT has also been shown to be more semantically informative than other methods, and to be able to recover most of the semantic information using only the higher visual cortex (HVC)."
            },
            "weaknesses": {
                "value": "About the novelty. Though the authors consider the proposed method as the first task-agnostic neural decoding model, CLIP-like models have been integrated in extensive research areas to make alignment with their specific representations in a similar way.\nAlso the evaluation, could you compare your model with exist CLIP-like models and show the strength of yours. \nThe dataset is not big enough to support the model."
            },
            "questions": {
                "value": "How well does MindGPT perform on a variety of different visual stimuli?\nHow does MindGPT compare to other state-of-the-art methods for interpreting visual stimuli using non-invasive brain recordings?\nCould you add more detail on the dataset? How many subjects you used? How long the fMRI signal? \nDoes your work design a alignment between fMRI signal and images?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1299/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698792659959,
        "cdate": 1698792659959,
        "tmdate": 1699636056952,
        "mdate": 1699636056952,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cGkzUpEIB2",
        "forum": "ujX2l7mNX6",
        "replyto": "ujX2l7mNX6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1299/Reviewer_SjKZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1299/Reviewer_SjKZ"
        ],
        "content": {
            "summary": {
                "value": "In this paper authors propose MindGPT to generate captions of the images perceived by humans from the fMRI responses during perception. To do so first they align fMRI responses to visual feature space (CLIP) using a fMRI encoder (ViT) and then guide a language decoder to generate captions of images from fMRI embeddings using cross attention.\n\nThe training and evaluation is performed on publicly available fMRI dataset(Horikawa and Kamitani, 2017 ; Shen et al. ). The results show that captions generated by MindGPT correctly capture some of the semantic information present in the scene perceived. They also perform additional analysis which show\n\n1. voxels from higher visual cortex (HVC) lead to more accurate captions as compared to lower visual cortex (LVC). \n2. which visual cues were relevant for caption generation"
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is easy to follow, well written with descriptive figures.\n2. Although the idea to generate captions from fMRI responses is not new and has been explored previously (see references in weaknesses section) but the use of recent methods such as image captioning (SMALLCAP) to generate pseudo groundtruth captions, alignment with CLIP encoder and use of cross attention to guide GPT-2 makes this a new contribution\n3. Qualitatively the results are impressive (I would have preferred to see the captions of all 50 test images ). I do not have expertise in captioning literature so I am not sure how good are quantitative results though.\n4. The analysis showing HVC generates more accurate captions is exciting and could be combined with reconstruction methods to generate reconstructions that are both pixel-level and semantically accurate. In reconstruction LVC contribution is more, this paper shows that complementary information can be decoded from HVC.\n5. Authors perform additional analysis to show which visual cues were relevant for caption generation and tsne analysis to show latent representation of different brain regions \n6. Use of virtual training examples to augment the smaller training dataset."
            },
            "weaknesses": {
                "value": "1. The results in this paper are from DIR dataset (1200 training and 50 test images) which is smaller compared to NSD dataset (10k images). Another advantage of NSD is that images are from MS-COCO dataset which contain more semantically complex scenes as compared to imagenet images in DIR which contain a single object centered. So, I would like to know author\u2019s reasoning on why NSD dataset was not considered for this paper.\n2. There are a few relevant references missing (Matsuo et al. , Sakata et al.) which generate captions from brain activity. I am not sure if the code of these papers are available that\u2019s why I am not asking to compare the results but I believe they should be at least discussed for readers to understand how this paper is different from previous fMRI\u2192 caption generation methods.\n3. Self-attention maps of MindGPT encoder can be used to inform which brain region was more relevant in generating captions. This analysis could have provided more insight into captions of which category of images are generated by which brain regions.\n4. It is not clear what groundtruth is used to compute language similarity metrics. If it is compared to Image captions generated by SMALLCAP then it is a major limitation of this approach. Accuracy of image generated captions will be upper bound by SMALLCAP. Collecting human captions for at least the test set and comparing both SMALLCAP and MindGPT would have been more informative.\n5. Minor: I assume blue color text in Figure 3,4 indicates correct captions and black colored text indicate incorrect text. These should be mentioned in the Figure captions for clarity.\n\n- Matsuo, Eri, et al. \"Describing semantic representations of brain activity evoked by visual stimuli.\"\u00a0*2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)*. IEEE, 2018.\n- Takada, Saya, et al. \"Generation of viewed image captions from human brain activity via unsupervised text latent space.\"\u00a0*2020 IEEE International Conference on Image Processing (ICIP)*. IEEE, 2020."
            },
            "questions": {
                "value": "Questions\n\n1. Why was NSD not considered for this paper?\n2. Was any ablation performed to assess how crucial was the data augmentation performed using virtual training examples? e.g. will including more images per category further improve results?\n\nClarification\n\n1. Identification of visual cues using cosine similarity between fMRI encoder output and patch tokens inform which cues can be extracted from fMRI responses? Can this be helpful in answering where in an image subject was attending. If that is the case generating captions from fMRI data of Horikawa and Kamitani 2022 can lead to interesting findings.\n\nSuggestions\n1. Please refer to weaknesses section. I am confident that this contribution has the potential to be a good paper if the authors address weaknesses. \n\nReference: \nHorikawa, Tomoyasu, and Yukiyasu Kamitani. \"Attention modulates neural representation to render reconstructions according to subjective appearance.\"\u00a0*Communications Biology*\u00a05.1 (2022): 34."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1299/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1299/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1299/Reviewer_SjKZ"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1299/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698831925382,
        "cdate": 1698831925382,
        "tmdate": 1700692939073,
        "mdate": 1700692939073,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yzASKmEHqW",
        "forum": "ujX2l7mNX6",
        "replyto": "ujX2l7mNX6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1299/Reviewer_7fxY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1299/Reviewer_7fxY"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new method to reconstruct visual stimuli from the brain activities using fmri, that seeks to decode the information encoded in the human brain when processing visual information. One of the main bottlenecks of this approach is the limit on acquiring more high quality features while staying non invasive. Previous work has achieved only limited success in this field, where most of the reconstructions are blurry and without any low-level texture in the image. Having this said, the authors open new doors by using semantic feature extraction as opposed to pixel wise, feeding them to another generative model. They argue that the mentioned method is not only more robust, but obtain more detailed and relevant images after all."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Paper is very well-written even for a general reviewer. Furthermore, the settings of the model and experiments are accurately described as to make it easier for reproducibility.\n2. The novelty of using semantic features in the middle of their end-to-end model is quite intuitive. Plus this message has been delivered quite clear to the reader. \n3. Using augmentation to compensate that lack of data is justified, and they modify already existing methods to apply better in their case. \n4. Formulation of the loss is carefully written to adhere to the main purpose of the paper. \n5. Images generated with the proposed algorithm contain more low-level details and texture as shown in the paper."
            },
            "weaknesses": {
                "value": "1. The paper has circumvented the main goal of visual reconstruction, we are completely relying on the generative model to build the images.  In other words, images generated this way won't be closer than a certain amount despite having enormous details. \n2. Comparison with other methods in the field has been mentioned scarcely after the introduction part, e.g., in the experiments it's done only among the different versions of their own algorithm."
            },
            "questions": {
                "value": "Can you please mention more methods before this paper that aimed the same goal? Also highlight if using semantic methods are a complete novelty or it has been used before to some extent, with other approaches."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1299/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698861077354,
        "cdate": 1698861077354,
        "tmdate": 1699636056821,
        "mdate": 1699636056821,
        "license": "CC BY 4.0",
        "version": 2
    }
]