[
    {
        "id": "Kn9lAz7Ssr",
        "forum": "pmweVpJ229",
        "replyto": "pmweVpJ229",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8643/Reviewer_qRJd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8643/Reviewer_qRJd"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an algorithm for pure DP or GDP DP-ERM that achieves nearly linear time complexity with an optimal error rate under regularity assumptions on the loss. The algorithm uses the exponential mechanism with MCMC sampling, and uses a novel analysis to show that the approximate sample from the MCMC can still meet pure DP or GDP with some additional privacy loss."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is fairly easy to understand, despite having lot's of theory. The MCMC technique of obtaining a pure DP or GDP guarantee from an approximate exponential mechanism is novel to my knowledge, and could find applications elsewhere, although implementing the mechanism in practice doesn't currently seem possible. While the performance guarantee of the proposed DP-ERM algorithm is purely theoretical, it advances the current knowledge on what is possible under DP."
            },
            "weaknesses": {
                "value": "The biggest weakness of the paper is the purely theoretical nature of the method in its current form. Implementing it would be difficult, as the required number of MALA iterations is only given as an asymptotic expression, so it would not be possible to know the required number of iterations for any given problem. If possible, it would be great to have a non-asymptotic expression for the number of iterations in the Appendix, or some other way to compute it.\n\nMinor comments:\n- Lemma 14 should be mentioned in the proof of Theorem 2 when referring to the adaptive composition theorems. In the proof, I think it is possible that for some $y$, the sensitivity of $f_{u,y}$ is greater than the bound with more than probability 0 under $\\zeta(\\cdot | y) \\times \\zeta(\\cdot | y)$, but this set of $y$ would have 0 measure under $\\zeta$ or $\\zeta'$. I think the proof still works due to Lemma 14 allowing the second mechanism to be almost surely DP, but this should be explicitly mentioned.\n- In footnote 1, both sides of the DP inequality have $D'$.\n- In the $P(A)$ expression in the GDP part of the proof of Lemma 14, line 3 is a repeat of line 2. In the expression of $H_\\alpha(\\mathcal{M}(D) || \\mathcal{M}(D'))$, there's an extra $\\frac{d P_1}{d Q_1}(x)$ in the first indicator on line 7. When that expression continues on the next page, the Gaussian Radon-Nikodym derivatives involving $\\mu_2$ at the end of the integral are the wrong way around on lines 2-5."
            },
            "questions": {
                "value": "- Would it be possible to use ASAP outside ERM, with a generic utility function $F(\\theta)$ for the exponential mechanism? What conditions would $F$\u00a0need to meet?\n- When discussing related work, you mentioned that an existing algorithm could solve DP-SCO in nearly linear time, but adapting their algorithm to DP-ERM would not result in a linear time. What are the differences between DP-ERM and DP-SCO, and how do they lead to this difference?\n- Isn't Lemma 5 effectively assuming that the loss is bounded, as Lipschitzness of the loss implies continuity, and $\\Theta$ must be bounded due to the upper bound on $\\gamma$?\n- In the proof of Theorem 1, shouldn't the expressions for $K$ have $\\Omega$, not $\\mathcal{O}$, so they would be lower bounds instead of upper bounds?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8643/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8643/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8643/Reviewer_qRJd"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8643/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697581339746,
        "cdate": 1697581339746,
        "tmdate": 1699637082647,
        "mdate": 1699637082647,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lpD4vbbi7t",
        "forum": "pmweVpJ229",
        "replyto": "pmweVpJ229",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8643/Reviewer_kgTq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8643/Reviewer_kgTq"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a novel MCMC algorithm for pure DP and Gaussian DP empirical risk minimization. The new MCMC sampling scheme does not introduce extra privacy failure probablity and thus preserves pure DP. Moreover, the algorithm runs nearly linearly in the dataset size."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. DP-ERM is a very fundamental task. The paper addresses very practical and important issues in differentially private optimization. \n2. The paper proposes a new algorithm for DP-ERM. The major benefit is that it avoids catastrophic privacy failures and thus guarantees pure DP, and runs linearly with the dataset size $n$. \n3. The paper has interesting technical contributions, for example, a new MCMC algorithm, the relation between TV distance and  Wasserstein-infinity distance."
            },
            "weaknesses": {
                "value": "1. $\\theta^*$ seems to be defined as the minimizer for the regularized loss in Section 2.1, but in Assumption 3 $\\theta^*$ is used again for the minimizer of the original loss. The authors should clarify what is the $\\theta^*$ in Theorem 3. This is a crucial problem as in existing DP-ERM literature (e.g. Bassily'14) we should care about the excess risk w.r.t. $\\min_{\\theta}\\mathcal{L}(\\theta)$, not the regularized loss. If $\\theta^*$ is the minimizer of the regularized loss, then the authors should clarify why it is used instead of the commonly adopted formulation of excess risk.\n2. In view of 1 the notation causes confusion in understanding the main theoretical results. There are other notation issues: $J$ was first defined as $\\sum_{i}\\ell_i$ which collides with $\\mathcal{L:}$, and then redefined in Algorithm 3 as the regularized loss centered at $\\theta_0$. The authors should check other notation issues.\n3. The algorithm requires rejection sampling which in the worst case may not terminate.\n4. A minor issue is that the work lacks empirical evaluations. Even a small experiment on even the simplest convex optimization problem can provide very strong support for the computation efficiency of this work and demonstrate the practicality of the proposed algorithm.\n\n====Update====\nRaised my score to 6 after the authors have addressed my concerns."
            },
            "questions": {
                "value": "1. See weakness 1.\n2. If in Algorithm 1, we are only allowed to perform the sampling N times, and thus must return a FAIL state if none of the $\\theta^K$s are valid, would the algorithm still be pure-DP or pure Gaussian DP?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8643/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8643/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8643/Reviewer_kgTq"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8643/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698702020375,
        "cdate": 1698702020375,
        "tmdate": 1700583257205,
        "mdate": 1700583257205,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Zjv2sTMUyp",
        "forum": "pmweVpJ229",
        "replyto": "pmweVpJ229",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8643/Reviewer_kWWS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8643/Reviewer_kWWS"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the problem of approximate posterior sampling with pure differential privacy guarantee, for which prior works either have only proved weaker notions of DP guarantees (such as approximate DP and R\\'enyi DP) or have only proposed computationally expensive pure DP algorithms. The difficulty in establishing pure differential privacy guarantees lies in the gap between approximate and exact posterior sampling, contributing to a non-zero probability of unbounded information leakage. The authors circumvent this limitation by proving that as long as the posterior sampling process converges in $W_{\\infty}$ distance to a reference distribution that satisfies pure DP, then perturbing the MCMC sample with calibrated noise also satisfies pure DP. For strongly convex smooth loss functions, the author then designed a posterior sampling algorithm that converges in $W_{\\infty}$ distance by combining the Metropolic-Hasting algorithm (that converges in total variation distance) and a rejection step that only accepts samples within a bounded ball (this boundedness enables conversion from total variation distance bound to $W_{\\infty}$ distance bound). Finally, the authors showcased an application of their DP posterior sampling algorithm for the DP empirical risk minimization problem, which achieves the optimal rates in near-linear runtime."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The idea of perturbing an MCMC sampling process (that only satisfies weaker notions of DP guarantees) with noise to achieve the stronger pure-DP guarantee is interesting and novel. To make this idea concrete, the paper has used an interesting new technique that converts the weaker  TV distance bound to a $W_{\\infty}$ distance bound under the conditions that the domain is a bounded ball and the posterior density is bounded away from zero.\n\n- The proposed algorithm achieves a near-optimal rate within near-linear runtime under pure DP for the DP ERM problem, under strongly convex smooth loss function on $\\ell_2$-bounded ball). The proved rate improves with a multiplicative factor $\\kappa\\log n$ where $\\kappa$ is the condition number, and $n$ is the number of data records."
            },
            "weaknesses": {
                "value": "- The improvement for DP ERM due to the designed approximate posterior sampling algorithm (and several parts of the proofs) requires further clarification. See questions 1 and 2 for more details.\n\n- The setting is quite constrained, i.e., strongly convex and smooth loss functions over $\\ell_2$-bounded ball. It is unclear whether the privacy analysis, especially the critical conversion theorem (from total variation distance bound to $W_{\\infty}$ distance bound), is generalizable to more general forms of bounded domain or loss functions."
            },
            "questions": {
                "value": "1. In the proof for Theorem 3 (improved utility bound for DP-ERM), equations (5) and (6) utilize Lemma 6 to analyze the **approximate** posterior sampling component MALA with constraint. However, Lemma 6 only holds under **exact** posterior sampling from $p(\\theta)\\propto \\exp(-\\gamma F(\\theta))$. Could the authors explain this discrepancy and justify this usage of Lemma 6?\n\n2. In proof of Theorem 1, when proving iteration complexity guarantee for convergence of MALA in total variation distance, the authors cited [Dwivedi et al. 2019] and [Ma et al. 2019]. Could the authors cite the specific theorems? Otherwise, it is hard to validate these claims.\n\n3. Could the authors elaborate on the possibility of extending the privacy bound to more general settings, such as general bounded domain (e.g., probability simplex) or more relaxed convex/non-convex/non-smooth loss functions?\n\nMinor comments:\n- The idea of perturbing distributions with bounded $W_{\\infty}$ distance with noise to strengthen the differential privacy guarantee seems quite similar to the shift reduction lemma in the \"privacy amplification by iteration\" analysis [Lemma 20, a]. The main difference seems to be that [Lemma 20, a] focuses on R\u00e9nyi DP guarantees while the authors focus on pure DP and Gaussian DP guarantees. Maybe the authors could add more discussion regarding the non-trivialness of this extension.\n\nReference:\n- [a] Feldman, Vitaly, Ilya Mironov, Kunal Talwar, and Abhradeep Thakurta. \"Privacy Amplification by Iteration.\" arXiv preprint arXiv:1808.06651 (2018)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8643/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8643/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8643/Reviewer_kWWS"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8643/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698810810106,
        "cdate": 1698810810106,
        "tmdate": 1700493936410,
        "mdate": 1700493936410,
        "license": "CC BY 4.0",
        "version": 2
    }
]