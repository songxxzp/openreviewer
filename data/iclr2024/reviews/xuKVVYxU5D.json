[
    {
        "id": "hjQDSTCUXb",
        "forum": "xuKVVYxU5D",
        "replyto": "xuKVVYxU5D",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1968/Reviewer_7vQe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1968/Reviewer_7vQe"
        ],
        "content": {
            "summary": {
                "value": "This work develops a three time-scale algorithm, aiming to solve the robust RL problem with a single trajectory with an online fashion. The work is important considering the current works in the area."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The idea is interesting. To solve the unbiased estimation issue, another time scale is introduced. I believe this idea is interesting and novel.\n2. The experiment results are promising."
            },
            "weaknesses": {
                "value": "1. The presentation of the results is infusing and can be improved. E.g., there are too many assumptions made to imply the results. If this work is focusing on the three time-scale stochastic approximation framework itself, it is OK to make these assumptions; But this works is for a concrete problem, i.e., DR-RL, I believe it should not be reasonable to make all these assumptions. \n2. As mentioned, with so many assumptions made, I doubt the soundness of the convergence results. E.g., how can I know that under the DR-RL problem, the associated ODE has a unique global asymptotically stable equilibrium?"
            },
            "questions": {
                "value": "1. The work use gradient descent to solve the support function. Why does this DRO problem can be solved by GD? Why is this problem smooth? Why does the gradient exist? I didn't see this gradient descent approach much used in previous DRO works.    \n2. How do you justify the assumptions you made?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1968/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697565098153,
        "cdate": 1697565098153,
        "tmdate": 1699636128269,
        "mdate": 1699636128269,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Q36Rxii7ox",
        "forum": "xuKVVYxU5D",
        "replyto": "xuKVVYxU5D",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1968/Reviewer_JJMJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1968/Reviewer_JJMJ"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors proposed a distributionally robust variant of Q-learning aiming to solve distributionally robust MDP in an online fashion. Their algorithm utilizes a three-timescale stochastic approximation framework and possesses an almost surely convergence guarantee. They conduct thorough experiment illustrating the performance of their algorithm."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "They extend the classical two-timescale stochastic approximation framework into the DRO problem, and design an online algorithm for DRRL. Comprehensive experiments are conducted to illustrate the performance."
            },
            "weaknesses": {
                "value": "The writing is problematic. I find the paper hard to follow. There are many notations/terms lack of definitions and several critical statements lack of discussion and explanation. Moreover, there are typos and factual errors in this paper, which make the results not credible. All in all, it's hard to verify the validity of this paper."
            },
            "questions": {
                "value": "1. What does the misspecified MDP in the paper mean?\n2. Badrinath & Kalathil (2021) and Roy et al. (2017) didn't use R-contamination model. Their uncertainty set is a general one and covers yours. Both of them study the online setting and present asymptotic results. Why don't you compare with their works in experiment and compare their theoretical results with yours.\n3. Why the first equation on page 5 is true? \n4. On page 6, why keeping the learning speeds of $\\eta$ and $Q$ different can stabilize the training process?\n5. The three-timescale regime used in algorithm 1 is complicated and provided with limited explanations. Why and how it works? \n6. In the proof, there are more than 10 assumptions and most of them don't have any explanation. Why do they hold in your case? Are they necessary? Are they reasonable in practical problems?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1968/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1968/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1968/Reviewer_JJMJ"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1968/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698597489234,
        "cdate": 1698597489234,
        "tmdate": 1699636128164,
        "mdate": 1699636128164,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EnZgdmmCjN",
        "forum": "xuKVVYxU5D",
        "replyto": "xuKVVYxU5D",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1968/Reviewer_DkEx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1968/Reviewer_DkEx"
        ],
        "content": {
            "summary": {
                "value": "This paper considers a distributionally robust RL problem. A model-free online TD Q-learning type method is developed to find the robust Q values without relying on a simulator. Specifically, a three-timescale framework is introduced to approximate the robust Bellman equation and asymptotic analysis is provided. The main algorithm is validated through a tabular RL example; a more practical DQN type algorithm is introduced to handle more complicated examples."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper considers distributionally robust MDP using f-divergence as the uncertainty set, which is novel.\n- The motivations of not using SAA to approximate robust bellman equation are clear.\n- The reasons of not using multilevel Monte-Carlo method are clear."
            },
            "weaknesses": {
                "value": "- I guess the paper was written in parallel with the Panaganti 2022 (Robust offline) paper. However, as the Panaganti 2022 paper is published for over 6 months, it is better to compare and discuss the difference choices of uncertainty sets and problem settings. The authors claim that this paper is the first model-free DR RL paper in the literature, which is not true as the Panaganti 2022 is also model-free. To some extent, their paper considers a harder offline problem while this paper considers an online version. \n- I don't think this is an issue when evaluating the contribution of this paper. However, it is better to state the contribution according to the latest literature (indeed, RFQI was mentioned in the experiment) and explicitly comment on the differences. \n- The proposed algorithm 1 only works in the tabular setting. This is fine. The authors introduce a more practical algorithm using DQN type of learning scheme, which is provided in algorithm 4 in the Appendix. However, algorithm 4 looks almost the same as algorithm 1. Is this paper an older version? If yes, please provide the full algorithm 4 description in the future. More importantly, I believe algorithm 4 could potentially have a greater impact on DRRL problems; its connection and modifications with algorithm 1 could be further explained in details in the main paper."
            },
            "questions": {
                "value": "- In my opinion, the combination of online RL with DRO is a bit weird. It makes more sense to study offline RL with DRO. The question is that why would you prefer to learn a robust policy in the testing environment, i.e., this robust policy is not optimal in the testing environment. Is it because there are some technical challenges when applying f-diverngence to offline dataset, e.g., when no e-greedy policy is allowed? \n- Same to the weakness, please compare with Panaganti 2022 in details and explicitly."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1968/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1968/Reviewer_DkEx",
                    "ICLR.cc/2024/Conference/Submission1968/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1968/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698610244741,
        "cdate": 1698610244741,
        "tmdate": 1700585625866,
        "mdate": 1700585625866,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZTmBB0ScsW",
        "forum": "xuKVVYxU5D",
        "replyto": "xuKVVYxU5D",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1968/Reviewer_ogVe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1968/Reviewer_ogVe"
        ],
        "content": {
            "summary": {
                "value": "This work designs model-free algorithms for distributionally robust RL problems in a sample-efficient manner. It proposed a three-time scale algorithm that solves a class of robust RL problems using the uncertainty set constructed by the Cressie-Read family of f-divergence. A theoretical asymptotic guarantee has been provided. Moreover, this work conducted experiments to evaluate the performance and sample efficiency of this work."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. It targets an interesting problem: design a model-free algorithm for distributionally robust RL problems.\n2. A three-timescale algorithm has been proposed that enjoys an asymptotic guarantee and practical sample efficiency.\n3. The introduction of the algorithm is clear and easy to follow."
            },
            "weaknesses": {
                "value": "1. For the experiments in Figure 5. It seems the proposed algorithm DDQR has a very similar performance compared to the existing robust algorithm SR-DQN. It will be helpful to add more discussion about this.\n2. As mentioned in the algorithm, the three-timescale serves as a key role in the algorithm to ensure convergence. So it will be better to introduce what is the three learning rates that the practical algorithm uses. And ablation study using different learning rates will give a message about whether the algorithm is sensitive to the learning rate."
            },
            "questions": {
                "value": "1. As the update of $\\eta$ is independent for each $(s,a)$ pair, will the proposed algorithm works for $s$-rectangular cases?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1968/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1968/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1968/Reviewer_ogVe"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1968/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698710758825,
        "cdate": 1698710758825,
        "tmdate": 1700182479947,
        "mdate": 1700182479947,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ufnfop19MW",
        "forum": "xuKVVYxU5D",
        "replyto": "xuKVVYxU5D",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1968/Reviewer_UFU7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1968/Reviewer_UFU7"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel distributionally robust reinforcement learning (DRRL) algorithm which is model-free and uses single trajectories to update Q function estimates and compute the robust policy. The approach consists of a multi-scale approximation scheme that utilizes existing stochastic approximation results. Asymptotic convergence is proven. Moreover, the method is evaluated experimentally on two tabular environments and a DQN-based implementation is tested on larger (and widely used) control tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Distributionally robust reinforcement learning is a relevant and active area of research. While a few methods have been already proposed, the paper is novel in that it considers a model-free setting, without assuming access to a simulator but only to single trajectory data. I believe this is significantly more practical and makes a good contribution.\n\n- A reasonable amount of experiments illustrate the features of the proposed approach, compared to existing DRRL baselines.\n\n- The paper is nicely written and the results look sound, although i could not verify their proofs."
            },
            "weaknesses": {
                "value": "- Only asymptotic convergence is proven, and no sample-complexity guarantees. Do the authors have a guess on how these may compare with previous work, e.g. (Panaganti et al. 2022)?\n\n- In Section 4.2, a practical implementation of DQR is utilized for the experiments. How is this implemented? I would be nice to discuss such implementation in the main text."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1968/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698856183670,
        "cdate": 1698856183670,
        "tmdate": 1699636127890,
        "mdate": 1699636127890,
        "license": "CC BY 4.0",
        "version": 2
    }
]