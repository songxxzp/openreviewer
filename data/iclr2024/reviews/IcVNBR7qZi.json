[
    {
        "id": "hdzzAzaQ7a",
        "forum": "IcVNBR7qZi",
        "replyto": "IcVNBR7qZi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6841/Reviewer_voez"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6841/Reviewer_voez"
        ],
        "content": {
            "summary": {
                "value": "This manuscript identifies a problem in policy-gradient-based Reinforcement Learning Fine-Tuning (RLFT): when the reward variance is small, the language model (LM) could be susceptible to vanishing gradients.  The authors provided both theoretical and empirical evidence to support their argument, and demonstrated that a supervised fine-tuning \"jumpstart\" could help alleviate the vanishing-gradient issue."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The authors identified an interesting issue (although rather self-explanatory), and advocated for a simple and straightforward solution.\n\n- I admire the organization of this manuscript -- very clean and structured.\n- The authors included a comprehensive set of empirical results, most of which are carefully presented and scrupulously discussed."
            },
            "weaknesses": {
                "value": "- The writing has room for improvement.  The authors tend to make heavy use of long sentences that are difficult to parse.\n- The second paragraph on page 4, I am not sure how the sentence \"the contribution of inputs... is still negligible...\" is logically connected to the next sentence.\n- The experiment setup in Section 4.1 is worth closer scrutiny.  It does not seem a fair comparison to me that RLFT optimizes for a task-specific metric while SFT is performed based on ground-truth completions, which means that RLFT *does not* have access to the ground-truth, whereas SFT *does*.\n- It is worth noting that running RLFT with \"task-specific\" metrics, such as ROUGE, is no longer considered a good practice in the RLHF community.  Specifically for the GRUE benchmarks, I believe that the community has reached a consensus that it is not a suitable playground for RLFT.  I suppose that the authors would agree that the predominant approach in RLFT is to train a reward model using a preference (or ranking) dataset, and apply policy-gradient algorithms based on the reward model.  I could imagine that the same issue identified in this manuscript is also present in such scenarios, but the authors did not provide sufficient evidence.  (I wouldn't place too much weight on this weakness point, because, understandably, the authors might be short on compute resources.)\n- Suppose that we are given a preference dataset: each prompt comes with two responses, one labelled as preferred.  How should we run SFT on such dataset?  Shall we only use the one preferred response for each prompt, or shall we also somehow leverage the other response as a negative sample?  This is subject to debate in the LLM community, and I hope the authors can shed more light on this point."
            },
            "questions": {
                "value": "Please refer to the \"Weaknesses\" section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6841/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698629473570,
        "cdate": 1698629473570,
        "tmdate": 1699636792341,
        "mdate": 1699636792341,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MD54xwadXy",
        "forum": "IcVNBR7qZi",
        "replyto": "IcVNBR7qZi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6841/Reviewer_xsTc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6841/Reviewer_xsTc"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the vanishing gradients in RLHF. Specifically, the authors argue that shrinkage in the standard deviation (std) of the reward would lead to vanishing gradients, leading to sub-optimal performance. The authors first show that norm of the gradient is upper bounded by the std of the reward plus some other factors, suggesting that as std goes to zero, training would stall due to vanishing gradients. On GRUE benchmark, the authors show that this is indeed the case; there are a substantial number of examples that have very small std under the pre-trained model. They show that RFT impacts these examples less than others; SFT, on the other hand, doesn't have this problem as ground truth labels force proper signal propagation. Using conventional heuristics such as larger learning rate, tuning temperature or entropy regularization is not effective in solving this problem. But, a few steps of SFT is shown to be effective in improving RFT performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper investigates a very important problem. It is well written with relevant experiments to support the claims."
            },
            "weaknesses": {
                "value": "Some parts of the paper need clarification.\n\n1. Theorem-1 applies more broadly beyond autoregressive models. The only part that is relevant is the Jacobian which is easily separated from the rest using the triangle inequality in the proof. Take a simple non-autoregressive example with action space A ($|A|=2$), a linear policy $\\theta^T x$ such that $p_\\theta(y=0|x)=\\theta_0 x_0$, $p_\\theta(y=1|x)=\\theta_1 x_1$, and $r(x, y=0)=r(x, y=1)=1$. The standard deviation should be zero and lead to vanishing gradients. But, the gradient is $\\nabla_\\theta(x; \\theta)=\\theta_0 x_0 [x_0, 0]/(\\theta_0x_0) + \\theta_1 x_1 [0, x_1]/(\\theta_1x_1)=[x_0, x_1]$ which doesn't necessarily have zero gradient norm. Can you clarify if the gradient estimation in this example is incorrect or how I should interpret this result?\n\n2. Can you discuss in more detail when small vanishing gradient becomes a problem? Assuming that an LLM is trained with RLHF to convergence, we would expect many of the completions to be of equal quality, i.e., similar rewards. Why would low standard deviation be a problem in this case? Similarly, at convergence, we expect gradient to be close to zero which also is not detrimental to training.\n\n3. While not guaranteed (also not always in practice), ADAM update equations suggest O(1) updates to the weights. For a batch size of 1, would this solve the vanishing gradient problem? What is the maximal batch size where this problem is negligible? Can you also plot some of the statistics from ADAM training, like $\\mu$, $\\nu$, and $u$ (update) norms? What would be the impact of $\\epsilon$ when the significant portion of a batch has vanishing gradients, i.e., would $\\epsilon$ dominate second order moments?\n\n4. Please clarify O-notation in Appendix D.4 w.r.t. concerned variable. Otherwise, it suggests negative value inside O or instant learning when STD goes to 1 in SFT."
            },
            "questions": {
                "value": "1. Can you please clarify the above example in the context of theorem-1?\n\n2. Can you discuss when and why small vanishing gradient is a problem?\n\n3. Can you discuss more on ADAM update and present more results?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6841/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6841/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6841/Reviewer_xsTc"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6841/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698799896252,
        "cdate": 1698799896252,
        "tmdate": 1700608206069,
        "mdate": 1700608206069,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oAEVwY2L2F",
        "forum": "IcVNBR7qZi",
        "replyto": "IcVNBR7qZi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6841/Reviewer_4NxS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6841/Reviewer_4NxS"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on the challenge of vanishing gradients in the context of reinforcement finetuning (RFT) of pretrained language models. The authors identify that the expected gradient for an input tends to vanish when its reward standard deviation is small, regardless of the expected reward's optimality. This phenomenon can hinder the training process. The paper suggests a solution by introducing an initial supervised finetuning (SFT) phase where a small percentage of input samples may be needed."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper pinpoints a specific, nuanced problem in the domain of reinforcement learning finetuning of language models, i.e., vanishing gradients due to small reward standard deviation.\n2. The proposed theory for small standard deviation leading to a small gradient norm is pretty intuitive, but the proof seems novel."
            },
            "weaknesses": {
                "value": "1. It is well-known that policy gradient could be suboptimal when the reward is pretty flat even though it's not the optimal solution. However, specific to the proposed scenario where the standard deviation is small, there could be many naive remedies not mentioned in the paper and it remains questionable whether the proposed problem could be easily solved by them. See  Q1. \n2. If we use a learned reward model, whether SFT initialization is helpful should be highly related to the dataset that the reward model is trained on, and there seems to be no discussion on this part. See Q2. \n3. To solve the suboptimal problem, it seems like designing a better reward function is more important. If there are many flat areas in the reward model, a different initialization might not necessarily help much."
            },
            "questions": {
                "value": "Q1: Consider simple reward scaling with gradient clipping, value function learning in [1], and reward normalization in [2]. Such tricks could help the model to escape the area when it is not exactly flat but the standard deviation is small since the exact value of the reward does not matter, but the relative one matters. I wonder whether such tricks could address the problem.\n\nQ2. It could be easy to imagine that if the reward is trained on a dataset that is significantly better than the pretrained model, all outputs from the pretrained model will be bad and the reward std would be small. If we start fine-tuning with the SFT model, some of the outputs would be better after fine-tuning and the reward std would be larger. However, if the reward model is trained with human evaluation on the output of the pretrained model itself, it would be more likely that the reward std would be larger. I wonder whether the usefulness of SFT initialization is specific to some reward models trained on specific datasets.\n\n[1] DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models\nYing Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, Kimin Lee\n\n[2] Training Diffusion Models with Reinforcement Learning\nKevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, Sergey Levine"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6841/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6841/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6841/Reviewer_4NxS"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6841/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698821199191,
        "cdate": 1698821199191,
        "tmdate": 1699636792102,
        "mdate": 1699636792102,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "S9dFbrCZjX",
        "forum": "IcVNBR7qZi",
        "replyto": "IcVNBR7qZi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6841/Reviewer_PKCD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6841/Reviewer_PKCD"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors identify an issue in the reinforcement finetuning phase of language models. Specifically, they first prove that the gradient norm of the value function is upper bounded by the standard deviation of the reward scores over the input sequences. They then empirically demonstrate on three datasets that small reward standard deviations are prevalent in the GRUE benchmark. Together, they suggest that the RLHF paradigm (i.e. preference reward + PPO) struggles to improve the quality on massive samples with small reward deviation. Finally, they suggest that simply using an initial supervised fine-tuning phase will greatly help mitigate the problem.\n\nOverall, the motivation of this paper is very well justified and supported. In particular, they match the theory and practice quite well in a current fast-evolving paradigm. In addition, it provides a simple and effective solution to the discovered problem. I believe the paper could be potentially impactful in RLHF for language models.\n\nMy concerns are framed as questions below."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The motivation of this paper is very well justified and supported. In particular, they match the theory and practice quite well in a current fast-evolving paradigm. In addition, it provides a simple and effective solution to the discovered problem. I believe the paper could be potentially impactful in RLHF for language models."
            },
            "weaknesses": {
                "value": "My concerns are framed as questions below."
            },
            "questions": {
                "value": "1. Is there a potential explanation of why the reward model generates small standard deviations on the GRUE benchmark?\n2. Could you list all the assumptions used in your proofs explicitly and discuss their realisticity?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6841/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698832556637,
        "cdate": 1698832556637,
        "tmdate": 1699636791966,
        "mdate": 1699636791966,
        "license": "CC BY 4.0",
        "version": 2
    }
]