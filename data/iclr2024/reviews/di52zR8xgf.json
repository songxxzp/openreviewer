[
    {
        "id": "Ek8PcFB3Wy",
        "forum": "di52zR8xgf",
        "replyto": "di52zR8xgf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3626/Reviewer_rcq7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3626/Reviewer_rcq7"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces the following techniques to improve Stable Diffusion and it achieves much better performance according to user study. The trained model is also promised to be released publicly:\n1. A more scalable model architecture: modifications on transformer blocks, more powerful text encoders.\n2. Enable conditioning on image size and cropping.\n3. Training with multiple aspect ratio.\n4. Better Autoencoder.\n5. Additional refinement stage enables more high-resolution details.\n6. Additional finetuning that conditions on pooled text embedding enables multi-modal control during inference."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. The paper is well-written with clear high-level intuitions/ideas, low-level implementation details, as well as visualizations that justify the improvements. It is easy to follow and understand even for the audience that only have very basic knowledge or practice about diffusion models.\n\n2. The problems the authors are trying to tackle are very realistic in practice and the their proposed solutions are simple and effective: \n- in order to fully utilize all the training data with varying image resolutions, they propose to add conditioning on image size during training. \n- in order to solve the commonly-seen cut-out artifacts during generation (as in Fig. 4), they propose to add conditioning on cropping parameters during training.\n- in order to improve high-resolution details, they propose to train another refinement model and use it in a SDEdit way during inference.\n- in order to enable multi-modal control during inference, they propose to finetune only the embedding layer of a text-to-image SDXL such that it can condition on pooled CLIP text embedding as well, which could be naively replaced with image CLIP embedding during inference.\n\n3. The authors promised to open-source the model (last paragraph of Sec. 1), plus all the implementation details in both the main text and supplementary materials including the pseudo code in Fig. 17. I believe this work could be a huge add-on to the image generation community and enable more future works that build upon it, just like Stable Diffusion."
            },
            "weaknesses": {
                "value": "1. Multiple techniques are proposed in this work, therefore a more detailed ablation study would help the audience better understand what\u2019s the influence of each individual component, especially for the two additional conditioning (size and crop). \n2. Particularly, I\u2019m very curious on which method would be more critical to address the aspect ratio issue, the proposed cropping conditioning or data bucketing from NovelAI? According to the description in Sec. 2.3, these two techniques are combined together and there\u2019s no separate evaluation on each of them.\n3. The rightmost examples in Fig. 5 look confusing to me: does (512, 512) mean the whole images are cropped out and there should be nothing inside (the caption says images are from a 512^2 model)? In this case, why does the top example still show basically the same image as the leftmost one while the bottom example shows mainly the background?\n4. The training data of SDXL is not mentioned in the paper. Is it the same as Stable Diffusion, i.e., LAION? If not, would the comparison still be able to justify the effectiveness of proposed techniques since the training data is different?"
            },
            "questions": {
                "value": "It would be great if the authors could respond to the weakness points mentioned above. Thanks!"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3626/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3626/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3626/Reviewer_rcq7"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3626/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698702032089,
        "cdate": 1698702032089,
        "tmdate": 1699636317996,
        "mdate": 1699636317996,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iExMMBJ8ul",
        "forum": "di52zR8xgf",
        "replyto": "di52zR8xgf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3626/Reviewer_gmj5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3626/Reviewer_gmj5"
        ],
        "content": {
            "summary": {
                "value": "This method proposes SDXL,  a latent diffusion model for text-to-image synthesis. Compared to previous versions of Stable Diffusion, SDXL leverages a three times larger UNet backbone, achieved by significantly increasing the number of attention blocks and including a second text encoder. Also, the authors propose multiple novel conditioning schemes and train SDXL on multiple aspect ratios. Competitive performance achieved with SoTA models such as Midjourney."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Very sound elaboration and experiments. The proposed improvement is reasonable and effective.\n2. The method is open sourced."
            },
            "weaknesses": {
                "value": "1. Lacks some comparison with SoTA pixel-space diffusion model such as Simple Diffusion from Google.\n2.  Also, SDXL still relied on two-stage training where a high-quality encoder is required. I wonder whether the two stage can be combined and lead to further performance boost?"
            },
            "questions": {
                "value": "1. As discussed in your future work section, can SD be combined into single-stage training and further boost the performance?\n2. Can similar training pipeline introduced in this paper be applied to 3D diffusion training, and what are the challenges?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No."
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3626/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3626/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3626/Reviewer_gmj5"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3626/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698796730798,
        "cdate": 1698796730798,
        "tmdate": 1699636317915,
        "mdate": 1699636317915,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ObwicDKojq",
        "forum": "di52zR8xgf",
        "replyto": "di52zR8xgf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3626/Reviewer_n7kc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3626/Reviewer_n7kc"
        ],
        "content": {
            "summary": {
                "value": "This paper presents SDXL, an extended version of the existing StableDiffusion model, showcasing its training and capabilities. The authors detail the methodology used in image training, including the resolution, crop, top, and left condition techniques, as well as the innovative multi-aspect resolution training approach. Furthermore, the paper sheds light on the advancements in AutoEncoder technology and the implementation of a Refinement model. The overall generation process is structured in two stages, allowing for high quality images. Also authors introduce diverse multimodal controls. This comprehensive explanation highlights the significant enhancements and versatility that SDXL brings to image generation, setting a new standard in the field."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "This paper not only introduces a variety of innovative training methods but also provides clear intuition and thorough analysis to support them. The novel conditioning techniques and multi-aspect resolution training approaches presented in the manuscript are both groundbreaking and well-substantiated with comprehensive evaluations. The authors have done an excellent job of providing numerical metrics and a wide array of visual comparisons to showcase the effectiveness of their proposed methods. The manuscript is well-written, with a coherent structure that facilitates easy understanding, making it a significant contribution to the field."
            },
            "weaknesses": {
                "value": "The manuscript appears to lack detailed explanations on how the autoencoder (AE) has been improved or advanced. There is also no clear description of the user study conducted, raising questions about its methodology and implementation. Is it conducted in a manner similar to what is described in the supplementary materials? Providing more information and context on these aspects would greatly enhance the comprehensibility and robustness of the paper."
            },
            "questions": {
                "value": "- Refinement training : Could you kindly explain about details of refinement training? This training is only for  200 (discrete) noise scales which is [200, 0] ? \n\n- Could you give me some intuition and the effect of using a combination of language models (CLIP ViT-L & OpenCLIP ViT-bigG)? Is it crucial to use all of them?\n\n- I wonder the author's opinion about using TransFormer architecture to train the DMs. Could you share more information on \"no immediate benefit\"?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No concerns"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3626/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3626/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3626/Reviewer_n7kc"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3626/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698839937853,
        "cdate": 1698839937853,
        "tmdate": 1699636317817,
        "mdate": 1699636317817,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TUdTXVGvGW",
        "forum": "di52zR8xgf",
        "replyto": "di52zR8xgf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3626/Reviewer_pUdr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3626/Reviewer_pUdr"
        ],
        "content": {
            "summary": {
                "value": "This paper presents SDXL, a latent diffusion model for text-to-image synthesis. SDXL uses a larger U-Net compared to previous Stable Diffusion models, and adds a refiner module to improve visual quality of image samples. During SDXL training, the U-Net is conditioned on image size, image cropping information, and receives training data in multiple aspect ratios. A new VAE is trained, with improved reconstruction performance compared to earlier SD versions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. SDXL demonstrates impressive text-to-image generation results. It can serve as a strong base model for a broad range of research and applications in downstream image synthesis tasks.\n2. Some training and architectural choices are backed with convincing ablation experiments (e.g. Table 2). This can offer some valuable insights for future training of image and video generation models.\n3. The refinement model could be an additional contribution, to refine local details of real / generated images."
            },
            "weaknesses": {
                "value": "1. For \"conditioning on cropping\" and \"multi-aspect training\", this paper lacks adequate quantitative experiments to demonstrate their effectiveness.\n2. Minor mistakes:\n(1) In Figure 3, the image size should be (256, 256) instead of (256, 236).\n(2) In Figure 3 and Figure 4, the quotation mark for text prompt is incorrect."
            },
            "questions": {
                "value": "1. There could more explanations on why your current autoencoder provides better reconstruction performance: which factor contributes more, the EMA choice, or larger batch size? \n2. How well does the refinement model work when refining other real images, or images generated by previous SD versions? That is, instead of directly refining latents in VAE latent space, but refining the VAE encoded latents from other real / generated images. \n3. I wonder the relative impact, or the priority of various choices, that is, which plays a bigger role: larger UNet, or each of the conditioning mechanism. I believe these insights are what the research community is insterested in."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3626/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3626/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3626/Reviewer_pUdr"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3626/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699105924301,
        "cdate": 1699105924301,
        "tmdate": 1699636317752,
        "mdate": 1699636317752,
        "license": "CC BY 4.0",
        "version": 2
    }
]