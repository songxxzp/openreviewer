[
    {
        "id": "Qc212gAv2q",
        "forum": "mzxKLZNbrQ",
        "replyto": "mzxKLZNbrQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4734/Reviewer_kj3u"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4734/Reviewer_kj3u"
        ],
        "content": {
            "summary": {
                "value": "- The paper introduces Youku-mPLUG, the largest public Chinese video-language dataset and benchmarks, collected from Youku, a Chinese video-sharing website, with strict criteria of safety, diversity, and quality.\n- The paper also proposes mPLUG-video, a decoder-only video-language model that leverages a frozen large language model and a visual abstractor module to reduce the computation burden and improve the performance.\n- The paper evaluates mPLUG-video and other models on three downstream tasks: video category classification, video captioning, and video-text retrieval. The results show that mPLUG-video achieves good results in video category classification and video captioning, and demonstrates impressive zero-shot video instruction understanding ability."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper introduces a novel and large-scale Chinese video-language dataset and benchmarks, which can facilitate the research and development of video-language models for the Chinese language and culture. The paper also proposes a decoder-only model that leverages a frozen large language model and a visual abstractor module, which is a creative combination of existing ideas that reduces the computation burden and improves the performance.\n- The paper is well-written and organized, with clear figures and tables. The paper provides details and analysis on the proposed method and dataset. \n- The paper explains the problem statement, the motivation, the challenges, and the gap in the existing literature clearly in the abstract and introduction. The paper also describes the dataset collection, annotation, and preprocessing process, and provides some statistics and examples of the data. The paper also explains the model architecture, training, and fine-tuning process, and provides some examples.\n- The paper makes a significant contribution to the field of video-language modeling, especially for the Chinese language and culture. The paper presents a large-scale and diverse dataset that can enable various downstream tasks, such as video category classification, video captioning, video-text retrieval, and video instruction understanding. The paper also presents a state-of-the-art model that can achieve impressive results on these tasks."
            },
            "weaknesses": {
                "value": "- After downloading the dataset, it was found that there were many duplicate clips from the same source and static clips. Does the situation exist where these 400 million video clips come from the same original video? If so, during the filtering process, how is the quality of the selected videos ensured given the lack of quantifiable performance measures, such as CLIP similarity?\n- There is a lack of exploration into the status of text annotation in the dataset. Chinese and Latin languages such as English have significant differences in vocabulary, grammar, and sentence structure. The diversity of the text part of this dataset is not sufficiently demonstrated, and the text quality is slightly lower compared to the WebVid10M dataset. The paper should also compare the dataset with other existing video-language datasets, such as translated HowTo100M, WebVid10M or CNVid-3.5M[1], and discuss the advantages and limitations of the dataset.\n- This paper only explores the zero-shot capability in instruction understanding. Why not further investigate the zero-shot performance in video classification, retrieval, and description?\n- In instruction understanding, does VideoLLaMA also receive Chinese prompts? Has it been trained on Chinese instruction data? Comparing a MLLM trained on English datasets with one training in Chinese is unfair.\n- During data collection, the online model achieved a performance of about 94% in video category classification. However, in Table 4, the model trained by Youku-mPLUG actually performs worse than the unfiltered online model.\n\n----\n\nReference:\n[1] https://github.com/CNVid/CNVid-3.5M"
            },
            "questions": {
                "value": "see weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4734/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698802395655,
        "cdate": 1698802395655,
        "tmdate": 1699636455466,
        "mdate": 1699636455466,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "g1ZNJaArIX",
        "forum": "mzxKLZNbrQ",
        "replyto": "mzxKLZNbrQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4734/Reviewer_bL3j"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4734/Reviewer_bL3j"
        ],
        "content": {
            "summary": {
                "value": "This paper argues that the development and application of Chinese VLP and multimodal LLM are lagging behind the English counterpart, due to the lack of a large-scale Chinese video-language dataset. Thus, they propose a new dataset Youku-mPLUG, which consists of 10 million Chinese video-text pairs for pertaining, and a dataset with 0.3 million videos for downstream benchmarks, including video-text retrieval, video captioning, and video category classification. Meanwhile, they investigate popular video-language models (e.g., ALPRO, mPLUG-2), and the new proposed model mPLUG-video. The model mPLUG-video consists of a trainable video encoder, a visual abstractor module, and a frozen pre-trained LLM decoder. Experiments show that models pre-trained on Youku-mPLUG gain on multiple tasks. Furthermore, by building on top of Bloomz, mPLUG-video can achieve impressive zero-shot performance with very few trainable parameter."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ This paper proposes a large-scale dataset with 10 million Chinese video-text pairs for pertaining, and a dataset with 0.3 million videos for downstream benchmarks. Several off-the-shelf techniques have been used to ensure the high-quality of training videos."
            },
            "weaknesses": {
                "value": "+ The novelty of the new model mPLUG-video is limited. The proposed three modules, and partially efficient tuning are all well studied techniques in this area.\n\n+ The improvements brought by the proposed mPLUG-video are limited.\n\n+ One of the key contributions in this paper is the proposed new dataset. It would be better to demonstrate the high quality of the newly collected data. Based on the example shown in Figure 9, the text annotations look very noisy."
            },
            "questions": {
                "value": "In the first paragraph of the introduction section, the authors argue that existing methods of translating English to Chinese suffer intrinsic linguistic and cultural gaps. Could you give more explicit examples to show the harmfulness of these methods?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4734/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698839190544,
        "cdate": 1698839190544,
        "tmdate": 1699636455398,
        "mdate": 1699636455398,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "F2AGmEZblC",
        "forum": "mzxKLZNbrQ",
        "replyto": "mzxKLZNbrQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4734/Reviewer_umKn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4734/Reviewer_umKn"
        ],
        "content": {
            "summary": {
                "value": "The authors introduce Youku-mPLUG, the largest high-quality video-language dataset in Chinese. And present a human-annotated benchmark encompassing three downstream tasks: Video-Text Retrieval, Video Captioning and Video Classification. The authors also propose modularized mPLUG-video, a decoder-only model that is pre-trained on Youku-mPLUG, which gain a state-of-the-art result on theses benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This paper is going to release a 10 million Chinese video-language pretraining dataset and provide benchmarks on different model architectures, which is in great demand by the field.\n    \n- This dataset seems to be of high quality (hire well-educated people to double check the data) and well-curated (filtered 10 million Chinese video-text pairs out of 400 million raw videos).\n    \n- Propose a modularized decoder-only mPLUG-video model and achieves state-of-the-art results on these benchmarks."
            },
            "weaknesses": {
                "value": "- The experiments are not very comprehensive. The selected baseline models in different downstream tasks is limited, two were selected only.\n    \n- No details about the selection of the original 400 million videos are provided."
            },
            "questions": {
                "value": "This paper mentions that currently existing large-scale Chineses video-language datasets are not publicly accessible. This also demonstrates that not only the collection and curation of large datasets are challenging, but the release process is also difficult. Could the authors provide their plans to prove that you can genuinely release this dataset and make it easily accessible to researchers, thus making a real contribution to the research community?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety",
                    "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4734/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698839973945,
        "cdate": 1698839973945,
        "tmdate": 1699636455319,
        "mdate": 1699636455319,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EevoFiw57w",
        "forum": "mzxKLZNbrQ",
        "replyto": "mzxKLZNbrQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4734/Reviewer_HLpF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4734/Reviewer_HLpF"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes Youku-mPLUG, a high-quality video-language dataset in Chinese, along with a human-annotated benchmark comprising three downstream tasks. The experiments on downstream tasks (i.e. Video-Text Retrieval, Video Captioning, and Video Category Classification) evaluate the video language comprehension and modeling abilities of models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tYouku-mPLUG is currently the largest Chinese video-language dataset.\n2.\tThe exploration of different architectures (like encoder-only, encoder-decoder, decoder-only) is well done."
            },
            "weaknesses": {
                "value": "1.\tThe zero-shot experiment is too simple. The authors should evaluate on video-text retrieval task using more models and other pre-train datasets quantitatively.\n2.\tThe results in Table 5 are not convincing enough. The authors only compare one publicly available dataset VATEX and do not show a gap with current state-of-the-art results. \n3.\tIncorrect paragraph spacing in the second and third paragraphs in \u201c2 RELATED WORD\u201d section."
            },
            "questions": {
                "value": "Data augmentation will almost certainly bring performance improvements to the model. Therefore, how to prove that Youku-mPLUG is superior to other dataset like CNVid-3.5M?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4734/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699189113103,
        "cdate": 1699189113103,
        "tmdate": 1699636455245,
        "mdate": 1699636455245,
        "license": "CC BY 4.0",
        "version": 2
    }
]