[
    {
        "id": "91J6Rk7LjJ",
        "forum": "sq5gkjC9jv",
        "replyto": "sq5gkjC9jv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5964/Reviewer_XYHA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5964/Reviewer_XYHA"
        ],
        "content": {
            "summary": {
                "value": "This article is about topology of certain sublevel sets of functions defined by fully connected ReLU neural networks.  Asymptotic bounds for Betti numbers of the sublevel sets of such functions are established."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Paper is situated within the framework  trying to approach expressivity of neural networks via topology. A set of interesting mathematical results concerning bounds for Betti numbers is proven"
            },
            "weaknesses": {
                "value": "Weaknesses:\n\n1)In deep learning  both the positive and negative data points typically lie near very low dimensional surfaces, so in general, there is no relation between Betti numbers of the sublevel set of the function defining a decision boundary and the Betti numbers of the support of the distributions from which the data points are sampled. The Betti numbers of the support of distribution can be arbitrary big, while at the same time the zeroth Betti number of the sublevel set of the function defining a decision boundary can be  equal to one. Therefore the practical meaning of the paper's results is limited.\n\n\n2)Lower bound is obtained only for some specific weights of neural network. However, given a neural network architecture there are always weights that produce constant function and thus have strictly zero Betti numbers, so the meaning of  paper's \"lower bound\" term is not quite clear. \n\n3)Also it is not clear whether the constructed network weights can be  found via regular optimization algorithms, \n\n4)The calculation of Betti numbers is difficult so it  also undermines practical implications of the work.\n\n5)The claim of the exponential gap is somewhat unclear in the paper.\n\n6)The upper bound proof lacks some details, only about half a page is devoted to upper bounds, the paper mostly concerned with lower bounds.\n\n7)The lower bounds found by the authors are similar to previous ones that have appeared in the literature, e.g. in  Bianchini and Scarselli (2014). For example the principal zeroth Betti number result is an extension of the loc.cit to  ReLU activations. \n\nMinor remarks:\n\nGrammar errors: section 1.2.1 : ares -> are\n\nNotations are somewhat confusing M_a and M_b are topological spaces, but M is an integer in section 2, lemma 4  etc. \n\nWhy does this graph, consisting of two points, represents the functions that folds the interval on Figure 1, there seems to be a problem with this figure, are some lines missing ?"
            },
            "questions": {
                "value": "Can authors provide an example from real world data when their bounds have practical implications ? \n\nThe paper really needs to show concrete real world examples  with practical meaning of the paper's results."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5964/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698795170261,
        "cdate": 1698795170261,
        "tmdate": 1699636636827,
        "mdate": 1699636636827,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jBYJUKzrMb",
        "forum": "sq5gkjC9jv",
        "replyto": "sq5gkjC9jv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5964/Reviewer_UFRF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5964/Reviewer_UFRF"
        ],
        "content": {
            "summary": {
                "value": "The paper studies ReLU networks from the perspective of their topological expressivity. The measure used here is that of Betti numbers that is a suitable measure for characterizing how complicated the topological properties of a network are.\n\nThe main contribution of the paper is to derive several upper and lower bounds for the Betti numbers depending on the depth and width of the ReLU network, by using clever constructions of functions.\n\nThe main takeaway is that Betti numbers depend on the depth, and can significantly grow with the depth. If the depth is unbounded, Betti numbers increase exponentially with the size of the network. In contrast, if the networks is shallow then its Betti numbers do not grow as fast. This is interesting as it showcases that a possible bottleneck for effective data representation is the depth.\n\nThe constructions in the paper are heavily inspired by previous ideas used in Montufar et al. where the goal was to characterize another measure of ReLU neural network complexity, that of linear regions. We know that the number of linear regions can exponentially grow with the depth, but not the width. The paper under review essentially sets out to formally establish the connection and proposes clever constructions to transfer the results to the complexity measure of Betti numbers."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+well-motivated theoretical question about the expressivity\n\n+the question has been empirically observed and the paper develops interesting theory to address this in a simple binary classification setting\n\n+in my opinion, the paper proves a very elegant characterization for expressivity and interesting dependence on Betti numbers for the depth and width\n\n+potential interesting connections to dynamical systems (see comments below)"
            },
            "weaknesses": {
                "value": "Overall, the paper is strong and there are not major weaknesses in my opinion. One thing I believe should pointed out though has to do with the novelty of the final conclusion of the paper.\n\n- The key takeaway of the paper is that depth is more important than width. The paper has an elegant way of proving this via the Betti numbers. However, the reviewer just wants to point out that similar depth-width tradeoffs were known, albeit using different techniques and different connections. So in some sense we already knew that depth is exponentially better than width. For example:\n\nThe authors cite Telgarsky's works who used a basic triangle construction and as a measure of complexity he used the number of linear regions. Similarly, Montufar et al. had the number of linear regions as a way to show that depth is much more important. \n\nThere is also a generalization of the works of Telgarsky that use connections to dynamical systems (Li-Yorke chaos, periodic orbits) and the notion of *topological* entropy [3]. See [1], [2], [3]. Papers [1] and [2] give lower bound constructions using more general functions that than Telgarsky's triangle and [3] provides characterization using topological entropy.\n\n[1] Depth-WidthTrade-offs for ReLU Networks via Sharkovsky\u2019s Theorem\n[2] Better depth-width trade-offs for neural networks through the lens of dynamical systems\n[3] Expressivity of Neural Networks via Chaotic Itineraries beyond Sharkovsky\u2019sTheorem\n\nIt would be interesting to see if the characterization of the Betti numbers for the depth/width tradeoffs can actually follow in certain cases because of the connection to Li-Yorke chaos and periodic points."
            },
            "questions": {
                "value": "Q: Related to the weakness comments above, do the authors see any connection between their construction and the notion of periodic points/topological entropy in dynamical systemts? At least their examples in Fig. 3,4,5,6 for the binary classification problem resembles both Telgarsky's triangle characterization, and also the more general result proved in [1] Depth-WidthTrade-offs for ReLU Networks via Sharkovsky\u2019s Theorem.\n\nQ: For 1-dimensional neural networks (i.e. input is just a real number) similar to the ones that Telgarsky used, do your results imply the exact separation that Telgarsky proved? Is there a sense why your results are stronger in this special case? I believe this is the simplest case where we can understand whether or not the connection to dynamical systems is valid."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "-"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5964/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698831198530,
        "cdate": 1698831198530,
        "tmdate": 1699636636713,
        "mdate": 1699636636713,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BSypZxvjln",
        "forum": "sq5gkjC9jv",
        "replyto": "sq5gkjC9jv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5964/Reviewer_o7g5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5964/Reviewer_o7g5"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the expressivity of ReLU neural networks in the setting of a binary classification from a topological perspective. \nThe authors prove new lower and upper bounds for topological expressivity of ReLU networks. Here, the topological expressivity is the sum of Betti numbers of input subspaces, which network separates. Such expressivity grows polynomially with the width (for fixed depth) and exponentially with the depth (for fixed width). Most of the paper is dedicated to obtaining the lower bound by explicitly constructing weights of a network."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Research on the intersection of topology and deep learning is active right now.\nRegarding the expressivity analysis and proving UAT-like theorems, I am not an expert in this area and I can't evaluate originality and impact of the manuscript.\nI haven't thoroughly checked math, but I don't see evident errors. \nOverall, the paper is well written and language is fine."
            },
            "weaknesses": {
                "value": "1. I don't understand the notation $\\beta_0(F) \\in \\Omega(M^d \\cdot n_L)$. Is it the same as $\\beta_0(F) = \\Omega(M^d \\cdot n_L)$ ? (that is, $C_1 M^d \\cdot n_L \\le \\beta_0(F) \\le C_2 M^d \\cdot n_L$\n2. The most of the paper is dedicated to the proof of the **existence** of a network with a given topological expressivity.\nBut in deep learning we are interested in a practical algorithm for finding such a network.\nThe manuscript will benefit from computational experiments. You can use simple synthetic datasets with known Betti numbers (like in Naitzat et al. (2020)) and estimate depth/width of a network which is able to classify it with accuracy > 0.95, for example. \n3. The manuscript is very long (30 pages), the Appendix is dedicated to proofs. Maybe some journal will be a better destination for such a manuscript."
            },
            "questions": {
                "value": "1. You explicitly construct a network with a given topological expressivity, but are this network's weights reachable by gradient optimization?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5964/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5964/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5964/Reviewer_o7g5"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5964/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698934647546,
        "cdate": 1698934647546,
        "tmdate": 1699775838567,
        "mdate": 1699775838567,
        "license": "CC BY 4.0",
        "version": 2
    }
]