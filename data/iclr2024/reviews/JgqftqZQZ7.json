[
    {
        "id": "IuyTFVkuyl",
        "forum": "JgqftqZQZ7",
        "replyto": "JgqftqZQZ7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1423/Reviewer_Hpug"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1423/Reviewer_Hpug"
        ],
        "content": {
            "summary": {
                "value": "The paper investigates the text-to-video editing task. To improve the temporal consistency, they introduce dense spatio-temporal attention and flow-guided attention to acquire information from the whole video. The proposed method achieves good performance on the test videos."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed flow-guided attention is novel and effective.\n\n2. The proposed flow-guided attention can be applied to other base models to further improve the temporal consistency.\n\n3. Extensive experiments and ablation studies demonstrate the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1. The comparison with previous works is very limited. For the video editing task, multiple images should be shown for qualitative comparison in the main paper (Figure 5), which is important to verify the temporal consistency. Furthermore, even in the supplementary material, only one example is compared with previous works, which is not convincing. I compared the ``wolf\u2019\u2019 example with the TokenFlow example on its website. The results of this paper are not good to me.\n\n2. From my perspective, the text information for editing is only acquired by cross attention with editing prompts. Other editing techniques like prompt-to-prompt are not used. This might cause the inaccurate editing of the background. For example, in the first example of Figure 5, the grass also turns yellow, while TokenFlow and FateZero can better keep the background.\n\n3. DSTA conducts cross attention across all 32 frames, which takes many computational resources. It is better to compare the computational cost and inference time with other methods.\n\n4. There are also some works based on optical flow trajectory for video generation [ref-1,ref-2,ref-3], which should also be discussed in the related work.\n\n[ref-1] Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory.\n\n[ref-2] Generative image dynamics.\n\n[ref-3] Motion-Conditioned Diffusion Model for Controllable Video Synthesis."
            },
            "questions": {
                "value": "The paper is good, but I still have some concerns as in the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1423/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1423/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1423/Reviewer_Hpug"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1423/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698636639403,
        "cdate": 1698636639403,
        "tmdate": 1700963842371,
        "mdate": 1700963842371,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FHixBwB65R",
        "forum": "JgqftqZQZ7",
        "replyto": "JgqftqZQZ7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1423/Reviewer_F5Lm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1423/Reviewer_F5Lm"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes text-guided video editing systems that considers optical flow to preserve temporal consitency\nIn detail the temporal attention is guided by the paths estimated from optical flow."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "[+] The idea makes sense that involves optical flow into the diffusion model for holding temporal consistency \n[+] Performances are enhanced compared to previous editing systems"
            },
            "weaknesses": {
                "value": "[-] Optical flow can be effective when a single objective appears. However, we can easily come up with other cases including occlusion, objects' appearing or disappearing, or else. Therefore the method seems sensitive to the input video, which can ruin the attention or even worse than previous temporal attention methods. Can you explain why the method should be better than previous temporal attention?\n\n[-] The performances are better than many previous works. What are the samples that this method validates? I am quite curious about the videos that they evaluated.\n\n[-] Are there any qualitative or quantitative results about the trajectory patches in the aforementioned cases in question [1]? I want to see if the trajectory is truely following the flow of objectives in the video."
            },
            "questions": {
                "value": "My questions are above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1423/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698654275571,
        "cdate": 1698654275571,
        "tmdate": 1699636070763,
        "mdate": 1699636070763,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "61Mo1KlEEa",
        "forum": "JgqftqZQZ7",
        "replyto": "JgqftqZQZ7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1423/Reviewer_1FeZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1423/Reviewer_1FeZ"
        ],
        "content": {
            "summary": {
                "value": "To improve the visual consistency for text-to-video editing\uff0c FLATTEN is proposed to enforce the patches on the same flow path across different frames to attend to each other in the attention module. Experiment results on existing text-to-video editing benchmarks show that the proposed method achieves the new state-of-the-art performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed Flow-guided attention is intuitive and makes sense."
            },
            "weaknesses": {
                "value": "1. The method is only suitable for scenarios where every pixel in the original video aligns spatially with the generated video. For misaligned areas, the optical flow trajectory of the original video is not appliable for the motion of the generated video, leading to incorrect key and query identifications. For instance, in the example of transforming a cat to a tiger in Figure 1, the tiger's face is larger than the cat and thus there exist pixels that belong to the tiger's face and belong to the background in the cat example. For the original video with the cat, these pixels belong to the background with an optical flow near zero. However, for the tiger, it's part of the face and should rotate with the head, requiring an optical flow describing a leftward movement. This sets too high a requirement for editing scenarios.\n\n2. If optical flow tracking is accurate enough, why not simply select a keyframe and then directly copy pixels following the same optical flow path to other frames? This approach seems more accurate than aligning through attention. For example, bilinear interpolation combined with optical flow is often used to predict the next frame in videos.\n\n3. In the provided MP4, there is only one visual comparison, which is too limited. It's suggested not to cherry-pick comparisons so that the effectiveness of the method can be judged intuitively."
            },
            "questions": {
                "value": "See Weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1423/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1423/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1423/Reviewer_1FeZ"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1423/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698737744023,
        "cdate": 1698737744023,
        "tmdate": 1699636070670,
        "mdate": 1699636070670,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kqi1UYWfGu",
        "forum": "JgqftqZQZ7",
        "replyto": "JgqftqZQZ7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1423/Reviewer_UKpY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1423/Reviewer_UKpY"
        ],
        "content": {
            "summary": {
                "value": "Summary: The paper focuses on text guided video editing. Previous methods to tackle this problem extend the text-to-image U-net to the temporal dimension to implement spatiotemporal attention where patches from different frames attend to one another. The paper argues that such methods introduce irrelevant information, since they allow all patches in the video to attend to one another where in fact many of these spatiotemporal patch to spatiotemporal patch connections might be irrelevant. To address this problem, the paper suggests using optical flow to guide the attention. Specifically, a pre-trained optical flow network is used to estimate the flow field and tracks of patches along flow trajectories are aggregated to enforce only patches on the same trajectory to attend to one another in a second step of MHSA. This results in more visual-consistent videos as the paper demonstrate both qualitatively and qualitatively \n\nMethod: First, the \"standard\" the text to image U-net architecture is inflated to account for the temporal dimension and the image patch spatial self attention mechanism is replaced with spatiotemporal self-attention with all patches in the video used as tokens for Q,K,V. Secondly, a pre-trained optical flow network is employed to compute the flow field along the frames of the video. Tracks of patches (in the latent space) are aggregated using the downsampled flow field. Next, self attention is performed between patch-embeddings on the same track. Specifically, the queries are taken from the original dense spatiotemporal MHSA, but for every query associated with a specific patch - the keys and values in MHSA are only the ones which are associated with patches on the same track. Note that this method does not require re-training as it only refines the existing embedded spatiotemporal patch embedded tokens with additional information by applying MHSA again but with restrictions on which patches can attend to one another (where this \"restriction\" is derived from the flow field).\n\nExperiments: the paper compares the proposed method against 5 publicly available text-to-video editing methods on two standard benchmarks. The proposed method performs favourably both in terms of visual quality/alignment metrics  and visual consistency metrics. The paper also presents quantitative results as well as a user-study demonstrating the effectiveness of the method, particularly in the aspect of visual-consistency when motion is introduced."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed method is sound and original. The framework is very simple, does not require further training and can be easily plugged to various existing architectures. The paper is well written and the effectiveness of the method is demonstrated relatively well."
            },
            "weaknesses": {
                "value": "In my opinion, a drawback of the method is the heavy reliance on pre-computed flow field using a pre-trained network that is used as a black box. Thus, errors in this step can negatively affect the results of the proposed pipeline. However, the paper does not address this issue and there are no results to measure the robustness of the method. See question in the section below."
            },
            "questions": {
                "value": "As I understand, the method is designed to improve visual consistency, particularly with respect to motion. The method relies on optical flow to \"enhance\" the embedded tokens with motion information derived from the flow field. As the pipeline relies on flow field computation and errors introduce in that step may affect the results. Something that is missing in the paper in my opinion is some discussion/experiments/results on how robust is the method to mistakes in the flow field computation. Specifically: \n\n1. How well the proposed method can handle large motion (large displacement in the flow field) or abrupt motion? are there any examples you can provide? \n2. How well the proposed method can handle videos in which both global motion (camera movement) and local motion (object movement) are present? are there any examples you can provide? \n3. Are there any situations where the method can do \"more harm than good\"? I mean, cases where the errors in the flow-field computation can cause the method to produce worse results than the baseline? how often do they occur? \n4. Are there any examples that you can provide in which the flow field is far from accurate? In those cases, are the results worse than the baseline, meaning the method did \"more harm than good\"?\n5. How does the results change with respect to accuracy of the flow field? For example, by taking a specific video and flow field results from several models where some perform dramatically worse than others or by gradually corrupting the flow field and measuring the affect on the results?\n\nI find that the qualitative results provided the supplemental video are extremely helpful (particularly the \"racing trucks\" example in which the results of other methods are provided). I would be grateful if the authors would be able to provide more examples that address the questions above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1423/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1423/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1423/Reviewer_UKpY"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1423/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698781211911,
        "cdate": 1698781211911,
        "tmdate": 1700937387161,
        "mdate": 1700937387161,
        "license": "CC BY 4.0",
        "version": 2
    }
]