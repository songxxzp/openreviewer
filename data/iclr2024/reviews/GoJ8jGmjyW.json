[
    {
        "id": "cQU8n9AiQK",
        "forum": "GoJ8jGmjyW",
        "replyto": "GoJ8jGmjyW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3817/Reviewer_hb9c"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3817/Reviewer_hb9c"
        ],
        "content": {
            "summary": {
                "value": "Freely available web videos provide a rich source of multimodal text-video data. However, training on such data presents challenges, primarily due to the limited guidance offered by video subtitles for text-visual learning. \nIn this paper, it tackle this issue by harnessing the capabilities of large-language models (LLMs). The paper introduces a novel method called HowToCaption, which entails instructing an LLM to generate detailed video captions based on automatic speech recognition (ASR) subtitles. \nTo assess the effectiveness of the proposed HowToCaption method, the authors have curated a comprehensive HowToCaption dataset.This research showcases the potential of large-language models in creating annotation-free, large-scale text-video datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- They propose  a novel dataset HowToCaption with high-quality human-style textual descriptions.\n\n- The paper is easy to understand.\n\n- This paper proposes a HowToCaption method to efficiently leverages recent advances in LLMs and generates high-quality video captions at scale without any human supervision."
            },
            "weaknesses": {
                "value": "- Novelty is insufficient.  This paper propose a approach, HowToCaption, prompting an LLM to create detailed video captions based on ASR subtitles.  However, similar methods have been used widely in dataset industry, including Tencent, OpenAI and so on, which could improve the worker's efficiency. Additionally, the process of method is so engineering and not suitable for ICLR.\n\n- Experiment  is insufficient.  Firstly, in ablation study, the authors need to evaluate different speech recognition methods. Also they don't compare the influences of  captions length.  Secondly, Other downstream tasks such as VQA and Video Caption should be performed to prove the efficiency of dataset. the paper only test it on text-video retrieval which is far from enough. \n\n- Analysis is not enough. The setting of MSVD is not describled clearly.  the difference between HowTo100M and other large scale caption dataset such as WebVid10M  is also not describled. Also, this paper lacks some quantitative analysis  of HowToCaption, such as the distribution of caption length and the . Some settings are not be evaluate such as WebVid2M + CC3M, which is used widely in pre-trained field.\n\n- Writing. In the process of reading, the same name of dataset and method sometimes could make reader confused. Apart from that, there are some grammar errors in paper, such as in third paragraph in 3.3, \"first\" is wrong."
            },
            "questions": {
                "value": "the same as weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3817/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3817/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3817/Reviewer_hb9c"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3817/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698115864575,
        "cdate": 1698115864575,
        "tmdate": 1699636339241,
        "mdate": 1699636339241,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "N0GGDlPhtu",
        "forum": "GoJ8jGmjyW",
        "replyto": "GoJ8jGmjyW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3817/Reviewer_qBgz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3817/Reviewer_qBgz"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a framework to improve and collect text descriptions for videos by leveraging the powerful LLMs introduced recently. It designs a prompting mechanism that asks the LLM to rephrase subtitles extracted by automatic speech recognition (ASR) from the audio. To ensure good visual-text alignment, the framework (1) prompts the LLM to output timestamps for the generated sentences and (2) utilizes a vision-language model to filter and realign sentences that are not well aligned. The ablation studies confirm the effectiveness of the prompting mechanism. Furthermore, the experiments highlight (1) the importance of filtering and realignment, and (2) the vision-language model trained on the newly collected dataset outperforms baselines in various benchmarks on the text-to-video(+audio) retrieval task."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+) Combining LLM assures the production of high-quality descriptive sentences for videos. This is an intriguing and novel approach. Moreover, prompting the LLM to assist in text-video alignment is compelling. One could speculate that the commonsense knowledge or reasoning ability inherent in LLMs could greatly enhance alignment results. The qualitative samples depicted in Figure 2 appear impressive. The authors intend to make the dataset and code publicly available.\n\n+) The ablation study in Table 1 clearly demonstrates the design choice behind the proposed prompting mechanism. Yet, it is somewhat surprising that, as per Table 2, long context information only yields marginal improvements in downstream tasks. The analysis of the effects of filtering and alignment also highlights the necessity for robust text-clip alignment annotations.\n\n+) The zero-shot text-to-video retrieval results depict improvements from training BLIP on the newly assembled dataset. Although the progression seems marginal compared to WebVid2M data, Table 5 showcases that BLIP training on the proposed dataset achieves state-of-the-art performance in the text-to-video retrieval task.\n\n+) The zero-shot text-video+audio retrieval outcomes highlight the strength of the proposed dataset, as models trained on it seemingly perform better."
            },
            "weaknesses": {
                "value": "-) A primary concern is that the authors solely validate the utility of the proposed dataset for the text-video(+audio) retrieval task. Presumably, the newly collected data could be an great resource for training foundational video-text representations, video captioning models, or video question-answering systems. It's somewhat disappointing to only see results related to retrieval tasks, especially considering BLIP's capability in visual captioning and visual question-answering.\n\n-) While the authors show that filtering/alignment augments the final retrieval performance, a direct quantification of the assembled dataset would be valuable. For instance, what is the alignment accuracy when implementing the proposed filter/alignment technique? This would help subsequent users understand the noise level they might encounter when using the dataset.\n\n-) Merely a few qualitative examples are presented in Figures 1 and 2, with no instances of failure cases. It would be beneficial if the authors included a broader range of generated sentences in the appendix, encompassing both successful and subpar examples. Additionally, given the notorious propensity of LLMs to fabricate or mislead, this work lacks both qualitative examples and quantitative analysis to assess such issues within the proposed dataset.\n\n-) The work lacks qualitative results for the text-video(+audio) retrieval tasks. Specifically, for the text-video+audio task, insight into whether the proposed rephrasing method can effectively circumvent issues related to overly relying on ASR-generated information would be helpful."
            },
            "questions": {
                "value": "o) Can the authors present more qualitative examples from the newly introduced dataset, including both exemplary and flawed examples?\n\no) Could the authors share some qualitative results for the text-video(+audio) retrieval tasks?\n\no) Would the authors clarify why evaluations were restricted to retrieval tasks using BLIP? Is it feasible to assess the model's performance in visual captioning or question-answering scenarios?\n\no) The font size in Figure 1 appears too small. Could the authors enlarge it for easier readability?\n\no) Considering the generality of the proposed framework, which should be applicable to any video-text dataset, have the authors considered employing these techniques across all publicly available benchmarks to train a large-scale video-language model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3817/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698708464935,
        "cdate": 1698708464935,
        "tmdate": 1699636339163,
        "mdate": 1699636339163,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "agrHCaXzPK",
        "forum": "GoJ8jGmjyW",
        "replyto": "GoJ8jGmjyW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3817/Reviewer_d1GH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3817/Reviewer_d1GH"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a novel way to construct large-scale video datasets, i.e., prompting an LLM to create both natural and rich video descriptions (based on ASR narrations). In this way, this paper contributes a large-scale video dataset, HowToCaption."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well-written and easy to follow. Different sections are well organized to present the proposed method and the experimental results.\n\n2. Existing large-scale video datasets typically don't include detailed annotations (in the form of dense captioning with both timestamps and captions), since the time and labor costs of annotating temporal segments would be rather expensive. This paper presents a possible solution to this issue by automating the annotation process with pre-trained LLMs."
            },
            "weaknesses": {
                "value": "1. This work is more of a prompt engineering than a research paper, since the core components, i.e., the pre-trained Large Language Model and video-language encoder are both borrowed from existing literature. In essence, the technical contribution is a little bit weak.\n\n2. Considering the generated captions include fine-grained timestamp annotations, it would be better to evaluate the proposed method on temporal localization tasks like moment retrieval, instead of text-to-video retrieval only that doesn't require fine-grained modeling."
            },
            "questions": {
                "value": "It would be better to evaluate the proposed dataset on more challenging tasks, e.g., dense video captioning or moment retrieval."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3817/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3817/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3817/Reviewer_d1GH"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3817/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698719818500,
        "cdate": 1698719818500,
        "tmdate": 1699636339039,
        "mdate": 1699636339039,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WajScU5T0N",
        "forum": "GoJ8jGmjyW",
        "replyto": "GoJ8jGmjyW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3817/Reviewer_MMYt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3817/Reviewer_MMYt"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new dataset HowToCaption by prompting LLMs to modify the existing ASR subtitles in HowTo100M dataset in a human readable form. The newly created dataset is then pretrained on a model and compared against existing datasets and models on zero-shot video retrieval and text-video + audio retrieval."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "**Clarity:** \n- The paper is well written and easy to follow.\n\n**Significance:** \n- This paper proposes a new method to reduce the mis-alignment and noise in video-text datasets without the need for human supervision. It can provide a framework for future works to create large datasets without human supervision.\n- Results on text-video + audio are promising. This is generally an ignored area of research due to lack of quality datasets. Training on the proposed dataset show its effectiveness."
            },
            "weaknesses": {
                "value": "**Unclear advantages of using this dataset on video retrieval task:**\n- In Table-5 the authors present a comparison with SOTA models. However, a lot of models are missing in the Table. For example LAVENDER shows 37.8 points and 46.3 points on MSRVTT and MSVD datasets respectively which is more than HowToCaption while trained much smaller data (5.5M). This begs the question why does the community need to use the proposed dataset of 25M as opposed to Vid2.5M + CC3M. \n\n**Terminology usage:** \n- In section 3.1, the authors mention that the aim is to \"generate\" captions. However, the term \"generate\" might be mis-leading as the LLM merely \"rephrases\" semantically the ASR subtitle within a time-step in a more human-alike sentence and doesn't add any new details. \n- In section 3.3, the authors hint at \"fine-grained\" captions, I am not fully convinced that LLMs always produces better fine-grained captions. Sometimes, they add new unrelated details to the ASR subtitles. Since no quantifiable measure is provided in the paper, I would suggest the authors avoid this terminology."
            },
            "questions": {
                "value": "1. Are the ground truth time-stamps of ASR subtitles provided in the HowTo100M dataset? If not how are they determined?\n\n2. In Section 4 the authors mention that dual-encoder of the BLIP is used as \"T-V\" model. Is it initialized with pre-trained weights of BLIP? How is it different from frozen-in-time architecture?\n\n3. In Table-3, the authors present metrics of R-10 and MR for comparison which is rather unusual in video retrieval. Is there any specific reason for this? Is it possible to provide R-1 and R-5 scores?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3817/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3817/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3817/Reviewer_MMYt"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3817/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698984757993,
        "cdate": 1698984757993,
        "tmdate": 1699636338976,
        "mdate": 1699636338976,
        "license": "CC BY 4.0",
        "version": 2
    }
]