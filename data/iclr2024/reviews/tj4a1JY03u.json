[
    {
        "id": "j3RtMTUVAR",
        "forum": "tj4a1JY03u",
        "replyto": "tj4a1JY03u",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6737/Reviewer_bdXG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6737/Reviewer_bdXG"
        ],
        "content": {
            "summary": {
                "value": "This work improves the collection pipeline of instruction-following data, allowing for the collection of a large-scale dataset of text-rich images. Leveraging GPT-4, this work further constructs an instruction tuning dataset consisting of 422K noisy pretraining data and 16K conversations and validates it on the recent work LLAVA. The results on multiple text-based VQA datasets show that this dataset improves the performance of LLAVA in text understanding. Case analysis also demonstrates that LLAVAR has stronger image-text understanding abilities than LLAVA."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well organized and easy to follow.\n2. The improved data collection pipeline overcomes the limitations of the existing dataset, which lacks text-rich images and relevant instruction-tuning annotations.\n3. The explanation of the data collection process is detailed, and the relevant experimental analysis provides support for investigating how to enhance visual Large Language Models' understanding of text-rich visual content."
            },
            "weaknesses": {
                "value": "1. The contribution of this paper appears to be limited. The proposed data collection pipeline in this paper is based on the one used in LLAVA, but with incremental improvements. Similarly, the model architecture in this paper also follows LLAVA without a specific design for text-rich scenarios.\n2. The dataset introduced in this paper only brings limited improvement. In Table 2, LLAVAR achieves comparable performance with mPLUG-Owl, a model that was not trained on a text-rich dataset. The paper also does not provide a detailed comparison with other state-of-the-art models in the field of text-rich image understanding, which would help to better understand the relative performance of the proposed method.\n3. The fragmented OCR results in a few words may also exist in real-world text-rich data such as poster, table, directly removing this kind of data may be also different from the real-world distribution."
            },
            "questions": {
                "value": "1. How is the performance if your dataset is trained based on mPLUG-Owl? Since it is a high baseline of your method. Can you further fine-tune other open-source models using the dataset from this article and provide performance comparisons?\n2. Can you provide experimental results and analysis on more text-rich image understanding  benchmarks (e.g., Information Extraction, Document Classification, OCR)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6737/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698730806183,
        "cdate": 1698730806183,
        "tmdate": 1699636775432,
        "mdate": 1699636775432,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UXSuGCPT2m",
        "forum": "tj4a1JY03u",
        "replyto": "tj4a1JY03u",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6737/Reviewer_miv5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6737/Reviewer_miv5"
        ],
        "content": {
            "summary": {
                "value": "The paper collects noisy and high-quality instruction-following data to enhance visual instruction tuning for text understanding in images. Their model LLaVAR incorporates this new data and improves performance on text VQA and instruction following for text-rich images. The enhanced capability allows more natural interaction based on real-world online content combining text and images."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper focuses on an important problem of improving OCR ability for multimodal LLMs like LLaVA.\n2. It identifies that key factors to improve model performance are training data and image resolutions. To address this, the paper collects over 400k specialized training examples to enhance OCR capabilities.\n3. Extensive experiments verify the effectiveness of the proposed training data."
            },
            "weaknesses": {
                "value": "1. The conclusions are a bit obvious - that higher resolution inputs and more specialized training data improve LLaVA's OCR performance.\n2. The most important contribution of the paper is the collected dataset. It succeeds in showing the data improves LLaVA's OCR capabilities, but does not demonstrate it is superior to other visual instruction datasets. For example, mPLUG-Owl has comparable OCR performance to LLaVAR under the same resolution in Table 2. This raises the question of whether OCR-specific data is needed, or if the scale of data in the paper is insufficient.\n3. The evaluation is limited, mostly relying on 4 OCR QA datasets. As the authors admit in Fig 4(5), this evaluation may be unreliable. More scenarios like the LLaVA benchmark would be expected, especially in ablation studies."
            },
            "questions": {
                "value": "1. Why did the authors collect data based on LAION, rather than some well-annotated OCR dataset?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6737/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6737/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6737/Reviewer_miv5"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6737/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698731300758,
        "cdate": 1698731300758,
        "tmdate": 1700738693934,
        "mdate": 1700738693934,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FLtgA9eyH2",
        "forum": "tj4a1JY03u",
        "replyto": "tj4a1JY03u",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6737/Reviewer_2XNz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6737/Reviewer_2XNz"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a methodology to improve the text reading capability of large language and visual assistants. There are mainly two contributions: the data collection procedure and the improvement of LLaVA with the data. Text-rich images are collected by applying an image classifier with some filtering criteria. Off-the-shelf OCR tools are then used to obtain the texts in the images. A pretraining task is defined to output the transcribed texts as target. For finetuning, GPT-4 is used to generate instruction-following data. GPT-4 is asked to generate a sequence of question-answer pairs. The model is finetuned with the generated data. The experimental results confirm that LLaVAR improves LLaVA for tasks requiring reading texts. The code, data and model will be released to the public."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper shows a practical way to collect a large amount of text-rich data. The quality of the data is confirmed by the experiments where the training with the collected data improves the model. The data will be released to the public and the community will be able to benefit from the work.\n\nThe methodologies to generate the pretraining and finetuning data are reasonable. The use of GPT-4 to generate instruction-following data is very similar to the idea of LLaVA, but it seems also effective to generate such data for tasks requiring reading texts."
            },
            "weaknesses": {
                "value": "What seems important to improve text reading capability of this type of models is to train the models with a task that requires to read texts. This work also does it by generating data with OCR-ed texts and defining tasks that require reading texts. As expected, it improves the model in terms of text reading capability. However, the problem is that this seems a shared problem in this field and there are other studies that tried to improve text reading capabilities of this type of models (e.g. PreSTU, Pix2Struct). There is no discussion in this aspect and it looks like this is yet another attempt with the same objective. It would be required to make the novelty and advantage clear against other studies.\n\nThis is essentially an extension of LLaVA with OCR tasks. It is certainly important to improve text reading capability of this type of models, but it looks a little bit incremental in terms of methodological novelty."
            },
            "questions": {
                "value": "I wanted to understand the detail of \"GPT-4-based instruction-following evaluation\". My assumption was that GPT-4 was treated as Oracle (or GT) and some scores were computed against it. However, it was not very clear how text-based GPT-4 can be used to generate GT for tasks with image inputs. Also, how to compute the scores was not clear."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No concern."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6737/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698741944938,
        "cdate": 1698741944938,
        "tmdate": 1699636775180,
        "mdate": 1699636775180,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Xc3tmzbAFs",
        "forum": "tj4a1JY03u",
        "replyto": "tj4a1JY03u",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6737/Reviewer_sAP9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6737/Reviewer_sAP9"
        ],
        "content": {
            "summary": {
                "value": "This paper enhances the visual text understanding ability of large multimodal models by instruction tuning methods. First, two sets of noisy and high-quality instruction-following data are constructed. Specifically, the high-quality instruction-following data are generated by prompting text-only GPT-4 with OCR results and captions. Then, a two-stage training strategy is developed, with the first stage learning OCR capability and the second stage learning high-level understanding capability. Extensive experiments verify that the proposed LLaVAR model can improve performance on both natural and text-rich images."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1) This work a pioneering exploration of visual instruction tuning for text images, which can provide some useful insights to the community. \n2) The proposed model has ability to deal with high-resolution images by integrating extra visual encoders and cross-attention modules.\n3) The experiments are basically sufficient to demonstrate the superiority of the LLaVAR method, especially relative to the LLaVA baseline."
            },
            "weaknesses": {
                "value": "1) This work is less innovative in approach, as it mainly focuses on the construction of instruction-following data, while the proposed model and implementation pipeline basically follow LLaVA.\n2) From the results in Table 2, LLaVAR has no significant performance advantage compared with existing methods under the same resolution (2242), such as mPLGU-Owl (2023). Besides, it is better to evaluate the parameter sizes of these comparison models.\n3) In Section 5.3, only one case study is carried out, so the derived conclusion is hardly convincing.\n4) In Figure 7, the notations are not clearly explained, and the implementation details cannot be visually reflected in this figure."
            },
            "questions": {
                "value": "1) As can be seen in Figure 4 and Figure 5, OCR errors are inevitable, e.g., \u201cBoynton\u201d vs. \u201cByington\u201d. Can you provide some results to analyze the impact of OCR errors?  \n2) As mentioned in the paper, the adopted metric only considers the recall, so it is not very reliable. Have you tried other quantitative metrics to prove the effectiveness of the method, such as the metrics designed for the image captioning task?  \n3) What does \u201ctemperature\u201d refer to in the first paragraph of Section 5?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6737/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6737/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6737/Reviewer_sAP9"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6737/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698810547614,
        "cdate": 1698810547614,
        "tmdate": 1699636775060,
        "mdate": 1699636775060,
        "license": "CC BY 4.0",
        "version": 2
    }
]