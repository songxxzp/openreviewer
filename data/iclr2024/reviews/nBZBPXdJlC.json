[
    {
        "id": "WHL1hg7nOW",
        "forum": "nBZBPXdJlC",
        "replyto": "nBZBPXdJlC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4061/Reviewer_wB1w"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4061/Reviewer_wB1w"
        ],
        "content": {
            "summary": {
                "value": "The authors propose to connect an audio encoder (AST) with a large language model (LLM) through LoRA adapter tuning for audio understanding. To fine-tune this model, the authors also propose and curate an OpenAQA-5M dataset which mixes of existing audio tasks such as audio classification, audio captioning, and sound event detection as close-ended tasks, and leverage LLMs to generate question and answer pairs given text metadata. The authors further conducted human evaluation to verify these generated data."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- OpenAQA-5M is a good contribution to provide open-ended question answering in audio domain, especially it is verified with human evaluation.\n- Ablation study shown in table 5 provides good insights for choices of LoRA params and the benefit of curriculum in staged training."
            },
            "weaknesses": {
                "value": "- For the open-ended questions, this work seems to focus mainly on solely LLMs assisted question answer generation. It is not intuitive to understand to what extent the model relies on the input audio versus on the common sense knowledge that is already encoded in the LLMs. It would be great to define and identify beyond current close-ended tasks with new lower level tasks which really require using the audio, such as counting sound events, ordering of events, etc. These type of questions might already exist in the proposed dataset, it would provide more insights to dive deeper into those.\n- Table 5 on the right for the training curriculum, it would be great to also include the language instruction following rate. And further discuss the correlation between the classification performance and the instruction following rate, if there is any insights that can be drawn."
            },
            "questions": {
                "value": "- The acoustic features mentioned in 3.1 are also generated from LLMs, how are these verified?\n- In the temporal analysis paragraph in 3.1, how are the understanding of order of sounds evaluated?\n- In 5.2.1, LTU can answer follow-up questions about the details, there is an example where LTU shows that the bell rings three times, are there other examples showing the counting capabilities? Is it possible to further quantify this?\n- In 5.2.1, multi-turn conversation is mentioned, are the previous audio tokens and questions also passed in as context or are the multi-turn independent each turn? This can be another interesting property for few-shot in-context learning if possible."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4061/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4061/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4061/Reviewer_wB1w"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4061/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698503684274,
        "cdate": 1698503684274,
        "tmdate": 1700692104105,
        "mdate": 1700692104105,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YJekjxGWUE",
        "forum": "nBZBPXdJlC",
        "replyto": "nBZBPXdJlC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4061/Reviewer_oFtB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4061/Reviewer_oFtB"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an audio foundation model, Listen, Think, and Understand by combining an existing audio encoder and LLM. It also creates a new audio dataset called OpenAQA-5M for training the proposed LTU, so that it can enhance the existing audio perception capabilities and also provide a clear explanation of training details. \n\nThe paper effectively delivers that the existing audio models have limitations in reasoning and comprehensibility and suggests that combining audio models and LLM would resolve the problems that the existing audio techniques have suffered.\n\nIt is also well-supported experimentally in two key areas: audio perception and reasoning abilities. Audio perception is evaluated through tasks like classification and captioning. Reasoning ability is evaluated through human assessment, and the paper claims that it outperforms GPT-4."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper claims that it is the first time designing a reasoning comprehension-capable model.\n2. Details of training and dataset are logical and delicate. It handles the hallucination problem of LLM by training close-ended dataset and then non-answerable question-answer pairs. It considers the training direction to be \"first to perceive, and then comprehend the sound\" so that the training starts from using close-ended datasets to open-ended datasets. The paper shows the reasonable claim that it is necessary to gradually train the model from close-ended datasets to open-ended ones because if the open-ended dataset is trained first, the model is heavily dependent on language capability so it is hard to train the audio representation.\n3. The paper is clear and easy to understand."
            },
            "weaknesses": {
                "value": "1. There seems to be a lack of thought about model structure and loss. It is just a combination of the strong pretrained LLM and the existing audio encoder, AST. Utilizing strong pretrained LLM with multimodal inputs has an alignment issue. Thus, while concatenating the audio feature and the text feature can introduce desired performance, there could be some advancements not just combining pretrained audio model and LLM. For example, BLIP-2 [1] solves misalignment between text and image using 3 losses: 1) Image-Text matching, 2) Image-Grounded Text generation, and 3) Image Text Contrastive Learning. The simple combination of the audio model and LLM does not seem to be novel.\n2. The concurrent works show higher performances. Compared to Pengi, the closed-ended audio task performances are lower. I understand that the proposed paper is focusing on the open-ended problem, but it would be better to elaborate more in detail that the proposed paper is competitive compared to the concurrent works.\n\n[1] Li, Junnan, et al. \"Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.\" arXiv preprint arXiv:2301.12597 (2023)."
            },
            "questions": {
                "value": "1. I don't understand 64(time) X 16(frequency) in the 5th line of Audio Encoder part in Section 2: LTU Model Architecture. Is it typo(I think it should be 8)? Or what is the meaning of frequency 16?\n2. I want to know how the dataset is formulated (Q and A) that is generated from GPT."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4061/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4061/Reviewer_oFtB",
                    "ICLR.cc/2024/Conference/Submission4061/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4061/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698656415519,
        "cdate": 1698656415519,
        "tmdate": 1700637488121,
        "mdate": 1700637488121,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TxHUyhSEJ5",
        "forum": "nBZBPXdJlC",
        "replyto": "nBZBPXdJlC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4061/Reviewer_2qgj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4061/Reviewer_2qgj"
        ],
        "content": {
            "summary": {
                "value": "The authors present an Audio Language Model (ALM) capable of understanding generic audio signals. The system in its current form is not designed for speech or music transcription but to provide a generic description/caption summarizing the audio. The crux of the problem to solve in such models has to do with text generation conditioned on audio signal. The fundamental strategy of approaches in this realm has been to interface an audio encoder somehow with a Large Language Model (LLM) for audio conditioned text generation. \n\nIn this work the authors propose to use Audio Spectrogram Transformer (AST) as the Audio Encoder and interface it with the LLaMA model using a simple projection layer in between with LoRA (Low RAnk) adapters added on top of LLaMA model. As has been the common practice such a network is trained on multiple audio related tasks by unifying the input output representation of the network in the form of [Audio, Question] as input to [Answer] as output format. To train this model the authors curate data from popular public datasets across 8 different tasks and augment the same audio with multiple Question answer pairs. The Question-Answer pairs are generated in 2 buckets - Close-Ended and Open Ended. In the Close-Ended bucket Questions are paraphrased using GPT3.5-Turbo for diversity. Answers are generated using a rule based system. In the Open Ended bucket Questions and Answers are generated using GPT-3.5 based on Audi metadata (audio events, captions, acoustic features, temporal information) and prompt engineering. The LTU model is then trained solely on the OpenAQA Dataset with an interesting Curriculum training. the gist of curriculum training is to go from simple to complex. Train the projection layer ---> Train AST + Projection layer + LoRA with frozen LLaMa. In terms of tasks: Simpl close ended tasks  --> Close + Open ended tasks. \n\nOverall the paper is very well written. The authors have done an excellent job in presenting the idea, exploring the idea with effective ablation studies, a straight-forward depiction of results with comparison to the right baselines and providing a plausible explanation to interpret the results in the context of baselines."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "Originality:\n1. The idea, at the time the paper was originally written, was indeed very novel as there were not many audio language models around back in May. The idea makes a lot of sense. \n2. Audio Instruction Generation (AIG) is also quite nice and interesting. \n3. The curriculum learning is yet another contribution which makes a lot of sense and the authors have proposed an intuitive curriculum and backed it up with apt ablation study to show its utility. \n4. The Human evaluation study adds a ton of value in judging LTU in my perspective. \n\n\nQuality:\n1. A high quality manuscript with sufficient experimental details. \n2. I thoroughly enjoyed reading the full paper including all the Appendix sections. Very thoughtfully done experimental ablation study. \n\nClarity: \n1. Very well written manuscript with clear non-monotonic description of details. \n\nSignificance: \n1. In my opinion this is a significant paper as it explores one of the straight-forward ways to couple an audio encoder with a trained LLM and carefully examines this coupling from several different viewpoints and contributes the OpenAQA dataset which can be a useful public resource for future research. The curriculum training is yet another aspect that makes this effort worthy as it shows that a brute force approach to just wrap in all possible audio-text paired data may not be as good overall."
            },
            "weaknesses": {
                "value": "1. It is not clear to me what the value of OpenAQA dataset is on top of of the textual metadata available with most of these datasets. It would have been fascinating if the authors were to do an ablation study to train their model in the format of (Audio, Text) --> Text format - something similar to what PENGI does. Use the text data that comes with each dataset as is and compare this with :\n  a) Using OpenAQA (current setting)\n  b) Augmenting the original audio text pairs with OpenAQA\n\n2. It might be valuable to evaluate PENGI on Open ended tasks. My hunch is LTU would be far better than Pengi in open ended tasks although Pengi might be better on Close-ended tasks. This would probably highlight LTU significantly as the two approaches are contemporary in many ways and are very similar in the overarching goal. However the approaches taken are different. I realize that PENGI's checkpoint was probably not available when this paper was submitted but it is now. I would encourage the authors to do this comparison to show how LTU could be a more generic model which can be super useful for users to interact with an audio language model through natural text."
            },
            "questions": {
                "value": "Question raised in Weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4061/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698779589973,
        "cdate": 1698779589973,
        "tmdate": 1699636370057,
        "mdate": 1699636370057,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YRcGQDk46L",
        "forum": "nBZBPXdJlC",
        "replyto": "nBZBPXdJlC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4061/Reviewer_1UHZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4061/Reviewer_1UHZ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a multimodal large language model with the ability of general audio perception. The model combines the AST audio encoding frontend with the LLaMA large language model, utilizing LoRA for fine-tuning, resulting in a model with both audio perception and reasoning capabilities. Moreover, this paper constructed a large-scale dataset for this task, encompassing closed-ended and open-ended Q&A pairs. Extensive experiments illustrate that this model exhibits outstanding performance across various audio-related tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.The paper introduces for the first time a large language model that combines both general audio perception capabilities and language reasoning abilities, along with the datasets used for training. It is highly innovative and holds significant importance for the development of general artificial intelligence.\n\n2.Through a significant number of ablation experiments, this paper extensively researched the model's hyperparameter configurations and training strategies, offering highly instructive guidance for related work.\n\n3.The model excels in various audio-related tasks and open-ended question answering, demonstrating its outstanding performance."
            },
            "weaknesses": {
                "value": "1.The performance of this model is closely related to both the AST encoding frontend and the LLaMA model's performance. Ablation studies on the varying parameter counts of these two components would be valuable, if possible.\n\n2.The authors may add subjective evaluations to the ablation experiments to better demonstrate that the LoRA fine-tuning strategy mitigates catastrophic forgetting issues."
            },
            "questions": {
                "value": "1.Why is the results of LTU on open-ended problems preferred compared to those of GPT-4 in Section 5.2.2? In terms of the data generation method of open-ended Q-A pairs, LTU's inferential knowledge appears to be transferred from the data generation model, i.e. GPT 3.5 Turbo model. These experimental results seem to indicate that the performance of the student model is superior to that of the teacher model (family).\n\n2.Did the authors investigate whether errors (or hallucinations) in open-ended question generation model (i.e. GPT 3.5 turbo) could impact the performance of the LTU? In other words, is the correctness of the Q&A pairs generated by GPT from audio metadata reliable enough?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4061/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699352269193,
        "cdate": 1699352269193,
        "tmdate": 1699636369996,
        "mdate": 1699636369996,
        "license": "CC BY 4.0",
        "version": 2
    }
]