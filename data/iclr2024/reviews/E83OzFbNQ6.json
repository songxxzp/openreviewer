[
    {
        "id": "SZ8J3EhNIY",
        "forum": "E83OzFbNQ6",
        "replyto": "E83OzFbNQ6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1415/Reviewer_HFxe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1415/Reviewer_HFxe"
        ],
        "content": {
            "summary": {
                "value": "In this study, the authors delve into online continual learning under the constraints of limited computational resources, while relaxing storage capacity constrain. The central concept revolves around the integration of a K-nearest neighbors (KNN) approach with a static, pre-trained feature extractor. The authors substantiate the appropriateness of KNN for this scenario, underscoring its capability to swiftly adapt to dynamic data streams without compromising stability. Furthermore, they underscore its efficiency in the context of constrained computational resources, attributed to its ability to store essential features exclusively and uphold a consistency property, thereby preserving previously encountered data."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "In this paper, online  continual learning in the presence of drift is studies, which indeed is an interesting and a practical topic as data streaming applications keep on increasing. The paper is well written and easy to read, and the algorithm is clearly presented and also demonstrated in Figure 1. The paper very well included state of the art. The improvement obtained in the experiments is considerable."
            },
            "weaknesses": {
                "value": "In my opinion, the assumed setup seems simplified and unrealistic, given that it presumes the use of a fixed pre-trained feature extraction method for all forthcoming data in the data stream, as also mentioned by the authors. In a data stream, the data distribution and their features structure can evolve, and new classes can emerge, using a fixed pre-trained feature extraction method might not be enough in data streaming learning. The authors discussed about mobile devices, but can one really claim that we don't have  storage constraints in mobile devices in data explosion era? \nThis core idea in this study is very similar to (Nakata et al., 2022), in both studies KNN has been used as a reminder assistance for the backbone model for quick adaptation after drift. The idea is indeed interesting, but unfortunately it does not contain novelty. I cannot find any major novel differentiation within this work in compare with the mentioned study."
            },
            "questions": {
                "value": "Can you provide some explanation and comparison with KNN for the approaches mentioned in:\"Fixed Feature Extractor based Approaches\"\nCan you add some justification or empirical results to prove this: \"We additionally highlight that ACM has a substantially lesser computational cost compared to traditional OCL methods.\"\n\nYou have used a fixed set of hyperparameters determined from the pretrained set, but did (can) you explore how the need for hyperparameters tuning can vary when observing drift? BTW,  Please mention in the text the concrete hyperparameters here as well: \"We first tune hyperparameters of all OCL algorithms on a pretraining set\"\n\nCould you please provide experiments or explain how your algorithm in compare with other approaches work when a new class emerges that has not been see in the pretraining phase? \n\nIn KNN have you assumed a fixed K?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1415/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698146440061,
        "cdate": 1698146440061,
        "tmdate": 1699636069509,
        "mdate": 1699636069509,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YlFYiRjnsi",
        "forum": "E83OzFbNQ6",
        "replyto": "E83OzFbNQ6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1415/Reviewer_9XgP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1415/Reviewer_9XgP"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a kNN based retrieval of previous sample from the infinite memory for reminding the past information to alleviate forgetting. While the setup could be realistic as the memory cost becomes negligible, even using the efficient version of kNN, the retrieval of relevant sample from the infinite memory is still computationally expensive and reminding previously learned knowledge may not be desirable as large models are arguably much less forgettable about previously learned knowledge. By the help of perfect remind of previously given data, the method improves the classification accuracy significantly over the other methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Superior empirical gain over other methods\n- Simplicity of the method\n- Good empirical setup using CGLM and CLOC datasets"
            },
            "weaknesses": {
                "value": "- The presented setup with infinite memory is arguably realistic online continual learning setup. The infinite memory would eventually prevent forgetting by perfect reminding (by using properly efficient version of kNN retrieval) and the proposed method is not surprising with that. Thus, it is questionable whether the proposed setup and the method is indeed helping us to solve online continual learning for real world deployment or not.\n- Method is not well motivated. It is not clear why the kNN let the model adapt to new sample fast and lead to zero stability gap inherently.\n- Why the proposed method only has very high initial accuracy in Rapid adaptation plot using CLOC in Fig. 2 (right upper)."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1415/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698678553401,
        "cdate": 1698678553401,
        "tmdate": 1699636069436,
        "mdate": 1699636069436,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Psc9GFdxhe",
        "forum": "E83OzFbNQ6",
        "replyto": "E83OzFbNQ6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1415/Reviewer_2LkF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1415/Reviewer_2LkF"
        ],
        "content": {
            "summary": {
                "value": "The proposed submission questions whether the popularly enforced storage constraints in online continual learning is really realistic and develop an online continual learning method with no storage constraints, e.g. storing all data points (whether raw or processed). They use an approximate kNN classifier to make predictions using the stored data points, which boosts fewer computation costs in terms of training and predicting using the continually learned model. Evaluation is done on YFCC-100M and Google Landmarks V2 datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The submission poses an interesting question of whether the storage constraint is realistic or not.\n- The proposed method is simple and straightforward.\n- The paper reads well."
            },
            "weaknesses": {
                "value": "- The novelty is limited. The methodology itself is an approximate kNN, with little modifications. Additionally, the method merely uses pretrained feature extractors as well, which does not add to technical novelty.\n- The consideration of the storage constraint seems a bit uni-dimensional to me. There are other factors than just storage costs that are not taken into account. For instance, the data itself may be volatile, i.e., some data points may be required by law to be deleted upon a set duration."
            },
            "questions": {
                "value": "- The feature computation part seems to have little difference with regards whether it is used in a regular, static data manner or in the continual learning setup. Can the authors clarify on this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1415/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698813717579,
        "cdate": 1698813717579,
        "tmdate": 1699636069341,
        "mdate": 1699636069341,
        "license": "CC BY 4.0",
        "version": 2
    }
]