[
    {
        "id": "NS33PRG53M",
        "forum": "VoLDkQ6yR3",
        "replyto": "VoLDkQ6yR3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6267/Reviewer_kua5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6267/Reviewer_kua5"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the reconstruction attack, and shows that the reconstruction attack can recover all samples in the training data set. Studies are carried out on the properties of easily-reconstructed images."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper provides interesting experimental results. The main paper is clear and easy to understand."
            },
            "weaknesses": {
                "value": "My major concern is about the theoretical results in this paper. The paper claims their theoretical results as one of their major contributions, but from the presentation in the paper, this contribution is not as sound as the empirical side.\n\nFor the two theorems, Theorem 1 and 2, their presentation needs improvement.\n*    For Theorem 1, one can intuitively understand its meaning, but it is hard to interpret the English sentence into a formal mathematical statement. The proof is also vague, with many descriptions but few math formulas and equations. It is hard to rigorously understand the mathematical meaning of this theorem, and it is also hard to check the correctness of the proof. The authors need to rewrite Theorem 1 (maybe leave an informal description in the main paper and postpone the full theory statement in the appendix) and provide a more rigorous proof.\n*    For Theorem 2, the derivation in the appendix is readable, but the theorem statement in the main paper needs to be more clear. However, when considering infinite width of the neural network, some derivations are missing: for example, in Page 19, the first \"->\" needs more details. Although k_{\\theta_0} -> k_{NTK}, the error terms are needed in the later derivation to show that a negligible |k_{\\theta_0}-k_{NTK}| really leads to a negligible error term in the first \"->\" in Page 19."
            },
            "questions": {
                "value": "I think the empirical studies in this paper are sound but the theoretical parts are below the acceptance standard, so I give a score 5. Please consider improve the theorems, and update them either in the submission or reply in the rebuttal."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6267/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6267/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6267/Reviewer_kua5"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6267/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698613612122,
        "cdate": 1698613612122,
        "tmdate": 1699924305670,
        "mdate": 1699924305670,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ldzQUENlip",
        "forum": "VoLDkQ6yR3",
        "replyto": "VoLDkQ6yR3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6267/Reviewer_GBp6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6267/Reviewer_GBp6"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors investigated the reconstruction attack in the view of neural tangent kernel (NTK). From a well formulated description, the authors showed interesting results with both theoretical and practical meaning."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The memory of deep neural networks is of great interests and importance. It is believed that the memory and also the memorization is closely related to the training dynamics, which is however not clearly investigated. Thus, I personally like the way of modelling the data reconstruction by NTK, which indeed capture the main properties on memorization dynamics. Though there are still many simplification, the good performance demonstrate the rationality of the modelling. So I think the main strengths include:\n\n- A novel and interesting way of modelling memorization from training dynamics.\n\n- Theoretical discussion well coincides with numerical experiments.\n\n- Clear discussion on the weakness, which actually could inspire future works."
            },
            "weaknesses": {
                "value": "The main weakness are for some unclear settings. Please see the questions below."
            },
            "questions": {
                "value": "In the current version, the reconstruction performance is related to the number of training samples as well as the property of the samples. How about the effect of data dimension. Especially, the author cast the reconstruction loss as a sparse coding, also Haim et al. (2021) regarded the training process as encoding. Then, can the authors obtain some conclusion about data dimension?\n\nThe reconstruction problem is a complicated optimization problem and the result could be totally different when different initial solutions are used. I notice that in algorithm 2 \"randomly initialized reconstruction images\" are used. Then how about the divergence of the reconstruction result? Is that necessary to use special initialization, e.g., an image in one of the two classes, an image from another class, a natural image of which the class is not in the training set, or a random generated matrix?\n\nAfter reading the rebuttal and good discussion, I would like to increase the score from 6 to 8. But please talk more about the link to e.g., GradViT: Gradient Inversion of Vision Transformers; Deep Leakage from Gradients, which can use local gradient information (even the model is not well trained) to reconstruct the training samples."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6267/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6267/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6267/Reviewer_GBp6"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6267/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698641426798,
        "cdate": 1698641426798,
        "tmdate": 1700580089567,
        "mdate": 1700580089567,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RrocTUkBO1",
        "forum": "VoLDkQ6yR3",
        "replyto": "VoLDkQ6yR3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6267/Reviewer_FNQS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6267/Reviewer_FNQS"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a new, more stable method for performing reconstruction attacks against neural networks based on the neural tangent kernel. This method requires access to both the initial weight state and the trained weight state, but does not require any information about the data distribution. A number of ablation studies confirm conventional wisdom that larger networks essentially memorize their training sets, that outlier datapoints are most vulnerable to reconstruction attacks, and that there is a strong mathematical connection between reconstruction attacks and dataset distillation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper develops a very nice algorithm for dataset distillation based on inducing point methods. Figure 8 in particular shows the value of the proposed approach. The underlying theoretical connections to dataset inversion provide confidence in the method.\n\nThe paper provides thorough experimental evidence for the theoretical claims. The datasets used (restricted MNIST and CIFAR) are small, but this is to be expected with a computationally-intensive object like the NTK. The reconstruction attacks on ImageNet are impressive.\n\nThe discussion about the choice of kernel in Appendix I is interesting. The authors should at least incorporate the insight about combining initial and final weight states into the main text. Likewise, why are the results on more complex architectures reserved for Appendix J? These are interesting results that should be incorporated into the paper, if even briefly."
            },
            "weaknesses": {
                "value": "The authors claim that \u201coutliers\u201d are more vulnerable to reconstruction attacks, but this notion of \u201coutlier\u201d is not well defined up front. I believe that your technical definition for outliers is points that have high $\\alpha$ values (i.e. points that are \u201chard to fit\u201d), but this does not necessarily mean that these are points that are distant e.g. under the Euclidean metric in the input space.\n\nThe text on most of the figures is so small it makes them hard to read, even on a screen.\n\nReconstruction plots are not explained until the first paragraph of Page 6, but the plots appear as early as page 4. This creates a readability problem, since the construction of these plots is not self-evident.\n\nIt would be nice to have a central definition of all model variants (e.g. RKIP, RKIP-finite). These definitions are currently spread across the paper.\n\nSmall issues:\n* This phrase in the abstract doesn\u2019t make sense: \u201cof its effective regime **which** datapoints are susceptible to reconstruction.\u201d\n* The statement that non-privacy preserving networks are \u201cuseless\u201d is probably a bit hyperbolic in the introduction, this is very application-dependent.\n* In Table 1, specify whether the values are \u201caccuracy\u201d."
            },
            "questions": {
                "value": "1. Can you make claims about the effect of self-supervised learning on the effectiveness of your reconstruction attacks? Does your method naturally extend to structured outputs, not just scalar outputs?\n2. Do you have any speculation how informative your results will be for networks trained with different losses than MSE? Is this a simplifying assumption to enable your proofs or a factor that could significantly change the structure of the empirical NTKs your method uses?\n3. Is there an underlying assumption in (1) that there is a prior (i.e. weight decay) on the parameters of the network?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6267/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6267/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6267/Reviewer_FNQS"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6267/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699453079376,
        "cdate": 1699453079376,
        "tmdate": 1699636685631,
        "mdate": 1699636685631,
        "license": "CC BY 4.0",
        "version": 2
    }
]