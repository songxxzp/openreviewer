[
    {
        "id": "9E5t6g03LH",
        "forum": "DDX1u29Gqr",
        "replyto": "DDX1u29Gqr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3111/Reviewer_NmnF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3111/Reviewer_NmnF"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a hierarchical way for 3D content generation from text inputs, following the line of dreamfusion.\nInstead of using 2D diffusion only, it combines zero123 to give 3D priors. \nIt also proposes a bootstrapped score sampling method, which finetunes the stable diffusion using dreamboth during the distillation  process. \nCombining carefully-tuned parameters, this paper achieves impressive results of text-to-3D."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The results of this paper is impressive. The improvement seems significant compared to previous methods.\n2. This paper combines many tricks in a convincing way, and clearly shows the effectiveness of each trick. \n3. The paper is well written."
            },
            "weaknesses": {
                "value": "1. The running time of methods. This method involves several stages an looks quite complicated. I am curious about how long does it take to finish the whole process? And how does it compare to other baselines?\n2. It would be more clear if the difference between the proposed bootstrapped diffusion prior and LORA updates in ProlificDreamer is elaborated, since they both update the diffusion during the distillation.  What's intuitive difference and actual different in implementation? \n3. I understand it is hard to evaluate generation task, but more details about metrics used in Table 1 would be convincing. All CLIP, Contextual ,PSNR and LPIPS are image-level evaluation, and it is not clear how to convert them to 3D level. PSNR and LPIPS are measured on reference images, which is understandable. But how is the CLIP and Contextual calculated? How many views are rendered from each scene for evaluation?"
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3111/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698430698806,
        "cdate": 1698430698806,
        "tmdate": 1699636257692,
        "mdate": 1699636257692,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zP5MjED4T7",
        "forum": "DDX1u29Gqr",
        "replyto": "DDX1u29Gqr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3111/Reviewer_GYis"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3111/Reviewer_GYis"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an image-conditioned 3D generative model and focuses on improving view consistency upon current techniques. It proposes to use a view-conditioned 3D prior, and various techniques to improve the texture fidelity. Results show empirical benefits of the proposed method compared to prior arts."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "* Qualitative results selected in the paper show a large margin of advantages of the proposed method compared to the baselines. \n* The paper focuses on addressing the view consistency problem, and the examples shown in the paper provide strong evidence for this claim."
            },
            "weaknesses": {
                "value": "* The paper combines several existing techniques, including Zero-1-to-3, DreamBooth, VSD, and a bag of tricks including progressive view sampling and a transition between different 3D representations. The novelty of this paper is limited. \n* The pipeline depends on Zero-1-to-3 to provide a good initial shape. For scenes that Zero-1-to-3 fails on, e.g. scenes that differ greatly from the synthetic training distribution of Zero-1-to-3, it's unclear if the proposed method can still generate reasonable results. \n* Multiple loss or gradient terms are introduced in Section 4.1, but the relative weights of these terms are not described."
            },
            "questions": {
                "value": "* Do all examples shown in the paper share the same hyperparameter configuration?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3111/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698816888132,
        "cdate": 1698816888132,
        "tmdate": 1699636257598,
        "mdate": 1699636257598,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rpgmdSExYN",
        "forum": "DDX1u29Gqr",
        "replyto": "DDX1u29Gqr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3111/Reviewer_vdnk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3111/Reviewer_vdnk"
        ],
        "content": {
            "summary": {
                "value": "The authors present a framework for text-to-3D generation. This is achieved by first synthesizing a single view image from a text point and using a view conditioned diffusion model to guide the geometry generation process. In particular, the framework uses a combination of SDS losses from a view-conditioned diffusion model and a text conditioned diffusion model. Further, the renderings from the 3D representation are used as additional training data to train a personalization model to improve texture quality. State of the art results are demonstrated on text-to-3D creations compared to several recent baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. **Result Quality** : The quality of the generated assets are very impressive. Particularly, in examples where the back side of the object hallucinates necessary details.\n2. **Novelty** : The proposed approach is reasonably novel as it proposes a combination of 2D SDS and 3D aware SDS losses for better geometric reconstruction. The paper also compares against contemporary work with similar ideas (Magic123) and shows state of the art results against the same. \n2. **Related work**: An adequate treatment of the related work has been provided. Efforts have been made to reference and compare against contemporary unpublished work that also have compelling results. \n3. **Reproducibility**: All the implementation is based off of available open source code bases, make it easy to reproduce. Network and training details have been provided to further aid in reproducibility. \n4. **Progressive view training**: Is a simple and elegant idea to make sure that the generated geometry is not flat.\n5. **Benchmark dataset**: This work shares a evaluation benchmark of 300 prompt image pairs, which can be used to evaluate future work in text-to-3D synthesis.  \n6. **Structure aware latent regularization**: Is another interesting strategy for making sure already generated texture information is preserved. This section would benefit from shifting it from the appendix to the main section."
            },
            "weaknesses": {
                "value": "1. **Multi component-Combination** : Although reasonably novel, the framework uses several different diffusion networks for different components. Particularly, Deep-IF for initial image and base 3D geometry generation, SD for personalization and Zero-123 for viewpoint guided SDS. The final framework appears to be a combination of Magic123, Zero1-to-3, ProlificDreamer and Fantasia3D[1]\n1. **Claims** : The authors claim that personalizing the diffusion model based on the multiview renderings helps improve texture, however, this seems counter intuitive, since the initial texture generated from the 3D representation are expected to be worse than the high quality texture. Providing additional insights about this would be helpful.\n2. **Writing**: The manuscript contains certain syntactic and language errors (such as some of the nits indicated below) and would benefit from a thorough proof reading pass of both the main paper and the appendix.\n3. **Ablations**: Qualitative ablations are provided for some of the components. But adding some quantitative ablations that show change in quality would be helpful. Particularly, metrics for 3D consistency for number of BSD steps, texture quality with and without BSD and effect of progressive training, timestep annealing and choice of diffusion model used for SDS (SD vs DeepFloyd).  \n4. **Training and inference time costs**: A comparison of training and inference time cost and memory footprint of the proposed approach compared to the baselines is also important and would provide some insights about the inference time trade off and memory required to achieve this quality.\n\n\nNits:  \nSec 4. \"in the next...\" -> \"in the next section?\" or \"next\".  \nSec 4.2 \"multiview rendering from last stage..\" -> \"from the previous stage?\"  \n  \n \n[1] Chen et al. Fantasia3D, ICCV23"
            },
            "questions": {
                "value": "What is the effect of time step annealing? and progressive viewpoint training?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3111/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698838542358,
        "cdate": 1698838542358,
        "tmdate": 1699636257532,
        "mdate": 1699636257532,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HpVgAS6YII",
        "forum": "DDX1u29Gqr",
        "replyto": "DDX1u29Gqr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3111/Reviewer_c9Ad"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3111/Reviewer_c9Ad"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the task of text-to-3D generation, incorporating a series of enhancements that yield highly promising results. The introduced framework DreamCraft3D generates 3D assets with remarkable fidelity and minimal artifacts, marking a great advancement in the field. Key innovations of DreamCraft3D encompass initial reference image generation, a geometry sculpting stage, and a subsequent texture boosting stage.\n\nThrough an extensive series of experiments, DreamCraft3D emerges as a formidable contender, outperforming both optimization-based methods such as DreamFusion and single-image-based 3D reconstruction techniques exemplified by Magic123.\n\n----- After rebuttal ------\nThanks for providing more experiments and most of my concerns/doubts are addressed. I have raised my rating to 8-good paper.  I suggest the authors to include the extra ablation studies into the main paper, which can be informative to many readers."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper demonstrates several notable strengths, as outlined below:\n\n1. **Impressive Qualitative Results**: Unlike existing methods, the generated 3D assets in this study exhibit great fidelity and significantly fewer artifacts. Furthermore, the Janus problem is substantially mitigated, marking a substantial improvement.\n\n2. **Comprehensive Comparative Analysis**: The paper conducts comparisons with both optimization-based methods and single-image-based 3D reconstruction techniques, enhancing the credibility of the experimental findings.\n\n3. **Effective and Reasonable Solutions**: The proposed solutions and submodules are not only reasonable in design but also demonstrate practical effectiveness.\n\n4. **Quantitative Evaluation and Benchmarking**: Despite the inherent challenges in evaluating text-to-3D methods, this paper makes a commendable effort to perform quantitative comparisons and establishes a new benchmark, contributing to the field's progress.\n\n5. **Clarity in Communication**: The writing in this paper is mostly clear and easy to follow."
            },
            "weaknesses": {
                "value": "Even though the final results are encouraging, there are some weaknesses that need to be addressed:\n\n1. **Clarity of Approach Section**: The introduction of multiple stages and new sub-modules (loss functions, optimization stages) makes the presentation in the approach section less clear. It would be beneficial to include an overview section with a clear definition of the total loss and a pseudo-algorithm summarizing the steps in different stages.\n\n2. **Technical Contribution Clarification**: The major technical contributions are not clearly defined. It's essential to clarify which submodules or loss functions represent the most significant changes and contribute the most to the final performance. This information is crucial for a deeper understanding of the work. Overall, the proposed framework is a bit complicated and contains many modules and stages. \n\n3. **Incomplete Validation**: Some modules are not thoroughly validated in the experiments. For instance, the importance and effects of the two losses in Eq.4, the L_RGB, and L_mask losses should be elaborated upon. It's also important to explain the advantages of using NeuS compared to NeRF-variants and provide validation results. Additionally, details on the implementation of \"Progressive view training\" should be provided.\n\n4. **Prompt Engineering Clarity**: The paper needs to address discrepancies in prompt engineering. As shown in Fig.2, the prompt for generating the reference image is \"an astronaut in a sand beach,\" while for dreambooth, it's \"an [v] astronaut.\" These differences need clarification to ensure consistency and reproducibility.\n\n**Miscellaneous:**\n\n- **Figure Order**: The order of figures is confusing. Fig.4 is mentioned later in the text than Fig.5/6 but appears earlier. Consider reordering them for coherence.\n\n- **Fig.6 Text Descriptions**: Fig.6 is missing text descriptions of the corresponding stages. It's unclear which figure corresponds to what stage, making it difficult to follow.\n\n- **Fig.5 Ablation Study**: The ablation study in Fig.5 could be expanded. Since text-to-3D is challenging to evaluate, a single example may not provide enough information. More comprehensive insights can help to strengthen the research."
            },
            "questions": {
                "value": "1. **training/inference time**: what's the training time of the entire pipeline and what's the time for each individual stage? Meanwhile, how \n1. **Training and Inference Time**: Could you provide insights into the training time for the entire pipeline and the time required for each individual stage? Additionally, how long does it take to generate a new 3D model from scratch, and could you provide a breakdown of the time allocation for this process?\n\n2. **Robustness of the System**: Given the numerous modules and training stages, how robust is the system? Are the same set of parameters applicable to different methods, and if not, what are the characteristics of failure cases? Have you observed any convergence issues in any of the stages?\n\n3. **Choice of Deepfloyd IF**: What is the rationale for not using SD in the first stage and opting for Deepfloyd IF instead? Could you elaborate on the advantages of Deepfloyd IF in this context?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3111/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3111/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3111/Reviewer_c9Ad"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3111/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699163974327,
        "cdate": 1699163974327,
        "tmdate": 1700752703619,
        "mdate": 1700752703619,
        "license": "CC BY 4.0",
        "version": 2
    }
]