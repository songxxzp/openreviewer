[
    {
        "id": "VaC7ZasBu1",
        "forum": "t3vnnLeajU",
        "replyto": "t3vnnLeajU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7130/Reviewer_bHgA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7130/Reviewer_bHgA"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a novel method for universal image restoration. The method, named as DA-CLIP, is based on the CLIP and diffusion model (IR-SDE). An additional controller is proposed to help predict high-quality content embedding and degradation type embedding. These two embeddings are then used through cross attention in a diffusion unet to help restore the degraded images. The proposed method is tested on the image denoising, inpainting, deblurring, etc. The experimental results show that the proposed method can achieve better performance than the state-of-the-art methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper proposes a novel framework, DA-CLIP, to learn high-quality content and degradation embeddings through contrastive learning. \n1. The integration of the degradation and content prompts to universal image restoration appears effective and innovative.\n1. Extensive experiments on various tasks are conducted to show the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1. There are no justifications about what is embedded in the HQ content embedding. It is better to provide some comparisons with the HQ content embedding and the original image embedding.\n1. The effectiveness and necessity of the prompt learning module is not well discussed. It is better to provide some ablation studies to show the effectiveness of the prompt learning module compared with naive cross-attention.  \n1. The performance compared with `NAFNet+DA-CLIP` is not superior.  \n1. Although the paper discussed the computation complexity in supplementary material, it only provides #params and FLOPS. It is known that the FLOPS is not a good metric for the computation complexity, especially for diffusion models which require multiple iteration steps. The paper should provide the inference time for the proposed method, and better in main paper. \n1. Experiments on more complex degradation scenarios, involving multiple concurrent degradation types, would emphasize the model's robustness and versatility.\n1. A deeper discussion about the text encoder's role in the performance could lead to a better understanding of the proposed framework."
            },
            "questions": {
                "value": "#### **1. Why VLM like CLIP is necessary in this paper ?**\n\nAfter reading the paper, I think that the proposed method does not have much relationship with VLM like CLIP. The DA-CLIP serves as a degradation type classifier and a content classifier. It seems OK to replace CLIP with simple CNNs such as ResNet. Given that the degradation types are quite limited in this paper, a text-encoder like CLIP is easily to be replaced with simple classification. As for the captions, we may also simply replace it with just clean image embeddings to perform contrastive learning.   \n\n#### **2. Why diffusion model is used for restoration process ?**\n\nGiven the slow inference of diffusion model, it is not clear why diffusion model is used for restoration process. After all, the proposed method has little relationship with diffusion model, especially when the diffusion network is trained from scratch. Compared with `NAFNet+DA-CLIP`, the diffusion based backbone does not have much advantages.\n\n#### **Summary & Conclusion**\nIn summary, I think that using contrastive learning to learn degradation type embedding and content embedding for universal image restoration is a good idea. And the experiments are also quite comprehensive and effective. However, the proposed integration with VLM and diffusion models are not well justified. And I do not think it is appropriate to claim vision-language models as the main contribution and novelty of this paper.\n\nTo conclude, I would like to give borderline to this paper. However, there is no such options in the review form. So I choose to give a marginal accept. I hope the authors can address my concerns in the rebuttal."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7130/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7130/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7130/Reviewer_bHgA"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7130/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698565775621,
        "cdate": 1698565775621,
        "tmdate": 1699636843975,
        "mdate": 1699636843975,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9eCh4Rv3hG",
        "forum": "t3vnnLeajU",
        "replyto": "t3vnnLeajU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7130/Reviewer_3ZRs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7130/Reviewer_3ZRs"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a degradation-aware vision language model (DA-CLIP) to generate high-quality image representation and distinct the degradation types of low-quality inputs for all-in-one image restoration. The DA-CLIP model can be integrated into different image restoration networks to improve the performance. Extensive experiments are conducted to demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The idea of constructing a vision-language model to restore clean semantic image representation and distinct degradation types of low-quality images is interesting.\n2. The method of using clean image representative and degradation prompt to instruct restoration networks for better performance is sound.\n3. The results look good, and the experimental analysis demonstrate the effectiveness of the DA-CLIP on all-in-one image restoration.\n4. The writing is well, and the paper is easy to read."
            },
            "weaknesses": {
                "value": "1. It is questionable that the caption embedding can provide a high-quality image representation supervision for the content embedding. $e_c^T$ can indeed provide a semantic supervision, but there is no guarantee that it is a clean image representation. Therefore, I think the claim that the image encoder with the controller outputs high-quality content features is not rigorous. It seems that $e_c^I$ mainly serves to provide semantic instruction for the restoration network, especially for diffusion-based models.\n2. The used experimental setup is too simple to demonstrate the superiority of this complex method. It is not difficult for a unified network, e.g., a vanilla version Restormer, to deal with the all-in-one image restoration setting with a specific degradation level for each degradation type.\n3. This paper do not provide the experiment about the generalization ability of the proposed method. I do some tests using the code provided by the authors, and the results show that the model cannot deal with out-of-distribution degradations as well as OOD degradation levels well. It is not surprising as the used degradation model is too simple. This also reflects that $e_c^I$ is not always a high-quality image representation.\n4. This method may be difficult to handle tasks with different degradation levels, because it is difficult to describe the specific degradation level in texts. Since the authors do not provide relevant experiments, the potential of this method to handle multiple degradation levels is still questionable. As far as the current results are concerned, the approach is not practical enough."
            },
            "questions": {
                "value": "1. Do both $e_c^I$ and $e_d^I$ have an important impact on the restoration performance? Intuitively, it is reasonable for diffusion-based models as $e_c^I$ controls the content and $e_d^I$ indicates the degradation. However, it seems not reasonable for mse-based models to use $e_c^I$ in the restoration process. \n2. What performance can be achieved by directly providing the semantic text prompt and the degradation type prompt to train the all-in-one image restoration diffusion model?\n3. What performance can be achieved by directly training a vanilla Restormer under the same all-in-one setting?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7130/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7130/Reviewer_3ZRs",
                    "ICLR.cc/2024/Conference/Submission7130/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7130/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698690136479,
        "cdate": 1698690136479,
        "tmdate": 1700749796903,
        "mdate": 1700749796903,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7hq7sRekAF",
        "forum": "t3vnnLeajU",
        "replyto": "t3vnnLeajU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7130/Reviewer_hnNQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7130/Reviewer_hnNQ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a degradation-aware CLIP model. It is aligned with language through dual aspects of image content and degradation during training. This DA-CLIP can extract information not only about the image content but also about image degradation. The authors also combined DA-CLIP with image restoration, proposing what they call a \"universal\" image restoration method. This method is based on Diffusion-Based restoration techniques, but the DA-CLIP's results are used as controlled prompts for input. The authors have showcased many results."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "I hold a positive view on the idea of incorporating degradation information into CLIP."
            },
            "weaknesses": {
                "value": "My main concern with this paper is its task setting. First, \"Universal Image Restoration\" is a term that is not so easily justified. This paper simply brings together ten different image restoration tasks, which is closer to \"multi-task\" than the so-called \"universal\". For a large model, mixing these ten tasks in such a separate manner for training, the model would internally categorize the problems before handling them in a single-task manner [R1]. This would not endow the model with sufficient generalization capabilities. For instance, an image with both rain streaks and subsequent compression artifacts cannot be accurately restored. This is not \"universal\". Moreover, this paper seems to ignore a host of more \"universal\" solutions, such as Real ESRGAN, BSRGAN, StableSR, DiffBIR, etc. Merging degradations to achieve better generalization is a new direction (which is not so new anymore). But this paper barely discusses whether these methods are \"universal\" or not.\n\nSecondly, DA-CLIP's ability to predict degradation is not particularly special, considering the task setting only involves ten types of degradation; it can be said that almost any image restoration model trained on these degradations or any model that understands or classifies them would have this ability [R2]. I can only say that introducing degradation into CLIP is a very promising direction and could be very useful. However, the approach taken in this paper fails to reflect any significance in doing so. Due to the inherent issues with the task setting of the paper, the experimental part also fails to demonstrate the corresponding contributions.\n\n[R1] Finding Discriminative Filters for Specific Degradations in Blind Super-Resolution\n[R2] Discovering \u201cSemantics\u201d in Super-Resolution Networks"
            },
            "questions": {
                "value": "See Weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7130/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699103740911,
        "cdate": 1699103740911,
        "tmdate": 1699636843744,
        "mdate": 1699636843744,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0HgrRCzrkc",
        "forum": "t3vnnLeajU",
        "replyto": "t3vnnLeajU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7130/Reviewer_j7ME"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7130/Reviewer_j7ME"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a framework called degradation-aware CLIP (DA-CLIP) that combines large-scale pretrained vision-language models with image restoration networks. The authors address the issue of feature mismatching between corrupted inputs and clean captions in existing vision-language models (VLMs) by an Image Controller that adapts the VLM's image encoder to output high-quality content embeddings aligned with clean captions. The controller also predicts a degradation embedding to match the real degradation types. The paper presents the construction of a mixed degradation dataset for training DA-CLIP and demonstrates its effectiveness in both degradation-specific and unified image restoration tasks. The results show highly competitive performance across ten different degradation types."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This paper proposes a novel framework, DA-CLIP, which combines large-scale pretrained vision-language models with image restoration networks.\n- This paper introduces an Image Controller that addresses the feature mismatching issue between corrupted inputs and clean captions in existing vision-language models. In addition, they introduce a prompt learning module to better utilize the degradation context for unified image restoration.\n- It demonstrates that DA-CLIP in both degradation-specific and unified image restoration tasks, achieving highly competitive performance across all ten degradation types."
            },
            "weaknesses": {
                "value": "- In Figure 1, DA-CLIP achieves surprisingly high accuracy in ten degradation types. How are these experiments set up? In contrast, CLIP performs poorly in many types. What prompts do the authors use for classifying degradations in CLIP?\n- In Figure 6, PromptIR is comparable or even better than the proposed DA-CLIP in most tasks on fidelity metrics.\n- In Table 2(c), the PSNR of DA-CLIP highly deviates from that of MAXIM. In addition, the results on task-specific restoration do not show a clear benefit of using a universal model for all tasks. It is believed that the merit of universal models is that different tasks can benefit each other, or at least be helpful in generalization to new domains."
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7130/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699122532933,
        "cdate": 1699122532933,
        "tmdate": 1699636843638,
        "mdate": 1699636843638,
        "license": "CC BY 4.0",
        "version": 2
    }
]