[
    {
        "id": "XrGyrtHfdo",
        "forum": "r7OB810eaP",
        "replyto": "r7OB810eaP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5611/Reviewer_kiF6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5611/Reviewer_kiF6"
        ],
        "content": {
            "summary": {
                "value": "This paper exposes the difficulties of dealing with non-ergodic reward\nsequences in RL. In particular, a simple example is given which shows\nthat such non-ergodic settings can result in policies with high\nexpected value, but low returns with probability 1. The authors\npropose a method for transforming reward signals to be ergodic, and\ndemonstrate that policies optimizing the transformed reward tend to\nearn much more reward than policies optimizing the original\nobjective. Then, the authors present a method for learning this\ntransformation from data, and demonstrate the benefits of doing so in\nfamiliar RL benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper, for the most part, is extremely well written and easy to\nfollow (except for a few points mentioned in the Questions\nsection). The problem of optimizing the expected return is often\nneglected in RL, and this paper mentions a fresh perspective on why\nthis can be problematic, which is convincing. The proposed method is\nnovel, and appears to work well."
            },
            "weaknesses": {
                "value": "My main concern with this paper is that it is not clear that the\nreward transformation that is proposed is generally useful in RL\nsettings. Firstly, as I mention in the Questions section, it seems\nunlikely that a reasonable reward transformation can be learned in\ngeneral without access to a sufficiently good policy or sufficient\nexploratory data. Moreover, given access to this data, I suspect that\nother transformations not based on ergodic increments can lead to\nsuperior performance (i.e., what if you learn a value function from\nthe data and use that as the reward function in the test phase)?\n\nThe paper does not clearly define what it means for reward increments\nto be ergodic. While I suppose the reader is meant to infer that\nreward increments are ergodic if the resulting return is ergodic, this\nshould be stated more clearly. It would also help to have some more\nintuition about the nature of ergodic increments."
            },
            "questions": {
                "value": "In definition 2, what is the variance function? Is the integral in\nthis equation an integral over the space of random variables?\n\nI am a little skeptical about the proposed framework for learning the\nergodic transformation given at the end of section 4. As I understand\nit, the proposal is to first collect a bunch of trajectories using\nsome default strategy, then learn the ergodic transformations based on\nthe rewards from those trajectories, and finally perform policy\noptimization w.r.t. the reward function transformed by the learned\ntransformation. I can see how this works with the coin game, since\nhere the states are essentially the returns, and the dynamics are\nextremely simple. Crucially, it is almost trivial to 'explore' the\nrewards in this setting -- by setting $F=1$, the agent will have\nexperienced all behaviors of the reward function within a few\nsteps. This is because the reward function is essentially the same at\nevery state, up to a factor. When you train an agent on the\ntransformed reward function, the distribution of observed states will\nlikely be very different (hopfully, otherwise the transformation isn't\nhelping much), in which case I would not expect the reward\ntransformation to generalize properly to the new trajectories in most\ncases. Are some assumptions needed?\n\nIs there any reason to expect such a large performance improvement in\nReacher? Given that without the transformation, the agent gets\nbasically no rewards, how is it even conceivable that the learned\ntransformation is helpful here?\n\n## Minor Issues\nIn Figure 1, it would be a lot more clear if the red and blue lines\nwere more distinguished from the sample paths, for instance if they\nwere dashed lines."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5611/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5611/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5611/Reviewer_kiF6"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5611/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698702248128,
        "cdate": 1698702248128,
        "tmdate": 1699636579057,
        "mdate": 1699636579057,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "O4U3dp1BOm",
        "forum": "r7OB810eaP",
        "replyto": "r7OB810eaP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5611/Reviewer_Euet"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5611/Reviewer_Euet"
        ],
        "content": {
            "summary": {
                "value": "The paper discusses non-ergodicity in reinforcement learning with a method that avoids departures of RL methodologies by using a method that converts a time series of rewards into a time series with ergodic increments. The paper includes various examples of how and where such problems arise thus motivating the importance of studying such problems."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper studies an important problem which seems to have been somewhat underexplored despite there being some works on the subject. The vision of the paper is supported by some empirical evidence showing improved performance when applying the technique proposed by the paper."
            },
            "weaknesses": {
                "value": "The paper has several notable weaknesses:\n\n**Structure**\nThe structure of the paper seems quite disorderly since the flow is interrupted by examples that seem misplaced as well as an unexpected detour to discuss risk-sensitivity. It is also very unclear which are the novel results of the paper which the authors seek to highlight as main results and which are the results of lesser importance.\n\n**Motivation**\nIn the introduction the definition of ergodic is not clearly introduced. For example, ergodicity can be defined by a process which is stationary and every invariant random variable of the process is almost surely equal to a constant. Additionally, as the authors discuss, non-ergodic reward functions have been studied within RL --- the authors write \u201cnone of these works, as a consequence of non-ergodicity, question the use of the expectation operator in the objective function\u201d which seems a little vague. For these reasons, the work seems poorly motivated with it being unclear as to why the examples given aren\u2019t resolved by using other existing approaches such as risk-sensitive objectives (though these approaches are discussed)\n\n\n\n**Formal details**\nThe paper at times lacks precision and consistency with formal details - for example the paper begins with a discrete time analysis then makes a diversion to studying continuous time. Additionally, some formalities are missing or unclear for example the transition dynamics aren\u2019t specified and it\u2019s unclear why a subindex is needed on the time index.\n\nThere is also an awkward transition to a Markov decision setting with reward dynamics being governed by a stochastic difference equation. Without further discussion it is unclear how restrictive this is since the standard MDP/RL setup does not require conditions similar to this.\n\n**Empirical Evaluation**\n\nThe empirical evaluation lacks comparison to a range of techniques."
            },
            "questions": {
                "value": "1. What are the restrictions we get from imposing the reward expression in equation 3.\n\n2. Can the authors clarify the contribution beyond what has already been studied towards the goal of RL with non-ergodic reward functions e.g. what is new compared to Majeed and Hutter (2018).\n\n3. How does this approach compare with risk-sensitive RL?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5611/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698795647776,
        "cdate": 1698795647776,
        "tmdate": 1699636578959,
        "mdate": 1699636578959,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TDAJh9WQu2",
        "forum": "r7OB810eaP",
        "replyto": "r7OB810eaP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5611/Reviewer_R8mx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5611/Reviewer_R8mx"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the viability of dealing with non-ergodicity in reinforcement learning (RL) by transforming potentially non-ergodic reward processes generated during learning into ergodic reward processes. Unlike most existing works on RL theory, which typically frame ergodicity as a property of the Markov chain induced by a given policy applied to the underlying Markov decision process (MDP), this paper considers a certain notion of ergodicity of the overall reward process, independent of the policy used. A recurring example based on a coin toss betting problem is presented to illustrate what is meant by non-ergodic rewards, a general class of ergodic transformation and a procedure for approximately learning the transformation are proposed, a derivation is provided linking recent formulations of risk-sensitive RL to the proposed framework, and experiments illustrating benefits of the proposed transformations on the coin flip problem and two RL benchmarks (Cartpole and Reacher) are provided."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper considers the issue of non-ergodicity in RL from an interesting perspective: it is novel and potentially practically useful, especially in light of applications of RL to finance and economics, to directly consider non-ergodicity in induced Markov reward processes. The improvements experimentally observed on the coin toss betting problem suggest the method has merit and the connection to risk-aware RL bears further study."
            },
            "weaknesses": {
                "value": "Though the paper presents an interesting perspective on non-ergodicity in RL, there are serious issues in the proposed approach:\n1. The proposed notion of (non-)ergodicity and how it relates to the underlying sequential decision-making problem is not clearly defined, seriously undermining the proposed approach and its potential relevance to existing work. This is a major drawback and my most serious concern. RL is typically viewed as a family of methods for (approximately) solving MDPs, where the goal is to find a policy -- a mapping from states to distributions over actions -- maximizing some notion of expected (discounted, average, or total) reward. For a fixed policy, a Markov chain (and corresponding Markov reward process) is induced over the underlying MDP. In RL, (non-)ergodicity is a property of these induced Markov chains and reward processes, so specifying a policy is necessary before one can consider (non-)ergodicity. In the paper, these issues are not discussed, resulting in lack of clarity at several critical points detailed in the questions section below. A critical issue is left unaddressed in the paper: **what is the relationship between the (non-)ergodicity of the reward process and the (non-)ergodicity of the underlying sequential decision-making process?**\n2. The variance-stabilizing transform developed in Sec. 4, though interesting, is insufficiently motivated. Importantly, the mechanism whereby it produces an ergodic, transformed reward sequence is unclear. This weakens support for the validity of the approach.\n3. Sec. 5 points out an interesting connection between risk-sensitive RL and the transformation proposed in Sec. 4, yet it remains unclear for what classes of problems (i.e., what classes of MDPs, rewards, and policies) the $\\hat{\\text{I}}$to process of Eq. (4) is a reasonable model for the reward process.\n4. The experiments are limited: performance improvements over baseline methods are observed only on the simple coin toss betting problem and Reacher; comparable performance to baseline is observed on Cartpole."
            },
            "questions": {
                "value": "Some specific questions:\n* how does your notion of (non-)ergodicity relate to that considered in (Puterman, 2014), (Sutton & Barto, 2018), and many recent works on RL theory?\n* on page 2, the reward function is defined both as $R : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$ and $R(T) = \\sum_{t_k=0}^T r(t_k)$, which appear incompatible; which is correct?\n* what is the definition of $t_k$ in Eq. (1)? if $t_k$ denotes timestep, then what is $r(t_k)$ and how does it relate to $\\mathcal{S}$ and $\\mathcal{A}$?\n* what is the expectation in Eq. (2) taken w.r.t.?\n* what is the action space in Sec. 2.1? what policies were used in the experiment in Fig. 1?\n* what is $T$ on the LHS of the equation in Def. 1 on page 2? what is the role of policies in Def. 1?\n* what is the role of actions and/or policies in Eq. (3)?\n* why is the variance-stabilizing transform proposed in Sec. 4 an ergodic transform, even if only approximately?\n* under what conditions on the underlying problem is Eq. (4) a good reward model?\n* the last two paragraphs of Sec. 5 are deflating; is the proposed ergodic transform not useful for risk-aware RL?\n* the test phases in Fig. 4 appear to perform comparably for both methods -- what does this mean?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5611/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5611/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5611/Reviewer_R8mx"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5611/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698802455253,
        "cdate": 1698802455253,
        "tmdate": 1699636578810,
        "mdate": 1699636578810,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XvFEv2sg9o",
        "forum": "r7OB810eaP",
        "replyto": "r7OB810eaP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5611/Reviewer_txDk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5611/Reviewer_txDk"
        ],
        "content": {
            "summary": {
                "value": "The paper makes an interesting point that while RL practices aim at maximizing expected value of the accumulated reward, this assumption is only guaranteed to be valid if the system is ergodic but this is not the case even in a rather simple toy stochastic settings (a coin toss game). Thus ergodicity, i.e., the property that the average accumulated reward over the statistical ensemble of infinitely many trajectories agrees with the average over a single but infinitely long trajectory, is defined as a desirable attribute of such systems. In rather simple, hand-crafted  settings it is possible due to prior work to find transformation of payoffs such the new system is ergodic. However, since this approach cannot be guaranteed beyond such setting the paper next opts out from simpler to satisfy proxies such variance/second moment stabilizing transforms, which have also been considered in prior works. Adapting the payoffs of a PPO agent accordingly in the coin toss game improves its performance. Then the paper examines connections between ergodicity and risk aversion based on the assumption that a specific class of risk-sensitive transformation extracts an ergodic observable. A calculation shows that these assumptions are valid only if the rewards grow logarithmically in time, which does not hold for the guiding example of the coin toss games which has exponential costs. The paper ends with another couple of simple RL settings (e.g. cart-pole and reacher) showing that the ergodic transformation can improve the performance of vanilla REINFORCE."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper brings interesting ideas from the recently evolving area of ergodicity economics to RL.  It is an intriguing connection that most likely will be a totally novel perspective for most of the ICLR audience. The argument made by the authors is easy to follow with simple examples to build intuition. The presentation does not require effectively any prerequisites in ergodic theory, random dynamical systems or even RL."
            },
            "weaknesses": {
                "value": "On the antipode of the easy to read part of the paper, at some times I feel the paper makes choices that reduce its value as a stand alone piece of work. For example, the paper keeps referencing to recent works by Peters et al. as an important precursor if not originator of the major ideas in the current paper. As not an expert in this line of research, I am not very confident about the added value of this line of work. \nSimilarly, despite the promises about the advantages of the technique, its RL applications seem somewhat underwhelming and as the authors themselves point out constitute more of a proof of concept than a smoking gun that this technique actually delivers. The risk sensitive transformation is not applied successfully in any setting."
            },
            "questions": {
                "value": "Q1. In your approach you only consider summation of costs instead discounted summation, which I think as you report in the paper results into numerical issues. Is this approach applicable under the standard case of discounted rewards?\n\nQ2. Even if ergodicity is true this implies than an idealized infinite length orbit captures the statistics of the whole system. But real life samples are of course finite and without discounting trajectories have to be \"cut\" artificially even if the system could go on forever to avoid overflows. What conditions allow for fast convergence guarantees and would be expect these conditions to be a good match for RL settings?\n\nQ3. The current approaches seem to fit either the case of exponentially fast increasing rewards or logarithmic slow increasing rewards. What about the in between values such as linear/polynomial rewards?\n\nQ4. Which are the thorniest obstacles when scaling these ideas to more complex RL benchmarks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5611/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699058290359,
        "cdate": 1699058290359,
        "tmdate": 1699636578695,
        "mdate": 1699636578695,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7eEaEG46XC",
        "forum": "r7OB810eaP",
        "replyto": "r7OB810eaP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5611/Reviewer_trX8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5611/Reviewer_trX8"
        ],
        "content": {
            "summary": {
                "value": "This paper begins by addressing the distinction between non-ergodicity and ergodicity in reward functions within the context of Reinforcement Learning (RL). It proceeds to introduce an algorithm for transforming rewards, particularly effective in non-ergodic reward environments. Additionally, the paper explores the relationship between optimizing time-average growth rates instead of expected values and discount factors. The proposed approach is supported by a concise experimental section for validation."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The paper is well-written for the most part; however, there are a few inaccuracies in the figures descriptions that should be addressed.\n\n* The concept of transforming rewards from non-ergodicity to ergodicity is intriguing and appears to be well-founded."
            },
            "weaknesses": {
                "value": "* Could the author clarify the meaning of 't_k'? If it represents the 'k'-th step within one episode, then the use of 'R(t)' in Figure 4(a) should be reconsidered, as this should represent the cumulative rewards of each episode. It is essential for the author to provide a clearer explanation regarding the interpretation of the vertical axis.\n\n* The author's analysis focuses solely on the case of an exponential distribution. However, given the multitude of reward functions in simulations or real environment, it becomes challenging to determine whether they exhibit ergodicity. A broader exploration of different reward functions would enhance the paper's comprehensiveness."
            },
            "questions": {
                "value": "* Could the author provide a more detailed explanation for the observed difference in improvement between the Reacher and Cartpole environments? Does this suggest that the reward dynamics in Cartpole are ergodic, while they are non-ergodic in Reacher?\n\n* In the Reacher environment, it's noted that the reward in the testing phase is significantly lower than during training. Could the author clarify whether there are differences in the parameters between Reacher's test and training environments?\n\n* It would be beneficial if the author could elaborate on the specific scenarios in which this reward transformation is most beneficial. Is it guaranteed to yield a better policy when applied?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5611/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699074646020,
        "cdate": 1699074646020,
        "tmdate": 1699636578584,
        "mdate": 1699636578584,
        "license": "CC BY 4.0",
        "version": 2
    }
]