[
    {
        "id": "W7LbP6dh6D",
        "forum": "9OevMUdods",
        "replyto": "9OevMUdods",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7061/Reviewer_SmAT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7061/Reviewer_SmAT"
        ],
        "content": {
            "summary": {
                "value": "This work presented a benchmark containing 20K diverse factual questions that span different sources, timelines, domains, regions, and languages. This work also thoroughly investigated different sizes and types of LLMs on this benchmark and reported their findings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This work presented a comprehensive benchmark for testing LLMs' factual and commonsense knowledge.\n\nThis work annotated 20K factual questions from seven diverse tasks, which provides a good testbed for the community.\n\nThe analyses presented are solid and insightful."
            },
            "weaknesses": {
                "value": "Actually, I am satisfied the most part of this paper and willing to raise my score if the author can cover the following points:\n\nSome points in the main results could be verified through better experiment settings, such as the point that \"LLMs without instruction tuning underperform those with instruction tuning by 16.0%\". It's better to conduct a peer-to-peer comparison since the instruction tuning is not the only factor of the two groups. For example, we can compare LLaMA with Alpaca and Vicuna, OPT vs. OPT-IML, T5 vs. Flan-T5, BLOOM vs. BLOOMz. I am not saying that we need to compare all of them, but the comparison could be made between the instruction tuning model and its backbone. \n\nFor the \"Using the CoT method, we observed a relative boost in performance in LLMs subjected to\ninstruction tuning and RLHF\", the tested models may not have strong CoT ability. It's good to see LLaMA-2 or GPT-4 and other recent models that claim to have strong reasoning abilities. Since CoT is not a generic ability that every LLM has. \n\nBesides, you can include an additional metric to report the cases that LLMs failed to generate meaningful answers. Since LLM may refuse to answer according to security or other customized policies, treating all unanswered questions as wrong predictions may be unfair. \n\nSec-4.2 could be improved, especially the discussion around temporal analysis, \" GPT-3.5-Turbo exhibits superior performance when dealing with outdated data as compared to updated data.\", I think the numbers are close, and it may not be sufficient to support the claim. \n\nThere are some missing references for testing LLM's factuality. \n\nSelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models\nFActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation\nFacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios"
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7061/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698304459845,
        "cdate": 1698304459845,
        "tmdate": 1699636830947,
        "mdate": 1699636830947,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zVC7YqzaTc",
        "forum": "9OevMUdods",
        "replyto": "9OevMUdods",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7061/Reviewer_9WHz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7061/Reviewer_9WHz"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a benchmark named Pinocchio to research if LLMs learn factual knowledge correctly from seven aspects. The authors gather the text containing rich factual knowledge from existing datasets. Questions and answers are annotated manually based on the gathered text. With this proposed benchmark, the authors evaluate current popular LLMs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Identifying the drawbacks of LLMs in terms of the learned factual knowledge is important. The proposed benchmark contains questions from different aspects, which could encourage researchers to find the specific issues of LLMs and explore how to address them."
            },
            "weaknesses": {
                "value": "1. Data leaky may happen. Note that the questions and answers are annotated based on the text of previous public datasets. The pretraining data of evaluated LLMs is likely to contain the text. This may cause data leaky, and LLMs might have memorized/learned shortcuts to answer the question. It makes the evaluation results on the benchmark less convincing.\n\n2. Overclaimed contribution (as well as title) to the research question. I think the main contribution of this paper is the newly proposed benchmark. In the experiment, the authors research the extent and scope of factual knowledge within LLMs. However, the authors only test LLMs on the benchmark. There are no baselines (LLMs, datasets) as a comparison. Only showing accuracy numbers is not sufficient to give the answer to this research question."
            },
            "questions": {
                "value": "1. May I ask how you design the seven aspects for evaluation? Any high-level intuition?\n\n2. Can 92.4% annotation accuracy (as well as 85.6% inter-annotator agreement rate) promise the quality of the dataset? Does that mean when the test accuracy of LLMs is larger than ~= 85%, it's meaningless to use this benchmark for evaluation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7061/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698580039978,
        "cdate": 1698580039978,
        "tmdate": 1699636830757,
        "mdate": 1699636830757,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7l76qLi5Yh",
        "forum": "9OevMUdods",
        "replyto": "9OevMUdods",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7061/Reviewer_1LZe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7061/Reviewer_1LZe"
        ],
        "content": {
            "summary": {
                "value": "1. Lack of datasets to evaluate LLMs ability to hold and reason over factual knowledge is the primary motivation of the paper to come up with a new benchmark called Pinocchio benchmark. This benchmarks can evaluate LLMs in tasks that needs to know factual knowledge from diverse set of domains, multiple facts to reason, and the facts may be present in different information sources. The work also performs varied experiments and analysis on factuality or non factuality for multiple large language models. The experiments and conclusions are presented well. \n    2. Datasets: \n        1. Most datasets have evidence but Pinnachio only cares yes or no answers without any evidence, specifically multiple choice question answers. More explanation on how this is a good strategy to conclude if LLMs are able to memorize and use facts is unclear from the paper.\n        2. There is no clear motivation in the paper on why these datasets were chosen \u2014 are these the only specific 7 categories of questions? Or was it the distribution of the known datasets and the data points?\n        3. Evaluating answers with evidences has led to better understanding on the performance of large language models. It will be good to compare performances of the large language models on the datasets vs how they are transformed for this setting\n    3. Conclusions from this work:\n        1. The work also performs varied experiments and analysis on factuality or non factuality and comes up with the following conclusions:\n            1. Instruction tuning performs better than non-instruct tuning\n            2. Few shot with COT > Few shot > Zero-shot with COT > Zero Shot\n            3. The number of hops in multi-hop reasoning dictates the performance decrease of GPT\n            4. LLMs have limited ability to use structured data\n            5. Numerical reasoning is harder for LLMs\n            6. LLMs are unable to use the latest data (temporal) but are able to answer with older data (trained on) \n        2. Comments: \n            1. These conclusions hold for Large Language Models in a more generic sense rather than just factuality. It\u2019s important for the authors to clarify the important of new conclusions in comparison on what already exists. Explanations of how these are different when made on the same datasets before deriving them to a multiple-choice QA dataset."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. An annotated dataset of 20k that can evaluate factuality of large language models.\n2. Work is presented well with experiments and conclusions."
            },
            "weaknesses": {
                "value": "1. Data is derived from existing datasets but the motivation for transformation is not clearly specified\n2. Most conclusions of the experiments are already made in isolation prior to this work. The novelty in experiments and conclusion should be well specified."
            },
            "questions": {
                "value": "Already in the summary"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7061/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699110054872,
        "cdate": 1699110054872,
        "tmdate": 1699636830646,
        "mdate": 1699636830646,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "29FKDF5JWE",
        "forum": "9OevMUdods",
        "replyto": "9OevMUdods",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7061/Reviewer_XTL4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7061/Reviewer_XTL4"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a benchmark that specifically evaluates the parametric knowledge acquired by LLMs during pretraining. The authors collected and recast existing knowledge-intensive datasets such as fact checking (e.g., FEVER) across seven domains:\n- Multiple-facts (e.g., multiple supporting facts)\n- Structural (e.g., with tables)\n- Adversarial (e.g., unrelated facts)\n- Temporal (e.g., Wikipedia revisions) \n- Real-world (e.g., politifact)\n- Domain-specific (e.g., medical and science)\n- Multi-lingual (e.g., French, Chinese, and more)\nAll examples are formulated as multiple-choice problems with three classes. 10 human annotators rewrote the original claims into questions while preserving the original facts and their labels. \n\nThe experiment setup primarily focuses on LLMs with prompting (zero-shot, few-shot, w/CoT). The major pretrained LLMs are evaluated on this benchmark, including public LLMs such as OPT, BLOOM, and LLaMA and commercial LLMs such as GPT3 and ChatGPT. Overall, ChatGPT and GPT3 outperform other LLMs by large margin, regardless of the prompting methods used. The fine-grained results show that the commercial LLMs are strong on many domains but underperform other LLMs on the temporal domain. Additionally, the authors performed analysis on different aspects such as question types and performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This paper investigates factual knowledge in LLMs, which is a well-motivated problem.\n- A comprehensive benchmark covering different aspects of factual knowledge is proposed, particularly suitable for prompting."
            },
            "weaknesses": {
                "value": "Dataset Design: _\u201c...multi-choice questions are a simple but good proxy to evaluate the potential of advanced abilities of LLMs\u2026\u201d_: I think this claim should be supported by prior work. I tend to disagree with this statement as Chen and Durrett (2019) concludes that, for some datasets, multiple-choice questions could be easily gamed (i.e., spurious correlations between questions and labels).  Link: https://arxiv.org/pdf/1904.12106.pdf\n\nA significant number of related papers have not been cited. There are numerous strands of prior work that investigate the factual knowledge and reasoning abilities of large language models (LLMs), including multi-hop reasoning, temporal and geographical questions, new entities, and complex inference. (I\u2019m listing a few of them here)\n- Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies (https://arxiv.org/abs/2101.02235) \n- SituatedQA: Incorporating Extra-Linguistic Contexts into QA (https://arxiv.org/abs/2109.06157)\n- A Dataset for Answering Time-Sensitive Questions (https://arxiv.org/abs/2108.06314)\n- Entity Cloze By Date: What LMs Know About Unseen Entities (https://arxiv.org/abs/2205.02832)\n- RealTime QA: What's the Answer Right Now? (https://arxiv.org/abs/2207.13332)\n- StreamingQA: A Benchmark for Adaptation to New Knowledge over Time in Question Answering Models (https://arxiv.org/abs/2205.11388)\n- Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge (https://arxiv.org/abs/2305.01651)\n- FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation (https://arxiv.org/abs/2310.03214)"
            },
            "questions": {
                "value": "- _\u201c...but in Pinocchio, we only need to judge the factuality of the question.\u201d_ Does this mean all supporting facts are not included in the questions?\n\nMinor:\n- The terms \u201cMultifaceted\u201d and \u201cmultifacted\u201d are used interchangeably. Is this a typo?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7061/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7061/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7061/Reviewer_XTL4"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7061/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699231245185,
        "cdate": 1699231245185,
        "tmdate": 1700760814009,
        "mdate": 1700760814009,
        "license": "CC BY 4.0",
        "version": 2
    }
]