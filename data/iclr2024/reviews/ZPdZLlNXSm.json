[
    {
        "id": "Fgo9BKpaLr",
        "forum": "ZPdZLlNXSm",
        "replyto": "ZPdZLlNXSm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2363/Reviewer_q1qw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2363/Reviewer_q1qw"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the mean field theory is introduced into the domain of deep metric learning. By incorporating foundational components such as the Contrastive loss and Class Wise Multi-Similarity loss, the authors construct the Mean Field Contrastive loss and Mean Field Class Wise Multi-Similarity loss. The proposed method is evaluated through extensive experiments on benchmark datasets, covering two benchmark protocols. The results demonstrate the effectiveness of the proposed approach."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The authors have integrated the mean field theory into the realm of deep metric learning. \n\n2. They have introduced two pair-based loss functions, namely the Mean Field Contrastive loss and the Mean Field Class Wise Multi-Similarity loss. \n\n3. The experiments and ablation studies conducted in the paper are comprehensive."
            },
            "weaknesses": {
                "value": "Deep metric learning involves a range of loss functions, and it is unclear whether the mean field theory can be applied to other loss functions commonly used in this context. It would be valuable for the authors to specify under what conditions and contexts the mean field theory is applicable and offer practical guidance for its implementation in other scenarios."
            },
            "questions": {
                "value": "How can the mean field theory be extended to encompass other commonly used loss functions in deep metric learning?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2363/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2363/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2363/Reviewer_q1qw"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2363/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698305304405,
        "cdate": 1698305304405,
        "tmdate": 1699636168926,
        "mdate": 1699636168926,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "z4OVPAtfIT",
        "forum": "ZPdZLlNXSm",
        "replyto": "ZPdZLlNXSm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2363/Reviewer_pubU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2363/Reviewer_pubU"
        ],
        "content": {
            "summary": {
                "value": "The paper considers mean field theory approximations of pair-based loss functions for metric learning. Inspired by techniques in statistical physics, pairwise calculations are approximated by comparing to a mean approximation, ie, turning a summation over $i$ and $j$ to only that of $i$. Using such an approach, two different mean field contrastive loss functions are proposed. Empirically, the proposed loss functions are evaluated and are shown to even out perform their non-mean field counterparts."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The approximation technique for reducing pairwise summations to their mean field approximation intuitively makes sense. This concept is particularly well illustrated in Figure 1.\n- The proposed approach seems promising. Surprisingly, the mean field approximations perform better than their non-mean field counterparts in many circumstances."
            },
            "weaknesses": {
                "value": "- Part of the motivation for the approximation (not including its statistical physics analogy) was the reduction in runtime complexity pair-based loss function in metric learning. However, there is no runtime values reported in the paper.\n- Some terms in the paper are not explained. (See questions below)."
            },
            "questions": {
                "value": "- What are the runtimes of the mean field variants compared to their regular counterparts? Does the additional complexity of requiring optimization of mean fields $\\mathbf{M}_c$ outweigh the reduction of computational complexity via the mean field approximation?\n- One of the major interesting components of the paper is that mean field approximation performs better than its non-mean field counterparts. Is there a good hypothesis for why this may be the case? Have you found a good characterization of when one out performs the other? Additional insight here would be great since the paper claims in the empirical section that the mean field losses provide better \"training complexity but also results in better embeddings\".\n- I am unsure exactly what the authors mean be \"resummation\" and \"unstable terms\". Clarification here would be great.\n- Furthermore, what do that authors mean by \"... the above discussion implies the mean field theory is independent of the concept of anchors ...\"?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2363/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698725908333,
        "cdate": 1698725908333,
        "tmdate": 1699636168852,
        "mdate": 1699636168852,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fnYKuslAGb",
        "forum": "ZPdZLlNXSm",
        "replyto": "ZPdZLlNXSm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2363/Reviewer_uk7M"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2363/Reviewer_uk7M"
        ],
        "content": {
            "summary": {
                "value": "The manuscript proposes two new metric learning algorithms inspired by mean-field analysis from physics. In pair-based algorithms, the loss function pushes pairs of the same class to be closer and pairs of different classes to be apart, and need to be calculated over many training pairs; in contrast, for the proposed mean-field approach, the loss pushes each sample to be close to the class mean and away from other classes' mean, and further pushing class means away from each other. The underlying technical derivation is quite general, through taking a derivating of an energy function. Thus, the same method can be applied to other cases, such as proposing a loss function using class means in a minibatch setting."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* Regardless of any inspiration taken from physics, replacing pair-based methods with mean-based methods seems a scalable approach, well grounded in statistics.\n * The proposed class of methods is possibly prudent in the sense that they can be used to derive loss functions, taking into account mean-class information for other problems.\n * The simulations performed seem great, and the authors explain the optimisation carried out on both their method and other methods used for comparison. The results suggest the proposed class of methods achieve very competitive results. \n * The writing is clear, almost tutorial-like, and easy to follow."
            },
            "weaknesses": {
                "value": "* The authors hide the actual derivation in the appendix, so they do not detail enough their technical approach. On the face of it, the modified loss function could have been suggested just through statistical intuition, not derived from an energy approximation (Hubbard-Stratonovich, saddle-point approx, etc.), so it is a pity the authors don't sketch their methodology in the main text.\n * It is hinted that the method can be more efficient (e.g. due to the lack of pair sampling or anchor points choice), but it is not reported if the training time is superior to the other methods or if the optimisation over `M`-s causes a substantial overhead (which may be justifiable with the improved results)."
            },
            "questions": {
                "value": "* Is it always favourable to optimize the means `M` and the parameters `theta` together? Maybe alternating between optimisation steps would be superior in numerical terms.\n * The method seems similar to classic soft-clustering approaches, which were optimised by alternating between two optimisation steps, such as the EM algorithm. Can you make the connection more explicit?\n * Are the new methods faster or slower than other methods?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2363/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698793053283,
        "cdate": 1698793053283,
        "tmdate": 1699636168780,
        "mdate": 1699636168780,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dFCz5yFHUd",
        "forum": "ZPdZLlNXSm",
        "replyto": "ZPdZLlNXSm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2363/Reviewer_aqYA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2363/Reviewer_aqYA"
        ],
        "content": {
            "summary": {
                "value": "This paper explores the mean field theory into metric learning  by designing two loss functions to train deep neural networks. The model's performance is evaluated on various benchmarks, including CUB, Cars, and SOP. While the paper is generally easy to follow, it lacks a sufficient level of novelty and performance improvement."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper explores the mean field theory into metric learning  by designing two loss functions to train deep neural networks. The model's performance is evaluated on various benchmarks, including CUB, Cars, and SOP, by comparing several other methods. The paper is generally easy to follow."
            },
            "weaknesses": {
                "value": "The major concern is that the paper lacks a sufficient level of novelty and performance improvement.\n\nFirst, they mainly explore mean filed theory into metric learning. Such metric is close to central loss [R1]. \n[R1]. Wen, Yandong, et al. \"A discriminative feature learning approach for deep face recognition.\" Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11\u201314, 2016, Proceedings, Part VII 14. Springer International Publishing, 2016.\n\nSecond, the performance is not significant. In table I, compared with ArcFace and ProxyAnch, it is hard to justify the significant improvement. It is essential to do t-test."
            },
            "questions": {
                "value": "The clarification of model novelty.\nThe performance improvement."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2363/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698807235568,
        "cdate": 1698807235568,
        "tmdate": 1699636168674,
        "mdate": 1699636168674,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TiLZ1cgHMH",
        "forum": "ZPdZLlNXSm",
        "replyto": "ZPdZLlNXSm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2363/Reviewer_zMTp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2363/Reviewer_zMTp"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes new DML losses that are inspired from the Mean-Field Theory (MFT), which is a concept from statistical physics. Specifically, the authors follow the constructive loss and the multi-class loss to implement two MFT losses. Their extensive experiments demonstrate the efficiency of the new losses on popular DML benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The idea of introducing unique losses based on the theory\u00a0of statistical physics looks\u00a0interesting and novel. No prior research has taken on this particular task.\n\nThe proposed method is evaluated on several popular DML benchmarks. The authors evaluate their method on advanced MLRC metrics, making their results convincible."
            },
            "weaknesses": {
                "value": "My major concern is that the proposed theory does not seems to be solid when it is applied on DML task. There is not enough theoretical clue that the mean-field theory (MFT) would directly benefit the DML task compared with the proxy-based losses. The authors should provide more analysis to explain the intrinsic connection between the interaction between the magnetic spin and the similarity (distance) between the data points in DML task.\n\nThe relation and comparison between the proposed loss and proxy-based loss is still not clear. The intuition behind the MFT looks similar to the proxy-based loss, where they both compare a sample with an anchor instead of all class members. Thus, a systematic compare between it and other close related losses should be provided. \n\n\nIt seems the computation complexity to get the mean field in Eq.5 is higher than compared with the proxies. Thus, a comparison of performance and running time may be essential to be discussed. To further clarify its arithmetic progress and complexity, it is also suggested to list the pseudo-code of the loss.\n\n\nExperimental results show that the improvement in some datasets is not significant (as illustrated in table 2, figure 2). But this is not a big issue for me."
            },
            "questions": {
                "value": "Is there any assumption or internal connection between the point distances and the spin configuration?\n\nPlease respond to the above weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2363/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699171185857,
        "cdate": 1699171185857,
        "tmdate": 1699636168583,
        "mdate": 1699636168583,
        "license": "CC BY 4.0",
        "version": 2
    }
]