[
    {
        "id": "UcdDbYwaKF",
        "forum": "nhub8Pjp7y",
        "replyto": "nhub8Pjp7y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8984/Reviewer_zL4D"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8984/Reviewer_zL4D"
        ],
        "content": {
            "summary": {
                "value": "This paper explore the secure risk of parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs) toward trojan attacks. Specifically, the authors present a novel attack PETA that accounts for downstream adaptation through bilevel optimization: the upper-level objective embeds the backdoor into a PLM while the lower-level objective simulates PEFT to retain the PLM\u2019s task-specific performance. Bedise, the authors also propose a fine-tuning method to defense the PETA attack. Emperically, the authors show the effectiveness of the proposed attack method and the defense method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Exploring the risk of PEFT toward trojan attacks is valuable.\n- The authors propose the paired attack-defense method to promote the exploration of PEFT toward backdoored security.\n- The authors run numerous experiments across a variety of downstream tasks and trigger designs to empirically verify the effectiveness of the proposed attack method."
            },
            "weaknesses": {
                "value": "My major concern is that the proposed method in this paper needs strong assumptions:\n- The authors assume that the attacker is equipped with knowledge about (i) the downstream task and (ii) the PEFT method used by the user. Based on these strong assumptions, the authors design the attack method through bilevel optimization, where the upper-level objective embeds the backdoor into a PLM and the lower-level objective simulates PEFT to retain the PLM's task-specific performance. The feasibility of the bilevel-optimization-based attack is heavily targeted and relies on the assumed downstream task and the PEFT method. \n- However, in practice, the attackers are hard to know (or limit) both the downstream task and the PEFT method used by the users/defenders in advance. Thus, the assumption is too strong and impractical. If the users/defenders choose a novel PEFT method or change the downstream task, I believe the proposed attack method is hard to work.\n- The authors could run experiments to explore the effectiveness of the proposed method toward unseen downstream task/PEFT methods. I'm interested in the transferability of the proposed trojan attack method."
            },
            "questions": {
                "value": "Please see the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8984/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8984/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8984/Reviewer_zL4D"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8984/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698504007605,
        "cdate": 1698504007605,
        "tmdate": 1700641164375,
        "mdate": 1700641164375,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BnSthEjtQG",
        "forum": "nhub8Pjp7y",
        "replyto": "nhub8Pjp7y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8984/Reviewer_tYon"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8984/Reviewer_tYon"
        ],
        "content": {
            "summary": {
                "value": "This work concerns trojan attack in PLM and present PETA. It contains two stages: (1) bilevel optimization, which inserts the backdoor into a general-purpose pre-trained language model and is conducted by attacker and (2) parameter-efficient fine-tuning on a clean dataset, which is conducted by user."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This work is the first to study backdoor attack for PEFT.\n\n2. The experiments are sufficient and convincing.\n\n3. This work also investigates how to solve the backdoor attack from PETA."
            },
            "weaknesses": {
                "value": "1. The most important is that the motivation of the studied problem is unclear. I doubt if there are any scenarios in reality where exists corrupted PLM trained with so much (25%) poisoned data and it needs PEFT. I suggest the authors focus on discussing the motivation in introduction.\n\n2. I suggest the author add explanations of poisoned data to improve readability."
            },
            "questions": {
                "value": "1. The learning rate of DP is 10x of other baselines from Appendix. Besides, more epochs are used. Is there any explanation?\n\n2. I wonder how would the poisoned rate affects the results?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8984/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8984/Reviewer_tYon",
                    "ICLR.cc/2024/Conference/Submission8984/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8984/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698547158431,
        "cdate": 1698547158431,
        "tmdate": 1700723618680,
        "mdate": 1700723618680,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5jkUN5QwbN",
        "forum": "nhub8Pjp7y",
        "replyto": "nhub8Pjp7y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8984/Reviewer_EEAs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8984/Reviewer_EEAs"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on trojan/backdoor attacks in the parameter efficient fine-tuning (PEFT) setting, which is an important research topic. The authors propose PETA, a novel attack to inject the backdoor into the PLM using bilevel optimization. Extensive experiments demonstrate the effectiveness of PETA. The authors also discuss potential countermeasures."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- important topic\n- well-written paper\n- effective attacks and countermeasures"
            },
            "weaknesses": {
                "value": "- technical novelty is limited\n- unclear attack description\n- more evaluations are needed"
            },
            "questions": {
                "value": "- My main concern is that the proposed attack leverages existing backdoor attack methodology in the scenario of PEFT, making the technical novelty limited. Please correct me if I am wrong. In both Eq. 2 and Eq. 3, the whole model's parameters seem to be updated (including $\\theta$ and $\\delta$), which is the same as the backdoor attack in the fine-tuning stage. After the attack, $\\delta$ will be discard and a new $\\delta$ will be trained (with $\\theta$ being fixed) by the victim user to perform the downstream task. I appreciate it if the authors could better clarify the advantage of the proposed attack compared to previous attacks and discuss the attack process more clearly.\n\n- Regarding the evaluation, it seems that BadNet can also achieve both high ACC and high LFR. Would it be the case if we discard the classifier $\\delta$ backdoored by BadNet and train a new $\\delta$?  \n\n- During the backdoor process, the dataset is from the same distribution as the testing data, which is a relatively strong assumption. I would suggest the authors also evaluate the attack performance when the downstream dataset is from a different distribution than the attack dataset.\n\n- I like the authors' idea regarding the defense. As shown in Fig. 3, the LFR can be largely reduced if we could select the optimal layer. However, it would be hard to select the optimal one. Previous work[a] also suggests that fine-tuning the whole model could be an effective defense. Would it be possible to make a trade-off by fine-tuning the last few layers?\n\n[a] https://arxiv.org/abs/2212.09067"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8984/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8984/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8984/Reviewer_EEAs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8984/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698661309941,
        "cdate": 1698661309941,
        "tmdate": 1700635895409,
        "mdate": 1700635895409,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XLjwNpsJGB",
        "forum": "nhub8Pjp7y",
        "replyto": "nhub8Pjp7y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8984/Reviewer_oWxS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8984/Reviewer_oWxS"
        ],
        "content": {
            "summary": {
                "value": "This work reveals that Parameter-efficient fine-tuning (PEFT) exhibits unique vulnerability to trojan attacks. A novel attack called PETA was presented that accounts for downstream adaptation through bilevel optimization: the upper-level objective embeds the backdoor into a PLM while the lower-level objective simulates PEFT to retain the PLM\u2019s task-specific performance. Extensive evaluation across a variety of downstream tasks and trigger designs demonstrate PETA\u2019s effectiveness in terms of both attack success rate and unaffected clean accuracy, even after the victim user performs PEFT over the backdoored PLM using untainted data."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed trojan attack under Parameter-efficient fine-tuning (PEFT) setting is interesting and practical.\n2. the experimental results seems promising.\n3. The paper is generally well motivated and written."
            },
            "weaknesses": {
                "value": "1. How complex is the bilevel optimization?\n2. Baselines are all old ones back to 2-5 years before.\n3. The defense part is bit weak, just considered a simple one.\n4. Some highly relevant works on backdoors are missing:\nFine-mixing: Mitigating Backdoors in Fine-tuned Language Models\nBackdoor attacks on self-supervised learning\nASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep Learning Paradigms\nReconstructive Neuron Pruning for Backdoor Defense\nAnti-Backdoor Learning: Training Clean Models on Poisoned Data\nNeural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks"
            },
            "questions": {
                "value": "see the Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8984/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698740883964,
        "cdate": 1698740883964,
        "tmdate": 1699637130577,
        "mdate": 1699637130577,
        "license": "CC BY 4.0",
        "version": 2
    }
]