[
    {
        "id": "6opcRn0jqh",
        "forum": "FvqGxAyCpi",
        "replyto": "FvqGxAyCpi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6963/Reviewer_oAHD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6963/Reviewer_oAHD"
        ],
        "content": {
            "summary": {
                "value": "-This paper studies federated learning, with a specific focus on training a global model that can adapt to arbitrary target distribution. The authors propose a self-supervised learning based approach that aggregates the client models through a reweighting mechanism, where the weights are learned by maximizing a similarity metric (cosine similarity in the paper) between predictions generated from different transformation views (e.g., varying light conditions). They also conduct experiments on standard ML benchmark datasets and provide an ablation study."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "-The paper is well-written and easy to follow. The authors conduct a thorough literature review and provide a comparison of their proposed method against a substantial set of baseline approaches."
            },
            "weaknesses": {
                "value": "-My first concern is about the novelty of the paper given that using self-supervised learning ideas to tackle the non-IID data challenges in federated learning is not new, for example, see the following:\n\nDivergence-aware federated self-supervised learning, ICLR 2022\n\nFederated Self-supervised Learning for Heterogeneous Clients. arXiv 2022\n\n-The title of the paper (given by the paper pdf but not the one on the openreview page) places an emphasis on \u201carbitrary target distribution\u201d which appears to be a bit over-claimed to me. In fact, the paper only empirically demonstrates that the proposed FedSSA method works for a few heterogeneous settings where the target distribution undergoes some standard distribution shifts, i.e., class-prior shift (see definitions in Dataset Shift in Machine Learning, The MIT Press). \n\nThere is no theoretical analysis on how the proposed FedSSA method can cater **arbitrary target distribution**, or even to what extent the proposed FedSSA method can be robust to the target distribution shift. In Appendix A the authors analyzed the convergence of the proposed method, however, with an extreme target distribution shift, Assumption A.4 can never hold. Think of the target distribution shift problem in standard centralized ML, generalization to the target distribution always depends on the difference of cross-domain distributions and cannot work for arbitrary target distributions (see the following paper for a reference). Therefore the analysis for FedAvg **without** target distribution shift cannot work for the analysis for FedSSA **with** distribution shift.\n\nGeneralizing to Unseen Domains: A Survey on Domain Generalization, TKDE 2022"
            },
            "questions": {
                "value": "-I wonder the rationale behind employing a reweighting-based approach for aggregating client models. Is there a particular reason for not considering a sub-sampling strategy to select clients for aggregation in each round instead?\n\n-How can the novelty of the current paper be justified in the context of previous research that has also explored the application of self-supervised learning ideas to address non-IID data challenges in federated learning?\n\n-What are the theoretical guarantees that support the proposed FedSSA method? How can it be ensured that this method is effective for a wide range of target distributions, even **arbitrary target distributions**?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6963/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698675408291,
        "cdate": 1698675408291,
        "tmdate": 1699636813330,
        "mdate": 1699636813330,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "nMMZsvZ8t2",
        "forum": "FvqGxAyCpi",
        "replyto": "FvqGxAyCpi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6963/Reviewer_Q6HC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6963/Reviewer_Q6HC"
        ],
        "content": {
            "summary": {
                "value": "This study introduces a novel Federated Learning (FL) setup referred to as \"Pragmatic Federated Learning.\" In this setup, the primary objective of the server is to develop a global model that is specifically geared towards the unlabeled data stored on the server. This is accomplished through the \"Self-Supervised Aggregation (FedSSA)\" method.\n\nUnder FedSSA, the server maintains fixed local model parameters while performing two key tasks. First, it aggregates the predictions made by the individual local models. Second, it learns aggregation weights using a self-supervised loss function, which is applied to the unlabeled dataset residing on the server. This approach enables the server to iteratively refine and enhance the global model towards the unlabeled data."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The writing is clear in the description\n* The experiments provide enough details and ablation studies."
            },
            "weaknesses": {
                "value": "* I am not very convinced by the motivation of the new proposed setup. The setting assumes that the decentralized labeled data and the server unlabeled data are from the same domain but just differ in their label distribution, at least as suggested by the current experiments. This doesn't sound very well motivated for practical applications. Even in centralized learning literature, I did not find much research on this; stronger motivation will be needed for pragmatic  FL.  \n\n* The proposed approach is a bit disconnected from the setup. If only the label distributions are different, likely it does not need to change the whole model but only calibrate the classifier based on the target label distribution p(y). It is not obvious to me why should we learn the aggregation weights. \n\n* The idea of leveraging unlabeled data in the server for distillation or aggregation is the most relevant technique, but many previous works are missing in comparison and discussion such as [i, ii, iii]\n\n[i] Decentralized Learning with Multi-Headed Distillation, CVPR 2023  \n[ii] Data-Free Knowledge Distillation for Heterogeneous Federated Learning, ICML 2021  \n[iii] FedBE: Making Bayesian model ensemble applicable to federated learning, ICLR 2020"
            },
            "questions": {
                "value": "* Since it is in a federated setting, how can we know if the server dataset and the decentralized data share enough similarity to ensure the pseudo labels are meaningful? What if they are from different domains?\n\n* Since the target data are actually seen on the server, what is the difference in test time adaptation applied to FL?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6963/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698801834139,
        "cdate": 1698801834139,
        "tmdate": 1699636813205,
        "mdate": 1699636813205,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RHfsmka7Eb",
        "forum": "FvqGxAyCpi",
        "replyto": "FvqGxAyCpi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6963/Reviewer_ovj7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6963/Reviewer_ovj7"
        ],
        "content": {
            "summary": {
                "value": "The author consider FL to address an unknown target distribution and propose a self-supervised aggregation scheme that satisfies transformation invariance. The authors provide substantial experiments on $12$ datasets. \n\nI have substantial concerns regarding the developed method and lack of any sort of guarantees, which shows there is a mismatch between the claims in the paper and the actual results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Providing experiments on several datasets and a number of classical FL baselines are among strengths of this paper."
            },
            "weaknesses": {
                "value": "One major issue with the considered setting in this paper is the underlying assumption that there is a unique and fixed target distribution ${\\cal D}_T$. In fact, each client may have their own target distribution which may evolve over time. \n\n----------\n\nThe formulation (2) is unclear. The objective is over $w_k$'s while ${\\theta_k}$'s are constant for $k=\\{1,\\cdots,k\\}$ . This objective does not provide any guarantees in terms of closeness of the solution to the solution of Eq (1).\n\nThe heuristic solution in Eq. (3) also does not provide any guarantees for the solution in Eq. (1). \n\n----------\n\nThere is \"Theorem 1\" proof in Appendix E but I cannot find the theorem statement. The paper is not backed by theoretical results since the proposed method is just a heuristics without any sort of provable connection between the developed solution and the original objective minimizer in Eq. (1). \n\n----------\n\nOverall, I think this paper lacks of any sort of guarantees, which shows there is a mismatch between the claims in the paper that FedSSA addresses adaptation to diverse target datasets and the actual results."
            },
            "questions": {
                "value": "Please see above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6963/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699129940300,
        "cdate": 1699129940300,
        "tmdate": 1699636813065,
        "mdate": 1699636813065,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "h9XIMr929i",
        "forum": "FvqGxAyCpi",
        "replyto": "FvqGxAyCpi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6963/Reviewer_6Ffb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6963/Reviewer_6Ffb"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes FedSSA, a novel federated learning approach that addresses the scenario where the target distribution may be different from the true one and the target data is unlabeled. FedSSA uses a self-supervised aggregation method that adjusts to specific, arbitrary data distributions of the target dataset, improving model performance. It dynamically adjusts aggregation weights for local models to ensure transformation invariance. The method is tested on 4 benchmark datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper studies a practical and novel problem, where the target distribution may be different from the training distribution, and the target data distribution is unlabeled.\n2. The proposed three terms are well-motivated, and the performance is verified by the experiments on popular benchmark datasets."
            },
            "weaknesses": {
                "value": "The main weakness is the lack of theoretical guarantees. The three essential spirits in Section 4.2 help readers understand why the proposed method may work. However, theoretical analysis is necessary to prove when the proposed method works, and when the proposed method may fail. It is important when the target label distribution is unlabeled."
            },
            "questions": {
                "value": "In Table 1, when target distributions are imbalanced, how is the accuracy calculated? Should it be an average weighted by the sample frequency of each label class?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6963/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699164932618,
        "cdate": 1699164932618,
        "tmdate": 1699636812927,
        "mdate": 1699636812927,
        "license": "CC BY 4.0",
        "version": 2
    }
]