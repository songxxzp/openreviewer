[
    {
        "id": "glWMmru4NH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6683/Reviewer_VVKB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6683/Reviewer_VVKB"
        ],
        "forum": "W2HJKGnb5y",
        "replyto": "W2HJKGnb5y",
        "content": {
            "summary": {
                "value": "This paper propose a new algorithm combined with a fitness-based randomization scheme."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The algorithm is descried very detailedly."
            },
            "weaknesses": {
                "value": "1. The language used in this paper is not good. I recommend the author to use large language models (e.g. ChatGPT) to go through your work.\n\n2. Too many tables, algorithms, and subjective comments in the paper. You should use more rigorous statements.\n\n3. Based on the current version. I think the paper is more suitable for evolution journals like TEVC/Soft computation.\n\n4. Too many irrelated sentences in the introduction. Everyone knows the property of global opt... Should make it more compact. \n\n5. Fmnist and cifar10 are just too simple. Since it is a algorithm-like paper. Just doing such weak experiments are not enough."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6683/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697176342963,
        "cdate": 1697176342963,
        "tmdate": 1699636766200,
        "mdate": 1699636766200,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2uBELEFocG",
        "forum": "W2HJKGnb5y",
        "replyto": "W2HJKGnb5y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6683/Reviewer_zXbc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6683/Reviewer_zXbc"
        ],
        "content": {
            "summary": {
                "value": "This paper introduced a new simple evolutionary algorithm to support hyper-parameter tuning while training deep neural networks. Preliminary experiment results show that the new algorithm may be useful to some extent in practice."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "Hyper-parameter tuning is important for many deep learning systems and many real-world applications. It is important to develop effective and efficient hyper-parameter tuning techniques. This paper presented a new attempt along this research direction."
            },
            "weaknesses": {
                "value": "This paper does not have strong technical novelty. The literature review was brief and did not cover many advanced tools and methods for hyper-parameter tuning or meta-learning in general. For example, the IRACE package is getting increasingly popular for hyper-parameter tuning. It remains largely questionable why it is necessary to develop a new evolutionary algorithm for hyper-parameter tuning, instead of using existing tools or technologies.\n\nThe design of the new evolutionary algorithm lacks technical novelty. It is common to use normalized fitness for individual selection in many evolutionary algorithms. It is also common to replace the worst individuals with mutated individuals. Controlling the randomness in mutation based on the performance/fitness of each individual is not new either. Furthermore, according to Algorithm 1, all individuals in the population need to be trained separately in each generation. This is computationally expensive and may not be as efficient as other gradient-based meta-learning techniques that can also fine-tune some hyper-parameters.\n\nBesides the major concern on the technical novelty, the experimental evaluation is not sufficiently strong. Given the ever-expanding literature on hyper-parameter tuning techniques, the competing methods examined in the experiment appear to be quite limited, insufficient to show that the new algorithm can achieve state-of-the-art performance in both efficiency and effectiveness. Moreover, only two relatively simple benchmark datasets were utilized in the experiment. Results obtained on the two benchmark datasets cannot conclusively show the performance advantage of the new algorithm.\n\nSome statements seem to be confusing. For example, I don't understand what the statement \"actively choosing how much to explore the parameter and hyperparameter space\" on page 2 means. It is also hard to understand the statement \"struggle against fine-tuned local search solutions\" on page 2.\n\nThe authors mentioned several limitations with the new algorithm in Subsection 2.3. It is not clear why they don't try to address these limitations, which appear to be closely relevant to the practical usefulness of the new algorithm and cannot be simply declared as future works.\n\nTypos and grammatical errors can be spotted frequently throughout the paper. The authors are highly recommended to conduct more rounds of proof-reading to significantly improve the presentation quality and clarity of this paper."
            },
            "questions": {
                "value": "Why is it necessary to develop a new evolutionary algorithm for hyper-parameter tuning, instead of using existing tools or technologies?\n\nCompared to other hyper-parameter tuning and meta-learning techniques, how efficient is the newly proposed evolutionary algorithm and why?\n\nCan the new algorithm achieve state-of-the-art performance in both efficiency and effectiveness and why?\n\nWhy didn't the authors try to address the limitations discussed in Subsection 2.3?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6683/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698537253248,
        "cdate": 1698537253248,
        "tmdate": 1699636766086,
        "mdate": 1699636766086,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lpFEPCRpk5",
        "forum": "W2HJKGnb5y",
        "replyto": "W2HJKGnb5y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6683/Reviewer_TkFw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6683/Reviewer_TkFw"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes population descent, or PopDescent - a memetic algorithm that combines local gradient-based search with a global population-based search. Local search is applied to traverse the parameter space (per individual in a population), and global search is applied to traverse hyper-parameters. The method is deliberately simple, and designed to not be sensitive to its own hyperparameters. Experiments demonstrate that the proposed method effectively optimizes both the problem and the local algorithm search parameters (regularisation, learning rate)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "In the gradient-dominated field of neural network optimisation, it is refreshing to see an approach that attempts to bring global-search algorithms to the table without severe efficiency trade-offs.\n\n**Originality:** The proposed method seems reasonably novel, although I would have appreciated a more critical comparison to other memetic algorithms of similar kind.\n\n**Quality and clarity:** The paper is easy to follow, pseudo code is provided for the proposed algorithms. Authors also provide an anonymised link to their code, which is a big plus.\n\n**Significance:** On two benchmarks (FMINST and CIFAR-10), the proposed method is shown to outperform both random parameter search and a competing memetic algorithm."
            },
            "weaknesses": {
                "value": "**Literature review:** The authors propose a new memetic algorithm, but fail to sufficiently discuss existing state-of-the-art memetic algorithms in their literature review. ESGD is briefly mentioned in the \u201cBenchmarks\u201d section, but its workings are not described or critically compared to the proposed approach. Evolutionary/population-based algorithms are plenty, and without a critical discussion of existing methods, it is hard to evaluate the originality of the proposed method. I am also not certain why authors decided to move related work discussion to just before the conclusions - this does not make for a good narrative structure, and should be moved to the beginning of the paper.\n\n**Experimentation:** In the experiments, all methods employ Adam except for ESGD. This seems like an unfair comparison: perhaps the superiority of the proposed method is due to the superior performance of Adam as compared to SGD? Adam is known to converge faster than SGD, which might also explain why the proposed method converged quicker than ESGD.\n\nAuthors list the total number of parameters, but do not specify the architectures of the CNNs use (how many channels p/layer etc.)."
            },
            "questions": {
                "value": "Authors use cross-validation error to perform genetic evolution of the hyperparameters. Isn\u2019t this going to leak information about the test set? The final errors reported - are they calculated on some hold-out set that is not seen during the evolutionary process?\n\nFormatting issues:\n1. Citations are not enclosed in parenthesis - for example, \u201c\u2026Large Language Models Cheng et al. (2023)\u201d - should rather be \u201c\u2026Large Language Models (Cheng et al., 2023)\u201d\n2. Acronyms: there is no need to capitalize every word that is going to be abbreviated. I.e., instead of \u201cNeural Networks (NNs)\u201d one can simply write \u201cneural networks (NNs)\u201d.\n3. \u201cwhen the magnitude is at 0, None\u201d -> when the magnitude is at 0, none\n4. \u201cas see with a lower\u201d -> as seen with a lower\n5. \u201cOne iteration defines one local and global update together. gradient updates the algorithm takes before performing a mutation.\u201d - the two sentences seem malformed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6683/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698761158578,
        "cdate": 1698761158578,
        "tmdate": 1699636765959,
        "mdate": 1699636765959,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OMVgElpAKO",
        "forum": "W2HJKGnb5y",
        "replyto": "W2HJKGnb5y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6683/Reviewer_nmn7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6683/Reviewer_nmn7"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a memetic algorithm, Population Descent, which combines the benefits of gradient descent and black-box optimization methods. POPDESCENT is based on population-based evolution, helping to explore more space in the loss function and performing better than existing frameworks. Experiments on FMNIST and CIFAR-10 datasets demonstrate its effectiveness."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The considered problem is important.\n2. The idea of combining first-order optimizer and black-box optimizer is sound."
            },
            "weaknesses": {
                "value": "1. The problem formulation in this paper is rather unconventional. To enhance the overall coherence of the paper, I recommend commencing with an overview of Black-box optimization and leveraging the context of evolutionary algorithms to guide the logical progression.\n2. The introduction of the problem is overly simplistic and fails to provide an in-depth explanation of non-convex optimization and saddle points. Additionally, it lacks essential references on these topics, which are crucial for a comprehensive understanding.\n3. The novelty of the proposed method appears to be limited in comparison to existing approaches.\n4. The experimental section is notably inadequate in terms of datasets and compared methods. It is essential to incorporate a comparison with advanced optimizers, such as the Sharpness-aware optimizer, to provide a more comprehensive evaluation and gauge the effectiveness of the proposed method against state-of-the-art techniques.\n5.  Furthermore, the experimental results exhibit mediocre performance and lack clarity in demonstrating significant effects."
            },
            "questions": {
                "value": "See Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6683/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6683/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6683/Reviewer_nmn7"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6683/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699202068655,
        "cdate": 1699202068655,
        "tmdate": 1699636765857,
        "mdate": 1699636765857,
        "license": "CC BY 4.0",
        "version": 2
    }
]