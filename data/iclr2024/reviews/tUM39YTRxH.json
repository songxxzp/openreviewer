[
    {
        "id": "C81YNNAhLY",
        "forum": "tUM39YTRxH",
        "replyto": "tUM39YTRxH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5039/Reviewer_bPzM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5039/Reviewer_bPzM"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a method for leveraging language models to write code computing reward functions for simulated reinforcement learning tasks, given a textual description of the task.\n\nTheir method makes use of an expert provided environment \u201cabstraction\u201d (a specification) written as a sequence of pythonic class declarations, and combines it with a human provided textual instruction describing the desired trainable task. This is used to prompt GPT-4 into generating python code for computing rewards. Optionally, the prompt given to the language model can be augmented with few-shot examples of expert and previously-generated reward functions, retrieved from a database using embedding similarity between desired and stored instructions.\n\nWithin the proposed framework, it is also possible to iteratively improve the generated reward function by having a human provide written and descriptive feedback given a video from a policy learnt with the current generated reward. Such feedback is introduced in the language model prompt and used to generate a new version of the reward function code.\nIn a series of experiments over a variety of simulated environments, the authors demonstrate the effectiveness of the approach, while generally showing few-shot and iterative prompting to improve results. Overall, generated reward functions appear to be similar in performance to expert provided rewards."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper addresses the important problem of dense reward specification in reinforcement learning, using a method based on zero-shot and few-shot prompting of a language model, something not shown before except for concurrent work **[1]**.\n\nThe authors show that their method is competitive with human expert reward specification in a broad series of experiments. They moreover demonstrate one such policy to transfer successfully to real world execution.\n\nThe authors also include an analysis of the failure modes of reward specification, specifically for the cases in which the generated reward function code leads to a python runtime error.\n\n**[1]** Ma et al., Eureka: Human-Level Reward Design via Coding Large Language Models, https://eureka-research.github.io/"
            },
            "weaknesses": {
                "value": "The paper essentially proposes prompting techniques for code generation with pre-trained language models accessible via API, specifically GPT-4. Despite its interesting conclusion and results, it consists at most of an interesting observational study over the capabilities of GPT-4, as the prompting techniques appear straightforward, fundamentally easy to execute manually by any user of the GPT-4 online api platform in a process of trial and error for reward design. I would not consider this a problem worthy of rejection per se, as similar \u201cprompting techniques\u201d results have been published before in machine learning venues, such as the famous \u201cchain of thought\u201d prompting technique. Still, it is my opinion that this limits the significance of the work, as GPT-4 simply fills in the shoes of an expert reinforcement learning reward coder on well known simulation environments (while \u201cchain of thought\u201d prompting was an innovation in how to query a model for more general purpose NLP benchmarks).\n\nMore importantly, to my understanding, no truly novel environments are tackled in this paper. For most of the shown tasks, GPT-4 can reasonably be expected to draw from training data containing countless reward function specifications from those environments, if not reward functions for the desired task itself.\n\nMoreover, the reliance of the paper on GPT-4 makes the results inherently irreproducible, as OpenAI does not have a policy of indefinitely supporting api access to specific snapshot versions of their models (specifically, GPT-4-0314 will be deprecated on June 13th 2024 at the earliest)."
            },
            "questions": {
                "value": "* Would it be possible to show how this method generalizes to environments that are sure not to have been included in the training data of the model? (e.g: a simple simulated environment designed specifically for this paper)\n* Did you try your method on open source or otherwise publicly available models that would allow for a snapshot of the results to be preserved? If so, how do they compare to GPT-4?\n\nEdit: I raised the score according to my latest comment."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5039/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5039/Reviewer_bPzM",
                    "ICLR.cc/2024/Conference/Submission5039/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5039/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698341106516,
        "cdate": 1698341106516,
        "tmdate": 1700581998507,
        "mdate": 1700581998507,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DrwOPh39nW",
        "forum": "tUM39YTRxH",
        "replyto": "tUM39YTRxH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5039/Reviewer_VQaY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5039/Reviewer_VQaY"
        ],
        "content": {
            "summary": {
                "value": "The authors present a method to use LLMs to write dense reward functions to train policies from natural language instructions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "************************************************************Evaluation Comprehensiveness:************************************************************ The authors utilized 17 tasks in ManiSkill2 and Metaworld, 6 in Mujoco, and also a few real world experiments \u2014 this is quite comprehensive compared to the average ICLR RL paper.\n\n****************************Experiments:**************************** \n\n- Real world experiments are always nice!\n- Human feedback experiments are also great and demonstrate that on certain tasks the method can improve the policy learning performance \u2014 something that isn\u2019t always straightforward even with humans redesigning reward functions to try to improve polichy learning\n\n******************Results:****************** I think results are solid compared to the oracle.\n\n**********Motivation:********** The paper motivation is nice \u2014 reducing human effort in scaling policy training by using LLMs for python/numpy reward function design."
            },
            "weaknesses": {
                "value": "************************************************************Contribution over prior work:************************************************************ I\u2019m not too convinced on the major **********technical********** contribution over Yu et al. 2023 (Language to Rewards for Skill Synthesis). Compared to that paper, the main claimed novelty is dense vs sparse reward and the use of standard python code: \u201cDifferent from recent work\u2026.our free-form dense reward code has a wider coverage of tasks and can use established coding packages.\u201d But that paper also uses very pythonic code, furthermore utilizes sparse reward mainly due to using MPC. I\u2019m not too convinced that the pythonic \u2192 python and MPC \u2192 RL are large technical contributions on their own. This should be clarified more specifically/clearly in the paper if there is another technical contribution over Yu 2023, and if not, then is one of the main reasons for my score.\n\n************************Experiments:************************\n\n- The authors should compare against Yu 2023, especially if claiming their dense free-form reward + use of established coding packages can result in superior performance. The comparison isn\u2019t exactly 1-1 given the claims, but currently there is no comparison to any baseline to contextualize the performance of the method. In fact, I think a comparison with Yu 2023 + RL would be fairest, as Yu 2023 likely can use RL instead of MPC without change.\n- Open-source LLMs: Utilizing closed-source LLMs has obvious downsides, e.g., reproducibility (API backend can change at any time) and access to academic researchers (cost per token vs able to be used on a standard GPU setup). It would be beneficial to the community to demonstrate some results with some smaller open source models like LLaMa-2.\n\n**************************Minor Issues:**************************\n\n- 4.1: Appendex \u2192 Appendix\n- I think it\u2019d be nice to have a few small examples in the **********main paper********** of generated reward functions (not full things, just a few lines). This makes the experiment section more readable without needing to jump around to the appendix."
            },
            "questions": {
                "value": "Putting it here because this is not a \u201cweakness\u201d as this paper just came out: Eureka has recently come out (https://eureka-research.github.io/), and as concurent work, what do the authors think they contribute compared to Eureka?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5039/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5039/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5039/Reviewer_VQaY"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5039/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698379991197,
        "cdate": 1698379991197,
        "tmdate": 1700251949948,
        "mdate": 1700251949948,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pCZXpMBSgf",
        "forum": "tUM39YTRxH",
        "replyto": "tUM39YTRxH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5039/Reviewer_XwRM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5039/Reviewer_XwRM"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces Text2Reward, an automated generation framework of dense reward functions based on large language models. T2R produces interpretable, free-form dense reward codes using a compact representation of the environment source code. T2R is able to outperform human-written reward codes on a range of tasks and allow iterative refinement with human feedback."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper studies the pertinent problem of automated reward design using LLMs. Given that reward design is a fundamental challenge in RL and that LLMs for decision making have largely been limited to high-level planning tasks, this paper offers a fresh perspective and a nice solution to the growing literature of LLMs for problem design and low-level robotic control.\n\nThis paper's method is novel and more flexible than a prior work (Yu et al., 2023) in that it does not require extensive manual templating per robotic embodiment or task and is capable of generating free-form reward functions. It incorporates a compact representation of the environment, background knowledge (e.g., available function APIs), and/or few-shot examples to successfully do so. Finally, this paper demonstrates interesting use case of the proposed method, such as real-world evaluation as well as learning from human feedback.  \n\nThe paper is well-written and free of grammatic errors."
            },
            "weaknesses": {
                "value": "1. The primary weakness of the paper is that most evaluation tasks are from benchmarks that have been released before GPT-4's knowledge cutoff date (September, 2021). Mujoco and Metaworld tasks have been extensively studied in the reinforcement learning literature; ManiSkill2, though released recently, have many overlapping tasks with ManiSkill, which was released in mid 2021; in particular, most of the tasks, to the best of my knowledge, were in the original ManiSkill benchmark. Given this, it is not clear whether the reward design capability of T2R can readily transfer to an unseen task in a new simulator. \n\n2. Relatedly, the \"novel\" behavior on the Mujoco locomtoin tasks have appeared in prior literature; for example, Hopper back flip is shown in Christiano et al., 2017. It's unclear whether T2R has benefited from that knowledge possibly being in the training set of the backbone LLM. \n\n3. Most manipulation tasks are of \"pick-and-place\" or opening/closing/pushing nature. These are also the most common types of manipulation tasks that the RL literature has studied. It is possible that GPT-4 is generally adept at writing reward functions for those task types. \n\n3. T2R appears to still work best with few-shot examples. In many tasks that do not belong to a family of tasks introduced by a benchmark, providing few-shot examples can still be difficult. \n\n3. For each task, only one reward function is reported. It is not clear whether T2R is robust to stochasticity in LLM generation."
            },
            "questions": {
                "value": "1. There are 11 tasks in MetaWorld, but only 6 of them have results in the paper?\n\n2. Is there a reason few-shot examples are not used for MetaWorld?\n\n3. How robust is T2R to different random generations? The temperature used for the LLM generation is not shown. \n\n4. Can T2R work for non pick-and-place, opening/closing/pushing robotic manipulation tasks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5039/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698595626804,
        "cdate": 1698595626804,
        "tmdate": 1699636493250,
        "mdate": 1699636493250,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DHH1jLUgfp",
        "forum": "tUM39YTRxH",
        "replyto": "tUM39YTRxH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5039/Reviewer_fmuH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5039/Reviewer_fmuH"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a method to generate dense rewards for continuous control RL tasks using LLMs and an API summary of the environment. The method runs either completely zero-shot on top of the prompt with environment information, or in a few-shot manner by concatenating the nearest neighbor task specification and expert reward into the context. In addition, after RL training, human feedback can be solicited, leading to further refinement of the reward code by the LLM, which leads to hopefully stronger RL. Across a number of tasks, the zero-shot dense reward code is competitive with the expert code, and in some settings few-shot generation can improve the results, though in others it is harmful."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Interesting application of a new tool! Using LLMs to generate reward code seems like an easy way to simplify problems we may not already have solutions for, but can describe in language, and is a completely different way around the sparse reward problem\n- Zero-shot results seem pretty strong across all environments\n- Nice results on new tasks that (as far as I know) we don't have expert reward for (e.g. Hopper flipping)"
            },
            "weaknesses": {
                "value": "- There is no qualitative analysis/discussion of what the source of the improvement is:\n  - Why does Zero-shot outperform Few-shot on Turn Faucet, Open Cabinet Door, Open Cabinet Drawer?\n  - Why does Few-shot fail to outperform Oracle even though Oracle is in context (Lift Cube, Turn Faucet, Push Chair, Open Cabinet Drawer)?\n  - In the cases that few-shot improves on zero-shot, what is the source of this improvement?\n- There are a lot of missing details in the experiments:\n  - There is only one example of human feedback (Figure 6) and it is in a schematic diagram for a task that does not have ground truth, I would like to see a few traces of the whole round (generation, feedback, generation, feedback, generation) in order to understand what exactly is happening\n  - For results in Table 1 and Figure 4 on novel behaviors, the standards for human evaluation and who conducted the evaluation (the authors, peers from the same institution, crowdsourced to a platform) are missing\n  - The experiments in Table 1 and Figure 4 are supposedly conducted in a zero-shot setting (caption in Figure 4), yet Figure 6 gives a schematic for ambiguity resolution which would imply a few-shot result for novel behaviors, an experiment which I do not see in the paper and appendices.\n  - I do not see the choice of $k$ for the number of neighbors that appear in the few-shot context\n  - For generated rewards in Appendix D.1 on Pick Cube, the few-shot vs. oracle code is almost indistinguishable except for 2 constants (cube placed at goal reward, grasped reward). Given this difference is so small, it seems important to know what the human feedback was: are we just getting lucky?\n  - As before, given that the few-shot vs. oracle code is so close on Pick Cube (the only example we have to judge), why is it the case that the few-shot generation is underperforming oracle generation in other settings (Lift Cube, Turn Faucet, Open Cabinet Drawer, Push Chair)?\n  - Is it always the case that the Oracle code for the task is put into the context for Few-shot?\n- Section 4.2 is about code errors that occur before any RL happens, and this seems like a necessary filtering step, but I think having an example of the generation/feedback process is much more important in the main body than Table 2.\n- How long does the iteration loop take? Each iteration requires training policies, so it is quite expensive, and it may be nice to think about early evaluation\n- It would be nice to include code examples for the novel behavior tasks to see what is happening. Given there is no baseline in this case, simply presenting a quantitative evaluation without any analysis is a little sparse..."
            },
            "questions": {
                "value": "See weaknesses above, I'm most interested in analysis on the source of the improvements in each of the tasks, and how the language model goes about creating reward for a novel task"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5039/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5039/Reviewer_fmuH",
                    "ICLR.cc/2024/Conference/Submission5039/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5039/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698719933425,
        "cdate": 1698719933425,
        "tmdate": 1700692577206,
        "mdate": 1700692577206,
        "license": "CC BY 4.0",
        "version": 2
    }
]