[
    {
        "id": "n4jKHwAhVr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6736/Reviewer_AU5E"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6736/Reviewer_AU5E"
        ],
        "forum": "YcM6ofShwY",
        "replyto": "YcM6ofShwY",
        "content": {
            "summary": {
                "value": "This paper proposes to leverage epistemic uncertainty in the (reverse) sampling process of diffusion models. This is done by applying the last-layer Laplace approximation to the image-to-image neural network that approximates the score function. Using some approximations, this uncertainty gives rise to uncertainty in the image sample at each time step of the backward diffusion process. Applications like filtering low-quality samples and sample diversity enhancement are discussed."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I find the paper is well-written and easy to follow even for me, who is not very familiar with diffusion models. In any case, the proposed method is sound and most importantly, very practical---the authors noted that the overhead of their method is no more than 1x of the standard diffusion sampling. \n\nI especially find the applications presented to be very interesting, illuminating, and again: practical. I emphasize the practicality of this paper since Bayesian neural networks are often quite impractical for large-scale problems like diffusion models."
            },
            "weaknesses": {
                "value": "1. Some details that might be useful for potential readers are glossed over. E.g. how is $\\mathrm{diag}(\\gamma_\\theta^2(x_t, t))$ computed? What approximation of the Hessian is used? etc.\n2. Sec. 4.2 and 4.3 are a bit handwavy---it would be much better if the authors could make them more quantitative, like Sec. 4.1. Handpicked examples are not useful to instill confidence about the benefits of BayesDiff.\n3. Some figures are quite hard to follow:\n    1. Fig. 2 is quite hard to understand. The caption is not descriptive at all and the colors are hard to see in print.\n    2. Fig. 3 & 4: need more spacing between \"left\" and \"right\" groups. I was really confused at first trying to parse what is \"left\" and what is \"right\".\n    3. Fig. 5: The colors are horrible (esp. in print), they're indistinguishable. It's better to use different markers or different linestyle instead."
            },
            "questions": {
                "value": "The last-layer Laplace approximation can still be very expensive for networks with high output dimensionality, e.g. in text or image generation. For example, if the image is $d \\times d$, then the network has an output dim of $d^2$. Assuming the last-layer feature dim of $h$, this means the last-layer weight matrix is $d^2 \\times h$ and so the Hessian is $hd^2 \\times hd^2$. Then, to get the variance over outputs $\\gamma^2(x, t)$, you need to multiply the Hessian with the last-layer Jacobian, which itself is large---$d^2 \\times hd^2$. Can the authors elaborate on how BayesDiff overcomes this issue in practice?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6736/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6736/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6736/Reviewer_AU5E"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6736/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697385417371,
        "cdate": 1697385417371,
        "tmdate": 1700509809021,
        "mdate": 1700509809021,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XwOiSxMHmQ",
        "forum": "YcM6ofShwY",
        "replyto": "YcM6ofShwY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6736/Reviewer_CcvE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6736/Reviewer_CcvE"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to use a last-layer Laplace approximation in diffusion models and derives how to propagate variance iteratively through the diffusion dynamics to obtain per-pixel uncertainty estimates. The experiments leverage these for filtering out low-fidelity samples, rectifying visual artifacts and visualization.\n\nOverall the method seems sensible, although I am unfortunately not too familiar with diffusion models, so may not be the best person to judge this. The evaluation seems to largely rely on subjective, qualitative analysis, and where it is quantitative the differences are mostly small and error bars missing. So all in all I would slightly lean towards rejection, but I am not strongly opinionated either way due to a lack of confidence.\n\nEDIT: In light of the rebuttal, I now lean towards acceptance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* Overall the methodology seems sensible, utilizing the fact that diffusion models are probabilisitic to perform inference is quite a natural approach.\n* The paper is well structured and clear on what prior work it builds on."
            },
            "weaknesses": {
                "value": "* I find it hard to tell whether the proposed method does anything meaningful. Most of the comparisons are rather qualitative, and where they are quantitative they are hard to interpret, e.g. it is quite difficult to decide what to make of Table 1. Many of the differences are quite small and without error bars it seems impossible to know whether those correspond to meaningful performance gains.\n* There are no baselines. I appreciate that there may not have been any prior work in this direction (although I would imagine that there would be some non-probabilistic filtering techniques. Perhaps from the literature on GANs?), however given that the iterative sampling process involves a Gaussian at every step, if I am understanding things correctly I would think that a deterministic diffusion model would also give us pixelwise variances that could be used as a baseline.\n* Alternatively, it might have been interesting to experiment with different covariance structures for the Laplace approximation to see if those make a difference.\n* I did not find the background section on diffusion models (2.1) particularly helpful as it relies on a lot of terminology on SDEs. The opening paragraph is good, perhaps something similar that briefly summarizes things from an algorithmic perspective (what kind of network are we typically training to predict what and w.r.t. what objective, what is being sampled, ...)."
            },
            "questions": {
                "value": "* I would like to see error bars for (some of) the quantitative results.\n* Is there a strict need to use a Laplace approximation for inference? The likelihood is a Gaussian, so if you are only estimating uncertainty over the final layer weights, shouldn't the posterior be Gaussian as well? Or is this not the case due to the iterative sampling process?\n* Could the sampling variances be used to create a baseline with a deterministic diffusion model?\n\nMinor:\n* I think it would be helpful to complement Figure 2 with a plot of skipping intervals vs Spearman correlation with the no-skipping ranking.\n* For Figure 5, I would suggest using a different color palette (with different colors rather than differing shades) and distinct markers. It is unnecessarily difficult to match the lines and legends as is."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6736/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6736/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6736/Reviewer_CcvE"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6736/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698253084785,
        "cdate": 1698253084785,
        "tmdate": 1700585575936,
        "mdate": 1700585575936,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wegjXrexYR",
        "forum": "YcM6ofShwY",
        "replyto": "YcM6ofShwY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6736/Reviewer_NYDr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6736/Reviewer_NYDr"
        ],
        "content": {
            "summary": {
                "value": "Diffusion models are powerful generative models but it is not easy to output standard bayesian uncertainty statistics from them such as posterior predictive probability, or pixel-wise uncertainty etc. Knowing if there are some pixels in an image, or an entire image, can be very helpful in ensuring high quality in downstream tasks.\n\nThis work uses the well known method of Laplacian approximation to estimate parameter uncertainty in the score network of an image diffusion model. For computational efficiency, and tractability in converting parameter uncertainty into sample uncertainty the authors use the well known approximation to only estimate the variance in the last linear layer of the neural network. Once the uncertainty update from a single application of the score network can be computed then the final uncertainty can be easily computed by deriving the update equation for different first-order and second order discrete time samplers.\n\n==== After rebuttal ======\nThanks for the updates. No changes to rating. Best wishes."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is reasonably novel and presents a methodology that practitioners may find useful. Specially the use of pixel-uncertainty in fixing the sample may be useful."
            },
            "weaknesses": {
                "value": "While the overall method is quite simple and the experiments show some potential for the method but the actual experiments are not clear/substantive enough. See questions section for more details."
            },
            "questions": {
                "value": "1. Section 4.2 shows that pixel wise uncertainty can be used to correct bad portions / artefacts in the original images. Many questions come to mind about this experiment. Were the bounding boxes for the artefacts determined automatically based on pixel uncertainty ? Even if they were identified manually ? How  often are the refined samples sampled using rejection sampling on pixel uncertainty score better than the original ? In other words are the examples in figure 8 cherry picked or representative of the pixel-wise uncertainty rejection sampling method ? \n\n2. Figure 2 tries to demonstrate that despite skipping the \"pixel-variance sum\" statistic is able to separate out high uncertainty images from low uncertainty images. However at skipping=3 and skipping=4 the two clusters seem to be mixed quite a lot. Also it's not clear why the mean of the scores decreases for skipping=5 and skipping=6 when it was increasing monotonically from skipping=1 to 4."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6736/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6736/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6736/Reviewer_NYDr"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6736/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698639968967,
        "cdate": 1698639968967,
        "tmdate": 1700629677618,
        "mdate": 1700629677618,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cJkqOqH5pf",
        "forum": "YcM6ofShwY",
        "replyto": "YcM6ofShwY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6736/Reviewer_B5Gy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6736/Reviewer_B5Gy"
        ],
        "content": {
            "summary": {
                "value": "Authors propose to obtain a Laplace approximation to the last layer of a diffusion model to filter out low-fidelity images."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "**Originality.** The idea of obtaining Laplace approximations to the weights of neural networks is not new, neither is the idea of filtering out low-fidelity images.\n\n**Quality and clarity.** The paper is easy to follow.\n\n**Significance.** I do not find the proposed approach a theoretically sound approach, hence not significant."
            },
            "weaknesses": {
                "value": "* Why not use the model likelihood to rule out low-fidelity images? The likelihood of diffusion models is tractable as done in [Song et al.](https://openreview.net/pdf/ef0eadbe07115b0853e964f17aa09d811cd490f1.pdf)\n\n* Despite authors' justification, I am not convinced that the posterior distribution over the weights of the last layer can be accurately approximated with a Gaussian distribution. This statement is as accurate as the statement that a Gaussian prior is a good prior for the weights of a neural network. Is that true? I suggest plotting per-weight histograms of the last layer of a trained diffusion model to see if they are Gaussian.\n\n* The whole notion of removing low-fidelity images and promoting generative models to create \"good looking\" images has been recently highly criticized due to this process biasing generative models. See [this paper](https://arxiv.org/pdf/2106.10270.pdf) and [this paper](https://arxiv.org/abs/2306.06130) and similar papers (in reference and citations)."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6736/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6736/Reviewer_B5Gy",
                    "ICLR.cc/2024/Conference/Submission6736/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6736/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698789450115,
        "cdate": 1698789450115,
        "tmdate": 1700585790565,
        "mdate": 1700585790565,
        "license": "CC BY 4.0",
        "version": 2
    }
]