[
    {
        "id": "EFGV0nwDb2",
        "forum": "YZ7NWYBd5z",
        "replyto": "YZ7NWYBd5z",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8919/Reviewer_X4Gr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8919/Reviewer_X4Gr"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose adding an extra 1x1 convolutional kernel and a simple softmax layer to VGG-19 for detecting deepfakes. To put it succinctly, this is not a novel idea. Furthermore, there are significant issues with both the text and figures. As a result, the reviewer votes to 'reject.'"
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "This paper is easy-to-follow."
            },
            "weaknesses": {
                "value": "1. The method lacks novelty. The proposed LICA and SSA are both simple modifications to the backbone model. Functionally, they resemble fine-tuning, transferring a pre-trained VGG-19 to the task of deepfake detection. This approach is essentially common sense in computer vision and cannot be considered an independent contribution.\n\n2. The experiments lack comparisons with state-of-the-art (SOTA) methods. Deepfake detection sees numerous related works every year, and the authors do not compare with all of them. At the very least, it is expected that the authors compare their method with common benchmark methods such as Face X-ray, MultiAtt, RECCE, and others.\n\n3. The manuscript requires more polishing. For instance, Section 1 devotes a substantial amount of space to discussing the necessity of this work. In Section 2, the summary of related work is limited to publications up to 2020, without summarizing or mentioning any recent developments.\n\n4. All images have extremely low resolutions."
            },
            "questions": {
                "value": "n/a"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8919/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698648804307,
        "cdate": 1698648804307,
        "tmdate": 1699637122458,
        "mdate": 1699637122458,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Do2Fof3xHI",
        "forum": "YZ7NWYBd5z",
        "replyto": "YZ7NWYBd5z",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8919/Reviewer_Bbmb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8919/Reviewer_Bbmb"
        ],
        "content": {
            "summary": {
                "value": "The paper discusses the development of a complementary attention-based deep learning system for detecting identity swaps in fake images.  The proposed method incorporates Layer-Integrated Channel Attention (LICA) and Scaled Spatial Attention (SSA) mechanisms in the VGG network architecture to capture the importance along each channel and at each spatial location."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The topic could be of wide interest. Layer-Integrated Channel Attention and Scaled Spatial Attention mechanisms are proposed and sound reasonable."
            },
            "weaknesses": {
                "value": "The paper evaluates the proposed method on a limited number of deepfake models. It would be beneficial to include more diverse models to validate of the approach. \n\nBesides, there are major problems in the writing and formatting of this article. 1) page 6 and page 7 are filled with oversized pictures. 2) It remains unclear to me the relation between the existing issues and the motivation of the proposed methodology, plus how it resolves the issues (in the current version, nor was it explicitly explained in the abstract or in the introduction)."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8919/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698649159084,
        "cdate": 1698649159084,
        "tmdate": 1699637122349,
        "mdate": 1699637122349,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "D6kiV35Gee",
        "forum": "YZ7NWYBd5z",
        "replyto": "YZ7NWYBd5z",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8919/Reviewer_9aRj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8919/Reviewer_9aRj"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a complementary attention-based deep learning system for detecting identity swaps in fake face images. The proposed system incorporates Layer-Integrated Channel Attention (LICA) and Scaled Spatial Attention (SSA) mechanisms in the VGG network architecture to capture the importance along each channel and at each spatial location. It further utilizes Local Interpretable Model-agnostic Explanations (LIME) as an explainable AI technique to provide a transparent analysis of its effectiveness. Experimental results demonstrate that the proposed system outperforms state-of-the-art methods in terms of accuracy and area under curve metrics."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The Layer-Integrated Channel Attention (LICA) and Scaled Spatial Attention (SSA) mechanisms capture the importance along each channel and at each spatial location, enhancing the detection performance.\n\n2. The proposed system outperforms state-of-the-art systems in terms of accuracy and area under curve metrics in detecting fake faces generated by identity swaps."
            },
            "weaknesses": {
                "value": "1. The evaluation of the proposed system is limited to the DFFD dataset, which may not fully represent the diversity and complexity of real-world scenarios.\n2. In section 5.1, \u201cWe test our model (VGG19 plus complimentary attention)on 7865 such images to get an accuracy of 74.84%.\u201d Can you give more evidence about this result?\n3. This paper does not provide insights into the generalizability of the proposed approach to different types of manipulation techniques or datasets.\n4. My comprehension of the operational utility of the Local Interpretable Model-Agnostic Explanations (LIME) method is currently lacking. In the context of the present research, LIME is exclusively deployed for the purpose of visualizing the model's outcomes. However, it is noteworthy that attention mechanisms, specifically the employment of an attention heatmap, are viable alternatives capable of achieving a similar objective. Can you explain why you chose LIME instead of attention heatmap?"
            },
            "questions": {
                "value": "Can you explain why Layer-Integrated Channel Attention (LICA) can learn the relative importance of data across the channels?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8919/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8919/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8919/Reviewer_9aRj"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8919/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699411938935,
        "cdate": 1699411938935,
        "tmdate": 1699637122236,
        "mdate": 1699637122236,
        "license": "CC BY 4.0",
        "version": 2
    }
]