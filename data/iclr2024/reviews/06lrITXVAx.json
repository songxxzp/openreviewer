[
    {
        "id": "8SjJOErGTH",
        "forum": "06lrITXVAx",
        "replyto": "06lrITXVAx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6272/Reviewer_mkUf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6272/Reviewer_mkUf"
        ],
        "content": {
            "summary": {
                "value": "This paper points out that overfitting is a common challenge in bilevel training task. To address this issue, the author proposes the dropout mask for the bilevel optimization problem. Specifically, the author proves the convergence of dropout bilevel methods theoretically and empirically shows that proposed method mitigates the overfitting issue."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Overfitting issue has not been fully investigated in the bilevel area and this paper proposes the new method to address it."
            },
            "weaknesses": {
                "value": "1. It looks like this paper studies data cleaning problem rather than the bilevel problem. Both the introduction and experiments are based on data cleaning context. The authors are suggested to discuss and conduct experiments on other bilevel optimization settings to demonstrate applicability of proposed algorithm.\n\n2. Overfitting issue can be easily addressed by early stopping. The authors are encouraged to make an experimental comparison between proposed dropout and early stopping to demonstrate the necessity of adopting dropout.\n\n3. In Theorem 2, the authors demonstrate that dropout rate \"influences\" the upper bound. However, it only influences the constant term and does not influence the convergence rate. Furthermore, the theoretical results only demonstrates the dropout rates \"influence\" the convergence but does not demonstrates it \"improves\" the convergence rate, even for the constant term. I expect to see inspiration about dropout rate selection from this theorem. The same issue also occurs for the Lemma 1 which is about variance term."
            },
            "questions": {
                "value": "Can you discuss the technical difficulties of applying dropout analysis into bilevel optimization problem? That will be helpful to understand the theoretical contribution."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6272/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6272/Reviewer_mkUf",
                    "ICLR.cc/2024/Conference/Submission6272/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6272/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698624070166,
        "cdate": 1698624070166,
        "tmdate": 1700517744501,
        "mdate": 1700517744501,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "S3j0qOxj5t",
        "forum": "06lrITXVAx",
        "replyto": "06lrITXVAx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6272/Reviewer_fs9w"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6272/Reviewer_fs9w"
        ],
        "content": {
            "summary": {
                "value": "The paper uses dropout methods for bilevel training tasks. Bilevel optimization problems consist of two intertwined optimization problems and can be particularly sensitive to small changes, especially when data is limited. The study introduces a bilevel optimization model that considers the distribution of dropout masks and examines how varying dropout rates impact the hypergradient of this model. The authors adapt an existing bilevel method to incorporate dropout and provide theoretical convergence guarantees for this new approach. Empirical tests on data cleaning problems show that this method can mitigate overfitting."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Incorporating dropout in bilevel training tasks is a novel approach, offering a new method to combat overfitting in such tasks. The authors study the convergence properties of the introduced method. The study offers empirical proof, especially in the context of data cleaning problems, demonstrating the efficacy of the proposed method in reducing overfitting. The paper paves the way for adapting other state-of-the-art bilevel methods to account for dropout, making it a foundational study for further research in this direction."
            },
            "weaknesses": {
                "value": "The use of datasets like MNIST, which is relatively small and simplistic, might not fully showcase the potential or limitations of the proposed method in real-world, complex scenarios."
            },
            "questions": {
                "value": "Beyond data cleaning, how might this method be applied to other machine learning tasks, especially those that inherently involve bilevel optimization?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6272/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6272/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6272/Reviewer_fs9w"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6272/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698813829752,
        "cdate": 1698813829752,
        "tmdate": 1699636686820,
        "mdate": 1699636686820,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "p6oCa4K9iR",
        "forum": "06lrITXVAx",
        "replyto": "06lrITXVAx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6272/Reviewer_hA9j"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6272/Reviewer_hA9j"
        ],
        "content": {
            "summary": {
                "value": "This paper brings dropout to bilevel optimization to avoid overfitting issues. The author forms a statistical bilevel optimization problem that includes the distribution of the dropout masks and propose a dropout method to solve it. To analyze the convergence of the proposed method, they delicately quantify the bias induced by the dropout. The empirical performance of the proposed method is tested on the data hyper cleaning task and shows that dropout efficiently improve the test accuracy and is more stable."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper incorporates the dropout technique in the bilevel optimization to avoid overfitting. The idea is new to the bilevel optimization community and has been tested effective. \n2. The analysis of dropout by modeling the dropout mask as a stochastic distribution is thorough and novel. \n\nOverall, I felt this is a solid and novel paper for bilevel optimization."
            },
            "weaknesses": {
                "value": "1. The motivation of considering the dropout in bilevel optimization could be explained more. For example, why avoiding overfitting is important in bilevel optimization, that is, does bilevel structure exacerbate the overfitting issues? \n2. The convergence analysis is built up on the stationarity measure $\\frac{1}{T-1} \\sum_{k=1}^{T-1} \\mathbb{E}\\left\\\\|\\nabla F_{M^{k}}\\left(\\lambda^{k}\\right)\\right\\\\|^{2}$, which is the dropout bilevel objective. To fortify the analysis, it is conceivable to draw connections between this measure and the one based on the original bilevel objective $\\frac{1}{T-1} \\sum_{k=1}^{T-1} \\mathbb{E}\\left\\\\|\\nabla F\\left(\\lambda^{k}, w(\\lambda^{k})\\right)\\right\\\\|^{2}$. This could elucidate the relaxation error introduced by dropout and establish a guarantee for the original objective's convergence, thereby validating efficiency of the dropout approach more rigorously."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6272/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6272/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6272/Reviewer_hA9j"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6272/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699045617003,
        "cdate": 1699045617003,
        "tmdate": 1699636686710,
        "mdate": 1699636686710,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oCgE2TpDUR",
        "forum": "06lrITXVAx",
        "replyto": "06lrITXVAx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6272/Reviewer_Kt41"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6272/Reviewer_Kt41"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a dropout method to address the issue of overfitting in bilevel training tasks. The authors provide theoretical convergence guarantees from an optimization perspective and demonstrate its effectiveness through experiments, using data cleaning as an illustrative example."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is generally well-written and easy to follow. It investigates a relatively unexplored area and aims to analyze the dropout bilevel method for addressing this problem. Another advantage is the detailed theoretical analysis provided in the paper."
            },
            "weaknesses": {
                "value": "- Based on the results figures, it appears that early stopping can effectively resolve the issue of overfitting, even though the accuracy without dropout is higher.\n- The theoretical analysis in the current context is not particularly challenging. In contrast, in many stochastic bilevel optimization studies, such as [2], a more comprehensive framework is presented, along with convergence rate guarantees for stochastic bilevel optimization. You can try to answer the extra challenge in these theoretical literatures [2].\n- The experimentation conducted on data cleaning is insufficient, and it would be better to observe more results on other bilevel optimization tasks.\n- Each figures should be accompanied by a brief caption."
            },
            "questions": {
                "value": "- In addition to the theoretical analysis, literature [1] also investigates the impact of dropout rate on bilevel optimization. However, what are the other distinguishing factors between these studies?\n- What is the relationship between the objective function after incorporating dropout and the original objective function? \n\n[1] Delta-STN: Efficient Bilevel Optimization for Neural Networks using Structured Response Jacobians. Juhan Bae, Roger Grosse. NeurIPS 2020.\n\n[2] Bilevel Optimization: Convergence Analysis and Enhanced Design. Kaiyi Ji, Junjie Yang, Yingbin Liang. ICML 2021."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6272/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6272/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6272/Reviewer_Kt41"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6272/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699675658261,
        "cdate": 1699675658261,
        "tmdate": 1700666715149,
        "mdate": 1700666715149,
        "license": "CC BY 4.0",
        "version": 2
    }
]