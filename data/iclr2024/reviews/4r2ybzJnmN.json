[
    {
        "id": "PNPwefDiwT",
        "forum": "4r2ybzJnmN",
        "replyto": "4r2ybzJnmN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7757/Reviewer_7RCo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7757/Reviewer_7RCo"
        ],
        "content": {
            "summary": {
                "value": "As far as we know, the plastic delays greatly increase the expressivity in SNNs. However, efficient algorithms to learn these delays have been lacking. In this manuscript, the authors propose a new discrete-time algorithm that addresses this issue in deep feedforward SNNs using backpropagation (i.e., offline manner). Then, the kernels contain only a few non-zero weights \u2013 one per synapse \u2013 whose positions correspond to the delays. Thus, these positions are learned together with the weights using the dilated convolution with learnable spacings (DCLS). The authors show us in a practical way that building deep SNNs can learn together with fixed delays and weights."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The gap between theory and practice is opened up, especially an efficient fixed delays with weights learning algorithm is designed.\n2. The effects of delays can be well explained by visual examples.\n3. The anonymous open source code is shared with readers, and the detailed implementation helps inspire readers to build complex and deep SNNs."
            },
            "weaknesses": {
                "value": "1. The reviewers are very concerned about the innovation of the structure, despite the effort that went into achieving such a particularly efficient discrete-time learning algorithm. The reviewer noted these sentences: \"The trick is to simulate delays using temporal\nconvolutions and to learn them using the recently proposed Dilated Convolution with Learnable Spacings (Khalfaoui-Hassani et al., 2023a;b). In practice, the method is fully integrated with PyTorch and leverages its automatic-differentiation engine.\" So can we say that structurally this contribution is just a combination that happens to work. This contribution would be improved if the author could further clarify the motivation or give a more solid analysis. After all, a trick feels like an inadequate contribution.\n2. Just two datasets with similar statistic information may not seem sufficient, and it would be better if the authors had time to supplement the experiment with a new dataset."
            },
            "questions": {
                "value": "Please look at the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7757/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7757/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7757/Reviewer_7RCo"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7757/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698463260844,
        "cdate": 1698463260844,
        "tmdate": 1699636947388,
        "mdate": 1699636947388,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9i9GiOyu8s",
        "forum": "4r2ybzJnmN",
        "replyto": "4r2ybzJnmN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7757/Reviewer_ybas"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7757/Reviewer_ybas"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to study an important problem in spiking neural networks, which involves the explicit incorporation of propagation delays between different neurons in the network. This is an important problem that has been addressed regularly in recent years, and the paper provides a novel and simple solution based on a temporal convolution parameterized by a certain precision. The delays are learned using a variant of the surrogate gradient method, and numerical simulations demonstrate the learning of delays by this method. Experimental results show very good performance on traditional community datasets (in particular, a top score in the leaderboard for the Spiking Heidelberg dataset) and also demonstrate a certain robustness of the method when certain connections are pruned."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The paper effectively introduces the problem and motivation and presents the methods clearly. A major strength of the paper is the model's relative mathematical simplicity and its successful performance in supervised classification on two datasets. Promising experimental results indicate that significant energy savings can be achieved with the application of such models to neuromorphic chips by demonstrating the network's robustness when connections are removed."
            },
            "weaknesses": {
                "value": "The paper's connections with related works are satisfactory; however, it could benefit from presenting neuroscientific evidence on the plasticity of neural delays in biology. Additionally, it lacks a discussion on the relationship between the model's parameters and those observed in biology. For instance, the maximum delays utilized are around 250 milliseconds (300 milliseconds for SSC), while delays used in Izhikevitch's polychronization model are around 20 milliseconds. Furthermore, the paper does not establish any predictions made by the model that can be experimentally observed in biology.\n\nThe model has several limitations, such as the use of discrete time, a forward propagation training model, or a limited number of computational layers. However, the presented performance of the network validates the decisions made."
            },
            "questions": {
                "value": "What is the influence of the meta-parameters on the obtained performance? The influence of the characteristic time of the membrane potential would be interesting to study, as it corresponds to a kind of regularization of spike precision.\n\nCould you comment on the fact that \"We found that a LIF with quasi-instantaneous leak \u03c4 = 10.05 (since \u2206t = 10) is better than using a Heaviside function for SHD.\" ? Would such a difference matter in biology?\n\nConcerning \"We used a one-cycle learning rate scheduler (Smith & Topin, 2018) for the weights and cosine annealing (Loshchilov & Hutter, 2017) without restarts for the delays learning rates. \": Could you comment on your choice of learning rate schedulers? Would different schedulers significantly alter our results? Or does it just improve learning speed?\n\nMinor:\n- complete reference for Kingma, for Warden. There seems to be a newer one by Grimaldi for \"Learning heterogeneous delays\" instead of \"Learning hetero-synaptic delays\" - plus an additional application paper on motion detection by the same authors. \n- spacing: \"weights.Hammouamri et al. (2022)\"\nThe LaTeX formatting of the paper is excellent but could be further enhanced. In Figure 1, utilize \"N_2\", \"S_1\", and other symbols for clarity. Some citations in the text (\"Spike-Element-Wise ResNet Fang et al. (2021b) \", ...) should be enclosed in parentheses, e.g. using `citep`. Text \"reset\" appearing in equation (1) should be formatted as text, e.g.  using the `\\text{}` formatting."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7757/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698665150616,
        "cdate": 1698665150616,
        "tmdate": 1699636947259,
        "mdate": 1699636947259,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YHNDSBss1E",
        "forum": "4r2ybzJnmN",
        "replyto": "4r2ybzJnmN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7757/Reviewer_sQsE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7757/Reviewer_sQsE"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a way to learn synaptic (or axonal) delays in spiking neural networks (SNNs), where the delay of each synapse is realized as a discretized kernel of temporal convolution with a single non-zero element. An evaluation of classification accuracy on three temporal datasets shows that the method works well, with the authors claiming to surpass the state of the art in those datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "When evaluated within the context of SNNs alone, the paper offers several strengths. Namely, the method is relatively original, the evidence that the method works well is rather convincing, and the impact on SNNs could be significant, given the relative ease of implementation and effectiveness. SNNs themselves interest a growing community."
            },
            "weaknesses": {
                "value": "The main weakness of the paper is common to many works on SNNs. Specifically, the significance, novelty, potential impact, and experimental validation are limited to the narrow field of SNNs themselves. Very rarely does an SNN paper show its advantages in the broader literature on neural networks, let alone in the real world. The present manuscript too, when evaluated in a broader scope, suffers from the same issues.\n\nMore concretely:\n- the method is only new for SNNs, but not for neural networks in general.\n- the performance is claimed to surpass the state of the art (already in the abstract), but the authors do not actually compare with the true state of the art, including non-spiking networks.\n- there is no experimental comparison with standard (i.e. less constrained) temporal convolutions.\n\nTherefore, it is unclear what the true contribution of the work is, beyond a nice conceptual analogy between temporal convolutions and synaptic delays.\n\n\nSecondary weaknesses:\n- Even within spiking networks, the work does not seem to surpass the state of the art, contrary to the authors' claims. In [1], a partly spiking neural network reached 95.6% on the GSC v0.02, where the authors report 95.35% at most. The manuscript does not cite that prior work.\n- The paper does not motivate sufficiently the choice of spiking neurons as a model. A paragraph explaining the advantages of SNNs *in comparison with the true state of the art, i.e. ANNs*, supported with citations that demonstrate them measurably, such as energy efficiency, but also rarely in other metrics such as speed of inference and training [1] and even classification accuracy [2]. Any other arguments and citations that the authors can add to support that choice would be useful.\n- The authors claim that there is no recurrency in their models, but a leaky integrate-and-fire neuron's leak membrane potential is equivalent to a self-recurrent connection. I understand what the authors mean, but, again in the spirit of appealing to the broader ICLR community and not only to the SNN niche, this should be clarified.\n\n[1] Jeffares et al., Spike-inspired rank coding for fast and accurate recurrent neural networks, ICLR 2022\n[2] Moraitis et al., Optimality of short-term synaptic plasticity in modelling certain dynamic environments, arXiv 2021"
            },
            "questions": {
                "value": "Could the weaknesses be addressed? Most importantly, could the paper better clarify its significance in the broader field of neural networks? Changes and additions to the text might help address the issues somewhat, but missing experimental evaluations should ideally also be performed, or other measurements of any possible advantage claimed, e.g. number of parameters, energy efficiency etc."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7757/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698856728599,
        "cdate": 1698856728599,
        "tmdate": 1699636947144,
        "mdate": 1699636947144,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "L7MgfnQcI0",
        "forum": "4r2ybzJnmN",
        "replyto": "4r2ybzJnmN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7757/Reviewer_Wbur"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7757/Reviewer_Wbur"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors use the previously published Dilated Convolution with Learnable Spacings (DCLS) method to learn delays in a deep feed-forward spiking neural network using back-propagation. They demonstrate this method on various temporal tasks such as spiking Heidelberg dataset and versions of google speech commands. The authors also demonstrate that learning delays contributes to an increase in performance in sparse networks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Learning delays, and more generally, using temporal information is a very relevant topic.\n- The paper is generally well written and the experiments and setup are clearly described.\n- The improvement of performance in networks with fixed sparsity when delays are included is very interesting and this analysis is novel."
            },
            "weaknesses": {
                "value": "- The novel contribution of this paper over the DCLS paper is not clear. Is it just the evaluation on multiple tasks? It is very important to clarify this aspect.\n- The comparison of the model with delays versus no-delays in Sec. 4.3 may not be completely fair: Using more layers (with same number of parameters) for the no-delay network seems more comparable.\n- The statement of \"Here we show for the first time that delays can be learned together with the weights, using backpropagation, in arbitrarily deep SNNs.\" is not true. (Shrestha & Orchard 2018) do exactly that.\n- Some of the related work are incorrectly cited or not cited:\n    - The SLAYER paper (Shrestha & Orchard 2018) does train the delays along with the weights but the authors don't mention it in this context (although it is cited in a different context).\n    - dynamically adapting firing thresholds for deep (recurrent) SNNs was first proposed in (Bellec et al. 2018)\n    - Spike based transformer references should include SpikeGPT (Rui-Jie et al. 2023) and Spikingformer (Zhou, Chenlin, et al. 2023)\n\n(Shrestha & Orchard 2018) Shrestha, S.B., and Orchard, G. (2018). SLAYER: Spike Layer Error Reassignment in Time. In Advances in Neural Information Processing Systems 31, S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, eds. (Curran Associates, Inc.), pp. 1412\u20131421.\n\n(Bellec et al. 2018) Bellec, G., Salaj, D., Subramoney, A., Legenstein, R., and Maass, W. (2018). Long short-term memory and Learning-to-learn in networks of spiking neurons. In Advances in Neural Information Processing Systems 31, pp. 787\u2013797.\n\n(Rui-Jie et al. 2023) Zhu, Rui-Jie, Qihang Zhao, and Jason K. Eshraghian. \"Spikegpt: Generative pre-trained language model with spiking neural networks.\" arXiv preprint arXiv:2302.13939 (2023).\n\n(Zhou, Chenlin, et al. 2023) Zhou, Chenlin, et al. \"Spikingformer: Spike-driven Residual Learning for Transformer-based Spiking Neural Network.\" arXiv preprint arXiv:2304.11954 (2023)."
            },
            "questions": {
                "value": "## Suggestions:\n\n- The DVS gesture recognition dataset, due to its event-based nature, might have been a really good fit for a method that learns delays.\n- Since delays use temporal information, it might have made more sense to use a loss function that made use of this (for e.g. time-to-first-spike loss)\n\n### Minor:\n- Acronyms for task names are not explained in the results section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7757/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7757/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7757/Reviewer_Wbur"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7757/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698869152858,
        "cdate": 1698869152858,
        "tmdate": 1700685295627,
        "mdate": 1700685295627,
        "license": "CC BY 4.0",
        "version": 2
    }
]