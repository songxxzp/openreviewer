[
    {
        "id": "4x2hXFU7ZL",
        "forum": "dNMsieEiAc",
        "replyto": "dNMsieEiAc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5282/Reviewer_EWwu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5282/Reviewer_EWwu"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes leveraging LLMs, specifically RoBERTa, to extract essential information from textual review without fine-tuning. For each review, the authors use several cloze-test style prompts to extract various information, such as \u201c[All reviews][SEP] I value the [MASK].\u201d. The information is then used to train a CNN-based recommendation system, which consists of local a word attention module, a CNN module, user-item feature fusion module, and a neural factorization machine for rating prediction. They study the attention weights of the local word attention module, showing that the attention weights can be focused on representative words."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The paper is well-written and easy to follow. \n2. Large-language models has shown impressive performance on understanding textual data. Exploring LLMs is a promising direction in recommender systems."
            },
            "weaknesses": {
                "value": "1. The core idea of the paper is using LLMs for extracting essential information from reviews such as its keywords. However, it has limited novelty due to the extensive prior exploration of using LLMs for information extraction. Suggest referencing the survey paper. [1]\n2. Roberta is not a very large language model. It makes more sense to me to directly fine-tune the Roberta rather than only using it to extract the key information without fine-tuning. With supervised training signal, I believe fine-tuning Roberta can better filter out noisy information in reviews. This should be one of the baselines. In addition, directly using LLMs as a recommender system has been explored in previous work, and this line of work should be discussed and compared. [2][3][4]\n3. The comparison of using different keyword or keyphrase extraction methods such as TF-IDF should be included.\n\nReferences:  \n[1]A Survey on Recent Advances in Keyphrase Extraction from Pre-trained Language Models, Song et. al.  \n[2]Language Models as Recommender Systems: Evaluations and Limitations, Zhang et. al.  \n[3]Zero-Shot Next-Item Recommendation using Large Pretrained Language Models, Wang et. al.  \n[4]M6-Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems, Cui et. al."
            },
            "questions": {
                "value": "1. Instruction-tuned language models such as Llama2 have shown better performance on prompt-based learning than Roberta. Why these models were not chosen?\n2. How does using different prompts affect the model performance? Are there large differences in performance using different prompts?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5282/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5282/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5282/Reviewer_EWwu"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5282/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698638663955,
        "cdate": 1698638663955,
        "tmdate": 1699650672441,
        "mdate": 1699650672441,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yTskaGhFIv",
        "forum": "dNMsieEiAc",
        "replyto": "dNMsieEiAc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5282/Reviewer_TbAk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5282/Reviewer_TbAk"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a method to extract user feature and item features from reviews based on RoBERTa, where the features are some word factors that a user may care about, and then feed such feature word embeddings into a neural network model (CNN) for recommendation score calculation. Experiments are conducted on several e-commerce datasets and compared with some text-based recommendation baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper tries to explore prompt-based recommendation which is a trending topic, though the paper does not mention any other existing prompt-based recommendation models."
            },
            "weaknesses": {
                "value": "The motivation is somewhat weak: The key motivation of the research is to remove \"noise\" from user review data, i.e., if all review texts are used, there is likely to be a significant amount of irrelevant or unnecessary data (noise) unrelated to user preferences or item attributes. However, the attention mechanism of modern deep learning architectures such as Transformers and LLMs are exactly developed to solve the problem, by automatically learning the importance of difference parts of a text so that we don't need to manually select the parts from text. The proposed method actually goes back to devoting efforts to purposely selecting some parts of text for model learning, which is exactly what modern AI models tries to avoid.\n\nThere have been many prompt-based methods for recommendation, however, the paper did not compare with or even mention any of them. \n\nThe related work section is messed up: (1) It cites a paper published in 2022 as an \"early study\" of using topic models for review processing, and then cites a paper published in 2017 to criticize the weakness of the 2022 paper. (2) There have been many prompt-based models for recommender systems, but the related work does not mention any of them. The experimentation does not compare with any existing prompt-based recommendation model either.\n\nThe proposed model does not provide much insight for the community, it uses some prompts to extract some features from reviews, and then feed these feature embeddings into a traditional recommendation model. It's not like a research paper but more like a class project report.\n\nThe experiments used MSE as the evaluation metric, however, it has been widely known that MSE (or RMSE) cannot reflect the real ranking performance of recommender systems. It is important to evaluate the proposed method based on widely recognized ranking metrics such as Hit Ratio, NDCG, precision, recall, F1, etc."
            },
            "questions": {
                "value": "What is \"re-characterize\", what are \"key factors\"? what factors are key and what factors are not key?\n\nIn this sentence: \"Prompts are designed to enable the generation of key factors that newly define user preferences and item attributes from review text.\", authors used the word \"newly define\"; in the following sentence: \"The key factors generated by Prompt2Rec are essential words representing user preferences (or item attributes) that newly refine an integrated review text.\", authors used \"newly refine\". Are \"newly define\" and \"newly refine\" talking about the same thing or different things?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5282/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698855627214,
        "cdate": 1698855627214,
        "tmdate": 1699636528171,
        "mdate": 1699636528171,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9Gnd1Jyblr",
        "forum": "dNMsieEiAc",
        "replyto": "dNMsieEiAc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5282/Reviewer_EFBG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5282/Reviewer_EFBG"
        ],
        "content": {
            "summary": {
                "value": "The authors propose an LLM prompting-based approach for collaborative filtering. The main idea is to learn key factors to summarize the main characteristics of users and items from review texts, and then use this new information to train a model for predicting review ratings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Using the extracted key factors as part of input were shown to improve the performance on the Amazon product review datasets. Augmenting recommendation with key factors extracted from review data is an interesting idea."
            },
            "weaknesses": {
                "value": "One question about the proposed approach is its generalizability. Why is it necessary to divide the Amazon review datatset into 5 five datasets for different product types? Can we combine the datasets together so that the method become more generally applicable?\n\nThe key factors are embedded by Glove embeddings. Given there are more advanced transformer-based embeddings in recent years. Have the authors tried these advanced embeddings? Why just Glove embeddings?  \n\nThe application in the paper is review ranking prediction. It is a bit misleading to claim that it is a recommendation model, where behavior data such as click, purchase, add-cart actions are more of the focal point, and items are recommended for users to consume. I don\u2019t think claiming recommendation is essential for the paper."
            },
            "questions": {
                "value": "One question about the proposed approach is its generalizability. Why is it necessary to divide the Amazon review datatset into 5 five datasets for different product types? Can we combine the datasets together so that the method become more generally applicable?\n\nThe key factors are embedded by Glove embeddings. Given there are more advanced transformer-based embeddings in recent years. Have the authors tried these advanced embeddings? Why just Glove embeddings?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5282/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699196393081,
        "cdate": 1699196393081,
        "tmdate": 1699636528082,
        "mdate": 1699636528082,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "U9E87gLA4Z",
        "forum": "dNMsieEiAc",
        "replyto": "dNMsieEiAc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5282/Reviewer_dWh3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5282/Reviewer_dWh3"
        ],
        "content": {
            "summary": {
                "value": "The author addresses the issue of sparse interaction data in collaborative filtering by leveraging advanced NLP techniques to extract critical user preference indicators and item attributes from review texts. Rather than using entire reviews, key factors are distilled using these texts as prompts for a pre-trained model, mitigating the issue of noisy data. Additionally, the author enhances the model's transparency by visualizing its attention weights on these key factors, thereby offering explanations for its recommendations."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The author builds upon prior research that identifies user and item IDs as crucial for uncovering latent information, innovatively enriching this information with insights from review texts.\n- The novel approach of using review texts as prompts to generate key factors for a pre-trained model enhances the recommendation system\u2019s relevance and accuracy.\n- Visualization of attention weights provides an explanatory layer to the recommendation process, offering valuable insights into model decisions."
            },
            "weaknesses": {
                "value": "- The paper could be strengthened by including an ablation study that isolates the impact of using the Neural Factorization Machine model without review text integration.\n- Readers are left without a clear benchmark of improvement attributed to the proposed method as there is no comparative analysis between the baseline model and the enhanced model with review information.\n- There is an assumption that key factors generated from reviews will always enhance recommendation quality, which may not account for potential biases or inaccuracies in the review content itself.\n- No top-k evaluations are conducted, which should be more common in RS."
            },
            "questions": {
                "value": "1. Why did the author opt for GloVe embeddings over RoBERTa for generating key factor embeddings? Are there specific concerns or desired GloVe characteristics at play?\n2. Would the model's performance improve by limiting the number of key factor tokens to reduce noise and increase generation efficiency?\n3. What criteria are used for selecting words within the key factor corpus?\n4. ow does the variety of key factors influence the model's performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5282/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5282/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5282/Reviewer_dWh3"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5282/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699256883440,
        "cdate": 1699256883440,
        "tmdate": 1699636527972,
        "mdate": 1699636527972,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kRdtHkJXm0",
        "forum": "dNMsieEiAc",
        "replyto": "dNMsieEiAc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5282/Reviewer_ZfrZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5282/Reviewer_ZfrZ"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces the prompt-based learning paradigm of Natural Language Process (NLP) for re-characterizing users and items in rating prediction problem in recommendation. The paper proposes to use a pre-trained language model to generate user and item characteristics (also called key factors in the paper) from review text and use such key factors for training a rating prediction model. The paper empirically demonstrates that using key factors improves the performance of existing rating prediction models using review text. The paper also argues that the attention weights associated with key factors can be used as explanations for recommended items."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The related work is well-done.\n2. The proposed method (termed Prompt2Rec) beats all baseline models in terms of Mean Squared Error (MSE) in five widely used benchmark datasets.\n3. The paper provides a few good case studies showing that the key factors generated by the pretrained language model (RoBERTa) are intuitive and explainable."
            },
            "weaknesses": {
                "value": "1. My major concern is the novelty of the paper. The pretrained lagnuage model RoBERTa is simply used as a black box and the template used to construct prompt looks straightforward. The ways how the proposed method Prompt2Rec uses the generated key factor as inputs and trains a rating prediction model are also very standard.\n2. Presentation of the paper can be improved. E.g., the words in Figure 1 and Figure 4 are very small and it is pretty difficult to read."
            },
            "questions": {
                "value": "1. How many templates do you use for constructing prompt and how do you obtain the templates?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5282/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5282/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5282/Reviewer_ZfrZ"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5282/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699647774910,
        "cdate": 1699647774910,
        "tmdate": 1699647774910,
        "mdate": 1699647774910,
        "license": "CC BY 4.0",
        "version": 2
    }
]