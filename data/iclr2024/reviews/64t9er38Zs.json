[
    {
        "id": "HPGgTPmz5S",
        "forum": "64t9er38Zs",
        "replyto": "64t9er38Zs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5920/Reviewer_gZ3c"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5920/Reviewer_gZ3c"
        ],
        "content": {
            "summary": {
                "value": "The paper demonstrates how to build neural networks with cascaded layers of spherical neurons (neurons with spherical decision surfaces) that are equivariant to the action of the orthogonal group $O(n)$. Earlier works have only considered $O(3)$-equivariant spherical neurons in the first layer (i.e., no cascaded spherical neuron layers). The paper also describes how to layers with bias and normalization and proves exact equivariance to $O(n)$ action in all cases."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is easy to follow and the ideas are developed and presented well.\n\n2. The proposed architecture seems to produce good results for some tasks tested on compared to strong baselines including Vector Neurons and EMLPs in many cases."
            },
            "weaknesses": {
                "value": "1. One of the weaknesses is the motivation for this architecture. I am not able to see why spherical neurons, their equivariant versions, and cascaded layers of such neurons are an improvement over conventional multi-layer perceptrons (MLPs) and Equivariant MLPs (EMLPs), other than looking at experimental results. \n\n2. Although the architecture is technically novel compared to the earlier work on equivariant spherical neurons in 3D for the single layer case, the generalization appears to be straightforward, the arguments follow just by increasing the dimension from 3 to $n$. \n\n3. The limitation of the architecture is quite large. In the case of the convex hull prediction task, many algorithms outperform the proposed one when the training set is large. The authors say this is because higher-order interactions are not present in this architecture. Both VN and GVP outperform the proposed algorithm, but do not model second-order interactions. Furthermore, VN and GVP are tested only for the convex hull prediction task, not the other two tasks, so this leaves some questions about how good this architecture actually is."
            },
            "questions": {
                "value": "No additional questions, but it would be good if the authors can address the weaknesses I mentioned."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5920/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5920/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5920/Reviewer_gZ3c"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5920/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698782179410,
        "cdate": 1698782179410,
        "tmdate": 1700664938312,
        "mdate": 1700664938312,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cXcYc9fCw7",
        "forum": "64t9er38Zs",
        "replyto": "64t9er38Zs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5920/Reviewer_sbi9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5920/Reviewer_sbi9"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel Deep Equivariant Hypersphere model that naturally exhibits group equivariance and invariance properties for arbitrary O(n) groups encompassing rotation and reflection operations in n-dimensional spaces. This achievement is made possible through the utilization of spherical neurons and the generalized steerable function theorem. While prior research predominantly concentrated on symmetry within O(3), which involves rotation and reflection operations in three-dimensional space, this paper distinguishes itself by extending the concept to a broader context, the O(n) groups. The authors delve into various theoretical aspects of the proposed model and substantiate its capabilities through experiments, showcasing its effectiveness in learning O(n) equivariance and excelling in tasks such as O(n) invariance regression and classification."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "In the field of geometric deep learning, it is essential to develop a model that can effectively capture O(n) symmetry. This paper achieves this goal by utilizing well-defined geometric techniques, specifically spherical neural networks and steerable functions."
            },
            "weaknesses": {
                "value": "I have doubts about whether the model proposed in this paper can be easily applied to real-world mixed-transformed datasets.\n\nIf I understand correctly, the overall proposed approach can be summarized as follows: First, a spherical neuron-based model is created to match the dimensionality of the given dataset. Then, n + 1 transformed copies of this baseline model are generated through applying O(n) transformations (using a form of n-simplex). These transformed n + 1 models serve as the basis functions for constructing steerable functions, resulting in the creation of an O(n) equivariance model called the Deep Equivarint Hyperspheres.\n\nConsidering the definition of steerable functions, these generated bases represent rotations of the original function in specific directions, implying that each copy of the spherical neuron model should have learned data aligned with a single specific orientation. In other words, to create a Deep Equivariant Hypersphere, it is essential to have a dataset aligned with a canonical frame, to initially train the baseline spherical neurons. Indeed, the experiments in this paper demonstrate that a model trained on data aligned with the canonical orientation can effectively generalize to randomly transformed test datasets. This experiment is certainly meaningful.\n\nHowever, in real-world scenarios, we often need to learn equivariance or invariance for datasets that are already randomly transformed. In most cases, we cannot know the angles or group actions applied to instances within a given dataset beforehand. Obtaining a dataset aligned with the canonical orientation is often challenging in practice. I believe that in such situations, the proposed Deep Equivariant Hypersphere may not perform effectively.\n\nAnother concern with this paper is that, while the paper rigorously explains the theoretical properties of the proposed model, it seems to lack sufficient explanation of how to construct a practical model and training algorithm in practice. At the very least, I suggest that such information should be added to the appendix. The current version of the paper is quite challenging to follow, especially for those who are not familiar with the two prior papers authored by Melynk et al [1, 2].\n\n***\n\n[1] Melnyk, P., Felsberg, M., & Wadenb\u00e4ck, M. (2021). Embed me if you can: A geometric perceptron. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 1276-1284).\n\n[2] Melnyk, P., Felsberg, M., & Wadenb\u00e4ck, M. (2022, June). Steerable 3D spherical neurons. In International Conference on Machine Learning (pp. 15330-15339). PMLR."
            },
            "questions": {
                "value": "The paper primarily emphasizes O(n) groups and acknowledges their importance in real-world scenarios. However, within the realm of geometric deep learning literature, a significant challenge lies in developing models that can generalize effectively for broader Lie groups beyond O(n). I am curious to know if the proposed Deep Equivarint Hyperspheres can effectively learn and handle such general Lie groups.  I understand that concepts such as the spherical neuron or n-simplex used in this paper may not easily apply to other Lie groups. However, I would like to hear the authors' thoughts on the extension of steerable functions for arbitrary Lie groups."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5920/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698942486655,
        "cdate": 1698942486655,
        "tmdate": 1699636629521,
        "mdate": 1699636629521,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bSDK6e4Q1F",
        "forum": "64t9er38Zs",
        "replyto": "64t9er38Zs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5920/Reviewer_MJcE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5920/Reviewer_MJcE"
        ],
        "content": {
            "summary": {
                "value": "This paper generalizes spherical neurons to the orthogonal group of dimension greater than 3, yielding a scalable $O(n)$-equivariant architecture. Their architecture creates steerable filter banks based on the higher-dimensional simplex. They evaluate their architecture on an $O(3)$-invariant classification dataset, as well as on two synthetic $O(5)$-equivariant datasets, and obtain strong results in the low sample complexity regime."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The extension of spherical neurons to higher dimensions is novel, and this work presents an $O(n)$-equivariant architecture that is distinct from previous $O(n)$-equivariant architectures and requires nontrivial theoretical development. Their experimental results are better than their selected baselines in low sample regimes. The proofs are clearly written and thorough."
            },
            "weaknesses": {
                "value": "1. Generally, there is a severe lack of comparison to other O(n)-invariant and -equivariant architectures, such as EGNN, EMLP, vector neurons, canonicalization (alt. frame-averaging), and for invariance, simply taking inner products. O(n) equivariance is not new, so what is superior about this particular framework? If the benefit over baselines like CGENN is inference speed, this should be recorded in a table or figure. Many of these baselines were not included in the experiments, either, especially the only non-synthetic experiment (O(3)-invariant skeleton classification). It would also be helpful to articulate a theoretical comparison between the hyperspheres approach and existing approaches \u2014 which is more expressive, which has better scaling, which (if any) can be considered a special case of hyperspheres or vice versa. \n2. The presentation of the paper could be significantly improved. Not having seen spherical neurons or TetraSphere before, I found the paper\u2019s writing and architecture description very hard to follow \u2014 even after checking these original papers. I remain unsure what the guiding intuition for the hyperspheres equivariant framework is. Also, I would suggest making this paper more self-contained in a future revision. For example, steerability in section 2.3 is a crucial concept that is defined only with high-level words, whereas an equation would be important to properly understand the concept (especially since it is so overloaded). The subsequent claim that a \u201c3D steerable filter consisting of spherical neurons needs to comprise a minimum of four 3D spheres\u201d is also unclear; the background on spherical neurons in section 2.1 does not provide enough detail to resolve the meaning of this sentence without checking the original paper. Moreover, the proofs in the main body of the paper could be moved to the appendix, to make space for figure and writing that better clarifies the method and appeal of hyperspheres. For example, Proposition 7 simply states that composing equivariant layers yields and equivariant end-to-end function; this is quite standard and intuitive, and probably does not merit a full paragraph\u2019s explanation in the main body.\n3. The only experimental comparisons to a real dataset is on an O(3)-invariant action recognition dataset. However, the main innovation of the paper is for O(n) with n>3. Justification of the practical applications of O(n)-equivariance would much better motivate this work. \n4. Even on the simulated O(5) regression datasets, performance is worse than the CGENN baseline."
            },
            "questions": {
                "value": "1. How expressive is the proposed $O(n)$-equivariant architecture? Is it universal, i.e. can it represent any continuous $O(n)$-equivariant function? \n2. As the authors find suboptimal experimental performance relative to methods like CGENN which have higher order interactions between the points, is it possible that there\u2019s an expressivity gap between equivariant hyperspheres and methods which enjoy higher order equivariance?\n3. How well did each method perform without any test-set rotation augmentation on the real dataset (UTKinect-Action3D)?\n4. Why is there no comparison to equivariant baselines on the real dataset (UTKinect-Action3D)? Moreover, why is there no comparison to equivariant methods other than EMLP, such as vector neurons, EGNN, etc? \n5. How are equivariant hyperspheres related to the irreducible representations of $O(n)$, if at all? \n6. What are some practical applications of $O(n)$-equivariance, for $n>3$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5920/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698990940235,
        "cdate": 1698990940235,
        "tmdate": 1699636629414,
        "mdate": 1699636629414,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "k8X1XsCtlx",
        "forum": "64t9er38Zs",
        "replyto": "64t9er38Zs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5920/Reviewer_RNHY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5920/Reviewer_RNHY"
        ],
        "content": {
            "summary": {
                "value": "The paper presents Deep Equivariant Hyperspheres - a class of neural networks equivariant under $O(n)$ transformation group. The model is based on spherical neurons and regular $n$-simplexes. The authors present the idea of $O(n)$ equivariant features of a special type, then they study their properties when such features are composed one after another and form a deep model. \n\nWhile the theory is correct and coherent, the experimental evaluation of the method is very limited. It does not allow a reader to infer the advantage of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well-written. It uses mathematical notation when it's required and the rest is clearly explained with plane text. It makes it easy to follow the ideas of the authors\n- The presented method is alternative to more common MLP-based or graph-convolution-based models. It makes the field richer in terms of approaches. \n- The theory is mathematically correct"
            },
            "weaknesses": {
                "value": "The main weakness of the paper is that the authors do not clearly demonstrate the real-life fields where this method is most advantageous and what its limitations are. To address this, it makes sense to:\n\n1. **Organize a proper related work section:** The current version uses the Introduction and Background to position the current paper in the field of previously developed methods. This approach is rather implicit and doesn't allow the reader to properly understand which papers served as inspiration for the authors, which ones are direct competitors, and which ones are not relevant at all but are mentioned.\n\n2. **Discuss the limitations:** The authors can significantly improve the quality of the paper by explicitly discussing the limitations of the approach or even demonstrating them experimentally.\n\n3. **Describe the hyperparameters:** The current paper lacks a description of the hyperparameters of the method. It would improve the quality if the authors described the main hyperparameters of the proposed approach and conducted ablation studies on them.\n\nAnother significant weakness is the **limited experimental evaluation** of the proposed method. The current experimental results demonstrate some improvement over a subset of the competitors. However, it is achieved on small datasets, with small models and only in the very-low-data regime. The common number of training samples in the experiments for which the proposed method outperforms the others is 1000. A datasets of 1000 samples is unrealistically small for the field of machine learning nowadays."
            },
            "questions": {
                "value": "- In $O(5)$ regression experiment, CGENN significantly outperforms your method. Is the use of higher order tensor features in CGENN the only reason it performs better? If so, can you train a modification of CGENN which uses only scalar and vector features?\n- In $O(5)$ convex hulls experiment, your method perfoms worse than other, when the number of training samples increases. Can you demonstrate the effect with $10^6$ samples? \n- In both $O(5)$ experiments the proposed method reaches a plateau. While some of the other methods continue to improve. I can't infer the advantage of the proposed method from the plots. All 3 plots should be extended to a larger number of samples, because so far it looks like you demonstrated only a small part of the X-axis where your method performs well. \n- Figure 2. Left plot: your method seems to be outperformed by MLP Aug in the next step. The plot requires an extension to a larger number of samples.\n- So far, the presented method performs well only of small datasets and with small neural networks. What is the main limitation of the method? \n- What is the time consumption of the proposed method?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5920/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699107985981,
        "cdate": 1699107985981,
        "tmdate": 1699636629322,
        "mdate": 1699636629322,
        "license": "CC BY 4.0",
        "version": 2
    }
]