[
    {
        "id": "HswuhN1oHy",
        "forum": "OI3RoHoWAN",
        "replyto": "OI3RoHoWAN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3906/Reviewer_LF4g"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3906/Reviewer_LF4g"
        ],
        "content": {
            "summary": {
                "value": "In summary, this study introduces GenSim, a scalable framework harnessing large language models (LLMs) to enhance robotic policy simulation tasks. \n\nThe primary goal is to distill LLM capabilities into practical, low-level policies for robots. We delved into LLM finetuning and prompting methods, exploring both goal-directed and exploratory approaches to generate novel simulation task codes. \n\nThe study focused on table-top pick-and-place tasks, and addressing challenges in generating intricate robotic tasks remains an area for future exploration. Moreover, future research could explore algorithms for training multitask policies capable of efficiently adapting to these larger-scale generated task benchmarks, thereby paving the way for more sophisticated and realistic robotic simulations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The authors successfully leverage Large Language Models (LLMs) to tackle the issue of limited task-level diversity in robotic simulation. By tapping into the reasoning and coding capabilities of LLMs, the proposed GenSim framework autonomously designs and validates task asset arrangements and progressions, leading to the creation of over 100 tasks.\n\nSecondly, the paper presents a meticulous evaluation of the generated tasks using tailored metrics, ensuring the tasks' quality and achievability. The authors go further to compare different LLM models, including GPT4 and Code-Llama, and demonstrate that prompting and finetuning techniques based on the task library significantly enhance the LLMs' ability to generate tasks of higher quality. \n\nAdditionally, the paper showcases the practical impact of the generated tasks on language-conditioned visuomotor policies. The rigorous evaluation and comparison of different LLM models, and the tangible improvements demonstrated in the generalization capabilities of language-conditioned visuomotor policies."
            },
            "weaknesses": {
                "value": "Although the paper discusses sim-to-real transfer briefly, there is a lack of in-depth evaluation regarding the effectiveness of policies trained on LLM-generated tasks in real-world robotic applications. Generating over 100 tasks is a positive step, but the computational resources, time, and effort required for this scale of task generation are crucial aspects, especially when considering real-world applications. Without addressing these scalability issues, the framework's practical utility might be limited.\n\nThe paper acknowledges the imperfections in the evaluation metrics used for code generation, including misaligned language descriptions. However, it does not delve deeply into the potential implications of these imperfections on the quality of the generated tasks. A more comprehensive analysis of how these imperfections affect task quality and subsequent policy training is essential.\n\nCompared with the vision modules, the language planning modules are much more sophisticated. The potential of this model is not fully unlocked."
            },
            "questions": {
                "value": "Could you provide more details on the types of syntax errors and lack of grounding observed in the generated code? Understanding these specific issues would be valuable for refining the framework. Additionally, how do these errors impact the subsequent policy training and task performance?\n\nConsidering the limitations and challenges identified, what are the key areas of future research you are planning to explore? Are there specific aspects of GenSim that you aim to enhance or refine based on the feedback and insights gathered from this study?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics concerns"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3906/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698428651930,
        "cdate": 1698428651930,
        "tmdate": 1699636350762,
        "mdate": 1699636350762,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QPc8iZFKCT",
        "forum": "OI3RoHoWAN",
        "replyto": "OI3RoHoWAN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3906/Reviewer_hwME"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3906/Reviewer_hwME"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a framework for generating new tasks given a fixed number of existing tasks by leveraging the code generation capability of LLMs for robotics simulation environments. It further uses the generated over 100 tasks for training multi-task policies and evaluates them in both simulation and real world. Stronger generalization is observed by training across more diverse tasks. The idea is novel and profound for breaking the hardness of creating new robot learning tasks in simulation."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The task-level diversity is indeed a hard problem due to the required human efforts and this paper proposes a promising approach for solving it, by a novel usage of LLMs. It verifies the effectiveness of this approach and is inspiring.\n\nThe strong generalization of the multi-task policies even with zero-shot transfer on new tasks is impressive. \n\nThe paper is well-written.\n\nThe experiments are thorough for verifying the generalization capability of the policies."
            },
            "weaknesses": {
                "value": "Most of the current tasks are within the domain of top-down pick and place, and the generalization of policies within such a domain is relatively easy. It could be more impressive to see more dexterous manipulation tasks and policy generation over those.\n\nFor the failure cases of task generation, perhaps providing more examples will draw a better boundary on the limitation of the current approach, or for each model."
            },
            "questions": {
                "value": "In Sec. 3.1, for exploration task generation, which LLM is used? Is it fine-tuned or GPT models without fine-tuning? Also for the generation of 120 tasks, are the newly generated tasks used for fine-tuning as well or just as prompt examples? Please provide more explanations.\n\nFor Fig. 6, it seems GPT-4 without fine-tuning outperforms all other fine-tuned models, so is it true that using fine-tuned smaller models is just for consideration of inference cost?\n\nAs mentioned in Sec. 1 the task library is initialized with only 10 tasks, for fine-tuning the LLMs to gain context of robotic simulation tasks, is the overfitting to these 10 tasks a problem? If so, how is it handled?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3906/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698534245346,
        "cdate": 1698534245346,
        "tmdate": 1699636350660,
        "mdate": 1699636350660,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yc3yaNAF5c",
        "forum": "OI3RoHoWAN",
        "replyto": "OI3RoHoWAN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3906/Reviewer_1vRz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3906/Reviewer_1vRz"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the problem of automatically generating robotic tasks in synthetic simulation environments. A method relying on LLMs is presented, where a LLM is used to generate the code that describes a task (assets selection, initial configuration of the assets, and language/robot motion instructions). A task library is initialized using 10 examples written by a human, and the LLM is asked to generate new tasks given carefully designed prompts. The generated tasks are added to the task library if they pass validation tests (e.g. the syntax is correct, the success rate of a policy trained on this task is high enough, it passes some human inspection etc), or the feedback is explicitly given to the LLM as additional instructions in a few-shot prompting scheme. 120 tasks are generated and policies are pre-trained on these tasks. The experimental evaluation shows that pre-training on a large number of tasks increases zero-shot generalization performance on new tasks (held-out test set also generated) in both simulation and real environments after fine-tuning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "S1. The problem addressed in this paper is important. Multi-task robot policies strongly rely on diverse training data (in terms of environments and task) but access to such training data is limited by real collection efforts, or having to manually invent and design new tasks. Automatically generating such training data could have a significant impact.\n\nS2. The overall method is technically sound. The appendix provides many details. The details of the prompts which are provided to the LLM is appreciated. Their design design is key as demonstrated in Fig 6 left). However, additional intuition why the two-stage prompt chain is better than the single-prompt would be appreciated.\n\nS3. The evaluation of Table 1 is convincing. In this experiment, multiple models pre-trained on synthetic data are fine-tuned using few real demonstrations. The models pre-trained on 70 generated tasks performs significantly better than a model pre-trained on 50 generated tasks and on the 10 original tasks from CLipport. This experiment demonstrates the practical applications of the proposed approach."
            },
            "weaknesses": {
                "value": "W1. Figure 3 mentions that human inspection of a generated task is required. Was feedback collected for each of the generated tasks ? How many tasks were manually annotated in total. In addition, what is the granularity of feedback provided by the human ?\n\nW2. The experiments presented in Table 1 use a different set of tasks than the real tasks used to evaluate CLIPort. In particular, the tasks do not involve sweeping, or folding of objects. The variety of asset is also smaller than what is shown in CLIPort where scissors or chess pieces are manipulated. A similar observation is made regarding the synthetic experiments, e.g. the held-out simulation tasks used for evaluation mostly use blocks and omit more complex objects like ropes. Additional explanations on how these tasks were selected are needed to convince the readers that the method indeed generalizes to a wide variety of tasks. Experiments presented in Fig 6 left) evaluate models on the same tasks as CLIPort but it is not clear if the models were pre-trained on all generated tasks in this experiment (see W3.)\n\nW3. The \u201cfew-shot policy generalization to related tasks\u201d experiment needs additional description to understand the meaning of \u201csingle-original, single+2, single+4 etc\u201d. Were the models pre-trained on original clipport tasks, original clipport tasks + 2 generated tasks etc ? Or are the models pre-trained on all CLIPort tasks + all the ones that were generated automatically ? It is important to clarify this experiment as the current version lets the reader think that only a small subset of of the generated tasks were used to pre-train the policy, and the question of how these were selected arises.\n\nW4. Page 6 mentions \u201cWhen only pretraining on the 10 tasks in clipport, the policy does not generalize well on the GPT4 tasks.\u201d In fig 7 right) the model pre trained on tasks generated with LLMs is better than the one trained on CLipport tasks, but the difference is 15-20% of relative improvements, the claim should therefore be mitigated. It is also suggested to justify how the set of testing tasks (given in Appendix A.1) was selected. See also W2.\n\n W5. Details of the metrics used in Fig 6 would be appreciated. In particular, how is \u201ccode reasoning capability\u201d measured in \u201cruntime-verified\u201d, and is the \u201ctask completed\u201d an average over several environment resets ?"
            },
            "questions": {
                "value": "The paper addresses an important problem and the method is technically sound. The experiments demonstrate that policies pre-trained on the automatically generated tasks can generalize to novel tasks in both synthetic and real environments. I am however concerned about the diversity of the tasks which the method is evaluated on. In particular, the evaluation tasks seem to be less varied compared to the ones used in CLIPort (W2). I am also questioning the \u201cautomatic\u201d nature of the method as human intervention is required (W1). I will gladly update my review if the authors can address these concerns and respond to my questions (see weaknesses)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3906/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3906/Reviewer_1vRz",
                    "ICLR.cc/2024/Conference/Submission3906/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3906/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698772869680,
        "cdate": 1698772869680,
        "tmdate": 1700488420584,
        "mdate": 1700488420584,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PVckJUssoG",
        "forum": "OI3RoHoWAN",
        "replyto": "OI3RoHoWAN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3906/Reviewer_XopG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3906/Reviewer_XopG"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a methodology to automatically generate tasks in simulation and corresponding expert policies to increase the task diversity when training robot policies. Existing large language models are utilized for this, and a number of ablations are conducted to study various design choices of the proposed method. Experimental evaluation both in simulation and in the real world reveals that the proposed methodology leads to better generalization than the baseline."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- Interesting idea. The proposed idea is very interesting and novel.\n- Well-written. The paper is very well-written and easy to follow. The figures very much aid in understanding the paper.\n- Experimental Evaluation. The experiments reveal that the proposed methodology outperforms baseline methods by non-trivial margins. In particular, the proposed method enables both few-shot policy generalization to related tasks and zero-shot policy generalization to unseen tasks. The experiments are conducted in both simulation and in the real world, which further speak to the strength of the proposed method.\n- Ablations. Ablations are conducted to better understand various design choices of the proposed methodology. Visualizations and qualitative results also aid in this."
            },
            "weaknesses": {
                "value": "- Simplistic tasks. The experimentation is limited to table-top pick-and-place task. Furthermore, the use of a suction gripper is a weakness -- this simplifies the already simple task, and limits generalizability to other kinds of tasks. Unclear whether the proposed methodology can be adapted for more complex tasks, and how well it would do. Experiments on more complex tasks would be more convincing.\n- Experimental Evaluation. While a diverse set of experiments are conducted, only one setting (ie environment) is adopted. The results would be more convincing if the presented trends were shown to hold true across a number of settings."
            },
            "questions": {
                "value": "- A number of approaches exist in the literature for data augmentation using foundation models (e.g. ROSIE, CACTI, etc.). How do such approaches compare to the proposed methodology when it comes to generalization to unseen tasks? Arguably, some of these are simpler to implement and use.\n- What is done in the cases where the output of the LLM has a syntactical error and cannot be run?\n- How much prompt engineering went into this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3906/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3906/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3906/Reviewer_XopG"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3906/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698827863422,
        "cdate": 1698827863422,
        "tmdate": 1699636350505,
        "mdate": 1699636350505,
        "license": "CC BY 4.0",
        "version": 2
    }
]