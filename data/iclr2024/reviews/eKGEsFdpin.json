[
    {
        "id": "q8TQhb3osa",
        "forum": "eKGEsFdpin",
        "replyto": "eKGEsFdpin",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7265/Reviewer_gCgZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7265/Reviewer_gCgZ"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the challenge of identifying texts generated by Large Language Models (LLMs), such as ChatGPT, in the context of the potential harms posed by machine-generated misinformation and plagiarism. The authors propose a novel watermarking method aimed at embedding a unique, algorithmically identifiable pattern within machine-generated texts. Unlike existing methods, this approach intervenes in the token sampling process during text generation, ensuring that the generated content remains coherent and natural to human readers while carrying distinct, detectable markers.\n\nThe proposed watermarking method is model-agnostic and robust against token-level paraphrasing attacks. Through extensive experiments, the authors demonstrate the effectiveness of their approach, showing that it can accurately detect watermarked texts in almost all cases without significantly compromising the textual quality."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The major strengths of the paper include:\n\n1. Robustness to Attacks: The proposed watermarking method has been designed to be robust against token-level paraphrasing attacks, ensuring that the watermark remains detectable even when parts of the text are altered.\n\n2. Model-Agnostic Approach: The watermarking method is model-agnostic, meaning it can be applied across various Large Language Models (LLMs), making it versatile and widely applicable.\n\n3. Comprehensive Evaluation: The authors conducted extensive experiments to evaluate the effectiveness of their watermarking scheme in distinguishing between watermarked and non-watermarked text, achieving high detection rates while maintaining textual quality."
            },
            "weaknesses": {
                "value": "1. Limited Exploration of Attacks: The paper primarily focuses on token-level paraphrasing attacks for evaluating the robustness of the watermarking method. Other types of attacks, such as deletion, unicode attacks, and human paraphrasing, are mentioned as areas for future exploration.\n\n2. Dependency on Datasets and Prompts: The performance of the watermarking method seems to be influenced by the given prompts and datasets used in the experiments. For instance, watermarking the output to factual questions with limited flexibility in answers is noted as challenging.\n\n3. Focus on Text Completion Tasks: The evaluation of the watermarking method is mainly conducted in the context of text completion tasks. The applicability and effectiveness of the watermark across different downstream tasks, such as question-answering and summarization, are suggested as areas for future evaluation.\n\nAll the above limitations have been mentioned in future work, which implies the authors still have substantial work to complete to make the results more convincing."
            },
            "questions": {
                "value": "1. Can the proposed method be extended to watermark multiple models?\n2. The performance of the proposed method is not significantly better than the baseline method and is influenced by various factors such as the models used, the datasets, and the given prompts. I doubt the generalizability of the proposed method based on the current experiments."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7265/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698715172817,
        "cdate": 1698715172817,
        "tmdate": 1699636866532,
        "mdate": 1699636866532,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qYz2TV6xRg",
        "forum": "eKGEsFdpin",
        "replyto": "eKGEsFdpin",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7265/Reviewer_b2Mo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7265/Reviewer_b2Mo"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a decoding procedure for embedding a statistical watermark in samples from an autoregressive language model.\n\nThe idea is to sample multiple candidate tokens from the model at each step i, and choose the candidate that maximizes a random number X_i generated by seeding a PRNG with the SHA256 hash of the candidate token together with the previous k tokens in the sequence. Text decoded in this way can be detected by calculating the average average of X_i, and testing the hypothesis that this mean deviates from the null hypothesis (no maximization). There are several hyper-parameters to this algorithm: the length of the hash sequence, the number of resamples, and whether to sample with or without replacement.\n\nDifferent hyper-parameter configurations of the proposed method are compared to a baseline watermarking procedure proposed by Kirchenbauer et. al. using the OPT-1.3B, BTLM-3B, and Llama2-7B models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The proposed method is easy to understand and simple to implement."
            },
            "weaknesses": {
                "value": "The setting (watermarking, robustness to attacks) and methodology (decoding algorithms, hashing) of this paper are quite similar to Kirchebauer et al., and I am not convinced that the newly proposed method is an significant improvement over the Kirchenbauer baseline. There is no formal analysis of the proposed watermark, and the experimental results are a step back compared to the breadth of evaluations and attacks presented in the Kirchenbauer baseline paper. Notably missing here are studies of watermark strength as a function of sequence length, and robustness beyond a simple substitution attack.\n\nThe discussion of the proposed watermark suffers from both overclaiming and insufficient analysis. \"In our work, we interfere the sampling process without changing LLMs\u2019 probability distribution over vocabulary while Kirchenbauer et al. (2023a) interfere the probability distribution.\" This is not true. The proposed algorithm is a uniform resampling over candidate tokens (assuming the SHA256 hash behaves well) which clearly changes the distribution; this change might be amenable to a clean mathematical description.\n\nI am not fully convinced of the metrics chosen to evaluate the quality of generated text. Why is a paraphrasing similarity model (P-SP) being used to evaluate generation quality? Is the premise that the generated text to be similar/paraphrasing of the human text; isn't this contrary to the premise of open-ended text generation? This seems like a misapplication of P-SP. Why not use, e.g., sample perplexity under a larger LM (as used as a proxy for quality in the Kirchenbauer watermarking paper).\n\nThe detectability results in Table 1 emphasize a regime where all proposed watermarks work well. The claim of superior detectability vs. the Kirchenbauer watermark is based on z-scores of ~17 vs ~10. In the more challenging paraphrasing attack setting, there seems to be a significant performance advantage only for sampling without replacement (Figure 1; SWOR). But if we believe the proposed sample quality metrics, SWOR causes significant degradation in sample quality (Table 3). If anything, I suspect the metrics underestimate the degradation caused by SWOR: for low-entropy predictions, it forces the model to sample uniformly among unlikely candidates which seems quite bad."
            },
            "questions": {
                "value": "Why do the reports of experimental results distinguish between a SWR detector and a SWOR detector? Aren't these the same algorithm?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7265/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698883943274,
        "cdate": 1698883943274,
        "tmdate": 1699636866432,
        "mdate": 1699636866432,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FPVr7DOOc8",
        "forum": "eKGEsFdpin",
        "replyto": "eKGEsFdpin",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7265/Reviewer_bSV5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7265/Reviewer_bSV5"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a watermarking scheme by interfering with the randomness of generating the next token. The sampling watermarkers first multinomially sample some tokens and then choose the token that can maximize the secret number as the next token. Experiments show that the watermark is detectable and robust against token-level paraphrasing attacks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed watermarking scheme offers a way to compute the statistical confidence interval to analyze the sensitivity of the watermark.\n2. Experiment results reveal that the watermarking scheme only slightly decreases the quality of the generated text.\n3. The watermarking scheme is also robust against token-level paraphrasing attacks."
            },
            "weaknesses": {
                "value": "1. The main idea of this approach seems very similar to the one proposed by Kirchenbauer et al. (2023a). The algorithm in [Kirchenbauer 2023a]:\ni. Compute the probability distribution of the next token\nii. Use the previous tokens and a hash function to randomly partition the vocabulary into\n\u201cgreen list\u201d and \u201cred list\u201d\niii. Modify the probability distribution and then sample the next token\nThe algorithm proposed in this paper:\ni. Compute the probability distribution of the next token\nii. Use the previous tokens and a hash function to randomly generate the secret number for the candidate tokens.\niii. Choose the next token based on the secret number\nIf we treat the sampled candidate tokens as \u201cgreen list\u201d and other tokens as \u201cred list\u201d, then these two algorithms are very similar. The proposed approach just changes the partition method and the hash function. It would be better if the authors could provide more intuitions about what is the main difference between the proposed approach and that in [Kirchenbauer et al. (2023a)]. Otherwise, the contribution of this work seems limited.\n\n2. The authors only evaluate the robustness of text by text substitution attack. This work would be better if the authors could evaluate the robustness against text deletion and text insertion attacks.\n\n3. This approach does not have a factor that can control the strength of the watermark. It always chooses the candidate token that can maximize the secret number. If we can control the strength of the watermark injection, then we can balance the tradeoff between the quality of generated watermarked text and the strength of the watermark."
            },
            "questions": {
                "value": "1. What is the main difference between the proposed approach with Kirchenbauer et al. (2023a)?  The proposed approach does not interfere with the probability distribution of LLM, but it chooses the tokens according to their secret numbers instead of the original probability distribution. Is this just another way to change the probability distribution implicitly because the proposed approach adds some \u201clogits\u201d to the token with the largest secret number?\n2. Why does the proposed approach work better than Kirchenbauer et al. (2023a)? Instead of changing the interference from a probability distribution to the sampling process, are there other reasons the proposed approach is better?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7265/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698902535275,
        "cdate": 1698902535275,
        "tmdate": 1699636866309,
        "mdate": 1699636866309,
        "license": "CC BY 4.0",
        "version": 2
    }
]