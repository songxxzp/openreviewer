[
    {
        "id": "jixmHVgPj3",
        "forum": "aaBnFAyW9O",
        "replyto": "aaBnFAyW9O",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4096/Reviewer_nQBi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4096/Reviewer_nQBi"
        ],
        "content": {
            "summary": {
                "value": "This paper...\n- theoretically shows DMs suffer from an expressive bottleneck due to the assumption that the denoising distribution is Gaussian,\n- proposes soft mixture denoising to address this problem,\n- shows SMD improves the performance of DMs on various datasets."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This paper proposes a novel approach to improving the sampling efficiency of diffusion models.\n- SMD is well-motivated through rigorous theoretical analysis.\n- This paper well-written and I had no trouble following the logic.\n- There are non-trivial performance improvements after applying SMD."
            },
            "weaknesses": {
                "value": "- SMD requires training of additional $g_\\xi$ and $f_\\phi$ networks, so I would expect training SMD requires more VRAM and time compared to training standard diffusion models. A comparison of VRAM / training time / inference time of SMD vs. standard diffusion would be insightful."
            },
            "questions": {
                "value": "- Is SMD compatible with fast samplers such as EDM [1]? If it is, can the authors provide results? If not, can the authors suggest how SMD could be modified to be compatible with such fast samplers?\n- How does the performance of SMD vary as we change the size of $g_\\xi$ and $f_\\phi$ networks? Does SMD work better if we use larger networks or is it sufficient to use small networks?\n\n[1] Elucidating the Design Space of Diffusion-Based Generative Models, Karras et al., NeurIPS, 2022."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4096/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4096/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4096/Reviewer_nQBi"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4096/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698221323935,
        "cdate": 1698221323935,
        "tmdate": 1699636374651,
        "mdate": 1699636374651,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ihRqp6ghZO",
        "forum": "aaBnFAyW9O",
        "replyto": "aaBnFAyW9O",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4096/Reviewer_mAPH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4096/Reviewer_mAPH"
        ],
        "content": {
            "summary": {
                "value": "This paper studied the reverse process in the diffusion models. Specifically, the authors theoretically showed that the Gaussian assumption in the reverse process of the original diffusion models is not expressive enough for complicated target distribution, and proposed a soft-mixture model for the reverse denoising process. The authors theoretically demonstrated the expressiveness of the new model, and derived training and sampling algorithms for it. Experiments have been conducted to demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The idea is new and reasonable.\n\n2. The authors provided theoretical foundations for their proposed method.\n\n3. The effectiveness of the proposed method has been empirically verified."
            },
            "weaknesses": {
                "value": "To me, there is no significant weakness of this work."
            },
            "questions": {
                "value": "To my knowledge, there are studies considers better parameterizing the distribution in the reverse process, such as:\n\n1. Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling the Generative Learning Trilemma with Denoising Diffusion GANs. ICLR, 2022.\n2. Yanwu Xu, Mingming Gong, Shaoan Xie, Wei Wei, Matthias Grundmann, Kayhan Batmanghelich, and Tingbo Hou. Semi-Implicit Denoising Diffusion Models (SIDDMs). arXiv:2306.12511, 2023.\n\nThe authors should discuss these studies, and better to empirically compare with them."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4096/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698313294996,
        "cdate": 1698313294996,
        "tmdate": 1699636374558,
        "mdate": 1699636374558,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kCXc7ADffr",
        "forum": "aaBnFAyW9O",
        "replyto": "aaBnFAyW9O",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4096/Reviewer_eLFe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4096/Reviewer_eLFe"
        ],
        "content": {
            "summary": {
                "value": "this submission introduces soft mixture denoising for improving the expressive bottleneck of diffusion models. It first shows that diffusion models have an expressive bottleneck in the backward denoising steps, when approximating p(xt-1|xt) using a Gaussian distribution, that leads to unbounded local and global denoising. It then proposes soft mixture denoising (SMD) that approximate the backward step p(xt-1|xt) using a  gaussian mixture distribution, where the number of modes is infinity. This soft gaussian mixture is a universal approximator for continuous probability distributions and the result shows that the local and global errors would be bounded. Experiments with image datasets indicate that SMD improves different diffusion models such as DDPM and DDIM."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Improving the design of diffusion models and make them more efficient is a timely problem\n\nIdentifying the expressiveness bottleneck of single gaussian approximation, and the unbounded denoising errors is novel for denoising diffusion models\n\nThe experiments are extensive"
            },
            "weaknesses": {
                "value": "The performance gains for the new soft mixture models are not significant. One would expect a significant reduction in number of steps if soft mixture is a better approximation for p(xt-1|xt), but that is not the case in the experiments. \n\nThe architectural changes for the new denoising networks are not discussed well. It\u2019s a bit confusing how the"
            },
            "questions": {
                "value": "The mean parameterization in eq. 11 needs to be clarified? What is the hyper network representation? what is \\theta \\cup f_{\\phi}?\n\nWhile the theory supports that soft gaussian mixture to be a universal approximator. However, the performance gains compared with single Gaussian are not significant. What are the limitations and approximations that led to that? Could it be the identity assumption for the covariance matrix?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4096/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698717300748,
        "cdate": 1698717300748,
        "tmdate": 1699636374462,
        "mdate": 1699636374462,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0fBpTE0Jxt",
        "forum": "aaBnFAyW9O",
        "replyto": "aaBnFAyW9O",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4096/Reviewer_izts"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4096/Reviewer_izts"
        ],
        "content": {
            "summary": {
                "value": "This paper identifies an expressive bottleneck in the backward denoising process of current diffusion models, challenging the strong assumptions underlying their theoretical guarantees. The authors demonstrate that these models can incur unbounded errors in both local and global denoising tasks. To address this, they introduce Soft Mixture Denoising (SMD), a more expressive model that theoretically can approximate any Gaussian mixture distribution. The effectiveness of SMD is validated through experiments on various image datasets, particularly noting significant improvements in diffusion models, like DDPM, with few backward iterations."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is articulate and presents a clear logical progression.\n\n2. Both theoretical exposition and experimental verification are provided to substantiate the authors\u2019 arguments."
            },
            "weaknesses": {
                "value": "1. The critique leveled against existing diffusion models seems to be somewhat overstated. These models have achieved considerable success across various applications and can represent complex distributions effectively. The alleged expressive bottleneck is contingent upon the noise scheduling strategy deployed. For instance, in typical diffusion models, a small value of $\\beta_t$, such as 0.0001, is assumed to be used as the initial. As indicated in Equation (25), the transition probability $q(x_{t-1} | x_t)$ approaches a Gaussian distribution as $\\beta_t$ tends toward zero, which contradicts the claim of an inherent expressive limitation.\n\n2. The selection of datasets for experimentation\u2014LSUN and CelebA\u2014seems narrow given the criticism of diffusion models' multimodality capacity. For a robust evaluation, a more complex and varied dataset like ImageNet, encompassing 1k categories, would be more appropriate.\n\n3. There appears to be a flaw in the derivation of local denoising error $M_t$. The associated loss term in $L_{t-1}$ is predicated on the KL divergence $KL[q(x_{t-1} | x_t, x_0) || p_\\theta(x_{t-1} | x_t)]$. Here, $q(x_{t-1} | x_t, x_0)$, which is a known Gaussian distribution, should not be conflated with $q(x_{t-1} | x_t)$, which represents an unknown distribution. The validity of Theorems 3.1 and 3.2 is reliant on the accurate definition of $M_t$.\n\n4. The paper does not reference the FID (Fr\u00e9chet Inception Distance) results from the Latent Diffusion Model (LDM) study. In the LDM research, the reported FID scores were 4.02 for LSUN-Church and 5.11 for CelebA-HQ, which are superior to the performance metrics achieved by SMD as presented in this paper. This omission is significant as it pertains to the comparative effectiveness of the proposed model."
            },
            "questions": {
                "value": "1. There seems to be a typographical error involving a comma in the superscript at the end of Equation (3).\n2. Could you detail the noise schedule utilized in your algorithm? The experiments section suggests that the original DDPM scheduling is retained while only the model component is modified. Considering that your paper emphasizes the importance of shorter chains and the expressiveness issue within them, it would be beneficial to see experimentation with significantly fewer steps to underscore the advantages of your proposed Soft Mixture Denoising (SMD).\n3. The SMD approach bears resemblance to a Variational Autoencoder (VAE) in its structure. Could you confirm if this observation is accurate or elaborate on the distinctions between SMD and VAE?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4096/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4096/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4096/Reviewer_izts"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4096/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699221675677,
        "cdate": 1699221675677,
        "tmdate": 1700515120215,
        "mdate": 1700515120215,
        "license": "CC BY 4.0",
        "version": 2
    }
]