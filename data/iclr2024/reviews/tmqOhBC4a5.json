[
    {
        "id": "ILVMilGTaN",
        "forum": "tmqOhBC4a5",
        "replyto": "tmqOhBC4a5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5524/Reviewer_9JkX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5524/Reviewer_9JkX"
        ],
        "content": {
            "summary": {
                "value": "This work addresses cooperative multiagent reinforcement learning (MARL). It builds on the idea of MaxEnt RL, and proposes the maximum entropy heterogeneous-agent reinforcement learning (MEHARL) framework for learning stochastic policies in MARL. On the theoretical/technical side, it uses the PGM to derive the MaxEnt MARL objective, and prove monotonic improvement and QRE convergence properties for the corresponding HASAC algorithm, as well as the unified MEHAML template. On the empirical side, HASAC has been implemented on six benchmark MARL tasks and it achieves the best performance in 31 out of 35 tasks across all benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Excellent presentation. The methodology is well motivated with an illustrative matrix game example. The related work is well discussed and the contribution of this paper is clear. The paper is overall well structured and easy to follow. \n\n2. Clear technical contribution. The algorithmic framework MEHAML as well as the specific practical algorithm HASAC are novel and theoreticall grouned with proofs on the nonotonicity improvement and convergence to QRE. The method is not a simple combination of MaxEnt RL and MARL, but the derivation of the MARL version is built on the PGM formulation connecting from the idea of control as an inference task.\n\n3. Superior empirical performance over a wide spectrum of benchmark tasks -- HASAC has been implemented on six benchmark MARL tasks and it achieves the best performance in 31 out of 35 tasks across all benchmarks."
            },
            "weaknesses": {
                "value": "I don't see a major weakness. \n\nA minor point: the IGO assumption is not explained when first being introduced. I don't think people outside MARL are familiar with this term."
            },
            "questions": {
                "value": "The MEHAML framework (or HASAC) is proved to converge to QRE, but not the optimal NE. But it seems that empirically HASAC does learn a good equilibrium due to the stochastic policies. I am curious if is it possible at all to have some sort of guarantees to reach the optimal NE?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5524/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698465348780,
        "cdate": 1698465348780,
        "tmdate": 1699636566293,
        "mdate": 1699636566293,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OTR5luNcIc",
        "forum": "tmqOhBC4a5",
        "replyto": "tmqOhBC4a5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5524/Reviewer_iakN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5524/Reviewer_iakN"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a MaxEnt MARL approach that employs sequential policy updates and uses a centralized Q-function. They provide the representation of QRE policies maximizing the MaxEnt objective of MARL and demonstrate that the joint policy updated through the sequential policy updates converges to the joint QRE policy. The proposed practical algorithm is simple and outperforms the baselines on various benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1.\t(Theoretical) With the multi-agent soft Q-value function defined in the main paper, the authors demonstrate that through the sequential policy updates, the joint policy converges to a QRE policy while monotonically improving the objectives for the policy and Q-value function.\n\n2.\t(Contribution) The authors provide remarkable combinations for the MaxEnt problem of those that exist in previous works; the monotonic improvement and the convergence to a QRE policy for the MaxEnt objective seem to be rigorous, and they extend the MaxEnt MARL problem to the general one with possible constraints.\n\n3.\t(Simplicity) The proposed practical algorithm is straightforward. The algorithm doesn\u2019t require recomputing Q-value estimation for the sequential policy updates.\n\n4.\t(Experimental) On a bunch of benchmarks, the proposed algorithm consistently achieves superior performance and high sample efficiency, compared to the baselines."
            },
            "weaknesses": {
                "value": "1. (Unclear effect of the sequential updates) Although there is an example [1] to show why the sequential policy updates for the standard MARL objective are needed, an example or technical explanation for the MaxEnt MARL objective is also needed. It is because the authors define the multi-agent soft Q-function (eq. (7)) and local policy update (eq. (8)). Also, the practical objective for the policy (eq. (10)) can be reduced to the expectation of $\\alpha\\log\\pi_{\\phi_{i_m}}^{i_m}-Q_{\\pi_{old}}^{i_{1:n}}$, which is consistent with the pseudocode of HASAC, since the MA soft Q-function consists of the centralized Q-function $Q_{\\pi_{old}}^{i_{1:n}}$ and the entropy term of its complementary agents, which is not subject to optimize. So, in the proposed algorithm, the sequential policy update may be just additional sampling actions of some agents.\n\n2. (Sensitive to the entropy temperature) The Ablation study shows that the proposed algorithm may not be robust to the entropy temperature and converge to different policies according to the temperature. In this paper, each domain has a different entropy temperature; one domain has a fixed temperature, and another has an automatic temperature with a fixed target entropy. A more effective method to tune entropy terms is needed, like ADER[2].\n\n[1] Kuba et al., Trust Region Policy Optimisation in Multi-Agent Reinforcement Learning, ICLR 2022.\n\n[2] Kim et al., An Adaptive Entropy-Regularization Framework for Multi-Agent Reinforcement Learning, ICML 2023."
            },
            "questions": {
                "value": "For the weaknesses, could you provide results of MASAC, which is the HASAC without the sequential policy updates, on the benchmarks and the idea of an adaptive entropy temperature tuning method for better exploration?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5524/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5524/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5524/Reviewer_iakN"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5524/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698503129241,
        "cdate": 1698503129241,
        "tmdate": 1700197339880,
        "mdate": 1700197339880,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qe77jl5L5V",
        "forum": "tmqOhBC4a5",
        "replyto": "tmqOhBC4a5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5524/Reviewer_ZAWK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5524/Reviewer_ZAWK"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a novel algorithm, Heterogeneous-Agent Soft Actor-Critic (HASAC), based on the Maximum Entropy (MaxEnt) framework. The paper theoretically proves the monotonic improvement and convergence properties of HASAC to a quantal response equilibrium (QRE). The authors also introduce a generalized template, Maximum Entropy Heterogeneous-Agent Mirror Learning (MEHAML), which provides any induced method with the same guarantees as HASAC. The proposed methods are evaluated on six benchmarks, demonstrating superior performance in terms of sample efficiency, robustness, and exploration."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well-structured and clearly written, making it easy to follow the authors' line of thought.\n2. The authors provide a comprehensive theoretical analysis of the proposed methods, including proofs of monotonic improvement and convergence to QRE.\n3. The proposed methods are evaluated on a variety of benchmarks, demonstrating their versatility and effectiveness."
            },
            "weaknesses": {
                "value": "1. The novelty of the paper is limited, as the main contribution is the application of the Soft Actor-Critic (SAC) algorithm to the multi-agent setting.\n2. The authors should have tested their method in scenarios where sample efficiency is crucial (such as real robots, stock exchange, etc), given that their proposed method is off-policy.\n3. The validity of the experimental results is questionable. The training curves show significant fluctuations, and the authors only present a selection of results in the main paper, which may give a biased view of the method's performance.\n4. The authors should provide more comprehensive experimental results, including results from a larger number of seeds, to fully demonstrate the effectiveness of their method."
            },
            "questions": {
                "value": "1. Could the authors provide more details on how sensitive is the performance of HASAC to the choice of \u03b1?\n2. How does the proposed method perform in scenarios where sample efficiency is crucial? Could the authors provide experimental results in such scenarios?\n3. Could the authors provide more comprehensive experimental results, including results from a larger number of seeds ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5524/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698658062051,
        "cdate": 1698658062051,
        "tmdate": 1699636566097,
        "mdate": 1699636566097,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pX8TiasVf0",
        "forum": "tmqOhBC4a5",
        "replyto": "tmqOhBC4a5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5524/Reviewer_YGsv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5524/Reviewer_YGsv"
        ],
        "content": {
            "summary": {
                "value": "This paper considers the problem of co-operative Multi-Agent Reinforcement Learning, where issues of sample complexity, training instability, and sub-optimal exploration affect leading methods. The authors propose a method for learning stochastic policies to overcome these limitations, by drawing a connection with Graphical models and deriving a familiar Maximum Entropy solution optimization approach.\n\nThe paper is well written, seems comprehensive in it's theoretical establishment of the new method, and thorough in the range of depth of empirical evaluations.\n\nI am familiar with single-agent RL (and have a background in Inverse Reinforcement Learning theory), however am only tangentially aware of work in the multi-agent RL setting. As such, I may have overlooked details or not been aware of relevant prior work when reviewing this paper. I have read the paper, and skimmed the appendices, however did not do a detailed check of the proofs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* Well written, easy to follow the argument development. Seems to engage thoroughly with prior work.\n * Empirical evaluations are strong, and results support the conclusions"
            },
            "weaknesses": {
                "value": "* The contribution of the method in part hinges on the limitations induced by the 'IGO' assumption from prior work (Sec 2, p2), but this is never elaborated on in the paper. Can you define IGO more clearly and explain exactly what limitations this assumption introduces? This will help the reader not intimately familiar with MARL.\n* The proposed methods introduces hyper-parameters, notably the temperature term $\\alpha$ and the drift functional and neighborhood operator. However any alternate method will also have hyper-parameters, so this isn't a big drawback. Some elaboration of the 'automatically adjusted' $\\alpha$ schedule (citation #9) just before the heading for Sec. 6 might be helpful for the reader here."
            },
            "questions": {
                "value": "# Questions and comments\n\n* It seems the design of the drift functional and neighborhood operators will be key to the success of the proposed HASAC, or MEHAML based methods (as you note in Sec. 6). Can you provide any comment on what factors should be taken into consideration in the construction of these terms? E.g. in what ways will this depend on the nature or definition of the MARL task? Some discussion of the design/selection of these terms for your empirical experiments might be helpful here.\n * The core method (e.g. end of Sec.4 on p7) seems to have high-level similarities to PPO methods for single-agent RL (e.g. constraint to keep the policies from drifting too far) - do you see any connection to this family of methods? Is this something that could be explored further in the literature or has been already?\n * The notion of Quantal Response Equilibrium is key to this optimization objective, but I'm not familiar with this term. You provide a citation (#20, also #6), but the paper would be strengthened with a little bit more explanation of this notion in Sec 4.1. E.g. can you give some intuition for what this objective means in practice compared to regular Nash Equilibrium? In what situations is QRE to be preferred over NE?\n * What is the $\\omega$-limit (Point 4 in Theorem 3) - I could not find a definition and am not familiar with this terminology.\n\n# Minor and grammatical points\n\n * Under heading 5.1 - '2 hundred' - could write as '200'\n * There are a lot of acronyms in this paper - please consider adding a table of acronym definitions in the appendix to aid readers."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5524/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5524/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5524/Reviewer_YGsv"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5524/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699233318290,
        "cdate": 1699233318290,
        "tmdate": 1700796117246,
        "mdate": 1700796117246,
        "license": "CC BY 4.0",
        "version": 2
    }
]