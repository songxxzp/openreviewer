[
    {
        "id": "Ehkp92YsYu",
        "forum": "WdhtdjoaVw",
        "replyto": "WdhtdjoaVw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3285/Reviewer_kp8E"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3285/Reviewer_kp8E"
        ],
        "content": {
            "summary": {
                "value": "Bayesian neural networks (BNNs) are powerful for uncertainty quantification. However, the connection between the parameter space and the function space of NNs is highly nontrivial---it is thus hard to impose an interpretable prior over functions on BNNs. Functional variational BNNs (FVBNNs) can alleviate this issue. However, they are defined through the KL-divergence which leads to some pathologies. The main issue is that the KL-divergence-based variational objective is often ill-defined for FVBNNs since absolute continuity between the variational posterior and the prior is needed. \n\nTo alleviate these issues, the authors propose to \"translate\" functional BNNs problems into parametric ones. First, they match the (arbitrary) functional prior to the same parametric family as the variational posterior (e.g. Gaussians) by minimizing the 1-Wasserstein distance, approximated using samples. Then, they use the resulting parametric prior for standard parametric variational inference. These two steps can be collapsed into one, resulting in the Functional Wasserstein Bridge Inference (FWBI) objective.\n\nExperimental results show that FWBI is better than previous KL-based FVBNNs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- FWBI as a method is sound and backed by a guarantee (Prop. 1) that it results in a valid lower bound to the marginal likelihood.\n- FWBI achieves better results than previous FVBNNs."
            },
            "weaknesses": {
                "value": "There are two main issues from my point of view. I'm happy to increase my score if they are sufficiently addressed.\n\n**Baseline selection** \n\nWhile the experiment setup is good (it encompasses different applications such as regression, bandits, and classification), it ignores non-variational baselines, such as the deep kernel learning (DKL, Wilson et al., 2016) and the Laplace approximation (LA, Daxberger et al., 2021). They are both function-space BNNs---for the latter, it is given by the linearized (Immer et al., 2021) and the last-layer Laplace (Riquelme et al., 2018) formulations. They have been shown to be very scalable (for the last-layer version, even applicable to transformers) and yield good uncertainty quantification performance. Moreover, have also been applied in similar experimental setups as in the present paper (e.g. bandits (see [Sec. 3.2 here](https://arxiv.org/pdf/2310.00137.pdf)) or [Bayesian optimization](https://arxiv.org/abs/2304.08309)). So I think comparing the FWBI with a Laplace baseline is a must, both in terms of performance and costs. The latter is very important since it dictates whether a method is practical or not.\n\nRegarding Tab. 2, the classification performance presented is very bad. I suggest the authors use more standard networks for those problems. E.g. LeNet-5 for MNIST and FMNIST, and ResNet for CIFAR-10. Otherwise, Tab. 2 will only cast FWBI in a bad light.\n\n**Unclear contributions**\n\nThe authors mention that \"prior distillation\"---the first step in FWBI---and \"Wasserstein variational inference\"---the second step of FWBI, have been done separately before. This makes FWBI seem to simply combine two prior things together. Could the authors comment on this?\n\nThe \"Related Work\" section is supposed to address this confusion. However, I find that that section simply describes prior works without discussing the differences between them and the authors' present work. I suggest the authors to rework that section for better clarity."
            },
            "questions": {
                "value": "First, some minor suggestions: I believe Algorithm 2 and Prop. 1 should not be in the appendix. I think they are integral parts of the paper and can be very useful for the readers to gain intuition about the proposed method.\n\nSome questions:\n\n1. Prop. 1 tells us that the FWBI objective is a lower bound to the standard ELBO. While it is good that FWBI defines a lower bound to the marginal likelihood, I think the result is incomplete. For example, a sufficiently small constant function will also be a valid lower bound, but obviously, it is a bad lower bound. Could you please comment on how good is the FWBI objective? \n\n2. Still in Prop. 1: Ignoring the obvious flaws presented in Sec. 2, can you comment on why FWBI yields better performance than other (KL-based) FVBNNs even though the KL-ELBO is a tighter lower bound?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3285/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697832821227,
        "cdate": 1697832821227,
        "tmdate": 1699636277268,
        "mdate": 1699636277268,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UH6agU9H4f",
        "forum": "WdhtdjoaVw",
        "replyto": "WdhtdjoaVw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3285/Reviewer_ExHU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3285/Reviewer_ExHU"
        ],
        "content": {
            "summary": {
                "value": "In pursuit of assigning meaningful functional priors and securing well-behaved posteriors, this study introduces a groundbreaking approach termed \"Functional Wasserstein Bridge Inference\" (FWBI), diverging from traditional parameter space variational inference. The manuscript commences by critically examining the limitations inherent in the use of Kullback-Leibler (KL) divergence. Subsequently, it unveils a two-step variational inference technique, capitalizing on functional priors and a bridging distribution to directly approximate the posterior."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. **Solid Theoretical Analysis**: The theoretical groundwork laid out in your paper is both rigorous and comprehensive. Notably, it addresses the important question of meaningful priors within the framework of variational inference, effectively advancing the understanding of this topic.\n\n2. **Estimation of Prior**: I highly commend your innovative use of the optimal transport technique for prior estimation, a commendable departure from the more conventional approach of employing iid Gaussian distributions. This choice adds a layer of sophistication to your work and enhances its theoretical soundness.\n\n\n3. **Ample Experiments**: The experimental design and execution in your manuscript are both abundant and well-executed, effectively corroborating your proposed work. The results lend considerable credibility to your methodology, reinforcing the paper\u2019s overall impact."
            },
            "weaknesses": {
                "value": "1. **Evaluation Index**: My expertise primarily lies in regression tasks, and I wonder whether the authors have considered employing alternative evaluation metrics such as calibration curves, as suggested in Reference [1]. The inclusion of such metrics could potentially provide a more comprehensive assessment of the model's performance.\n\n2. **Paired-sample $t$-test**: I recommend that the authors include a paired-sample $t$-test in the results section. This statistical test would serve to demonstrate the model's superiority more convincingly, and it is generally considered a robust method for comparing the means of two related groups.\n\n3. **Definition of Convergence**: Upon reviewing Appendix D.3, I noted that the convergence line appears to be based on the learning objective. Could you please clarify your definition of convergence within this context? Furthermore, is it possible to theoretically prove the algorithm's convergence, given your chosen definition?\n---\nReferences:  \n[1]. Kuleshov V, et. al, Accurate uncertainties for deep learning using calibrated regression[C]//International conference on machine learning. PMLR, 2018: 2796-2804."
            },
            "questions": {
                "value": "The questions are listed in weakness. I would raise my score if the authors answer weakness 1) and  3)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3285/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3285/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3285/Reviewer_ExHU"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3285/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698345752286,
        "cdate": 1698345752286,
        "tmdate": 1699636277172,
        "mdate": 1699636277172,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yhyvrETedc",
        "forum": "WdhtdjoaVw",
        "replyto": "WdhtdjoaVw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3285/Reviewer_n4ZG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3285/Reviewer_n4ZG"
        ],
        "content": {
            "summary": {
                "value": "The authors focus on functional priors for Bayesian neural networks, which allow the incorporation of  better-motivated priors rather than the _naive_ weight-space priors that are usually. For this task, they build an ELBO that consists of two additional Wasserstein-based loss terms regularizing the variational posterior towards the desired functional prior."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The method is well-motivated and theoretically justified\n- It is evaluated on several data sets consisting of regression and classification tasks and shows clear improvements upon its baselines\n- The paper is well-written and easy to follow"
            },
            "weaknesses": {
                "value": "- While the paper already evaluates several priors (RBF, Mat\u00e9rn, linear kernels, and a BNN), it lacks some more interesting ones, e.g., how would a periodic kernel-based prior (as mentioned in the introduction) perform in Figure 2? \n- The contribution section promises \"reliable uncertainty estimation\", while barely providing such an experiment in the form of OOD detection. A more detailed evaluation would involve reporting predictive log-likelihoods for all regressions and classification experiments (in addition to the current RMSE/Accuracy), as well as, e.g., expected\nclassification errors for the classification setup\n- For Figures 1/2 the mean fit looks a lot better than for the baselines, but still lacks a lot with respect to predictive uncertainty. Compare this to Figure 1 in Wild et al. (2022) who use the same function\n- Sec 2, second paragraph claims \"if prior and likelihood distributions are both assumed to be Gaussian, the posterior $p(\\mathbf{w}|\\mathcal{D})$ can be solved analytically as a Gaussian. As the posterior for neural-network-based mappings within the likelihood is highly multimodal, this is a false statement. \n- Similarly, on page 3 (top) it is stated that parameter-space variational objectives can be optimized if the variational posterior \"is assumed to be a fully-factorized Gaussian distribution\". While true, the sentence reads as if that were the only case, but $q$ could also be a variety of other distributions and still remain tractable to stochastic gradient descent.\n\n\n_Note: Especially the experimental concerns are what keeps the current score at six instead of eight which it would otherwise be in my opinion._"
            },
            "questions": {
                "value": "- Figure 1/2 only visualize an FBNN baseline. What would a corresponding figure look like for your main competitor, i.e., GWI?\n- How does the posterior for a GP with each of the three kernels look like for Figure 1/2?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3285/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698739896583,
        "cdate": 1698739896583,
        "tmdate": 1699636277061,
        "mdate": 1699636277061,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9TtecJPH8t",
        "forum": "WdhtdjoaVw",
        "replyto": "WdhtdjoaVw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3285/Reviewer_b5oJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3285/Reviewer_b5oJ"
        ],
        "content": {
            "summary": {
                "value": "The paper builds on the literature on functional variational inference for Bayesian deep learning and proposes a new Bayesian inference framework in the function space. To achieve this, the authors build a bridge process and propose to use 1-Wasserstein distance between random processes to minimize the distance between the bridge process and an apriori random process that provides a suitable prior over the functions (e.g. Gaussian Process). Then in the second step, the variational posterior over the network weights is optimized to match the bridging distribution in the parameter space where the optimization is done by minimizing the 2-Wasserstein distance between the parametric variational family and the bridging distribution. In order to avoid constraining the solution space of the bridging distribution, the authors propose to perform the first and second optimization simultaneously, and provide efficient sample-based algorithms to minimize the weighted sum of the 1-Wasserstein functional distance and 2-Wasserstein parameter distance. Experiments are done on toy datasets, contextual bandits, the UCI dataset, and image classification. In all cases, the proposed method (FWBI) outperforms compared methods in terms of accuracy. The OOD detection results are presented on the image classification dataset where FWBI shows better performance than the compared methods on two out of three datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Despite their promise and excellent motivation, BDL methods have rarely been used in applied settings. This is due to a multitude of reasons. First, performing exact inference in the parameter space is intractable, leading to a handful of approximate inference techniques developed to remedy this. Second, incorporating prior knowledge about the problem space or task space in the parameter space is challenging due to a lack of clear understanding of how parameter priors translate into functional priors. This has motivated the development of building priors directly in the function space but most of the existing methods are not general enough for a wide range of applications or suffer from intractability of inference in high dimensions. This paper proposes an alternative framework based on Wasserstein bridges that remedies the existing issues and appears to possess strong prediction and calibration properties.\n\nThe paper is self-contained and the presentation of arguments follows a logical order.\n\nIncluding both quantitative and qualitative results on multiple datasets is an important strength of the paper."
            },
            "weaknesses": {
                "value": "Some of the strong existing methods (see the questions section) are not included in the comparisons. I believe even if the existing methods outperform FWBI in some datasets this paper is still a valuable contribution. So including those results will just add to the strength of the paper.\n\nAlthough there's a contribution section with a list of proposed contributions, I still had a hard time disentangling what existed in the literature and what was new in the paper (see the questions section).\n\nAlthough BDL methods originally were developed for calibration purposes, I don't see many calibration results in the paper. It's a common practice in BDL papers to present results on IND and OOD calibration on real and toy datasets. Again, this will add to the strength of the paper and help practitioners assess the circumstances under which FWBI achieves its best performance.\n\nResults are not systematically shown as a function of varying the dimension, noise, and smoothness of the prior, among other factors."
            },
            "questions": {
                "value": "Parts of the presented methods are very similar to [1]. Although the authors cite the paper in the paragraph where their method is used the similarities and differences between the two methods are not clearly stated. In addition, despite sharing a lot of similarities that paper is not included in the compared methods among the functional prior BNNs. Please include a more detailed discussion of what exactly are the differences and how the methods compare in terms of their performance.\n\nAlthough the main motivation of the presented methods is uncertainty estimation and calibration most of the focus of the paper is on the in-distribution accuracy measurements as opposed to calibration properties. Common ways of measuring calibration are to use an in-distribution (IND) test dataset (such as MNIST) and an out-of-distribution (OOD) test dataset (such as Not-MNIST) and evaluate metrics such as ECE, MCE, Entropy of the predictions, calibrated, etc. Can the authors include these results to show that the model class proposed here achieves better calibration?\n\nIn Fig. 2, why does the model provide such narrow uncertainty bounds for regions without data? This is a behavior that\u2019s not expected from a well-calibrated model such as GP with RBF kernel.\n\nI\u2019m not quite sure how to interpret the results presented in Figs. 1, 2. Do the error bars represent standard deviation? If so, to me a method that does a reasonable job of calibration should have error bars that cover the true function. This seems to be the case only for the KLBBB method. How do the authors justify these results?\n\nCan the authors include GP as the gold standard for the comparisons in FIg. 1, 2? Also, can you include an MCMC method such as HMC performed on BNN with normal weights (use a small BNN so that HMC can run in a reasonable amount of time, or use SG-MCMC for intermediate BNNs) in compared methods?\n\nA recent work [2] proposes to use Lipschitz functions in the architecture of a network to achieve what they call \u201cdistance awareness\u201d which will lead to proper uncertainty estimation. While the use of Lipschitz functions in that work is motivated from a completely different perspective it seems to be a shared component of your work (and the original work by [1]).\n\n[1] https://arxiv.org/abs/2011.12829 \n\n[2] https://arxiv.org/abs/2205.00403"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3285/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698848964232,
        "cdate": 1698848964232,
        "tmdate": 1699636276968,
        "mdate": 1699636276968,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8UXE0YTI4e",
        "forum": "WdhtdjoaVw",
        "replyto": "WdhtdjoaVw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3285/Reviewer_RTAX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3285/Reviewer_RTAX"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on function-space inference for Bayesian deep learning models and proposes a novel variational method termed functional Wasserstein bridge inference (FWBI). Typical function-space VI approaches define the similarity between distributions over functions in terms of the Kullback-Leibler divergence, but this measure is known to be ill-defined for such distributions. As an alternative similarity measure, the authors propose to use the Wasserstein distance between the variational posterior and the (parametric) prior, which itself is fitted to a functional prior (e.g., a Gaussian process) by minimising another Wasserstein distance. The resulting objective contains the likelihood and the two Wasserstein distances, but the authors show (in the appendix) that this is a proper lower bound on the model evidence. Finally, they compare FWBI against both weight-space and function-space inference approaches, showing good predictive performance and uncertainty quantification."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper presents an interesting idea that addresses an important issue. It is clearly interesting to the ICLR community and is likely to become a significant reference in the field of function-space inference.\n2. The work appears to be original, although it seems to be a combination of the methods presented by Tran et al. (2020) and Wild et al. (2022). The authors are entirely open about this and cite both papers.\n3. The paper is clearly written and seems to be of high technical quality.\n4. Empirically, the method seems to work well and has a very competitive running time.\n\n\nReferences:  \nTran et al., Functional Priors for Bayesian Neural Networks through Wasserstein Distance Minimization to Gaussian Processes, AABI 2020.  \nWild et al., Generalized Variational Inference in Function Spaces: Gaussian Measures meet Bayesian Deep Learning, NeurIPS 2022."
            },
            "weaknesses": {
                "value": "1. I am mainly missing some empirical analysis of the proposed objective compared to competitors. The running time is faster, but how does, say, the stability of the training or the convergence speed compare to other methods?\n2. The proposed objective function contains two hyperparameters weighing the contributions from each of the Wasserstein distances, which makes sense practically, although it is a little unsatisfying theoretically. The authors do not discuss how to choose or tune these hyperparameters.\n3. The experiments use the functional BNNs of Sun et al. (2019) as the main competitor, but more recent and stronger baselines exist, for instance, Ma & Hern\u00e1ndez-Lobato (2021) and Rudner et al. (2022), which are both cited by the authors. It would have been fair to at least include one such baseline in the experimental evaluation.\n4. The authors highlight uncertainty quantification as one of the method's strengths, but no uncertainty quantification metrics (say, predictive log-likelihood) are provided for the UCI experiments.\n5. It is a shame that quite many important results, such as the proof that the objective is an ELBO and some of the empirical analysis, have been put in the appendix. There is, of course, a limit to how much one can fit on 9 pages, but for instance the introduction and preliminaries could perhaps have been shortened a bit to at least make room for a proof sketch. Given that this proof is highlighted as a contribution, at least something should be said about it in the main paper.\n\nReferences:  \nMa & Hern\u00e1ndez-Lobato, Functional Variational Inference based on Stochastic Process Generators, NeurIPS 2021.  \nRudner et al. Tractable Function-Space Variational Inference in Bayesian Neural Networks, NeurIPS 2022."
            },
            "questions": {
                "value": "1. In section 3.2, you call $\\boldsymbol{\\theta_q}$ and $\\boldsymbol{\\theta_q}$ \"stochastic parameters\". It also seems like they are sampled in Eq. (12), but in Eq. (11), they are minimised, which confuses me. Can you elaborate on this?\n2. Did you compute uncertainty quantification metrics, such as the predictive log-likelihood, for the UCI experiments? Based on figures 1 and 2, it seems to me that FWBI produces quite over-confident predictions.\n3. Do you have advice on how to select or tune the hyperparameters in the objective?\n4. What does the running time in E.2 measure? Is it training time or prediction time? If training time, is it per epoch or until convergence?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3285/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699307214633,
        "cdate": 1699307214633,
        "tmdate": 1699636276881,
        "mdate": 1699636276881,
        "license": "CC BY 4.0",
        "version": 2
    }
]