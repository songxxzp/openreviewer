[
    {
        "id": "dmaX0Ts63j",
        "forum": "TgeVptDYAt",
        "replyto": "TgeVptDYAt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1441/Reviewer_FDY4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1441/Reviewer_FDY4"
        ],
        "content": {
            "summary": {
                "value": "Inspired by the success of foundation models on text and image, this paper proposes a foundation model for causal inference. The model is able to perform zero-shot causal inference."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Causal effect estimation is an important task and proposing a foundation model to conduct this task is interesting.\n2. Estimating the causal effect in a zero-shot manner is interesting,"
            },
            "weaknesses": {
                "value": "1. The writing of the paper needs to be improved. The abstract and introduction is very misleading. It is only after reading the main framework section that the reader will realize the proposed method is essentially a spacial kind of large foundation model which is designed to perform causal inference and the main contribution of the paper is to not make foundation models causal, i.e., make them rely on the causal relations rather than the spurious correlations.\n\n2. The paper needs to provide an intuition of why this model works. What kind of information is learned by this model that enables zero-shot causal inference. Though it is relatively intuitive when it comes to text and image based foundation models, it needs further intuition and justification for causal effect estimation.\n\n3. As a followup to the previous point, the  experiments are not comprehensive. ALso more information on the model implementation and experimental set is needed."
            },
            "questions": {
                "value": "Please refer to weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1441/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1441/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1441/Reviewer_FDY4"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1441/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698779299494,
        "cdate": 1698779299494,
        "tmdate": 1699636072685,
        "mdate": 1699636072685,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3dJiggTX3c",
        "forum": "TgeVptDYAt",
        "replyto": "TgeVptDYAt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1441/Reviewer_YMpy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1441/Reviewer_YMpy"
        ],
        "content": {
            "summary": {
                "value": "This paper delves into the intricate relationship between causal inference and attention mechanisms in transformer-like models, understanding these models through a causal lens. To address this, they introduce a novel method called Causal Inference with Attention (CInA), which utilizes self-supervised causal learning, enabling zero-shot causal inference on unseen tasks. The method's foundation lies in the primal-dual connection between optimal covariate balancing and self-attention, enabling inference through a transformer-type architecture's final layer. Empirical experiments validate CInA's effectiveness, showing its capability to generalize across out-of-distribution and real-world datasets, often matching or outperforming traditional causal inference methodologies."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper is well-written and well-organized. \n\n2. The introduction of the Causal Inference with Attention (CInA) method is interesting and theoretically sound."
            },
            "weaknesses": {
                "value": "I am not very familiar with this topic and I may change my score after further discussions with authors and other reviewers. \n\n1. Did the paper exaggerate its findings regarding \"Casual foundation models\"? From what I gathered, the study introduced a method to understand the attention mechanism in transformer-type networks through a causal lens. However, they only tested their approach on relatively small datasets. This makes it challenging to view this work as a blueprint for developing foundation models, and its efficacy for such models remains unproven. While the related work section delves into causal reasoning for Large Language Models (LLMs), the connection between the proposed method and LLMs isn't clearly articulated.\n\n2. This paper purports to be an initial step towards creating causal foundation models. But how does it compare to paper [1]? And could you elucidate the link between the causal transformer and the methods they proposed?\n\n[1] Causal Transformer for Estimating Counterfactual Outcomes.\n\n3.  The methods you've presented can be challenging to understand, particularly for those not well-versed in the subject. It would be helpful to include an introductory section that provides background information. After that, you can highlight your contributions."
            },
            "questions": {
                "value": "Please refer to the section above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1441/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1441/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1441/Reviewer_YMpy"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1441/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698780094103,
        "cdate": 1698780094103,
        "tmdate": 1699636072598,
        "mdate": 1699636072598,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CesKmBpGDX",
        "forum": "TgeVptDYAt",
        "replyto": "TgeVptDYAt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1441/Reviewer_BTeh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1441/Reviewer_BTeh"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors present a step towards creating a self-supervised learning paradigm for causal foundation models, which they call Causal Inference with Attention (CInA), a transformer-based model that uses covariate balancing as self-supervised tasks and is trained on multiple unlabeled observational datasets for treatment effect estimation, backed by the primal-dual connection between covariate balancing and regularized self-attention. Their study is backed by both synthetic and real-world datasets to show good zero-shot generalisation performance with moderate causal mechanism shifts considering the metric of mean average error (MAE) between the true Average Treatment Effect (ATE) and the predicted ATE in both the single causal graph setting and when the data comes from multiple different causal graphs."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- Causally aware foundation models that are realisable for complex tasks such as inferring ATE on complex datasets like ER-5000, Twins dataset, and IHDP is an important step in deep learning, in general.\n- The contribution of this paper showing zero-shot causal inference on unseen data is a big strength, and its MAE is shown to be lower than even per-dataset causal inference approaches.\n- The authors prove the duality between covariate balancing and self-attention, showing that under mild regularity conditions, the optimal parameters of the self-attention layers provide optimal covariate balancing weighs and ATE."
            },
            "weaknesses": {
                "value": "- Just a minor suggestion since this is also a part of the paper already-- it could potentially be useful to state the causal reasoning task explicitly, such as estimating ATE with greater generalizability.\n- The source code is not available, but the authors say they will release it upon publication."
            },
            "questions": {
                "value": "- I couldn't understand the connection between different causal graphs and covariate balancing-- the authors say they shuffle units from different datasets using training-- how is the differentiation between how different the causal structures are, performed? And why does this not help with increasing the number of covariate balancing problems without acquiring more data?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics concerns."
            },
            "rating": {
                "value": "10: strong accept, should be highlighted at the conference"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1441/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698902493430,
        "cdate": 1698902493430,
        "tmdate": 1699636072535,
        "mdate": 1699636072535,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AMRcts5snt",
        "forum": "TgeVptDYAt",
        "replyto": "TgeVptDYAt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1441/Reviewer_deJS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1441/Reviewer_deJS"
        ],
        "content": {
            "summary": {
                "value": "The paper claims three contributions aimed towards the problem of zero-shot causal inference with transformer models, as a step towards \"causally-aware foundation models\":\n1. A theoretical equivalence between optimal covariate balancing under the assumption of back-door admissibility of covariates (ignorability) and self-attention with a particular loss function.\n2. A algorithm implementing covariate balancing using transformers.\n3. Experiments illustrating improvements in model performance and runtime."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Originality: The idea of linking self-attention, SVM-like optimization, and an estimator for the classic back-door adjustment (a.k.a. optimal covariate balancing) is a novel one to my knowledge.\n- Significance: The work introduces a method that can perform competitively with prior sample-reweighting methods such as DML in small (n<=1024) datasets. It also introduces a \"zero-shot\" method which trains on multiple datasets and is evaluated on a single new dataset; this method significantly outperforms prior methods in performance and speed due to the fact that re-training is not required, when trained and evaluated on datasets similar to the new dataset (e.g., when trained on ER-5000 datasets, it outperforms DML on ER-5000, and when trained on ACIC datasets, it outperforms DML on other ACIC datasets).\n- Clarity: The notation and claimed contributions in the paper are generally clear, and the paper is generally well-written, aside for key points mentioned in the weaknesses section."
            },
            "weaknesses": {
                "value": "Overall, I believe the framing of the work as a step towards causal foundation models is misleading, as the method introduced is limited to estimating the ATE under the classic ignorability assumption. The theoretical novelty of the work does not seem strong, as Theorem 1 is a direct implication of results from prior work cited in the paper (Tarr & Imai, 2021; Kallus, 2020; Nguyen et al., 2022). Finally, the strength of the empirical results is tempered by the facts that there is no comparison in performance to various neural ATE estimation methods, and that it is not possible to rule out overfitting as an explanation for outperformance of zero-shot (supervised) learning (see point 7). \n\n1. The title \"Towards Causal Foundation Model\" seems to be misleading, along with the claim \"we take a first step towards building causal foundation models, focusing on causal inference tasks\". The paper does use self-attention, a key element of most 'foundation models' in the language or vision sense. However, the proposed method is limited to estimation of the ATE under the assumptions of ignorability and sample non-interference; indeed, it relies on the structure of the problem to produce a weighted average of observed outcomes. According to (Bommasani et al., 2021), \"A foundation model is any model that is trained on broad data (generally using self-supervision at scale) that can be adapted (e.g., fine-tuned) to a wide range of downstream tasks; current examples include BERT [Devlin et al. 2019], GPT-3 [Brown et al. 2020], and CLIP [Radford et al. 2021].\" Clearly, this does not hold for a method that can only estimate the ATE under a specific set of assumptions, independent of any efficiency or performance improvements.\n2. The assumption of sample non-interference, along with the fact that the self-attention is applied on the units of a dataset, seems to remove any connection with the \"Causal Inference with Large Language Models\" section, as one main reason attention is useful in language models is the fact that it handles sequences well. In theory, arbitrarily re-ordering a dataset should not affect the output of Algorithm 1, due to the non-interference assumption. Re-ordering the words in a document inputted to an LLM *should*, of course, affect the output of an LLM. Therefore, it is difficult to see any connection between self-attention as used in this work and as used in LLMs - or any connection between this work and LLMs, beyond the fact that it is known that LLMs have difficulty solving the problem of ATE estimation from data and assumptions.\n3. Section 3.1 appears not to contain novel theoretical results; the results showing optimal covariate balancing is equivalent to the dual SVM problem appear to be repeated from (Tarr & Imai, 2021; Kallus, 2020). Section 3.2 appears to rely on (Nguyen et al., 2022) for its result showing the dual SVM problem may be reformulated as self-attention. Therefore, the novelty of Theorem 1 does not seem to be particularly strong, as it is a direct implication of prior work's results.\n4. Algorithm 1 seems to be limited to estimating the ATE - that is, the method does *not* extend to counterfactual estimation, as described in Appendix D. The **sample** average treatment effect, or SATE, (not to be confused with the average treatment effect, or ATE) is not identifiable under the assumptions of consistency, non-interference, and ignorability ($Y_i(0), Y_i(1) \\perp T_i | \\mathbf X_i$). As a simple demonstrative example, let $n = 1$, and consider the single sample $(X = 1, Y = 1)$. It is possible for $Y_1(0)$ (and therefore the sample average treatment effect) to take the value of 0 or 1, and in the absence of other assumptions, we cannot infer its value. Adding more samples will not help us infer its value. This is also known as the fundamental problem of causal inference (Holland, 1986), and the causal hierarchy theorem (Bareinboim et al., 2022) shows that it is in general not possible to identify counterfactual queries from observational data alone, which is what is specified in the problem formulation.\n5. There is no comparison to methods such as (Shi et al., 2019), cited in the paper, which is a neural estimation method for the average treatment effect. There is also no comparison to (Xia et al., 2021; Xia et al., 2022; https://causalai.net/r80.pdf, https://causalai.net/r87.pdf), a neural identification and estimation technique for arbitrary interventional and counterfactual queries.\n6. It is unclear how the method will scale to datasets of sample size n > 1024, as evaluation time scales quadratically with n. For example, ACIC contains 100k samples, but the experiments subsample datasets of size <=1000, as described in Appendix E. The comparison with ACIC seems unfair, because other methods could learn from the 100k-sample dataset in at most linearly scaling time, while the proposed method's runtime would scale quadratically.\n6. It is unclear whether or not the method in the experiments is overfitting to the class of datasets used, since the training and test datasets are drawn from the same dataset generator or kind of real dataset. If the goal is truly to illustrate \"foundation model\"-like properties, experiments showing cross-dataset generalization would be required. Not enough details are given on the generation of the ER-5000 dataset for me to ascertain that overfitting on the dataset class is not occuring. For example, if the self-attention model is trained on ER-5000, it should be tested on datasets from simulation study A and real datasets, such as IHDP, Twins, or ACIC; if it is trained on real datasets of one variety (e.g., ACIC), it should be tested on datasets of another (e.g., IHDP). Other methods do not have the benefit of training on multiple datasets, so this would be a fair comparison betwen the zero-shot (supervised) learning method and other methods.\n\nReferences:\n1. Bareinboim, Elias, et al. \"On Pearl\u2019s hierarchy and the foundations of causal inference.\" Probabilistic and causal inference: the works of judea pearl. 2022. 507-556.\n2. Holland, Paul W. \"Statistics and causal inference.\" Journal of the American statistical Association 81.396 (1986): 945-960."
            },
            "questions": {
                "value": "1. How can the current work be viewed as a \"causal foundation model\", or related, given that it is targeted towards solving the particular task of ATE estimation under a particular set of assumptions?\n2. Given that the self-attention used in the paper lacks the classic sequence-learning benefits of self-attention in LLMs due to the assumption of sample non-interference, what is the connection between this work and LLMs?\n3. Are there novel theoretical results in this work beyond direct implications from (Tarr & Imai, 2021; Kallus, 2020; Nguyen et al., 2022)?\n4. Are there any assumptions that I missed in the paper which would allow one to identify or infer any bound on the sample ATE when $n = 1$ and we have the single sample $(X = 1, Y = 1)$?\n5. I am curious to know if training on multiple datasets to make inferences on a new one may have induced overfitting on the class of datasets used. For example, what is the performance of the ZS method when trained on ER-5000 (supervised or unsupervised) and evaluated on ACIC?\n6. I understand that the method proposed in the paper likely cannot scale well to dataset of ~100k samples, such as ACIC 2018. However, it would be informative to apply DML to each full dataset of 100k and compare this performance to the performance of CInA when given an allowance of the same runtime.\n7. Shouldn't the estimator in Section 3.1 minimize the expected absolute (or squared) conditional bias, rather than the absolute value of the expected conditional bias? That is, shouldn't there be an absolute value inside the expectation, or shouldn't the difference be squared? If not, then one can have mean-zero error with arbitrarily high variance, which does not seem to describe a good estimator."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1441/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1441/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1441/Reviewer_deJS"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1441/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699239989025,
        "cdate": 1699239989025,
        "tmdate": 1699647949439,
        "mdate": 1699647949439,
        "license": "CC BY 4.0",
        "version": 2
    }
]