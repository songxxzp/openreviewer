[
    {
        "id": "UzX03N0Wg7",
        "forum": "B5kAfAC7hO",
        "replyto": "B5kAfAC7hO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6845/Reviewer_rUpE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6845/Reviewer_rUpE"
        ],
        "content": {
            "summary": {
                "value": "This work aims to leverage the L-step decodable POMDPs to tackle the linear structure of POMDPs. Specifically, by conditioning on the recent L-step history (x_h), Q value does not need to rely on belief states or full history. Further, they show that Q value can be expressed in a linear form wrt P(z_h | x_h,a_h) where z_h is a latent variable, learned by some ELBO. The algorithm uses linear structure and learns the representation and Q value together with some sampling method. On both continuous and visual benchmarks, the proposed approach outperforms baselines in the terms of sample efficiency."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This work is original in POMDP literature with some theoretical guarantees (but I don\u2019t have the expertise to check) and good quality. The technical writing is mostly clear, but some clarification is still needed. \n\nThe empirical results are persuasive that the proposed approach outperforms the other baselines in most domains in the chosen continuous and visual control benchmarks."
            },
            "weaknesses": {
                "value": "This work has a main issue in its story writing:\n\n1. The title and abstract is quite vague and overly broad \u2013 basically it just said it is about a theoretical framework on RL or planning (I am confused which one) in POMDPs. \n\n2. Moreover, the introduction on the theoretical framework is rather limited, unclear, unstructured, and seems overly strong. The 4 bullet points are most useful, but structured. \n\n3. The claim \u201capplied to a real-world problem\u201d is especially strong, as the work obviously requires some assumptions on POMDPs, and no real world (like real robots) evaluation is performed. \n\n4. The claim \u201cstate-of-the-art\u201d is also too strong, as obviously Dreamerv2 and DrQ-v2, published in 2021, are no longer SOTA. \n\n5. The claim also touches \"offline POMDPs\" but I did not see any results.\n\nI don\u2019t think this work has much technical significance since it is heavily relied on recent work (Ren et al, 2023a). Also, it would be better to point out how important linear structure is in solving POMDPs more explicitly."
            },
            "questions": {
                "value": "1. Is L-step decodability same as (or subsumed by) L-order MDPs?\n2. Lack some definition on the latent variables z. From Eq 17, the objective of representation learning is exactly the same as belief-based approach. Let k=1, it is the standard ELBO of observation reconstruction/prediction, plus a KL divergence regularization between posterior and prior. In this sense, the optimal z seems to be belief state b. Is this correct? For k > 1, z might be different from b as it involves policy. \n3. The paper talks about \u201clow-rank POMDPs\u201d, but no definition is provided. How is it connected to L-step decodability? \n4. How partially observable is in the visual control benchmarks? As they were also tackled by Markovian methods."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Unprofessional behaviors (e.g., unprofessional exchange between authors and reviewers)"
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6845/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6845/Reviewer_rUpE",
                    "ICLR.cc/2024/Conference/Submission6845/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6845/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698284741194,
        "cdate": 1698284741194,
        "tmdate": 1700696782259,
        "mdate": 1700696782259,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MP7uaZ7CPl",
        "forum": "B5kAfAC7hO",
        "replyto": "B5kAfAC7hO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6845/Reviewer_wQNF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6845/Reviewer_wQNF"
        ],
        "content": {
            "summary": {
                "value": "This paper contributes a new algorithm for RL in structured POMDPs.\nIt proposes to use a latent variable model to learn a linear representation of the value function in L-step decodable POMDPs.\nThe proposed approach shows good performance in a large set of tasks when compared with baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The work is highly relevant for the RL community, as it explicitly tackles problems with partial observability, a fundamental challenge for applying RL in real-world tasks.\n\n- The method proposed is relatively novel, as it combines efficient linear representations with L-step decodable POMDPs.\n\n- The empirical evaluation considers many tasks and shows the proposed method has strong performance compared with multiple baselines."
            },
            "weaknesses": {
                "value": "- The presentation could be improved. Some technical details are inconsistent or lack an appropriate definition (see detailed comments below). This makes important parts of the paper challenging to comprehend, such as the discussion of Eq 9.\n\n- The paper is also very dense, which makes some parts too condensed. For instance, the theoretical analysis only states the assumptions and an informal version of the sample complexity of the algorithm without including an analysis of this result.\n\n- The empirical evaluation is limited to a comparison with other algorithms. It would be interesting to provide an ablation study to show how the different components of the algorithm contribute to its performance. For example, how the algorithm performs without optimistic exploration.\nFurthermore, it would be interesting to make a hyper-parameter sensitivity analysis, for example, evaluating how the algorithm performs with different values of L.\n\n\n[Detailed comments]\n- wrong typesetting of the observation function in the first paragraph of the preliminaries\n- in the preliminaries, should the agent receive a reward r(s_h, a_h)?\n- In the belief definition, it is unclear what is P(s1\\mid o1). It is also unclear what is \\tau.\n- Wrong index in the actions of Eq 2\n- Unclear what is \\theta on Eq 5?\n- \\mu is used for initial state distribution and as a feature map\n- are Eq 6 and Eq 9 missing some <> delimiters?\n- after Eq 7: an practical -> a practical\n- Eq 8: R(s,a) -> r(s,a)\n- Eq 2 and 3 are defined for problems with a finite horizon, then Eq 6 and 8 use discount factor $\\gamma$.\n- Eq 12 uses the latent variable z without a proper introduction"
            },
            "questions": {
                "value": "1. After Eq 16, could you provide some intuition about what is the parameter l?\n\n2. Could you provide a formal definition of the policy \\mu_pi?\n\n3. The experimental evaluation mentions that the algorithms were tested after running 200K environment steps. Could you comment on this choice of training time? In particular, is this training budget sufficient for the convergence of all algorithms?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6845/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6845/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6845/Reviewer_wQNF"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6845/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698749582209,
        "cdate": 1698749582209,
        "tmdate": 1699636792967,
        "mdate": 1699636792967,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rHBY5jU0Ck",
        "forum": "B5kAfAC7hO",
        "replyto": "B5kAfAC7hO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6845/Reviewer_cvkF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6845/Reviewer_cvkF"
        ],
        "content": {
            "summary": {
                "value": "While Partially Observable Markov Decision Processes (POMDPs) were introduced to address partial information in RL algorithms where full\nobservability is unavailable, such formulation brings computational challenges in learning, exploration, and planning, due to the non-Markovian dependence between observations. This paper aims to address the computational and statistical challenges in Partially Observable Markov Decision Processes (POMDPs). In particular, the authors develop a representation-based perspective that leads to a coherent framework and tractable algorithm."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper studies the problem of designing an efficient and practical RL algorithm for structured partial observations in RL frameworks. Authors introduce a structured POMDP with a low-rank property that allows for a linear representation, called Multi-step Latent Variable Representation (\u00b5LV-Rep), which is a counterpart of linear MDP in the POMDP context. As such, this representation overcomes computational barriers and enables a tractable representation of the value function. The extension of linear MDP to POMDP can be beneficial.\n\nThe paper also proposes a planning algorithm that can implement both the principles of optimism and pessimism in the face of uncertainty for online and offline POMDPs. \n\nTheoretical analysis in sample complexity and PAC guarantee are provided to justify the performance guarantee. \n\nEmpirical comparisons are performed to demonstrate the performance on a set of benchmark environments compared to existing SOTA RL algorithms for POMDPs."
            },
            "weaknesses": {
                "value": "1. The theoretical analysis relies on quite a few assumptions (including (Finite Candidate Class with Realizability, Normalization Conditions, Regularity Conditions and Eigendecay Conditions), which may not always fulfilled in reality. Can authors comment on the performance of the algorithms when these assumptions break, e.g., how worse the performance is going to be, and which of the assumptions are essential to retain the performance? \n\n2. The theoretical analysis is mainly based on Ren et al., 2023a. It is unclear what are the technical novelties in the analysis compared to Ren et al., 2023a. Authors are expected to explain the difference and highlight the key insights in the proofs.  In particular, the proof of Theorem 12 is unclear by just claiming \"This is a direct extension of the proof of Theorem 9 in Ren et al. (2023a)\". The technical contribution in theory remains questionable.\n\n3. Algorithmically, the proposed main algorithms borrow lots of the elements from Ren et al., 2023a, the novelty appears to be limited.\n\n4. In section 4, it is unclear how the planning algorithm implements pessimism for offline RL.\n\n5. Can authors comment on the tightness of the sample complexity bounds in Lemma 11?"
            },
            "questions": {
                "value": "See above. In addition, there are some minor grammatical errors in the draft. It is suggested that authors carefully proofread the draft for improvement."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6845/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698815240125,
        "cdate": 1698815240125,
        "tmdate": 1699636792814,
        "mdate": 1699636792814,
        "license": "CC BY 4.0",
        "version": 2
    }
]