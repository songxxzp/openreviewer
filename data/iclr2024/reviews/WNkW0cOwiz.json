[
    {
        "id": "dWS4dZDVrv",
        "forum": "WNkW0cOwiz",
        "replyto": "WNkW0cOwiz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1058/Reviewer_gGRw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1058/Reviewer_gGRw"
        ],
        "content": {
            "summary": {
                "value": "The paper elaborates upon an important observation concerning the presence of infinite-Lipschitz constants in the diffusion process, made earlier by (Song et al., 2021a; Vahdat et al., 2021). It also proposes a simple yet effective approach to address this challenge."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Theorem 3.1 is a nice piece of rigorous analysis of diffusion models, albeit indebted to Song et al.\n\nThe proposed approach to address this infinite-Lipschitz challenge, which is based on improving the resolution of the discretisation, does indeed seem to be effective. \n\nNumerical results in Figures 3 and 4 seem quite impressive."
            },
            "weaknesses": {
                "value": "The observation concerning the presence of infinite-Lipschitz constants in the diffusion process is not original (Song et al., 2021a; Vahdat et al., 2021). Concerning it has been observed before, the authors should like to tone down their claims of having observed it first. \n\nSome of the English is stilted (\"vexing propensity of diffusion models\" in the abstract, \"Recently, there have been massive variants that significantly promote the development of diffusion models\" on page 3)."
            },
            "questions": {
                "value": "How would you describe the differences in your observation and those of (Song et al., 2021a; Vahdat et al., 2021)? \n\nYou could make your observation more original by noting that the infinite Lipchitz constants mean the SDE need not have a unique strong solution (\u00d8ksendal, 2003). Exhibiting multiple solutions would indeed be of interest."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1058/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698005299056,
        "cdate": 1698005299056,
        "tmdate": 1699636032033,
        "mdate": 1699636032033,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uRmiVcf9KH",
        "forum": "WNkW0cOwiz",
        "replyto": "WNkW0cOwiz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1058/Reviewer_qCuR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1058/Reviewer_qCuR"
        ],
        "content": {
            "summary": {
                "value": "This paper deals with an exploding Lipschitz constant in the function a neural network is asked to learn in a DDPM model, and the negative effects of trying to learn a function with such.\n\nThe authors present an argument based on taking time derivative of the quantity $-\\sigma_t \\nabla \\log q_t(x)$, where the $\\sigma_t$ are the standard deviations of the forward noising process in the time-discrete forward noising process in the DDPM formulation and $q_t$ is the density of the data distribution diffused to discrete time step $t$.\n\nThey demonstrate that the Lipshitz constant in time explodes to infinity for a most parameter settings of common noising schedules under the DDPM/VPSDE setting.\n\nThe authors propose a method for fixing this issue by applying a transform to the time input of the score network, tying together multiple timestep near t=0 to have the same score.\n\nThey demonstrate significant empirical benefit over a range of diffusion modelling tasks.\n\nThe authors also discuss a number of other possible methods to alleviate the issue of learning high lipshitz constants in diffusion models, but show that these methods despite being theoretically attractive, do not perform as well in practise."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1) The method proposed is simple to implement.\n2) The method clearly demonstrates significant empirical benefit.\n3) The authors discuss alternative proposals and show these are less effective"
            },
            "weaknesses": {
                "value": "1) The only weakness I would like to highlight is the discussion of the alternative methods presented.\n - I believe 1 of the methods from the appendix is not mentioned in the main text - namely the Remp method (D.3.3).\n - It would be nice to see an expanded discussion of these with some small experiment to show the quantitative difference between the proposed method and these other methods. I appreciate the space limitation, but I think this is really an interesting point."
            },
            "questions": {
                "value": "1. Could the authors highlight better which lipshitz constant is is that is important, and why we care about it? While I understand I believe which and why it is cared about, it is perhaps not the clearest from reading the paper. The sentence in the abstract \"they frequently exhibit the infinite Lipschitz near the zero point of timesteps\" is a good example of this - it does not specify _what_ function has high lipshitz constant, or why indeed that matters. From reading the paper in depth, the authors care about the lisphitz constant of the quantity $-\\sigma_t \\nabla \\log q_t(x)$ as a) neural networks find it difficult to learn high lipshitz constant functions, and this is the function we are asking the score net to learn, and b) because this quantity is involved in the reverse rollouts, having a term with high lipshitz constant makes discretising the SDE challenging to do accurately, but this should be apparent from the abstract/introduction."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1058/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1058/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1058/Reviewer_qCuR"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1058/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698670571623,
        "cdate": 1698670571623,
        "tmdate": 1699636031960,
        "mdate": 1699636031960,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1KIyF4ZSWp",
        "forum": "WNkW0cOwiz",
        "replyto": "WNkW0cOwiz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1058/Reviewer_DUEU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1058/Reviewer_DUEU"
        ],
        "content": {
            "summary": {
                "value": "This paper demonstrates theoretically (and confirms empirically) that the limit of the Lipschitz constant of the noise prediction network for a timestep of zero is infinite. Such a result is a source of instability for using diffusion models in many generative tasks, and the authors propose a technical solution to alleviate this issue and confirm the superiority of the approach with extensive numerical simulations."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This is an excellent paper, and the presentation is very well carried out. The authors point out a very interesting theoretical property that could explain some practical instabilities encountered in DDPM samples. They then present a practical solution to the problem. The authors' contribution is excellent for the community, as reducing the instabilities in the generative process, such as diffusion models, has important practical consequences."
            },
            "weaknesses": {
                "value": "This paper as it is impeccable in terms of presentation and contribution, both theoretically and practically. The only drawback is that no open-source code is available to experiment with their approach."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1058/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698774759169,
        "cdate": 1698774759169,
        "tmdate": 1699636031873,
        "mdate": 1699636031873,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "syx7W5AtBo",
        "forum": "WNkW0cOwiz",
        "replyto": "WNkW0cOwiz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1058/Reviewer_gj5E"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1058/Reviewer_gj5E"
        ],
        "content": {
            "summary": {
                "value": "Diffusion models, utilizing stochastic differential equations to generate images, have become a leading type of generative model. However, their underlying diffusion process hasn't been thoroughly examined. This paper reveals a concerning tendency in diffusion models: they often display infinite Lipschitz (for $\\sigma_{t} \\cdot \\text{score function}$) near the initial timesteps. Through theoretical and empirical evidence, the presence of these infinite Lipschitz constants is confirmed, which can jeopardize the stability and precision of the models during training and inference. To combat this, the paper introduces a new method, E-TSDM, that uses quantization to reduce these Lipschitz issues. Tests on various datasets support the presented theory and approach, potentially offering a deeper understanding of diffusion processes and guiding future diffusion model design."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "This paper highlights a unique and previously unexplored challenge with DDPM: the instability encountered when learning $\\epsilon_{\\theta} = \\sigma_{t} \\cdot \\nabla \\log q_{t}(x)$ during the time steps where $\\sigma_{t}$ is minimal. One might naturally question why DDPM doesn't directly learn $\\nabla \\log q_{t}(x)$. I conjecture that the optimization process for learning $\\nabla \\log q_{t}(x)$, which involves solving $E\\|\\nabla \\log q_{t}(x) - \\frac{1}{\\sigma_{t}} \\|^2$, becomes problematic with a small $\\sigma_{t}$. As a workaround, DDPM employs a transformation to learn $\\sigma_{t}\\cdot \\nabla \\log q_{t}(x)$ directly. However, this paper reveals the inherent price of such an approach (no free lunch indeed).\n\nThe paper validates the infinite Lipschitz problem with $\\epsilon_{\\theta}$ both theoretically and empirically. Moreover, it introduces E-TSDM, an innovative solution that essentially employs a quantization strategy when $\\sigma_{t}$ is minimal, particularly during the initial t=100 steps. Comprehensive experiments demonstrate E-TSDM's enhanced stability and performance, even setting a new benchmark for FFHQ 256\u00d7256.\n\nThe paper's novelty is commendable, presenting a compelling and succinct argument with an impressive practical performance. Its insights could significantly influence the diffusion model community. I'm inclined to strongly endorse its acceptance."
            },
            "weaknesses": {
                "value": "- One minor suggestion is to avoid saying $t$ being small (rather, it is about $\\sigma_{t}$ being small). Since $t$ is in fact $0, 1, 2, 3, .. 100.$ \n- May add more discussions to the alternative approaches (see Questions below). \n- It may be worth showing that directly learning $\\nabla \\log q_{x}(t)$ with the least square is prohibitve."
            },
            "questions": {
                "value": "I am looking for comments from the authors on a few alternative methods:\n1. Learning $\\nabla \\log q_{x}(t)$ directly with weighted least square: can we reduce the weight of the least square when $\\sigma_t$ is small, e.g., learn $E \\sigma_{t}^2\\|\\nabla \\log q_{x}(t) - \\frac{1}{\\sigma_{t}}I\\|^2$?\n2. For Eq (9), what if we only learn $\\epsilon_{\\theta}(\\alpha_{f_T(t)}x_0 + \\sigma_{f_T(t)}\\epsilon, f_{T}(t))$, i.e., only learn the score function for time $f_{T}(t)$ and only use those time steps to do sampling? \n3. Is that $\\sigma_{t}$ an input of the neural network: what if we learn $\\epsilon_{\\theta}(x, \\sigma_{t})$ (the intuition is that $\\sigma_{t}$ will help adjust the network Lipschize automatically). \n\nMinor comments:\n- Eq 10, are $\\beta_{t}$ and $\\eta_{t}$ defined?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1058/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1058/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1058/Reviewer_gj5E"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1058/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698820591544,
        "cdate": 1698820591544,
        "tmdate": 1699636031787,
        "mdate": 1699636031787,
        "license": "CC BY 4.0",
        "version": 2
    }
]