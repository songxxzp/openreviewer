[
    {
        "id": "GAIHjtCCc0",
        "forum": "AfiM6F2YPY",
        "replyto": "AfiM6F2YPY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2245/Reviewer_VtUd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2245/Reviewer_VtUd"
        ],
        "content": {
            "summary": {
                "value": "The paper considers using machine learning approaches to study a problem in algebraic topology, namely the problem of sampling elements from the numerator of Wu's formula. It does so by proposing several variants of natural language models to generate possible words which may be elements of this numerator set, after generating a synthetic dataset based on sampling from Dyck paths."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper has an original premise in using tools from machine learning to study a specific problem in algebraic topology which requires generative modeling. This may be of  interest to NLP researchers who wish to understand the breadth of applications which are suitable for their architectures. Table 2 also clearly shows that the proposed methods outperform the baselines."
            },
            "weaknesses": {
                "value": "My main concern with this paper is its relevance within the machine learning field. \n\nFrom a presentation perspective, the work emphasizes the mathematical contributions by framing the problem with Wu's formula, rather than focusing on the machine learning methodology. Because the setup of Wu's formula is quite complicated for anyone unfamiliar with algebraic topology, the paper may be difficult to parse for all but a few attendees of the conference. As written, it's hard to understand some of the notation (e.g., $[x_i, x_j]$ is defined for elements of $F$, but $[R_i, R_j]$ is not for the subgroups, which is a core component of the formulas). Further, because the paper spends so much time on these preliminaries, it is difficult to understand which machine learning methods are used and why. For example, the training procedure is difficult to understand. \n\nFrom a more substantive perspective, the paper doesn't provide convincing evidence that this use of machine learning can provide meaningful progress for the problem in the domain of mathematics. For example, scalability of the method is highlighted, but the experiments only go up to $n = 5$ due to computational constraints (p. 9). It's not immediately clear from the writing that this contribution can be built upon for further research.\n\nUltimately, the paper may be of more interest to a pure mathematics community.\n\nSome secondary concerns:\n\n1) The use of the completion/reduction ratio metrics could be better justified: can this distinguish whether the model is suggesting a wide variety of elements from that set or simply repeating the same element multiple times?\n\n2) The notation throughout the work is often difficult to understand: for example, the $\\pm$ notation in (1) is unclear, and it's not immediately clear how the $y_{k,i}$ terms in (5) are related to the notation $y_i$ often used to describe elements of the set $F$"
            },
            "questions": {
                "value": "1) Related to secondary concern 1 above: how do we know that the model is generalizing well, i.e. finding words which are not from the training set?\n\n2) Are there simpler generative models which could work well for this task?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2245/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2245/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2245/Reviewer_VtUd"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2245/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698583288718,
        "cdate": 1698583288718,
        "tmdate": 1699636157854,
        "mdate": 1699636157854,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5iNUTV030Z",
        "forum": "AfiM6F2YPY",
        "replyto": "AfiM6F2YPY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2245/Reviewer_LHhA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2245/Reviewer_LHhA"
        ],
        "content": {
            "summary": {
                "value": "In this work the authors propose the use of language modeling techniques and architectures as a means for generating elements of the homotopy group of the sphere. First, the authors cite Wu's formula, which provides a connection between the homotopy groups of a sphere and a quotient group of particular combinations of free groups. Specifically, the $n+1$ homotopy group is given by a quotient group, the numerator of which is an intersection of subsets $R_i$, called *normal closures*, whose elements are strings comprised of characters $\\\\{x_i^{\\pm 1}\\\\}_{i=1}^n$.\n\nThe authors claim that generating words in the intersection $w\\in \\cap_{i=0}^n R_i$ is not straightforward, however any partial intersection (i.e. omitting at least one $i \\in \\\\{0,\\ldots, n\\\\}$) has an explicit description and can be sampled from. The authors propose a particular sampling method, and propose to use it to generate words with multi-labels as to which $R_i$ they belong. They then train a decoder-only transformer on these sampled words, leveraging the multi-labels via attention masking or prompting (eg. if $x_1 x_2^{-1} \\ldots x_t \\in R_0 \\cap R_1$, they train the model on $R_0 R_1: x_1 x_2^{-1} \\ldots x_t$). They then exploit these mechanisms at inference time to generate elements of the full intersection.\n\nThe authors propose a number of symbolic baselines (random search, an evolutionary method, and a greedy algorithm) which attempt to generate elements in the full intersection. They compare these methods to their approaches using language modeling and measure the *completion ratio*, which is essentially the percentage of generated words which are actually in the full intersection. In all but one case they find the language model is far better at generating words in the full intersection than any of the synthetic approaches."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper's greatest strength is in it's originality. To be fair, this is not a line of work I am intimately familiar with, however a literature review on the topic suggests to me that this work is novel in both application and method.\n\nThe authors are also operating in a highly technical application area (homotopy theory), and to their credit I feel they do an admirable job conveying the necessary information without getting bogged down in details, and include additional background material in the appendix."
            },
            "weaknesses": {
                "value": "The largest weakness, in my opinion, is that the significance of this work is unclear. The authors motivate the work by a connection with homotopy theory, however their approach does not generate elements of the homotopy group itself but rather just the numerator (i.e. full intersection of normal closures). Moreover, there is no notion of the *coverage* of this generative procedure. More specifically, based on the coverage metric, a constant function which simply always returned a fixed element $w \\in \\cap_{i=0}^n R_i$ would score perfectly, but this would not be useful in any way.\n\nThe lack of any notion of \"coverage\" means that, even if their approach *did* generate elements of the homotopy group with some high probability, it is unclear to me exactly how those words could be used to gain any insight into the structure of the homotopy group itself, because we would not have any assurance that it was covering the full set of potential words in the homotopy group.\n\nThe paper also needs significant editing before publication. There are many mistakes and typos, some of which lead to unparsable text."
            },
            "questions": {
                "value": "1. Can you explain more what you meant by \"thereby enforcing it not to 'learn' the pattern of the trivial words in the denominator\" at the end of Section 3? I don't understand this statement, because (as I understand it) you were not doing anything in particular to encourage it not to learn the trivial words, and indeed the results in Figure 4 suggests that the models very much did predominantly learn trivial words.\n2. In your discussion on random search (in section 4.2) it was mentioned that the difference between naive sampling and bracket-style sampling from $R_i$ was evident in the number of generated words from intersection per batch. Could you please expand on this - what does the number of generated words from intersection per batch of random search imply about naive vs. bracket-style sampling?\n3. Could you provide a simple explanation of why the bracket-style sampling is preferred to naive? (a) It is stated in section 4.1 that \"adjacent letters of different conjugators $y_k, y_{k+1}$ do not interact with each other, resulting in reduced variability within the generated dataset\" but this notion of \"variability\" has not been defined. Also, isn't this straightforward to solve by introducing dependencies between $y_k$ and $y_{k+1}$? (b) I understand the statistics calculated in Figure 1, but I don't understand why the distribution for the bracket-style sampling is \"better\" than naive. Also, if one wanted a more uniform distribution of valleys, couldn't we just always include the inverse of every word?\n4. Could you provide quantitative evidence that the LLM results are \"covering\" the space well? I realize this might be challenging, the metrics which come to mind often involve knowing the ground-truth $cap_{i=0}^n R_i$ in order to provide a percentage, but without this there's seemingly no way to know if the model is not simply exploiting some degenerate pattern to solve the task easily but in a useless way. Would it be possible to provide such a metric if we limit ourselves to words with length less than some threshold?\n5. You mention in the \"Datasets\" section that the training dataset is infinite and generated online, and that the validation dataset is also generated in an online fashion. Does this mean there is potential for train/test overlap?\n\n**Typos / Minor Suggestions** (small selection)\n* $y_i$ in equation (1) should probably be $y_j$ (I assume the subscript here has no relation to the subscript of $R_i$).\n* The set $[R_i, R_j]$ is not defined. I assume $[R_i, R_j] = \\\\{[u,v] \\mid u \\in R_i, v \\in R_j\\\\}$.\n* I wasn't able to parse the first sentence of Remark 5.1."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2245/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698815312953,
        "cdate": 1698815312953,
        "tmdate": 1699636157787,
        "mdate": 1699636157787,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "b20qA3DPdF",
        "forum": "AfiM6F2YPY",
        "replyto": "AfiM6F2YPY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2245/Reviewer_aKQP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2245/Reviewer_aKQP"
        ],
        "content": {
            "summary": {
                "value": "This paper aims at studying the simplicial homotopy groups of the 2-dimensional sphere. Following Wu's formula, each homotopy group is isomorphic to the quotient of a free group. Each element in the homotopy group, as a reduced word, can be connected to a sentence in the language models. The decoder part of a transformer is used to generate approximate samples from the homotopy group, and a multi-label is used to denote whether the sample satisfies relations of the homotopy groups. Multiple approaches for handling the multi-label and training the transformer are designed. Adequate numerical experiments are provided, with comparison to multiple baselines."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. The problem originating from algebraic topology is important.\n2. The idea is very innovative.\n3. The paper is well-written and mostly clear.\n4. Adequate numerical experiments are provided."
            },
            "weaknesses": {
                "value": "1. It would be helpful to discuss a bit of how the samples drawn from the homotopy group can be used to understand the topological properties of the 2-dimensional sphere, compared to the more obtainable homology groups.\n2. It would be nice to discuss the significance and potential applications of the proposed method to topological spaces other than the 2-dimensional sphere."
            },
            "questions": {
                "value": "1. Does the samples drawn using the decoder transformer have a much wider diversity compared to the baseline methods? What are the proportions of repeated samples?\n2. Is it possible to intuitively understand the distributions over the homotopy group that the transformer or the baseline methods are drawing from?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2245/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2245/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2245/Reviewer_aKQP"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2245/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698826446164,
        "cdate": 1698826446164,
        "tmdate": 1699636157717,
        "mdate": 1699636157717,
        "license": "CC BY 4.0",
        "version": 2
    }
]