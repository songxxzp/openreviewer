[
    {
        "id": "7yYW46vdCo",
        "forum": "KBGbEncHZF",
        "replyto": "KBGbEncHZF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6193/Reviewer_eCwJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6193/Reviewer_eCwJ"
        ],
        "content": {
            "summary": {
                "value": "This paper analyzes the uncertainty of various AI models for deepfake detection. In particular, the authors propose two uncertainty metrics based on established Bayesian frameworks (Bayesian neural networks and Monte Carlo dropout) to quantify the confidence of the deepfake detectors' judgments. These metrics are used to compare the in-domain and cross-domain generalization ability of different model architectures, i.e., convolutional networks and detectors of biological signals. The authors further conduct leave-out-one experiments, extract uncertainty-driven saliency maps for explainability, and analyze the robustness against adversarial attacks. The paper\u2019s main conclusion is that underconfident certain models are superior to overconfident uncertain models in terms of generalization."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The main strength of the paper is the problem formulation and the angle of study, i.e. to focus on the uncertainty of deepfake detection models, which is a great concern when deploying in real-world systems. Estimating the uncertainty of deepfake detectors is indeed a worthwhile goal that is underexplored in the literature. The Bayesian techniques for uncertainty quantification that are used in the paper are emerging in the literature and have sufficient theoretical justification. They truly have the potential to restore trust in AI models and enhance the reliability of deepfake forensics."
            },
            "weaknesses": {
                "value": "Despite the promising idea, the authors do not convincingly and clearly show the value of the proposed metrics and the goal of the uncertainty analysis. First, the meaning of the metrics is not explained and must be found in the literature, e.g., in the similar setting of Ranganath et al, which is not cited near the definitions of the metrics. I understand that the predictive uncertainty captures the concentration of the predictive distribution after we average the distributions of the sampled models, while the mutual information captures the consistency of the distributions of the sampled models. It is difficult for a user to draw meaningful conclusions about a model\u2019s output from such metrics, at least not without a straightforward interpretation and calibration  This analysis is more appropriate for model comparison but does not clarify which metric is better suited to the task, while it can be argued that the paper conclusions are not supported by the results\n\nRecall that the paper\u2019s main conclusion is that underconfident certain models are superior to overconfident uncertain models in terms of generalization. Table 1 evaluates various models and their Bayesian counterparts on the in-domain generalization task, i.e., the models are trained on FaceForensics and are evaluated on the unseen test set of  the same dataset . The authors conclude that, since the accuracy of the blind detectors drops more when switching to the Bayesian setting (which incorporates model uncertainty) compared with the biological detectors, the blind detectors are more prone to overfitting and this is reflected in the uncertainty metrics. From the two blind models of the experiment however only the accuracy of the BNN_EfficientNetB4  drops, while the accuracy of the BNN_Resnet18 increases. Even for BNN_EfficientNetB4, the conclusion is hard to justify. For example, the 99.38% accuracy of the deterministic EfficientNetB4 drops to the 90.93% of the BNN_EfficientNetB4 on the unseen test set of FF. Does this mean that the deterministic model overfits or that it generalizes better than the Bayesian model in the same domain?  The BNN_EfficientNetB4 with accuracy 90.93% also has higher uncertainty than the BNN_Motion detector with accuracy 87.40%. Does this mean that the higher test accuracy of the BNN_EfficientNetB4 is less reliable than the other model\u2019s?\n\nIf overfitting here is meant in the context of cross-domain generalization, no conclusions can be drawn because no cross-domain results are reported for EfficientNet B4. On the other hand, both BNN_Resnet18 and BNN_FakeCatcher generalize with >90% accuracy to FAVC despite BNN_Resnet18 having higher uncertainty than BNN_FakeCatcher in Table 1.The authors claim that BNN_Resnet18 has higher uncertainty on FAVC compared with BNN_FakeCatcher but does this imply that the 93.54% accuracy of BNN_Resnet18 on the unseen domain is less reliable? These concerns are even more striking in the leave-one-out experiments of Table 3; when BNN_Resnet18 and BNN_Motion are evaluated on the unseen FSw manipulation, their accuracies drop considerably to 18.25% and 52% respectively, yet no appreciable difference is reflected in the uncertainty metrics. Similar observations can be made for the cross-domain results of Appendix C.\n\nA further weakness of the paper is its poor writing, which is without spelling mistakes but  frequently confusing. For example, different groups of models with different training setups are compared in different experiments. Crucial details are omitted or  difficult to find, e.g., the fact that detection is studied at the frame level and aggregation is not  performed is stated in Appendix A, retention curves are not explained etc. Plots are also confusing, e.g., Figure 1 is difficult to read and the values of mutual information seem to extend in negative values, adjacent plots within the same figure do not have the same y-axis, etc. GANs are mentioned extensively in the introduction but concern only 2 out of the 5 manipulations of FaceForensics.\n\nAs final comments, while the authors state that they chose a representative collection of detectors, all blind detectors are convolutional-based and all detector models operate at the frame level. Newer architectures tend to operate at the video level directly and use more complex architectures such as 3D-convolutional, LSTM, and vision transformers. The authors also claim that FaceForensics and FAVC are the only datasets that reveal the manipulation method; however, ForgeryNet is another choice that is substantially larger and more diverse.\n\nBased on the poor interpretation of the results and writing of the paper, my recommendation is unfortunately to reject the paper, despite its very promising focus. I would like to encourage the authors to rethink the usage of the uncertainty metrics, refocus their objectives, and resubmit their improved work."
            },
            "questions": {
                "value": "See my detailed comments in the weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6193/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697808244502,
        "cdate": 1697808244502,
        "tmdate": 1699636674052,
        "mdate": 1699636674052,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uVSx4HtIS0",
        "forum": "KBGbEncHZF",
        "replyto": "KBGbEncHZF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6193/Reviewer_DTE3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6193/Reviewer_DTE3"
        ],
        "content": {
            "summary": {
                "value": "The authors introduce various uncertainty metrics to evaluate the performance of deepfake detectors. Specifically, they employ a Bayesian approach for numerical uncertainty evaluation and present two visualization methods for uncertainty assessment. These methods are subsequently applied to two renowned deepfake datasets for validation."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "\u2022\tThe paper is articulately written, making it easy for readers to understand the presented content.\n\n\u2022\tThe authors provide numerous visualizations throughout the paper, aiding in comprehension and the conveyance of key points.\n\n\u2022\tThe study covers multiple uncertainty metrics to deliver a comprehensive evaluation."
            },
            "weaknesses": {
                "value": "\u2022\tThe paper's contributions appear somewhat limited. While the authors present their evaluations and experiences, there seems to be a lack of deeper analytical insights.\n\n\u2022\tThe sole reliance on FAVC for generalization evaluation appears to be an uncommon choice, especially when there are other prominent datasets available like CelebDfv2, DFDC, and WildDeepfake.\n\n\u2022\tConsidering the scope of this study, the number of deepfake generation methods and detectors analyzed seems limited.\n\n\u2022\tPrevious work has introduced certain metrics that the authors seem to reuse. Given the limited datasets and models used in this study, the novelty and depth of insights appear constrained.\n\n\u2022\tIt would be beneficial for the authors to emphasize key findings in each experiment to guide readers through the paper's narrative more effectively."
            },
            "questions": {
                "value": "\u2022\tCould the authors clarify the term \"blind detector\" as mentioned in the manuscript? An explanation or definition would help in understanding its context.\n\n\u2022\tI would appreciate it if the authors could address the concerns and weaknesses outlined above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6193/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698711480435,
        "cdate": 1698711480435,
        "tmdate": 1699636673901,
        "mdate": 1699636673901,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yMzG3qzwb6",
        "forum": "KBGbEncHZF",
        "replyto": "KBGbEncHZF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6193/Reviewer_Aw4C"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6193/Reviewer_Aw4C"
        ],
        "content": {
            "summary": {
                "value": "This paper applies Bayesian Neural Networks to measure the uncertainty of deepfake detectors. A pixel level uncertainty map can also be obtained by the gradient of predictive uncertainty over pixels. Uncertainty for multiple deepfake detectors are computed under different experiment setting."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Uncertainty of deepfake detectors is a relatively less investigated area."
            },
            "weaknesses": {
                "value": "1) The motivation of this paper is not well justified. In the introduction, the authors mentioned \"Understanding model response\n... help compare their generalization in-the-wild, their overfitting to the artifacts, their performance beyond the training distribution, their robustness against adversarial attacks, and their effectiveness in source detection.\" However, the paper does not provide enough information about any of them. Section 4.3 and 4.6 talk about source detection and adversarial samples, but both sections are too short and not informative, so it is unclear how the predictive uncertainty help these two tasks?\n\n2) The technical contribution is quite low, since the major technique is to adopt BNN (a technique from 12 years ago).\n\n3) How do we know whether predicted uncertainty of deepfake detectors is accurate? In other words, how to evaluate uncertainty prediction in the case of deepfake detectors?\n\n4) The paper is poorly written. The technical part is not clearly presented: for example, in (1) and (2), most notations are not explained, e.g., what is x, y, D? There are lots of typos, grammar errors, and even broken sentences. For example, in page 4 above equation (1), \"We measure predictive uncertainty (predictive entropy) capturing a combination of input uncertainty and model uncertainty, and model uncertainty (mutual information) computing the difference between the entropy of the mean of the predictive distribution and the mean of the entropy\".  It is too hard to understand what the sentence is trying to say."
            },
            "questions": {
                "value": "See the weakness section. \n\nWith all three reviewers giving low ratings, the authors came up with a rebuttal that did not provide sufficient meaningful information, but lots of aggressive languages. Not really professional."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6193/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6193/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6193/Reviewer_Aw4C"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6193/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698799490797,
        "cdate": 1698799490797,
        "tmdate": 1700756135205,
        "mdate": 1700756135205,
        "license": "CC BY 4.0",
        "version": 2
    }
]