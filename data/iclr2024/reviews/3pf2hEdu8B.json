[
    {
        "id": "3p6nEsVpeG",
        "forum": "3pf2hEdu8B",
        "replyto": "3pf2hEdu8B",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5525/Reviewer_u9aL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5525/Reviewer_u9aL"
        ],
        "content": {
            "summary": {
                "value": "This study develops a new metric assessing the robustness of learnt representations by measuring collapse degree of them. The authors first point out the weakness of previous metrics proposed by Wang & Isola (2020) in its insensitivity with dimensional collapse. In addition, five characteristics are determined as the criteria for the ideal metric. The new metric (*Wasserstein Distance* between normalized learnt representations\u2019 distribution with zero-mean isotropic Gaussian distribution) is introduced satisfying all these criteria, and illustrates its sensitivity towards dimensional collapse. Empirically, this metric help boosting existing frameworks\u2019 performance in downstream tasks.\n\n\nReference:\n\nWang, T., & Isola, P. (2020, November). Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In\u00a0*International Conference on Machine Learning*\u00a0(pp. 9929-9939)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Contribution:\n    - This study stems from a good motivation: The lack of current researches explicitly dealing with representation collapse - here dimensional collapse.\n    - 5 listed criteria for the ideal \u2018uniformity\u2019 metric are logically/mathematically sensible. By pointing out the failure of previous work\u2019s metric (Wang & Isola, 2020), the authors clarify the need for a new metric.\n    - The experiments (e.g. Table 2) compare various vanilla approaches with their variance with the proposed metric. These are representative frameworks following three existing directions in dealing with constant collapse. This suggest the potential robustness of proposed metrics when being incorporated to a wide range of models.\n\n- Presentation:\n    - The intuitive flow makes it easy catch on what the authors want to deliver.\n    - The authors provide trackable mathematical notations and derivations."
            },
            "weaknesses": {
                "value": "### \n\n- While the 5 criteria are sensible, they can be considered as **necessary conditions** for a good metric. However, no assessment made to ensure they are **sufficient** to construct an ideal one.\n- The key contribution of the work - new \u2018uniformity\u2019, currently rely on the assumption that the learnt representations follow a Gaussian distribution, which, in turn, can fall apart and the metric can cause undesired effect.\n- For experiments, only what related to collapse analysis there exists the comparison between Wang & Isola\u2019s \u2018uniformity\u2019 metric with the proposed one. We can conclude nothing on the better performance of the proposed WD-based loss when fused with existing frameworks. It seems not convincing for developing a new loss if the existing loss still works better."
            },
            "questions": {
                "value": "- Can the authors empirically justify if the proposed loss outperform existing loss when being incorporated with other frameworks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5525/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698442490927,
        "cdate": 1698442490927,
        "tmdate": 1699636566533,
        "mdate": 1699636566533,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wmzLeyOnWa",
        "forum": "3pf2hEdu8B",
        "replyto": "3pf2hEdu8B",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5525/Reviewer_a1G3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5525/Reviewer_a1G3"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the author revisits the alignment and uniformity property in Self-Supervised Learning and shows that the metric in [1] lacks the ability to measure dimensional collapse, which is a phenomenon where the representations learned through self-supervised learning span a low-dimensional subspace instead of being distributed uniformly in the representation space. Therefore, the author proposes a new metric utilizing the Wasserstein distance between the distribution of the representation space and the normalized isotropic Gaussian distribution.\n\n[1] Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In ICML, 2020."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper includes a lot of empirical analysis to demonstrate the effectiveness of the proposed metric.\n2. The paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "1. The font size of the figures is too small.\n2. It should be made more clear why the original metric doesn't correspond to the proposed 5 properties and how these properties are related to dimensional collapse.\n3. It should be made more clear why dimensional collapse is a undesired property in self-supervised learning.\n4. Since $\\mathcal{W}_2$ is also related to the covariance matrix of representation, what is the relationship between the proposed method and Barlow Twins/VICReg?\n5. Why is the KL divergence of $Y$ and $\\hat{Y}$ not presented for comparison as in Figure 3?\n6. As in [2], the phenomenon of dimensional collapse happens in the embedding space. However, in [2], when the projector is presented, the representation space doesn't collapse. Therefore, why do the singular values of the representation in Figure 8 collapse?\n[2] Li Jing, Pascal Vincent, Yann LeCun, and Yuandong Tian. Understanding dimensional collapse in contrastive self-supervised learning. In ICLR, 2022."
            },
            "questions": {
                "value": "Please refer to Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5525/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5525/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5525/Reviewer_a1G3"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5525/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698599773905,
        "cdate": 1698599773905,
        "tmdate": 1700653391111,
        "mdate": 1700653391111,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "j0zfLwAShu",
        "forum": "3pf2hEdu8B",
        "replyto": "3pf2hEdu8B",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5525/Reviewer_KkL8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5525/Reviewer_KkL8"
        ],
        "content": {
            "summary": {
                "value": "This paper explains the shortcomings of the existing uniformity metric and the potential dimensional collapse then the authors propose new metrics of uniformity. Numerous experiments have proved its effectiveness."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The uniformity metrics proposed in this paper are more comprehensive and their validity has been verified experimentally."
            },
            "weaknesses": {
                "value": "Can the authors further elaborate on the importance of ICC, FCC and FBC to better understand the difference between the proposed metric and previous metrics?"
            },
            "questions": {
                "value": "Why the new metric has a relatively small performance increase on zero-CL?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5525/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5525/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5525/Reviewer_KkL8"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5525/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698673513495,
        "cdate": 1698673513495,
        "tmdate": 1699636566316,
        "mdate": 1699636566316,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qeUY9DAvpd",
        "forum": "3pf2hEdu8B",
        "replyto": "3pf2hEdu8B",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5525/Reviewer_RABe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5525/Reviewer_RABe"
        ],
        "content": {
            "summary": {
                "value": "The authors propose criteria for a uniformity loss during representation learning. This criteria is met through the use of the quadratic Wasserstein distance between learned representations and a uniform Gaussian distribution as a loss function, promoting uniformity without reduction in rank of the learned representation. Empirically this is shown to improve performance on the CIFAR-10/100 for a variety of models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is very well written, clearly stating the desired criteria and showing the proposed approach meets these criteria. The overall motivation behind the desired properties for a representation learning loss seem reasonable with a combination of theoretical justification and intuitive explanation/visualization provided.\n\nEmpirically, the results show an improvement by adding the proposed loss function. The authors do a nice job of evaluating the impact of this additional loss term for a variety of approaches."
            },
            "weaknesses": {
                "value": "The main weakness of this paper is that experiments are only done for the CIFAR-10/100 datasets. Given the proposed approach is claiming that dimensional collapse represents a fundamental issue with representation learning, the impact/relevance of this claim would be significantly strengthened by showing dimensional collapse is a fundamental issue and not necessarily a product of the CIFAR datasets.\n\nOn a related note it would be interesting to see the impact of the proposed metric on differing representation dimensions. In particular would the performance improvements still be observed if the representation dimension was doubled or tripled given that property 5 penalizes constant dimensions, intuitively making it seem that this would require careful selection of the representation dimension."
            },
            "questions": {
                "value": "Are there any results for differing representation dimensions?\n\n(Not a negative comment, just hoping to get some insight into the behavior of the loss) Is there any intuitive reason as to why the top-5 accuracy decreases for MoCo and BarlowTwins on CIFAR-10 using the proposed loss even though the top-1 accuracy increases?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5525/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698796195099,
        "cdate": 1698796195099,
        "tmdate": 1699636566199,
        "mdate": 1699636566199,
        "license": "CC BY 4.0",
        "version": 2
    }
]