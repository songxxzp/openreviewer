[
    {
        "id": "pOv6KPPABs",
        "forum": "qrGjFJVl3m",
        "replyto": "qrGjFJVl3m",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1807/Reviewer_2REz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1807/Reviewer_2REz"
        ],
        "content": {
            "summary": {
                "value": "This model introduced a Vision Language Model QWEN-VL which is both pretrained and instruction finetuned. The model shows decent multimodal capability, especially in terms of bounding box reasoning. The model will be open sourced which will be helpful to the community."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Open sourcing the model is going to help the research community\n- The model shows decent multimodal capability, especially in terms of bounding box reasoning"
            },
            "weaknesses": {
                "value": "My concerns are regarding the scientific and technical contributions from this paper.\n- The claim in the related work section, \"Despite achieving significant progress, previous vision-language models still have several limitations such as poor robustness in instruction following, limited generalization capabilities in unseen tasks, and a lack of in-context abilities.\" lacks justification. For example many models are not instruction-tuned (yet). That does not mean they have a fundamental difficulty in instruction following.\n- There is limited innovation on the model architecture and training recipe. For example the use of interleaved data and multi-stage, multi-resolution training has been proposed in previous works. Also there is limited novelty in showing that supervised finetuning with interleaved chat data can lead to chatting capability.\n- The ablation study is not written clearly (also see questions below)"
            },
            "questions": {
                "value": "In the ablation study of Figure 7, which stage is that, lower-res or higher-res? If it is lower-res (224) stage it makes sense to use 256 tokens as the native number of patches is just (224/14)^2 = 256. If it is the higher-res (448) stage, then it is counterintuitive that using more tokens, i.e., 400, with more degrees of freedom, will lead to worse performance."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Discrimination / bias / fairness concerns"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Since it is an open source generative model, there should be analysis regarding Discrimination / bias / fairness"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1807/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1807/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1807/Reviewer_2REz"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1807/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698826244107,
        "cdate": 1698826244107,
        "tmdate": 1699636110151,
        "mdate": 1699636110151,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Q7EqLOlK8L",
        "forum": "qrGjFJVl3m",
        "replyto": "qrGjFJVl3m",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1807/Reviewer_8ZLo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1807/Reviewer_8ZLo"
        ],
        "content": {
            "summary": {
                "value": "This paper showcase the qwen-vl as a versatile LMM, being able to perceive and understand both texts and images. The  qwen-vl series contains a multitask finetuned 7B model and a chatbox trained with interleaved data. The modeling is similar to flamingo but the trainable parts are different in different stage. The model achieves reasonable generalist scores."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(1) Very clear pretraining data size and mixture weights that helps the general audience get a sense of the pretraining distribution, though the paper uses some internal data, which is understandable\n\n(2) good ablation study over different parts, window attention for highres\n\n(3) good experiment setups that consider sufficient academic benchmarks, \n\n(4) well written and easy to follow"
            },
            "weaknesses": {
                "value": "(1) seems missing generalist PaLI results. The PaLI-X authors also have multitask finetuned model for VQA and captioning mixtures separately.  \n\n(2) missing the design / motivation or ablation of which part being trained during different stage. The stage 2 unfreezes ViT is for adapting to higher solution?"
            },
            "questions": {
                "value": "Please comment on the weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1807/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698887459608,
        "cdate": 1698887459608,
        "tmdate": 1699636110091,
        "mdate": 1699636110091,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kRcmdqlBJh",
        "forum": "qrGjFJVl3m",
        "replyto": "qrGjFJVl3m",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1807/Reviewer_RE8P"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1807/Reviewer_RE8P"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes QWEN-VL, a series of large-scale vision-and-language models (LVLMs). Further details can be found in Strengths."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- S1: This is one of a few open-source models where the model weights are released (though I don\u2019t think the data is; there is also some \u201cin-house\u201d data; see Table 2). This can benefit the research community; the claim is that while the performance of QWEN-VL is still behind private models, it excels in the open-source community, especially in terms of capabilities it supports (Figures 4-7).\n\n- S2: The training pipeline (Figure 3) is sound and simple."
            },
            "weaknesses": {
                "value": "- W1: Weak research significance and contributions. This work is a huge engineering effort and it is appreciated. However, research-wise, I am not convinced that it can provide any insights in terms of large-scale model training, architecture, or evaluation.\n\n- W2: Weak discussion of related work and clarity: To make W1 worse, the paper does not properly discuss the relevant work. If the paper would like to focus on the open-source aspect, I think it can expand this part much more heavily. What are the existing open-source LVLMs and what are \u201copen\u201d about them? What are the capabilities they support and so on? However, based on the current presentation this is unclear."
            },
            "questions": {
                "value": "- Is the train-test overlap between benchmarks taken care of? Especially COCO-based datasets.\n\nPlease address the two points in my Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1807/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699257281491,
        "cdate": 1699257281491,
        "tmdate": 1699636109970,
        "mdate": 1699636109970,
        "license": "CC BY 4.0",
        "version": 2
    }
]