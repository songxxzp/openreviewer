[
    {
        "id": "uApeKKTfDQ",
        "forum": "3fRbP8g2LT",
        "replyto": "3fRbP8g2LT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2910/Reviewer_vob1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2910/Reviewer_vob1"
        ],
        "content": {
            "summary": {
                "value": "This paper points out that the message passing mechanism entails redundancy, limiting expressiveness and cause over-squashing. To solve this, the authors presents solutions based on directed line graph / directed acyclic line graph. Furthermore, they show that the solution improves expressiveness, and validate it across various benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper points out the cause of redundancy in graph neural networks into two aspects, cycles and backtracking, and eliminate it by using path trees and directed line graphs (DLG).\n2. A theoretical analysis on the expressive power exists for the proposed method, though under certain conditions."
            },
            "weaknesses": {
                "value": "1. The paper title says \u201chigher expressiveness\u201d and \u201cless over-squashing\u201d. While there exist a theoretical analysis for the expressive power in section 2.7, there are no explanation or mention about over-squashing neither in theory or experiments.\n2. The paper doesn't have any expressive power analysis for conventional GNNs, while they do for DLGN and ERFGN. When using k-WL for the expressivity power of GNNs for example, one claims that a new architecture is as powerful as the 2-WL test while conventional GNNs are as powerful as the 1-WL test$^{[1]}$, concluding that the new architecture is more powerful than the original. However, the paper lacks expressive power analysis for conventional GNNs with ${L}$ layers, and only propose the expressive power analysis for DLGN and ERFGN. \n3. The paper is somewhat difficult to read, with some typos (ex. For table 4, there are only 2 datasets while the caption states three datasets, while for table 5, they are no bold results for the ogbg-molhiv dataset).\n\n[1] Xu et al., How Powerful are Graph Neural Networks?, ICLR 2019"
            },
            "questions": {
                "value": "1. Following weakness #1, are there any theoretical and experimental support for the claim DLGN/ERFGN results less over-squashing?\n2. I have been familiar to using the term expressive power of graph neural networks to measure the ability of the model distinguishing non-isomorphic graphs. How can the graph radius used to be to measure the expressive power of graph neural networks?\n3. Following Table 1, authors claim that DLGN and ERFGN shows improved efficiency in addressing message passing redundancy. However, the complexity of DLGN/ERFGN seems to be higher when compared to the complexity of typical MPNNs. Does the author mean the proposed method offers improved efficiency compared to the prior work, DLGN? If so, it would have been clearer by adding a checklist row at the bottom of Table 1, whether the method considers redundancy."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2910/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2910/Reviewer_vob1",
                    "ICLR.cc/2024/Conference/Submission2910/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2910/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698311906724,
        "cdate": 1698311906724,
        "tmdate": 1700729182454,
        "mdate": 1700729182454,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "w2JXHYJuO0",
        "forum": "3fRbP8g2LT",
        "replyto": "3fRbP8g2LT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2910/Reviewer_WgZE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2910/Reviewer_WgZE"
        ],
        "content": {
            "summary": {
                "value": "Two surrogate structures for graphs are proposed: Directed Line Graphs (DLGs) and Directed Acyclic Line Graphs (DALG). They provably enable relatively flexible message passing neural networks (DLGN and ERFGN) to distinguish non-isomorphic graphs. This makes them provably expressive in graph classification tasks. They further allow to achieve redundancy-free message passing to address over-squashing."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Directed Line Graphs (DLGs) are proposed as surrogate structure of graphs to enable non-backtracking message passing. This is supposed to overcome issues with over-squashing, message confusion, and expressiveness of GNNs.\n- The composition of multiple algorithms are proposed to convert a graph into a DLG or DALG.\n- The expressive power of the proposed GNN architectures is proven by showing that they could distinguish non-isomorphic graphs.\n- The runtime of the proposed graph conversion and message passing models are analysed. Even though the runtime is considerable (see weaknesses), at least the message passing scheme of DLGN is more efficient than the typical one (RFGNN) that also tries to avoid redundant messages."
            },
            "weaknesses": {
                "value": "- Alternative approaches to reduce message redundancy (like SPAGAN (Yang et al., 2019) or PathNNs (Michel et al., 2023)) could be discussed in more detail and be compared to in experiments.\n- The proof of Lemma 1 (as well as Lemma 2) is not detailed and does not verify that each step of the conversion actually defines a bijection. In this sense, the proof is not complete. \n- The proposed graph conversion is computationally very costly and therefore does not scale to large graphs. In particular, the TPT extraction and sub-DALG construction have a complexity of $O(|V_C|!)$.\n- Furthermore, dense graphs would have large DALGs.\n- The definition of the proposed architectures (e.g. ReadNCG) require training relatively complex models with a high number of trainable parameters and trainable functions that might be difficult to train in practice.\n- Many of the experimental evidence that is presented does not lead to significant improvements.\n\nMinor points:\n- Chordless cycles are not introduced. Their knowledge is just assumed.\n- It could help the reader to present an example where backtracking messages actually present a problem for the expressiveness of GNNs."
            },
            "questions": {
                "value": "- Does the removal of circles not imply that a graph with cycles and the same graph without cycles become indistinguishable?\nWith additional message aggregation for cycles, this is addressed, but how does the aggregator need to look like so that full expressiveness is achieved? Can this aggregator be learned in practice?\n- Please add a precise definition of ERFGN.\n- Please add a definition of the maximum radius of a component of DLGN or ERFGN.\n- Is it clear that backtracking messages are bad for solving a task? Couldn't they also cover computations that are not realisable by DALG or ERFGN?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2910/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2910/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2910/Reviewer_WgZE"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2910/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698788008786,
        "cdate": 1698788008786,
        "tmdate": 1700692357775,
        "mdate": 1700692357775,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RCNQ76hA6X",
        "forum": "3fRbP8g2LT",
        "replyto": "3fRbP8g2LT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2910/Reviewer_Gehp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2910/Reviewer_Gehp"
        ],
        "content": {
            "summary": {
                "value": "The authors propose to transform the input graphs of GNNs into directed (acyclic) line graphs and process them using custom neural architectures. The transformations aim at reducing redundancy in message passing. Experimental results show improvements of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Redundancy is a key problem in message-passing GNNs; reducing it has been shown to alleviate oversquashing.\n2. The approach builds on recent work in the same direction.\n3. Experimental results are promising."
            },
            "weaknesses": {
                "value": "1. The presentation is not sufficiently clear and several claims and results require further substantiation:\n   - The construction of the DALG is fundamental for the work, but its description (list with 7 steps on page 4) needs to be clearer: In step 1 the chordless cycles are extracted from the graph. While not mentioned in the paper, this can be an extremely expensive step as the number of chordless cycles can be exponential in the graph size. However, the chordless cycles are only used in step 2 to partition the graph into two components based on the edges contained in cycles and those that are not contained in cycles. However, this can be achieved in linear time using a standard algorithm for finding biconnected components. The crucial part of the construction is the generation of path trees for the biconnected components using an approach closely related to RFGNN proposed by Chen et al. (NeurIPS 2022). These are finally combined with the representation of the acyclic part. No motivation is given as to why it is advantageous to build the path trees only for the biconnected components.\n   - The complexity analysis needs to be clarified: While it is claimed that the approach is efficient, this is not clear from the construction of the transformed graphs and the analysis in Section 2.8. First, I would expect no advantage of the approach on biconnected graphs compared to RFGNN. Unfortunately, the parameter $V_C$ used to specify the running time has not been explained. If $V_C$ is the set of nodes in the biconnected components (which I assume), I do not understand why the approach is considered efficient, since its running time is factorial in $|V_C|$. If it is only efficient for graphs with small biconnected components (like molecular graphs) this limitation needs to be made explicit.\n   - Minor remarks:\n      - In the introduction, walks are specified by their label sequence. It would be much clearer if vertex identities were used instead.\n      - Section 2.1: The sentence \"A cycle $c \\in C$ consists of connected nodes/edges in graph $G$.\" is not a definition of cycles; and also not a helpful statement.\n      - Section 3: The conclusion \"The experimental results demonstrate the expressive power of our models.\" is not justified. As the results show test accuracy (I assume), they also reflect the generalization of the approach.\n2. The contribution is incremental and the novelty is limited.\n   - A similar path tree has been proposed for RFGNN by Chen et al. (NeurIPS 2022).\n   - Directed line graphs have been used in several other papers, e.g., \n      - Pierre Mah\u00e9, Nobuhisa Ueda, Tatsuya Akutsu, Jean-Luc Perret, Jean-Philippe Vert: Extensions of marginalized graph kernels. ICML 2004\n      - Zhengdao Chen, Lisha Li, Joan Bruna: Supervised Community Detection with Line Graph Neural Networks. ICLR 2019\n\n      The first paper by Mah\u00e9 et al. introduced the idea of avoiding backtracking to graph learning, more specifically, to random walk kernels. The second paper introduced a similar technique for GNNs. Both papers are not cited.  The novelty of the specific use of (directed) line graphs needs further discussion.\n\n3. The expressivity analysis is not sufficiently rigorous. First, there are statements that need justification, e.g., the proposed approach is claimed to achieve \"higher-order expressiveness\" (abstract). However, the method is not formally related to k-WL (if the term refers to this). I have some concerns regarding the proof of Lemmas 1 and 2 (in the appendix): The \"if and only if\" statements require to show two directions. Unfortunately, only one direction is discussed in detail; namely, that isomorphic graphs lead to isomorphic transformed graphs. However, the reverse direction is much more interesting. These are not discussed but just claimed to be true. A rigorous proof is necessary."
            },
            "questions": {
                "value": "1. Is my understanding that the cyclic subgraph is the union of all biconnected components?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2910/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698832023164,
        "cdate": 1698832023164,
        "tmdate": 1699636234245,
        "mdate": 1699636234245,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3ien21a1MS",
        "forum": "3fRbP8g2LT",
        "replyto": "3fRbP8g2LT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2910/Reviewer_Dnf7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2910/Reviewer_Dnf7"
        ],
        "content": {
            "summary": {
                "value": "This paper discusses improved heuristics for the ``redundancy-free'' design of graph neural networks. The development of the GNN leverages the line graph and makes substantial improvements in computational complexity by mining the acyclic line graph structure. An extended model for the interplay between DALG and cycles is also proposed."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper is clearly motivated and nicely organized. \n2. The empirical improvement is significant."
            },
            "weaknesses": {
                "value": "Given that the major motivation and contribution of this paper are easy to follow, I suggest that further details should be provided to help readers further understand this method from theoretical and practical aspects, please check my questions."
            },
            "questions": {
                "value": "1. What are the empirical time costs of constructing the DALG for DLGN / ERFGN for different datasets?\n2. Given that DLGN follows the message-passing scheme on DLG/DALG and the isomorphism of DLGs/DALGs is equivalent to the isomorphism of the original graphs, can we conclude the expressiveness of DLGN is upper bounded by 1-WL test? Can the interplay of DALG and cycle improve the expressiveness?\n3. Table 6 and 7 are nice, it could be even better if the authors would like to compare the comparison of #parameters for maybe the most competitive baselines.\n4. What is the empirical effect of changing tree-height $L$? How to choose a proper number of ERFGN layers if L changes?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2910/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2910/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2910/Reviewer_Dnf7"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2910/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699106736628,
        "cdate": 1699106736628,
        "tmdate": 1699636234182,
        "mdate": 1699636234182,
        "license": "CC BY 4.0",
        "version": 2
    }
]