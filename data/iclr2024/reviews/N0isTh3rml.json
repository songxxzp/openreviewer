[
    {
        "id": "CZLDoM207f",
        "forum": "N0isTh3rml",
        "replyto": "N0isTh3rml",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission595/Reviewer_VZpe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission595/Reviewer_VZpe"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a method to sample layouts based on adjacency matrices. The layout can better capture the graph structures by injecting the sampled layout information into the GNN message-passing process. Several layout sampling methods are proposed. The authors evaluate the proposed methods on several benchmarks and increase the performance of the backbone graph learning models.  The layout computation process can be seen as a preprocessing step to boost the GNN's ability to learn graph structures."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is well-written and easy to follow. The proposed approach to sample layout based on free energy seems well-attuned to real-world molecular applications. The proposed method naturally generates layout for each input graph which makes it easier to examine important substructures within the graph, such as functional groups in molecules."
            },
            "weaknesses": {
                "value": "- The evaluation is limited to smaller datasets (max 4k nodes). However, larger datasets such as QM9 and ZINC contain over 100k graphs. Showing performance increases on these datasets can further validate the method.\n\n- I noticed that the computational efficiency is sub-optimal, requiring ~0.1 to 2 seconds per graph. Note that while preprocessing can save training time, it does not save inference time, which is more critical practically.\n\n- The method is a preprocessing method that computes information unobtainable by MPNNs. While the experiments compare the method with some existing preprocessing methods, a clear comparison/analysis would help understand the mechanism behind the proposed method and why it is superior to RWPE. (In fact, it appears odd to me that the authors omit LapPE, since LapPE also considers global graph information (spectrum). Ideally, a comparison to LapPE should also be provided)."
            },
            "questions": {
                "value": "Please refer to the weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission595/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission595/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission595/Reviewer_VZpe"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission595/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698282748846,
        "cdate": 1698282748846,
        "tmdate": 1699635986858,
        "mdate": 1699635986858,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aGpQ3nwwCB",
        "forum": "N0isTh3rml",
        "replyto": "N0isTh3rml",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission595/Reviewer_8jRK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission595/Reviewer_8jRK"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces Distributional Edge Layouts (DEL) as a novel approach to graph representation learning where topological layouts are generated by sampling from a Boltzmann distribution. DEL is a  pre-processing technique which is independent of subsequent GNN architectures, demonstrating high flexibility and applicability. The integration of DELs into various GNNs yields improvements on diverse datasets, highlighting the practical potential of this approach. While the idea is innovative and shows promise, there are areas that need to be addressed to strengthen the paper"
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1) The concept of DEL, which involves sampling topological layouts from a Boltzmann distribution to improve graph representation learning, is a novel and interesting approach. \n\n2) The paper presents a set of experiments using DEL in combination with various GNN backbones. The results demonstrate that DEL can enhance the performance of GNNs on diverse datasets, which is a notable strength.\n\n3)  The paper provides clear explanations of the methodology, implementation details, and experimental setup."
            },
            "weaknesses": {
                "value": "- Lack of Theoretical Analysis: The paper could benefit from a more in-depth theoretical analysis of DEL. While the empirical results are promising, a theoretical framework that explains why DEL works and under what conditions would provide stronger support for the proposed approach.\n\n- Dimensionality Exploration: The exploration of DEL in high-dimensional spaces is a valuable experiment. However, the paper would benefit from a more in-depth discussion of the implications of these findings. \n\n- Sampling Layout Numbers: While the study addresses the impact of the number of sampled layouts on the performance, the explanations and discussions in this section are somewhat brief. More detailed insights into how the number of layouts affects the performance and potential trade-offs would enhance the paper's depth.\n\n- Computational Complexity: The paper discusses the computational complexity of DEL, but there is room for a more comprehensive discussion, particularly concerning scalability. Discussing the implications of computational complexity in real-world, large-scale applications is crucial."
            },
            "questions": {
                "value": "why DEL works and under what conditions would provide stronger support for the proposed approach ?\n\nWhy does DEL perform better in lower dimensions, and what does this mean for practical applications?\n\nWhat are the core theoretical principles behind DEL's effectiveness in graph representation learning?\n\nCould you explain the trade-offs in choosing the number of sampled layouts for DEL, and are there practical constraints to consider?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission595/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699072883682,
        "cdate": 1699072883682,
        "tmdate": 1699635986770,
        "mdate": 1699635986770,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "f8xGbWMhXf",
        "forum": "N0isTh3rml",
        "replyto": "N0isTh3rml",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission595/Reviewer_YS2Q"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission595/Reviewer_YS2Q"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a new GNN architecture where the main idea is first to sample a graph layout and then use this layout for a standard message-passing GNN. The first step is done by sampling according to a Boltzmann distribution (this is not learned and done before the training of the GNN). It can be combined with any GNN architecture afterwards."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The idea of doing first a preprocessing on the graph in order to get a new layout is interesting as it can be coupled with any GNN architectures for downstream tasks."
            },
            "weaknesses": {
                "value": "The preprocessing step is not clearly defined in the paper. The authors use notions like Boltzmann distribution and Langevin dynamics, but if you look at the code, they only use a preprocessing using the Python library `networkx`. Algorithm 1 is unclear as it never uses the graph structure. \n\nThe evaluation is very weak because the datasets used (MUTAG, IMDB...) have been used extensively and are known to be very poor in order to compare the power of various architectures. Indeed, the differences between the various GNNs are very small."
            },
            "questions": {
                "value": "Please do not use complicated notions like Boltzmann distribution or Langevin dynamics unless these are really required. It is OK to use 'networkx` algorithms but then say it explicitly in the paper and remove the unnecessary parts. If I am making a mistake, please give me the lines of your code where your Algorithm 1 and Algorithm 2 are implemented.\n\nFor proper benchmarking of GNNs, you can use: Dwivedi, Vijay Prakash, et al. \"Benchmarking graph neural networks.\" arXiv preprint arXiv:2003.00982 (2020). or more recent datasets."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission595/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699288790362,
        "cdate": 1699288790362,
        "tmdate": 1699635986697,
        "mdate": 1699635986697,
        "license": "CC BY 4.0",
        "version": 2
    }
]