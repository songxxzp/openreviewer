[
    {
        "id": "Y8SbFOYFze",
        "forum": "EyDPfGy4Wh",
        "replyto": "EyDPfGy4Wh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9177/Reviewer_dyhc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9177/Reviewer_dyhc"
        ],
        "content": {
            "summary": {
                "value": "This paper presents Expert Projection Attention (EPA). It reduces compute and memory requirement for attention by using MoE layers for values and output projections."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "the method is clearly illustrated and the analysis in 2.3 is helpful for understanding the difference."
            },
            "weaknesses": {
                "value": "1. The author seems to misunderstand the position of flash-attention, see questions below.\n2. the scale of experiment is small, how would this method generalize to larger models such as llama?\n3. Some experiments have not been finished (Table 4)."
            },
            "questions": {
                "value": "(1) In what sense flash-attention reduces compute. Do you mean FLOP or wall clock time? FA is exact attention and does not reduce FLOP, it is just a series of clever fusion. If it is wall clock time, then this paper should keep the definition consistent, and provide wall clock time analysis instead of MAC.\n(2) \"Unlike FlashAttention (Dao et al., 2022), it is research-friendly, because it does not hide the internal details of the attention mechanism inside a CUDA kernel.\" is an either arguably wrong or highly subjective judgement to flash-attention."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9177/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9177/Reviewer_dyhc",
                    "ICLR.cc/2024/Conference/Submission9177/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9177/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698441315276,
        "cdate": 1698441315276,
        "tmdate": 1700507494419,
        "mdate": 1700507494419,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZdjmMUtzMD",
        "forum": "EyDPfGy4Wh",
        "replyto": "EyDPfGy4Wh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9177/Reviewer_obUv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9177/Reviewer_obUv"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a method for selecting one head but keeping a small number of Q,K and V matrix.\nI really feel this paper is quite poorly written. First of all, the notations are extremely unclear especially in the method section. The authors have a schematic in\nFigure 1 but it's unclear what this schematic means. There is no explanation of different boxes.\n\nThe authors constantly compare to FlashAttention, they have not even a single run time comparison. \nI think the paper needs a complete re-write. The method section is using non-standard notation where is unclear what dimensions they are reducing to. The experiments do not talk about fine-tuning overheads, do not have a single timing results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The idea at a high level looks decent. However, the poor writing and underwhelming evaluation really makes it hard to appreciate it."
            },
            "weaknesses": {
                "value": "Please see the summary."
            },
            "questions": {
                "value": "Please see the summary."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9177/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9177/Reviewer_obUv",
                    "ICLR.cc/2024/Conference/Submission9177/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9177/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698778448716,
        "cdate": 1698778448716,
        "tmdate": 1700485069302,
        "mdate": 1700485069302,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HvwP4taCax",
        "forum": "EyDPfGy4Wh",
        "replyto": "EyDPfGy4Wh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9177/Reviewer_DLPx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9177/Reviewer_DLPx"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a modification to the attention mechanism by incorporating a mixture of experts in both the source (K, Q) and destination (V, O) projections. This modification enables the selection of fewer active heads, thereby reducing computational and memory costs during both training and inference. The paper is based on the premise that not all attention heads are necessary for a given task. By utilizing an expert to select the required heads, it is possible to decrease computation and memory expenses. The effectiveness of this algorithm is demonstrated by comparing its accuracy to that of the dense counterpart and by visualizing the attention matrices."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is well-written and effectively highlights the issues with the current attention architecture in terms of computational and memory demands.\n2. The paper conducts experiments on various datasets and compares its results with existing baseline methods, including MOA.\n3. The paper conducts a thorough analysis of attention maps to facilitate a qualitative study and comparisons with conventional attention matrices."
            },
            "weaknesses": {
                "value": "1. The paper refers to FlashAttention multiple times and compares against their CUDA kernel (SW designed to exploit HW efficiently) optimization vs algorithmic insight in this paper. I am not sure if its an apple-to-apple comparison since there are tons of other literature for transformers which aim to reduce computation/memory cost (like quantization/sparsity methods) and the paper doesn\u2019t compare against these.\n2. While authors compare against FlashAttention custom kernel implementation and mention that as a drawback, EPA algorithm itself requires a custom CUDA kernel with its own set of restrictions (pointed in the results section).\n3. For the EPA algorithm, the paper mentions that K/Q source experts are not necessary for good results and only output/value experts are required, which seems to contradict the disadvantages shown in 2.2 naive algorithm."
            },
            "questions": {
                "value": "1. Can the authors compare against architectures other than TransformerXL? It is not evident from the text why only 1 architecture is chosen for comparison?\n2. It is not evident from the paper how nHead is chosen for a task. Most results demonstrated fixed the nHead to be 2 or 4. Did the authors perform smaller experiments to first search for optimal nHead before scaling up?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9177/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698794030470,
        "cdate": 1698794030470,
        "tmdate": 1699637154379,
        "mdate": 1699637154379,
        "license": "CC BY 4.0",
        "version": 2
    }
]