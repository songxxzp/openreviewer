[
    {
        "id": "h8vaKLHPA8",
        "forum": "Zb6qOouUJO",
        "replyto": "Zb6qOouUJO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission110/Reviewer_oXEw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission110/Reviewer_oXEw"
        ],
        "content": {
            "summary": {
                "value": "This paper considers the stochastic bilevel optimization problem and proposes a new fully single-loop method using the LSVRG to approximate the gradient. Also, theoretical analysis and experiments are presented to show the superiority of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper is well-written and easy to follow. \n2. The proposed is novel and the theoretical analysis and experiments are presented to show the superiority of the proposed method."
            },
            "weaknesses": {
                "value": "1. More experiments are expected, such as meta-learning, and poison attack. I think hyperparameters selection and data hyper-cleaning are somewhat similar, experiments on other applications are expected.\n2. Some O(1) sample complexity methods should be compared, e.g. SUSTAIN[1]\n3. The proposed method seems can not effectively solve the large-scale problem. Can the author give some results on the large-scale datasets?\n4. The convergence analysis is based on the PL condition. An analysis on a more general case is expected.\n\n[1] Khanduri P, Zeng S, Hong M, et al. A near-optimal algorithm for stochastic bilevel optimization via double-momentum[J]. Advances in neural information processing systems, 2021, 34: 30271-30283."
            },
            "questions": {
                "value": "1. Can the author explain the relation between the gradient estimation of the proposed method and other hypergradient methods?\n2. Why the complexity of SVRB is different from VRBO in Table 1? I think they have the same complexity since they all use STORM. \nSee other questions in weakness.\n\nOn page 4, below Equation (4), the sentence is not correct."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission110/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission110/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission110/Reviewer_oXEw"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission110/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698403169005,
        "cdate": 1698403169005,
        "tmdate": 1699635936003,
        "mdate": 1699635936003,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "E61eH6hrQz",
        "forum": "Zb6qOouUJO",
        "replyto": "Zb6qOouUJO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission110/Reviewer_UgVc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission110/Reviewer_UgVc"
        ],
        "content": {
            "summary": {
                "value": "The paper proposed a single-loop algorithm inspired by L-SVRG solving (finite-sum) stochastic bilevel optimization problem with an iteration complexity of $\\mathcal{O}((m+n)^{3/2}/T )$ and a memory cost of $\\mathcal{O}(d + p)$. The main contribution is reducing the memory cost from $\\mathcal{O}((m+n)(d+p))$ to $\\mathcal{O}(d + p)$ but achieving the same iteration complexity compared with the state-of-the-art algorithms."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Originality: In this paper, the authors considered using L-SVRG for problems with a nested structure and proposed a method for solve stochastic bilevel optimization problem based on it. \n\nQuality: Compared with SABA, their approach reduced the memory cost significantly. \n\nClarity: The overall structure and presentation of the paper is clear. \n\nSignificance: This research provide lower memory cost without affecting iteration complexity for stochastic bilevel optimization and other related problems."
            },
            "weaknesses": {
                "value": "1. In Contribution (c), the authors stated \"We establish the link between our method and related areas, such as federated learning and minimax optimization, and we provide a theoretical analysis for both of these areas.\" However, I don't see any theoretical analysis about federated learning. It is only in the future work section.\n\n2. From my perspective, the novelty of the paper is limited. The only improvement is reducing cost memory by using a different variance reduction technique. \n\n3. The plots in the paper are hard to read. For example, in figure 2, what is the x-axis of the plots. It is better to provide some plots in terms of running time."
            },
            "questions": {
                "value": "1. In Theorem 1, the authors stated \"This result leads to the convergence rate $\\mathcal{O}(\\epsilon^{\u22121})$, which is optimal in stochastic bilevel optimization. \" I think this result is for general stochastic bilevel optimization problem. But in this paper, the authors considered a finite-sum version of it, which is easily than the general version. The convergence rate could be potentially improved. It would be more convincing if the authors point out more related references.\n\n2. In Corollary 1, the authors stated \"the rate under nonconvex conditions remains unclear. We initially introduce a rate of $\\mathcal{O}(n^{2/3}\\epsilon^{\u22121})$\". But the authors did not state what kind of convergence criteria do they consider here? Can you provide more references related to the single-level result here?\n\n3. In Corollary 2, do you assume $F$ is convex? Or it could be possibly non-convex. If it is non-convex, how do you get the rate $\\mathcal{O}(n^{2/3}\\epsilon^{-1})$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission110/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission110/Reviewer_UgVc",
                    "ICLR.cc/2024/Conference/Submission110/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission110/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698448294188,
        "cdate": 1698448294188,
        "tmdate": 1700723903368,
        "mdate": 1700723903368,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dBi3tEMLgU",
        "forum": "Zb6qOouUJO",
        "replyto": "Zb6qOouUJO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission110/Reviewer_L11w"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission110/Reviewer_L11w"
        ],
        "content": {
            "summary": {
                "value": "This work provides a study for stochastic bilevel optimization and provided a fully single-loop stochastic bilevel algorithm using an idea from Loopless-SVRG. Compared to SABA, another fully single-loop stochastic optimizer via SAG, the proposed SBO-LSVRG method achieves a similar sample complexity but with less cost in memory and space. The algorithm and analysis are also applied to minimax problem as well. Experiments are provided to demonstrate the effectiveness."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThe motivation is clear and the study on fully single-loop bilevel method is very important given its simple structure and implementation. Applying the idea from loopless-SVRG is a good contribution, and the authors have done a good job in the algorithmic design and literature review. \n\n2.\tThe proposed algorithm achieves the same sample complexity as SABA, the best fully single-loop stochastic bilevel method, but with much less memory cost. In terms of the performance, it seems the method can be more efficient than SABA."
            },
            "weaknesses": {
                "value": "1.\tApplying the idea of L-SVRG in bilevel optimization sounds a little bit incremental. However, I feel that some challenges such as the probabilistic selection step and the proof of variance reduction may introduce some new analysis and designs. I strongly suggest that the authors can explicitly point them out instead of just saying \u201cThis approach is far from trivial\u201d. \n\n2.\tThe comparison miss an important baseline as show here (https://arxiv.org/pdf/2302.08766.pdf): it proposes a single-loop SARAH-based bilevel optimizer named SRBA, which achieves a near-optimal $(n+m)^{1/2}\\epsilon^{-1}$ sample complexity. It would be good to have a comparison here."
            },
            "questions": {
                "value": "Overall, I think this work has provided some interesting approach based on the idea from L-SVRG. I like the method and give 6. However, I cannot give higher score given several questions regarding the novelty clarification and the missing baseline. The questions and suggestions can be found in the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission110/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission110/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission110/Reviewer_L11w"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission110/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698691814477,
        "cdate": 1698691814477,
        "tmdate": 1699635935821,
        "mdate": 1699635935821,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "U3jgsa2K79",
        "forum": "Zb6qOouUJO",
        "replyto": "Zb6qOouUJO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission110/Reviewer_vkov"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission110/Reviewer_vkov"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the problem of bilevel optimization, and introduce  a new method named SBO-LSVRG. This method achieves the SOTA iteration complexity with a lower memory cost. The experiments confirms the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well-organized, and easy for readers to follow.\n2. This paper can obtain SOTA complexity with lower memery cost. The rigorous proof is provided."
            },
            "weaknesses": {
                "value": "1. The novelty is limited. This paper mainly follows Dagreou et al. (2022). Therefore, the theoretical contribution is limited. It is better if the authors could highlight the challenges in the analysis.\n2. Some assumptions made in the paper seems quite strong, would it hold in practical scenarios?"
            },
            "questions": {
                "value": "1. Can the proposed method handle non-convex or non-strongly convex lower-level problems, which are common in many real-world applications?\n2. This paper investigate the iteration complexity of the proposed method, how about the sample compexity?\n3. Is the rate obtianed in this paper optimal in terms of $\\epsilon$, $m$ and $n$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission110/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission110/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission110/Reviewer_vkov"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission110/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698815308835,
        "cdate": 1698815308835,
        "tmdate": 1699635935732,
        "mdate": 1699635935732,
        "license": "CC BY 4.0",
        "version": 2
    }
]