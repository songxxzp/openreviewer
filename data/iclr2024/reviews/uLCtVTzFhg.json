[
    {
        "id": "xyxMmg9ny3",
        "forum": "uLCtVTzFhg",
        "replyto": "uLCtVTzFhg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4808/Reviewer_FfAt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4808/Reviewer_FfAt"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new approach for positive unlabeled learning, which involves two steps: contrastive representation learning and psudo labeling. The proposed method is practical, theoretical analysis is provided, and good experiment results are shown. However, there are some concerns in motivation and experiments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed method combing contrastive learning and psudo-labeling makes sense and seems to be practical. \n2. The paper is well written and one can easily follow.\n3. Theoretical analysis is provided for different components of the method. \n4. Good experiment results are shown."
            },
            "weaknesses": {
                "value": "1. The paper was motivated by applications in recommender systems, drug, and others, but is only evaluated on image classification dataset. In fact, the proposed method relies on self-supervised constrastive learning, which mostly works well for images (or texts), as you can easily augment the data. However, for recommender systems and drug applications, there is no easy way to perform augmentation. It would be better to properly motivate the method to avoid overclaim. \n2. The novelty of the method seems to be limited. Using contrastive learning for positive unlabeled learning is not new, e.g. it is considered in[1]. Psudo-labeling with kmeans using positive examples for positive unlabeled learning is also considered literature[2]. It seems to be the novelty of the method is combing the two existing methods.\n3. Experiment evaluation can be improved to make the work more solid.\n\n[1] Chuang, Ching-Yao, et al. \"Debiased contrastive learning.\" Advances in neural information processing systems 33 (2020): 8765-8775.\n\n[2] Liu, Qinchao, et al. \"A novel k-means clustering algorithm based on positive examples and careful seeding.\" 2010 International Conference on Computational and Information Sciences. IEEE, 2010."
            },
            "questions": {
                "value": "1. How does the each component of the method compare to existing methods in literature? Specifically, how does contrastive learning component compare to the one in[1] and the kmeans component compare to the one in [2]? These are only two examples and I believe there should be more alternatives in literature.\n\n2. For image classification task, the most popular benchmark dataset is imagenet. Can you also evaluate the method on imagenet, e.g. by sampling 100 class following[1]?\n\n3. How many positive examples are used in each dataset? How does the method compare to the baselines when the number of positive examples vary? Can you show a plot about this?\n\n[1] Chuang, Ching-Yao, et al. \"Debiased contrastive learning.\" Advances in neural information processing systems 33 (2020): 8765-8775.\n\n[2] Liu, Qinchao, et al. \"A novel k-means clustering algorithm based on positive examples and careful seeding.\" 2010 International Conference on Computational and Information Sciences. IEEE, 2010."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4808/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4808/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4808/Reviewer_FfAt"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4808/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698605932137,
        "cdate": 1698605932137,
        "tmdate": 1701015701202,
        "mdate": 1701015701202,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ybOVwZiMRG",
        "forum": "uLCtVTzFhg",
        "replyto": "uLCtVTzFhg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4808/Reviewer_sTW4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4808/Reviewer_sTW4"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on the positive unlabeled classification problem. Unlike in the usual binary classification setup, our training data does not have any negatives. Instead, we have positives and unlabeled samples. There are two issues in the previous PU learning approaches. One is that most of them assume access to the underlying class prior, or if we do not have this knowledge, we need to estimate the class prior. The second issue is that previous approaches tend to perform poorly with few positive training data. This paper proposes a new method based on contrastive learning which improves over the self-supervised baseline empirically and theoretically. After the representation learning step, the paper further propose a method called PUPL, which is a pseudo-labeling clustering method with theoretical guarantees on when it can recover the true underlying labels. As a framework, the paper finally propose to train a classifier based on the clustering results with pseudo labels."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The statistical properties of the proposed method are studied: it is unbiased but also has smaller variance compared to the self supervised contrastive learning (ssCL) counterpart. These results are intuitive, since we are utilizing the additional positive label information that is not used in the ssCL algorithm.\n- Furthermore, a clustering algorithm is proposed to prepare pseudo labels for classifier training.\n- The experimental results show comparison with many baselines, and also show the proposed method is often the best performing method.\n- Code and jupyter notebook are provided in the supplementary link."
            },
            "weaknesses": {
                "value": "- I am wondering how strong the separable assumption discussed towards the end of Section 3 is. For example, it would be interesting to empirically check if the method's performance will degrade and class-prior based PU methods (with oracle class prior) become better when the two Gaussians approach each other in Figure 10 in the Appendix (corresponding to the case that the classes do not form a cluster).\n- The experiments that use previous PU methods after representation learning with puCL (instead of the proposed puPL) are interesting and important as an ablation study (shown in Table 1). However, the main baseline is nnPU, which is a method that is motivated to learn a classifier directly from input space and is not meant to be used for a linear classifier. It makes me wonder if there are other suitable baselines here, e.g., the other ones used in the other experiments. (However, if the experiments here are showing negative training loss even with a linear model without the non-negative component, then I feel it may be fine to use this as a comparison here.)\n- Some discussions about the relationship with other weakly supervised contrastive learning papers, such as \"PiCO: Contrastive label disambiguation for partial label learning\" (ICLR 2022) or \"ComCo: Complementary supervised contrastive learning for complementary label learning\" (Neural Networks, 2024) would be helpful. I understand that the type of weak label is different since this paper focuses on PU learning, not partial labels/complementary labels. However, it would make the contributions more clear if we can see that similar ideas have not been proposed before in papers that worked on weak supervision + contrastive learning."
            },
            "questions": {
                "value": "Other than the points I raised in the Weaknesses section, I would like to ask some minor questions and list some minor suggestions.\n\n- The class prior in page 1 is defined as $\\pi_p = p( y = 1 \\mid x)$ and this is also used in the appendix p22 (Definition 2). I am wondering if this should be $\\pi_p = p(y=1)$?\n- The legend of figures is quite small. For example, I cannot read the legend on Figure 11. Some of the colors seem similar to my eye and it is hard to distinguish between them.\n- In Algorithm 1: In my understanding, the training is based on (pseudo-)PN labels. Would it be more accurate to denote it as \"C. Train PN Classifier\" in the 3rd step? While the term \"PU Classifier\" is not incorrect, this adjustment might offer a more precise representation. Nevertheless, this is a minor comment, and I respect the authors' choice on this matter."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4808/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698624548497,
        "cdate": 1698624548497,
        "tmdate": 1699636463884,
        "mdate": 1699636463884,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "e9T3fyC8up",
        "forum": "uLCtVTzFhg",
        "replyto": "uLCtVTzFhg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4808/Reviewer_Yt2Y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4808/Reviewer_Yt2Y"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a framework for Positive Unlabeled (PU) learning, targeting the limitations of existing PU methods that require additional class prior knowledge and struggle in low-data scenarios. It utilizes pretext-invariant representation learning to create a feature space where unlabeled examples are pseudo-labeled based on the cluster assumption. The authors show that the proposed framework is particularly effective in scenarios with a lot of labeled data. Empirical results demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ Some theoretical analyses are provided. In a PU setting, the authors have shown that the loss based on treating the unlabeled training examples as pseudo-negative instances is biased. The authors have also analyzed the consistency of their method.\n+ Clarity of Presentation. The paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "+ The theoretical advantage is not clear. Firstly, the authors propose that the proposed method is not sensitive to the estimated class prior. It seems to be a more general method. However, to make contrastive learning reliable, it relies on another assumption related to the data generative process. Specifically, it requires the distribution of data $P(X)$ containing information about $P(Y|X)$. A clustering assumption is assumed in the paper which is an instance of the assumption. This actually cannot always hold in different datasets. For example, it can be hard to generally apply the proposed method to causal datasets as the cluster assumption cannot be satisfied. Moreover, it is unknown how to use contrastive learning on non-image datasets with theoretical guarantees such as UCI datasets. I believe existing PU methods with theoretical guarantees do not suffer from these issues and can be generally applied to other non-image datasets.\n+ Minor Empirical Improvement. The empirical improvements shown in the paper, while present, are relatively minor. This raises questions about the practical significance of the proposed method. It would be beneficial if the authors could demonstrate more substantial improvements or discuss scenarios where their method is expected to have a more pronounced impact."
            },
            "questions": {
                "value": "- How does the proposed method handle scenarios where the distribution of data $P(X)$ does not contain adequate information about  $P(Y|X)$, particularly in causal datasets where the clustering assumption may not hold?\n- Can you provide insights or theoretical justification for the applicability of your contrastive learning approach to non-image datasets and causal datasets (could refer https://pl.is.tue.mpg.de/p/causal-anticausal/) where clustering assumptions may not be valid?\n- Given the relatively minor empirical improvements reported, could you elaborate on specific scenarios or types of datasets where the proposed method is expected to yield more significant advantages?\n- Could you discuss how the proposed method compares in terms of practical utility and significance against existing Positive and Unlabeled (PU) learning methods, which have theoretical guarantees and broader applicability?\n- Are there plans to test the proposed method on large-scale, real-world datasets, particularly high-dimensional ones like image datasets, to evaluate its performance and generalizability in more complex scenarios?\n- How do you anticipate the proposed method would perform on such datasets, and what are the potential challenges you foresee in these environments?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4808/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698777098894,
        "cdate": 1698777098894,
        "tmdate": 1699636463785,
        "mdate": 1699636463785,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "icOxD7YOhk",
        "forum": "uLCtVTzFhg",
        "replyto": "uLCtVTzFhg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4808/Reviewer_xzEP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4808/Reviewer_xzEP"
        ],
        "content": {
            "summary": {
                "value": "This manuscript adopts a simple PU-specific modification of the standard self-supervised contrastive objective to take into account the available weak supervision in the form of labeled positives."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This manuscript represents the work tailoring contrastive learning specifically to the PU setting. Based on the self-supervised learning, the PU setting introduced more positive supervised information, which could result in a better performance."
            },
            "weaknesses": {
                "value": "The experiments are not conducted on a recommendation data set though the PU learning setting applies in recommendation."
            },
            "questions": {
                "value": "In step (b), assigning pseudo-labels to the unlabeled examples may introduce label noise, Can the methods deal with label noise can improve the performance? I'm interested in seeing these outcomes."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4808/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698839552580,
        "cdate": 1698839552580,
        "tmdate": 1699636463698,
        "mdate": 1699636463698,
        "license": "CC BY 4.0",
        "version": 2
    }
]