[
    {
        "id": "8dfM4ey5I4",
        "forum": "P50qJuu4IY",
        "replyto": "P50qJuu4IY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8379/Reviewer_PMfK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8379/Reviewer_PMfK"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a new self-supervised method that reduces the gap between the ground-truth matching and optimal matching. The proposed loss, matching gap, could potentially alleviate the problematic signal that force different views of the same image to collapse to the same point, even when they capture dramatically different contents, i.e. foreground vs background. To further ease the optimization, the paper proposes to learn the optimal matching via the Sinkhorn algorithm. Experimental results show that the proposed method performs on par with SOTA approaches with strong data augmentations and outperforms several latest self-supervised works with simpler data augmentations.\n\n***\nPost-rebuttal comment:\n\nFollowing the discussion with the authors regarding their revision, the reviewer has maintained the original rating (marginally below the acceptance threshold), as the revision has not fully addressed the reviewer\u2019s two primary concerns: i) the need for fair comparisons on the reported metrics (mainly about performance) ii) lack of sufficient justifications for the claimed benefits other than performance. The first concern was partially addressed in the rebuttal, which seems to indicate that MG is less effective than the baselines in terms of performance. The second concern was not addressed in the rebuttal due to time constraints,  making it difficult for the reviewer to understand the extra advantages of the work beyond the performance aspects. The reviewer suggests the author further explore and demonstrate MG\u2019s benefits beyond mere performance. The authors have recognized these shortcomings, and are committed to further improving the work in accordance with the feedback from both Reviewer 1FzB and the present reviewer."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(S1) [Motivation] self-supervised learning learns feature representation via pretext tasks. As there are typically no human supervision involved. The \u201cground-truth\u201d signals of such tasks are usually pretty \u201cnoisy\u201d. The paper aims to alleviate the noisy supervision by reducing the gap between the ground-truth matching and optimal matching that could be computed on-the-fly. The reviewer believes this is an interesting topic.\n\n(S2) [Method] the paper proposes to approximate the optimal matching via the Sinkhorn algorithm, which could further ease the online optimization.\n\n(S3) [Ablation] Ablations on different components of the proposed method are included"
            },
            "weaknesses": {
                "value": "(W1) [Evaluation] The evaluation section could have been more comprehensive. For example, when strong data augmentations are involved (Table 1), only several SSL baselines are included, e.g. MoCo-v3, DINO. The settings shown in Table 1 are also not consistent, e.g. different number of epochs, which makes it difficult to interpret the results, e.g. could the proposed method match DINO\u2019s performance when trained for the same number of epochs? Also, it would be beneficial to include architectures beyond ViT-B(L)/16, e.g. ViT-S, CNN, etc.\n\n(W2) [Performance] With strong augmentations, the proposed method shows no benefit compared to SOTA methods. The method outperforms several SSL approaches with weaker augmentations. However, at least from the application perspective, it is unclear to the reviewer what are the advantages of using only weak augmentations, especially when the training epochs are the same.\n\n(W3) [Claim] Some of the claims are not well-justified. For example, i) compared to CL that may \u201ccollapse representations for very different images\u201d, the proposed method learns diverse representations for different views of the same image; ii) stronger data augmentations could help mitigate the representation collapsing problem. In order to justify the claims, the authors may measure/compare the mean distance of different views of the same image, or different samples of the same classes, across different models (i.e. the proposed method and baselines) and settings (i.e. strong/weak augmentations)\n\nOverall, the paper studies an interesting topic, the proposed method is also technically sound. However, he reviewer has concerns about the evaluation, performance and some of the claims in the paper. At this moment, the reviewer rates the paper as marginally below the acceptance threshold."
            },
            "questions": {
                "value": "N.A."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8379/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8379/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8379/Reviewer_PMfK"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8379/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698791140980,
        "cdate": 1698791140980,
        "tmdate": 1700956884795,
        "mdate": 1700956884795,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "W9ybmgKZdT",
        "forum": "P50qJuu4IY",
        "replyto": "P50qJuu4IY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8379/Reviewer_tJTN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8379/Reviewer_tJTN"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed an alternative loss, Matching Gap (MG), to contrastive loss for self-supervised representation learning. Unlike contrastive loss enforcing the sample-wise invariance to data perturbations, the MG loss is a set-based loss, driven by minimizing the difference between the ground-truth transport loss and the optimal transport loss computed in the representation space using the Sinkhorn algorithm. The authors detailedly discussed the differences and connections of MG loss to contrastive loss and prior optimal transport, showing the unique properties of the proposed method. Finally, experiments on ImageNet-1k dataset suggested a comparable performance of MG to prior arts."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is overall well-motivated. The reliance on data augmentation is one of the most prominent nuisances of contrastive learning. It is good to see more exploration toward bypassing this issue.\n\n2. The theoretical analysis presented MG loss in a straightforward way and is overall easy to grasp. It also discussed the links between MG loss and contrastive loss/invert optimal transport loss, showing its unique properties as a set-based loss with single-level optimization.\n\n3. MG loss exhibited superior performance to contrastive loss in weak augmentation and low training epochs regime."
            },
            "weaknesses": {
                "value": "1. The advantages of the single-level optimization in MG loss over the bi-level optimization in IOT loss are not provided clearly. Figure 2 shows that MG loss slightly underperforms but is competitive with IOT loss. I wonder if it improves the training speed/convergence or reduces the memory consumption?\n\n2. Unfair comparisons. The implementation of the experiments largely followed the setting of Dino, which used two global crops and ten local crops by default. However, some of the baseline methods, e.g., MoCov3 and I-JEPA, used only two global crops, making it unfair to directly compare the performance with MG loss on the default setting.\n\n3. Even under the potentially unfair comparison, the performance of MG loss is only comparable and sometimes even inferior to the contrastive loss.\n\n4. Some notations are used without first introduced, e.g., $c(\\cdot,\\cdot)$ in Introduction and $t(\\cdot,\\cdot)$ in Sec. 3."
            },
            "questions": {
                "value": "See the weaknesses.\n\nOverall, I think the proposed loss is interesting, and I like the presentation of this paper. However, the evaluation part still has significant room for improvement."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8379/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8379/Reviewer_tJTN",
                    "ICLR.cc/2024/Conference/Submission8379/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8379/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698865306138,
        "cdate": 1698865306138,
        "tmdate": 1700656959880,
        "mdate": 1700656959880,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "J3IYuTbNEl",
        "forum": "P50qJuu4IY",
        "replyto": "P50qJuu4IY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8379/Reviewer_zo3d"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8379/Reviewer_zo3d"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new contrastive loss based on the matching gap. The proposed method is an extension of the paper \"Understanding and generalizing contrastive learning from the inverse optimal transport perspective\u201d. Also, the main idea of this paper is related to \"whether the cost of the identity ground-truth pairing is significantly higher than the optimal matching cost that can be achieved, and use their difference as a loss\"."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "This paper provides an explicit relationship between the gradients of the proposed matching gap loss with that of InfoNCE."
            },
            "weaknesses": {
                "value": "1. The novelty of this paper is rather limited. This article only uses a previously proposed technique to improve the computational complexity of inverse OT-based contrast loss in the optimization process. I do not find any new insights related to the field of contrastive learning.\n2. This paper is really hard to follow. There are many mathematical symbols and proper nouns that lack explanation. For example, what is t in eq. 6 and 7, what is bistochastic matrices, and what are the difference between matching cost,  measuring agreement,  matching gap, and optimality gap?\n3. The organization of Section Introduction is superfluous. I cannot find the relationship between the first two paragraphs and the last paragraph.\n4. The experimental results cannot verify the effectiveness of the proposed method. First, the performance gain is pretty small. Second, there are many cases where the proposed method obtains a bad result.\n5. The \"A Link between InfoNCE and the Matching Gap\" part and the \"Our Contribution: Single Level Optimization with the Matching Gap\" part are so vague that I have read them many times without understanding the logical relationship."
            },
            "questions": {
                "value": "1, How do you get eq. 8 from eq. 7?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8379/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8379/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8379/Reviewer_zo3d"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8379/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698929713948,
        "cdate": 1698929713948,
        "tmdate": 1700645169887,
        "mdate": 1700645169887,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YbaUIYrEWr",
        "forum": "P50qJuu4IY",
        "replyto": "P50qJuu4IY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8379/Reviewer_1FzB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8379/Reviewer_1FzB"
        ],
        "content": {
            "summary": {
                "value": "This paper suggest a novel approach to unsupervised representation learning. Following previous work (e.g. IOT), the main idea is to use an optimal transport plan between different view to guide the metric learning process. The main contribution is in the way this is done, which relies on trying to match the *cost* of the plan, rather than the plan itself to the ground truth pairing of augmented views. \nThis seemingly simple change brings several advantages in training, with very competitive results, and is shown to have an interesting interpretation when compared to the commonly used InfoNCE loss."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1] The matching gap loss is an important finding that I expect to have impact on the field. It is well motivated as a way to allow the needed flexibility in the contrastive learning setup, where positive views should not necessarily be forced to the same point. \n2] Its ease of use and computational advantages are clearly shown - the ability to use the OT guidance without the need to differentiate through the Sinkhorn iterations, with computations involving only the pairwise n x n pairwise matrices.\n3] The analysis that compares the new loss to the known InfoNCE is very enlightening and gives a very good understanding about what is happening in the optimization.\n4] The paper is well written in all aspects, from the motivation, throughout the solution and experimental results."
            },
            "weaknesses": {
                "value": "Here are some, but rather minor:\n1] Experimentation - I think that this new form of loss would be better justified if there would be empirical evidence that supports the intuitions (in addition to the standard benchmarking and ablations). It would perhaps be interesting to see how the embeddings of an augmented batch behave, in comparison to standard NCE, or some statistics of that kind.\n2] The formulation is restricted to the 2-view setting. While this is simple, it would be interesting to know whether there are effective generalizations to multi-view settings.\n3] There is no specification or discussion regarding batch size, which has an important role in contrastive learning. Supposedly, the compact computation and 2-view setup could allow for larger batch sizes. It would be interesting to see how performance scales with batch-size.\n4] Several minor inaccuracies (which don't affect the analysis or correctness): (i) Bistochastic should be non-negative (ii) Should be <P,logP> in Equation 3 (without the -1) (iii) Last row of the loss equation is wrong, resulting in a matrix rather than a value: should probably be \\eps<P,logP> instead of \\eps\\logP."
            },
            "questions": {
                "value": "* Please related to the above 'weaknesses'\n* I understand (and am in favor of) the limited budget experimentation. Did you ablate on number of epochs, within the budget, to see if the dimnishing returns behavior is comparable to other methods?\n* Due to the approach that does not require positives to converge to the same point - Perhaps there is actually room for more aggressive augmentation, that can exploit a richer extension of the training data?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8379/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699081605117,
        "cdate": 1699081605117,
        "tmdate": 1699637042764,
        "mdate": 1699637042764,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hOSD1BUbnk",
        "forum": "P50qJuu4IY",
        "replyto": "P50qJuu4IY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8379/Reviewer_iZps"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8379/Reviewer_iZps"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel contrastive loss based on optimal matching cost. The proposed matching gap loss may avoid feature collapse according to its property. The author conducts experiments mainly on ImageNet classification, and the experiment results show its superiority."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "i) The idea of introducing matching costs is reasonable. The experiment results show it superior to some baseline results\n\nii) The writing is clear and easy to follow."
            },
            "weaknesses": {
                "value": "i) As I know, DINOv2 and SwAV also use the optimal transport algorithm to solve contrastive learning. But I can not find the discussion about such methods in related work. I would like to find the discussion about the difference between Matching Gap and SwAV/DINO\n\nii) As for experiments. In Table 1, the performance of the Matching Gap is not as good as DINO, which is a strong baseline proposed 1 year before.\n\niii) The author only conducts downstream experiments on transfer classification. Many self-supervised learning methods evaluate the downstream detection(COOC, VOC) and segmentation(COCO, aed20k) performance. I think the simple downstream classification task is not enough."
            },
            "questions": {
                "value": "Refer to the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8379/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699582094743,
        "cdate": 1699582094743,
        "tmdate": 1699637042608,
        "mdate": 1699637042608,
        "license": "CC BY 4.0",
        "version": 2
    }
]