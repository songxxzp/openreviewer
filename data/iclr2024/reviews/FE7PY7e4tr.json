[
    {
        "id": "8wYC6vi3wF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3904/Reviewer_mSxa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3904/Reviewer_mSxa"
        ],
        "forum": "FE7PY7e4tr",
        "replyto": "FE7PY7e4tr",
        "content": {
            "summary": {
                "value": "The paper presents theory in regards to the expressivity of a multi-layer perceptron (MLP) in classifying between two classes, when each class is drawn from a smooth low-dimensional submanifold of a Euclidean space $\\mathbb{R}^D$. In a restricted setting where topological theory is more rich (intrinsic dimension at most 3, compact, orientable, and have boundary), the authors make an explicit construction of a neural network classifier, where the network size is bounded by terms of separated topological and geometric complexity. The authors then finally present experimental results to demonstrate the trend between topological complexity (measured through Betti numbers) and needed network size to sufficiently classify the data."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Visual aids for mathematical ideas were well designed and placed, helping greatly with readability.\n- The theory is very well done and written in a way that highlights the connection between Betti numbers and neural network classification."
            },
            "weaknesses": {
                "value": "(roughly ordered from least to most important)\n- A small handful of relatively minor typos (e.g. \"quaratically\" under main theorem, Definition 2 $\\mathcal{M} \\subset \\mathbb{R}^D$ instead of $\\in \\mathbb{R}^D$, \"fundamental\" quote formatting on page 5). While needed to be fixed, these make 0 impact on my overall review.\n- The term \"classification\" is overloaded in this paper, and the expected ICLR reader would likely be mislead by some of the writing. Namely, there are times when the author is talking about geometric manifold classification problems, where the goal is determining if one (or a set of) manifold(s) is homeomorphic to another. This is fundamentally different from neural networks classifying two classes. Since explicit clarification and distinction is missing, there are a number of misleading parts of the text. For example, the statement \"classifying manifolds beyond three dimensions is known to be equivalent to the word problem on groups [...] and is undecidable\" would lead the standard reader to think that this paper's restriction to at most 3-dimensional manifolds actually retains generality for neural network classification theory, but indeed this statement is talking about geometric manifold classification, not neural network classification.\n- The authors' claim that their \"result reveals for the first time how the topology, as a global structural characterization of data manifold, affects the network expressiveness\" is misleading; while this analysis related to specifically Betti numbers and neural network expressiveness, this work is not the first to relate \"structural characterization of data manifold\" to nextwork expressiveness. While their related works section does cover a good breadth of related work in the intersection of topology and deep learning, one work in particular was missed that is quite close in setting and goal to this paper, namely an ICLR 2021 paper titled [\"Deep Networks and the Multiple Manifold Problem\"](https://openreview.net/pdf?id=O-6Pm_d_Q-). This referenced paper is of course not a strict superset (e.g. \"Deep Networks[...]\" only focuses on 1-dimensional manifolds), but Buchanan et al. do lay down a lot of fundamental theory on relating different sizes of a network (e.g. width and depth) to the complexity of the data manifolds and the distributions over them that data is drawn from, much of which this paper misses and could benefit from integrating in terms of practicality.\n- I am having a harder time seeing how this theory applies to deep learning, outside of the analysis being over MLP's. If the goal is a new method for constructing MLP's from data, the paper's main neural network construction seems to rely on a decomposition of a data manifolds into a union of fundamental manifolds (e.g. ball, torus), which is hardly known or even computable from fixed, finite samples of practical data manifolds, and likewise does not characterize the type of functions typical MLP's fit. If the goal is to highlight the expected relationship between the sum of Betti numbers and needed network size to classify data, then the experiments shown are not nearly thorough enough to demonstrate this, not only due to unrealistic settings (incredibly high sampled data density, low dimensional, etc.), but because the only trend plotted was over a single family of manifolds (g-genus tori), which alongside its Betti numbers sum has many other geometric properties growing that could explain the expressivity trend. It is thus not clear from these demonstrations why it is meaningful to focus on the Betti numbers."
            },
            "questions": {
                "value": "- Suppose someone has two classes of rotated-MNIST, the 5's and the 9's (for our rotated-MNIST each image from MNIST is rotated 2*pi radians exclusive at, say, fidelity 10; so a single MNIST training class goes from 6,000 to 60,000 data points). They want to train an MLP-classifier between these two classes. How would they use the theory and results introduced in this paper to inform their initial choice of the width and depth of the MLP they start off with?\n- In the study of data manifolds and MLP classifiability, why should the research community focus analysis on Betti numbers for data manifolds instead of other previously studied measures of extrinsic geometry?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3904/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3904/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3904/Reviewer_mSxa"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3904/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697235210967,
        "cdate": 1697235210967,
        "tmdate": 1700678966898,
        "mdate": 1700678966898,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dyC0pOWKs4",
        "forum": "FE7PY7e4tr",
        "replyto": "FE7PY7e4tr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3904/Reviewer_ai47"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3904/Reviewer_ai47"
        ],
        "content": {
            "summary": {
                "value": "The paper studies expressive power of ReLU networks. The main result makes a connection between topology of a data manifold, as evaluated by Betti numbers, and a size (depth and width) of a ReLU network sufficient for its good approximation. The paper is mostly theoretical. Empirical results demonstrate tightness of the proposed bounds for one setting."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1) Only a few papers studies connection between topological complexity of data manifold and size of deep NN.\nNew theoretical results establish such a connection.\n2) The paper is well written and easy to follow.\n3) Experiments with tori in 3D demonstrate the tightness of bounds."
            },
            "weaknesses": {
                "value": "1) The paper contains only one experiment on a synthetic dataset, which is a union of solid tori.\nA typical paper from ICLR is expected to have a larger experimental section. Why not to try more complex datasets: entangled tori, nested spheres, etc. (see Naitzat, 2020).\n\n2) In Section 3, you write \"To explore the influence of manifold topology on network size, our analysis framework primarily\nleverages manifold classification theorems\". There seems to be an ambiguity of the term \"classification\". Here, you, probably mention **classification** of manifolds by their structure. While in ML, by classification we mean a separation of two manifolds in the shared ambient space, via a hyperplane of a non-linear separating surface.\n\n3) In experimental section, the testing loss, which is of main interest in ML/AI is not studied.\n\nFor now, I think that limited experimental section is the main drawback of this manuscript (I haven't checked math carefully)."
            },
            "questions": {
                "value": "1) In assumption 1, you state that M can be embedded into R^d, where d \u2264 3 and d \u2264 D.\nThis restriction seems to be very tight, because dimensionalities of manifolds from real datasets are much higher.\nCan it be relaxed?\n\n2) Naitzat, et al. 2020 showed that the topological complexity of data manifold (and its representations in NN) decreases with depth, while in your work you find that width is more important (beta^2 vs. log(beta)). Can you explain it?\n\n**Post rebuttal**. Thank you for the response. Only a minor part of issues were resolved. I realize that the manuscript is a theoretical one, but I would like to see more experiments, see weaknesses 1,3. For now, I prefer to remain my score unchanged."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3904/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3904/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3904/Reviewer_ai47"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3904/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698668760763,
        "cdate": 1698668760763,
        "tmdate": 1700835819070,
        "mdate": 1700835819070,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "KFMpKuu37t",
        "forum": "FE7PY7e4tr",
        "replyto": "FE7PY7e4tr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3904/Reviewer_9Qss"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3904/Reviewer_9Qss"
        ],
        "content": {
            "summary": {
                "value": "This work looks at neural network expressive power in terms of the topological and geometric properties of the underlying data manifolds. The work establishes upper bounds on the size of the network needed to distinguish between data drawn from two manifolds as a function of their geometric and topological properties. To quantify topology, the work uses the sum of the Betti numbers (also known as the *topological complexity*) and to quantify geometry, the work uses the reach of the manifolds. Informally, the latter can be thought of as being related to curvature. The main result of the paper gives an upper bound on network depth and network size (the sum of the hidden dimensions) in terms of these two quantities. The work puts a number of assumptions on the manifolds in question, including having dimension $3$ or less and being able to be embedded in $\\mathbb{R}^3$, being compact, and being orientable."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "**Quality of mathematics:** While the reviewer did not have time to check all the proofs, the mathematical constructions seemed reasonable. It makes sense to start with topological complexity and reach when looking for statistics that capture topology and geometry respectively. \n\n**Presentation:** Besides some typos and grammatical mistakes, the work was mostly well-presented. The format used for proceedings papers at machine learning conferences is not especially conducive to deep mathematical works that require some building up towards a proof, but the reviewer felt that this paper did a good job conveying the main ideas in the body even if all the proof work had to go into the Appendix. The reviewer appreciated that all the main ideas and assumptions were both motivated and clearly defined. An informal explanation of the significance of individual results was also useful.\n\n**Experiments:** The experimental section was appreciated, and helped ground some of the results. The relationship between the experiments and the main theorem was obvious and required only minimal explanation."
            },
            "weaknesses": {
                "value": "- **Assumptions on manifolds:** The types of manifolds that are considered are relatively limited. Some of the assumptions (such as compactness) are reasonable when thinking about data manifolds. Other assumptions, such as $d$-dimensionality, where $d \\leq 3$, seems somewhat limiting (for instance, [1] provided evidence that ImageNet has an intrinsic dimension between 26 and 43). That being said, other works in this research area also put constraints on some part of the set-up (or have correspondingly weaker conclusions) so this may be the price of progress.\n\n- **Hints at the proof technique:** For papers such as this where the main contribution is a theorem, it is useful to give a high-level overview of the proof, even if the proof itself is relegated to the Appendix. This was done to some extent in this paper, but some additional details could probably be included for clarity. For instance, it seemed that some classification theorem was being used (and this presumably was what determined the choice to constrain to *solid manifolds*), but it was not clear what this was from the body of the work. Further spelling out some of these dependencies might illuminate why various assumptions were made.\n\n### Nitpicks\n- The abstract and introduction consider the \u201csize\u201d of the network. This should be at least informally defined (later it is revealed to be the sum hidden dimensions).\n- \u201cindication function\u201d $\\mapsto$ \u201cindicator function\u201d.\n- There are a number of works that look at topological properties of ReLU networks that were not cited in the paper that may be worth considering (e.g., [2]).\n\n[1] Pope, Phillip, et al. \"The intrinsic dimension of images and its impact on learning.\" arXiv preprint arXiv:2104.08894 (2021).\n[2] Grigsby, J. Elisenda, and Kathryn Lindsey. \"On transversality of bent hyperplane arrangements and the topological expressiveness of ReLU neural networks.\" SIAM Journal on Applied Algebra and Geometry 6.2 (2022): 216-242."
            },
            "questions": {
                "value": "- The reviewer was not previously aware of the term \u201csolid manifold\u201d. Is this an idea developed in this work or elsewhere?\n- The work references the use of manifold classification theorems, what manifold classification theorem was actually used?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3904/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698820224553,
        "cdate": 1698820224553,
        "tmdate": 1699636350307,
        "mdate": 1699636350307,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zoYeJ1xb6F",
        "forum": "FE7PY7e4tr",
        "replyto": "FE7PY7e4tr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3904/Reviewer_DRpb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3904/Reviewer_DRpb"
        ],
        "content": {
            "summary": {
                "value": "This paper shows the relationship between the topological complexity of the data manifold and the depth and size required for expressive power of the classification network. This result indicates that the topological complexity perspective of data manifolds may be useful for network construction."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Upper bounds on the depth and size of the ReLu network are theoretically given, taking into account the phase complexity of the data manifold."
            },
            "weaknesses": {
                "value": "The settings discussed in this paper seem unrealistic from a realistic machine learning perspective.\n-  It is generally recognized that data manifolds are embedded in low-dimensional manifold, but anything below 3 dimensions seems too low-dimensional. In three dimensions or less, the target is likely to be relatively simple and easy to analyze by machine learning. Therefore, even if correctly shown in these settings, many theories do not hold true in higher dimensions. In other words, this result is not recognized as a result that can be passed on a general level. On the other hand, if it does not hold in these settings, it will not hold in general. So I recognize that the discussion in this paper is positioned as a pilot study that suggests that it may be important to consider topological complexity.\n- The discussion in this paper uses Betti numbers for data manifolds. However, since the data is typically given as a point cloud, the definition of the Betti number is not unique. This is the same as the discussion of filtration in persistent homology. In other words, it is not appropriate as a theory to evaluate a classification network if the results differ depending on the method used to calculate the Betti number, even for the same data.\n- The assumption is that the phase of the data manifold is known, but it seems difficult to know that in advance. Are we not specifically discussing building a network, but merely discussing relationships? In other words, is the goal to suggest the value of considering topology?\n- The network discussed in this paper introduces $N_p$ and $N_{\\phi}$ first. However, a general classification network does not include $N_p,N_{\\phi}$, nor is there a need to introduce it. Also, from the proof, the result of the main theorem seems to be strongly influenced by $N_p,N_{\\phi}$. In other words, the authors have failed to show that topology considerations are important for general classification networks, the only thing that can be shown is the obvious fact that networks with the additional topology analysis functionality introduced by the author himself are affected by topology complexity.\n- The assumption in this paper seems to be that the classes of classification are disjointed. Is that a realistic setting?\n- This discussion seems to be overly argumentative for the purpose of simply binary classification. Perhaps it would be more appropriate to discuss this in a setting such as expressive learning."
            },
            "questions": {
                "value": "I would like you to answer the questions I wrote in the weaknesses section above. I would like to hear thoughts on how this fits into the scope of this meeting, especially since the setting is so far from a realistic machine learning setting."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3904/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698834313729,
        "cdate": 1698834313729,
        "tmdate": 1699636350226,
        "mdate": 1699636350226,
        "license": "CC BY 4.0",
        "version": 2
    }
]