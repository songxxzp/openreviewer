[
    {
        "id": "4caCnChRrE",
        "forum": "bQfJLRlfYO",
        "replyto": "bQfJLRlfYO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission811/Reviewer_YEjv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission811/Reviewer_YEjv"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a new method of Question-Answering by utilising a combination of a Large Language Model and a Knowledge Base. The primary technique is to decompose a question into a set of simple questions, retrieve candidate answers for each simple question from Knowledge Base, and then select one candidate answer as the answer to a simple question, finally integrate all these simple question+answer into a text. On a number of benchmark datasets, this method reaches the SOTA performance. The authors claim the proposed method can perform better in the future."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper is very well written and easy to read. The authors applied the Chain-of-Thought (CoT)  idea to general question-answering, and very well motivated the whole design using existing techniques and datasets. This is beautiful."
            },
            "weaknesses": {
                "value": "At the methodological level, this paper is within the paradigm of the art of alchemy, in which authors demonstrated professional and proficient skills. The processes of question decomposition, selection, as well as the reasoning of answers, are all black-boxes. This might answer the question why this method has not outperformed the SOTA performance. \n\nA small mistake is that Authors forgot to remove the reference to the Appendix, which resulted in Appendix ?? everywhere in the text."
            },
            "questions": {
                "value": "In Section 4.2, there is a sentence \"we believe the performance of Keqing can be further improved with more powerful LLMs, like LLmMA-2\", and will include the results in the future. How is the related with the primary method by Question-decomposition using Knowledge bases?\n\nWhat if applying this method for the CoT logic reasoning? Could Keqing outperforms the SOTA level?\n\nCan you provide one error case to illustrate the limitation of the current Keqing system?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission811/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission811/Reviewer_YEjv",
                    "ICLR.cc/2024/Conference/Submission811/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission811/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698122310508,
        "cdate": 1698122310508,
        "tmdate": 1700724099295,
        "mdate": 1700724099295,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "r5CKr1nWX9",
        "forum": "bQfJLRlfYO",
        "replyto": "bQfJLRlfYO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission811/Reviewer_6eaX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission811/Reviewer_6eaX"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a model to assist LLMs, such as ChatGPT, to retrieve question-related structured information on the knowledge graph, and demonstrates that Knowledge-based question answering (Keqing) could be a nature Chain-of-Thought (CoT) mentor to guide the LLM to sequentially find the answer entities of a complex question through interpretable logical chains. Specifically, the workflow of Keqing will execute decomposing a complex question according to predefined templates, retrieving candidate entities on knowledge graph, reasoning answers of sub-questions, and finally generating response with reasoning paths."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Experiments on one-hop, two-hop, and three-hops are interesting, and the baseline methods compared against seem to be comprehensive, with good experiment results demonstrated."
            },
            "weaknesses": {
                "value": "1. Recent developments in question answering also consider utilizing graph neural network methods e.g., \nQuestion-Answer Sentence Graph for Joint Modeling Answer Selection. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 968\u2013979, Dubrovnik, Croatia. Association for Computational Linguistics.\n\n2. I have a concern regarding the novelty of the approach. This work simply uses RoBERTa-based similarity scores and DPR-based knowledge augmentation, both works which have already been proposed before. The authors need to better highlight their contributions and why they consider it original work."
            },
            "questions": {
                "value": "Can the authors also illustrate the runtime and memory complexity of their work, as it is highly dependent upon LLMs which incur large runtime for finetuning?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission811/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698794164044,
        "cdate": 1698794164044,
        "tmdate": 1699636008303,
        "mdate": 1699636008303,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yDcASNWVF7",
        "forum": "bQfJLRlfYO",
        "replyto": "bQfJLRlfYO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission811/Reviewer_jydE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission811/Reviewer_jydE"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a knowledge graph question answering using LLMs using chain of thought prompting of LLMs. Authors propose a 4 step process to process KBQA using LLMs namely Question Decomposition, Knowledge, Retrieval, Candidate Reasoning, and Response Generation. Authors try to show that using these steps to prompt LLMs can generate better response than text-SQL or structured query generation. This is demonstrated through experiments with few KBQA datasets and openly available LLMs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Experimental results showing the effectiveness of the approach on two KBQA benchmarks. \nAdapting question logical forms to aide chain of thought prompting in LLMS for KBQA."
            },
            "weaknesses": {
                "value": "Question decomposition to aid better performance is studied in the literature through BREAK paper etc. Only difference I see is just applying or solving some of those problems using LLMs and stitching the pipelines together. Not sure about the novelty of the overall approach. \nAuthors claim KBQA can be a nature guide to help LLMs in CoT prompting. I don't see how this can be transferred to other settings like lets say normal Open-domain QA using LLMs. Any results to show that these method can aid in solving open domain QA as well? Applicability of methods proposed methods beyond KBQA setting. \nWriting can be improved and Appendix reference missing consistently across the paper."
            },
            "questions": {
                "value": "1. How are the answers for KBQA are extracted for final F1 measure ? since LLMs generate free text, what method is used to extract the final answer from the LLM response ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission811/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698829092569,
        "cdate": 1698829092569,
        "tmdate": 1699636008188,
        "mdate": 1699636008188,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ToSSJgPmgb",
        "forum": "bQfJLRlfYO",
        "replyto": "bQfJLRlfYO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission811/Reviewer_eCHK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission811/Reviewer_eCHK"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces \"Keqing\", a groundbreaking framework aimed at amplifying the performance of Large Language Models (LLMs) in knowledge-based question answering (KBQA). While LLMs, such as ChatGPT, have demonstrated notable proficiency in various NLP tasks, they occasionally generate incorrect or nonsensical responses, particularly when faced with questions that exceed their training data's scope. To counter this, Keqing incorporates an IR module to extract structured information from a knowledge graph, systematically guiding the LLM to answer intricate questions. This methodology not only bolsters the trustworthiness of the LLM's answers but also ensures that these responses are interpretable."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.The comprehensive four-stage workflow (decomposition, retrieval, reasoning, and response generation) offers a systematic approach to knowledge-based question answering.\n\n2. The framework guarantees that the produced answers are not just accurate but also transparent, revealing the logical journey leading to the conclusion.\n\n3. Experiments conducted on GrailQA, WebQ, and MetaQA validate the effectiveness of the framework."
            },
            "weaknesses": {
                "value": "1. The title is somewhat misleading, obscuring the paper's main contribution. Given that the paper primarily centers on a novel framework integrating an IR module to derive structured data from a knowledge graph, the connection between this pipeline and CoT remains unclear.\n\n2. The paper's novelty in comparison to traditional KBQA systems is ambiguous. While elements like question decomposition and candidate reasoning aren't new to the field, it's uncertain whether this is the first instance of such a pipeline being employed with LLMs.\n\n3. The model's performance in a few-shot scenario appears to lag behind state-of-the-art fine-tuned models, such as Decaf. It would be beneficial to pinpoint the reason for this shortfall or determine which step in the process contributes to this gap."
            },
            "questions": {
                "value": "1. The title of the paper suggests a focus on the CoT mentor, but the main content seems to be centered around a framework that integrates an IR module with a knowledge graph. Could you clarify the relationship between this pipeline and the concept of CoT?\n\n2. In terms of originality, how does the proposed framework distinguish itself from traditional KBQA systems? Specifically, while aspects like question decomposition and candidate reasoning are familiar in the literature, is this the inaugural application of such a pipeline using LLMs?\n\n3. The results indicate that the model's performance in a few-shot learning scenario is not on par with state-of-the-art models like Decaf. Could you shed light on the reasons behind this disparity? Which part of the framework or which specific stage might be contributing to this performance gap?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission811/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698898569917,
        "cdate": 1698898569917,
        "tmdate": 1699636008110,
        "mdate": 1699636008110,
        "license": "CC BY 4.0",
        "version": 2
    }
]