[
    {
        "id": "OJu9E16Ua2",
        "forum": "2gwo9cjOEz",
        "replyto": "2gwo9cjOEz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6389/Reviewer_bUqG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6389/Reviewer_bUqG"
        ],
        "content": {
            "summary": {
                "value": "This study offers a theoretical analysis of Graph Neural Networks (GNNs), emphasizing their infinite-width limit behavior through the lens of the Neural Tangent Kernel (NTK). The research illuminates the constancy of the NTK in such scenarios, advancing the understanding of neural network learning dynamics. \n\nFurthermore, the paper explores the implications of varying training intensities across GNN layers, enhancing interpretability in a field often perceived as a 'black box.' The authors validate their theoretical assertions with detailed experiments, utilizing public datasets to ensure reproducibility and credibility.\n\nThis paper bridges complex theoretical insights with practical machine-learning applications, contributing substantially to the discourse on neural network training, optimization, and generalization. Its rigorous approach and novel findings mark a significant stride in understanding and leveraging the full potential of GNNs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. By exploring the constancy of the NTK in the infinite-width limit, the study sheds light on complex learning dynamics, enhancing the scientific understanding of how GNNs train and generalize. This rigorous theoretical foundation pushes the boundaries of existing knowledge in neural network behavior.\n2. By dissecting layer-specific training implications, the paper contributes to the interpretability of neural networks, helping researchers to better understand and optimize their GNN models, which is particularly valuable in the 'black box' context of deep learning."
            },
            "weaknesses": {
                "value": "1. While the paper is strong in theoretical analysis, it may not provide extensive insight into the practical applications of the findings. The implications for real-world scenarios, particularly how these theoretical insights into GNNs and NTK behavior could be harnessed for tangible improvements in specific use cases, might not be thoroughly discussed. \n2. If the experiments were conducted within a narrow set of conditions or datasets, they might not fully represent the complexities of real-world data and scenarios. This limitation could raise questions about the generalizability of the findings and their robustness when applied to diverse, practical challenges in the field of machine learning."
            },
            "questions": {
                "value": "1. Given the depth of the research, what do the authors see as the next steps or future directions in this domain? Additionally, are there any inherent limitations or challenges in the proposed methods or findings that might need to be addressed in subsequent research?\n2. Can the analysis be extended to deeper (more than 2 layers) GNNs where the aggregation operation is used in internal layers?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6389/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6389/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6389/Reviewer_bUqG"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6389/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698645442932,
        "cdate": 1698645442932,
        "tmdate": 1699636707930,
        "mdate": 1699636707930,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Cvwo40F2WF",
        "forum": "2gwo9cjOEz",
        "replyto": "2gwo9cjOEz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6389/Reviewer_TKzH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6389/Reviewer_TKzH"
        ],
        "content": {
            "summary": {
                "value": "The authors delve into the theoretical aspects of Neural Tangent Kernels (NTKs) and their influence on the learning and generalization behaviors of over-parameterized neural networks in supervised learning tasks. They introduce the concept of \"alignment\" between the eigenvectors of the NTK kernel and the given data, which appears to play a significant role in governing the rate of convergence of gradient descent and the generalization to unseen data. The paper specifically explores NTKs and alignment in the context of Graph Neural Networks (GNNs). The authors' analysis reveals that optimizing alignment corresponds to optimizing the graph representation or the graph shift operator within a GNN. This investigation leads to the establishment of theoretical guarantees concerning the optimality of certain design choices in GNNs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The paper is well-organized, with a clear delineation of concepts such as NTKs, alignment, and their relevance in the context of GNNs.\n\n* The paper's findings have the potential to advance the understanding and analysis of Graph Neural Networks, providing theoretical insights that could be valuable for the community."
            },
            "weaknesses": {
                "value": "* The paper seems to miss a crucial related work: Huang, W., Li, Y., Du, W., Yin, J., Da Xu, R. Y., Chen, L., & Zhang, M. (2021). \"Towards deepening graph neural networks: A GNTK-based optimization perspective,\" ICLR 2022. Including and discussing this work could provide a more comprehensive perspective and strengthen the literature review section.\n\n* The theoretical framework primarily relies on the existing NTK theory regarding optimization and generalization. While the authors have cited relevant works, a more distinct and innovative theoretical contribution that extends beyond the current NTK theories would enhance the paper's novelty and impact.\n\n* The paper's discussion on alignment seems closely related to node classification and graph classification tasks. However, there appears to be a lack of relevant examinations or experiments to empirically validate the proposed concepts and theories in these tasks, making it difficult to assess their practical relevance and effectiveness.\n\n* The paper could significantly benefit from a more robust and comprehensive experimental section. Ensuring that the experiments thoroughly validate the theoretical findings, involve extensive comparisons with baseline methods, and are evaluated across various datasets and tasks is essential for demonstrating the approach's practical significance and effectiveness.\n\n* A more detailed and thorough comparison with existing NTK and GNN methods is necessary. The paper should highlight the proposed approach's novelty and advantages, supported by theoretical or empirical evidence, to clearly showcase the contributions and distinguish the work from existing literature."
            },
            "questions": {
                "value": "* Could the authors elaborate on how the alignment concept is related to node and graph classification tasks? Are there any practical insights or guidelines on how to effectively apply the proposed theories to these tasks?\n\n* Can the authors highlight the novel aspects of their theoretical framework that go beyond the existing Neural Tangent Kernel (NTK) theories? What are the unique contributions that differentiate this work from existing NTK-based studies?\n\n* Are there plans to include more comprehensive experiments to validate the proposed theories and concepts? What datasets, tasks, and baselines are considered for these experiments?\n\n*Will there be experiments conducted specifically to verify the proposed theorems, such as theorem 2 and theorem 3? If so, could you provide insights into how these verifications will be carried out empirically?\n\n* Why the size of $\\mathbf{y}_i$ is $\\mathbb{R}^{n \\times 1}$, given the size of $\\mathbf{x}_i$ is $\\mathbb{R}^{n \\times 1}$? Is there a specific rationale behind this choice of dimensionality?\n\n* Could you elucidate the rationale behind choosing the HCP-YA dataset for your experiments? How does this dataset align with the objectives and hypotheses of your study, especially considering that common node classification or graph classification tasks are typically used in related works?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Other reasons (please specify below)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The ACKNOWLEDGEMENTS on page 13 might violate the anonymity."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6389/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698754958868,
        "cdate": 1698754958868,
        "tmdate": 1699636707787,
        "mdate": 1699636707787,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PTK1kL4GQ4",
        "forum": "2gwo9cjOEz",
        "replyto": "2gwo9cjOEz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6389/Reviewer_GUVA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6389/Reviewer_GUVA"
        ],
        "content": {
            "summary": {
                "value": "This paper draws on the idea that NTK based generalization is based on the associations of eigenvectors of the NTK with the data. Drawing on this alignment in the case of a GNN is used to derive the graph shift operator (i.e. equivalent of a graph laplacian) that is different from the input graph. To do this they solve an optimization to derive the graph as being the cross-covariance matrix of the input with the output."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Generalizing the NTK to GNNs expands the theory associated with this area, and the suggestion of using a cross covariance matrix involves a graph that uses both input and output variables as nodes, which is not commonly done in current GNNs. The theory could be useful in cases where the graph is not given but constructed as an affinity matrix from data as well."
            },
            "weaknesses": {
                "value": "The key weakness is that empirically VNNs and graphs that are based on covariance matrices rather than non-linear affinities have fared worse in practice. This may be an instance of the NTK not explaining the entire behavior of neural networks. Moreover the experimental validations seem fairy limited without comparison to GCNs and Graphormers and other modern graph neural network architecture."
            },
            "questions": {
                "value": "Is this a case of the theory not explaining the entire empirical phenomenon? Can further experiments be performed on a variety of kernels to see what works better in practice on common datasts/"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6389/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698801981132,
        "cdate": 1698801981132,
        "tmdate": 1699636707654,
        "mdate": 1699636707654,
        "license": "CC BY 4.0",
        "version": 2
    }
]