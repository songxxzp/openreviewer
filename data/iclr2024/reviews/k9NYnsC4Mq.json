[
    {
        "id": "Z9RgAlBa7G",
        "forum": "k9NYnsC4Mq",
        "replyto": "k9NYnsC4Mq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1667/Reviewer_s6fA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1667/Reviewer_s6fA"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes one of the first mechanism to do continual learning with Vision-Language Models (VLM) such as CLIP. Through a system of projectors and a revised definition of context, the authors tested their model, PROOF, on a variety of datasets for continual learning obtaining state-of-the-art performances."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The authors tested for the first time a VLM model for continual learning. \n- The authors tested their PROOF on a variety of datasets testing the effectiveness of the model.\n- The authors proved the effectiveness of the model with very interesting and detailed ablation studies."
            },
            "weaknesses": {
                "value": "- The paper lacks motivation and innovation: The authors suggest using CLIP for class-incremental continual learning, but it would be more interesting to see its performance on tasks like incremental captioning or retrieval. Unlike L2P, where a large pretrained model was used, CIL could have been just one application.\n- Furthermore, the PROOF mechanism, while innovative, lacks depth. Projection networks are common in continual learning, and the new context definition isn't explored.\n- The main paper lacks standard deviation in results and doesn't consider multiple runs with different class orders. \n- There's no analysis of time and memory usage, except for a basic mention of memory requirements in supplementary materials. \n- The paper's narration could also be improved"
            },
            "questions": {
                "value": "- It looks like the supplementary materials are more informative and present more interesting results w.r.t. the main paper. Why did the authors exclude them from the main paper?\n- The definition of W is not reported in the paper. How W is defined in the context?\n- Can the authors provide an analysis of the accuracies of the model varying the number of exemplars?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1667/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1667/Reviewer_s6fA",
                    "ICLR.cc/2024/Conference/Submission1667/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1667/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698658440497,
        "cdate": 1698658440497,
        "tmdate": 1700662948739,
        "mdate": 1700662948739,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3yMp8grbnX",
        "forum": "k9NYnsC4Mq",
        "replyto": "k9NYnsC4Mq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1667/Reviewer_VzW8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1667/Reviewer_VzW8"
        ],
        "content": {
            "summary": {
                "value": "Prior works only focus on the visual branch of CLIP for incremental learning. This paper argues both modalities are important.\n- PROOF freezes the image and text encoders of the pre-trained VLM (e.g. CLIP). These contain the generalizable representations learned during pre-training.\n- For each new incremental task, it adds new projection layers (P_i, P_t) on top of the frozen encoders. These projections are task-specific.\n- When a new task arrives, only the parameters of the projections for the new task are trained. The old projections remain frozen.\n- Cross-modal attention fusion is used to adjust the query embedding using context like prototypes and prompts. This allows utilizing both visual and textual information to obtain comprehensive embeddings.\n- At inference time, the projections are aggregated to obtain a unified classification. But the old projections remain unchanged."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Technically a novel idea to incorporate both the visual and the text encoders. \nImproves upon SOTA."
            },
            "weaknesses": {
                "value": "- Inference Mismatch - Projections are combined at inference time which may not fully match the training conditions for a specific task projection. \n\n- Representation Drift - The post-attention module representations learned by the frozen projections may drift or shift slightly during new task training due to weight updates elsewhere. Small drifts can accumulate.\n\n- Section 3 is really long and has a lot of redundant information, it should be made much shorter. That space should be given to increase the length of section 4 to give a better understanding of the fusion module."
            },
            "questions": {
                "value": "- Any comments on the issues pointed out in the weaknesses will be appreciated.\n\n- Also please make it more clear how you are using attention."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1667/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698789113732,
        "cdate": 1698789113732,
        "tmdate": 1699636094720,
        "mdate": 1699636094720,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aMEheRX2i2",
        "forum": "k9NYnsC4Mq",
        "replyto": "k9NYnsC4Mq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1667/Reviewer_asYX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1667/Reviewer_asYX"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a class-incremental learning (CIL) method based on vision-language models. Specifically, this paper mainly focuses on two key challenges to CIL, named how to adapt the model without forgetting and how to make full use of the multi-modal information. To deal with the first challenge, a task-specific projections are proposed based on the frozen image/text encoders. To deal with the second challenge, a fusion module is proposed for better exploit the cross-modality information. Experiments have shown the state-of-the-art performance of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- In general, the proposed method is well motivated and clearly presented.\n- The paper turns a VLM into a continual learner that is both retentive and comprehensive.\n- Good performance is achieved."
            },
            "weaknesses": {
                "value": "- The effectiveness of alleviating forgetting is uncertain. The process involves incrementally learning image projection heads and text projection heads, which are then combined for various tasks. When new tasks are learned, the projections of previous tasks are fixed and not updated. However, during inference, the projections of all tasks are merged, which might not be ideal for test data from older tasks due to potential side effects caused by the projections from the new tasks.\n- The extent to which contextual information is effective has not been extensively studied. The projection fusion method proposes to contextualize and merge embeddings and contextual information using self-attention. However, in the experiments, only the results of Projection & Fusion are compared with Projection & Fusion & Context Prompt, without explicitly evaluating the effectiveness of the concatenated context information in Q, K, V as [P_i(z), Context] in self-attention, or the effectiveness of the context prompt. In other words, the final context information is defined as Context = [P, W, C], but the specific contributions of W and C to the final results need further analysis.\n- The evaluation metric used may not provide a comprehensive measure of the extent of forgetting."
            },
            "questions": {
                "value": "- To what extent the proposed method could alleviate forgetting?\n- How does each component of the contextual information contribute to the final results?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1667/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1667/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1667/Reviewer_asYX"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1667/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698819925330,
        "cdate": 1698819925330,
        "tmdate": 1700635408475,
        "mdate": 1700635408475,
        "license": "CC BY 4.0",
        "version": 2
    }
]