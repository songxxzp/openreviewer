[
    {
        "id": "KaJUBoveeo",
        "forum": "sOJriBlOFd",
        "replyto": "sOJriBlOFd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3283/Reviewer_WpJV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3283/Reviewer_WpJV"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces NeRM, a generative model for high-framerate human motion synthesis using Implicit Neural Representations (INRs).\nNeRM can handle varied-size data and capture the variational distribution of motions for high-framerate motion synthesis."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper provides a clear and concise description of the problem statement, methodology, and evaluation metrics. The paper addresses the underexplored task of generating realistic human motions with high framerates. By leveraging the advantages of INRs and diffusion models, NeRM offers a memory-friendly and efficient solution for high-framerate motion synthesis."
            },
            "weaknesses": {
                "value": "* The paper could provide more detailed explanations and insights into the limitations and challenges of using Implicit Neural Representations (INRs) for high-framerate motion synthesis. This would help readers understand the potential trade-offs and constraints associated with the proposed approach.\n\n* The paper could benefit from a more extensive discussion on the generalizability of NeRM to different datasets and motion types. It would be valuable to explore the performance of NeRM on diverse motion datasets and evaluate its ability to handle complex and varied motion patterns\n\n* Some more motion synthesis literatures can be included in this paper, such as:\n[a] A unified 3d human motion synthesis model via conditional variational auto-encoder\n[b] Towards diverse and natural scene-aware 3d human motion synthesis."
            },
            "questions": {
                "value": "It would be beneficial to include more detailed explanations and insights into the proposed clip-FID metric for evaluating the quality of high-framerate generative details. How does clip-FID preserve target framerates without downsampling and how does it capture local details and artifacts such as foot sliding?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3283/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698467870699,
        "cdate": 1698467870699,
        "tmdate": 1699636276905,
        "mdate": 1699636276905,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rWxhRXi8AI",
        "forum": "sOJriBlOFd",
        "replyto": "sOJriBlOFd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3283/Reviewer_KnYZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3283/Reviewer_KnYZ"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on the human motion synthesize for frame rate scenarios. It recognizes the limitations of previous efforts of generating high-frame-rate human motion sequences. The key idea is to fuse the data of different framerates into training by normalizing the time positions into relative and centerized time indices with a continuous mapping from time position to the pose configurations. A progressive training is leveraged to relieve the pressure of learning motion patterns under different frame rates by bootstrapping from the fixed frame rate. The generation of motion sequences is done by a conditional diffusion model where the latent code is the motion code from encoders and a codebook-based attention module."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The proposed method can accept training data in arbitrary frame rate while supporting human motion generation in high frame rate.\n- The proposed method is flexible to support human motion generation of different schemes, such as unconditional, or conditional to different constraints, such as action label or text descriptions.\n- Leveraging the latent code for diffusion model and integrating the codebook-based attention module, the method is designed in a whole to support different modalities for conditions and good expressiveness."
            },
            "weaknesses": {
                "value": "I put both my recognized weaknesses and the questions I would like authors\u2019 response to here. \n\n- Question: Eq 2 is confusing to me. Normalizing the clip length from the standard seconds to the relative length has been commonly adopted. In Eq 2, given both the relative time position and the frame rate, the input information to the generation function f_\\theta is exactly the same as what previous works input. Could the authors elaborate more about their differences here?\n- Question: with different frame rates for the target motion sequence, is the number of time steps in diffusion, i.e. k in Eq 5, the same? I understand that they are two different \u201ctime steps\u201d but I still would like to get a sense that whether the diffusion model is able to capture the motion of different frame rates or it is only to recover the pose in a single time step, static.\n- In my understanding the claim that \u201cNeRM can generate motions with arbitrary framerates s and durations l by setting appropriate temporal coordinates.\u201d may be inaccurate. Technically, by the design of the method, the claim can be partially correct. But given the training data captured under certain frame rates, without considering the motion speed and sensitiveness of capture sensor etc, the model is unlikely to learn motion patterns in frame rates exceeding the highest frame rate contained in the training data.\n- Question: is there any reason that a close baseline Nemf (He et al 2022) is not included in the benchmarking comparisons?\n- The method is constructed with multiple components and requires settings of many introduced hyper-parameters, such as the normalizing of time positions, the codebook-coordinate attention and etc. The authors may need to provide corresponding ablation studies to support the effectiveness of these modules and the help readers understand the resources of performance gains more clearly."
            },
            "questions": {
                "value": "Please see my questions above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3283/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698582035341,
        "cdate": 1698582035341,
        "tmdate": 1699636276834,
        "mdate": 1699636276834,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jjdQx27eSh",
        "forum": "sOJriBlOFd",
        "replyto": "sOJriBlOFd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3283/Reviewer_9SXt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3283/Reviewer_9SXt"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a neural representation, i.e., NeRM, for representing continuous human motions. NeRM directly learns a continuous motion field over temporal coordinates without explicit modeling, making the training with varied-framerate motions and high-framerate motion generation possible. The authors leverage the proposed representation to (un-)conditional motion generation with diffusion models, showing the efficiency and effectiveness of high-quality motion generation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The primary contribution of this work is the proposed neural continuous motion representation, NeRM, which I find both interesting and innovative. It effectively represents continuous motion sequences at any framerate level. This representation addresses limitations observed in previous works, such as MLD, which necessitates the input motion sequence to have the same framerate and fails to capture high-frequency details. The experiments conducted on various conditional and unconditional motion generation tasks are thorough and robust. The paper is well-organized and clearly presented."
            },
            "weaknesses": {
                "value": "My main concern revolves around the motivation of representing and directly generating high-framerate human motions. As discussed in the introduction, the authors present two key points: (1) high-framerate motion generation is inefficient, and (2) training with a fixed framerate cannot adequately utilize the dataset. However, I believe that training with fixed, low-framerate data might suffice to produce high-framerate results through interpolation. Hence, there might be no imperative need to use per-frame human poses during training.\nThe related discussion in the introduction appears to be not highly convincing."
            },
            "questions": {
                "value": "1. How can we determine if the feet sliding shown in Figure 4 is a result of training with a low framerate?\n\n2. Does the use of variational INR, i.e., normalizing the latent code $\\mathbf{z}_i$ to a normal distribution with KL loss, affect the preservation of high-framerate details? Such normalization typically leads to a smoother representation space?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3283/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3283/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3283/Reviewer_9SXt"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3283/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698672731381,
        "cdate": 1698672731381,
        "tmdate": 1699636276761,
        "mdate": 1699636276761,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1vbhyMyqFR",
        "forum": "sOJriBlOFd",
        "replyto": "sOJriBlOFd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3283/Reviewer_VxZF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3283/Reviewer_VxZF"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the challenge of generating realistic human motions at high framerates, a task made difficult by inconsistent training data framerates, memory constraints, and the slow performance of generative models. Current solutions downsample high-framerate details or discard low-framerate samples, leading to information loss. The authors propose NeRM, a generative model utilizing Implicit Neural Representations (INRs) to harness varied-size data for high-framerate motion synthesis without explicitly modeling raw motions. NeRM not only outperforms other methods but also efficiently produces motions at any desired framerate while remaining memory-efficient."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper proposes to use a novel variational INR to generate arbitrary framerate motion. The experimental results support this claim that NeRM outperforms other methods in different framerate generation. \n\n2. It enables to generalize INR to the new data without retraining by introducing the latent code to the INR input. \n\n3. The presentation is overall clear. The core idea as well as the technical details are well presented and easy to follow. \n\n4. The performance outperforms other baselines."
            },
            "weaknesses": {
                "value": "1.  As for the generation part, the part of the input for INR is z, it seems that the model heavily depends on the quality of the latent representation z. \n\n2. The idea of using time INR has been use in NeMF: Neural Motion Fields for Kinematic Animation, which may limit the novelty contribution of this paper."
            },
            "questions": {
                "value": "1. Two-stage training gives good performance. It would be good to give more details on training the auto-encoding of the latent code.\n\n2. In order to avoid retraining of INR, some other methods choose to use latent code to modulate the weights of INR. Is there any insight for the choice in the paper? \n\n3. How do you determine the number of codes in the codebook?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3283/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3283/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3283/Reviewer_VxZF"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3283/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698812366470,
        "cdate": 1698812366470,
        "tmdate": 1699636276671,
        "mdate": 1699636276671,
        "license": "CC BY 4.0",
        "version": 2
    }
]