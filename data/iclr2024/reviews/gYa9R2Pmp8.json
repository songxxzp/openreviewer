[
    {
        "id": "W09XM46mtN",
        "forum": "gYa9R2Pmp8",
        "replyto": "gYa9R2Pmp8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2149/Reviewer_quGs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2149/Reviewer_quGs"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new method to generate attack prompts for LLMs. The general idea is to have several stages of generation, where an unsafe category is combined with a persona to generate the attack prompts. The paper found that the method can lead to high attack success rates against several LLMs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. the presented method is intuitive and clearly presented.\n2. the method is tested on several different models to show its effectiveness\n3. the method is also simple to implement, and it addresses an important problem."
            },
            "weaknesses": {
                "value": "1. the biggest weakness is the evaluation method used in the paper. The authors claim that they use an LLM to evaluate the attack success rate through few-shot prompting. However, it's hard to trust such an automatic evaluation method. The authors should also conduct human evaluation for the method."
            },
            "questions": {
                "value": "Have you conducted human evaluation? It shouldn't be too hard since everything is in English."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2149/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698418392237,
        "cdate": 1698418392237,
        "tmdate": 1699636147931,
        "mdate": 1699636147931,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pJ5SkWICAz",
        "forum": "gYa9R2Pmp8",
        "replyto": "gYa9R2Pmp8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2149/Reviewer_2ytU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2149/Reviewer_2ytU"
        ],
        "content": {
            "summary": {
                "value": "This work explores jailbreaking LLMs by adding particular persona-related system prompt. Particularly, they design an automated (or semi-automated) way to search the system prompt based on the attacking field. The empirical results show that personalized system prompt could effectively stimulate the LLMs to generate harmful outputs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposal to induce an assertive persona within a Large Language Model (LLM) is well-founded. Consequently, this paper merits increased attention concerning the use of system prompts in real-world applications.\n\n2. The empirical results regarding Persona-modulated Human Response (HR) highlight the vulnerabilities associated with modifying system prompts, which currently remain accessible without restrictions."
            },
            "weaknesses": {
                "value": "1. It is advisable to incorporate additional baseline comparisons within the empirical results section. For instance, it would be valuable to assess the performance when transitioning from a persona-modulated prompt within the system prompt to one within the user prompt.\n\n2. The methodological exposition, particularly in relation to the automated procedures, would benefit from greater elaboration. For instance, it would be helpful to provide specifics on the prompting template utilized for the automatic generation of persona-modulated prompts by GPT-4.\n\n3. The outcomes stemming from the semi-automated red-teaming pipeline warrant quantification. This quantification is essential for a comprehensive evaluation of the trade-off between efficacy and efficiency in the context of the study."
            },
            "questions": {
                "value": "1. what is the performance of moving the persona-modulated prompt from system prompt to user prompt?\n2. what is the prompting template for GPT4 to automatically generate the persona-modulated prompts\uff1f\n3. What are the quantified results of semi-automated approach. Are there any illustration about the trade-off between its effectiveness and efficiency?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Potentially harmful insights, methodologies and applications"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The paper is about jailbreaking LLMs, which may raise ethics concerns about the safety of LLMs."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2149/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698462491737,
        "cdate": 1698462491737,
        "tmdate": 1699636147853,
        "mdate": 1699636147853,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "V6umIUEwYx",
        "forum": "gYa9R2Pmp8",
        "replyto": "gYa9R2Pmp8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2149/Reviewer_pyxV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2149/Reviewer_pyxV"
        ],
        "content": {
            "summary": {
                "value": "This proposes to design a persona modulation as a black-box jailbreak to make the LLMs to take on specific personas. Then it can comply with the harmful instructions and generate responses for harmful topics."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "This paper proposes a persona modulation to enable LLMs to follow harmful instructions and generate corresponding responses."
            },
            "weaknesses": {
                "value": "1. **No novelty:** It seems this paper only designs a persona prompt. I don't see any novelty at all.\n\n2. **Experimental setting has some issues:** \"The authors of the paper manually labeled 300 random completions.\" Only authors of this paper annotated the completions. It might have some bias. Besides, baseline only has the one without prompt, which seems not enough."
            },
            "questions": {
                "value": "1. What about steering the system to be not only one personas? How about two personas in consistent or contradictory status?\n\n2. It seems only one third of the categories achieved a harmful completion rate over 50%. Does that mean the proposed approach is not generalizable enough?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The paper proposed a method to tell LLMs to follow harmful instructions, and the corresponding outputs may be harmful!"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2149/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698789042941,
        "cdate": 1698789042941,
        "tmdate": 1699636147778,
        "mdate": 1699636147778,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "laM8n2Runf",
        "forum": "gYa9R2Pmp8",
        "replyto": "gYa9R2Pmp8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2149/Reviewer_DNY8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2149/Reviewer_DNY8"
        ],
        "content": {
            "summary": {
                "value": "The paper investigates persona modulation as a method to jailbreak language models at scale. It shows persona modulation can automate the creation of prompts that steer models to assume harmful personas, increasing harmful completions. This jailbreak transfers between models and is semi-automated to maximize harm while reducing manual effort."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper presents a novel and thorough study of persona modulation to jailbreak LLMs\n- Evaluating the attack against many different harm categories and several state-of-the-art models demonstrates the approach can generalize to different scenarios\n- The methodology is clearly explained and the experiments are well-designed overall."
            },
            "weaknesses": {
                "value": "- Although the attack achieves a very high one-off attack rate, the attack pattern remains fixed and obvious, which can potentially be identified by some adversarial classifier. It is unclear whether the proposed attack would still be effective if the target model is fine-tuned on the attack prompt with safe answers provided.\n- Factors that make some personas/prompts more effective than others are not analyzed.\n- Concrete mitigation strategies are not discussed in detail."
            },
            "questions": {
                "value": "Was any analysis done on what factors make some personas and prompts more effective than others in the automated workflow?\nWhat are some ways model providers could mitigate this attack specifically going forward? If some kind of mitigation is performed, can you estimate how the attack rate will change?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2149/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698820330816,
        "cdate": 1698820330816,
        "tmdate": 1699636147697,
        "mdate": 1699636147697,
        "license": "CC BY 4.0",
        "version": 2
    }
]