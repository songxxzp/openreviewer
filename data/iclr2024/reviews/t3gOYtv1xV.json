[
    {
        "id": "L2cUbIx9KC",
        "forum": "t3gOYtv1xV",
        "replyto": "t3gOYtv1xV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4064/Reviewer_RdPh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4064/Reviewer_RdPh"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors study whether simple Transformer architectures might learn a carrying over algorithm to solve arithmetic tasks. To this aim, they train and test two-layer Transformers on a 3-digit addition task, and show that the emerging attention patterns allow the model to solve the task in a modular fashion, by dedicating some resources to the simpler sub-task of digit summation and other resources to the task of deciding whether carrying over is required, and in such case to actually perform carrying over. The authors argue that their findings could extend to Large Language Models, such as Alpaca 7B, and provide \u201csuggestive evidence\u201d in support of this claim."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The article is overall well-written and well-organized.\n- Understanding the inner working of Transformer architectures is certainly an important and timely research topic, especially for tasks related to \u201csymbolic reasoning\u201d, where often these models fall short. The ICLR community is particularly interested in such topic.\n- Focusing on simple architectures and simple symbolic domains, such as arithmetic, allows to get useful insights that might then be extended to more complex architectures and tasks.\n- The authors share source code (though I quickly checked it, and it does not seem to be properly commented / explained). This should guarantee reproducibility of results, but I encourage the authors to improve the code structure and provide more complete documentation."
            },
            "weaknesses": {
                "value": "- Many related works have been omitted.\n- While I agree that digit arithmetics constitutes a valid testbed to build mechanistic accounts of the inner workings of deep learning models, the current task setting seems oversimplified. To establish the emergence of a putative carrying over algorithm, the authors should demonstrate that such algorithm allows to systematically solve the target problem, for example by testing the model over problem instances involving longer digits and more operands than those observed during training.\n- The authors frame their research as an investigation of mathematical reasoning abilities of LLMs, arguing that a fruitful approach is to study simplified architectures and then extrapolate the results to larger models. While I agree that this might be a reasonable approach, the authors provide only \u201csuggestive evidence\u201d that their findings extend to one LLM (Alpaca). I think it would be more effective to directly frame this contribution as an investigation of the computational capabilities of simple Transformers architectures, and mention as a side point their putative application to LLMs.\n- There are a few methodological details than require clarification (see questions below).\n- English phrasing and grammar could be improved."
            },
            "questions": {
                "value": "- The \u201cRelated works\u201d section only mentions a few studies related to the emergence of grokking in Transformer architectures. This section should be extended by including relevant papers that have investigated the capability of Transformer architectures to learn arithmetic problems. For example, others have proposed how to improve the standard Transformer architecture to improve generalization on simple arithmetic tasks [1], possibly by exploiting grid-like problem representations that promote the emergence of carrying over algorithms [2]. More recent work has also studied how systematic length generalization in basic integer arithmetic might be improved using relative position embeddings [3].\n- To demonstrate that Transformers are learning an algorithm, their behavior should be tested also with out-of-distribution problem instances (as usually done in the related literature, see papers mentioned above). This would include longer operands than those used during training (> 3-digits) but also more operands than those used during training (> 2, in this case).\n- A related point is the issue of memorization. The authors should better explain why they believe that one-layer models are memorizing the solutions, while two-layer models are actually learning a systematic behavior.\n- The authors argue that \u201cUsually the carrying over algorithm is employed in a right to left fashion\u201d. This is the \u201chuman way\u201d to implement the carry over algorithm, which might be due to processing constraints (we prefer to process things sequentially when the problem requires deliberate reasoning). This might not apply to Transformers, which should learn to parallelize the computation whenever possible (also see [2]) In line with this observation, it would be interesting to better investigate model behavior on the \u201cNon-carry\u201d (NC) scenario (subset #1). In such case, I would expect the Transformer to execute the entire operations in parallel.\n- What is the rationale for using three = at the end of the arithmetic operations, rather than just one?\n- \u201cThe attention patterns are averaged over the test dataset\u201d: shouldn\u2019t the authors average only similar cases, to avoid mixing carry vs. non-carry cases (or even according to subsets identified by the authors at pg. 3)?\n- Phase transitions: from Fig. 2 it is not evident that phase transitions are actually occurring. The figure only shows that at some point the attention pattern changes, but in order to be a phase transition such change should happen suddenly (e.g., between two consecutive epochs). Furthermore, following the authors\u2019 narrative this should happen not just during two consecutive epochs, but during the epochs corresponding to the abrupt changes in the loss function. From the figure, this does not seem to be the case. Can the authors clarify this?\n- In the figures, the curves representing trends of just one individual run should be replaced with curves representing mean and variance (or min-max) of the six multiple runs.\n- Extending the present findings to LLMs is not easy, though the anecdotical evidence provided by the authors suggests that there might be some commonalities. Since a major limitation of Alpaca under the setup investigated by the authors was the overall low accuracy, it would be interesting to study whether prompting techniques could improve model performance, and maybe promote the emergence of a more interpretable algorithmic strategy. Also testing other LLMs might allow to establish whether more accurate models would use more systematic strategies at inference time.\n\n\nReferences\n1.  Csord\u00e1s R, Irie K, Schmidhuber J. The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization. ICLR, 2022. \n2.  Cognolato S, Testolin A. Transformers discover an elementary calculation system exploiting local attention and grid-like problem representation. IJCNN, 2022\n3. Jelassi S, D\u2019Ascoli S, Domingo-Enrich C, Wu Y, Li Y, Charton F. Length Generalization in Arithmetic Transformers. http://arxiv.org/abs/2306.15400, 2023"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4064/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4064/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4064/Reviewer_RdPh"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4064/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698071270641,
        "cdate": 1698071270641,
        "tmdate": 1700671256867,
        "mdate": 1700671256867,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3uwlOON0S7",
        "forum": "t3gOYtv1xV",
        "replyto": "t3gOYtv1xV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4064/Reviewer_5YTw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4064/Reviewer_5YTw"
        ],
        "content": {
            "summary": {
                "value": "In this work, the authors study how transformer models implement carrying over algorithm. They first focus on one layer and two-layer encoder-only models and show that the carrying over algorithm is implemented in a modular fashion, then apply the learned lessons to a large language model."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed method is technically sound.\n1. The empirical results look promising.\n2. The paper is overall well presented."
            },
            "weaknesses": {
                "value": "1. Except for Alpaca 7B, the authors should provide more discussion for other transformer models in LLM and other modalities tasks.\n2. The innovation, is limited as the main ideas have been circulated outside carrying over algorithm in Transformers. overall, beyond limited novelty, the paper has no major weaknesses. The limitation comes from the approached theme, while being promising, has yet to make an impact outside the research community. This means also, that the auditorium is limited."
            },
            "questions": {
                "value": "1. It would be better for authors to provide a more detailed discussion on large-digit addition (e.g. 5-dight, 6-digit, etc), since it can be a more complex task and may have different situations with 3-digit addition.\n2. In this work, the authors analyze the attention pattern for 2 heads in each layer for one-layer and two-layer encoder/decoder-only models. It would be interesting to consider more heads for each layer in these small models."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4064/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698453052348,
        "cdate": 1698453052348,
        "tmdate": 1699636370528,
        "mdate": 1699636370528,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4JR8s0krnm",
        "forum": "t3gOYtv1xV",
        "replyto": "t3gOYtv1xV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4064/Reviewer_tJZK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4064/Reviewer_tJZK"
        ],
        "content": {
            "summary": {
                "value": "The objective of this work is to understand the nature of arithmetic operations as performed by transformers.  Specifically, the paper focusses on 3-digit addition with carryover as the use case.   The main results are in the nature of attention patterns and MLP weights that evolve as the transformer is trained."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper raises an interesting question of how the attention of a transformer and the MLP weights evolve to correspond to its performing arithmetic.   The paper is clearly written in general."
            },
            "weaknesses": {
                "value": "At some level, the paper's contribution is not in the transformer training itself, which follows on standard lines, but is on interpreting the outcome in terms of structure of attention and the MLP layers.   My general opinion is that, while interesting, this is not a sufficient contribution in itself.  For instance, the paper does not offer any major prescriptions on how best to train a transformer for enabling it to do arithmetic more accurately, nor does it provide an analysis of why the condition distribution generated by a transformer has the correct structure to perform carry over arithmetic."
            },
            "questions": {
                "value": "I do not have any technical questions.  However, I am concerned at the level of novel contribution of this paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4064/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698813014456,
        "cdate": 1698813014456,
        "tmdate": 1699636370455,
        "mdate": 1699636370455,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YZC90DMbp6",
        "forum": "t3gOYtv1xV",
        "replyto": "t3gOYtv1xV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4064/Reviewer_rE6h"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4064/Reviewer_rE6h"
        ],
        "content": {
            "summary": {
                "value": "1. Authors proposed a transformer models that implements carrying over algorithm on one,two and three layer model."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1.The novelity of the paper is excellent.\n2.Sufficient results are presented and their analysis is carried out."
            },
            "weaknesses": {
                "value": "No"
            },
            "questions": {
                "value": "1.Results of the figure 13 need to be cross verified."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "10: strong accept, should be highlighted at the conference"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4064/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4064/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4064/Reviewer_rE6h"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4064/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699069788031,
        "cdate": 1699069788031,
        "tmdate": 1699636370397,
        "mdate": 1699636370397,
        "license": "CC BY 4.0",
        "version": 2
    }
]