[
    {
        "id": "l5TSEWNGG9",
        "forum": "crMMk4I8Wy",
        "replyto": "crMMk4I8Wy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3788/Reviewer_Yv5K"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3788/Reviewer_Yv5K"
        ],
        "content": {
            "summary": {
                "value": "This paper present AucArena, an open ascending auction protocol for LLM-based bidding agent. The auction protocol specifies the value of items and communication process. The LLM agent is assumed to follow a certain architecture, which consist of (1) building belief, e.g., the sufficient statistics like remaining budget, profits (2) Desire, the objective that a bidder tries to optimize and (3) planning, the bidder's strategy to fulfill its goal. These components structuralized an LLM agent's decision-making process, and is purely generated by LLM itself. The evaluation studies the correlations between the agents' actual bidding behaviors, and the output of each of these components."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The presentation of the paper is good. The usage of LLMs as bidding agents are novel and promising."
            },
            "weaknesses": {
                "value": "Some of the settings and experimental results are not clear. Please see the question section."
            },
            "questions": {
                "value": "I like the setting of using auctions as benchmark to evaluate LLMs. However, there are a few questions I have in mind:\n\n1. About the auction protocol. Section 3.1 wrote the true value of a particular type of item is the same for all players, but the players cannot observe. This is very confusing to me. In a standard auction setting a player will have a private valuation, or some prior distribution of the ground truth value. The distributions could be correlated among different players, but I cannot understand the setup of not having any information of the ground truth values at all. Also, can the bidder observe its own utility every round? If it does then it can infer the ground-truth utility right? Can the author please clarify what is the observation space for a bidder each round.\n\n2. In section 3.3. Also I believe winner's curse happens when the players' beliefs of valuations are positively correlated.\n\n3. In the adaptivity experiment, what is the learning rule mainly about? And why not incorporate this learning rule in previous experiments?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3788/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3788/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3788/Reviewer_Yv5K"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3788/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698184120407,
        "cdate": 1698184120407,
        "tmdate": 1699636335742,
        "mdate": 1699636335742,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "X3RKfPaaZu",
        "forum": "crMMk4I8Wy",
        "replyto": "crMMk4I8Wy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3788/Reviewer_TdM7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3788/Reviewer_TdM7"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces AucArena, a simulated auction environment for language model agents, and conducts comprehensive benchmarks among PaLM-2, Claude-1.2, Claude-2, GPT-3.5, and GPT-4.\nThe authors test language model agents based on Belief-Desire-Intention (BDI) Model framework, where the agents are expected to keep the state of auctions (remaining budget, total profit, winning bids of all items) in mind, to follow the given instructions, and to replan the future bids dynamically. While language model agents can follow the given instructions during the auction (budget, priorities, objective, etc.), human players or even rule-based agents still work as competitive baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "### quality and clarity\n- This paper is clearly written and easy to follow.\n\n ### originality and significance\n- The auction might be a unique setting for language model agents.\n- In contrast to Reflexion [1],  tested on ALFWorld (a housing simulator) and HotpotQA (Wikipedia-based QA), the self-improvement loop from the previous experience is not observed (Section 3.3), which might be an interesting finding.\n\n[1] https://arxiv.org/abs/2303.11366"
            },
            "weaknesses": {
                "value": "- While auction might be a unique setting, it is unclear what aspects of language model agents would be evaluated through this benchmark.\n- (Re)Planning [1,2], Instruction-following [3], and belief (internal state of the agents) [4,5] is a typical design for language model agents. It does not seem as a novel proposal or evaluation of language model agents.\n- The authors mentioned AgentBench in Section 2 as `... are limited in terms of evaluating LLMs in dynamic environments.`, but in fact, they have static offline task planing evaluation (Mind2Web).\n- I'm not sure if the variety of items is enough (\\\\$2000 and \\\\$10000). Some justification would be helpful.\n- The descriptions of AucArena in Section 3.1 `For simplicity, we\ndo not introduce additional values such as various personal preferences and emotional value of items.` and the descriptions of BDI model in Section 3.2 `... yet some might\nalso have non-monetary motivations, such as a personal desire to own a specific item.` seem contradictory.\n\n\n[1] https://arxiv.org/abs/2303.17491\n\n[2] https://arxiv.org/abs/2305.16653\n\n[3] https://arxiv.org/abs/2207.01206\n\n[4] https://arxiv.org/abs/2210.03629\n\n[5] https://arxiv.org/abs/2207.05608"
            },
            "questions": {
                "value": "- How many participants do you have in ActArena? One LLM-based agent you are focusing on, and others are rule-based agents?\n- While GPT-4 achieves the lowest Corrected Failure Rate in Figure 2, the average profit of Claude-1.2/2 with adaptive planning is higher than GPT-4 in Figure 3 (GPT-3.5 does not seem a bad bidder here). Why does this happen?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3788/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698599526005,
        "cdate": 1698599526005,
        "tmdate": 1699636335651,
        "mdate": 1699636335651,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZghGi093FV",
        "forum": "crMMk4I8Wy",
        "replyto": "crMMk4I8Wy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3788/Reviewer_eqke"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3788/Reviewer_eqke"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes AucArena, a simulated auction environment for evaluating the strategic planning and execution abilities of LLMs. The idea of using auctions to assess skills like strategic reasoning, execution, and adaptivity in a multi-agent environment is interesting. With AucArena, the paper also proposes a new LM auction agent: LLMs act as bidders using prompting for actions like planning/ replanning, bidding, and belief updating. Experiments analyze LLM in different tasks in different variations of the environment that test rationality, adaptivity, and competition (with some asymmetry)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The auction simulation is a creative testbed for evaluating LLMs in a dynamic, competitive setting requiring strategic talents.\n- The experiments thoroughly probe diverse LLM models in the proposed arena. Testing multiple LLMs provides useful insights into their relative proficiencies.\n- Assessing strategic abilities of LLMs is an important open problem and this work makes a valuable first step. The arena concept has promise if further developed."
            },
            "weaknesses": {
                "value": "- The introduction and problem framing focus heavily on general capabilities, detached from the specific auction setting and contributions. Centering the intro around auctions would improve coherence. \n- I would have loved to see a motivation of specifically why agents competing in auctions should be studied where lying and deception could be emergent. Why are auctions special? And not other cooperative settings? A discussion on this is improtant.\n- The auction design lacks complexity with only basic factors like item values and bidder budgets. Expanding to more realistic auctions could better evaluate strategic skills. Authors provide suggestions in Appendix C, which if included would greatly improve the paper.\n- Prompting details and ablations could provide more insights: Why is the specific prompt design superior to others? Ablations of the prompt with comparisons to other prompts and agents would make the work stronger.\n- No discussion on the advantages of an LLM agent over a specialized strategic agent. The choice of LLMs as the agent model (over a symbolic RL agent for example) needs justification.\n- Missing Citations: Strategic Reasoning with Language Models [1] seems closely related to the method in the paper with planning, replanning, belief tracking and value estimation of other agents. Other works relating to factoring [2], llm cascades [3], need to be cited and discussed.\n- The comparison to human play provides useful context, but the protocol for human evaluation seems ad hoc. Standardizing the process and using skilled human players as a benchmark could make this comparison more rigorous. Increasing the number of human participants could be a good place to start.\n- The method was not immediately clear, parts of the instructions could be included in the main paper to make things clearer.\n- The evaluation metrics seemed narrow, focusing on the outcomes. It would be great if the authors could translate some of the qualitative measures about strategy into objective numbers.\n- Adding an open source model to the benchmark would be interesting!\n\n[1] Gandhi, K., Sadigh, D., & Goodman, N. D. (2023). Strategic Reasoning with Language Models. arXiv preprint arXiv:2305.19165.\n\n[2] A. Stuhlm\u00fcller and J. Reppert and L. Stebbing (2022). Factored Cognition Primer. Retrieved December 6, 2022 from https://primer.ought.org.\n\n[3] Dohan, David, et al. \"Language model cascades.\" arXiv preprint arXiv:2207.10342 (2022)."
            },
            "questions": {
                "value": "### Suggestions\nSpecified in detail in weaknesses, summarized here:\n- Improve the introduction and motivation, being specific to auctions and using LLMs with auctions. \n- Enrich the auction design with more realism and complexity as outlined in the paper (App C).\n- Ablate prompting methodology, compare to stronger baselines.\n- Increase and formalize human evaluations. Compare to skilled players.\n- Discuss how the approach builds on related work in strategic, factored LLMs."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3788/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3788/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3788/Reviewer_eqke"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3788/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698736089088,
        "cdate": 1698736089088,
        "tmdate": 1699672564245,
        "mdate": 1699672564245,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Xg4ryT4nvt",
        "forum": "crMMk4I8Wy",
        "replyto": "crMMk4I8Wy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3788/Reviewer_PeRU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3788/Reviewer_PeRU"
        ],
        "content": {
            "summary": {
                "value": "This paper studies an interesting problem about the LLM's ability in auction, which can test whether the LLM can obtain high reward. The authors designed some prompts, and the results show LLMs are still limited."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The studied problem is quite interesting.\n2. The paper is written well.\n3. The appendix provides enough details for the prompts."
            },
            "weaknesses": {
                "value": "1. The technical contribution is limited. The fundamental mechanism for the LLM agent is quite simple.\n2. The results are not so promising. There is no sufficient comparison among different LLMs.\n3. The setting of the auction is quite simple. I suggest the authors test different settings (those standard ones), and try to provide more insightul conclusions, compared with human."
            },
            "questions": {
                "value": "Please address my concerns in Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "There is no ethics concern."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3788/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3788/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3788/Reviewer_PeRU"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3788/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699640529591,
        "cdate": 1699640529591,
        "tmdate": 1699640529591,
        "mdate": 1699640529591,
        "license": "CC BY 4.0",
        "version": 2
    }
]