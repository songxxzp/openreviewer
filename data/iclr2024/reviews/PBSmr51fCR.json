[
    {
        "id": "PXYrht8nRz",
        "forum": "PBSmr51fCR",
        "replyto": "PBSmr51fCR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5516/Reviewer_xehP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5516/Reviewer_xehP"
        ],
        "content": {
            "summary": {
                "value": "This paper learns a unified embedding that is robust to view missing conditions by integrating information from multiple views and neighboring samples. Firstly, to overcome the limitations of cross-view contrastive learning, URRL-IMVC incorporates an attention-based auto-encoder framework to fuse multi-view information and generate unified embeddings. Secondly, URRL-IMVC directly enhances the robustness of the unified embedding against view-missing conditions through KNN imputation and data augmentation techniques, eliminating the need for explicit missing view recovery. Finally, incremental improvements are introduced to further enhance the overall performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The originality, quality, and significance of this paper are supported by the proposed unified representation learning framework that efficiently fuses both multiview and neighborhood information, allowing for better capturing of consensus and complementary information.\n\n2. The clarity of this paper is clear based on the framework figure and the corresponding illustrations."
            },
            "weaknesses": {
                "value": "1. The biggest problem of this paper is the limited novelty in formulation of URRL-IMVC\uff0cwhich learns a unified embedding that captures the comprehensive representation. The differences between URRL-IMVC and the closely related works can be analyzed from different aspects.\n\n2. The strategies including KNN imputation and data augmentation should be stated in details. Then the process of directly learning a robust representation capable of handling view-missing conditions without explicit missing view recovery is easily understood by the readers.\n\n3. In the experiments, the compared methods are not enough and the more datasets can be added, i.e., Table 2.\n\n4. The convergence analysis can be added in the experiment, which can be adopted to better the loss function."
            },
            "questions": {
                "value": "Why the visualization for 4400 iteration is not significantly improved compared with 2400 iteration in Figure 4 for the experiment?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5516/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5516/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5516/Reviewer_xehP"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5516/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697808552532,
        "cdate": 1697808552532,
        "tmdate": 1699636565291,
        "mdate": 1699636565291,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UDBnbQQG3y",
        "forum": "PBSmr51fCR",
        "replyto": "PBSmr51fCR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5516/Reviewer_LBwN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5516/Reviewer_LBwN"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces URRL-IMVC, an incomplete multi-view clustering method that does not rely on cross-view contrastive learning or missing view recovery. Instead, it leverages the complementarity of information across views by fusing data from two carefully designed encoders. It also eliminates explicit missing view recovery by employing KNN imputation and data augmentation techniques."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The organization of this paper is clear and the motivation is easy to understand.\n\n* Experimental results support the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "* This paper asserts that cross-view contrastive learning may overlook complementary information, and contrasting the unified embedding has the potential to capture a more comprehensive representation. However, there is a lack of both theoretical and experimental evidence to support these claims from either perspective.\n\n* More experiments need to be added to verify the sensitivity of model parameters, such as the setting of k in KNN and the initialization of cluster centers in clustering module.\n\n* The ablation studies on modules are rough. The effectiveness of incremental improvements on each module should be further investigated. \n\n* The related work Section appears to be somewhat concise. Some recent works should be discussed."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5516/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698672735513,
        "cdate": 1698672735513,
        "tmdate": 1699636565192,
        "mdate": 1699636565192,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Nc1QMzRAk6",
        "forum": "PBSmr51fCR",
        "replyto": "PBSmr51fCR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5516/Reviewer_umVo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5516/Reviewer_umVo"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel Unified and Robust Representation Learning for Incomplete Multi-View Clustering, which tries to learn a unified embedding that is robust to view missing conditions by integrating information from multiple views and neighboring samples. The method proposed in this paper is without explicit missing view recovery procedure, which is orthogonal to existing missing view recovery-based methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper first provides the idea to simultaneously consider the multi-view information fusion and neighborhood information incorporation for clustering under the view-missing conditions.\n2. To my knowledge, using an attention-based auto-encoder framework to fuse multi-view information is somewhat novel in multi-view learning.\n3. The experimental results are significantly better than former methods."
            },
            "weaknesses": {
                "value": "1. No evidence for the analysis on the computation cost and unreliability of explicit missing view recovery.\n2. The writing of the methodology is not compact and unclear: Some related contents are far away from each other, for example, Eq. (5) and its details.\n3. Some essential details are missing, for example, the calculation of KL divergence in Eq. (23) is too abstract to follow.\n4. Most of the formulations are postponed to the appendix, making the main text hard to be understood and undermining the clarity. \n5. No solid theoretical guarantee is provided for the proposed method."
            },
            "questions": {
                "value": "1. What is the first output of NDE module in the output choice of the proposed NDE?\n2. How does the Siamese Encoder work in Figure 2? Do you mean the architectures of the upper and lower parts are identical with shared parameters?\n3. How to determine the level of noise added to the original incomplete multi-view data?\n4. Do you consider the private information in each view? In addition to the consensus information, the complementary information is also essential, which has been indicated in this paper. However, I cannot understand what is the mechanism used to leverage the view-specific information in the proposed method. Please clarify this in details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5516/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5516/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5516/Reviewer_umVo"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5516/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698684387256,
        "cdate": 1698684387256,
        "tmdate": 1699636565094,
        "mdate": 1699636565094,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IhVOf0OIcE",
        "forum": "PBSmr51fCR",
        "replyto": "PBSmr51fCR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5516/Reviewer_3U96"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5516/Reviewer_3U96"
        ],
        "content": {
            "summary": {
                "value": "In this paper, a deep representation learning network is proposed for incomplete multi-view clustering. The method exploits the KNN imputation approach to fill the missing views and integrates the augmentation strategy. Many experiments, especially many ablation experiments are conducted to validate the method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors conducted many ablation experiments to validate the methods."
            },
            "weaknesses": {
                "value": "1. The experiments are not sufficient. Firstly, there are no experiments on large-scale datasets. Secondly, the authors only evaluate the method on the image datasets, where all views are extracted from the image. \n2. Efficiency and computational complexity are also the very important metric to evaluate the method. However, these are ignored.\n3. The novelty of the method seems not strong but the method seems very complex. Imputation the missing views for incomplete multi-view clustering is not new and has many related works. For example, the work \u2018Deep safe incomplete multi-view clustering: Theorem and algorithm\u2019 also exploits the KNN imputation for missing views. The method used Augmentation and KNN imputation to fill the missing views. However, the authors do not visualize the imputed missing views. This is not reasonable. In many existing works, such as \u2018Dual contrastive prediction for incomplete multi-view representation learning\u2019, the imputed missing views can be visualized to make the approach look more credible. However, just using ablation experiments is not convincing enough."
            },
            "questions": {
                "value": "1. How to validate the robustness as a robust method proposed in the paper? \n2. K-Nearest-Neighbor (KNN) imputation is introduced in the paper. Is the method sensitive to the nearest neighbor numbers?\n3. From Table 1, the feature dimensions of the datasets are not large, even very small. For example, one feature dimension of handwritten datasets is just 6. Is it necessary to use deep neural networks even Transformer to extract its features again?\n4. What is the impact of the design of deep neural network layers and the selection of dimensions for each layer on clustering results?\n5. For the experimental results, why are the experimental results you provided lower than the original papers? For example, DCP on the Scene 15 dataset is much lower than the published papers. In addition, how were the experimental results of Completer obtained? The original Completer is proposed for two view data which cannot be applied on the datasets you exploited in the paper directly."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5516/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698688749821,
        "cdate": 1698688749821,
        "tmdate": 1699636564957,
        "mdate": 1699636564957,
        "license": "CC BY 4.0",
        "version": 2
    }
]