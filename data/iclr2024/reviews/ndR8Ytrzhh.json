[
    {
        "id": "KuKtlTzhnY",
        "forum": "ndR8Ytrzhh",
        "replyto": "ndR8Ytrzhh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7556/Reviewer_UrCF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7556/Reviewer_UrCF"
        ],
        "content": {
            "summary": {
                "value": "To address the issue of high costs associated with self-consistency (SC), this paper introduces an approach called Early-Stop Self-Consistency (ESC). ESC incorporates an early-stop strategy into SC to reduce the overall number of samples needed. The method achieves this by dividing the large sample size used in SC into smaller sequential windows, and it stops sampling when all answers within a window are the same. Additionally, the paper presents a control scheme for ESC that dynamically selecting the size of window and maximum sampling times for different tasks and models. The effectiveness and reliability of ESC are supported by solid theoretical guarantee and extensive experiments. The empirical results demonstrate that ESC significantly reduces the average number of samples required in SC across six benchmark tasks, all while maintaining comparable performance levels.\n\nContributions: \n(1) This paper introduces an early-stop self-consistency method (ESC) to significantly reduce the computational cost of self-consistency while maintaining comparable performance.\n(2) This paper also puts forth a control scheme for ESC that assists in the selection of an optimal window size and maximum sampling times, considering the sampling budget and performance requirements.\n(3) Furthermore, this paper offers ample theoretical evidence to uphold the reliability of ESC."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(1) The method is simple and effective.\n\n(2) It is backed by a solid theoretical foundation.\n\n(3) Extensive experiments have been conducted to confirm its effectiveness and reliability."
            },
            "weaknesses": {
                "value": "(1) A related paper with a similar idea, called \"Let\u2019s Sample Step by Step: Adaptive-Consistency for Efficient Reasoning with LLMs\" (https://arxiv.org/pdf/2305.11860.pdf), was not referenced.\n\n(2) In Table 1, there appear to be inaccuracies in some of the results highlighted in green. For instance, in the row labeled \"L\u02c6-SC (GPT4)\" and the column labeled \"SQA,\" the value \"(-0.27)\" should actually be \"(+0.87)\" because the correct difference is 0.78 (81.42 - 80.55 = 0.78). Similar issues can be found in the \"SQA\" column. Additionally, it's puzzling that in the \"SQA\" dataset, L\u02c6-SC outperforms SC, even though SC has a larger sample size. This phenomenon requires further explanation."
            },
            "questions": {
                "value": "Question:\n\n(1) In the \"SQA\" dataset, L\u02c6-SC outperforms both SC and ESC, even though SC has a larger sample size. If there are no data errors, is there any possible reasonable explanation?\n \nSuggestion:\n\n(1) In Table 1, the accuracy of L\u02c6-SC seems decreases slightly (less than 0.5%) in more than half situations. Therefore, it might not be accurate to claim \"a large margin\", as the paper does, that \"We also test SC with L\u02c6 as the sampling size (L\u02c6-SC), whose accuracy drops by a large margin.\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7556/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7556/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7556/Reviewer_UrCF"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7556/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698415882136,
        "cdate": 1698415882136,
        "tmdate": 1699636914469,
        "mdate": 1699636914469,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2E5rjpfmQJ",
        "forum": "ndR8Ytrzhh",
        "replyto": "ndR8Ytrzhh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7556/Reviewer_rrBP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7556/Reviewer_rrBP"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a new technique called early-stopping self-consistency (ESC) aimed at improving the computational efficiency of machine learning models, particularly in the context of complex reasoning tasks. Leveraging the essence of Chain-of-thought Reasoning and Self Consistency, the authors introduce ESC as a mechanism to strike a balance between computational cost and performance. Through extensive experiments, the paper claims significant reduction in computational overhead without a noticeable drop in performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Originality: The introduction of ESC offers a fresh perspective in the realm of efficient machine learning algorithms.\nQuality: The experimental setup, including testing on six benchmarks, demonstrates the thoroughness of the research.\nClarity: The paper, for the most part, is well-written and concepts are explained clearly."
            },
            "weaknesses": {
                "value": "Comparison with State-of-the-art: It would be helpful to see direct comparisons with current state-of-the-art methods in terms of efficiency and performance.\nGeneralizability: The paper could discuss potential limitations or scenarios where ESC might not be the optimal solution."
            },
            "questions": {
                "value": "The Early-Stop Consistency (ESC) strategy is an optimized or \"pruned\" version of the Self-Consistency (SC) method, and its effect is not improved compared to SC. From this point of view, the method is lack of novelty. The primary innovation of ESC lies not in a theoretical advance but in its practical utility. It addresses real-world constraints by optimizing the balance between computational expenditure and performance fidelity."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7556/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698656968808,
        "cdate": 1698656968808,
        "tmdate": 1699636914335,
        "mdate": 1699636914335,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "A8jimmXNql",
        "forum": "ndR8Ytrzhh",
        "replyto": "ndR8Ytrzhh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7556/Reviewer_J879"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7556/Reviewer_J879"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes to use an early stopping criterion, based on answer entropy, when sampling alternative answers from a LLM in a self-consistency (SC) setting. SC is a form of ensemble answering, where multiple answers are sampled and a vote decides on the final answer. With early stopping answers are sampled window-wise iteratively until the whole window contains the same answer. Experiments show that this can reduce the number of necessary calls to a LLM while maintaining similar accuracy in reasoning benchmarks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "LLMs are a popular topic currently and their execution is costly, either in monetary terms or computationally. Therefore, it is a good approach to reduce the number of calls necessary, as is proposed in the paper.\nIt is also a positive thing that existing proven techniques and statistical approaches are re-visited and used in these settings, such as early stopping or using answer entropy as a cut-off criterion.\nThe experimental evaluation confirms the suitability of the approach over the more exhaustive standard SC technique. Experiments are extensive and consider many facets of the proposed approach."
            },
            "weaknesses": {
                "value": "The contribution is not particularly strong. Early stopping or using the confidence respectively the variation in multiple answers in an ensemble of answers is a well known technique. While we have (maybe, I'm not sure) not seen this in LLM sampling, it is not a particularly strong contribution in the context of an ICLR paper.\n\nI'm also not sure we actually need the notion of the window in the method or if other statistical measurements of the confidence resp. variability  could be used to determine the cut-off point. Unfortunately, this has not been discussed."
            },
            "questions": {
                "value": "No specific questions"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7556/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7556/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7556/Reviewer_J879"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7556/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698844187559,
        "cdate": 1698844187559,
        "tmdate": 1700643593013,
        "mdate": 1700643593013,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "C5gU1B0zgi",
        "forum": "ndR8Ytrzhh",
        "replyto": "ndR8Ytrzhh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7556/Reviewer_qSHs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7556/Reviewer_qSHs"
        ],
        "content": {
            "summary": {
                "value": "This paper presents Early-Stopping Self-Consistency (ESC), an adaptation of the original self-consistency to reduce the sampling cost. Instead of generating all samples at once, ESC generates samples in multiple sampling windows, and stops when all samples inside the same window produce the same results. They also provide a theoretical analysis on the sampling cost and the ESC performance compared to SC. They empirically evaluate ESC on multiple reasoning benchmarks, and demonstrate that ESC achieves comparable accuracies to SC, while the number of samples notably reduces."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. ESC is a simple yet effective adaptation of the original self-consistency to reduce the sampling cost.\n\n2. The ablation studies and theoretical analysis show that ESC is generally applicable to different benchmarks, and stays effective with different setups."
            },
            "weaknesses": {
                "value": "1. The novelty of this work is unclear. [1] already proposed an adaptation of self-consistency to reduce the sampling cost, but this work did not cite and discuss this prior work. Without a thorough discussion and direct comparison, it is unclear whether ESC is more effective.\n\n2. In Table 1, when comparing ESC and L-SC, the performance difference is generally small. The reason can be that the improvement of SC saturates when the sample size increases, thus reducing the sampling size also does not drastically degrade the performance for SC. It is helpful to show this comparison for smaller sampling sizes, e.g., those in Table 2, and see if the performance improvement achieved by ESC can be more significant.\n\n3. There are some issues in Table 1. For example, the SQA results of L-SC are generally much higher than SC, which look problematic. Also, it is confusing to list L in the table without additional notes, as L represents the sample size, while all other rows represent the task accuracies.\n\n[1] Aggarwal et al., Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning with LLMs, EMNLP 2023."
            },
            "questions": {
                "value": "1. Please clarify the novelty of this work. In particular, discuss and compare the approach to [1].\n\n2. Show this comparison in Table 1 for smaller sampling sizes, e.g., those in Table 2, and see if the performance improvement achieved by ESC can be more significant.\n\n3. Explain and fix issues in Table 1. For example, the SQA results of L-SC are generally much higher than SC, which look problematic. Also, it is confusing to list L in the table without additional notes, as L represents the sample size, while all other rows represent the task accuracies.\n\n[1] Aggarwal et al., Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning with LLMs, EMNLP 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7556/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699236188194,
        "cdate": 1699236188194,
        "tmdate": 1699636914099,
        "mdate": 1699636914099,
        "license": "CC BY 4.0",
        "version": 2
    }
]