[
    {
        "id": "T2rxc3PO9g",
        "forum": "LKx4rubqkO",
        "replyto": "LKx4rubqkO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7753/Reviewer_8Zoy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7753/Reviewer_8Zoy"
        ],
        "content": {
            "summary": {
                "value": "The paper proposed a solution to detecting texts generated by LLMs. It emphasized on computational costs, accessibility, and performance.\n\nSpecifically, the authors propose a metric-based detection framework that evaluates the similarity between a given text and an equivalent example generated by LLMs to determine the text's origination. The framework includes a text embedding model and a metric model, with a focus on designing the metric component. \n\nThe authors also introduce four datasets with over 85,000 prompts and triplets of responses for benchmarking, showing that their best architectures maintain F1 scores between 0.87 to 0.95 across various settings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The approach itself is new. Being able to detect this without access to the internal structure is very valuable.\n* The data sets will be very useful for any downstream tasks.\n* The experimental results are impressive.\n* The focus on computational cost and accessibility is especially relevant."
            },
            "weaknesses": {
                "value": "* This approach still relies heavily on the LLM themselves.\n* The scope of the study is rather small, focusing only on short responses."
            },
            "questions": {
                "value": "* Could you elaborate on the different types/variations of prompts?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7753/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698727613089,
        "cdate": 1698727613089,
        "tmdate": 1699636946568,
        "mdate": 1699636946568,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OWoVX8kvLT",
        "forum": "LKx4rubqkO",
        "replyto": "LKx4rubqkO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7753/Reviewer_CEgw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7753/Reviewer_CEgw"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a metric-learning based approach for the detection of LLM-generated text given a known context. The authors propose two metric-based neural architectures trained with triplet loss, one based on embedding the full-text, and another based on embedding individual sentences. The detection decision is then based on thresholding the distance between an input-text and an LLM generated text."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* Approach does not require knowing the logits of the LLM or modifying its logits in any way as with watermarking. \n* Focuses on relatively short pieces of text (3-5) sentences. \n* Authors created and will release a set of four datasets totaling over 85,000 generations on a wide variety of topics."
            },
            "weaknesses": {
                "value": "* There are many typographic errors spread throughout the manuscript. This made the paper overly difficult to understand.\n* The proposed approach requires knowing the context that may have been given to an LLM to generate the text. This seems like too big of a restriction, as it is often the case that we don\u2019t know how the LLM was prompted. Suggestion:\n  * Instead of requiring the context to generate a response by ChatGPT, a set of known ChatGPT generations on arbitrary generations could\u2019ve been kept aside. The detector would then take the distance between the input text and the set of known ChatGPT generations.\n  * The above could be compared to the case where one does know the context of the generation. \n* The only LLM considered is ChatGPT. It would\u2019ve been interesting to include a broader set of LLMs and explore the robustness of the approach to unseen LLM. Suggested Models: Llamav1, Llamav2, OPT, GPT-2, GPT-4, Cohere, Dollyv2, etc.\n* In the \u201csame corpus with paraphraser\u201d experiments the model was trained on the paraphrased LLM detections. This means that all the testing data is still in-domain, and hence it may defeat the purpose of the experiment. Suggestions:\n  * Use a different paraphraser on the testing data.\n  * Do not train on the paraphrased text.\n* It is unclear whether the sentence framework out-performs the full-text framework simply because it has more parameters. Suggestion: To experiment whether this is true, a best-effort could be made at matching the number of parameters between both frameworks by either increasing the number in the case of the full-text framework or decreasing it in the case of the sentence framework.\n* Instead of evaluating the F1 score on the best threshold found in the validation set, it would\u2019ve been better to just plot the ROC curves and show the median and 90th / 10th percentiles. This would give a better understanding of how the detector performs across varying ranges of FPR. \n* There are more up-to-date metric-learning losses that could\u2019ve been explored. Examples:\n  * Supervised Contrastive Loss: https://arxiv.org/abs/2004.11362\n  * InfoNCE: https://papers.nips.cc/paper_files/paper/2016/file/6b180037abbebea991d8b1232f8a8ca9-Paper.pdf\n  * Tuplet Margin Loss: https://openaccess.thecvf.com/content_ICCV_2019/papers/Yu_Deep_Metric_Learning_With_Tuplet_Margin_Loss_ICCV_2019_paper.pdf\n* Not enough baselines compared against. Some recommendations include:\n  * OpenAI Detector: https://huggingface.co/roberta-base-openai-detector\n  * LLMDet: https://github.com/TrustedLLM/LLMDet\n  * A RoBERTa detector fine-tuned on the context and LLM-generation / human-answer for each dataset."
            },
            "questions": {
                "value": "* In S5, the training time of various methods is brought up as a reason to exclude certain baselines from consideration. However, this is an offline cost, which is not relevant at inference/test-time, so it's not clear why it's relevant. How do various methods compare at inference time?\n\n* In practice, the robustness of machine-text detectors is important for many applications of machine-text detectors. For example, the cost of false positives is high when making false accusations of plagiarism. As a result, it is important to measure the robustness and calibration of such detectors. How does the proposed method fare when faces with various distribution shifts relative to the training data? (new LLMs, new domains, new decoding methods, etc.) Is it more robust than simple alternatives, such as simple supervised classifiers?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7753/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698776979927,
        "cdate": 1698776979927,
        "tmdate": 1699636946457,
        "mdate": 1699636946457,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hmVynsoRfi",
        "forum": "LKx4rubqkO",
        "replyto": "LKx4rubqkO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7753/Reviewer_7NGJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7753/Reviewer_7NGJ"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the problem of detecting texts generated by Large Language Models(LLMs). The authors propose a method that involves assessing the similarity between a given text and a comparable LLM-generated example to determine the source of the text. The authors conduct experiments on a novel, metric-based detection paradigm, utilizing four extensive datasets comprising over 85,000 prompts and response triplets, to evaluate the effectiveness of their approach in distinguishing between human-written and LLM-generated texts across various corpora and contexts, including paraphrasing scenarios. Experimental results show the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well-organized and easy to follow."
            },
            "weaknesses": {
                "value": "- The paper focuses exclusively on short text responses, with an average length of five sentences and no less than three. This limitation could hinder the generalizability and applicability of the proposed approach, as longer texts or documents might exhibit different characteristics and patterns that are not captured by the model trained on shorter responses.\n\n- Lack of convincing baselines. The authors merely use the distance threshold approach on the embedding generated by the standalone pretrained MPNet as the baseline. \n\n- The authors not evaluate the proposed method on the benchmark such as Human ChatGPT Comparison Corpus (HC3) [4]. \n\n- There is a lack of previous studies in the literature.  \n\n[1] Gehrmann, Sebastian, Hendrik Strobelt, and Alexander Rush. \"GLTR: Statistical Detection and Visualization of Generated Text.\" Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. Association for Computational Linguistics, 2019.\n\n[2] Clark, Elizabeth, et al. \"All That\u2019s \u2018Human\u2019Is Not Gold: Evaluating Human Evaluation of Generated Text.\" Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.\n\n[3] Sadasivan, V. S., Kumar, A., Balasubramanian, S., Wang, W., & Feizi, S. (2023). Can ai-generated text be reliably detected?. arXiv preprint arXiv:2303.11156.\n\n[4] Guo, B., Zhang, X., Wang, Z., Jiang, M., Nie, J., Ding, Y., ... & Wu, Y. (2023). How close is chatgpt to human experts? comparison corpus, evaluation, and detection. arXiv preprint arXiv:2301.07597."
            },
            "questions": {
                "value": "Please refer to the \"Weaknesses\" section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7753/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698832167117,
        "cdate": 1698832167117,
        "tmdate": 1699636946349,
        "mdate": 1699636946349,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qJjV9BUXXE",
        "forum": "LKx4rubqkO",
        "replyto": "LKx4rubqkO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7753/Reviewer_X5RJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7753/Reviewer_X5RJ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a neural classifier that detects LLM-generated texts from\nhuman-generated texts. Instead of directly relying internals of LLMs, the \nproposed classifier leverages triplet samples of the same meanings, and builds\na neural classifier on these triplets that uses metric learning internally.\nExperimental results indicate that the proposed classifier works good (but not\nshown better than baselines) on several datasets, and induced metrics are\nsignificantly different between LLM-generated texts and human-generated texts."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "While the rationale and the proposed approach is decent, the crucial drawback\nof this paper is that it is not compared with possible baselines. The basic\nidea that the classifier should base on the sentences of the same semantic\ncontent is of course good, but such a classifier can be built just from a set\nof pairs of texts, not triplets. Also, that central idea of using the\nsame-meaning sentence itself should be validated empirically: how about the\nperformance if we simply build a classifier on two sets of texts, one from LLM\nand one of humans?\n\nWithout such empirical investigations, this paper should not be accepted as a\nmachine learning conference paper."
            },
            "weaknesses": {
                "value": "See above."
            },
            "questions": {
                "value": "Nothing."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7753/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699003573873,
        "cdate": 1699003573873,
        "tmdate": 1699636946201,
        "mdate": 1699636946201,
        "license": "CC BY 4.0",
        "version": 2
    }
]