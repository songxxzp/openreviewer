[
    {
        "id": "YZOb47tY9j",
        "forum": "j9KV9wsHqC",
        "replyto": "j9KV9wsHqC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7780/Reviewer_3AJJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7780/Reviewer_3AJJ"
        ],
        "content": {
            "summary": {
                "value": "This work focuses on vision language models and proposes prompt-aware adapters. It utilizes a similar framework with InstructBLIP, which utilizes an instruction-guided adapter for text-aware visual tokens. Experiments show part of the effectiveness."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. This work focuses on the instruction-guided method for the vision language model, which is promising.\n2. The proposed method is simple and easy to follow.\n3. The presentation is clear."
            },
            "weaknesses": {
                "value": "1. The proposed method is very similar to InstructionBLIP from the framework to the training strategy. This work also utilizes the pre-trained QFormer in BLIP. It is essential to reveal the difference between this work and InstructBLIP. This strongly harms the technical contribution of this work.\n2. Comparisons with previous work are missing. It is necessary to compare it with other vision language models (especially InstructBLIP) on public datasets, like VQAV2, GQA, ScienceQA, COCO Caption, et.al."
            },
            "questions": {
                "value": "Please refer to the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7780/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698584726742,
        "cdate": 1698584726742,
        "tmdate": 1699636950510,
        "mdate": 1699636950510,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IDa59ApilH",
        "forum": "j9KV9wsHqC",
        "replyto": "j9KV9wsHqC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7780/Reviewer_1W2T"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7780/Reviewer_1W2T"
        ],
        "content": {
            "summary": {
                "value": "The paper aims to improve the conversion of visual inputs into relevant tokens for GPT4-style multimodal large language models (MLLMs) by addressing the limitations of current adapters.\n\n**Proposed Solution:** \nA \"prompt-aware adapter\" is introduced, designed to dynamically embed visual inputs based on textual prompts, thus optimizing visual information extraction.\n\n**How it Works:** \nThe adapter utilizes textual prompts as a guide during conversion, employs learnable query embeddings, a visual transformer, and a textual transformer to interact with visual features based on prompts.\n\n**Evaluation & Results:** \nTests on various visual tasks showed improved performance with the prompt-aware adapter, increasing accuracy across tasks by 2.89%, 5.80%, 1.43%, and 2.98%.\n\nThe new prompt-aware adapter enhances the visual reasoning of MLLMs by focusing on relevant visual cues based on textual prompts. The code for the research will be made public."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. **Innovation & Addressing a Gap:** The paper introduces a novel \"prompt-aware adapter\" that optimally converts visual inputs to relevant tokens for GPT4-style models, highlighting and addressing current model limitations.\n  \n2. **Practical Implementation & Validation:** The research provides a comprehensive description of the adapter's practicalities and validates its effectiveness through experiments, showcasing improved accuracy across multiple visual tasks."
            },
            "weaknesses": {
                "value": "**Weaknesses of the Paper:**\n\n1. **Simplicity of Method:** The method appears straightforward, with the primary approach being the direct addition of text embedding in the adapter.\n  \n2. **Limited Trainable Parameters:** The paper does not address the issue of having too few trainable parameters, which raises questions about its capacity to handle complex tasks.\n\n3. **Questionable Integration:** The decision to process text and visual attention operations layer-by-layer in the Prompt-Aware Adapter, instead of directly concatenating them, is not clearly justified.\n\n4. **Overemphasis on Common Knowledge:** A substantial portion of the content reiterates common knowledge, such as the formulas for attention, which may be redundant for seasoned readers.\n\n5. **Limited Experimental Scope:** Experiments were primarily conducted on the smaller scale MiniGPT-4 model. The paper lacks quantitative comparisons with state-of-the-art (SOTA) image-text understanding models like Lava and Flamingo.\n\n6. **Unclear Multimodal Context Handling:** It's unclear how the adapter would function in multi-turn dialogues when only textual input is present.\n\n7. **Potential Bias in Illustrations:** Figure 6 presents selective examples, which might be cherry-picking, thus not offering a clear advantage over comparative methods."
            },
            "questions": {
                "value": "Pls see the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7780/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698678741149,
        "cdate": 1698678741149,
        "tmdate": 1699636950398,
        "mdate": 1699636950398,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9mIayBE87k",
        "forum": "j9KV9wsHqC",
        "replyto": "j9KV9wsHqC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7780/Reviewer_p8Xv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7780/Reviewer_p8Xv"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a prompt-aware adapter for multimodal large language models (MLLM). Instead of simply mapping visual tokens into understandable tokens of LLMs via a linear layer, the prompt-aware adapter adopts a DETR decoder-like structure to aggregate information from prompts, easing the visual parsing of LLMs. The method is simple but effective across various tasks."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The motivation is clear, each time the prompt is switched (different tasks), the visual input to LLMs stays stationary, a natural way to improve it is to make it prompt-aware, i.e., conditioning the visual input to LLMs on prompts. The experiment results show remarkable improvement even over finetuning the Q-Former."
            },
            "weaknesses": {
                "value": "The chosen tasks are relatively low-level, they hardly need the reasoning capability of LLMs, could authors provide more experiments on high-level tasks to prove the prompt-aware adapter's generalization ability?"
            },
            "questions": {
                "value": "Is there any better prompt-aware adapter structures than this query-based decoder, or should the main focus of this paper put on designing the prompt-aware structure since this dynamic idea is prevalent across computer vision and exist for years ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7780/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698746849888,
        "cdate": 1698746849888,
        "tmdate": 1699636950298,
        "mdate": 1699636950298,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "L4t1GbFyBR",
        "forum": "j9KV9wsHqC",
        "replyto": "j9KV9wsHqC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7780/Reviewer_PKd5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7780/Reviewer_PKd5"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a prompt-aware adapter for multimodla LLM, which fuses visual and textual information as the input of LLM. Preivous works take visual information as the input to LLM, ignoring the relation between visual and textual information. The proposed method leverage a smaller language model as prompt adapter to extract prompt relevant information so that the LLM can better focus on the human instructions. The proposed method is evaluated on COCO-QA dataset with better performance than another MLLM, MiniGPT4."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The motivation is reasonable and the method is clearly explained.\n- The proposed method is evaluated on several tasks with promising results."
            },
            "weaknesses": {
                "value": "- The prompt adapter is nearly the same as Language-Guided Tokenizer in VisionLLM [1]. They all take the visual feature and text prompt as  input and output fused visual tokens. Both Language-Guided Tokenizer in VisionLLM and prompt adapter in this work utilize Bert the basic model.\n- The evaluation is very week. The proposed method has not evaluated on either traditional captioning, VQA tasks and recent multimodal benchmarks [2,3,4]. Beisdes, the quantitative comparison baseline is just MiniGPT4. More comparison is needed.\n- More ablation experiments are needed here to prove that prompt adapter is really helpful.\n\n[1] Wang, Wenhai, et al. \"Visionllm: Large language model is also an open-ended decoder for vision-centric tasks.\" arXiv preprint arXiv:2305.11175 (2023).\n[2] Fu, Chaoyou, et al. \"MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models.\" arXiv preprint arXiv:2306.13394 (2023).\n[3] Liu, Yuan, et al. \"MMBench: Is Your Multi-modal Model an All-around Player?.\" arXiv preprint arXiv:2307.06281 (2023).\n[4] Li, Bohao, et al. \"Seed-bench: Benchmarking multimodal llms with generative comprehension.\" arXiv preprint arXiv:2307.16125 (2023)."
            },
            "questions": {
                "value": "n/a."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7780/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698768828640,
        "cdate": 1698768828640,
        "tmdate": 1699636950175,
        "mdate": 1699636950175,
        "license": "CC BY 4.0",
        "version": 2
    }
]