[
    {
        "id": "kwUsdr80Fe",
        "forum": "jb37oaGZXy",
        "replyto": "jb37oaGZXy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2068/Reviewer_uHwb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2068/Reviewer_uHwb"
        ],
        "content": {
            "summary": {
                "value": "The paper studies how to more effectively train a unified (sequence-to-sequence) vision-language model across multiple tasks. In prior work, each training example comes with a simple description of the intended task, e.g., \u201cWhich region does the text V describe\u201d. The main argument in this work is that such simple descriptions may not be enough and more detailed and exhaustive task descriptions are beneficial. \n\nFor 7 pre-training tasks, the authors created detailed task descriptions consisting of data description, input/output format, and output description. Training with the detailed task descriptions shows improvement over training with simple and plain descriptions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Training with more detailed task descriptions is a natural step to take for training instruction-following vision-language models. The authors verify that using such task descriptions indeed improves performance upon baselines such as OFA."
            },
            "weaknesses": {
                "value": "- Limited novelty and performance improvement\n\nReplacing simple tasks descriptions with complex descriptions is a simple idea and has been successfully explored in language model literature [1]; thus the novelty is limited.\n\nThe paper shows that by using complex task descriptions, the model improves marginally upon the baseline on training tasks. It is not shown whether training on such task descriptions brings any new capacities (e.g., transfer to a new task with a new task description, or using task description to transfer to a new data domain, as is done in [1]); the core appeal of using complex task descriptions seems to be missing.\n\n[1] Generalization via Declarative Instructions on 1600+ NLP Tasks. Wang et al. 2022\n\n\n\n- Limited number of tasks and formats of task descriptions\n\nThe paper only studies 7 pre-training tasks, which makes the generalization of the conclusion questionable. For example, one big contributor in description is the inclusion of \u201cdata source\u201d. Could it be because out of the 7 tasks, many data are from COCO so the model learns to utilize this information? What if the pre-training datasets all come from different image sources?"
            },
            "questions": {
                "value": "- In Section 1.1, the paper states \u201cFor example, in visual grounding of some concept V , the prompt \u201cWhich region does the text V describe\u201d requires the model to interpret \u201cfind\u201d and represent the word \u201cregion\u201d with sets of coordinates on the image plane, which do not have a meaningful (topologically consistent) representation in natural language.\u201d  I am not quite sure what \u201cinterpret \u201cfind\u201d and \u2018represent the word \u201cregion\u201d\u2019 and \u201cdo not have a meaningful (topologically consistent) representation\u201d means. Could the authors elaborate on this issue? The base prompts seem okay to me and are not ambiguous.\n\n- How many \"soft task vectors\" does each task have for the learnable task vector baseline?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2068/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698821055064,
        "cdate": 1698821055064,
        "tmdate": 1699636138795,
        "mdate": 1699636138795,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Iqfw2l2zy3",
        "forum": "jb37oaGZXy",
        "replyto": "jb37oaGZXy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2068/Reviewer_nMNN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2068/Reviewer_nMNN"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates how to jointly finetuning a vision-language pretrained model onto several downstream tasks to achieve both optimal performance as well as task generalization. A model called Musketeer is proposed which utilizes Task Explanation Prompt (TEP) to reduce the interference among tasks, which helps the model to optimize each single-task better during multi-task downstream finetuning. The TEP contains sufficient task meta information, including data description, input/output format, output description and instance prompt. On downstream tasks, the Musketeer model achieves comparable or better single-task results over single-task finetuned baselines. Compared with multi-task finetuned baselines, Musketeer obtains much better results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The research question is clear and important. Currently most pretrained-then-finetuned VL models still cannot achieve task generalization and SOTA performance together.\n2. Detailed TEP of each downstream VL task are given, increasing the reproducibility of this work.\n3. The baseline used is competitive, demonstrating the effectiveness of Musketeer model.\n4. Abundant ablation analysis is conducted."
            },
            "weaknesses": {
                "value": "Since the TEP contains abundant downstream meta task information, if more discussion and experiment on zero-shot new task generalization, it will be much better."
            },
            "questions": {
                "value": "Since the OFA model not only unifies the VL tasks but also text-only tasks. Can Musketeer also be applied on text-only tasks? Is there any experimental evidence?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2068/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2068/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2068/Reviewer_nMNN"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2068/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698821714652,
        "cdate": 1698821714652,
        "tmdate": 1699636138712,
        "mdate": 1699636138712,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2q2X6URhgR",
        "forum": "jb37oaGZXy",
        "replyto": "jb37oaGZXy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2068/Reviewer_Dtxz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2068/Reviewer_Dtxz"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new sequence-to-sequence vision-language model called Musketeer that can be trained jointly on multiple visual tasks using a shared set of parameters. The key idea is to use a novel Task Explanation Prompt (TEP) to reduce interference between tasks and allow the model to leverage shared structures. The TEP provides detailed natural language instructions about the dataset, input/output formats, output targets, etc. Experiments on 7 vision-language tasks like visual grounding, VQA, captioning etc show Musketeer matches or exceeds performance of task-specific models and other multi-task baselines. Without any task-specific tuning, Musketeer shows strong performance on all tasks using the descriptive power of TEPs to instantiate task-specific pathways at inference."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is well-written and clearly presented; \n- The paper proposes a novel TEP approach to reduce multi-task interference using natural language specifications. It provides a unified architecture without any task-specific tuning or heads.\n- It shows strong empirical results demonstrating effectiveness for diverse vision-language tasks comparing to baselines; \n- Detailed experiments on the effects of each mixed dataset (vg, captain, ic, etc.) to the downstream have been provided across scales, which may benefit future researchers in the same area;"
            },
            "weaknesses": {
                "value": "- TEP still relies on pretrained weights for initialization which can be expensive, the discussion regarding the additional cost might be good to provide; \n- The hyper-parameter setting as well as the Needs carefully designed TEPs for new tasks which may require some expertise.\n- The study on how well TEPs could transfer to unseen tasks is unknown; \n- Some related works might also be good to include or discuss [1, 2, 3, 4]; \n\n[1] Dai, Wenliang et al. \u201cInstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning.\u201d ArXiv abs/2305.06500 (2023); \n[2] Shen, Sheng, et al. \"Multitask vision-language prompt tuning.\" WACV 2024.\n[3] Asai, Akari, et al. \"Attempt: Parameter-efficient multi-task tuning via attentional mixtures of soft prompts.\" Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022.\n[4] Liu, Haokun, et al. \"Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning.\" Advances in Neural Information Processing Systems 35 (2022): 1950-1965."
            },
            "questions": {
                "value": "- Could the author explain more on the varied performance on VQA in table 4, will using full VQAv2 training data mitigate the problems?\n- Could the author provide additional training cost including the pretrainining cost for the proposed methods in Table 3 and 4 for a comprehensive evaluations;"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2068/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698843690148,
        "cdate": 1698843690148,
        "tmdate": 1699636138645,
        "mdate": 1699636138645,
        "license": "CC BY 4.0",
        "version": 2
    }
]