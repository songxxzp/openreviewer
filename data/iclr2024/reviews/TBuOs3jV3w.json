[
    {
        "id": "EuGxhRwW7K",
        "forum": "TBuOs3jV3w",
        "replyto": "TBuOs3jV3w",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission497/Reviewer_P4QJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission497/Reviewer_P4QJ"
        ],
        "content": {
            "summary": {
                "value": "To tackle the issues of specific design for various editing types, this work proposes framework that allows for the direct acquisition of a NeRF model with universal editing capabilities, eliminating the requirement for retraining. After modify the 3D scene images, a filtering process to\ndiscard poorly edited images and use generalizable nerf to get consistent views. Cross-view regularization terms are used here."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "extensive experiments of different editing types are conducted;"
            },
            "weaknesses": {
                "value": "1. Generalizable nerf model can be limited to novel view generation in very limited view range;\n2. The robustness of fliter technique is questionable to me\n3. Abalation study cases are very limited."
            },
            "questions": {
                "value": "1. when using nearby view supervision, how to get depth is not well explained;\n2. how to deduce from eq3 to eq4 is not clear to me\n3. how many images used for generalizable nerf synthesis?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission497/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission497/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission497/Reviewer_P4QJ"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission497/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697733652853,
        "cdate": 1697733652853,
        "tmdate": 1699635976465,
        "mdate": 1699635976465,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4OO93zCkL8",
        "forum": "TBuOs3jV3w",
        "replyto": "TBuOs3jV3w",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission497/Reviewer_bafn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission497/Reviewer_bafn"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a framework for text-driven 3D scene editing tailored with image-based rendering. The author claims to achieve generalizable editing capability which facilitates training-free editing. The key contribution is to generate lots of multiview training data by adding perturbations to 3D scenes using BLIP\u2192GPT\u2192Null-text inversion pipeline. Then, a generalizable image-based rendering model is trained to get rid of scene-specific training. The experiments show promising results on 3D editing. However, the generalization to novel text/scene is not validated."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The idea to combine generalizable image-based rendering with 3D scene editing is interesting.\n- The pipline BLIP\u2192GPT\u2192Null-text inversion to generate edited images on caption-free multiview dataset is promising.\n- The visual result is compelling with good demonstration on various types of scenes, including object-centric scene, face-forward scene, and unbounded scene."
            },
            "weaknesses": {
                "value": "Given the lack of validation on the generalizability, I lean to rejection at this point. I\u2019m happy to be convinced by the authors\u2019 response.\n\n- My major concern is the generalizability. According to the paper and supplementary material, I do not find any descriptions on the train/test split for the experiments. If all the results presented in the paper are seen during training, the paper definitely overclaims the generalizability. Thus, the title is also misleading as \u201cwithout retraining\u201d.\n- I hope to see more details about the setting of experiments and corresponding evaluation protocols:\n    - The train/test split of text captions. If all captions used in the qualitative results are seen during training, please present the result of editing the scene with unseen captions\n    - The train/test split of 3D scenes. If all scenes presented in the qualitative results are seen during training, please exclude some of them and retrain the model. Then test on those unseen scenes. Otherwise, the method still needs retraining given user-specific 3D scenes, which does not support the main claim."
            },
            "questions": {
                "value": "- Please see the weakness section for my concern about generalizability.\n- What is the motivation of src_a and src_b. Please clarify the motivation for these two separate models."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission497/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission497/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission497/Reviewer_bafn"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission497/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698396934342,
        "cdate": 1698396934342,
        "tmdate": 1699635976396,
        "mdate": 1699635976396,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ku2WiCRtjq",
        "forum": "TBuOs3jV3w",
        "replyto": "TBuOs3jV3w",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission497/Reviewer_xjTT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission497/Reviewer_xjTT"
        ],
        "content": {
            "summary": {
                "value": "This paper uses diffusion model for 2D edits by text and consolidates the edits in NeRF. It claimed to work across several appearance editing and style transfer and also reduce the time overhead."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper contributes to quite a relevant NeRF editing topic with broad interest. \n2. Extensive results are presented."
            },
            "weaknesses": {
                "value": "1.  As Introduction mentioned, existing methods typically rely on known editing types in advance with limited modification capabilities. Please elaborate on this claim and raise what editing types or some works who heavily rely on editing types. As of my knowledge and experience, Instruct-Nerf2Nerf or Clip-based methods are capable of any text input. They are not confined in specific editing types demonstrated in their paper. \n\n\"These techniques are often less user-friendly\" seems semantically overlap with prior two claimed challenges. It does not add new information. Suggest to take off. \n\n2. 2D editing does not guarantee multi-view consistency for each input. However, each edited single frame is well-structured and jumpy between multiview may not be simply noisy perturbations. For example, changing the selfie into Fauvism can change the selfie into multiple (colorful/ high contrast) possible output and each output is well-structured but quite different. The difference between diffusion model outputs may not be noisy perturbation. \n\n3. The writing is complex and need much more clarity. For example, what is the purpose of input caption? Does it aims to provide a description and replace nouns or subjects to create a target caption for editing? What is the generalizable mean in the context? \n\n4. There is not enough information for the inference stage. It only mentioned content filter in the paragraph. In Fig. 2 inference time, how can NeRF generate closed eye image if it has not seen examples in the training time? From the figure, after the volume rendering it shows an open-eye image, but the next step in G's output, it abruptly show closed eye image.\n\nHow to use the filtered image at the inference time to avoid re-training NeRF is also not clear to me.\n\n5. The abstract claims doing appearance editing, weather transition, object changing, and style transfer. However, it seems the results only serve style transfer and appearance editing. It is not convincing to say changing to snow-covered roads is weather changes. The pineapple and strawberry examples seem just to change appearance only. It does not create new geometry."
            },
            "questions": {
                "value": "1. What exactly the \"generalizable\" term mean in NeRF? This term appears many times in the paragraph but is still vague. For example, what this NeRF model can attain that a vanilla NeRF model cannot.\n\n2. In Eq. 6, how to get M for overlapping areas? Is it provided in the data?\n\n3. In Fig.7, it mentioned a total of 6 methods for comparison but why in the exemplar Fig. 18 has only a total of 3 videos in comparison? (or is it just an example). How does one compute the ratios of one against the others in Fig. 7? It's hard to understand how the subjective test result in Fig. 7 is calculated. Besides, using only 4 scenes for a subjective test is far insufficient in my opinion. Also, please add a statistical significance p-value to validate the significance of the study."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission497/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission497/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission497/Reviewer_xjTT"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission497/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698796944706,
        "cdate": 1698796944706,
        "tmdate": 1699635976272,
        "mdate": 1699635976272,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4DELuvL0Xp",
        "forum": "TBuOs3jV3w",
        "replyto": "TBuOs3jV3w",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission497/Reviewer_A4Ej"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission497/Reviewer_A4Ej"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a way to perform editing of 3D scenes represented using NeRFs using text description of the target. They use BLIP for image captioning, GPT to generate target captions, image editing using text, etc., in the training step and train a generalisable NeRF model. During inference, text-based image editing is performed and the images that are 3D inconsistent are removed using a filtering step. The remaining images are used to generate new views using a network derived from IBRNet. The paper presents different editing results. The paper presents many ideas which are mostly explained in the long supplementary/appendix section."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Text-based editing of 3D scenes is an important problem and provides opportunities to create many variations of 3D representations if it can be done well. The paper leverages many recent methods to achieve this task in some way. They train the NeRF model using many more losses to achieve \"generalisability\". There are many good results shown."
            },
            "weaknesses": {
                "value": "Presentation/Writing: The presentation needs to improve as it is not clear what their training achieves. It is impossible to understand the method to a reasonable extent without reading the appendix or supplementary that runs into 15 pages. See my questions below.\n\nSignificance: While text-based editing will be useful, what is the predictability and controllability of such a method? How does one know if the generated model is according to the textual targets given? It seems there are few ways to do that beyond basic qualitative or visual assessments. This is a serious limitation of many methods that rely on a generative tool as there is little controllability or predictability on what such a tool generates. The DN2N method presented also has that problem also: what does the text-based-editing block generate at inference time? The method can only filter out inconsistent images. That is, it can only discard the images generated; it can't generate a better image by influencing the editing module. How do we know if selected good results only are shown here? The discussion in Section 4.5 needs to be far more elaborate."
            },
            "questions": {
                "value": "I have several doubts/questions about the inference or editing stage. These should be in the main paper clearly. \n- What is the \"effort\" involved during inference? How many 2D edited images are generated? How many are found to be inconsistent on an average and discarded?  What is the time taken?\n- Were there failure cases when sufficient # of consistent images couldn\u2019t be generated? Is the whole process iterated on if that happens?\n- Are edited images generated for the same camera poses of the input images used for training? Can other viewpoints be used?\n- Is there any way to ensure or control what we want of the text-based image editing module? Your method is very critically dependent on this module generating good ones. Your editing capability is limited seriously by this aspect. Please see my comments under \"weakness\".\n- The filtering step is rather too simple. How are the 4 measures used in the tuple prioritised or weighted? The details of the sorting could not be found anywhere. Why do you eliminate the top 10% of the matches? Aren't they the best?\n\nAlso, why is DN2N's output falling short in some methods in the user study?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None special beyond other methods that generate 3D objects which can be misused."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission497/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698928474306,
        "cdate": 1698928474306,
        "tmdate": 1699635976164,
        "mdate": 1699635976164,
        "license": "CC BY 4.0",
        "version": 2
    }
]