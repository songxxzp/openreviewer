[
    {
        "id": "vnGmA85xwa",
        "forum": "5PkgaUwiY0",
        "replyto": "5PkgaUwiY0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8416/Reviewer_dMSD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8416/Reviewer_dMSD"
        ],
        "content": {
            "summary": {
                "value": "VideoDirectorGPT presents an innovative framework for consistent multi-scene video generation, utilizing the capabilities of GPT-4 for video content planning and scene description. The process begins with a single text prompt, which is expanded by the video planner LLM (GPT-4) into a comprehensive \u2018video plan\u2019, detailing scene descriptions, entity layouts, background settings, and consistency groupings. This information guides the Layout2Vid video generator, enabling explicit control over spatial layouts and maintaining temporal consistency across multiple scenes. VideoDirectorGPT demonstrates competitive performance against state-of-the-art models in single-scene text-to-video generation. The framework showcases potential for innovative applications, offers dynamic control features, and supports user interaction, setting a promising precedent for the integration of LLMs in long video generation and laying the groundwork for future advancements in the field."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Reasonable Pipeline Design: The framework adeptly utilizes GPT-4 for meticulous video content planning, optimally harnessing the extensive capabilities of this large language model to bring an innovative and groundbreaking approach to the realm of video generation.\n\n2. Comprehensive Experimental Validation: The paper meticulously outlines a thorough and extensive experimental setup, ensuring a robust and all-encompassing evaluation of the framework\u2019s performance and capabilities. It highlights a diverse range of scenarios and use cases, showcasing the framework\u2019s exceptional versatility and its ability to seamlessly adapt to varying contexts.\n\n3. Diverse Visual Illustrations: The paper and accompanying demo are enriched with a wide array of visual examples, vividly demonstrating the framework\u2019s proficiency in generating multi-scene videos with varied themes and settings. Furthermore, the demo uniquely features support for user-provided images, thereby significantly enhancing user interaction and engagement with VideoDirectorGPT."
            },
            "weaknesses": {
                "value": "**Lack of Technical Contribution in Layout2Vid:** The Layout2Vid component of the VideoDirectorGPT framework, responsible for the actual video generation, appears to draw heavily from existing image generation work, particularly the Gilgen model which also operates based on layouts. There seems to be a noticeable lack of substantial technical differentiation or advancement in Layout2Vid, raising concerns about the novelty and contribution of this particular component to the field.\n\n**Lack of Environment Consistency:** The VideoDirectorGPT framework, while innovative in its approach to multi-scene video generation, exhibits a notable lack of consistency in environmental elements across different scenes, as prominently seen in the \"make caraway cakes\" demo example. Although the object (the woman) maintains a consistent appearance throughout, the environment suffers from visible discontinuities, leading to a disjointed, montage-like effect in the resulting video. This issue seems to be a direct consequence of the framework\u2019s heavy reliance on GPT-4 for generating scene descriptions, coupled with potential shortcomings in the Layout2Vid model's capability to translate these textual plans into visually cohesive sequences.\n\n**Minor: Limited Developmental Space:** The director model in VideoDirectorGPT demonstrates a substantial dependency on GPT-4 for generating scene descriptions. This reliance not only makes the system vulnerable to any limitations and performance issues inherent in GPT-4, but it also raises questions about the developmental prospects of the director model. Given that GPT-4 is a pre-trained model with fixed capabilities, improvements in scene planning and description generation may be challenging to achieve without significant advancements in language model technology or a change in approach. Additionally, despite the innovative integration of GPT-4, the current state of long video generation still leaves much to be desired in terms of visual continuity and narrative coherence, indicating a need for further refinement and development."
            },
            "questions": {
                "value": "See the weakness above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8416/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8416/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8416/Reviewer_dMSD"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8416/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698839759497,
        "cdate": 1698839759497,
        "tmdate": 1699637048348,
        "mdate": 1699637048348,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "854tBILTTY",
        "forum": "5PkgaUwiY0",
        "replyto": "5PkgaUwiY0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8416/Reviewer_GLr8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8416/Reviewer_GLr8"
        ],
        "content": {
            "summary": {
                "value": "This paper elegantly deconstructs the process of video generation into two distinct phases: planning and execution. In the planning stage, GPT-4 takes on the director's role, meticulously crafting scene descriptions, arranging entities with their corresponding layouts, setting the backdrop for each scene, and ensuring consistency among the groupings of entities and backgrounds. Following this intricate planning process, the baton is passed to the video generation model, Layout2Vid. Leveraging the foundation laid by the pre-trained ModelScopeT2V, a Gated Self-Attention module has been fine-tuned, enabling the direct input of text, images, and layout as conditions for manifesting videos. Remarkably, it transcends the boundaries set forth by ModelScopeT2V, excelling in metrics such as accuracy and the spatial placement of generated entities."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The task of video generation has been dissected, enabling the application of LLM's knowledge into the realm of video creation.\n2. The method of guiding video generation through layouts has been introduced to the task of video creation.\n3. The training process of Layout2Vid is notably efficient."
            },
            "weaknesses": {
                "value": "1. As per the CLIPSIM indicator in Table 2 and Appendix F, even with the employment of the costly GPT-4 for video planning, it still lags behind Make-A-Video and VideoLDM, and is even outperformed by ModelScopeT2V. This suggests that there may be certain issues with Layout2Vid's model fine-tuning method.\n2. As a T2V model, Layout2Vid's serious oversight lies in its failure to compare the FVD metrics with other models on UCF-101, a benchmark commonly used for T2V tasks.\n3. While the paper claims its ability to generate long videos, it merely compares model capabilities with ModelScopeT2V within its own VideoDirectorGPT framework. This seems more like a comparison between Layout2Vid and ModelScopeT2V rather than a qualitative or quantitative comparison with other long video generation models such as Phenaki and NUWA-XL.\n4. As a generation task, qualitative comparisons are crucial. However, the qualitative comparison in the paper only presents results between Layout2Vid and ModelScopeT2V, which is evidently inadequate.\n5. The techniques employed on the Layout2Vid model are primarily based on ModelScopeT2V and GLIDEN, which is not novelty enough."
            },
            "questions": {
                "value": "1. The human evaluation results presented in Table 5 leave us wondering about the number of people involved in rating, as well as the fairness and reliability of the process?\n2. Do you think that merely fine-tuning the Gated Self-Attention module might substantially diminish the original potent T2V generation capabilities of ModelScopeT2V?\n3. The accesibility to GPT-4 is not always possible. If other open-sourced LLMs are used, would they still be able to generate video plans of the same high quality? Moreover, are the same prompts still effective for other LLMs?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8416/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698857291297,
        "cdate": 1698857291297,
        "tmdate": 1699637048205,
        "mdate": 1699637048205,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "n58jl7XDA0",
        "forum": "5PkgaUwiY0",
        "replyto": "5PkgaUwiY0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8416/Reviewer_tZEz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8416/Reviewer_tZEz"
        ],
        "content": {
            "summary": {
                "value": "The paper presents VideoDirectGPT, a novel framework for generating consistent multi-scene videos by leveraging large language models (LLMs) for video content planning and grounded video generation. It expands a text prompt into a 'video plan' using an LLM, enabling explicit control over spatial layouts and ensuring temporal consistency across scenes, achieving improved video quality and movement control."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "In a word, the paper proposes a straightforward solution for video generation by leveraging GPT planning capability and pretrained text-to-video model."
            },
            "weaknesses": {
                "value": "1. How does the unclip prior affect the video quality and text image alignment?\n\n2. What if representations are not shared?\n\n3. Except for using GPT an the planner, the novelty is quite limited.  In particular, compared with both GLIGEN and ModelScopeT2V, the only contribution is unclip prior?\n\n4. Have the authors tried only finetune gated self-attn layer? What does the performance look like?\n\n5. Does the baseline ModelScopeT2V use the same dataset as the papers uses for fine-tuning?\n\n6. Comparing ModelScopeT2V and the proposed method on the actionbench-direction prompts, the object is actually not moving. Quality is not good as the baseline. And the advantage of text guidance actually comes from layout control, making the method contribution rather limited."
            },
            "questions": {
                "value": "Please see above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8416/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698962564208,
        "cdate": 1698962564208,
        "tmdate": 1699637048070,
        "mdate": 1699637048070,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "atMBLroJ3f",
        "forum": "5PkgaUwiY0",
        "replyto": "5PkgaUwiY0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8416/Reviewer_pdWu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8416/Reviewer_pdWu"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a two-stage text-to-video generation framework, consisting of video content planning and grounded multi-scene video generation. The first module employs a large language model (LLM), such as GPT-4, to generate a video plan. The second module, trained with image-level layout annotations, generates a consistent multi-scene video given the video plan. The authors conduct various evaluations to demonstrate the effectiveness of their work."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed framework achieves high efficiency by not requiring video training data and maintaining good results with 87% of total parameters fixed.\n2. The authors develop several novel evaluation methods that provide solid comparisons between the proposed framework and previous works.\n3. The framework uses both high-level and low-level conditioning to enable fine-grained control over generated videos.\n4. Intuitively and effectively, the framework uses shared features for the same subject across different scenes to ensure multi-scene consistency."
            },
            "weaknesses": {
                "value": "### A. Main paper\n1. This paper is somewhat too abstract throughout. The introduction is adequate, but I would expect to see more technical content in the following sections. For example, the loss function for the proposed image fine-tuning is not provided. Additionally, there is little evidence to support the correctness of the proposed methods beyond empirical results.\n2. The introduction to the datasets is limited. Some datasets provide fine-grained descriptions for each scene, while others only provide a single sentence for an entire video. Furthermore, the authors customize the Pororo-SV dataset by replacing character names with pronouns, but they do not justify this procedure. The lack of a clear explanation of the datasets makes it difficult to understand the task, such as the inputs and outputs for training and testing, and whether a large language model (LLM) is used.\n3. An ablation study should be conducted to demonstrate the effectiveness of the LLM. Additionally, if the LLM was used to refine prompts, these prompts should also be given to ModelScopeT2V to enable a fair comparison and provide readers with better insights.\n4. Since the consistency should be maintained regardless of the temporal distance between scenes, the authors should consider using the variance of CLIP features of all scenes instead of the average of similarities across adjacent scene pairs.\n5. The human evaluation does not have enough participants to provide reliable results.\n\n### B. Qualitative results\n1. The objects not exactly follow the bounding boxes.\n2. The \"pushing object\" video examples appear to show camera movement rather than object movement."
            },
            "questions": {
                "value": "1. How do you replace the original animation characters in Pororo-SV with real-world entities? Do the edited videos look natural enough? Why do you use pronouns to replace character names, and wouldn't this make it difficult for ModelScopeT2V to guess the correct content, leading to an unfair comparison?\n2. What is the exact loss function used for finetuning?\n3. Why are some numbers not available in the results, such as FVD and FID for Coref-SV and Consistency for HiREST?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8416/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8416/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8416/Reviewer_pdWu"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8416/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699554204474,
        "cdate": 1699554204474,
        "tmdate": 1699637047967,
        "mdate": 1699637047967,
        "license": "CC BY 4.0",
        "version": 2
    }
]