[
    {
        "id": "RaPKmTg6TQ",
        "forum": "wISvONp3Kq",
        "replyto": "wISvONp3Kq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission448/Reviewer_244k"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission448/Reviewer_244k"
        ],
        "content": {
            "summary": {
                "value": "This paper analyzes learning a generalized linear model in online scenarios. The paper proposes an algorithm and analyzes the performance, path of optimizer and regret. Specifically, this paper analyzes the action of adding and eliminating some data by an ODE, and the based on the KKT condition, it proposes the path of optimizer given by the implementation of the proposed algorithm. At the end the empirical study validates the algorithm.\n\nUpdate:\n\nThanks for the authors' comment. It makes sense to me, I raised the score to 8."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The algorithm is correct and concrete in maths, and the technique that treats the discrete actions of adding and removing data in a continuous space is normal and inspiring. It proposes a mathematical sound and practical algorithm and gives thorough analysis of the algorithm."
            },
            "weaknesses": {
                "value": "I do appreciate the discussion of the path of the optimizer in both inner loop and outer loop, but I'm still a little bit confused about how the notion of \u201cno-regret\u201d takes place, I would like to see more discussion about the final claim that can highlight the importance of the result."
            },
            "questions": {
                "value": "Lemma 3, eq 27, Why is there a minus sign in the second equation? However the proof below from eq 28 is correct.\n\nIn Assumption 1, shall the last assumption be there? If the function is symmetric about zero and has the first two derivatives, then the first derivative at zero must be 0, otherwise at the point 0 there is only subgradient but no gradient. However the examples, such as $\\ell_1$ norm, or the regularizers that encourage sparsity, are all non-smooth at 0, or, if you take out the absolute value thus it is linear, then it is not symmetric about 0, but instead $f(x) = -f(-x)$ right?\n\nAlgo 1, line 5, what does \u201csolve\u201d mean? Solve the whole trajectory of $\\hat \\beta$?\n\nPage 4 and later, I'm a bit confused about the set definitions and the elements in sets. For example, are $S_1,...,S_d$ elements of $S$, or $S = S_1 \\cup ... \\cup S_d$, is it consistent with the definition of $S_{\\bar Z}$ which says $j\\in S$? \n\n\"Formal statements ... in Appendix B.1\", what if place the formal theorem and proof sketch in main body and the detailed proof in appendix?\n\nShould Thm 2. be called continuity or Lipschitz?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission448/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission448/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission448/Reviewer_244k"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission448/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698015724324,
        "cdate": 1698015724324,
        "tmdate": 1700110934432,
        "mdate": 1700110934432,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CyzSFIwBqG",
        "forum": "wISvONp3Kq",
        "replyto": "wISvONp3Kq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission448/Reviewer_MPDs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission448/Reviewer_MPDs"
        ],
        "content": {
            "summary": {
                "value": "An online updating algorithm SAGO for generalized linear models based on solving ODE-PDE systems was proposed. It is proved that the algorithm has no regret. The merit of the SAGO is empirically tested  by comparing with ColdGrid (WarmGrid), ColdHyBi (WarmHyBi) and ColdByO (WarmByO). Improved accuracy was observed in numerical experiments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ Careful theoretical development"
            },
            "weaknesses": {
                "value": "- The proposed algorithm seems to require high computational resources, e.g. memory, making it incompetent with existing algorithms\n- Dimension of the datasets selected for empirical studies are relatively large, but are low in model dimension. It is unclear whether the proposed SAGO algorithm can handle modern datasets of interest"
            },
            "questions": {
                "value": "1. Comparing with existing methods ColdGrid, ColdHyBi, ColdByO:\n\n(a) It would be good to present the computational time SAGO and benchmark with the existing methods\n\n(b) Comparing the active sets f => do the methods agree on which ones are active?\n\n2. Minor: above Eq. (2), it should probably be $\\Sigma_{\\{i|({\\bf x_i}, y_i) \\in X^\\diamond\\}}$ in the definition of $\\ell^{\\diamond}$. Same applies to $\\ell^{-}$ and $\\ell^{+}$.\n\n3. In Algorithm 1, line 16-19: the thresholding condition is based on Lemma 1, which seems to only apply on $\\ell_1$ penalty. However, it is claimed that Algorithm 1 applies to penalties satisfying Assumption 1. There seems to be a gap and I wonder how the thresholding condition can be generalized to a wider class of penalty functions\n\n4. It's unclear how a coefficient can become active from inactive in SAGO in Algorithm 1, i.e. E1 on line 18-19. \n\n(a) The ODE in Eq. (5) describe the dynamics of coefficients in \"active\" set, so solving (5) doesn't seem to provide an update for the coefficients in inactive sets?\n\n(b) When applying the thresholding condition, should one replace the $L$ in Lemma 1 by the validation loss $\\mathcal L_{val}$? To make things concrete, probably it's better to repeat explicitly the thresholding condition in line 18-19.\n\n5. Minor: line 25 of Algorithm 1: should probably be arg zero."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission448/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission448/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission448/Reviewer_MPDs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission448/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698607922000,
        "cdate": 1698607922000,
        "tmdate": 1700546605852,
        "mdate": 1700546605852,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OGAv9TfXu2",
        "forum": "wISvONp3Kq",
        "replyto": "wISvONp3Kq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission448/Reviewer_oT9i"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission448/Reviewer_oT9i"
        ],
        "content": {
            "summary": {
                "value": "In this work, authors study GLMs with varying observations. Specifically, they develop an algorithm that can train optimal GLMs by only considering newly added (and removed) data points. This alleviates the need to train the entire model from scratch by only focusing on varied observations. They provide theoretical guarantees for their claim and conduct empirical studies to validate their theoretical contributions. Their algorithm outperforms the baselines in the numerical experiments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "I am not an expert in this specific research area, and it's important to take my opinions into consideration with that in mind.\n\nThe paper is well-written and presented. The theoretical results seem sound and the approach seems novel. As I perceive it, the introduction of the proposed bilevel optimization, coupled with the utilization of ordinary differential equations (ODEs) and partial differential equations (PDEs) to address these problems, represents a novel and inventive approach. While I have not conducted an exhaustive review of the mathematical details presented in the supplementary material, the findings in the main paper hold promise."
            },
            "weaknesses": {
                "value": "Please see questions."
            },
            "questions": {
                "value": "How easy is it to optimally solve the PDE-ODE procedure in both inner and outer problems? Are there existing solvers that solve them optimally with provable convergence guarantees and what are their computational/sample complexities? These can be answered for the models used in empirical study, i.e., for Sparse Poisson Regression and Logistic Group Lasso."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission448/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission448/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission448/Reviewer_oT9i"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission448/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699010643967,
        "cdate": 1699010643967,
        "tmdate": 1699635971061,
        "mdate": 1699635971061,
        "license": "CC BY 4.0",
        "version": 2
    }
]