[
    {
        "id": "fdVhCcV58X",
        "forum": "PcxQgtHGj2",
        "replyto": "PcxQgtHGj2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1152/Reviewer_jxkz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1152/Reviewer_jxkz"
        ],
        "content": {
            "summary": {
                "value": "Reid et al. (2022) demonstrated that pre-training a Decision Transformer (DT) on a Wikipedia corpus can substantially improve its performance on downstream Deep Reinforcement Learning (DRL) tasks. This paper explores whether a synthetic pre-training corpus can act as a substitute for the Wikipedia corpus. The main finding is that synthetic data generated using a one-step Markov Chain with a state space of 100 states and 75% fewer updates outperforms Wikipedia for pre-training. Additionally, the performance is relatively unaffected by the order of the Markov Chain or the size of the state space. A softmax temperature of 1.0 yields the best results. IID data performs marginally worse than Markov Chain samples, but still outperforms both no pre-training and pre-training with Wikipedia. For conservative Q-learning (CQL), synthetic pre-training data generated using a Markov Decision Process (MDP) leads to significant improvements over no pre-training. Similar to Decision Transformer, the best performance for CQL is achieved using a state space with a size of 1000, a softmax temperature of 1, and 100k updates. IID data performs slightly worse than MDP data, but still outperforms no pre-training."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* Presents empirical evidence that contradicts the prevailing belief that language data is essential for pre-training models for offline Deep Reinforcement Learning.\n\n* Demonstrates the benefits of synthetic pre-training data for both transformer and Q-learning based approaches to offline DRL.\n\n* Reports ablation studies to investigate the influence of various parameters such as the size of the state space, temperature, and order of the Markov Chain."
            },
            "weaknesses": {
                "value": "* The paper does not investigate the impact of the number of updates during fine-tuning, which is kept constant at 100k for DT and 1M for CQL. It would be useful to understand the relationship between the parameters of the synthetic data and the number of updates in fine-tuning.  This has the practical implication that fine-tuning data is typically task-specific and its availability may be severely limited. Alternatively, computational constraints may limit the fine-tuning budget.\n* The paper does not report the performance of the baseline (DT/CQL) if it is run for x more updates (where x is 80K for Wikipedia and 20k for the proposed synthetic data)."
            },
            "questions": {
                "value": "* How does the optimal configuration of synthetic data vary as a function of the number of fine-tuning updates?\n* What is the performance of the baseline (DT) if it is run for an additional 80k updates on Wikipedia or 20k updates on the proposed synthetic data? This would provide us a side-by-side comparison of the 3 models when run for the same number of updates."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1152/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1152/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1152/Reviewer_jxkz"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1152/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698187276355,
        "cdate": 1698187276355,
        "tmdate": 1699636041461,
        "mdate": 1699636041461,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YdIKAupbpj",
        "forum": "PcxQgtHGj2",
        "replyto": "PcxQgtHGj2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1152/Reviewer_WUkY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1152/Reviewer_WUkY"
        ],
        "content": {
            "summary": {
                "value": "This paper explored pre-training Transformer (DT) with synthetic data. They found that pre-training with synthetic IID data can match the performance gains from pre-training on large-scale language data. \n\nThe authors also apply the pre-training methods into the conservative Q-learning (CQL) framework. Experimental results show that pre-training with IID data and Markov decision process data can improve the performance of CQL."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper proposes a simple yet effective pre-training method with synthetic data for Decision Transformer.\n\n2. Results demonstrate that the proposed pre-training method with CQL can achieve significant improvements."
            },
            "weaknesses": {
                "value": "1. The experiments lack comparison for some pre-trained DT models, such as Future-conditioned Unsupervised Pretraining for Decision Transformer(https://proceedings.mlr.press/v202/xie23b/xie23b.pdf).\n\n2. It is more convincing to evaluate the proposed methods on more tasks."
            },
            "questions": {
                "value": "How much do different synthetic data construction methods affect the results?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1152/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698752996209,
        "cdate": 1698752996209,
        "tmdate": 1699636041374,
        "mdate": 1699636041374,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dB9dt84aHw",
        "forum": "PcxQgtHGj2",
        "replyto": "PcxQgtHGj2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1152/Reviewer_dSfS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1152/Reviewer_dSfS"
        ],
        "content": {
            "summary": {
                "value": "The manuscript presents a unique approach to pre-training for offline reinforcement learning, utilizing synthetic data generated by a Markov Chain in lieu of traditional real-world language resources. The core premise is that this synthetic data can achieve comparable results to real-world data in downstream task performance, which is a significant assertion in the field of offline RL."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Clarity of Presentation: The paper is well-structured, making it accessible even to those who may not be deeply versed in the domain. The significance of the research question is conveyed effectively, which facilitates a quick grasp of the paper's importance.\n\n2. Innovation in Data Construction: The methodology employed for the generation of synthetic data is both novel and straightforward, potentially offering a simpler alternative to more complex data generation strategies."
            },
            "weaknesses": {
                "value": "1. Methodological Justification: The rationale behind the adoption of a Markov Chain for synthetic data generation requires further elaboration. While the introduction suggests that understanding the underlying question is crucial for enhancing pre-training in deep reinforcement learning (DRL), the link between this understanding and the proposed method is not convincingly established.\n\n2. Need More Deep Analysis: The paper primarily demonstrates the efficacy of the proposed method without a robust analysis. It is advisable that the authors consider incorporating analysis akin to those found in the literature regarding Synthetic Data utilization in Transformer models (see Synthetic Pre-Training Tasks for Neural Machine Translation (ACL 2023) and its related works). This could potentially refine the proposed method and offer deeper insights through a more comprehensive analysis."
            },
            "questions": {
                "value": "1. Could you provide a more detailed justification for the methodological choices, specifically the use of a Markov Chain for data synthesis?\n\n2. Are there illustrative examples of synthetic data that could be shared to better understand its characteristics and how it compares to real-world data?\n\n**After Rebuttal**\nThank you for the response. I have raised my score from 5 to 6 and my confidence is 2."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1152/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1152/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1152/Reviewer_dSfS"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1152/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699277257448,
        "cdate": 1699277257448,
        "tmdate": 1700819916999,
        "mdate": 1700819916999,
        "license": "CC BY 4.0",
        "version": 2
    }
]