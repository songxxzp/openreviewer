[
    {
        "id": "nXzUMLloLy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission300/Reviewer_1cdx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission300/Reviewer_1cdx"
        ],
        "forum": "1SIBN5Xyw7",
        "replyto": "1SIBN5Xyw7",
        "content": {
            "summary": {
                "value": "This work proposes a transformer-based spiking neural network, Meta-SpikeFormer, that can achieve 1) low power; 2) versatility; and 3) high accuracy by a meta architecture. This work explore the impact of structure, spike-driven self-attention, and skip connection on the performance of Meta-SpikeFormer. On ImageNet-1K, Meta-SpikeFormer achieves 80.0% top-1 accuracy (55M), surpassing the current state-of-the-art (SOTA) SNN baselines (66M) by 3.7%."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ This work proposes a transformer-based spiking architecture consisting of RepConv2, SDSA, and ChannelMLP.\n+ This work compares itself against prior SNN-based designs."
            },
            "weaknesses": {
                "value": "- The motivation of this work is weak. Why is SNN important?\n- A metric of the result is not clear. How is the \"power\" value computed?"
            },
            "questions": {
                "value": "1. Why is the SNN architecture interesting and important?\n\"On ImageNet-1K, Meta-SpikeFormer achieves 80.0% top-1 accuracy (55M)\" is not the state-of-the-art. There are many prior works which can outperform this result. see: https://paperswithcode.com/sota/image-classification-on-imagenet\n\nFor example, tinyVIT https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136810068.pdf\ntinyVIT can achieve top-1 accuracy 80\\% on imagenet-1k by only 5M parameters.\n\nAnother example is, efficientNet B3: https://proceedings.mlr.press/v97/tan19a/tan19a.pdf\nefficientNet B3 can obtain top-1 accuracy 81.1\\% on imagenet-1k by only 12M parameters."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission300/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697168325267,
        "cdate": 1697168325267,
        "tmdate": 1699635956496,
        "mdate": 1699635956496,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rxVp9qXCBa",
        "forum": "1SIBN5Xyw7",
        "replyto": "1SIBN5Xyw7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission300/Reviewer_JmyY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission300/Reviewer_JmyY"
        ],
        "content": {
            "summary": {
                "value": "The paper extends the previous Spike-driven Transformer into a meta-structure with new macro-level conv and new self-attention SNN block designs, achieving SOTA accuracies on four types of vision tasks with controlled model parameters."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Clear paper structure and easy-to-follow presentation.\n- Compared to the previous Spiken-driven Transformer, the proposed new one achieves the best accuracy.\n- Extensively evaluated on four vision tasks: image classification, event-based action recognition,  currently the largest event-based human activity recognition dataset), object detection, and semantic segmentation.\n- Novel new self-attention SNN implementation that contributes to accuracy gain."
            },
            "weaknesses": {
                "value": "* Only focus on ViT. Although the paper claims a general spike-driven Transformer, it only discusses vision tasks and elaborates on a Transformer design for vision tasks with a two-stage Conv Block. It would be more beneficial to add a discussion on how to extend to language tasks."
            },
            "questions": {
                "value": "* Can the authors comment on how to extend the proposed techniques to Transformer for language tasks?\n* Can the authors provide the training costs of the Spiking Transformer with vanilla Transformer models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission300/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission300/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission300/Reviewer_JmyY"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission300/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698727174087,
        "cdate": 1698727174087,
        "tmdate": 1699635956426,
        "mdate": 1699635956426,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "e5sBlvgO78",
        "forum": "1SIBN5Xyw7",
        "replyto": "1SIBN5Xyw7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission300/Reviewer_Aepg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission300/Reviewer_Aepg"
        ],
        "content": {
            "summary": {
                "value": "The authors describe a new spiking transformer architecture and\ninvestigate the performance on a number of benchmarks, such as image\nclassification (ImageNet-1k), object detection and activity\nrecognition. They based their architecture mainly on the previously published\nspike-driven transformer (Yao et al. 2023), however, more initial conv layers are\nadded, the attention mechanism is changed somewhat, as well as some\nother previously published aspects are incorporated (e.g. repconv, sepconv). The\nmodel improves the SNN result for ImageNet compared to earlier works,\nand shows good performances on the other benchmarks as well."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "In general, it is a well written paper discussing an improved model\narchitecture and showing its performance. While performance is indeed\nimproved the architecture seems very much in spirit of Yao et al. with\nsome tweaks (e.g. more CNN layers) and a changed attention (matrix\nproduct instead of hadamard, which however, has worse\ncomplexity).   Here, it is shown additionally that the\nmodel performs well on other tasks as well, which is,\nhowever, generally known for a transformer-like architectures and\nthus not very surprising. The comparison and ablations with different\nattention mechanisms and short-cut structures is interesting."
            },
            "weaknesses": {
                "value": "While the study is well done, the contributions are\nrather incremental as the bulk of the model architecture was\npresented in earlier papers."
            },
            "questions": {
                "value": "- The main argument and novelty of the paper seems to be the \"meta\"\naspect of the architecture. I am not following the argument made\nhere. What exactly is meant with \"meta\" here? Its broad versatility for\ndifferent datasets? \n\n- While the accuracy on ImageNet-1k is improved compared to Yao et\nal. (79.7\\% versus 76.3\\%), the energy consumption is almost 10x the\nnumber (52.5 versus 6.1 mJ, according to Table 1). Maybe this is due\nto the more complex attention algorithm (although in the ablation\ntable 5 only a moderate energy reduction from SDSA-3 -> SDSA-1 is\nmentioned)?  However, in section 4.1 the power numbers written look\nvery comparable between the models (11.9 versus 10.2mJ) since one is\napparently stated for T=1 and the other for T=4. Should one not\ncompare T=4 with T=4? Would be also helpful to discuss the 10x power\nincrease and the dependence on T."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission300/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission300/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission300/Reviewer_Aepg"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission300/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698770315964,
        "cdate": 1698770315964,
        "tmdate": 1699635956354,
        "mdate": 1699635956354,
        "license": "CC BY 4.0",
        "version": 2
    }
]