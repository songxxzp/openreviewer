[
    {
        "id": "y8otIgKw3w",
        "forum": "30N3bNAiw3",
        "replyto": "30N3bNAiw3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1869/Reviewer_4LjD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1869/Reviewer_4LjD"
        ],
        "content": {
            "summary": {
                "value": "Ths paper discusses Contrastive Analysis, a sub-field of Representation Learning that aims to distinguish common and salient factors of variation between healthy and diseased datasets. \nCurrent models based on Variational Auto-Encoders have shown poor performance in learning semantically expressive representations. \nIn contrast, Contrastive Representation Learning has shown significant advancements in various applications. \nThe proposed method, called Sep-CLR, leverages Contrastive Learning to acquire semantically expressive representations suitable for Contrastive Analysis by utilizing the InfoMax Principle and optimizing Mutual Information terms.\nThe paper provides both theoretical and experimental analysis."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The theoretical analysis is reasonable and easy to follow.\n2. The proposed method outperforms baselines by a significant margin on several datasets."
            },
            "weaknesses": {
                "value": "1. The submission format of the paper should change to ICLR 2024. It is ICLR 2023 now.\n2. It will be better to give some qualitative results on not only the mnist dataset but also X-ray or other real-application data."
            },
            "questions": {
                "value": "1. It would be better if higher-resolution images could be provided in Figure 1."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1869/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698592296108,
        "cdate": 1698592296108,
        "tmdate": 1699636117507,
        "mdate": 1699636117507,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vSktyz8D92",
        "forum": "30N3bNAiw3",
        "replyto": "30N3bNAiw3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1869/Reviewer_F95G"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1869/Reviewer_F95G"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel theoretical framework for Contrastive Analysis based on the InfoMax principle, leveraging Contrastive Learning to estimate the common and salient terms, and suggests a strategy to reduce the information leakage between the common and salient spaces. Specifically, the framework consists of two InfoMax terms for the common space and the salient space, and k-JEM for preventing information leakage. In addition, the authors propose a Supervised InfoMax term to disentangle the salient factors. \n\nThe key contributions are:\n\n1) Reformulating Contrastive Analysis under the InfoMax principle with two Mutual Information terms to maximize - one for common factors and one for salient factors unique to the target dataset. \n\n2) Leveraging Contrastive Learning losses to estimate these Mutual Information terms - retrieving InfoNCE for the common factors and proposing a new background-contrasting loss for the salient factors.\n\n3) Introducing a new strategy called k-JEM to maximize joint entropy for reducing information leakage between common and salient spaces.\n\n4) Extending the framework with a Supervised InfoMax term to disentangle salient factors when attributes are available. \n\nThe experimental results on 5 datasets show k-JEM outperforms other mutual information minimization techniques and significantly promotes separating salient factors and common factors.\n\nOverall, the proposed SepCLR framework and k-JEM regularization demonstrate strong empirical results for contrastive analysis on both visual and medical datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Here are some strengths of the paper:\n\n1. The proposed theoretical framework provides new insights into Contrastive Analysis by formulating it under the InfoMax principle and identifying key mutual information terms to estimate. This enlightens future work on estimating these terms for contrastive analysis. \n\n2. The paper proposes a strategy to disentangle target-specific attributes within the salient space in a supervised manner when attributes are available. This extends the framework's capabilities.\n\n3. The paper provides an extensive discussion and comparison of several mutual information variational upper bound methods (vCLUB, vUB, vL1out, TC) as well as the strategies of mutual information minimization and distribution matching for reducing information leakage.\n\n4. The derivation of the InfoNCE loss and its alignment and uniformity terms from the InfoMax principle is clearly explained, connecting contrastive learning and information theory foundations."
            },
            "weaknesses": {
                "value": "1.  The choice of metrics could also be expanded and analyzed in more detail. For example, the reasoning behind expecting certain accuracy scores is not fully clear. Furthermore, it is not comparable between accuracy of 0% and 20% for (digits,C) on CIFAR-10. \n\n2. The evaluation is limited to a small set of datasets and tasks. A more comprehensive evaluation on a wider variety of datasets and downstream tasks could strengthen the results. There is limited discussion of hyperparameter sensitivity and scalability to larger datasets. Analyzing the impact of key hyperparameters and demonstrating scalability would be useful.\n\n3. In the disentanglement experiment (Figure 2), some entanglement seems to remain between factors. Using quantitative metrics like MIG and DCI could help analyze this. The sprite changes in Figure 2(b) could also be explained. \n\n4. Some architectural and mathematical details are unclear:\n- The encoder architectures and whether they are independent could be specified. \n- The notation for views v and number of samples Nx, Ny could be clarified.\n- Formulas and descriptions could be expanded for readability."
            },
            "questions": {
                "value": "Here are some potential questions about the paper:\n\n1. How does Equation 7 constrain `s'` to be information-less? This equation seems to promote the embeddings to be uniformly distributed rather than constraining s' specifically. Some clarification on how the information-less hypothesis is enforced would be helpful. \n\n2. The alignment terms in Equations 4 and 6 look different - one uses a log summation inside the log, while the other does not. What is the reason for this difference in formulations between the common space alignment (Eq 4) and salient space alignment (Eq 6)? Some explanation or intuition here could help the reader understand."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1869/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698599797397,
        "cdate": 1698599797397,
        "tmdate": 1699636117429,
        "mdate": 1699636117429,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GEMagnvX1j",
        "forum": "30N3bNAiw3",
        "replyto": "30N3bNAiw3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1869/Reviewer_etAa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1869/Reviewer_etAa"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a technique called SepCLR that uses contrastive learning in order to learn representations that separate common elements from salient ones for the downstream task at hand. The authors examine the performance of their method in creating separate representations for these two elements, and demonstrate improvements over previous work on the same subject."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The method proposed by the authors is novel, as far as I am aware. Limiting the shared information between the common and the salient space explicitly is an interesting way to ensure that the two encoders model different aspects of the data, leading to less overlap in the information between the two encoders.\n\n- The authors have performed extensive experiments on a variety of datasets, and have also examined several different variations of their proposed method, as can be seen in Table 1. I also appreciate the fact that the datasets used are not only standard ones like MNIST or CIFAR-10, but also come from the medical domain (although I would also appreciate results involving more complicated datasets like CIFAR-100 or ImageNet, which have more object classes available).\n\n- I also appreciate the detailed analysis that the authors provide for the datasets used in the appendix."
            },
            "weaknesses": {
                "value": "- As far as I understand, there is an inherent limitation for the method in that knowing the labels for the target dataset is required during training. This limits the applicability of SepCLR in the unsupervised setting, which is also the one most commonly examined by contrastive learning works.\n\n- I believe that there are some issues with the proposed method, that I would be grateful if the authors could elaborate on:\n\n  - The authors make some decisions when designing the loss that go against what is commonly done in related contrastive learning papers. In particular, the loss they propose has the formulation of $L_{unif}$ as found in Wang & Isola [A], but the most commonly used formulation is that of InfoNCE, which differs in that the resulting loss is a sum of Log-Sum-Exp functions, instead of a single Log-Sum-Exp. Similarly, in the alignment term they use a formulation closer to $L_{out}$ from Supervised Contrastive Learning [B], but the same paper notes that another formulation that simply sums the inner products, named $L_{in}$, is better experimentally (the authors examine this in the appendix, but do not explain why they chose $L_{out}$). I would be grateful if the authors could elaborate on these design decisions.\n\n  - Related to the above, it seems that the alignment terms in the common space and in the salient space are different (and similar to $L_{out}$ and $L_{in}$ respectively). I would be glad if the authors could explain why this is the case.\n\n  - In Equation (7), the first term in the sums essentially forces the representations of the salient encoder to be far from the constant vector $s\u2019$. It\u2019s not immediately clear to me why this term is there - it doesn\u2019t seem to arise from optimizing $\\hat{H}(S)$, and the informationless hypothesis only comes into play in Equation (8). I think the authors need to explain this part a bit more.\n\n  - Finally, the zero mutual information constraint is somewhat misleading - I understand the point the authors make that minimizing $I(c;s)$ is not the best thing to do, but at the same time, the proposed method does not directly force $I(c; s) = 0$. There is no guarantee that maximizing $H(c,s)$ does not affect the maximization of $H(c) + H(s)$, nor that the final solution will have $H(c,s) = H(c) + H(s)$. I believe that the authors should be clearer about this point.\n\n- I also believe that some points regarding the presentation of the paper can be improved:\n\n  - Tables 1 and 2 contain several variants of SepCLR, but it is not clear what each of them signify. The authors should better explain the variants of SepCLR in this table.\n\n  - Section 4 seems out of place, as it does not come up later in the main paper, and is in fact extremely similar to Section E in the Appendix. I believe that this part should be moved away from the main paper, as currently it throws the reader off (despite the paper having good structure overall).\n\n- Finally, I believe that it would be good to include the baseline of simply training the model using the entirety of the dataset via e.g. SimCLR. While I\u2019m fairly sure that this will not perform as well, it\u2019s still something good to include to get a sense of why the two different encoders are necessary."
            },
            "questions": {
                "value": "I would be grateful if the authors could clarify the points I made above regarding the design decisions made for the method and the details of its formulation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1869/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1869/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1869/Reviewer_etAa"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1869/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698635825877,
        "cdate": 1698635825877,
        "tmdate": 1700675786569,
        "mdate": 1700675786569,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "eFnE8afrSc",
        "forum": "30N3bNAiw3",
        "replyto": "30N3bNAiw3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1869/Reviewer_fALd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1869/Reviewer_fALd"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a theoretically grounded approach based on contrastive learning, SepCLR, to separate salient features from common features given a weak supervision in the form of a target dataset that contains both salient and common features and a background dataset that contains only common features. A series of mutual information based objective functions and regularization terms are presented along with clear motivation and background behind each of the terms, and the approximate loss functions are derived to achieve these objectives. Experimental results on multiple vision and medical benchmarks demonstrate that a good separation of latents is achieved and the method out-performs prior work. Visualizations of retrievals support their experiments."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "This paper is well-organized and well-written. It covers the necessary background on contrastive analysis, provides the theoretical and intuitive motivations on their various objective functions and puts them in context with prior literature, provide the derivations, and also discuss their limitations well.\n\nThe main novelty lies in their information-theoritic formulation of the various objectives and more importantly, exploiting contrastive learning and other prior literature to make the various objectives tractable. Another novelty is that they proposed joint entropy maximization to prevent information leakage between the common and salient latents as opposed to mutual information minimization, as the latter strategy can lose information.\n\nResults in Table 1 and 2 show good latent separation on synthetic vision tasks, and that the proposed approach outperforms prior approaches. In addition, they also reveal that the proposed joint entropy maximization is better than other strategies to prevent information leakage. Table 3, 4, and 5 show similar results on the medical domain and the retrieval visualizations support their findings."
            },
            "weaknesses": {
                "value": "This paper presents several objectives, but it's unclear which objectives are important and how they work together. This makes it unclear for a practitioner to transfer the results to a different problem. So, I encourage the authors to ablate the different objectives. \n\nAnother weakness is that in Table .4, which is perhaps an important real-world application of the proposed approach, the improvements over prior work is not much. This is in contrast with other experiments i.e. Table 1, 2, 3, 5. The reasoning for this is unclear and also not provided.  \n\nNit: MMD abbreviates to maximum mean discrepancy and is referred in the paper as moment matching distance [1]. \n\n[1]: A Kernel Two-Sample Test https://jmlr.csail.mit.edu/papers/v13/gretton12a.html"
            },
            "questions": {
                "value": "1. Could you provide an ablation study on the various objectives and their impact?\n2. In Table 1, 2, what is the performance on the background dataset? Is the salient latent non-informative as it is supposed to be? Is the common term informative?\n3. Is the task switch from classification to regression affecting the proposed approach's effectiveness in Table 4?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1869/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1869/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1869/Reviewer_fALd"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1869/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698795381398,
        "cdate": 1698795381398,
        "tmdate": 1699636117261,
        "mdate": 1699636117261,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "90CMGlIehS",
        "forum": "30N3bNAiw3",
        "replyto": "30N3bNAiw3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1869/Reviewer_Btev"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1869/Reviewer_Btev"
        ],
        "content": {
            "summary": {
                "value": "The paper provides a novel concept of contrastive learning for separating common and silent patterns. The approach utilizes two encoders, one responsible for learning silent representation and one for common. The authors propose the criterion to train the model that is based on the InfoMax principle. Due to the fact the direct optimization of the criterion for the problem is difficult and general for the problem, they propose a set of assumptions that allow to training of the model directly using a gradient-based approach. The approach is evaluated using big number of use cases."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The presentation of the paper is good, and the work is clear and well-written. \n- The proposed method is very interesting and sounds good technically. The flow of the proposed solution seems to be accurate. The authors clearly formulate the problem and general criterion given by eq. 1. Further, they decompose each component and propose a well-justified form of the component for given encoders. \n- The experiments are well-motivated, and the results seem to confirm the hypothesis stated in this work. It is very beneficial that datasets are from a variety of domains, going beyond standard benchmarks."
            },
            "weaknesses": {
                "value": "- The model is designed only to model $p(c|\\cdot)$ and $p(s|\\cdot )$. It would be nice to see some approximation of the distribution over data $p(\\cdot|s,c)$. I think that the proposed architecture can be enriched with the decoder that models this probability. \n- I am not quite sure if setting the same architecture for the proposed and reference methods is a good approach. In my opinion, the best architecture for each individual method should be used in experiments. \n- Only VAE-based methods are used as reference approaches. VAEs are doing the additional jobs, they have decoders and serve as generative models, while SEPCLR is only learning the common and silent representation. It would be nice to see the comparison with the models from similar groups, either SEPCLR as VAE, or reference methods that do not preserve autoencoding properties."
            },
            "questions": {
                "value": "I would like to ask the authors to respond to the weaknesses section. I will also would like to ask about selecting KDE as a model for this case. Can KDE be replaced with the normalizing flow instead?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1869/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698859290089,
        "cdate": 1698859290089,
        "tmdate": 1699636117171,
        "mdate": 1699636117171,
        "license": "CC BY 4.0",
        "version": 2
    }
]