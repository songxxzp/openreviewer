[
    {
        "id": "Lf5hiJfkci",
        "forum": "sdkB5j7yNr",
        "replyto": "sdkB5j7yNr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission168/Reviewer_e7ug"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission168/Reviewer_e7ug"
        ],
        "content": {
            "summary": {
                "value": "The authors both propose a new Bayesian mechanism for certifications, and attempt to demonstrate the relative performance of different architectures."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The presented framework is interesting, and the introduction as presented takes a very unique perspective on the reasons why there is a critical need for research in the field of AI security."
            },
            "weaknesses": {
                "value": "While the idea within this work is interesting, I do not believe it has suitable rigorous experimentation (especially in terms of dataset diversity), or experimentation (does not follow standard expectations regarding the trade off between certification proportion and size that are common in other certification papers). While there is validity in a paper that demonstrates that a new approach has the potential to extend the ability of certifications to new frontiers - however, part of doing this kind of validation would require a comprehensive set of experiments demonstrating scaling and performance, all of which are missing. \n\nAlso one of the stated contributions of this work is to extend Randomised Smoothing from $\\ell_2$ to $\\ell_p$. However, this is missing a wide range of literature on $\\ell_p$ certifications in randomised smoothing, see Yang et. al \"Randomised Smoothing of All Shapes and Sizes\", 2020 as an example of this. \n\nI also worry that this paper is attempting to cover quite a few bases - it's trying to both introduce a Bayesian optimisation mechanism and to demonstrate that VIT's are more robust than other architectures. But in attempting to cover both of these points I believe that neither contribution is sufficiently addressed - the Bayesian mechanism is insufficiently detailed, implementation details are sparse, and the range of experiments (including datasets and metrics) are sparse relative to the level of experimental evidence to truly make these points. \n\nAs a few other notes:\n- Figure 1 talks about \"adversarial risk certification for various models under AutoAttack\" - the involvement of AutoAttack is only tangentially referred to within the document. Figures 1 & 2 don't even seem to be referenced in text? So there's no \n- Just as a note on page 3 there's no space between \"functions.Lipschitz\" in the sentence relating to Wong & Kolter.\n- Citation capitalisation is inconsistent - especially when it comes to the names of journals / conferences / venues. \n- The paper could do with algorithms and implementation details, even just in the appendices."
            },
            "questions": {
                "value": "What's the difference between the $(\\alpha, \\zeta)$ safety framework relative to something like Differential Privacy (as considered by Lecuyer)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission168/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission168/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission168/Reviewer_e7ug"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission168/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698486547312,
        "cdate": 1698486547312,
        "tmdate": 1699635942529,
        "mdate": 1699635942529,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rZ291zgnUz",
        "forum": "sdkB5j7yNr",
        "replyto": "sdkB5j7yNr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission168/Reviewer_yeRm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission168/Reviewer_yeRm"
        ],
        "content": {
            "summary": {
                "value": "This work introduces PROSAC, a method to certify a machine learning model's robustness against an adversarial attack type, regardless of the hyperparameters chosen for that adversarial attack. The claims are substantiated by experiments attacking a few vision models with benchmark attacks. This work also applies Guassian Process Upper Confidence Bound (GP-UCB) to hyperparameter selection during the certification process."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* (Moderate) The paper is well-motivated in showing the need for robustness certifications as ML models are used for increasingly critical areas and will likely be subject to more government regulations."
            },
            "weaknesses": {
                "value": "* (Major) The work's presentation overall is difficult for me to understand. This includes the use of undefined variables and terms in the writing and algorithms as well as hard to find experimental details. Details in questions.\n\n* (Major) In certifying the space of attack hyperparameters a model is robust to, it is unclear what hyperparameters are being varied in each attack and what hyperparameter values the work is certifying are safe. Also, this work seems to omit relevant attack hyperparameters from the certification, including attack budget and constraint norm. \n\n* (Major) The experimental results in section 5 seem to suggest the certification is unreliable. For example, Figures 1 and 2 show very different p-values for only slightly different hyperparameter values where I would expect p-values to be similar for similar hyperparameter values. An example of this is Fig 2c showing epsilon 0.0011 having a p-value above 0.8, epsilon of 0.0012 having a p-value of 0.0, then epsilon of 0.0013 having a p-value of 1.\n\n* (Moderate) Section 2 contains some confusing statements about prior work. Details in questions.\n\n* (Moderate) Theorem 4 seems to say that multiple rounds of approximation are needed to certify a machine learning model, but it does not quantify or attempt to make a statement about that number of rounds of approximation. Later in the text, there seems to be a sentence saying \"See Supplementary Material\" for this information. However, as it is core to this work's claim, at least a summary of the math required to estimate how many rounds should be executed is required."
            },
            "questions": {
                "value": "* What is the relation between this work and PAC (Probably Approximately Correct) learning / adversarial PAC learning? It seems the guarantees are very similar to those proposed in this work.\n\n\n* Regarding the results shown in Figures 1 and 2, I would have expected similar values of epsilon to have similar p-values, with a monotonic increase in p-value as epsilon increases. Why is this not the case and why is there so much variability in p-values? These large differences indicates an unreliable certification since a very small decrease or increase of epsilon can change the p-value from 0 to 1 (as between epsilon = 0.0012 and 0.0013 in Fig 2c).\n\n\n* Section 2 states \"... RS (randomized smoothing) is limited  to certifying empirical risk of a machine learning model on pre-defined test datasets under $l_2$-norm bounded adversarial perturbations.\" What is meant by a pre-defined test dataset and how is randomized smoothing restricted to it? How is PROSAC not restricted to it?\n\n* Section 2 states \"randomized smoothing (RS) represents a versatile certification methodology free from model architectural constraints or model parameters access\" but then later states \"Our certification framework shares RS\u2019s versatility but a) it also exhibits the ability to accommodate a diverse range of lp norm-based adversarial perturbations; b) it is not restricted to particular model architectures...\". These statements seem to first state that RS is free from model architectural constraints but then states it is restricted to particular model architectures. Do I misunderstand what is being said?\n\n* In Table 1, why is the hyperparameter field \"N.A.\" for AutoAttack? There are several hyperparameters that can be set (e.g., number of gradient steps, number of expecation over transformation estimates, the type of attack to execute, etc.).\n\n\n* Equation 10 is difficult for me to understand. What is $h_1$? Could a narrative be given for what this equation is saying?\n\n* In section 4.1, a footnote says that the attack budget and norm are not considered hyper-parameters because it would not be possible to control the risk if the adversary can choose any attack budget. This justification does not explain why the norm is not considered a hyper-parameter. Is there a reason the norm is not considered a hyperparameter?\n\n* In equation 7, shouldn't the risk no longer have a $\\lambda$ subscript?\n\n* In the paragraph below equation 5, I don't understand what is meant by \"We will be assuming in the sequel, where appropriate,...\" and later \"We will be representing in the sequel...\" What is the meaning of the word sequel here?... \n\n* In algorithm 1, what is $\\beta$ and what is $k$? Is $\\mu$ and $\\sigma$ the mean and variance of $\\lambda$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission168/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission168/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission168/Reviewer_yeRm"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission168/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698701764025,
        "cdate": 1698701764025,
        "tmdate": 1699635942420,
        "mdate": 1699635942420,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pqszKWijKI",
        "forum": "sdkB5j7yNr",
        "replyto": "sdkB5j7yNr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission168/Reviewer_z9fX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission168/Reviewer_z9fX"
        ],
        "content": {
            "summary": {
                "value": "This paper derives provable statistical guarantees on the adversarial population risk given an attack algorithm, by computing p-values. This paper also uses a Gaussian Process Upper Confidence Bound (GP-UCB) algorithm for certification against attacks with set of hyperparameter configurations."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* This paper derives statistical guarantees on the adversarial *population* risk, which is different from many previous methods for machine learning certification. \n* This paper considers that the attack algorithm is known, but it allows the hyperparameters of the attack algorithm to vary within a set of configurations, which is different from previous works on the population risk.\n* The proposed method is independent from model architectures, and the experiments applied the proposed method on models including ViT and ResNet."
            },
            "weaknesses": {
                "value": "* There are lots of existing works on machine learning's robustness certification. Those works have been mentioned in Section 2, but that is probably too late. The first section does not mention the robustness certification works which are not about population risks. It is unclear from the beginning of the paper how this work differs from the previous works, and title is also not sufficiently informative. \n* Compared to the existing machine learning certification algorithms that are independent from attack algorithms, the one proposed in this paper requires a specific attack algorithm, which is not applicable when an attacker uses a different attack algorithm but still follows the same threat model. Thus, the importance of such a certification scheme is unclear.\n* It is unclear what the computational cost is, and how the number of data samples may affect the results."
            },
            "questions": {
                "value": "* See the last weakness point."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission168/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission168/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission168/Reviewer_z9fX"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission168/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698819931721,
        "cdate": 1698819931721,
        "tmdate": 1699728819141,
        "mdate": 1699728819141,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dLYL8Hnsyp",
        "forum": "sdkB5j7yNr",
        "replyto": "sdkB5j7yNr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission168/Reviewer_sSbB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission168/Reviewer_sSbB"
        ],
        "content": {
            "summary": {
                "value": "This submission proposes a new approach to certify the performance of machine learning models against adversarial attacks, in the sense of asserting the model's population risk is lower than some threshold with high probability for a range of hyperparameters of a given attack. Experiments on a few image-based models and both white and black attacks demonstrate the effectiveness of the proposed approach."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The rigorous risk control is a critical problem in trustworthy machine learning, given the legal requirements. The submission tackles this problem with an effective approach.\n\n- The proposed approach is scalable in terms of certifying large ViT models, and experiments cover a wide spectrum of attack methods."
            },
            "weaknesses": {
                "value": "- The submission may not be rigorous enough. Especially, Theorem 4 only states that \"we can do sth by relying on Alg 1\". But how is the Algorithm 1's result used to derive the final guarantee in Eqn. (12)? As a certification approach, this process needs to be made more clear. Furthermore, GP-UCB provides maximized $p$ value under some latent assumptions if I understand Appendix D correctly. If this is the case, such assumptions should be inherited in the main theorem under which the certification holds.\n\n- The experimental evaluation is not quite clear and may lack some baselines. For inference, on page 8, the submission has the text \"We use $\\alpha = 0.10$ and $\\zeta = 0.05%$ in the safety certification\". However, most results in the paper are presented in terms of $p$ value. How are $p$ values connected with these fixed certification parameters?\n\n- The certification may be a bit limited compared to other $L_p$-norm-based certification, where this work can only guarantee the population risk for a certain type of attack but the existing literature can guarantee the risk for any attack within the perturbation budget. These constraints may need to be made clear.\n\nMinor typos:\n1. On Page 3, \"relies on ReLU activation\"\n2. On Page 6, \"Fix the machine learning model M, and fix ...\"\n3. On Page 9, \"to the default one in Croce & Hein ...\""
            },
            "questions": {
                "value": "See the questions and suggestions above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "I may not have the expertise to rigorously evaluate whether the proposed approach can fully align with the requirements of safety and trustworthiness from AI regulations and laws, and such alignment appears to be the main motivation for the proposed method."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission168/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699176873792,
        "cdate": 1699176873792,
        "tmdate": 1699635942216,
        "mdate": 1699635942216,
        "license": "CC BY 4.0",
        "version": 2
    }
]