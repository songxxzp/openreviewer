[
    {
        "id": "f7IootBKsy",
        "forum": "AhCdJ93Wmi",
        "replyto": "AhCdJ93Wmi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3002/Reviewer_rTyJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3002/Reviewer_rTyJ"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel method, SSL-GM, for graph inference acceleration. The primary idea is to bridge GNNs and Multi-Layer Perceptrons (MLPs) through Self-Supervised Learning (SSL) to integrate structural information into MLPs. SSL-GM employs self-supervised contrastive learning to align the representations of GNNs and MLPs. Besides, it uses non-parametric aggregation, augmentation, and reconstruction regulation to avoid potential model collapse, improve model training, and prevent representation shift, respectively. The extensive experiments demonstrate SSL-GM's empirical superiority over existing models in terms of both efficiency and effectiveness."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* It introduces a novel approach, SSL-GM, which integrates structural information into MLPs by contrastive learning between GNN and MLP outputs.\n* The empirical results over 10 datasets demonstrate the effectiveness of SSL-GM.\n* The paper is well-written and structured, with clear explanations of the methodology and theoretical insights."
            },
            "weaknesses": {
                "value": "* In this paper, the structure-aware ability of SSL-GM is supported by the objective $\\mathcal{L}_{cont}$. However, its success depends on the quality of the learned representations from the GNN, rather than solely on the capabilities of the student MLP. In other words, $\\mathcal{L}{cont}$ enforces the student MLP to output representations similar to the aggregated representations from the GNN but does not inherently empower the student with structure-aware abilities. For unseen nodes during training, the student MLP may encounter challenges in learning structure-aware representations without the supervision provided by the GNN.\n* It's worth considering whether addressing the representation shift issue is necessary. The absence of the reconstruction strategy only leads to a slight decrease in performance (as shown in Table 1, \"w/o Rec.\" column), indicating that this strategy may not be indispensable in some cases.\n* To enhance reproducibility and ensure accurate results, it would be helpful if the authors could provide detailed information about the experimental environment used for SSL-GM.  I used the Google Colab platform to rerun the source code but obtained $83.80_{\\pm0.46}$ in the Cora dataset compared to  $84.60_{\\pm0.24}$ in the paper."
            },
            "questions": {
                "value": "Please refer to the Weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3002/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3002/Reviewer_rTyJ",
                    "ICLR.cc/2024/Conference/Submission3002/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3002/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697686991111,
        "cdate": 1697686991111,
        "tmdate": 1700534704038,
        "mdate": 1700534704038,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1NbW5axGtZ",
        "forum": "AhCdJ93Wmi",
        "replyto": "AhCdJ93Wmi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3002/Reviewer_eCUd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3002/Reviewer_eCUd"
        ],
        "content": {
            "summary": {
                "value": "This paper considers the problem of accelerating GNN inference by training an MLP on the node features. It proposes to use self-supervised learning to train the MLP and achieves strong empirical results."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThe paper is well-written and easy to follow.\n2.\tThe empirical results are rich and significantly outperform the baselines."
            },
            "weaknesses": {
                "value": "1.\tTheorem 1 is possibly wrong. I check the proof in the appendix, which basically connects each term in (17) to each term in (13) and leads to two problems. (1) The authors seem that minimizing each term in (17) also minimizes the corresponding term in (13). Pls prove these conclusions rigorously instead of only using intuitively explanations. (2) Even if the first point holds, (13) minimizes the summarization of 4 terms, and thus the minimizer may not be the minimizers of the 4 terms. Thus, there is no guarantee that (13) and (16) will have the same minimizer. Nowadays, many machine learning papers have theorems but my opinion is that theorems should be rigorous. Moreover, I failed to follow the mutual information part of Section 5, especially how the authors transform different models into mutual information forms. If this can be done, pls conduct the mathematical transformations in rigorous ways.\n2.\tIt is unclear what are the challenges of using self-supervised learning (SSL) to train MLP and what are the new designs of the paper. SSL is widely used for graph learning, and the author should be very specific in the challenges of using it to train MLP. Currently, descriptions of the weak points of existing works, e.g., \u201cinsufficient exploration of structural information when inferring unseen nodes\u201d, \u201ccannot fully model the structural information\u201d are rather vague. In section 3, the authors propose several techniques and loss terms, e.g., alignment loss, data augmentation, and reconstruction loss. These are not new for SSL, which are also evidenced by the citations provided by the authors. The question is that what are the new things proposed by the authors. The paper will be stronger if the authors can connect the challenges and the proposed new techniques. \n3.\tExperiments can be improved. (1) Pls provide the time for model training, which is also an important practical consideration. (2) Pls run the experiments on large practical datasets, e.g., Papers100M, MAG240M, and IGB.\n====================================================\nI have reade the author response. Instead of addressing my concerns, the author response raises more concerns. As such, I decide to lower my rating."
            },
            "questions": {
                "value": "NA"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3002/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3002/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3002/Reviewer_eCUd"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3002/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698114315608,
        "cdate": 1698114315608,
        "tmdate": 1700635475944,
        "mdate": 1700635475944,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Kcqsz1XW5o",
        "forum": "AhCdJ93Wmi",
        "replyto": "AhCdJ93Wmi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3002/Reviewer_WTGk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3002/Reviewer_WTGk"
        ],
        "content": {
            "summary": {
                "value": "This paper studies an important problem: how to accelerate graph inference and improve model generalization. For this paper, the authors propose SSL-GM to bridge graph context-aware GNNs and neighborhood dependency-free MLPs with SSL. In addition, the authors also provide theoretical analysis to prove the generalization capability of SSL-GM. Furthermore, the extensive experimental results show that the solution mentioned in this paper not only accelerates GNN inference but also exhibits significant performance improvements over vanilla MLPs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The problem studied in this paper is fundamental in the graph neural network area.\n2. The solution mentioned in this paper is basic and the performance of the method mentioned in this paper is great. The experimental results are extensive.\n3. This paper develops a theoretical analysis.\n4. The presentation of this paper is so clear that I can follow the paper easily."
            },
            "weaknesses": {
                "value": "1. The experimental results only contain the performance over node classification and graph classification. Is it possible to evaluate the proposed method over link prediction?\n2. It seems that this paper only borrows some ideas from contrastive learning. Based on contrastive learning, this paper develops a new objective function that can be used to solve the model generalization problem. Therefore, could the authors highlight some contributions here? I think it is a good paper but the contribution of this paper is a little bit marginal. If the authors are able to emphasize their contributions here, I am willing to improve my rate."
            },
            "questions": {
                "value": "See Strengths and Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3002/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698271146552,
        "cdate": 1698271146552,
        "tmdate": 1699636244665,
        "mdate": 1699636244665,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GhwmkKrn3X",
        "forum": "AhCdJ93Wmi",
        "replyto": "AhCdJ93Wmi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3002/Reviewer_N5QN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3002/Reviewer_N5QN"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the problem of Graph inference acceleration and summarizes two shortcomings in existing work: limited acceleration effectiveness and insufficient generalization performance. Based on insights from existing work, it is suggested that self-supervised learning can be used to infer structural information of unseen nodes from the nodes themselves. The paper introduces the SSL-GM algorithm, primarily aligning the consistency between GNN and MLP representations through self-supervised contrastive learning. This bridges GNN and MLP with self-supervised learning to achieve accelerated graph inference."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The problem is novel, the challenge of accelerating graph inference still persists.\n2. The proposed algorithm demonstrates favorable performance in multiple experimental validations."
            },
            "weaknesses": {
                "value": "1. Although a significant number of experiments were conducted, the novelty and contribution of the proposed method remain limited.\n2. In Section 3.1, the author introduces the Non-Parametric Aggregator to help align the representations of GNN and MLP. While the author explains the differences in the appendix, the aggregation method given in Equation 2 still resemble the form of APPNP. I did not find an explanation for this issue in the experimental section and other where of the paper. The author claims that, in contrast to SGC and APPNP, SSL-GM uses non-linear adjacency matrix aggregation instead of high-order adjacency matrices. So, from the perspective of acceleration effectiveness and improvement in generalization, what is the contribution of non-linear adjacency matrix aggregation to accelerating graph inference?\n3. I cannot understand Formula 4. The author injects randomness by perturbing the structure and features of the original graph, with the expectation that the MLP encoder can capture invariant key features. However, Formula 4 is perplexing. The first part of the formula computes the mutual information between G_1 and G_2, but is it merely a distinction of whether the perturbed structure is included? Why does this part enable the encoder to obtain high-quality representations? It seems more like an optimization of the random augmentation methods q_e and q_f, but the author did not clarify whether they are learnable.\n4. Given the method proposed in the paper, I believe it would be interesting to include GNN methods like SGC and APPNP in the experiments concerning acceleration effectiveness."
            },
            "questions": {
                "value": "Please see the comments in the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3002/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698752513172,
        "cdate": 1698752513172,
        "tmdate": 1699636244593,
        "mdate": 1699636244593,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uEzrv6KHAe",
        "forum": "AhCdJ93Wmi",
        "replyto": "AhCdJ93Wmi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3002/Reviewer_RVGt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3002/Reviewer_RVGt"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new method called SSL-GM, which integrates structural information into MLPs by connecting GNNs and MLPs using SSL. This method can accelerate graph inference and improve the generalization capability, resulting in a favorable balance between accuracy and inference time. Experimental results show that the method performs well in node classification tasks and is effective in accelerating inference. In addition, many theoretical analyses and corresponding experiments flesh out the work."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper propose a new method, which can accelerate GNN inference and performs well in node classification tasks over the state-of-art models.\n2. There are detailed theoretical analyses and corresponding experiments of SSL-GM ,which fleshes out the article and provides inspiration for the innovations that follow.\n3. This work has comprehensive and detailed experiments, which validates the performance and efficiency of SSL-GM."
            },
            "weaknesses": {
                "value": "1. The article lacks sufficient innovation and is merely a combination and application of existing methods, such as Bootstrap loss, SGC, graph augmentation, and reconstruction.\n2. In section 4.3, there is only 'Figure 3' to demonstrate the capability of SSL-GM for inference acceleration. However, it is necessary to provide detailed experimental results regarding accuracy and inference time. These results should be obtained from a wider range of datasets and classification settings.\n3. In section 3.3, it would be better to introduce representation shift in detail and explain how reconstruction helps in mitigating representation shift."
            },
            "questions": {
                "value": "1. Although this work is a combination of existing methods, it is fascinating to introduce this model from a higher level rather than loss function level. \n2. In B.3, learning rates is misspelled where \u20185e4\u2019 appears twice."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3002/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3002/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3002/Reviewer_RVGt"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3002/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698765691475,
        "cdate": 1698765691475,
        "tmdate": 1699636244517,
        "mdate": 1699636244517,
        "license": "CC BY 4.0",
        "version": 2
    }
]