[
    {
        "id": "wqhDkLwY7f",
        "forum": "S24zdyiWDT",
        "replyto": "S24zdyiWDT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7852/Reviewer_cGXj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7852/Reviewer_cGXj"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the problem of inverse reinforcement learning: given observations of an expert policy deployed in a (tabular) MDP, what reward function can one learn which is consistent with the actions of the expert policy? They consider both the offline (batch) setting as well as the online setting. They propose a metric for inverse reinforcement learning, defined in terms of the difference between the learned and ground truth rewards.\n\nFor the offline setting, they propose a pessimistic algorithm and show it achieves poly(S, A, H) sample complexity, as well as some dependence on the concentrability coefficient. For the online setting, they propose an algorithm which also achieves poly(S, A, H) sample complexity (and uses the offline algorithm as a subroutine). There is also an extension to a \"transfer learning\" setting."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is thorough, detailed, and mathematically rigorous. (As a disclaimer, I only checked in detail the proofs and associated lemmas for Thm 4.2, their main result on the sample complexity of the offline algorithm; but I am familiar with most of the literature and techniques used for online/offline tabular RL, so the results seemed correct to me.)\n- To me, the most interesting contribution is the new notion of \"performance\", which is measured in terms of a uniform distance for a set of \"reward mappings\" (Defn 2.1). I think this notion of metric merits future discussion and is a valuable contribution to this area. However, I have some comments/questions about it - see below.\n- On the algorithmic side, this paper also shows how several existing techniques (i.e., pessimism, reachable state identification / reward free exploration) can also be adapted to the IRL problem. While the algorithms and analysis themselves are not particularly novel, it is nice that we can use well-studied techniques in RL for the IRL problem."
            },
            "weaknesses": {
                "value": "- Given that this paper proposes new metrics for IRL, I found the discussion / comparison with previous work a bit vague. At times, the language and writing was a bit informal, and sometimes confusing to interpret. \n    - For example, in the appendix you write that \"our method that considers is greater than theirs\". What does it mean for a method to be greater than another method? Did you mean your metric?\n    - You write in C.3 that the \"metric can't capture transitions\". What does this actually mean?\n- I would urge the authors to rewrite their comparison with prior work in Appendix C, focusing on cases where one subsumes the other, and giving concrete examples when your algorithm(s) can achieve guarantees while previous algorithms cannot. What is currently lacking is a distinction in the writing between (1) comparing the quantities $d^\\pi$ and $d^\\mathrm{all}$ themselves to prior work; (2) comparing guarantees that your algorithms can achieve to guarantees that algorithms from prior work can achieve.\n- (minor) weakness: the upper/lower bounds seem to be loose. In particular, it would be good if the authors commented on the fact that the lower bound has a $\\min \\{S, A\\}$ term - where does this come from? can it be improved? In general, the regime that we care about is when $S \\ge A$, so the fact that the rate is only sharp when $S \\le A$ is not very meaningful.\n\nMinor comments:\n- Many typos throughout, especially in the appendix. Some examples:\n    - \"Broaderly\" in the first paragraph.\n    - e.g., at bottom of page 4, \"Given a policy $\\pi$, We\" -> lower case the \"we\".\n    - a missing citation before Corollary 4.3? I see an \"()\".\n    - \"week transferability\" should be \"weak transferability\"?\n    - some typo in the equation (C.6).\n    - At the beginning of page 23, in the first display equation, should it be $a \\in \\mathrm{supp}$ instead of $a \\notin \\mathrm{supp}$?\n- For lower bounds, use $\\Omega$, not $O$.\n- In the algorithm pseudocode, a quantifier over the set $\\Theta$ is missing: The algorithm needs to be run separately for every $\\theta \\in \\Theta$."
            },
            "questions": {
                "value": "1. I'm a bit confused about the discussion of the Hausdorff metric used by Metelli et al. The quantity that they propose seems to be some notion of diameter. I'm not sure why it should go to zero as you collect more samples, as even when $\\mathcal{R} = \\widehat{\\mathcal{R}}$ the quantity doesn't seem like it should be zero since the set of possible rewards could be large. But your results imply that an upper bound on this quantity goes to zero, so what am I missing here?\n2. While the definition of the metric seems mathematically well defined, it seems a bit weird: to define the reward mapping, you need a value and advantage function (V,A) (e.g., Eq. 3.2). However, inside the definition of the metric, you define a new value function of a particular policy and reward function (e.g., in Eq. 3.1). How do these two value functions relate?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7852/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7852/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7852/Reviewer_cGXj"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7852/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698697736220,
        "cdate": 1698697736220,
        "tmdate": 1699636962619,
        "mdate": 1699636962619,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QXmI5G1Zf4",
        "forum": "S24zdyiWDT",
        "replyto": "S24zdyiWDT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7852/Reviewer_uRxz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7852/Reviewer_uRxz"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes RIP and RLE method which opterates in offline and online IRL settings respectively. These methods do have some theoretical guarantees."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper builds metrics for both online and offline IRL settings. \n2. Informed by the pessimism principle, RLP is proposed for offline IRL setting with theoretical guarantees. \n3. RLE achieves great sample complexity compared to other online IRL methods."
            },
            "weaknesses": {
                "value": "Lack of some experiment results.\n\nI think the theoretical analysis for IRL is important. However, the title of this paper is a little confusing. The sample complexity analysis seems not relevant to this title."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7852/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7852/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7852/Reviewer_uRxz"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7852/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698720343308,
        "cdate": 1698720343308,
        "tmdate": 1699636962500,
        "mdate": 1699636962500,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rFXSqdA6bw",
        "forum": "S24zdyiWDT",
        "replyto": "S24zdyiWDT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7852/Reviewer_RnbG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7852/Reviewer_RnbG"
        ],
        "content": {
            "summary": {
                "value": "This paper gives theoretical guarantees for both online and offline setting in the inverse reinforcement problem."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I think this work really pushes the inverse RL community research efforts further by answering:\n\n> can we give theoretical guarantees for the inverse RL algorithm enforced by pessimism for offline and exploration for online settings?\n\nThis is a really nice idea worthy for publication. My score reflect the weaknesses."
            },
            "weaknesses": {
                "value": "Firstly note that this is an emergency review. I will rely on discussion with authors and reviewers, and other reviews, to decide on the score.\n\nI have only a few weaknesses for this work as follows:\n\n- The main paper writing needs to be improved. Yes, the soundness of this paper maybe good, considering similar pessimism and exploration ideas from past works. But this submission looks like a hurried submission with many typos like `Theoretical low bound in offline setting.`, ` $\\pi_b = \\pi_E ().,$`, `RLP utilizes empirical MDP and Pessimism frameworks`, `for all offline IRL problems with probability at least $1 \u2212 \\delta$, has to take at least ...\u0001 samples in expectation.` (both h.p. and in expectation?!), and so on. In addition to typos, the out-of-margin equation formatting makes a strenuous reading experience. To be honest, I am not sure if the authors can fix this writing issue of 56 pages during the rebuttal period, but I will welcome some attempts since proceedings require good quality.\n\n- The closest work I can think of is [1] that provides theoretical guarantees for imitation learning ($\\approx$ reward-free IRL+RL) in both online and offline data setting. The current work stops at reward learning, that is, the IRL problem. But without the extra RL step using the learned reward, is an incomplete story. The paper talks about similarities with RLHF; yes, there is a connection but one needs to eventually learn the optimal policy. Yes, one can just do planning with the learned reward and learned transition model, but equipping it with traditional model-based guarantees is important for making connections with other relevant works. _Model based guarantee_ will be unsatisfactory since [1] gives results for general function approximation. \n\n- Moreover, this manuscript subsumes many results from (Li et al., 2023) which is a non-peer reviewed work. This makes it hard to check soundness since one needs to evaluate both (at least the relevant parts required for this submission). I am mentioning this due to my emergency review. My score reflects the fact that generative model setting (samples from every state-action pairs $\\approx$ uniform concentrability) in Lindner et al. (2023), is equipped with pessimism and optimism terms to account for partial concentrability using the usual techniques in offline RL (Rashidinejad, et al. 2021) and thereafter.\n\nI am open to discussions with the authors and reviewers to make sure the work quality matches the score, which I believe so at this point, but a potential reach to 6 or 8 definitely exists. All the best for future decisions! \n\n[1] Jonathan D. Chang, Masatoshi Uehara, Dhruv Sreenivas, Rahul Kidambi, Wen Sun. Mitigating Covariate Shift in Imitation Learning via Offline Data Without Great Coverage, NeurIPS 2021."
            },
            "questions": {
                "value": "-na-"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7852/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698828406964,
        "cdate": 1698828406964,
        "tmdate": 1699636962381,
        "mdate": 1699636962381,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xpDipdoULS",
        "forum": "S24zdyiWDT",
        "replyto": "S24zdyiWDT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7852/Reviewer_9kcm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7852/Reviewer_9kcm"
        ],
        "content": {
            "summary": {
                "value": "The paper builds on recent works on Inverse Reinforcement Learning (IRL). It introduces a new metric different from the literature to analyze the distance between the feasible set of reward functions compatible with an expert's demonstrations in the limit of infinite samples and the estimated set. Their metric actually considers the distance between two mappings whose image set coincides with the feasible set. Next, the paper analyzes the sample complexity of estimating this mapping in the offline and online (forward model) settings. They provide algorithms for both the settings (and therefore upper bounds to the sample complexity) as well as lower bounds. Finally, the paper provides a sample complexity analysis on the error obtained when doing transfer learning with the estimated mapping, i.e., when transferring the estimated mapping instead of the true one."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- the main strengths are the sample complexity results for the offline and online (forward model) settings for IRL, which are the first results of this kind for IRL;\n- the idea of connecting transferability with concentrability when analyze the transfer learning setting is interesting"
            },
            "weaknesses": {
                "value": "- no technical novelty in the proofs which are based on (i) previous works on offline RL and (ii) previous works on IRL\n- the lower bounds (both for offline and online settings) are definitely not tight, since they do not depend on the confidence $\\delta$\n- the metric introduced to evaluate the complexity in the offline setting is actually not a metric in the mathematical sense"
            },
            "questions": {
                "value": "I am willing to adjust my score if the authors successfully answer my questions.\n\n1) Why do you use $d^\\pi$ (Definition 3.1) as a (pre)metric between reward functions? This is not a metric, and therefore the problem that you highlight in Lemma C.4 for the Hausdorff distance between sets sussists also for your (pre)metric $D_\\Theta^\\pi$ (Definition 3.2). It might be that the proposed (pre)metric $D_\\Theta^\\pi$ is zero even if the reward sets do not coincide. \n\n2) What is the rationale behind choosing the (pre)metric in Definition 3.1? In the usual pipeline, the reward recovered by IRL is then used for training RL agents. It does not seem to me that the (pre)metric in Definition 3.1 guarantees anything on how close the performance of the trained RL agent with the learned reward. In \"Towards Theoretical Understanding of Inverse Reinforcement Learning\" the authors focus on the actual distance between reward functions, not induced value functions. Can the author elaborate?\n\n3) Why do your lower bounds not depend on $\\delta$? This is quite significant especially when $\\delta$ is small. Inspecting the proofs in comparison with  \"Towards Theoretical Understanding of Inverse Reinforcement Learning\", it seems that the authors have adapted one construction only (the one that provides the part of the lower bound that does not depend on $\\delta$). Why this choice?\n\n4) How did you manage in the proof of the upper bound the fact that rewards defined as in the ground truth reward mapping are not bounded in $[-1,+1]$ even though the parameters $(V,A)\\in\\mathcal{V}\\times\\mathcal{A}$? In \"Towards Theoretical Understanding of Inverse Reinforcement Learning\", Lemma B.1 (appendix), they normalize the reward functions, but you don't. What allows you to avoid this step?\n\n5) The lower bounds do not match the upper bounds, especially for what concerns the dependence on the horizon H. What is the reason for this gap?\n\nCOMMENTS:\n- a section with the limitations of the results is missing and should be added;\n- the title has nothing to do with the paper; the paper concerns a sample complexity analysis in the IRL setting, stop. Nothing in the paper gives novel results on whether IRL is harder than RL, so the title must be changed;\n- the proof of the second part of Lemma C.1, although easy, is missing;\n- the proof of Proposition C.2 is missing;\n- in Section 1, Introduction, when listing the contributions, authors state that this work contributes at \"providing an answer to the\nlongstanding non-uniqueness issue in IRL\". This statement is factually false. Indeed, the authors investigate IRL as the reconstruction of the feasible reward set (and this formulation is not introduced in the paper), providing novel analysis for the offline and online (with forward model);\n- the use of O-tilde is incorrect for what concerns the dependence on $\\delta$. Conventionally, O-tilde does not hide dependences on $\\log(1/\\delta)$. This is not just a cosmetic comment, but seems to hide an additional term present in the upper bound and not present in the lower bound, spotting an additional term that is not matched;\n- you use the same symbol $d$ for both the visit distribution and the metric, and maybe you could change one of the two symbols to improve the presentation;\n- the paper contains many typos both in the main paper and in the appendix."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7852/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698858038858,
        "cdate": 1698858038858,
        "tmdate": 1699636962237,
        "mdate": 1699636962237,
        "license": "CC BY 4.0",
        "version": 2
    }
]