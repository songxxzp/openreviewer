[
    {
        "id": "MVJjorzMei",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5374/Reviewer_8ogw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5374/Reviewer_8ogw"
        ],
        "forum": "tMzPZTvz2H",
        "replyto": "tMzPZTvz2H",
        "content": {
            "summary": {
                "value": "The manuscript studies convergence rate and generalization in infinitely deep and wide ResNets within the mean-field scaling. Utilizing this limit, they recast the neural network's dynamics as a dynamics over weight probabilities per layer. They obtain the analog of the lowest NTK eigenvalue at initialization, and show that it remains of that scale up to some range (r) in Wasserstein-2 distance. This establishes a lower bound on the convergence rate in this r-region. They then turn to bound the KL divergence in this r-region (which they can also characterize by a maximal time t_0 for which the Wasserstein-2 distance remains below r). Finally, they characterize the generalization gap using a Rademacher complexity bound where they use the fact that at init the complexity is zero and afterwards it can be bounded by the KL divergence. All in all, they claim to obtain that within the r-region the generalization gap goes as 1/\\sqrt{n}."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper studies convergence rate and generalization in a challenging setting, a deep ResNet with no assumption on the data-set measure. \n\nIt is a technically challenging work which requires skill and effort."
            },
            "weaknesses": {
                "value": "Regarding weaknesses, I found several of those.  \n\n1. Perhaps this is a necessity for using such learning-theory tools, but I found the use of r-regions somewhat misleading. Fixing the r-region, it seems to be that, in one way or another, one is limiting the training time. Thus qualitatively, one is limiting the number of data points as training for a very short time on infinitely many data points is not much different than training for such a time on a finite large number of data points. If so what is the meaning of taking large n here if we don't give the network enough time to learn these points?\n\n2. Similarly confusing for me was the claim that $R_n(F) = \\beta \\sqrt{r/n}$ when \\beta scales larger than $\\sqrt{n}$. If so it seems that $R_n(F)=O(n^0)$ and not $1/\\sqrt{n}$. \n\n3. The authors' results do not reflect the complexity of the task of the dataset measure. While this can also be viewed as a positive point, I find it hard to believe that one can provide tight or even order-of-magnitude estimates on actual neural networks (in the regime where they generalize well) by putting so little data in. While this is not a criticism of the specific work under consideration, it does weaken its broader appeal in my mind.  \n\n4. With relation to point number 3, some numerical experiments (to the very least on toy data sets) showing how accurate these bounds are would be useful."
            },
            "questions": {
                "value": "1. Can the authors clarify what their results mean in practical terms? Fixing hyper-parameters how do they expect r to scale t_max and n? How do they expect the training error to decrease with r?\n\n2. Can the authors provide some supporting experiments showing how tight are their bounds?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5374/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697391283695,
        "cdate": 1697391283695,
        "tmdate": 1699636542974,
        "mdate": 1699636542974,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yiBP38qSEk",
        "forum": "tMzPZTvz2H",
        "replyto": "tMzPZTvz2H",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5374/Reviewer_x2zh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5374/Reviewer_x2zh"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the optimization and generalization properties of infinitely wide and deep residual networks in the mean-field limit, where the weights are described by a probability density. The network is trained by gradient flow over the empirical risk. The paper shows two main results: the convergence to the global minimum of the empirical risk, and a generalization bound for the trained network. The convergence result is obtained by lower-bounding the smallest eigenvalue of the Gram matrix of the gradients. The generalization bound is obtained by bounding the KL divergence between the initial weight distribution and the weight distribution after training, then studying the Rademacher complexity of the family of models in a ball around initialization for the KL divergence."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The paper provides a very interesting take on the important question of jointly understanding the training dynamics and generalization properties of deep neural networks. The final result is strong since it shows a bound on the population risk of trained deep networks under reasonably mild assumptions. The paper seems technically solid, although I have not read the proofs in full details.\n\nFurthermore, the paper is easy to read and well-presented. The paper presents all the results in a unified setting, which is enjoyable. I also liked that the three main contributions are well-separated throughout the paper (in the explanations, the mathematical statements and the proofs). This allows the community to easily build on proof techniques presented in the paper.\n\nTo my knowledge, the idea of bounding the Rademacher complexity through a bound on the KL divergence wrt the initialization is novel.\n\nAll in all, this makes this paper a very interesting contribution for the community."
            },
            "weaknesses": {
                "value": "I have two main questions for the authors. [Update on Nov 22: the authors have answered to all the questions, except the first one, see thread at the top of the page].\n1. The proof of global convergence relies on lower-bounding the smallest eigenvalue of the Gram matrix of the gradients of the two-layer neural network in the end of the architecture. There is no guarantee on the Gram matrix of the gradients of the infinite-depth ResNet part of the architecture. **For this reason, I am wondering if the proof would still hold if the ResNet part of the architecture was fixed to its initial value or even altogether removed.** If this is the case, I think this should be stated more clearly, and in my opinion questions a bit the narrative of the paper which gives an important focus on the ResNet part of the architecture, whereas I do not fully understand at this point the role it plays in the proof.\n2. The generalization bound is proven in the case where the radius $r$ of the KL ball decreases in $O(1/n)$. The paper proves that, with the appropriate scaling of $\\beta$, the trained network remains in this ball, and thus the generalization bound holds for the trained network. This means that the distribution of the weights is required to be increasingly close to the distribution at initialization as the sample size increases. **For this reason, I am wondering if the regime where the generalization bound holds can really be though of as feature learning.** While this is not a problem in itself, it questions in my opinion the statement in the conclusion of the paper that \u201cthis is the first paper to build a generalization bound for deep neural networks in the so-called feature learning regime\u201d. In my opinion, either more caution in the phrasing or more explanations would be highly welcome.\n\nI would be willing to raise my score if the remarks are answered by the authors, and the narrative of the paper changed accordingly if appropriate.\n\nA less important remark regards Theorem 4.11. The rate of convergence of the empirical risk in Lemma 4.7 actually depends on $n$ through $\\beta$ (which appears both in the exponential and implicitly in the loss at initialization, since the output of the network at initialization is proportional to $\\beta$). I don\u2019t think this dependence is taken into account by the authors in Theorem 4.11, or at least it is not stated since the authors state that the empirical risk decays as $O(e^{-t})$. While I believe that the stated theorem is correct, the result could probably be sharpened (since the rate of convergence is actually $O(e^{-nt})$, although the magnitude of the loss at initialization also depends on $n$ and would have to be taken into account). In any case, a proof would be beneficial.\n\n**Literature review**\n\nThere are several papers which show global convergence of deep residual networks and which are not cited. In particular, the beginning of Section 4 suggests that this paper is the first one to prove a rate of convergence with mild assumptions. There have actually been other papers proposing this, see e.g. https://arxiv.org/abs/1910.02934, https://arxiv.org/abs/2204.07261, https://arxiv.org/abs/2309.01213. Please rephrase accordingly the beginning of Section 4. Also, I believe Barboni et al. do not \u201cassume that the model parameters are close to the initialization\u201d, they prove it to be the case in their setting, which is a very different statement.\n\n**Minor remarks that did not influence the score**\n\n1. page 2, second paragraph: $\\tau$ et $\\nu$ should be swapped.\n2. First paragraph of Section 2.1 is not very clear.\n3. page 3: \u201cnerual\u201d -> neural.\n4. page 3: in your ResNet model, you assume a scaling in $1/L$. While this is necessary to obtain an ODE limit and usually assumed in the corresponding literature, it departs from the actual practice. I think it would be interesting to mention this. See https://arxiv.org/abs/2105.12245, https://arxiv.org/abs/2206.06929 for a related discussion. \n5. page 4: the sentence \u201cWe introduce a trainable MLP \u2026\u201d is not very clear. What do you mean by \u201czero initialization of the network\u2019s prediction\u201d?\n6. page 5, equations (11), (14), (15): $D$ should be $D_n$.\n7. pages 8 and 9: who is $\\Lambda$? Should it be $\\Lambda(d)$?\n8. page 9, after Theorem 4.10: $r_0$ is defined but was already defined in the Theorem.\n9. There are duplicated entries in the references, eg Chizat and Bach, and (probably) Ding, Chen, Li and Wright. Please double-check this.\n10. Lemma B.2: $N$ should be $n$? This typo is present in several places throughout the proof.\n11. Lemma B.7: I think the infinite norm is undefined. It is possible to infer what it means from context, but please define it."
            },
            "questions": {
                "value": "In your model, the output $y$ is a deterministic function of the input $x$. Could this assumption be relaxed (eg, adding noise to $y$)?\n\nAlso see weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5374/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5374/Reviewer_x2zh",
                    "ICLR.cc/2024/Conference/Submission5374/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5374/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698688247963,
        "cdate": 1698688247963,
        "tmdate": 1700638292835,
        "mdate": 1700638292835,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lztQgDwo8i",
        "forum": "tMzPZTvz2H",
        "replyto": "tMzPZTvz2H",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5374/Reviewer_JCPH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5374/Reviewer_JCPH"
        ],
        "content": {
            "summary": {
                "value": "The authors study the convergence of the gradient flow for infinitely wide and deep ResNets under a mean-field regime utilizing the technique developed by [Lu et al. (2020)] and [Ding et al. (2021, 2022)]. Specifically, they show the global convergence based on the estimation of the smallest eigenvalue of (varying) NTK matrix. Moreover, the generalization analysis using Rademacher complexity for the trained network is also provided."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This work studies the mean-field regime of the neural network, which has attracted attention due to the presence of feature learning. The optimization analysis with the convergence rate is quite challenging especially when not using the entropy regularization. This work nicely combines the mean-field analysis of infinitely deep and wide ResNet ([Lu et al. (2020)], [Ding et al. (2021, 2022)]) and the estimation of the smallest eigenvalue of NTK matrix, and derives the convergence rate. I think this work is interesting and suggests a new way of analyzing the deep ResNets under a mean-field regime.\n\n- The paper is well organized. The presentation is very clear and easy to follow."
            },
            "weaknesses": {
                "value": "- In fact, the mathematical method developed is quite interesting. However, I am not completely convinced whether the optimization dynamics considered in this paper indicates feature learning. The convergence analysis is based on the positivity of $G_2$ instead of $G_1$. Here $G_2$ is the Gram matrix corresponding to the last layer. Therefore, I suspect that the convergence property is fully relying on the optimization of the last layer. That is, it may not be possible to distinguish between the proposed dynamics and the optimization of the last layer only. Essentially, feature learning is caused by training the input layer rather than the output.\n\n- The theory of the paper does not seem to work when the last layer is frozen in contrast to many work for two-layer mean-field neural networks. This concern is related to the one above.\n\n- Missing reference. The following work is also closely relevant to the paper. \n\nZ. Chen, E. Vanden-Eijnden, and J. Bruna. On feature learning in neural networks with global convergence guarantees."
            },
            "questions": {
                "value": "It would be nice if the authors could prove that feature learning certainly appears in the proposed setup."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5374/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5374/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5374/Reviewer_JCPH"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5374/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698765703023,
        "cdate": 1698765703023,
        "tmdate": 1699636542756,
        "mdate": 1699636542756,
        "license": "CC BY 4.0",
        "version": 2
    }
]