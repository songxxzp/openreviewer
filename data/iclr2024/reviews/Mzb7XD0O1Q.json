[
    {
        "id": "h4NZOCl2sB",
        "forum": "Mzb7XD0O1Q",
        "replyto": "Mzb7XD0O1Q",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3776/Reviewer_aBV8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3776/Reviewer_aBV8"
        ],
        "content": {
            "summary": {
                "value": "The authors investigate the use and fusion of two common feature representations within the audio domain, the raw waveform and spectrogram."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The topic addressed in the paper is interesting."
            },
            "weaknesses": {
                "value": "The introduction is poorly written. There are too many terms introduced e.g., PSWaCL, SWaB, MSAE without details of what to expect in the paper and what are the real contributions of the paper.\nChallenges in feature fusion are not clear, and a lot of statements are loose or vague.\n--> waveform-based features concentrate more on capturing common patterns! What are these common patterns?\n--> Spectrograms predominantly emphasize time-frequency responses! What are these responses? Are we assuming a system here? Why can't waveform learning using a learned filterbank do the same?\n--> Enhance the comprehensiveness of the feature set! Comprehensiveness in what sense?\nNone of the above-mentioned statements are actually related to Semantic Misalignment. Semantic means something else.\nTemporal Misalignment: Again, the claim by the authors is wrong. In both cases, linear/non-linear processing methods are available and temporal alignment can be easily achieved. It's basic DSP! Nevertheless, One can always do late fusion after learning complementary features.\n\nRelated works: There is no mention of existing approaches that have tried feature fusion, which should be the main focus.  Instead, authors have just discussed existing approaches for audio classification, which could be omitted or briefly mentioned if compared against in the experimental section. \nhttps://dcase.community/documents/challenge2021/technical_reports/DCASE2021_Fedorishin_97_t1.pdf\nhttps://www.mdpi.com/1424-8220/19/7/1733\nhttps://www.isca-speech.org/archive/pdfs/interspeech_2018/yang18c_interspeech.pdf\nhttps://dl.acm.org/doi/pdf/10.1145/3240508.3240631\n\nMethod (Sec3): Overall, the proposed method is just a combination of well-known existing methods and small extension of  method by Gong et al., 2022a. Novelty is limited and not well highlighted in the context of the problem addressed in the paper.\n\n--> Our work is built upon SSAST. What is SSAST? \n--> fills the gap of lacking raw audio waveform embedding in the era of transformer. Again, this is a loose statement that is not explained.\n--> Contrastive learning is widely used in multimodal generative models. So, the method is not novel in itself. What do we mean by natural or unnatural pairing? \n--> MSAE is a known technique for designing adaptively learned filter banks. Whats novel here? Authors should refer to existing works here.\nPatichyfy operation is not explained. A diagram would help readers. Pooling will reduce the information for short kernel-based conv outputs with bigger dimensions. Instead, zero padding, dilation, adaptive strides, and deformed convolution kind of ideas can be used to learn multiscale features. In current practice, pooling has been established to be one of the worst choices.\n--> what is specify in Fig1?\n--> There is no description of how spectrogram and waveform feature inputs are processed in the transformer frontend. Is it a single transformer with shared weights or individual ones? A lot of these crucial details are superficially treated.\n--> spectrogram and waveform patches can naturally serve as contrasting pairs. It is unsure how this will happen. Is there a ref to existing work to establish this?\n--> what is t_spec t_wav in (5a,5b)? what are the dimensions? on which axis is the concatenation is happening? Again, it is unclear from where bottleneck features will come? Why are they required? Can't we just project the features to the same dimensional space? What's the design of a multi-stream bottleneck transformer?\n\nAuthors have used Mel-spec, which is a non-linear feature, and the arguments in the introduction about miss-alignment due to fixed resolution are in contrast. While I understand the author's point of view, the way things are explained or presented is misleading for a wider audience.\nOnly spectral domain augmentation is used. Why not the time domain? Existing works have utilized both for acoustic modelling and feature fusion using CNN backbone with remarkable success. Yes, transformers are hot these days! but they only shine for sequence modelling tasks. For classification CNNs are still the best (attention can be incorporated, too); one has just to train them well. \n\n\nExperimental results are not SOTA, and strong baselines are not considered.\nAuthors are encouraged to see https://paperswithcode.com/sota/audio-classification-on-esc-50\nExisting works have achieved over 98% on ECS-50 benchmark.\n\nSimilarly, for Audioset: https://paperswithcode.com/sota/audio-classification-on-audioset\n\n\n\nThe code is not available to replicate main experiments without which the claims hold no value at venues like ICLR."
            },
            "questions": {
                "value": "Please see the detailed feedback above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3776/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698604608943,
        "cdate": 1698604608943,
        "tmdate": 1699636334124,
        "mdate": 1699636334124,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jSCwdgOOyX",
        "forum": "Mzb7XD0O1Q",
        "replyto": "Mzb7XD0O1Q",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3776/Reviewer_7CEx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3776/Reviewer_7CEx"
        ],
        "content": {
            "summary": {
                "value": "The paper proposed a modeling approach that incorporates both waveform and spectrogram features in the audio domain. The authors also addressed the semantic misalignment and temporal alignment issue raised by the combination. The experiments demonstrate the effectiveness of the approach in audio classification tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Overall, it's an interesting idea to combine both waveform and spectrogram features and address alignment issues in one shot. \n- The experiments and ablation studies are quite compressive."
            },
            "weaknesses": {
                "value": "- Novelty is limited considering the ICRL standards. It might be a better fit for speech-related conferences (e.g. Interspeech)\n- The writing of this paper needs to be improved. Too many acronyms to make it less readable and hard to follow the idea, especially in section 3.2."
            },
            "questions": {
                "value": "In table 1, it shows that the performance of AST (spectrum-only) is still better than all the proposed methods in the paper. How to explain it?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3776/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698816626319,
        "cdate": 1698816626319,
        "tmdate": 1699636334048,
        "mdate": 1699636334048,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LNa0CEvLno",
        "forum": "Mzb7XD0O1Q",
        "replyto": "Mzb7XD0O1Q",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3776/Reviewer_Cffs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3776/Reviewer_Cffs"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a joint spectrogram-waveform representation learning method for audio classification task. Three techniques are introduced to solve the challenges in aspect of temporal alignment and semantic alignment problems. Specifically, MSAE model is proposed to align the waveform feature to spectrogram patches. Contrastive learning between spectrogram and waveform representations is proposed as a new pretraining objective. Fusion bottleneck token is introduced for better finetuning performance. System comparison and ablation studies are conducted on the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed method achieves higher or comparable performance on audio classification task compared to the existing SSL-based methods.\n2. Sufficient ablation studies are conducted to show the effectiveness of the proposed method and the effect of different hyper-parameters.\n3. The idea of patchfying 1d waveform representation to align with the 2d spectrogram representation is somehow novel."
            },
            "weaknesses": {
                "value": "1. Some statements are inaccurate or unclear. \n\n   a) In introduction paragraph 1, the authors try to illustrate the difference between spectrogram and waveform representation by differentiating the tasks based on them. However, many of the audio/speech tasks can build on both spectrogram and waveform representation and both achieve good results. Actually, in areas like audio signal processing and ASR, both spectrogram [1,2] and waveform [3,4] are frequently used. \n\n   b) In introduction paragraph 2, it is quite confusing why car engine sound is more clear in the time-frequency domain, while successive breezes is clear in waveform domain. Need clarification. Moreover, waveform representation can also present time-frequency patterns. Take an example of conv-TasNet [3] in audio signal processing domain, the waveform filters spontaneously converge to a frequency response similar to that of a log-mel filter bank. \n\n2. Some experimental results / settings are confusing. \n\n   a) To sufficiently prove the effectiveness of spectrogram-waveform representation combination, the authors should show the comparison between spectrogram-only, waveform-only, and joint spectrogram-waveform representations while **keeping other factors the same**. However, the waveform-only results come from very old research, where the SSL and audio transformer techniques are not well established. Since waveform includes more information than the spectrogram, maybe using WaPT will result in better performance than SSaPT and comparable performance of PSWaCL. If this is true, spectrogram can be completely substituted by waveform in audio classification task. Here, WaPT denotes \"Waveform modeling in PreTraining\" and the only difference form SWaPT is to remove the spectrogram input and the corresponding training loss.\n\n   b) The result of \"SWaPT is worse than SSaPT\" is confusing. Why adding additional representation degrades performance? If the reason is the conflict between spectrogram and waveform representation, why not use different parameters in waveform branch and spectrogram branch? If this simple way can solve the misalignment between spectrogram and waveform feature, the necessity of PSWaCL is quite doubtful.\n\n[1] Yin, Dacheng, et al. \"Phasen: A phase-and-harmonics-aware speech enhancement network.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. No. 05. 2020.\n\n[2] Gulati, Anmol, et al. \"Conformer: Convolution-augmented transformer for speech recognition.\" Interspeech 2020.\n\n[3] Luo, Yi, and Nima Mesgarani. \"Conv-tasnet: Surpassing ideal time\u2013frequency magnitude masking for speech separation.\" IEEE/ACM transactions on audio, speech, and language processing 27.8 (2019): 1256-1266.\n\n[4] Chen, Sanyuan, et al. \"Wavlm: Large-scale self-supervised pre-training for full stack speech processing.\" IEEE Journal of Selected Topics in Signal Processing 16.6 (2022): 1505-1518."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3776/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3776/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3776/Reviewer_Cffs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3776/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698818379734,
        "cdate": 1698818379734,
        "tmdate": 1699636333960,
        "mdate": 1699636333960,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PapOXpXVGK",
        "forum": "Mzb7XD0O1Q",
        "replyto": "Mzb7XD0O1Q",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3776/Reviewer_2yfr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3776/Reviewer_2yfr"
        ],
        "content": {
            "summary": {
                "value": "The paper describes representation learning method both using spectrogram and waveform. Usually, the model takes spectrogram-based feature as an input to the model, or rarely waveform is solely used. However, since the information we can extract from the spectrogram and waveform can be different, it might be better to use both cases as well. To do that, the author basically used a model presented before (which is joint discriminative and generative masked spectrogram patch modeling), and improved this model by adding several techniques to both deal with spectrogram and waveform. In the end, they made an auxiliary loss term using waveform encoder. This waveform encoder uses multi-scale front-end encoder and  the output of the waveform encoder is compared with spectrogram encoder like they are having a different view relationship in SimCLR loss term. Finally, bottleneck fusion method is used to further boost the performance. The result and ablation study showed that the proposed modules are effective in spectrogram and waveform modeling in environmental sound classification task."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The most strong part of the paper lies on the model performance. When we see the results in Table 1, we can find that the proposed method reached the best performing model among self-supervised learning approaches. Also, the paper is easy-to-read and written clearly."
            },
            "weaknesses": {
                "value": "However, I think the novelty of the paper is quite limited. When we see the results in Table 5 (ablation study), the results is quite obvious. It is well-known that the performance is increased if we apply multi-scale modeling on acoustic model. Also, SimCLR loss and bottleneck fusion methods are also quite known approaches."
            },
            "questions": {
                "value": "If there is more insights we can get from the model, then it would have more novelty on the paper. For example, since the proposed work contains both spectrogram and waveform based encoder, maybe we can compare the learned characteristics of each encoder (especially the waveform encoder is multi-scaled)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3776/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3776/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3776/Reviewer_2yfr"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3776/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698836574192,
        "cdate": 1698836574192,
        "tmdate": 1699636333798,
        "mdate": 1699636333798,
        "license": "CC BY 4.0",
        "version": 2
    }
]