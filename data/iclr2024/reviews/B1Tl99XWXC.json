[
    {
        "id": "wJVzH4xMhy",
        "forum": "B1Tl99XWXC",
        "replyto": "B1Tl99XWXC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4872/Reviewer_dyX4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4872/Reviewer_dyX4"
        ],
        "content": {
            "summary": {
                "value": "In this paper, authors propose a novel DPMs-based transfer learning method, TAN, to address the limited data problem. It includes two strategies: similarity-guided training, which boosts transfer with a classifier, and adversarial noise selection which adaptive chooses targeted noise based on the input image. As illustrated in the paper, authors think they haved achieved SOTA results compared with prior works."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.The paper introduces a binary classifier to guide the training and an adversarial way to generate noises, this idea is interesting.\n2.The exploration of overfitting in Fig.1 is helpful for the few-shot learning of generative models."
            },
            "weaknesses": {
                "value": "1.The quantitative results reported for different methods have large overlap when factoring in the std values, in general, the boldfacing of average values when ignoring the stds is not the best practice.\n2. I have read several related works and find almost all of them show use samples transferred from FFHQ as qualititative results. I found that the results of this paper on FFHQ are only shown in Supp. I cannot figure out the improvement of this method compared with DDPM-PA on those results. Actually, I think DDPM-PA shows better results.\n3. Results in this paper should be compared with more modern text-to-image methods based on diffusion models, including textual inversion, dreambooth, domainstudio. If this method is only applied to traditional methods, it's not convincing enough.\n4. In LSUN Church --> Landscape drawings, it seems that DDPM-PA carries out a style transfer process, this work fails to get samples of church actually. Therefore, I wonder if this comparison is fair. For FFHQ --> babies and sunglasses, this work and DDPM-PA share the same target. However, I think DDPM-PA performs better."
            },
            "questions": {
                "value": "My main concern is about the performance and applicable scenarios. See the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4872/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698038584499,
        "cdate": 1698038584499,
        "tmdate": 1699636471341,
        "mdate": 1699636471341,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jpksiH2YtW",
        "forum": "B1Tl99XWXC",
        "replyto": "B1Tl99XWXC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4872/Reviewer_ft3z"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4872/Reviewer_ft3z"
        ],
        "content": {
            "summary": {
                "value": "The paper presents an innovative approach to transfer learning in diffusion models with sparse target data. To tackle the limited data problem, the authors propose TAN, a new diffusion-based method including two strategies: similarity-guided training, which aims to enhance transfer with a classifier, and adversarial noise selection to improves the efficiency of training process. The authors conducted extensive experiments in the context of few-shot image generation tasks and demonstrated that their method is not only efficient but also excels in terms of image quality and diversity when compared to existing GAN-based and DPM-based methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThis paper is well written, easy to follow and conducts a lot of experiments  with comprehensive analysis, demonstrating the effectiveness of the proposed approach.\n2.\tThe design of the similarity-guided training and adversarial noise is intuitive and reasonable."
            },
            "weaknesses": {
                "value": "1.\tThe graphical representation of the results doesn't clearly demonstrate a significant improvement compared to other existing methods.\n2.\tRegarding the approach to adversarial noise selection for Eq. 7 and Eq. 8, could you provide further clarification on the choice of minimizing the maximum Gaussian noise at step t? Additionally, since optimizing for maximum noise can be particularly challenging, could you delve deeper into how the multi-step variant of PGD with gradient ascent ensures the performance of this methodology? Expanding on these aspects would significantly enhance the comprehensibility and value of this paper.\n3.\tThe paper lacks an in-depth exploration of the implications for time and GPU memory conservation when utilizing the supplementary adaptor module and the process of adversarial noise selection. To enhance the overall quality of the paper, I suggest providing a more comprehensive elucidation, along with relevant graphical representations, regarding the efficient training procedures involving these components."
            },
            "questions": {
                "value": "My main concerns and questions lie in the weaknesses. The author should discuss them in detail."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4872/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4872/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4872/Reviewer_ft3z"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4872/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698663982555,
        "cdate": 1698663982555,
        "tmdate": 1699636471225,
        "mdate": 1699636471225,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "E1gC4MwFbU",
        "forum": "B1Tl99XWXC",
        "replyto": "B1Tl99XWXC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4872/Reviewer_Pd9e"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4872/Reviewer_Pd9e"
        ],
        "content": {
            "summary": {
                "value": "Generative models can leverage large-scale datasets to learn and produce diverse high-quality outputs. To overcome the limitations of this approach, methods for transfer learning from models trained on extensive datasets have been studied. However, diffusion-based generative models, unlike directly step-wise generative models like GANs, generate samples through the diffusion of noises over a large number of steps, making it challenging to apply conventional transfer learning methods. Therefore, in this paper, a method called TAN, which utilizes adversarial noises to effectively transfer learning to diffusion models. \n\nGenerating high-quality and diverse results with generative models comes with the challenge of obtaining a large amount of training data and stabilizing the training process. In this regard, transfer learning and few-shot learning research are important, yet relatively less explored. Previous studies attempted transfer learning by matching the results of intermediate steps of DDPM, but they led to distorted transfer and overfitting issues. the proposed approach, incorporating similarity-guided training and adversarial noise selection techniques, yielded well-transferred results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper proposes simple yet effective methods, namely similarity-guided training and adversarial noise, for transfer learning with diffusion models. \n\nThe proposed methods yielded high-quality results and were demonstrated through various experiments. \n\nThe paper presents equations 4 and 5, which induce the Kullback-Leibler divergence between the source and target models during the reverse process of the diffusion model. \n\nThis divergence is defined as similarity and utilized to control transfer learning.\n\nThe proposed similarity-guided approach by the authors not only induces overall transfer for the target domain but also considers the characteristics of the diffusion generative model. \n\nBy utilizing adversarial noise selection, the method allows the noise to fit more accurately, resulting in high-quality output and addressing the issues faced by previous methods. \n\nThe paper presents a cohesive and easily understandable writing flow, effectively conveying the underlying arguments and the rationale of the study."
            },
            "weaknesses": {
                "value": "I think it would be better if there is deeper analysis regarding the similarity or adversarial noise in the proposed method. It would be even better if there were various experiments and analyses to examine the semantic effects of the redefined reverse step compared to the vanilla model. I am curious about the author's insights beyond mathematically deriving the KL divergence between the source and target domains, exploring different aspects."
            },
            "questions": {
                "value": "1. I am curious if this method can be applied not only to transfer learning or few-shot learning but also to fields such as domain adaptation or domain control.\n\n2. I wonder if the adversarial noise in this method can assist in achieving higher-quality results in the training of general DDPMs.\n\n3. I'm curious if this method has been experimented with in domains other than 2D images."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4872/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698829316842,
        "cdate": 1698829316842,
        "tmdate": 1699636471132,
        "mdate": 1699636471132,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VKi00ZcwaV",
        "forum": "B1Tl99XWXC",
        "replyto": "B1Tl99XWXC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4872/Reviewer_jP4S"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4872/Reviewer_jP4S"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a Diffusion Probabilistic Model (DPM)-based transfer learning method, i.e., to address the limited data problem. Specifically, similarity-guided strategy is designed to enhance the few-shot transfer learning and an adversarial noise selection method is proposed to address the underfitting problem during a limited number of iterations. Both qualitative and quantitative results demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is well-written with clear description.\n\nIt analyzes the limitations of DPM-based transfer learning and presents reasonable solutions."
            },
            "weaknesses": {
                "value": "How does the pre-trained DPMs come and if the proposed method can be applied on other pre-trained DPMs for transfer learning?\n\nIf the authors could provide visualizations on the selected noise for deeper analysis on its effect?\n\nThe step number used to perform the projected gradient descent (PGD) in Eq.(7)?"
            },
            "questions": {
                "value": "Please refer to the Weaknesses for details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4872/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698863692923,
        "cdate": 1698863692923,
        "tmdate": 1699636471037,
        "mdate": 1699636471037,
        "license": "CC BY 4.0",
        "version": 2
    }
]