[
    {
        "id": "CLdHsmXnBz",
        "forum": "ZzmKEpze8e",
        "replyto": "ZzmKEpze8e",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5320/Reviewer_VM8e"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5320/Reviewer_VM8e"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the challenging problem of online continual learning and introduces an approach based on the Kalman filter (KF).  This paper models non-stationary as parameter drift based on linear stochastic dynamics and employs KF to estimate the posterior distribution of adaptable model parameters. Additionally, the paper discusses updates to the forgetting coefficient of KF. The experimental results demonstrate the superior performance of the proposed method compared to previous online continual learning methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper studies a practical and interesting problem: online continual learning, emphasizing non-stationary. It explores the application of  KF in classification problems by updating the forgetting coefficient. \n\nThe proposed method, which utilizes a Kalman filter-based approach for prior and posterior estimation of model parameters, presents a reasonable solution to tackle the (non-stationary)  online learning problem.\n\nOverall, the paper is well-structured and effectively communicates the details of the proposed method.\n\nFurthermore, in the experimental evaluation, the proposed method demonstrates superior performance compared to previous approaches, as reported in the paper."
            },
            "weaknesses": {
                "value": "1) This paper could benefit from a comparison with related works that utilize the Kalman filter for online continual learning and adaptation tasks, as seen in [1]. Such a comparison or discussion regarding the difference between the proposed method and [1] would enhance the paper's comprehensiveness.\n\n2) The paper assumes that the ground-truth model follows a linear predictor based on features,  followed by adapting the last layer of the neural network using KF. The results in Table 1 suggest that the \"Backbone finetuning\" method outperforms \"No backbone finetuning (Purely linear model).\" This observation raises questions about the validity of the assumption regarding linear stochastic dynamics.  It is essential to engage in an in-depth discussion regarding the correctness and practicality of these assumptions.\n\n3) In the evaluation of CLOC in section 4.2.2, it's crucial to consider the real-time computational cost for each algorithm, as emphasized in [2]: In the evaluation of the proposed method could be a prudent step.\n\n[1] A. Abuduweili, et al, \u201cRobust online model adaptation by extended kalman filter with exponential moving average and dynamic multi-epoch strategy\u201d, L4DC 2020.   \n[2] Bornschein, et al. \u201cSequential learning of neural networks for prequential mdl.\u201d ICLR, 2022."
            },
            "questions": {
                "value": "1) How does the proposed method compare to related Kalman filter-based techniques for online continual learning and adaptation, such as KF in  [1]?\n\n2) Does the assumption concerning linear stochastic dynamics align with the outcomes presented in Table 1, given that the \"Backbone finetuning\" method outperforms the \"No backbone finetuning (Purely linear model)\"?\n\n3) What are the real-time computational costs associated with each algorithm?\n\n[1] A. Abuduweili, et al, \u201cRobust online model adaptation by extended kalman filter with exponential moving average and dynamic multi-epoch strategy\u201d, L4DC 2020."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5320/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5320/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5320/Reviewer_VM8e"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5320/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698641987634,
        "cdate": 1698641987634,
        "tmdate": 1699636533944,
        "mdate": 1699636533944,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Pq3UnC9rP8",
        "forum": "ZzmKEpze8e",
        "replyto": "ZzmKEpze8e",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5320/Reviewer_NkkH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5320/Reviewer_NkkH"
        ],
        "content": {
            "summary": {
                "value": "The authors present an application of online Kalman filter inference within dealing with concept drift in deep learning. There is no theoretical analysis, but the computational results on variously batched and randomized classification datasets (CIFAR-100) and CLOC are promising."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The experiments show a clear improvement over a number of methods that could be seen as the state of the art, incl:\nNo backbone finetuning (Purely linear model)\nBackbone finetuning\nBackbone finetuning with Replay\nOnline SGD\nOnline SGD + Replay\nER++ (Ghunaim et al., 2023).\n\nThe paper is well written, both interms of derivation of the method and explaining the expertiments."
            },
            "weaknesses": {
                "value": "The improvement over ER++ is marginal. \n\nThe experiments do not test against the best possible Kalman filter (in terms of the system matrices), or some restarted version of the best possible Kalman filter."
            },
            "questions": {
                "value": "Have you considered extending the work of Kozdoba et al (https://doi.org/10.1609/aaai.v33i01.33014098) showing that Kalman filters can be approximated arbitrarily closely using ARMA models? This could reduce the runtime of O(m^2) to O(d) for recursion depth of d. \n\nHow exactly do you envision to use \"reparametrize the integral to be an expectation under the standard normal and then apply Monte Carlo\"? Plausibly, the scaleability of Monte Carlo could be a constraint? Could you present the statistical performance and runtime of the precomputing and the actual application of the method parametrized by S, as in the O(KS) or O(Km + m^2)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5320/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698689128375,
        "cdate": 1698689128375,
        "tmdate": 1699636533822,
        "mdate": 1699636533822,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OliXpEZMUo",
        "forum": "ZzmKEpze8e",
        "replyto": "ZzmKEpze8e",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5320/Reviewer_reom"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5320/Reviewer_reom"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an approach to the online learning of regression and classification models using Kalman filtering. The key idea of the paper is to divide the learning into two parts: that of the feature extractor \"backbone\" and the \"last-layer\" representation. The backbone is assumed to be either pretrained and fixed, pretrained and fine-tuned, or trained from scratch using standard optimization approaches. When the backbone is fine-tuned or trained from scratch, the negative log-predictive probability is used as the objective and parameter updates are done using standard SGD steps. The last-layer representation is updated using efficient Kalman filter updates with standard Gaussian assumptions on the emission and transition densities. By allowing the weights on the last-layer representation to evolve as a Gaussian process, with a tuning parameter governing model plasticity (this parameter is shown to be learnable online), the model is able to adapt to \"natural\" (i.e., realistic) distribution shift. It is shown that the proposed hybrid scheme of an SGD-trained feature extractor and Kalman filter updates on the last layer allows the creation of an accurate online classifier in some large sequentially presented data sets."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Online training of neural networks with Kalman filters is not a new idea, but one that has gotten some attention recently with developments aimed at making the procedures more scalable. The present paper makes a good potential addition to this literature by proposing a method to achieve this on practically-sized neural networks for image classification problems. The paper is well-written with comprehensive appendices motivating the various design decisions made. A major strength of the paper are the convincing experimental results and carefully done comparisons. The combination of pre-training (or SGD training) with some plasticity on the parameters on the last-layer representation is, to the best of my knowledge, a novel idea. The use of a Kalman filter on the last layer only allows for efficient updates, as the number of parameters on the last layer is small relative to the rest of the network."
            },
            "weaknesses": {
                "value": "The paper is primarily focused on learning in the case of having a pre-trained extractor of a representation. Training a representation extractor can require considerable offline computational expenses. Also, having a fixed representation extractor does not ensure plasticity of the weights of the extractor itself, which may be important in cases where there is sufficient heterogeneity in the data over time that this becomes an issue. It is unclear how much of an issue this can be in practice, and therefore a comment, or some justification, should be made on the implicit assumption that plasticity in the last layer is sufficient. \n\nAs presented, the methodology does not allow for a simple incorporation of additional weight blocks into the Kalman filter update, as this breaks the linear Gaussian assumption that the method relies on. This can perhaps be circumvented with a linearization of the observation model. Have the authors considered extensions of their method in this direction? This would lead to the additional question of what weights to learn with SGD and which weights to learn with the Kalman filter (intuitively, the ones requiring the most plasticity), and how to select them."
            },
            "questions": {
                "value": "Figure 3 is perhaps the most interesting of the paper, and raises the main question that I have: why is the gap between using a pretrained extractor and Kalman filtering and SGD so much greater than between an extractor trained from scratch with Kalman filtering and SGD? Memory costs aside, why does the Kalman filter add no additional performance gain in this situation? I don't expect it to beat or match the pretrained case, of course, but perhaps be in between the SGD and the pretrained case. What is it about the interaction between learning these two parameter sets that causes this? Have the authors tried experimenting to try and improve the from scratch performance of their proposed method to be even a bit better than online SGD? An understanding of this is basically the missing part of the paper for me."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5320/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699218698671,
        "cdate": 1699218698671,
        "tmdate": 1699636533736,
        "mdate": 1699636533736,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DzwojnaUtK",
        "forum": "ZzmKEpze8e",
        "replyto": "ZzmKEpze8e",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5320/Reviewer_BEuY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5320/Reviewer_BEuY"
        ],
        "content": {
            "summary": {
                "value": "The article presents a method for adapting a neural network model to a possibly non-stationary stream of data. The method consists of applying a Kalman filter to infer the weights of the last layer of the neural network over a data stream defined by an Online Continual Learning task. In addition, the neural network providing the representation and a parameter setting the level of forgetting in the last layer are adapted online, further adapting to a new problem setting. The method was applied to a simple regression task and two large-scale classification problems in continual learning which used the CIFAR-100 and CLOC datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Impressive experimental results in terms of accuracy metrics. The classification experiments demonstrate that the method can be applied to large-scale high-dimensional tasks and provides clear improvements over previous work.\n- A novel method which clearly differentiates itself from similar recent work through improvements in efficiency and allowing for online adaptation of forgetting parameters\n- The modeling choices were well-motivated, and the discussion in the Related work section on a meaningful setting for continual learning tasks was convincing\n- Section 2.1. was well-written, as it thoroughly explains the details of online learning in the last layer, while also motivating the modeling choices"
            },
            "weaknesses": {
                "value": "- It was not clear while reading the Related work section how using Kalman Filters in the representation space impacted the results compared to EKF on the full network as in Chang et al. I understood that the method presented here is more efficient, but how is the performance of the model in CL tasks impacted by only considering the last layer in the filter, especially when the representation network is trained from scratch? \n- A further explanation of the backbone finetuning in the main text would be beneficial to understanding the Experiments section, since the impact of finetuning the representation neural network $\\phi$ was often even greater than the impact of tuning the forgetting parameter $\\gamma$, but the former did not appear in the list of contributions and was explained very quickly in the main text.\n- A code implementation was not provided within the supplementary material\n- Claims of efficiency in terms of computation were not supported by empirical results. The comparison to Chang et al. is supported by a theoretical analysis, but a comparison to the baselines Online SGD and ER++ would further support the claims of the method achieving improvements in accuracy metrics without sacrificing computational efficiency, especially when the backbone neural network is finetuned. \n- Some visualizations could be improved for better readability. For instance, Figure 2 has very thin lines in the legend and plot which makes it difficult to see what is happening. \n\nChang, P. G., Dur\u00e1n-Mart\u00edn, G., Shestopaloff, A. Y., Jones, M., and Murphy, K. (2023). Low-rank extended kalman filtering for online learning of neural networks from streaming data."
            },
            "questions": {
                "value": "How does the training of the representation neural network (backbone finetuning) impact the behaviour of the Kalman filter, since the latent space is changed mid-stream? Can this impact or explain the results in Figure 3, where the Kalman filter method performs better compared to previous work when used on a pre-trained network rather than from scratch?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5320/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699353750166,
        "cdate": 1699353750166,
        "tmdate": 1699636533644,
        "mdate": 1699636533644,
        "license": "CC BY 4.0",
        "version": 2
    }
]