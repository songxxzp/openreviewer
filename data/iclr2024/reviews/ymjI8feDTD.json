[
    {
        "id": "ZpW7I0sUQn",
        "forum": "ymjI8feDTD",
        "replyto": "ymjI8feDTD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7687/Reviewer_DUTv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7687/Reviewer_DUTv"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces the Consistency Trajectory Model (CTM), which extends the concept of Consistency Models (CM) and score-based diffusion models. CTM is a single neural network that can efficiently output scores in a single forward pass, allowing for flexible traversal between any initial and final time points in a diffusion process. CTM combines adversarial training and denoising score matching loss to achieve competitive performance in CIFAR-10 and ImageNet-64 datasets. Additionally, CTM accommodates various diffusion model inference techniques, including exact likelihood computation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper introduces a novel way to distill dynamical generative models such as Diffusion Model. To the best of my knowledge, the proposed method is novel and yields appealing results in different datasets. The experimental details are provided clearly and the paper is well-written."
            },
            "weaknesses": {
                "value": "I have some uncertainties regarding potential weaknesses, but I believe it is valuable to bring this matter to the forefront for discussion. \n\n1. The paper itself presents a novel concept, but the methodology employed appears to be excessively complex. The distillation process involves three networks, and various techniques, such as $\\lambda_{GAN}$ for ensuring stable training, are necessary to attain the results reported in Figure 15. Nevertheless, it is worth noting that this complexity also contributes to the uniqueness and innovation of the paper.\n\n2. One of the reasons for the popularity of diffusion models lies in their scalability with respect to data complexity. In Figure 15, it is evident that the distillation of diffusion models heavily depends on the underlying GAN training structure. This prompts the question of whether it might be more straightforward to solely employ GAN training for a single NFE generation. Essentially, this would involve passing through the computationally expensive pretrained Diffusion Model, followed by the resource-intensive distillation process (as CTM requires simulating trajectories), ultimately converging to the realization that the fundamental component driving performance improvement is the GAN training objective. Given the current state of the art where GANs can achieve superior results, it raises the question of why we necessitate a complex distillation methodology, notwithstanding the novelty of such an approach.\n\n3. Sorry for my ignorance, but I think some concepts need more elaboration. For example,  why the adversarial training can lead to better performance than the teacher model? Why Taylor's expansion is the main reason for the discretization error?\n\n4. To the fair comparison with extremely small NFE, the fair comparison should be Consistency Models instead of EDM."
            },
            "questions": {
                "value": "1. It would be great if the authors could provide further motivation for using such complicated training objective functions with pretrained model over just training GANs.\n\n2. It would be helpful if the author could provide the wall-clock of the training procedure. It seems like the training will be even more expensive than the diffusion model or GAN.\n\n3. More comparisons with CMs are needed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7687/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7687/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7687/Reviewer_DUTv"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7687/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698001041112,
        "cdate": 1698001041112,
        "tmdate": 1700573237174,
        "mdate": 1700573237174,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pNPgB3LlMh",
        "forum": "ymjI8feDTD",
        "replyto": "ymjI8feDTD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7687/Reviewer_RDDt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7687/Reviewer_RDDt"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes the Consistency Trajectory Model (CTM) that predicts any final time from any initial condition along the probability flow ODE of diffusion models. It can sample high-quality samples in one step while keeping the ability to access the score. The authors also introduce $\\gamma$-sampling method, which can consistently improve sample quality with more steps. The experiments show that CTM can achieve SOTA sampling results in one step."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is well written, with valuable insights into the proposed method. It also clearly compares to related works, positioning it well in the literature. \n2. The idea is natural and nicely connects the score-based and distillation methods. \n3. SOTA FID results in one step while maintaining the flexibility of accessing the original score. \n4. The provided ablation studies are helpful in understanding the effect of each design component."
            },
            "weaknesses": {
                "value": "1. While adversarial training greatly improves FID, it might lead to poor mode coverage, which is a known issue of GAN. Can you measure the mode coverage of CTM on ImageNet64? For example, reporting Recall in Table 2. \n2. CTM loss brings a lot more additional training cost compared to other distillation methods. It takes at least 4 model forward passes of CTM, multiple steps of the ODE solver at each training step, the cost of computing DSM loss, and also the cost of training the discriminators. Can you provide a detailed description of how many model forward passes are used per training step? Can you also provide the total training time in Table 3 and compare it to the cost of training the teacher model?\n3. Training CTM seems to require many tricks like the warm-up scheme for GAN loss and a particular time sampling scheme as described in appendix D.1. But there is little ablation study about them. Can you elaborate more on how you design these training strategies and how important they are? \n4. Minors:\n\t1. Why is the retrained CM much worse than the official report? \n\t2. The official report of DFNO is 3.78. \n\t3. The FID of Rectified Flow 4.85 is from 2-Rectified Flow with distillation. It could be more concrete about which Rectified Flow model is compared in the table."
            },
            "questions": {
                "value": "Listed in the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7687/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7687/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7687/Reviewer_RDDt"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7687/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698643405802,
        "cdate": 1698643405802,
        "tmdate": 1699636935569,
        "mdate": 1699636935569,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yQNngj9j6C",
        "forum": "ymjI8feDTD",
        "replyto": "ymjI8feDTD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7687/Reviewer_csVy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7687/Reviewer_csVy"
        ],
        "content": {
            "summary": {
                "value": "The work proposes a new work that is able to train generative models from scratch and distill a faster generative model from teacher models. At its core, CMT tries to learn an ODE transition function, which is achieved by minimizing a combination of soft-matching loss, and DSM loss. Moreover, the authors suggest additional GAN to treat numerical accuracy for perceptual quality. The work achieved SOTA performance with the help of a pre-train teacher model and additional gan discriminator."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- SOTA performance\n- Theorem 2 and related analysis and Classifier-Rejection Sampling is interesting and non-trivial. They are new and novel to me."
            },
            "weaknesses": {
                "value": "Many of the losses and concepts are not entirely new, and there are some existing works that share similar ideas. Since this work achieves state-of-the-art (SOTA) performance, I encourage the authors to include more experimental details (see below) and empirical analyses in the main paper.\n\nSome recent SOTA distillation or generative works are difficult to reproduce, such as the Guided Distilled Stable Diffusion (Meng et al.). The authors also encountered difficulties in reproducing CD, even when the source code is available. I would like the authors to discuss the challenges and unstable factors associated with reproducing CD, and whether similar factors will affect CMT. Have the authors encountered any stability issues?\n\nThe authors mentioned that the weights of GAN are tuned similarly to VQGAN. Can the authors provide concrete implementation details?"
            },
            "questions": {
                "value": "See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7687/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698653400253,
        "cdate": 1698653400253,
        "tmdate": 1699636935456,
        "mdate": 1699636935456,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GEDOTM0q59",
        "forum": "ymjI8feDTD",
        "replyto": "ymjI8feDTD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7687/Reviewer_6usj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7687/Reviewer_6usj"
        ],
        "content": {
            "summary": {
                "value": "The authors extend the recently introduced Consistency Models framework. Consistency Models were proposed as a distillation method to accelerate the sampling of diffusion models. They are trained to predict the output of the Probability Flow ODE trajectory. The authors of this work extend this method by training a model that can predict any intermediate point of the Probability Flow ODE trajectory. This has two benefits: i) the trained model can recover the conditional expectation (and the score) when the starting and the final time of the Probability Flow ODE are chosen to be close, ii) the framework provides a natural way to perform multistep sampling. The authors demonstrate successfully that their method works by achieving a new SOTA on CIFAR-10 and ImageNet."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Overall, I think this is a great submission. \n\n- The topic of the paper is timely. Consistency Models have been recently proposed and they offer a promising solution for one-step generation with diffusion models.\n\n- The proposed method is simple, intuitive, and novel. \n\n- This framework provides a natural way to perform multistep sampling. It mitigates a significant limitation of Consistency Models which is that the performance was not increasing significantly even if more computational budget was available.\n- The method has great empirical success -- it achieves a new SOTA for single-step generation in ImageNet and CIFAR-10.\n- The comparison with the baselines is thorough.\n- The presentation of the method is clear. I particularly liked Figure 2."
            },
            "weaknesses": {
                "value": "I don't see any major issues, but there are a couple of small concerns.\n- The presentation of the paper could be improved a little. Figure 1 is not very helpful I think -- I would advocate making Figure 2 the main Figure of the paper. There are some typos that could be fixed with more careful reading. For example, there is a parenthesis missing in the citation at the top of page 3.\n- In the definition of the $g$ function (Lemma 1), I think the integral bounds $s, t$ have to change order (or a minus sign needs to be added in front of the integral).\n- Some of the mathematical details are unnecessarily complicated or a little sloppy. For example, the fact that $G$ can be written in terms of $g$ is trivial and hence it should not be part of the lemma. What Lemma proves is that at the limit of $s\\to t$, $g$ becomes the conditional expectation. This can be easily seen by approximating the integrand as being constant in $[t, s]$ -- similar to how the first-order ODE solvers operate. \n- Following this point, I think being able to approximate the conditional expectation with the same network is not really important. For this method to work, we need to start from a pre-trained score model anyway. Additionally, the proposed method for learning this doesn't actually work because of numerical instabilities, and hence the authors resort to adding an additional loss term.\n- Section 3.2 is a little bit confusing. The notation added is too much and makes it hard to follow what's going on. I think (?) that the main point is that another network is used for the ODE solving and the student network learns these predictions. If that's the case, this needs to be a little bit more clear."
            },
            "questions": {
                "value": "See weaknesses above. Also, why would the student outperform the teacher? And why this method gives better results that Consistency Models when s=0?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7687/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699625664160,
        "cdate": 1699625664160,
        "tmdate": 1699636935361,
        "mdate": 1699636935361,
        "license": "CC BY 4.0",
        "version": 2
    }
]