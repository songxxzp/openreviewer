[
    {
        "id": "a4DBWTb1Y7",
        "forum": "dwzLn78jq7",
        "replyto": "dwzLn78jq7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8702/Reviewer_GVUd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8702/Reviewer_GVUd"
        ],
        "content": {
            "summary": {
                "value": "Papers concerns a new SDP method to approximate the Lipschitz constant of neural networks, with the main benefit being its scalability due to the reduction of explicit constraints as penalizations. The proposed methods is itself useful, however the paper did not convince me of its technical innovation. It just seems to me that the authors simply took the methods from (Liao et al., 2023) and applied it to the LipSDP method to ensure that they have the same optimal solution."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed technique itself is nice, and may be beneficial in many aspects. The numerical experiments are also convincing, demonstrating its usability. The presentation is clear and easy to follow."
            },
            "weaknesses": {
                "value": "1. The theoretical innovation is relatively weak, because anyone can apply different techniques in the convex programming space and use it on LipSDP to claim as a new idea. I didn't see any specific analysis and/or new technique proposed specifically for the Lipschitz problem of NN.  Maybe I am overlooking some details, and if so please point me to these places.\n\n\n2. The authors only showed that their new formulation has the same optimal value as the old LipSDP problem. However since LipSDP is a relaxation, there are no guarantees of its accuracy. The authors introduced redundant constraints in this paper, and this kind of technique is usually used to tighten relaxation gap in classic literature, so I hope the authors could also touch upon the tightness of relaxation, especially by tuning the parameter $\\rho$, as I believe this is possible to analyze.\n\nTypos\n- On page 3, it seems like when you say \"second term of the left hand, etc\" you meant the third term, and you grouped the first two terms together implicitly. Also please explain how your third term ties to the relu function (experts may understand, but since you're explaining here please be more precise)."
            },
            "questions": {
                "value": "Not questions, but I encourage the authors to check a major line of work concerning the SDP verification of robustness of neural networks. That problem and your Lipschitz problem are very similar from a technical standpoint, and there are already a number of different branches in that space. For example brand-and-bound methods, non-convex cut, chordal decomposition (as you have mentioned), and theoretical analysis of relaxation quality. By exploring that problem I think you can benefit the Lipschitz problem as well."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8702/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8702/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8702/Reviewer_GVUd"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8702/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698176662054,
        "cdate": 1698176662054,
        "tmdate": 1699637090687,
        "mdate": 1699637090687,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zMYdf5OIVE",
        "forum": "dwzLn78jq7",
        "replyto": "dwzLn78jq7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8702/Reviewer_peb1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8702/Reviewer_peb1"
        ],
        "content": {
            "summary": {
                "value": "The authors consider the classical Shor semidefinite programming (SDP) relaxation of the Lipschitz constant of neural networks. \nThey formulate this SDP program as a non-smooth eigenvalue problem with an exact penalty (EP) parameter. The resulting formulation is called EP-LipSDP. \nThe exact penalty parameter is obtained by means of proper redundant quadratic constraints. \n\nThen they provide different numerical algorithms to handle this eigenvalue optimization, including eigenvector approximation via Lancsoz' method, sparse matrix multiplication, and analytical initialization. These different algorithms allow one to tune the running time and convergence of the optimization process.\n\nThe resulting algorithm, called LipDiff is compared to standard techniques on three neural network benchmarks: MNIST DNN, MNIST CNN and CIFAR10 CNN."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors can obtain a non-trivial Lipschitz constant estimate on the CIFAR10 CNN.\nThanks to the different numerical techniques, the resulting algorithm can be tuned for adaptation to allocated computational resources, in particular memory."
            },
            "weaknesses": {
                "value": "1. The initial problem of Lipschitz constant computation can actually be formulated as a polynomial optimization problem. The authors have almost completely forgotten to mention related papers based on techniques from this field (at the exception of the one by Latorre et al.), in particular the following ones:\n\n- Chen et al. Semialgebraic optimization for Lipschitz constants of ReLU networks\n- Chen et al. Semialgebraic representation of monotone deep equilibrium models and applications to certification\n- Mai et al. Exploiting constant trace property in large-scale polynomial optimization\n\nThe latter paper relies on the constant (rather than bounded) trace property, and a proper comparison would be valuable. \n\n\n2. The LipDiff algorithm might not be very accurate in general. From the comparison on the basic benchmark MNIST DNN, one notices that LipSDP obtains a better bound. It would be interesting to see how LipSDP behaves on, e.g., MNIST CNN by using LipSDP with Mosek on a scalable computing platform with more RAM. \nFirst order methods are usually much faster than second-order solvers (in particular interior-point methods) but they are also much less accurate in general. The trade-off between accuracy and efficiency should be more explicitly investigated. \nIt would be also interesting to see what kind of results first-order SDP solvers like COSMO would yield."
            },
            "questions": {
                "value": "- p2: \"Dathathri et al. (2020) solved a different problem\" => which problem?\n\n- p7: Typo \"the the\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8702/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8702/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8702/Reviewer_peb1"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8702/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698267954341,
        "cdate": 1698267954341,
        "tmdate": 1699637090569,
        "mdate": 1699637090569,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7VQdVgFHi2",
        "forum": "dwzLn78jq7",
        "replyto": "dwzLn78jq7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8702/Reviewer_LyYu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8702/Reviewer_LyYu"
        ],
        "content": {
            "summary": {
                "value": "The authors tackle the problem of Lipschitz constant estimation of neural networks. Semidefinite programs (SDP) have shown great success for Lipschitz constant estimation, but the memory and time consumption avoid them to scale to modern neural network architectures. By transforming the problem into an eigenvalue problem, the proposed method is more memory efficient than the existing methods for solving SDP, and performs much better than simple matrix production norm bound."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. The proposed method seems reasonable and promising.\n2. The reduction into the eigenvalue problem is very useful and makes the proposed method practical and provides potential for further optimization."
            },
            "weaknesses": {
                "value": "1. While I can get that transforming into an eigenvalue problem would be beneficial (as there are many past developments in solving such problems), it is still kind of vague why and to what extent the proposed method is more memory efficient."
            },
            "questions": {
                "value": "1. Could the authors list the time and memory of each method, so it is clearer for comparison?\n2. Is it possible to provide some kind of lower bound to show how good the proposed method is? LipSDP serves as a comparison target, but is only viable in one of the compared architectures. Showing that the proposed method is competitive with LipSDP on more architectures can make the paper more convincing."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8702/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8702/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8702/Reviewer_LyYu"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8702/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698830722416,
        "cdate": 1698830722416,
        "tmdate": 1699637090449,
        "mdate": 1699637090449,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uHCx8Abg6s",
        "forum": "dwzLn78jq7",
        "replyto": "dwzLn78jq7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8702/Reviewer_CCdS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8702/Reviewer_CCdS"
        ],
        "content": {
            "summary": {
                "value": "The authors proposed an interesting reformulation (LipDiff) to use SDP to estimate the Lipschitz smoothness of a Neural Network (specifically, LipSDP.) The major observation is that there is a trace bound on the Shor's relaxation on the \"Rayleigh quotient\" QCQP of the Lipschitz-smoothness problem inspired by [Wang et al., 2022, section 5]. And inspired by [Liao et.al., 2023], the SDP primal problem with an explicit trace bound has an equivalent objective value to a dual optimization problem with only box constraints and a \"simple\" objective function with linear + $\\rho\\cdot$positive(eigmax(X)) for large enough penalty $\\rho$ that's computable. For such a boxed-contrained optimization problem with maximum eigenvalue in the objective function, the authors apply the Lanczos-inspired optimization method [Dathathri et al., 2022] and try to exploit the sparse structure in LipDiff. In the experiment section, the authors demonstrate the proposed method on \"classical\" multiple-layer CNN trained on realistic datasets."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The paper is surprisingly correct, given so many omitted parts. The result that the LipSDP can be reformulated as a boxed-constrained minimax eigenvalue problem is surprising and probably has some deeper connections. Further, although the proposed method is \"inspired\" by many recent papers, there are generally some adaptions/improvements to the original formulation or proofs. For example, the QCQP \"Rayleigh quotient\" problem considered in  [Wang et al., 2022, section 5] is L-$\\infty$-based, but it's L2-based in this work. The proofs in [Liao et al., 2023] require strict feasibility, but the authors generalized it to boxed constraints without requiring strict feasibility. The Lanczos-inspired optimization method in [Dathathri et al., 2022] doesn't exploit sparsity, but the authors try it in the work. Thus, the paper is not just a blind combination of various state-of-the-art methods. Thus, I would recommend an acceptance."
            },
            "weaknesses": {
                "value": "First, let me complain that the authors should have included more context in the description and derivations. This makes verification hard, but fortunately, the proofs are short and correct. Other than that, my biggest concern is the equivalence of the formulations in eq (1) and (3), in which the authors \"proved in the footnote\" by stating \"it can be proved\". Unfortunately, I didn't find the equivalence in the mentioned literature. In the worst case, eq (3) is an upper bound of eq (1) for the additional constraints (not equivalence), which is still okay due to its computational advantages. Further, the paper didn't do an approximation ratio analysis like [Wang et al., 2022, section 5] and other works. Finally, I am not sure the minimization of linear + positive(maxeig) objective function is easier to solve than the original SDP or the manifold alternatives (E.g., ManOpt.) The proposed formulation minimax in nature so the gradient-based method may be pretty unstable."
            },
            "questions": {
                "value": "1. Please explicitly clarify the relationship between the objective values in the different formulations (scaling / square roots).\n2. P3 after eq (1): The \"last\" term, not the 2nd term.\n3. Footnote 2: I'm unconvinced about the abovementioned equivalence. Also, please explain why it's $\\epsilon/2$-smooth.\n4. P5 on adding a trace norm: the rationale is not explained unless the reader goes to [Liao et.al., 2023].\n5. P5 on the exact penalty: [Liao et al., 2023] doesn't apply directly to the non-negative constraints, but your proof is correct.\n6. P13 in Appendix A: the $\\Lambda$s are not defined. Also, why $\\xi/2$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8702/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8702/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8702/Reviewer_CCdS"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8702/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699862388116,
        "cdate": 1699862388116,
        "tmdate": 1699862388116,
        "mdate": 1699862388116,
        "license": "CC BY 4.0",
        "version": 2
    }
]