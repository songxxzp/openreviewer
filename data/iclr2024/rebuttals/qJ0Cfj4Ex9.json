[
    {
        "title": "Learning Grounded Action Abstractions from Language"
    },
    {
        "review": {
            "id": "ue6URa3Ohf",
            "forum": "qJ0Cfj4Ex9",
            "replyto": "qJ0Cfj4Ex9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6756/Reviewer_geyi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6756/Reviewer_geyi"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method that learns a library of symbolic action abstractions (i.e., high-level actions) using LLMs.\nGiven a natural language instruction and a state (object classes, predicates, etc.), the proposed method uses LLMs to plan a sequence of high-level actions.\nIn the case of the undefined operator of any high-level action, it further uses LLMs to define the operator based on in-context examples.\nSuch obtained operators are iteratively refined.\nThe low-level actions to conduct each high-level action are acquired by BFS such that they satisfy the desired subgoal state.\nThe low-level action policies are updated based on rewards from the environment after task completion with the policies.\nThe proposed method outperforms baselines in its empirical validations based on ALFRED and Mini Mincraft."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is generally written well and easy to follow.\n- The paper tackles an important issue of action grounding present in LLMs used for action planning.\n- Using LLMs to acquire high-level actions and their unknown operators looks reasonable and intriguing.\n- The two-staged pipeline for the generation of candidate operator definition is well-motivated and sounds sensible.\n- The proposed method achieves strong performance over the baselines by noticeable margins."
                },
                "weaknesses": {
                    "value": "- Some assumptions made are a bit practically unrealistic. For example, $\\Phi$ is assumed to be perfect and all environmental information is known, but they are usually not the case, especially for task planning for robotic agents (Shridhar et al., 2020; Krantz et al., 2020; Weihs et al., 2021).\n- Obtaining high-level action abstractions needs the corresponding low-level policies to generate low-level actions but it seems that it requires extensive interaction with environments (e.g., brute-force search to find a low-level action sequence that satisfies the subgoal condition). Can the proposed method be also applied to some offline scenarios without these interactive environments? And is there any efficient approach to this?\n- The detail of the baseline in Table 1 is unclear. For example, for \"Code Policy Prediction,\" the authors prompt the LLM to predict 1) imperative code policies in Python and 2) the function call sequences with arguments. What is the modification made for each? \n\n\\* Krantz et al. Beyond the Nav-Graph: Vision-and-Language Navigation in Continuous Environments. In ECCV, 2020.\n\n\\* Weihs et al. Visual Room Rearrangement. In CVPR, 2021."
                },
                "questions": {
                    "value": "See weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6756/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6756/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6756/Reviewer_geyi"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6756/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697898559048,
            "cdate": 1697898559048,
            "tmdate": 1699636778524,
            "mdate": 1699636778524,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KsQRY9LOCB",
                "forum": "qJ0Cfj4Ex9",
                "replyto": "ue6URa3Ohf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6756/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6756/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review!"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful review and positive comments on our work! We answer your questions and general clarifications below:\n\n**Are the planning assumptions realistic?**\nThe assumptions we make are standard for many grounded instruction following settings in prior work (including many of those that use the ALFRED benchmark, recent work that uses LLMs to interact with the Minecraft domain through the implemented Mineflayer API, like our Voyager baseline.) \nThat being said, our approach should actually be applicable to environments under uncertainty. The symbolic operators that we learn do not *need* to assume access to perfectly downward refinable predicates (and in fact, in ALFRED, as we discuss in 4.1, the symbolic predicates are not actually a perfect model of low-level geometric detail in the environment) -- we give this formulation because it helps clarify the optimal learning case. The low-level policy learning technique we use is also a general one that is applicable to arbitrary stochasticity in the underlying environment as well. Thanks, and we will update our paper to clarify this!\n\n**Can the proposed method be also applied to some offline scenarios without these interactive environments? And is there any efficient approach to this?**\nThis is also an interesting idea that we can discuss as a direction for future work. Our paper is focused on the online RL learning setting (and uses a standard formulation for online policy learning), but our formulation could be adapted to use offline data in several ways and we can discuss this in future work. For instance, we might easily imagine initializing our low-level policy learning algorithm with direct supervision on offline expert demonstrations and state transitions. However, our approach is primarily geared at the online learning setting because we think it's much more interesting -- if you had a set of offline annotated expert demonstrations, there might be less need to use language as a prior to learn good action abstractions.\n\n**The detail of the baseline in Table 1 is unclear. For example, for \"Code Policy Prediction,\" the authors prompt the LLM to predict 1) imperative code policies in Python and 2) the function call sequences with arguments. What is the modification made for each?**\nThank you for your feedback -- we will update our paper to try and further clarify this baseline, though as it is intended to reimplement key aspects of the Voyager model, we also refer to that for details. Following that paper (and adapting it to be as close as possible to the learning setting in our work), the code policy prediction model also attempts to learn and refine a library of skills in two stages -- these stages just involve Python code (instead of planner-compatible PDDL operators) and use the LLM as a planner instead of Fast-Downward + Policy search. In stage (1), much like our task decomposition step, we condition on the lingusitic goal and ask the LLM to produce the names and the symbolic code definitions of functions with arguments that define composable policies (eg. for *picking_up_wood* as part of an overall plan to *make an axe*). Then, in (2), as in Voyager, for each specific task, conditioned on the linguistic task description and a full symbolic description of the initial environment state, we allow the LLM to import these previously defined functions (or define new ones on the fly), and produce a plan as a sequence of grounded function calls with their arguments. We execute this sequence of grounded function calls to see if it satisfies the goal to score task accuracy on the benchmark. \n\nLet us know if we can provide further clarification, thank you!"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6756/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700170302382,
                "cdate": 1700170302382,
                "tmdate": 1700170734856,
                "mdate": 1700170734856,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lV8LIwO6wj",
                "forum": "qJ0Cfj4Ex9",
                "replyto": "KsQRY9LOCB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6756/Reviewer_geyi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6756/Reviewer_geyi"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Reviewer geyi"
                    },
                    "comment": {
                        "value": "I thank the authors for their detailed response.\nThe response addressed my concerns raised, so I'd like to keep my positive rating."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6756/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635155017,
                "cdate": 1700635155017,
                "tmdate": 1700635155017,
                "mdate": 1700635155017,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Gar9SFqqgk",
            "forum": "qJ0Cfj4Ex9",
            "replyto": "qJ0Cfj4Ex9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6756/Reviewer_ikxW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6756/Reviewer_ikxW"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a method to iteratively learn a set of action operators that can be used by a symbolic planner to generate high-level plans that are then refined into a series of low-level plans. The action operators are learned using a LLM and their selection is guided by a reward signal. To propose a set of action operators, the LLM decomposes language-based instructions into series of high-level actions. High-level actions that do not have a corresponding action operator are then passed to the LLM (along with a few-shot prompt) for the LLM to generate an operator definition consisting of the list of variables the action operates over, the preconditions that must be met to execute the action, and the effect the action has on the environment. The set of proposed action operators are then evaluated by planning with them to solve environment tasks, and are scored based on how often they are used and how often their use leads to task success. Only action operators with high scores are retained. The authors evaluate on Mini Minecraft and Alfred, and compare against several baselines that use LLMs to provide a low-level sequence of actions, specify subgoals, and to specify the plan as code. Across tasks and baselines, the proposed method performs best."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is very well written and easy to follow. \n- The authors incorporate LLM to address the challenging problem of identifying abstract actions.\n- The baselines evaluate different ways to incorporate LLMs into planning and action selection tasks.\n- The environment on which the methods are evaluated assess actions of different complexity."
                },
                "weaknesses": {
                    "value": "- It would be helpful in the results section, \"How does or approach compare to using the LLM to predict just goals, or predict task sequences?\", to call out the specific parts of the proposed algorithm that address the limitations observed in the baseline approaches.\n- It would have been beneficial to include experiments with multiple LLMs in order to understand the required LLM characteristics.\n- It is not clear from the reported experiments and results how much noise the system can handle.\n- There are no comparisons to systems that rely on hand-coded action abstractions or other methods for identifying/learning the action abstractions."
                },
                "questions": {
                    "value": "- In section 3.2, the authors state that at each iteration operators are learned for only those tasks that were not solved in the previous iteration. How often were the found plans subpar? For example, taking unnecessary, but valid actions? In the experiment section, the tasks are listed as randomly ordered. How sensitive was the found action operator library to the task ordering?\n- In the results section the authors discuss Alfred failure cases as including \"operator over specification\". When over specification occurred, were multiple instances of the action with different objects seen? For example, a slice object with butter knife and one with steak knife. Were the over specifications arbitrary or driven by the training data? For example, a steak knife was chosen even through a butter knife would also work versus the sharpness level was needed to cut the object.\n- The authors suggest that encouraging more diverse proposals could address the failure mode. Was soliciting more diverse proposals attempted? Why would more diverse proposals address operator over specification? \n- Why Alfred instead of Habitat?\n- How accurate were the different parts of the LLM's output? How correct were the mappings from language description to goal specification? \n- Might the Mini Minecraft experiments, while good to test how composable the action abstractions are, simplify the action abstraction process by learning the action abstractions on the simpler tasks for which it is easier to identify action abstractions that are more primitive? Compared to learning the actions on the compositional tasks to see how well the method is able to identify useful and flexibly reusable action abstractions?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6756/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698181732576,
            "cdate": 1698181732576,
            "tmdate": 1699636778407,
            "mdate": 1699636778407,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "khm3wW1vUb",
                "forum": "qJ0Cfj4Ex9",
                "replyto": "Gar9SFqqgk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6756/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6756/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review!"
                    },
                    "comment": {
                        "value": "Thank you for the detailed review! To clarify and answer your questions:\n\n**In the results section, \"How does or approach compare to using the LLM to predict just goals, or predict task sequences?\", can you call out the specific parts of the proposed algorithm that address the limitations observed in the baseline approaches?** The section you name (S4.1) does explain why each of these two baselines underperforms relative to our model. However, in revision we can use the additional space to re-iterate how our approach specifically addresses these ablations. To briefly summarize these points here:\n- *Low-level planning only:* As defined, this baseline has no hierarchical planning at all: it directly attempts to search over the low-level action space towards the LLM-predicted goal. This shows that the problems are, indeed, too long-horizon to make direct low-level search feasible. The fact that our approach predicts operators at all -- decomposing an otherwise infeasible long-horizon goal into higher level components that can be searched for sequentially with associated learnable policies -- is what makes hierarchical planning with 'good' operators better than this approach in general; our performance on these benchmarks shows that we *do* in fact learn 'good' operators.\n- *Subgoal-prediction:* As we point out in the results, the fact that the LLM cannot itself accurately predict the sequence of subgoals showcases the value of the explicit high-level planning algorithm that we are able to leverage by learning planner-compatible operators. In our results, we also provide additional qualititative analysis showing that the LLM struggles to accurately track environment state. Our approach uses a symbolic planner at the high-level that definitionally tracks this state and finds satisficing plans.\n\n**How would results change with different LLMs?** Please see our general response for new results using GPT-4. As we show there, our model does not substantially change with GPT-4. We also re-run the Voyager baseline with GPT-4, as this is the closest baseline to our own work, and the original paper specifically uses GPT-4. We find that it does not close the performance gap between that model and ours.\n\n**How much noise can the system handle?** Please see our general response for results from three different replications on all experiments. Overall, we find that our proposed method consistently outperforms all baselines across replications.\n\n**How do results compare with hand-coded action abstractions?** Please see our general response for a *post-hoc analysis* of the learned operators. The ALFRED benchmark provides hand-coded operators for the dataset. Our model recovers *all* of the ground-truth operators (modulo one slightly underspecified case on the Slice operator that we discuss). \n\n**How often are the plans our system learns suboptimal?** Our high-level planning algorithm is FastDownward, so it searches for optimal plans at the high-level given the operator set and goal. Our low-level planning algorithm is a breadth-first planner that also will not take unnecessary actions. This is a benefit of using structured planners -- we should never take 'valid but unnecessary' actions! We *do* qualitatively find that our other baselines, which use LLMs to *predict* sequences of actions (like the code-policy baseline), do not have this guarantee and can take unnecessary actions. We will highlight this!\n\n**How sensitive is our system to task ordering?** Please see our general response on the randomized replications -- we use random task orderings, and our system learns robustly regardless of task ordering. This is because the algorithm itself is agnostic to task ordering. At each iteration, we always first propose operators in parallel, and evaluate operators on a per-task basis over the entire training dataset. Like other library-learning algorithms (eg. DreamCoder), we effectively recover our own 'curriculum', because our system can solve 'easier' problems first as it encounters them -- if it has already learned the right operators -- and then return to more challenging problems at the next iteration."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6756/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700170274988,
                "cdate": 1700170274988,
                "tmdate": 1700170274988,
                "mdate": 1700170274988,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DAv8eDIM4W",
                "forum": "qJ0Cfj4Ex9",
                "replyto": "hXorq98JYW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6756/Reviewer_ikxW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6756/Reviewer_ikxW"
                ],
                "content": {
                    "title": {
                        "value": "Thank your for your answers"
                    },
                    "comment": {
                        "value": "Thank you for your responses and the additional results. \n\nAs for operator overspecification, occurring only once Alfred is great and the data distribution being the cause makes sense.  It would be great to have a few sentences touching on how this might generalize to the real world."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6756/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700585160178,
                "cdate": 1700585160178,
                "tmdate": 1700585160178,
                "mdate": 1700585160178,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QbbERVwVEL",
            "forum": "qJ0Cfj4Ex9",
            "replyto": "qJ0Cfj4Ex9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6756/Reviewer_xxKc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6756/Reviewer_xxKc"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose to exploit the world knowledge of LLMs for learning action abstractions for hierarchical planning. These action abstractions can then be used to solve long-horizon planning problems, by decomposing a goal into subgoals and solving them using bi-level planning. More specifically, given a task and symbolic state, the authors use LLMs to propose symbolic (high-level) operators and their corresponding definitions (in PDDL) which are then used by a bi-level planner to generate a feasible low-level plan. The useful operators (planning-compatible and grounded) are retained in an operator library (i.e. reusable) and used for subsequent tasks, including those that require the composition of learned operators."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Overall, the paper is well-motivated, clearly written, and supported by strong empirical evidence.\n2. The proposed idea of exploiting knowledge of LLMs for action abstraction is intuitive and effective and would be of interest to the planning and decision-making community."
                },
                "weaknesses": {
                    "value": "It would be nice to have some statistics on the length of the plan sequence (both in terms of high-level and low-level actions), and the number of learned operators, to get an understanding of the task complexity (especially for the compositionality experiments) in Minecraft and ALFRED domains. It is hard to get an idea from just the empirical evidence."
                },
                "questions": {
                    "value": "1. In Sec 3.1 (Symbolic operator definition), is there a process through which you identify and/or discount semantically similar (redundant) operators from being added in the operator library since the LLM generation is not conditioned on it (i.e. the LLM is not aware of the existing operators in the library).\n\n2. In Sec 3.4 (Scoring LLM Operator Proposals), the operators are selected based on their executability in the low-level planning, but the overall goal is not accounted for. Wouldn't this lead to the selection of some operators that are just \"feasible\" but not \"useful\"?\n\nMinor Comment:\n* Sec 3.4: s/b > \\tau_r"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6756/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6756/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6756/Reviewer_xxKc"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6756/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698536090843,
            "cdate": 1698536090843,
            "tmdate": 1699636778274,
            "mdate": 1699636778274,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SoRlKzeIZ2",
                "forum": "qJ0Cfj4Ex9",
                "replyto": "QbbERVwVEL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6756/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6756/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review!"
                    },
                    "comment": {
                        "value": "Thanks for the thoughtful review and close read -- these are all great clarification points. To answer your questions and provide clarification on your comments:\n\n**Can you provide statistics on task and plan complexity (high and low-level plans) for your benchmarks?** Great question, and we will update our Appendix to include these numbers:\n- On *ALFRED*, high-level plans compose **5-10** high-level operators, and low-level action trajectories have on average **50** low-level actions. There **over 100** objects that the agent can interact with in each interactive environment. We refer to the ALFRED paper (https://arxiv.org/pdf/1912.01734.pdf) for additional details on this benchmark.\n- On *Mini-Minecraft*: **mining** problems have 2-4 high-level actions and on average 6.9 low-level actions; **crafting** have 4-6 high-level actions and on average 10.24 low-level actions; and **compositional** tasks have 2-26 high-level actions and on average 41.5 low-level actions.\n\n\n**How do we identify and discount semantically similar (redundant) operators from being added in the operator library?**\nAlso a good question, and one that helps distinguish our work from other skill learning papers (eg. Voyager). We avoid problems with redundancy in two ways:\n- First, the off-the-shelf high-level planning algorithm we use (FastDownward) is designed not to incur any additional computational cost for redundancy in the operator set. This is one major advantage of using a planning-compatible operator representation, rather than general purpose code -- we can leverage existing planners like this one.\n- Second, however, in general we don't want to include semantically redundant operators for library interpretability. We do this by incrementally growing the library as needed during learning -- we always attempt to plan first with the highest scoring operators, then only try out new operators if planning fails (indicating that we may need a new skill). Wewill update the manuscript to clarify this important detail. \nEmpirically, as you can see in our Appendix (A.2, Learned Operators), this means we don't learn redundant operators (and as we discuss in the general response, on ALFRED we in fact manage to just recover the ground truth hand-designed operator library without redundancy).\n\n**How do we avoid learning operators that are merely 'feasible' but not 'useful' if the overall goal success is not included in the operator score?** Also a good and subtle point that we will highlight in our revision. This falls out of how the high-level planner algorithm works: it is an optimizing planner that searches for minimum-description length plans that only include operators which are useful for achieving the goal specification. As a consequence, operators are only used in plans in the first place if they are 'useful' towards the predicted goals; they will only be verified at the low-level and retained in the library if they were both useful *and* achievable in the low-level environment.\n\n*Minor Comment: Sec 3.4: s/b > \\tau_r:* Thank you for pointing out the typo. We will fix the typo in the updated version of the paper. \n\nPlease let us know if we can provide any additional clarification! Thanks again."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6756/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700170236194,
                "cdate": 1700170236194,
                "tmdate": 1700170236194,
                "mdate": 1700170236194,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Mw3473dWbO",
                "forum": "qJ0Cfj4Ex9",
                "replyto": "SoRlKzeIZ2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6756/Reviewer_xxKc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6756/Reviewer_xxKc"
                ],
                "content": {
                    "title": {
                        "value": "Requesting further clarifications"
                    },
                    "comment": {
                        "value": "Thank you authors for taking the time to write the detailed clarifications. However, I'm still a bit confused about one of your answers.\n\n* **identify and discount semantically similar (redundant) operators**: I don't see how *adding new operators only if planning fails* resolves the redundancy issue since the LLM is not aware of the already learned operators. Can you elaborate on the following examples: (1) if a \"place *args\" operator is present in the library and the LLM suggests a \"put *args\" operator which is similar to the former, along with some additional operators which were missing earlier and had led to planning failure. (2) if a \"clean *args\" operator is present in the library and the LLM proposes a \"clean_and_cool *args\" operator along with some additional operators ... There should be some strict checks to identify this or resolve it at the source by providing the LLM with the known operators. Of course, the latter feels harder to control. I understand the first point that you made about the design of the high-level planner, nevertheless, it seems this library could grow endlessly in more real-world settings, making it computationally expensive for the planner.\n\nI'm satisfied with the other answers."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6756/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700336376022,
                "cdate": 1700336376022,
                "tmdate": 1700336376022,
                "mdate": 1700336376022,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BhYKjZWDgk",
                "forum": "qJ0Cfj4Ex9",
                "replyto": "pDmh0pwtYE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6756/Reviewer_xxKc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6756/Reviewer_xxKc"
                ],
                "content": {
                    "title": {
                        "value": "Thank You authors"
                    },
                    "comment": {
                        "value": "Thank you for the clarifications. I believe the overall framework sets a nice tone for future work to develop and play around with similar ideas of using LLM for proposing action abstractions while engaging a bi-level planner for grounding. While I still feel that there should be additional checks in place (postprocessing steps), they could be mere engineering tweaks. \n\nI also agree with other reviewers that the distinction between Voyager and your proposed setup should be made more explicit. I have no further questions for now. Cheers!"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6756/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700387294520,
                "cdate": 1700387294520,
                "tmdate": 1700387294520,
                "mdate": 1700387294520,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3sshSJaIIJ",
            "forum": "qJ0Cfj4Ex9",
            "replyto": "qJ0Cfj4Ex9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6756/Reviewer_ukwA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6756/Reviewer_ukwA"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed to leverage LLM to solve long-horizon planning problems by dynamically building a library of symbolic action abstractions and learning a low-level policy to execute the subgoals. They conducted experiments and relevant ablation studies on Mini Minecraft and ALFRED benchmarks to demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The method is technically feasible.\n+ The writing is easy to follow. \n+ Representing abstract actions with symbols and reasoning them with LLM is an interesting attempt."
                },
                "weaknesses": {
                    "value": "+ **Lack of novelty.** This pipeline is reminiscent of the Voyager model, which also aims to tackle long-horizon tasks via the creation of a dynamic skill library. This limits the contributions of this paper. It is better to highlight the differences between this work and Voyager. \n\n+ **Benchmark is too simple.** It should be noted that the test environment used in the current work (Mini Minecraft) is significantly less complex than the original Minecraft version, resulting in reduced task difficulty. Especially the success rate in Mini Minecraft even reaches 100%. \n\n+ **Concerns about generalization ability.** The proposed method relies heavily on symbolic representations. I'm concerned that it may be difficult to generalize to complex real-world environments, which are often not easily symbolized. \n\n+ **Missing important citations.** [1, 2] are also important methods that leverage LLM as the planner to decompose the long-horizon task into subgoals. I suggest the authors to include some discussion and comparison of such methods. \n\n[1] Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents, https://arxiv.org/abs/2302.01560\n\n[2] Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory, https://arxiv.org/abs/2305.17144"
                },
                "questions": {
                    "value": "As stated in the Weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6756/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6756/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6756/Reviewer_ukwA"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6756/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698685582628,
            "cdate": 1698685582628,
            "tmdate": 1700702775809,
            "mdate": 1700702775809,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Nwiza0GBmj",
                "forum": "qJ0Cfj4Ex9",
                "replyto": "3sshSJaIIJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6756/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6756/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review!"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful review. We address your comments and questions here:\n\n**How is our model different from Voyager?**\n- First of all, we actually *compare* to Voyager, which is reimplemented as one of our baselines (the 'code policy prediction model', which uses an LLM to propose a library of Python-based policies that it retrieves and composes to solve tasks). We find that our model *dramatically outperforms this model* on both the Mini Minecraft and ALFRED benchmarks. Our discussion includes a failure analysis of this model and highlights several reasons why we outperform the Voyager implementation.\n- Unlike Voyager, our algorithm uses the LLM as a *prior* over the task decompositions and action definitions, but nests the LLM within an outer learning objective designed to *refine and verify the library of skills to learn a compact library tailored to the environment and task distribution*. In contrast, we find empirically (as in the original Voyager paper) that the Voyager objective function tends to learn 100s-*1000s* of highly overfit skills that are locally relevant to specific tasks (eg. a function specific to *mining two pieces of wood*, rather than a more general *mine_wood* function). In practice, we find that this overfitting prevents generalization and efficient planning on more complex tasks, especially ones (like ours) that require adapting to novel environments. We suspect that the success of the original Voyager paper on Minecraft is due in part to the fact that it uses a relatively high-level, human-designed API that also appears in the LLM training data.\n- Unlike Voyager, we also aim to specifically learn *planning-compatible, grounded action representations that support existing hierarchical planners from the robotics literature* for long-horizon planning. We show in our experiments that bridging between LLMs and hierarchical planners allows us to solve tasks and generalize to more complex tasks much more accurately than systems that rely on the LLM itself as a planner (consistent with other literature we cite in our related work whcih documents the difficulty of using LLMs direclty to predict plans).\n \nOur new experiments also show that the gap between the Voyager model and our approach is not closed just by using GPT-4 for code policy prediction instead of GPT3.5.\n\n\n**Are these benchmarks too simple?** While a key goal in future work is to show how this approach can scale to other domains (as we discuss in the general discussion), we would actually push back against the claim that these are very simple benchmarks for testing long-horizon planning from linguistic goals. \nOur baseline results demonstrate that both benchmarks (ALFRED and Mini Minecraft) are *very challenging* for a suite of different LLM-based planning methods, including the Voyager-based code-policy baseline. We see our strong performance on this benchmark as a good sign that -- in the optimal case in which we have access to fully specified linguistic goals -- the model can learn and use highly efficient action abstractions to solve the domain of tasks. As we point out in S4 (Domains), Mini Minecraft tasks are actually quite challenging planning tasks -- the longest crafting tasks in this benchmark compose over 26 *high-level skills* and have an action space of *over 2000 potential actions* at each time step, because we define the benchmark to permit the creation of many potential new objects during action execution.\n\n**How generalizable are methods (like ours) that use symbolic environment predicates in planning?** Please refer to our general response for an answer to this!"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6756/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700170209788,
                "cdate": 1700170209788,
                "tmdate": 1700170209788,
                "mdate": 1700170209788,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IkQgYhA5Tl",
                "forum": "qJ0Cfj4Ex9",
                "replyto": "3sshSJaIIJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6756/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6756/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "***Additional citations on planning and LLMs***. We will update our related work section on prior LLM and planning work to cite the papers you mention; thank you for bringing these to our attention! To briefly discuss the contributions of our model in relation to these:\n- [1] This paper considers a somewhat orthogonal technique (self-explanation by an LLM to fix initially proposed plans) for improving an initial LLM-produced subgoal sequence (using a basic method that is similar to the 'subgoal prediction baseline' we use in our work.) We agree that this is an important adjacent contribution to using LLMs for planning, with similar Minecraft and ALFRED-based evaluations, and will cite this work; the self-explanation technique to fix plans is likely one that could be integrated positively into our approach and any of the other baselines we report. This model does not seek to directly learn a composable library of hierarchical actions and does not seek to bridge between LLMs and structured hierarchical planning algorithms, which we see as the core contributions of this work, but does share a broader goal of decomposing long-horizon problems into subgoals.\n\n- [2] This paper also uses an LLM to decompose long-horizon planning tasks (focused on the full Minecraft game) into goal specifications and subgoals, and (similar to Voyager) also uses the LLM to retrieve the full range of plans used in prior successful task executions (including to reference previously learned actions). We will also cite this in prior work. However, as we mention in our discussion of the Voyager model, we see the controlled long-horizon experiments (using our Mini Minecraft domain) and the need to learn under ambiguity (in our ALFRED domain) as highlighting the key contribution we make over these approaches, which use an LLM as an all-purpose policy proposal, decomposition, and retrieval model without the compact library-learning objective we frame in our work: as we find our experiments, we believe that planning-specific hierarchical representations and structured planning algorithms play an important role in accurately and efficiently using learned skills in general; and we believe that we contribute a *general library learning objective for learning a compact, efficient skill library adapted to the planning algorithm at hand*. This prevents learning 1,000s of arbitrary \"skills\" that may be accurate in the short term but overfit to the distribution of goals, and we believe is a key part of learning compositional, intuitive skills.\n\nWe hope this has been helpful -- please let us know if you have any additional questions!"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6756/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700170220509,
                "cdate": 1700170220509,
                "tmdate": 1700170496804,
                "mdate": 1700170496804,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Xkmb0sqUvo",
                "forum": "qJ0Cfj4Ex9",
                "replyto": "3sshSJaIIJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6756/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6756/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Final questions before review period closes"
                    },
                    "comment": {
                        "value": "Hi Reviewer ukwA,\n\nThank you again for taking the time to review this work. As the crux of your review seemed to hinge on novelty -- especially in comparison to Voyager, which we see our work as substantially improving over both in theoretical framing and empirical results -- please let us know if there's anything else we can help answer here to finalize or increase your score, with the review period closing shortly. Thank you!\n\nBest,\nThe Authors"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6756/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689619168,
                "cdate": 1700689619168,
                "tmdate": 1700689619168,
                "mdate": 1700689619168,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MpNdOb0pjq",
                "forum": "qJ0Cfj4Ex9",
                "replyto": "Xkmb0sqUvo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6756/Reviewer_ukwA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6756/Reviewer_ukwA"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the response. However, I believe the benchmarks are still simple for me. Thus I decided to raise my rating to (5)."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6756/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700702744834,
                "cdate": 1700702744834,
                "tmdate": 1700702744834,
                "mdate": 1700702744834,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GxQqV9RJvr",
            "forum": "qJ0Cfj4Ex9",
            "replyto": "qJ0Cfj4Ex9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6756/Reviewer_uRvH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6756/Reviewer_uRvH"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the challenge of long-horizon planning. To make this more tractable, the authors leverage hierarchical planning using temporal action abstractions, breaking down intricate tasks into manageable subproblems. The novel contribution is a system that harnesses language to derive symbolic action abstractions and associated learnable low-level policies. By querying large language models (LLMs), the system proposes symbolic action definitions, subsequently integrating these into a hierarchical planning framework for grounding and verification. This approach is framed within a multitask-reinforcement-learning objective, where an agent interacts with an environment to solve tasks described in natural language. The ultimate aim is to construct a library of grounded actions that are both planning-compatible and efficient."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The system leverages language to derive symbolic action abstractions, a unique approach to decomposing complex tasks, and subsequently verifies them within a hierarchical planning framework, ensuring the practical applicability of the abstractions,  which was tested on two benchmarks, Mini Minecraft and ALFRED, and outperformed other baseline methods that incorporate language models into planning.\n\nThe paper presents a commendable effort in bridging the capabilities of large language models with hierarchical planning, the innovative approach of using language to derive action abstractions is particularly noteworthy."
                },
                "weaknesses": {
                    "value": "- Goal Misspecification: Failures on the ALFRED benchmark often occurred due to goal misspecification, where the LLM did not accurately recover the formal goal predicate, especially when faced with ambiguities in human language.\n\n- Policy Inaccuracy: The learned policies sometimes failed to account for low-level, often geometric details of the environment.\n\n- Operator Overspecification: Some learned operators were too specific, e.g., the learned SliceObject operator specified a particular type of knife, leading to planning failures if that knife type was unavailable.\n\n- Limitations in Hierarchical Planning: The paper acknowledges that it doesn't address some core problems in general hierarchical planning. For instance, it assumes access to symbolic predicates representing the environment state and doesn't tackle finer-grained motor planning. The paper also only considers one representative pre-trained LLM and not others like GPT-4."
                },
                "questions": {
                    "value": "Questions:\n\n- The two-stage prompting strategy involves symbolic task decomposition followed by symbolic operator definition. How does the system ensure that the decomposition is optimal or near-optimal for complex tasks?\n\n- The author mentioned that one of the common failures on the ALFRED benchmark was due to goal misspecification, especially when faced with ambiguities in human language. Could you elaborate on how the system currently handles such ambiguities and if there are plans to improve this aspect?\n\n- The paper demonstrates that action libraries from simpler tasks in Mini Minecraft generalize to more complex tasks. Are there plans to test this generalization capability in more diverse environments or tasks outside of the current benchmarks?\n\n- How scalable is the proposed system? Specifically, if the number of tasks or the complexity of the environment increases significantly, how would the system's performance be affected?\n\nSuggestions:\n\n- Might consider introducing an interactive feedback loop where the system can ask clarifying questions when faced with ambiguous goals or tasks. This could help in refining the task understanding and improve planning accuracy."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6756/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6756/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6756/Reviewer_uRvH"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6756/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698822152435,
            "cdate": 1698822152435,
            "tmdate": 1699636778037,
            "mdate": 1699636778037,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CYUnq7yhgh",
                "forum": "qJ0Cfj4Ex9",
                "replyto": "GxQqV9RJvr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6756/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6756/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review!"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful review and positive feedback about this work! We answer your questions below:\n**How does the system ensure that the decomposition is optimal or near-optimal for complex tasks?**\nOur operator-learning objective function explicitly optimizes for efficient and accurate planning (which we take as our definition of 'optimality') for each individual task and on the domain as whole. *On any individual task*, we use a hierarchical planning algorithm that searches explicitly for high and low-level plans that are accurate (solve the specified goal) and efficient (minimize the plan length, both in the number of operators used and the number of low-level actions taken overall.) We see one of the benefits of learning symbolic operators as precisely this ability to take advantage of efficient planning algorithms from robotics and decision making literatures. Our baselines demonstrate that the hierarchical planner is important, because ablating any part of the hierarchical planner (or substituting it with an LLM as the planner) dramatically reduces performance. *On the domain as a whole*, our scoring function over operators ensures that we learn operators which consistently support efficient and accurate hierarchical planning across the entire set of tasks, including simple and more complex tasks (like those in both of our benchmarks.) At both the individual task and domain level, our formulation allows us to use the LLM as a *proposal* mechanism for task decomposition and operator definition, but nests this proposal within a larger hierarchical planning loop to optimize for learning useful abstractions and producing optimal plans.\n\n\n**Could you elaborate on how the system currently ambiguities in goal specification?**\nAs we discuss in the paper, the human-annotated ALFRED benchmark contains many instances of ambiguity in goal specification: for instance, people may ask to *slice a tomato and put it on a table* when there are at least four possible concrete interpretations of which table is intended in the underlying benchmark (eg. the underlying benchmark distinguishes between the dining table, side table, coffee table, and kitchen counter). Currently, for our implementation and all baselines, we address this by explictly prompting the LLM to list N=4 possible interpretations of a given goal description, then attempting to plan towards all N of these interpretations (all results report the best accuracy out of these N=4 possible attempts). We also find empirically that directly prompting the LLM we use in our paper (GPT3.5) to produce N=4 distinct interpretations if ambiguities exist is more effective than sampling N=4 interpretations from the model posterior. This formulation suggests several ways to address ambiguity, and we will update our paper to address this. First, of course, we can simply take a greater number (N>4) of samples, to address cases where there are more than 4 interpretations (which is the case for many goals in ALFRED that have ambiguity in both the intended object and final goal location). More importantly, sampling explicit formal interpretations of each goal allows us to directly quantify uncertainty over the goal interpretation, as we can quantify how many unique interpretations there are (and their conditional probability given the linguistic goal under the LLM) for a given goal. In the future, as you mention, we could use this uncertainty score in interactive settings to decide when to ask a user for clarification, or to provide multiple possible concrete interpretations directly to a user for them to select the intended one. We will add this in our future work, thanks!\n\n**The paper demonstrates that action libraries from simpler tasks in Mini Minecraft generalize to more complex tasks. Are there plans to test this generalization capability in more diverse environments or tasks outside of the current benchmarks?**\nThe ALFRED benchmark also tests generalization amongst simple and complex tasks (the original benchmark includes tasks that compose multiple skills, such as the 'heated, sliced' or 'chilled and chopped' tasks that we mention in the paper). More generally, evaluating the relative generalization and compositionality of the learned operators is certainly a key part of our future work, and we will make sure that this is adequately highlighted as a potential strength of this approach, as the learned operators are designed to be composed/planned over via standard robotics planning algorithms (though, as we mention in the general discussion above and in our paper, the tasks we show here are already quite complex in terms of the actual long-range planning involved, as Mini Minecraft involves sequences of 20+ composed operators)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6756/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700170173774,
                "cdate": 1700170173774,
                "tmdate": 1700170173774,
                "mdate": 1700170173774,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZdHXvBnJHc",
                "forum": "qJ0Cfj4Ex9",
                "replyto": "GxQqV9RJvr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6756/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6756/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Final check in before review period closes!"
                    },
                    "comment": {
                        "value": "Hi Reviewer uRvH,\n\nThank you again for your detailed initial review -- we were encouraged by your positive response to the work! The feedback you gave here (eg. to clarify how goal ambiguity is handled, and to encourage comparison to other LLMs) has resulted in several new experiments that will definitely improve the overall work. With the end of the review period closing shortly, we just wanted to check in to see if there was anything else we could help answer here to finalize or increase your score.\n\nBest,\nThe Authors"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6756/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689468893,
                "cdate": 1700689468893,
                "tmdate": 1700689487318,
                "mdate": 1700689487318,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]