[
    {
        "title": "Describe-and-Dissect: Interpreting Neurons in Vision Networks with Language Models"
    },
    {
        "review": {
            "id": "hlQxh0mWSq",
            "forum": "Rnxam2SRgB",
            "replyto": "Rnxam2SRgB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6902/Reviewer_orxh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6902/Reviewer_orxh"
            ],
            "content": {
                "summary": {
                    "value": "This method introduces a post-hoc interpretability method based on recent advances in Neuron Identification and LLMs. Specifically, the authors aim to bypass the common limitation of a priori set concept sets using a captioning model, while at the same time trying to filter spurious correlations by generating synthetic images based on Stable Diffusion. Compared to other approaches like CLIP-Dissect, the method augments the set of input data by attention cropping to capture both global and local information thus modeling spatial information. Qualitative evaluation studies suggest that the model exhibits significant improvements compared to the alternatives."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This method builds upon recent advances in Neuron Identification and specifically CLIP-Dissect. Within the CLIP-Dissect framework, a probing dataset and a predefined concept are considered, aiming to uncover the individual functionality of each neuron. This is performed via a matching of the images-concept similarity vector and an activation summary of each neuron given the probing set. However, this method fails to take into account spatial information, while being restricted to the predefined concept set, usually comprising single word concepts.\n\nOn the other hand, Describe and Dissect (DnD) bypasses the first issue using attention crops of the image; this allows for capturing both global and local infomation. For the second issue, the authors introduce an approach based on the concept generation via a caption model and a summarizer based on LLMs. To select the best concepts and deal with spurious correlations in the original probing set, additional synthetic images are generated via Stable Diffusion and the selection is based on the considered scoring function. The authors introduce appropriate scoring functions and assess their impact.\n\nDespite the usage of several different methods, the paper is overall well written and easy to follow. The methods used are clear and the pipeline consistent."
                },
                "weaknesses": {
                    "value": "However I find some issues with this approach:\n\n(i) Even though the pipeline intuitively makes sense, disentangling the contribution and the impact of each module is very difficult. The authors consider three distinct architectures: (i) BLIP, (ii) GPT and (iii) Stable Diffusion. Each one introduces a different bias to the model, rendering the interpretation of the results in various settings a bit demanding. Moreover, there are additional overheads arising from the attention cropping mechanism derived from contours of the most salient regions. With each introduced module, additional parameters and complications are introduced, such as the number of candidate concepts N,  the number  generated images Q, the number of lowest ranking images beta, and the construction of the prompt for the LLM.\n\n(ii) The effect of each one of the components is not addressed in this work. This applies to both the effect of the hyperparameters but also the modules themselves. This is a very important and missing element of the approach. What is the behavior when the attention crops are removed? What if an augmented concept set is used instead of image to text captioning? How impactful are the synthetic images generated via Stable Diffusion?\n\n(iii) Overall the experimental evaluation of the approach is only based on qualitative human studies with no apparent way to quantify the results. In the alternative CLIP-Dissect method, one could use the original labels as the concept set and assess the matching between the true labels and the neuron identification results. This could be performed either via similarity in the clip space (or using other text encoders). Evidently, this is not possible in the case of DnD. \n\n(iv) In this context, the fact that DnD uses an \"augmented\" concept set renders the comparison not straightforward. It's not clear to me what are the concept sets used for the other methods. Do the authors consider the respective concept sets used in the original publications? For example the concept set for ImageNet for CLIP-Dissect is the one found in the respective code implementation of said paper? Did the authors try using a different more expressive concept set with the other methods and compare the results? One could argue that by augmenting the concept set used in the CLIP-Dissect case, one can get more expressive concepts that could appear more relevant to a human evaluator.  \n\nDespite the fact that this constitutes a post hoc interpretability based on at least 4 black-box models (Attention Cropping, Caption Model, LLM, Stable Diffusion), it's still nevertheless an interesting approach. However, the absence of important ablation studies, as well as the inability to assess the performance of the method in a setting that does not solely rely on human studies, obstructs the thorough assessment and impact of the approach."
                },
                "questions": {
                    "value": "Please see the previous section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6902/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6902/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6902/Reviewer_orxh"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6902/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698081388107,
            "cdate": 1698081388107,
            "tmdate": 1699636803578,
            "mdate": 1699636803578,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wSzcei2Nna",
                "forum": "Rnxam2SRgB",
                "replyto": "hlQxh0mWSq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6902/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6902/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to Reviewer orxh (1/3)"
                    },
                    "comment": {
                        "value": "Thank you for the thoughtful review and good suggestions! Below we discuss some concerns you had and how we have addressed them.\n\n**#1 Effect of each part of the pipeline**\n\nThank you for the suggestion! \n\nWe have discussed the effect of Stable Diffusion\u2019s synthetic images in the original draft, please see sec 4.3 and ablation study in Table 5, Fig 4, where we find that on average DnD with the full pipeline is rated 0.13 (on a scale of 5) higher than DnD without Best Concept Selection.\n\nFollowing your suggestions, we have conducted additional ablation studies on *attention cropping*, *image captioning*, and *GPT summarization* and we describe the results in below #1(a), #1(b) and #1(c) respectively. \n \n**#1(a) Ablation study on \u201cAttention Cropping\u201d**\n\nWe perform an ablation study on the effect of attention cropping to DnD performance in Appendix B.2. We qualitatively observe this change in Figure 11, noticing that the primary concepts for layer 3 neuron 475 and layer 2 neuron 467 are brought out more with attention cropping.\n\n**#1(b) Ablation study on \u201cBLIP Image Captioning\u201d**\n\nWe also perform studies exploring the effect of BLIP image captioning on our pipeline. In Appendix B.3, we compare the performance of BLIP and BLIP-2 as image captioners. We found that BLIP and BLIP2 generally give similar results as shown in below Table R1, where we compare the cosine similarity scores between the neuron descriptions generated by BLIP and BLIP2 in a ResNet 50 model. For each layer in the model, we compute the mean CLIP cosine similarity between BLIP and BLIP-2 labels for 50 randomly chosen neurons. Similar conceptual ideas between both models are reflected in the high similarity scores.\n\nIt is also interesting that we found there are some cases where BLIP2 is too specific in its captions, causing it to miss the bigger concept and underperform compared to BLIP as seen in Figure 13 in Appendix B.3 in the revised draft. We can see that with neuron 248 in layer 1, the concept should clearly be something along the lines of \u201cblack\u201d or \u201cdark.\u201d DnD w/ BLIP is able to capture this with its label \u201clow-light scenes\u201d while DnD w/ BLIP-2 gives a worse label of \u201cmonochrome timepieces.\u201d We can see that BLIP-2\u2019s captions are trying too hard to see things in these simple images, detecting the color white and objects where there are none.\n \n*Table R1: Mean Cosine Similarity Between BLIP and BLIP-2 Labels.*\n\n| Metric | Layer 1 | Layer 2 | Layer 3 | Layer 4 | All Layers |\n| - | - | - | - | - | - |\n| Cos Similarity | 0.839 | 0.844 | 0.842 | 0.868 | 0.848 |\n\nTo further measure the effect of image captioning in our pipeline, we took your suggestion and experimented with replacing image captioning with a fixed concept set in Appendix B.4. We replace BLIP with CLIP, using CLIP to find text in the 20k concept set that are most similar to the highest activating images of a neuron. The text matched with each highly activating image serve as our image captions, which we then process through the rest of our pipeline. Our qualitative results in section B.4 with Figure 14 show that these short, often single-word captions aren\u2019t descriptive enough for GPT-3.5-Turbo to find semantic similarities in. The more descriptive captions with BLIP yield better results with our pipeline. For quantitative results, we gather the labels produced by this pipeline for all neurons in ResNet-50\u2019s FC layer and follow the experiment #2(b) to find the similarity between the labels and ground truths. We compare to our regular DnD pipeline and display the results in Table 9 (Table R2 below). Our original pipeline slightly outperforms DnD w/ CLIP using both CLIP cosine similarity and MPNet cosine similarity. Note that the final FC layer is likely where a fixed concept set would perform best, as its concise and common text elements are more likely to match up with well-defined class ground truths than the often abstract and polysemantic concepts of intermediate layer neurons. As such, we find the difference to be qualitatively larger on hidden layers and as such image captioning is essential for improving performance.\n\n*Table R2: Mean FC Layer Similarity of CLIP Captioning. Utilizing a fixed concept set (t = 20k) to caption activating images via CLIP Radford et al. (2021), we compute the mean cosine similarity across fully connected layers of RN50. Final labels that do not express concepts are given a similarity score of 0. With both CLIP cosine similarity and MPNet cosine similarity, we find the performance of DnD w/ CLIP Captioning is slightly worse than BLIP generative caption.*\n| Metric / Methods | Describe-and-Dissect (Ours) | DnD w/ CLIP-Captioning | % Decline |\n| - | - | - | - |\n| CLIP cos | **0.7598** | 0.7583 | 0.197% |\n| mpnet cos | **0.4588** | 0.4465 | 2.681% |"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6902/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734244898,
                "cdate": 1700734244898,
                "tmdate": 1700734244898,
                "mdate": 1700734244898,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jjwKAUkGqf",
                "forum": "Rnxam2SRgB",
                "replyto": "hlQxh0mWSq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6902/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6902/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to Reviewer orxh (3/3)"
                    },
                    "comment": {
                        "value": "**#2(d) Application: Finding Classifiers for New Concepts**\n\nFinally, we provide a novel application of neuron descriptions to find classifiers for unseen tasks in Appendix B.8, showing we can find relatively good classifiers for unseen tasks. To find a neuron to serve as a classifier, we found the neuron whose description was closest to the CIFAR class name in a text embedding space. Using DnD descriptions we found classifiers with average AUROC of 0.7375 on CIFAR10 classes, and 0.7606 on CIFAR100 classes. In comparison, using MILAN descriptions the average AUROC was only 0.5906 for CIFAR10 and 0.6514 for CIFAR100, likely due to their more generic descriptions. While this is mostly independent as an application of our method it also serves as another quantitative metric showcasing our descriptions are more useful than descriptions generated by alternative methods.\n\n[1] Oikarinen & Weng, Clip-dissect: Automatic description of neuron representations in deep vision networks, 2023\n\n[2] Bau et al., Network dissection: Quantifying interpretability of deep visual representations, 2017\n\n[3] Kalibhat et al., Identifying interpretable subspaces in image representations, 2023\n\n[4] Hernandez et al., Natural language descriptions of deep visual features, 2022\n\n\n**#3 Clarification regarding the role of concept sets in DnD and related works**\n\nOne thing we wanted to clarify is that DnD does not utilize a sort of \"augmented\" concept set. In fact, our method is entirely concept set free. Instead, we generate image captions using BLIP and semantically combine these captions into neuron labels using GPT-3.5-Turbo. In regards to the respective concept sets used in the original publications, we utilize those discussed in their respective papers. For CLIP-Dissect, we employ the 20k concept set as that is the most expansive one discussed in its respective paper which gives it the highest performance possible on intermediate layers of networks. Network Dissection only utilizes the Broden labels which is reflected in our experiments, and MILAN, like DnD, is generative and thus does not employ a concept set.  \n\n**#4 Summary**\nIn summary we have:\n- **#1** Performed further ablation studies for all components of our pipeline to explore the impact of each component\n- **#2** Added quantitative results through our experiment on the fully-connected layer of ResNet-50, as well as two other qualitative evaluations\n- **#3** Clarified the role concept sets play in our method and other baseline works.\n\nWe believe that we have addressed all your concerns. Please let us know if you still have any reservations and we would be happy to address them!"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6902/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734690504,
                "cdate": 1700734690504,
                "tmdate": 1700734817167,
                "mdate": 1700734817167,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "K9zcWlyGdc",
            "forum": "Rnxam2SRgB",
            "replyto": "Rnxam2SRgB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6902/Reviewer_pUSu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6902/Reviewer_pUSu"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces Describe-and-Dissect (DnD), a novel method for interpreting hidden neurons in vision networks. DnD stands out from existing neuron interpretation methodologies by eliminating the need for labeled training data or predefined concept sets, which enhances its broad applicability and generalization capabilities. The approach comprises three primary stages: 1) Enriching the probing image set with crops to encompass both global and local concepts; 2) Utilizing BLIP for generating natural language descriptions of highly activating images, followed by GPT-3.5 to condense these into candidate concepts; 3) Creating new images based on these candidates using Stable Diffusion, and re-evaluating the concepts via a scoring function that considers both original and generated images. The effectiveness of DnD is demonstrated through its application to ResNet-50 trained on ImageNet-1K and ResNet-18 trained on Places365, showcasing superior performance in human evaluations compared to established methods like CLIP-Dissect, MILAN, and Network Dissection."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The research addresses a crucial aspect of model interpretability by enabling natural language interpretation of neurons.\n\n- The proposed method DnD overcomes critical limitations of prior techniques, notably the dependency on large-scale annotated training data or a pre-defined concept bank. Its training-free nature, derived from the integration of various existing foundation models, ensures robust generalization across diverse neural network architectures and data distributions.\n\n- Human evaluations verify the method's effectiveness. DnD's interpretation for ResNet models trained on ImageNet and Places365 was preferred 2x and 3x times by humans compared to prior methods.\n\n- The paper is well-written and easy to understand. The paper has a clear logical flow and detailed experimental results."
                },
                "weaknesses": {
                    "value": "- The approach depends on existing pre-trained foundation models. A more profound exploration of how different model choices impact outcomes would enhance the paper's depth. For instance, the efficacy of DnD highly relies on the captioning technique, as it derives candidate concepts from captions of top activating images. BLIP is used in this work, but its brief captions might limit the method. A comparative analysis with improved captioning systems like BLIP2 or dense captioning models such as LLAVA with prompting, could offer valuable insights into the method's current limitations and potential enhancements.\n\n- The paper mainly relies on human evaluation, making it hard to reproduce the results. Additionally, there is no mention of plans to release the data and human annotation files.\n\n- Section 3.1 on Probing Set Augmentations needs more detail and sufficient ablation studies. The specifics of how attention crops are implemented and their contribution to enhancing DnD's performance remain unclear. Furthermore, Section 4.3, including Figure 4, needs more elaboration on how the synthetic images from Stable Diffusion and the scoring functions enhance DnD's effectiveness."
                },
                "questions": {
                    "value": "- In Table 4, the average human-annotated score for \"Top K Squared + Image Products\" (representing DnD's final design) is 3.33, notably lower than the 4.15 average in Table 2 and Table 3. Could you clarify the reason for this discrepancy?\n\n- Are there any plans to release the datasets and results for broader access and verification?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6902/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6902/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6902/Reviewer_pUSu"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6902/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698689722491,
            "cdate": 1698689722491,
            "tmdate": 1699636803456,
            "mdate": 1699636803456,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9shzB9x51p",
                "forum": "Rnxam2SRgB",
                "replyto": "K9zcWlyGdc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6902/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6902/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to Reviewer pUSu (1/3)"
                    },
                    "comment": {
                        "value": "Thank you for the positive feedback and comments! We would like to address your questions and concerns below. \n\n**#1 Impact of captioning model**\n\nThank you for the suggestion! Following your suggestions, we have conducted additional experiments using BLIP2 as a captioning model in our pipeline in place of BLIP. \n\nWe found that BLIP and BLIP2 generally give similar results as shown in below Table R1, where we compare the cosine similarity scores between the neuron descriptions generated by BLIP and BLIP2 in a ResNet 50 model. For each layer in the model, we compute the mean CLIP cosine similarity between BLIP and BLIP-2 labels for 50 randomly chosen neurons. Similar conceptual ideas between both models are reflected in the high similarity scores.\n\nIt is also interesting that we found there are some cases where BLIP2 is too specific in its captions, causing it to miss the bigger concept and underperform compared to BLIP as seen in Figure 13 in Appendix B.3 in the revised draft. We can see that with neuron 248 in layer 1, the concept should clearly be something along the lines of \u201cblack\u201d or \u201cdark.\u201d DnD w/ BLIP is able to capture this with its label \u201clow-light scenes\u201d while DnD w/ BLIP-2 gives a worse label of \u201cmonochrome timepieces\u201d. We can see that BLIP-2\u2019s captions are trying too hard to see things in these simple images, detecting the color white and objects where there are none.\n\n*Table R1: Mean Cosine Similarity Between BLIP and BLIP-2 Labels.*\n| Metric             | Layer 1     | Layer 2     | Layer 3     | Layer 4     | All Layers     |\n|-----------------|-------------|-------------|-------------|-------------|---------------|\n| Cos similarity | 0.839        | 0.844        | 0.842        | 0.868        | 0.848            |\n\n**#2 Additional Ablation Studies**\n\nIn addition to the effects of Image Captioning model studied above, we have conducted several additional ablation studies to study the importance of different parts in our pipeline, summarized below:\n* **Ablation: Attention Cropping.** We added section B.2 in the Appendix where we evaluate the effect of removing attention crops from our pipeline.\n* **Ablation: Fixed Concept Set.** In Appendix B.4 we evaluate how well our network performs if we use CLIP with a fixed concept set instead of a generative image captioning model, showcasing generative models that lead to much improved performance.\n* **Ablation: Effects of GPT Summarization.**  In Appendix B.5 we evaluated the performance of our method if we remove the GPT summarization step, showcasing it leads to noticeably worse concepts.\n\n**#3 Only human evaluation**\n\nThank you for the suggestion. While we believe that in the end best evaluations of interpretability requires human in the loop, we have provided 3 additional quantitative experiments that are less human dependent.\n\n**#3(a) Additional Quantitative results: Final layer.**\n\nTo provide additional quantitative results, we follow [1, e.g. sec 4.2] to evaluate the description accuracy of the final layer neuron. In [1], the authors proposed the idea as an alternative analysis to automatically quantify the quality of neuron descriptions because the \u201cground-truth\u201d description is available for the final layers (i.e. the class name). \n\nHere, we focus on comparing our method DnD to MILAN [4], as MILAN is the only other contemporary work that provides generative descriptions (see our table 1). Other works e.g. [1, 2] use fixed concept sets which give them an advantage when calculating the similarity between labels and ground truths. This is because the \u201cground truth\u201d class labels or similar can be incorporated into the concept set. Therefore, it is expected that this evaluation method will be less favorable for generative methods like DnD and MILAN, and would be more fair to compare DnD and MILAN only. \n\nWe report our result of a ResNet-50 model trained on ImageNet below in Table R2 and also in Appendix Sec B.6 Table 10 of the revised draft. We use two embeddings (CLIP and MPNet) to determine similarity between the generated neuron descriptions and the ground truth class label. It can be seen that our DnD labels are closer to the ground truths than MILAN\u2019s by a significant margin, indicating the effectiveness of our proposed method.  \n\n*Table R2: Cosine similarity between predicted labels and ResNet-50\u2019s \u201cground truths.\u201d We can see that on average, DnD provides labels that are more similar to the ground truths than MILAN\u2019s labels.*\n| Metric / Methods | MILAN     | Describe-and-Dissect (Ours) |\n|---------------------|------------|-----------------------------------|\n| CLIP cos              | 0.7080     | **0.7598**                                    |\n| mpnet cos           | 0.2788     | **0.4588**                                     |"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6902/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733811256,
                "cdate": 1700733811256,
                "tmdate": 1700735785980,
                "mdate": 1700735785980,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YWexrVXq2X",
                "forum": "Rnxam2SRgB",
                "replyto": "K9zcWlyGdc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6902/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6902/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to Reviewer pUSu (2/3)"
                    },
                    "comment": {
                        "value": "**#3(b) Additional Quantitative results: MILANNOTATIONS**\n\nWe also experimented with using MILANNOTATIONS as another quantitative evaluation but found the annotations to be too noisy to provide a useful signal, to the point that a trivial baseline describing all neurons as \u201cimages\u201d outperforms every explanation method. See Appendix B.7 of the revised manuscript for additional details.\n\n**#3(c) Application: Finding Classifiers for New Concepts**\n\nFinally we provide a novel application of neuron descriptions to find classifiers for unseen tasks in Appendix B.8, showing we can find relatively good classifiers for unseen tasks. To find a neuron to serve as a classifier, we found the neuron whose description was closest to the CIFAR class name in a text embedding space. Using DnD descriptions we found classifiers with average AUROC of 0.7375 on CIFAR10 classes, and 0.7606 on CIFAR100 classes. In comparison, using MILAN descriptions the average AUROC was only 0.5906 for CIFAR10 and 0.6514 for CIFAR100, likely due to their more generic descriptions. While this is mostly independent as an application of our method it also serves as another quantitative metric showcasing our descriptions are more useful than descriptions generated by alternative methods.\n\n[1] Oikarinen & Weng, Clip-dissect: Automatic description of neuron representations in deep vision networks, 2023\n\n[2] Bau et al., Network dissection: Quantifying interpretability of deep visual representations, 2017\n\n[3] Kalibhat et al., Identifying interpretable subspaces in image representations, 2023\n\n[4] Hernandez et al., Natural language descriptions of deep visual features, 2022\n\n**#4 Code/data release**\n\nThank you for the question, we are committed to release the source code, data and human evaluation results in the camera-ready version for reproducibility of our result. \n\n**#5 Attention Cropping Details (Sec 3.1)**\n\nFollowing your suggestion, we provided more details on Sec 3.1, Sec 4.3 and Fig 4 below and in the revised draft. \n\nThanks for pointing this out, following your suggestion we have included a detailed description of attention cropping in Appendix B.1 of the revised draft, including Figure 10 to visualize the procedure. \n\nWe also follow your suggestion to do an ablation study on the effect of attention cropping to DnD performance in Appendix B.2. We qualitatively observe this change in Figure 11, noticing that the primary concepts for layer 3 neuron 475 and layer 2 neuron 467 are brought out more with attention cropping.\n\n**#6 Additional details on Concept Selection (Sec 4.3, Fig. 4)**\n\nTo expand more on section 4.3, we elaborate on the importance of Best Concept Selection. Due to GPT-3.5-Turbo\u2019s generative and variable nature, it can produce different outputs each time when given the same input. This means that if you simply generate one concept label from it for a neuron, it can give you different labels each time; obviously some of these labels will be better than others. To account for this fact and maximize performance, we employ Best Concept Selection. Generating images from the multiple labels GPT can produce for one neuron, we calculate which label activates the neuron the most and is thus most likely to represent what the neuron encodes. Generating new images helps us remove spurious concepts that can be present in the highly activating images of a neuron due to correlation, but do not by themselves cause the neuron to fire. An example of this is in Figure 4b, where DnD without Best Concept Selection gives \u201cMonochromatic and patterned designs.\u201d We can see that monochromatic is a spurious concept not required for high activation of neuron 927, and as such DnD with Best Concept Selection accounts for this and labels the neuron \u201cabstract swirls.\u201d This is far closer to the true label of the neuron and signifies why Best Concept Selection is essential for our pipeline to give its best performance."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6902/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734033537,
                "cdate": 1700734033537,
                "tmdate": 1700734259151,
                "mdate": 1700734259151,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZGIP43RlDo",
                "forum": "Rnxam2SRgB",
                "replyto": "K9zcWlyGdc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6902/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6902/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to Reviewer pUSu (3/3)"
                    },
                    "comment": {
                        "value": "**#7 Clarification on discrepancy between Table 4 v.s. Table 2**\n\nThank you for asking this question! \n\nFor experiment 4.2, we augment the DnD pipeline by adding Network Dissection, MILAN, and CLIP-dissect labels along with step 2's candidate concepts into the set of potential concepts fed into Best Concept Selection to increase diversity in potential candidate concepts. The purpose of this experiment is to find the highest performing scoring function, which we accomplished through Table 4 and deemed \u201cTopK Squared + Image Products\u201d to be the highest performing scoring function. By adding more diverse potential labels into the mix, we aimed to enhance the difference in quality between each scoring function\u2019s selected label. Even then, we found that including descriptions from other methods into candidate concepts decreased average concept quality, as the scoring function may still occasionally select a worse label from the other methods, causing its average evaluation score to not be as high. \n\nHowever, we can see that \u201cTopK Squared + Image Products\u201d in Table 4 and Describe-and-Dissect in Table 6 follow the same general pattern in score when traversing the intermediate layers of ResNet-50: the score starts high in layer 1, drops a bit in layer 2, and then gradually increases until layer 4. As such we can see that DnD is still generally performing the same in both experiments.\n\nAdditionally, these experiments were performed with two different groups of evaluators, so the score ranges are also naturally different. To exemplify this point, we note that the evaluators for Tables 4 and 5 are the same evaluation group, but different from the one in Table 2. We can see that the evaluation group from Tables 4 and 5 generally score all labels lower than the one from Table 2, as the former group gave an average rating of 3.97 to DnD\u2019s labels while the latter group gave an average rating of 4.15. Though these scores are more similar, this difference alongside the aforementioned augmentation of the DnD pipeline for Table 4 explain the discrepancies between the scores for Table 2 and Table 4. As such we don\u2019t intend to compare values between tables, only within the same table. We will make this point clear in the draft. \n\n**#8 Summary**\n\nIn summary we have:\n* **#1** Added an analysis on using a different captioning model (BLIP2)\n* **#2** Added several other ablation studies to understand the importance of different parts of our pipeline\n* **#3** Provided 3 additional experiments not reliant on human evaluation\n* **#4** Clarified that we plan to release our code and data with the camera-ready version\n* **#5** Clarified attention cropping pipeline and its effects\n* **#6** Discussed the importance of using synthetic images, and scoring functions\n* **#7** Clarified the discrepancy in scores between Table 4 and Tables 2 \n\nPlease feel free to let us know if you have any additional questions or comments, and we would be happy to discuss further!"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6902/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734289350,
                "cdate": 1700734289350,
                "tmdate": 1700734310718,
                "mdate": 1700734310718,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vos5DPeMEn",
            "forum": "Rnxam2SRgB",
            "replyto": "Rnxam2SRgB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6902/Reviewer_gXLM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6902/Reviewer_gXLM"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes Describe-and-Dissect, a new method to describe neurons of a neural network with natural language descriptions. The multi-stage procedure first observes the activation of a target neuron for a set of images and image crops of higher activated regions, then captions top activating images with BLIP and summarizes the generated text into a list of concept proposals with GPT-3.5. Finally, a text-to-image generation model is conditioned on the concept proposals to generate images which are used to obtain neuron activations and ultimately a score to select the best matching concept. The neuron descriptions generated by Describe-and-Dissect are preferred over existing methods based on human evaluations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Describe-and-Dissect does not require training data and generates open-set concepts without human input or guidance.\n- The pipeline is reasonable, utilizing foundation models where appropriate, and its components are well explained.\n- The user study underlines the efficacy of the method compared to existing methods.\n- Ablation studies justify design choices to the most extend, i.e., the scoring function and using a generative model to score the concepts."
                },
                "weaknesses": {
                    "value": "- While there has been precedence of other methods trying to interpret neurons and when they activate, the motivation of this problem is rather weak without a clear application. MILAN showcased an application where neurons were suppressed in order to remove spurious correlations. It would have helped the presentation of the paper if such an application was evaluated and compared to competitors.\n- At least a subset of neurons are considered to be polysemantic [1-7], that is, they do not activate on a single concept but rather activate on multiple seemingly unrelated concepts. This issue is not discussed at all in the paper and the proposed method makes the assumption that a single concept exists as step 3 filters a single concept from a list of candidate concepts. A recent method [8] instead tries to decode full representations of layers instead of individual neurons. At least some of these related works should be discussed, but ideally the issue of polysemanticity could have been analyzed as a potential limitation/failure case of the model.\n- Related to the previous point, it is unclear what the proposed method predicts for neurons that are unexplainable (e.g. due to polysemanticity or when the neuron activates on concepts not understandable by humans). How often does it happen that each of the candidate concepts describes a subset of the images the neuron activates on, but not a single concept describes all images? One limitation seems to be that in case of polysemantic neurons, Describe-and-Dissect can only extract one of the concepts and this should be clarified and possibly even evaluated. Qualitative examples of failure cases could make the picture more complete.\n- The crowdsourced experiment did not indicate the location of the neuron activation in each image. Based on the MILAN dataset it seems crucial to have this information to determine the semantic concept the neuron activates on. Why was this choice made? This type of evaluation could create a bias towards explanation methods that describe global image content. Since a captioning model was used (BLIP), part of the performance in the user study could be explained by this image captioning bias. The authors could have also used the MILAN dataset for evaluation, but this experiment is missing.\n- The authors mention that human raters have deemed some neurons to be unexplainable (caption of Table 4). This begs the question how we could tell whether a description of a neuron is trustworthy (neuron is easily explainable/monosemantic) or whether we should not rely on the generated description (neuron is not interpretable) without asking humans about every neuron.\n\n[1] Olah et al., Feature Visualization, Distill, 2017  \n[2] Olah et al., Zoom In: An Introduction to Circuits, Distill, 2020  \n[3] Mu et al., Compositional Explanations of Neurons, NeurIPS 2020  \n[4] O'Mahony et al., Disentangling Neuron Representations with Concept Vectors, CVPR Workshops, 2023  \n[5] Scherlis et al., Polysemanticity and Capacity in Neural Networks, arxiv, 2022  \n[6] Gurnee et al., Finding Neurons in a Haystack: Case Studies with Sparse Probing, arxiv, 2023  \n[7] Bricken et al., Towards Monosemanticity: Decomposing Language Models With Dictionary Learning, Transformer Circuits Thread, 2023  \n[8] Dani et al., DeViL: Decoding Vision features into Language, GCPR, 2023"
                },
                "questions": {
                    "value": "- All hyperparameters should be listed for reproducibility purposes, i.e. $\\alpha$ and $K$ (images used in step 1) do not seem to be specified.\n- What is the influence of important hyperparameters, e.g., $\\alpha$, $K$, $N$, $Q$? Why were the reported values chosen?\n- What function is used for $g$? Do you spatially average pool the activations of a neuron?\n- In Table 4, the captions mentions \"neurons deemed uninterpretable by raters were excluded\". How was this determined by the workers? Why was this data excluded? Apart from this not being too relevant for the comparison of scoring functions, this data is highly relevant and interesting for the overall evaluation of the method."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6902/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698691388311,
            "cdate": 1698691388311,
            "tmdate": 1699636803321,
            "mdate": 1699636803321,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MgeSIKDXKt",
                "forum": "Rnxam2SRgB",
                "replyto": "vos5DPeMEn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6902/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6902/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to Reviewer gXLM (1/3)"
                    },
                    "comment": {
                        "value": "Thank you for the thoughtful review and good suggestions! Below we discuss some concerns you had and how we have addressed them.\n\n**#1 Motivation/application**\n\nThank you for the suggestion! \n\nWhile we were unable to reproduce the spurious correlation experiment of MILAN because they have not released the dataset used for the experiment, we came up with a new application to showcase the usefulness of our neuron descriptions.\n\nTo showcase a potential use case for neuron descriptions (and provide another way to quantitatively compare explanation methods), we experimented with using neuron descriptions to find a good classifier for a class missing from the training set. \n\nOur setup is as follows: we explained all neurons in layer4 of ResNet-50(ImageNet) using different methods. We then wanted to find neurons in this layer that could serve as the best classifiers for an unseen class, specifically the classes in CIFAR-10 and CIFAR-100 datasets. Note there is some overlap between these and ImageNet classes, but CIFAR classes are typically much more broad. To find a neuron to serve as a classifier, we found the neuron whose description was closest to the CIFAR class name in a text embedding space (ensemble of CLIP ViT-B/16 text encoder and MPNet text encoders used in this paper). We then measured how well that neuron(its average activation) performs as a single class classifier on the CIFAR validation dataset, measured by area under ROC curve. For cases where multiple neurons shared the closest description, we averaged the performance of all neurons with that description. Results are shown below in Table R1:\n\n*Table R1: The average classification AUC on out of distribution dataset when using neurons with similar description as a classifier. We can see Describe-and-Dissect clearly outperforms MILAN, the only other generative description method.*\n\n| Metric / Methods | MILAN | Describe-and-Dissect (Ours) |\n| ------------------- | --------- | -------------------------------- |\n| CLIP cos | 0.5906 | **0.7375** |\n| mpnet cos | 0.6516 | **0.7607** |\n\nWe can see DnD performs quite well, reaching AUROC values around 0.75, while MILAN performs much worse. We believe this may be because MILAN descriptions are very generic (likely caused by noisy dataset, see Appendix B.7), which makes it hard to find a classifier for a specific class. We think this is a good measure of explanation quality, as different methods are dissecting the same network, and even if no neurons exist that can directly detect a class, a better method should find a closer approximation.\n\nWe have included this discussion in Appendix B.8 of the revised manuscript.\n\n**#2 Address polysemanticity**\n\nThank you for the suggestions! \n\nWe have added the following discussion on polysemanticity under Appendix A.1: Limitations:\n\n**Polysemantic neurons:** Existing works Olah et al. (2020); Mu & Andreas (2020); Scherlis et al. (2023) have shown that many neurons in common neural networks are polysemantic, i.e. represent several unrelated concepts or no clear concept at all. This is a challenge when attempting to provide simple text descriptions to individual neurons, and is a limitation of our approach, but can be some- what addressed via methods such as adjusting DnD to provide multiple descriptions per neuron as we have done in the Appendix B.9. Another way to address this is by providing an interpretability score, which can differentiate good descriptions from poor ones, which we have explored in the Appendix B.10. We also note that due to the generative nature of DnD, even its single labels can often encapsulate multiple concepts by using coordinating conjunctions and lengthier descriptions. However, polysemantic neurons still remain a problem to us and other existing methods such as Bau et al. (2017) Hernandez et al. (2022) Oikarinen & Weng (2023). One promising recent direction to alleviate polysemanticity is via sparse autoencoders as explored by Bricken et al. (2023)\n\nTo address this issue of polysemanticity, we have performed an experiment in Appendix B.9 in which we modify the DnD pipeline to provide multiple neuron labels when necessary, rather than just filtering for a single concept. The scoring function now chooses the top 3 candidate labels rather than just the best one. If the similarity computed by CLIP between two of these labels exceeded a certain threshold (for our purposes we used a threshold of 0.81), only the top label would be taken and the other would be discarded. Through qualitative analysis in section Figure 18, we show that this method is capable of capturing multiple concepts of a neuron. With neuron 508 from layer 2, DnD is able to capture not only that the neuron encodes for polka dot and metal textures, but also that the textures are primarily black and white. Similarly with neuron 511 from layer 3, DnD labels the neuron as not only primarily encoding for interior elements, but also finds that these elements are black and white."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6902/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733092516,
                "cdate": 1700733092516,
                "tmdate": 1700733092516,
                "mdate": 1700733092516,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "n05lk23mPF",
                "forum": "Rnxam2SRgB",
                "replyto": "vos5DPeMEn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6902/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6902/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to Reviewer gXLM (3/3)"
                    },
                    "comment": {
                        "value": "**#6 Hyperparameters**\n\nFollowing your suggestion, we have gone through in the revised draft and defined what values we set certain hyperparameters to if we didn\u2019t already in the original draft.\n\nThis paragraph is dedicated to addressing your question regarding the influence of certain hyperparameters and our reasoning behind setting them to the values we did. \n- $\\alpha$ was set to 4 as after some testing we found that value to be reasonable. Less than that and we could miss out on important concepts in an image that are causing the neuron to activate; more than that and our top activating images could be dominated by crops of one image, preventing us from taking the other highly activating images into account. \n- $K$ was set to 10 as we found through testing that a value lower than this could exclude valuable information in other highly activating images and a value higher than this could introduce spurious concepts that confuse GPT. \n- $N$ was set to 5 as we observed that GPT rarely produced over 5 distinct candidate concepts, so increasing $N$ beyond that would be superfluous. Decreasing $N$ below 5 could possibly eliminate accurate potential labels from being fed into Best Concept Selection. \n- $Q$ was set to 10 to balance quality with runtime/computational cost, as while the quality of Best Concept Selection increases with $Q$, so does the runtime. \n- $\\eta$ was set to 0.5 as, intuitively, it makes sense for at least the majority of the space covered in our bounding boxes to be distinct; otherwise the attention crops would all be very similar. If $\\eta$ was less than 0.5, we could find a problem of the crops being too small, preventing the image captioner from perceiving comprehensive concepts.\n\nFinally, we clarified in the revised draft that, unless specified, the summary function g we use for our experiments is spatial mean. Spatial mean is used as the summary function in all experiments except for the MILANNOTATIONS experiment in section B.7 and Table 11. This experiment uses spatial max as the summary function since the \u201cground truth\u201d MILANNOTATIONS were created on the highest activating images calculated with max pooling.\n\n**#7 Summary**\n\nIn summary we have:\n- **#1** Added an application/use-case of DnD\n- **#2** Performed an experiment in which DnD provides multiple labels to address issues of polysemanticity and added a discussion of polysemanticity to our limitations\n- **#3** Developed a method for finding which neurons are uninterpretable and clarified our methodology regarding uninterpretable neurons.\n- **#4** Discussed crowdsourced evaluation choices and the difficulty of providing a fair evaluation between methods\n- **#5** Performed an experiment comparing our labels to MILANNOTATIONS, and shown why this is not a useful measure of description quality\n- **#6** Clarified our hyperparameters and what summary function g we used\n\nWe believe that we have addressed all your concerns. Please let us know if you still have any reservations and we would be happy to address them!"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6902/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733528561,
                "cdate": 1700733528561,
                "tmdate": 1700734793718,
                "mdate": 1700734793718,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Y8RyP0Meln",
            "forum": "Rnxam2SRgB",
            "replyto": "Rnxam2SRgB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6902/Reviewer_YsCF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6902/Reviewer_YsCF"
            ],
            "content": {
                "summary": {
                    "value": "This paper utilizes a captioning model and a large language model to provide descriptions for highly activated images for corresponding neurons."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The Describe-and-Dissect method presented in this paper addresses the limitations of previous research and is a model-agnostic, training-free approach.\n- The authors provide adequate ablation studies to study the difference between their chosen scoring function and conception selection.\n- The authors demonstrate that their proposed approach is qualitatively better than the baseline methods through crowdsourced experiments."
                },
                "weaknesses": {
                    "value": "- The authors use crowdsourced experiments to compare with the baseline method, providing qualitative results but lacking quantitative ones, which falls short of demonstrating the effectiveness of the proposed approach.\n- The authors only use 2 datasets and 2 models  to check the efficacy of their approach. To make the paper stronger and for completeness, it will be beneficial to include more datasets and models."
                },
                "questions": {
                    "value": "Please address the weaknesses above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6902/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698825450610,
            "cdate": 1698825450610,
            "tmdate": 1699636803201,
            "mdate": 1699636803201,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cfJ3AGZxGd",
                "forum": "Rnxam2SRgB",
                "replyto": "Y8RyP0Meln",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6902/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6902/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to Reviewer YsCF (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for the feedback! Please see our response below to address your concerns. \n\n**#1 Quantitative results**\n\n**#1(a) Only has crowdsourced experiments and lacking quantitative results**\n\nThank you for the suggestion! We believe that there might be some misunderstanding here. The human study results are usually considered as \u201cquantitative\u201d analysis in the prior work in this field, please see [1, table 5][2, table 3][3, table 1, 2, A.2]. For example, human evaluators give \u201cquantitative\u201d scores (e.g. ranging from strongly disagree to strongly agree as score 1 to 5) to evaluate the quality of neuron descriptions. Our original evaluation methods followed prior works in this field [1, 2, 3] by utilizing human evaluation for results on the intermediate layers of a network, and we reported the evaluation scores in Table 2, 3, 6, 7. Our results show that we are on average rated 0.94 better than the state-of-the-art models (which is a 37.56% increase), supporting the effectiveness of our proposed approach.   \n\n\n**#1(b) Additional Quantitative results: Final layer.**\n\nTo provide additional quantitative results, we follow [1, e.g. sec 4.2] to evaluate the description accuracy of the final layer neuron. In [1], the authors proposed the idea as an alternative analysis to automatically quantify the quality of neuron descriptions because the \u201cground-truth\u201d description is available for the final layers (i.e. the class name). \n\nHere, we focus on comparing our method DnD to MILAN [4], as MILAN is the only other contemporary work that provides generative descriptions (see our table 1). Other works e.g. [1, 2] use fixed concept sets which give them an advantage when calculating the similarity between labels and ground truths. This is because the \u201cground truth\u201d class labels or similar can be incorporated into the concept set. Therefore, it is expected that this evaluation method will be less favorable for generative methods like DnD and MILAN, and would be more fair to compare DnD and MILAN only. \n\nWe report our result of a ResNet-50 model trained on ImageNet below in Table R1 and also in Appendix Sec B.6 Table 10 of the revised draft. We use two embeddings (CLIP and MPNet) to determine similarity between the generated neuron descriptions and the ground truth class label. It can be seen that our DnD labels are closer to the ground truths than MILAN\u2019s by a significant margin, indicating the effectiveness of our proposed method.  \n\n*Table R1: Cosine similarity between predicted labels and ResNet-50\u2019s \u201cground truths.\u201d We can see that on average, DnD provides labels that are more similar to the ground truths than MILAN\u2019s labels.*\n| Metric / Methods | MILAN | Describe-and-Dissect (Ours) |\n| -------------------- | -------- | -------------------------------- |\n| CLIP cos | 0.7080 | **0.7598** |\n| mpnet cos | 0.2788 | **0.4588** |\n\n\n**#1(c) Additional Quantitative results: MILANNOTATIONS** \n\nWe also experimented with using MILANNOTATIONS as another quantitative evaluation but found the annotations to be too noisy to provide a useful signal, to the point that a trivial baseline describing all neurons as \u201cimages\u201d outperforms every explanation method. See Appendix B.7 of the revised manuscript for additional details.\n\n\n**#1(d) Application: Finding Classifiers for New Concepts**\n\nFinally we provide a novel application of neuron descriptions to find classifiers for unseen tasks in Appendix B.8, showing we can find relatively good classifiers for unseen tasks. To find a neuron to serve as a classifier, we found the neuron whose description was closest to the CIFAR class name in a text embedding space. Using DnD descriptions we found classifiers with average AUROC of 0.7375 on CIFAR10 classes, and 0.7606 on CIFAR100 classes. In comparison, using MILAN descriptions the average AUROC was only 0.5906 for CIFAR10 and 0.6514 for CIFAR100, likely due to their more generic descriptions. While this is mostly independent as an application of our method it also serves as another quantitative metric showcasing our descriptions are more useful than descriptions generated by alternative methods.\n\n[1] Oikarinen & Weng, Clip-dissect: Automatic description of neuron representations in deep vision networks, 2023\n\n[2] Bau et al., Network dissection: Quantifying interpretability of deep visual representations, 2017\n\n[3] Kalibhat et al., Identifying interpretable subspaces in image representations, 2023\n\n[4] Hernandez et al., Natural language descriptions of deep visual features, 2022"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6902/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732434562,
                "cdate": 1700732434562,
                "tmdate": 1700732434562,
                "mdate": 1700732434562,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]