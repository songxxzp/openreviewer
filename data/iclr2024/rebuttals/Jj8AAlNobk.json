[
    {
        "title": "A Differentiable Sequence Model Perspective on Policy Gradients"
    },
    {
        "review": {
            "id": "EOYZEYRvSP",
            "forum": "Jj8AAlNobk",
            "replyto": "Jj8AAlNobk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2774/Reviewer_YpGq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2774/Reviewer_YpGq"
            ],
            "content": {
                "summary": {
                    "value": "The paper shows a direct connection between backpropagation and policy gradients. The authors thus leverage the advances in deep sequence models to try to improve policy gradient methods. The model proposed is called the Action-Sequence Model (ASM), where the model takes the initial state and the action sequence to predict the state sequence. The authors use a few examples and a testbed called Myriad to demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper tries to study the connection between policy gradient methods and deep sequence models and then improves the stability of the policy gradient methods. \n\n+ The paper provides easy-to-understand illustrations and formulations to show the ideas of connecting deep sequence models and policy gradient methods;\n+ The paper conducts experiments with both synthetic tasks like \"one-bounce environment\", \"copy task\", etc and real-world tasks like the Myriad testbed."
                },
                "weaknesses": {
                    "value": "- There is some confusion about the experiments, especially on the comparison between action-sequence models and history-sequence models (which have states as conditions). The authors provide some explanations in the final paragraph of Page 8 and Page 9 but I don't think I am convinced. To me, state conditioning is necessary to predict the next states. Only conditioning on the actions does not provide complete information about the environment.  \n\n- The connection between policy gradients and RNNs/sequence models seems obvious in the literature. RL policies interact with environments in a recurrent function application manner, which corresponds to RNNs/sequence models. I don't quite see what brings the novel insights from the proposed understanding."
                },
                "questions": {
                    "value": "Please answer and explain the weakness points above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2774/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698313576916,
            "cdate": 1698313576916,
            "tmdate": 1699636220353,
            "mdate": 1699636220353,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wsNOHHXNec",
                "forum": "Jj8AAlNobk",
                "replyto": "EOYZEYRvSP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2774/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2774/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response"
                    },
                    "comment": {
                        "value": "Thank you for your feedback!\n\n> To me, state conditioning is necessary to predict the next states. Only conditioning on the actions does not provide complete information about the environment.\n\nIn deterministic environments, which are the object of study of our work, a sequence of actions is always sufficient to exactly determine future states, provided the inital state is given. Empirically, our experimental results show that they are sufficient for effective prediction and credit assignment. Even in certain stochastic environments, action sequences are often sufficient for next state prediction, as is seen from the latent models in [1] [2] and [3]. \n\n> The connection between policy gradients and RNNs/sequence models seems obvious in the literature.\n\nWe agree that this connection has been intuitively been brought up multiple times, in works we cited in our paper [1] [4] [5]. It can be seen as a contribution of our work to transform this intuitive connection into an actual theoretical and algorithmic framework, that allows to build algorithms with better credit assignment properties by just employing more powerful sequence models.\n\n> I don\u2019t quite see what brings the novel insights from the proposed understanding.\n\nWe encourage the reviewer to read our general response and re-evaluate the novelty of our paper. Our proposed understanding allows us, for the first time, to show how any arbitrary advanced sequence model can be used to directly improve credit assignment in policy gradient methods. \n\n[1] Amos, Brandon, et al. \"On the model-based stochastic value gradient for continuous reinforcement learning.\" Learning for Dynamics and Control. PMLR, 2021.\n\n[2] Hafner, Danijar, et al. \"Mastering atari with discrete world models.\" arXiv preprint arXiv:2010.02193 (2020).\n\n[3] Rezende, Danilo J., et al. \"Causally correct partial models for reinforcement learning.\" arXiv preprint arXiv:2002.02836 (2020).\n\n[4] Metz, Luke, et al. \"Gradients are not all you need.\" arXiv preprint arXiv:2111.05803 (2021).\n\n[5] Heess, Nicolas, et al. \"Learning continuous control policies by stochastic value gradients.\" Advances in neural information processing systems 28 (2015)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2774/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700337937682,
                "cdate": 1700337937682,
                "tmdate": 1700337937682,
                "mdate": 1700337937682,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "j7KHTxpcvZ",
            "forum": "Jj8AAlNobk",
            "replyto": "Jj8AAlNobk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2774/Reviewer_ruP8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2774/Reviewer_ruP8"
            ],
            "content": {
                "summary": {
                    "value": "This paper firstly introduced the task of reinforcement learning and the open-loop policy gradient with a deterministic MDP. Then it reformulates the policy gradient theorem using a sequence (action-sequence) model. By showing these two policy gradients are equivalent, it built the bridge between sequence modeling and the policy gradient.\n\nWith the theoretical connection, the authors then demonstrated that it is possible to leverage the advanced network structures in sequence modeling to improve the reinforcement learning, especially the tasks that need temporal credit assignment. This argument is supported with empirical results both under toy experiments and larger scale testbeds."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well-written and easy to follow. The motivation and the main idea are well presented.\n- The experimental results support the claim well, suggesting that advanced sequence modeling/prediction models can indeed lead to better credit assignment prediction."
                },
                "weaknesses": {
                    "value": "I would recommend adding some clarification between the proposed method and various temporal credit assignment methods using sequence models. It would be beneficial to include comparisons in the benchmarks and experiments as well. (Some literature in this domain uses different testbeds, so additional experiments might be necessary.)"
                },
                "questions": {
                    "value": "The results would be more convincing if some visualizations of what sequence models have learned are provided. This would help verify that the sequence model is indeed learning the credit assignment property, and that more advanced architectures might indeed enhance performance.\n\nThere is literature exploring the use of sequence models for direct temporal credit assignment, such as [1], [2], [3]. It would be beneficial to establish a connection between this work and these references, given the significant overlap in the motivation and methodology. Further clarification of the connections, differences, and novelties would be appreciated.\n\n[1]. Arjona-Medina, J. A., Gillhofer, M., Widrich, M., Unterthiner, T., Brandstetter, J., & Hochreiter, S. (2019). Rudder: Return decomposition for delayed rewards.\u00a0_Advances in Neural Information Processing Systems_,\u00a0_32_.\n\n[2]. Hung, C. C., Lillicrap, T., Abramson, J., Wu, Y., Mirza, M., Carnevale, F., ... & Wayne, G. (2019). Optimizing agent behavior over long time scales by transporting value.\u00a0_Nature communications_,\u00a0_10_(1), 5223.\n\n[3]. Liu, Y., Luo, Y., Zhong, Y., Chen, X., Liu, Q., & Peng, J. (2019). Sequence modeling of temporal credit assignment for episodic reinforcement learning.\u00a0_arXiv preprint arXiv:1905.13420_."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2774/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698835243877,
            "cdate": 1698835243877,
            "tmdate": 1699636220282,
            "mdate": 1699636220282,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ohS7vD0ifK",
                "forum": "Jj8AAlNobk",
                "replyto": "j7KHTxpcvZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2774/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2774/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response"
                    },
                    "comment": {
                        "value": "Thank you for your positive feedback!\n\n> I would recommend adding some clarification between the proposed method and various temporal credit assignment methods using sequence models. [...] It would be beneficial to establish a connection between this work and these references. Further clarification of the connections, differences, and novelties would be appreciated.\n\nWe have added to the updated version of our paper a more detailed account of related work. See general response.\n\n> It would be beneficial to include comparisons in the benchmarks and experiments as well\n\nWe appreciate this suggestion. Unfortunately, as far as we know, other more common benchmarks in credit assignment are usually POMDPs (at least the reward is history-dependent) and/or discrete in nature [1]. In our paper the main focus is on MDPs with continuous action spaces, where policy optimization via backpropagation through time can be applied. We are excited to generalize our method to such environments in future work.\n\n> The results would be more convincing if some visualizations of what sequence models have learned are provided.\n\nThanks for the suggestion! We added in Figure 9 of the updated paper a visualization of the attention weights learned by an Action-Sequence Model transfom in the Toy Credit-Assignment problem. For predicting future states, the transformer attends to the only important action, the first one: in this way, when computing the policy gradient, the credit can directly flow from a reward at the end of the sequence to an action at the beginning of the sequence. This happens naturally, just by virtue of backpropagation through time and the particular architecture of the Action-Sequence Model.\n\n[1] Ni, Tianwei, et al. \"When do transformers shine in rl? decoupling memory from credit assignment.\" arXiv preprint arXiv:2307.03864 (2023)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2774/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700337876591,
                "cdate": 1700337876591,
                "tmdate": 1700337876591,
                "mdate": 1700337876591,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gcuqc8qvBt",
                "forum": "Jj8AAlNobk",
                "replyto": "ohS7vD0ifK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2774/Reviewer_ruP8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2774/Reviewer_ruP8"
                ],
                "content": {
                    "comment": {
                        "value": "I am thankful for the author's response and the subsequent updates made to the paper, particularly the enhanced discussion of related work and the inclusion of similar papers from this domain."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2774/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685274323,
                "cdate": 1700685274323,
                "tmdate": 1700685274323,
                "mdate": 1700685274323,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "56w1FlWmVM",
                "forum": "Jj8AAlNobk",
                "replyto": "y00si5gdOG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2774/Reviewer_ruP8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2774/Reviewer_ruP8"
                ],
                "content": {
                    "comment": {
                        "value": "While I acknowledge the revisions made to the related work section of the paper, unfortunately my current rating remains unchanged. This decision is based on my view that the significance of this work has not been sufficiently clarified to warrant a higher score. Furthermore, the scope of the experiments presented in the paper appears somewhat limited, particularly considering that this work is confined to MDPs with continuous actions only."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2774/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690259741,
                "cdate": 1700690259741,
                "tmdate": 1700690259741,
                "mdate": 1700690259741,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KfGXJnVUw4",
            "forum": "Jj8AAlNobk",
            "replyto": "Jj8AAlNobk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2774/Reviewer_JUws"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2774/Reviewer_JUws"
            ],
            "content": {
                "summary": {
                    "value": "The paper endeavors to bridge the gap between gradient propagation in neural networks and policy gradients to advance the field of sequential decision-making. Through theoretical assertions, the paper posits that state-of-the-art neural network architectures can enhance policy gradients. However, the empirical evidence provided to support this claim is not sufficiently convincing, primarily due to the narrow scope of the testing environments utilized. While the authors report improvements in long-term credit assignment and sample efficiency within an optimal control testbed, the paper fails to demonstrate significant innovation or provide a comprehensive comparison with existing methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The theoretical analysis presented is thorough, suggesting a potential for improved understanding of policy gradient methods."
                },
                "weaknesses": {
                    "value": "1. Originality is a major concern for this submission. The idea of treating RL problems as sequence modeling tasks is not new and has been extensively covered in prior work, specifically in [1] and [2]. This paper does not clearly establish its unique contributions to the field, and the related work section is insufficiently detailed, lacking a critical analysis of how this work diverges from existing methodologies.\n\n2. The experimental design does not effectively differentiate the proposed method from established sequence modeling algorithms. A more robust comparison to state-of-the-art sequence modeling techniques, while not SAC and One-Step model, is necessary to validate the claims of the paper. Additionally, the benchmarks chosen for testing the methodology do not cover the breadth of scenarios needed to substantiate the authors' assertions. The inclusion of common continuous control benchmark tasks, such as Hopper, HalfCheetah, Walker and Antmaze, is essential for a more comprehensive evaluation.\n\n[1] Janner, Michael, Qiyang Li, and Sergey Levine. \"Offline reinforcement learning as one big sequence modeling problem.\" Advances in neural information processing systems 34 (2021): 1273-1286.\n\n[2] Chen, Lili, et al. \"Decision transformer: Reinforcement learning via sequence modeling.\" Advances in neural information processing systems 34 (2021): 15084-15097."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2774/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699224879502,
            "cdate": 1699224879502,
            "tmdate": 1699636220207,
            "mdate": 1699636220207,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IUTcoxok7p",
                "forum": "Jj8AAlNobk",
                "replyto": "KfGXJnVUw4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2774/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2774/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response"
                    },
                    "comment": {
                        "value": "Thank you for your feedback! \n\n> The idea of treating RL problems as sequence modeling tasks is not new and has been extensively covered in prior work, specifically in [1] and [2]\n\nOur work _does not treat the RL problem as whole as a sequence modeling task_. Instead, it is geared towards a problem formulation and general methodology based on policy gradients, and relies on sequence modeling as a tool to learn a model of the dynamics, and to connect the theory of deep learning to RL. Indeed, a large part of the conceptual appeal of our proposed framework compared to e.g., decision transformers, is that we are able to leverage powerful sequence models, at the same time being grounded in the traditional theory of model-based policy gradients. To state these differences in an even clearer way, in addition to the discussion about the work mentioned by the reviewer already present in our submission, we have included in the updated paper a detailed account of the relationship between our work and other work employing sequence models in RL. \n\n>  This paper does not clearly establish its unique contributions to the field, and the related work section is insufficiently detailed.\n\nThe length of our related work section was only due to space constraints. We included more discussion on the related work in the main text, the appendix of the updated paper, and on the general response above.\n\n> The experimental design does not effectively differentiate the proposed method from established sequence modeling algorithms. \n\nOur method does not introduce any new \"sequence modeling algorithm\". As opposed to that, the rationale behind our work is to marry together well-established deep learning architectures and training techniques with the core RL machinery of policy gradients. The result is a framework that, in its simplicity, both provides new theoretical understanding and natural ways to get improved credit assignment. We believe our work is a first stepping stone exploiting advanced sequence models in policy gradient methods, and we hope future work will put engineering efforts to scale our methodology to more complex tasks.\n\n> A more robust comparison to state-of-the-art sequence modeling techniques, while not SAC and One-Step model, is necessary to validate the claims of the paper.\n\nWe are not aware of any state-of-the-art sequence modeling RL algorithms immediately appropriate for our problem setting, which is concerned with long-term credit assignment in deterministic MDPs for online RL. The closest method highlighted in this review is the online decision transformer from [1]. We are in the process of running experiments using online decision transformers applied completely online, and will provide an update as soon as the results are out (**See edit, the experiments we're added into Appendix F.3**).\n\n\n> The inclusion of common continuous control benchmark tasks, such as Hopper, HalfCheetah, Walker and Antmaze, is essential for a more comprehensive evaluation.\n\nOur method is empirically evaluated for its ability to solve difficult temporal credit assignment tasks. As shown by recent work [2], the mentioned robotics locomotion benchmarks are not interesting from a temporal credit assignment perspective, and we are not aware of any common continuous temporal credit assignment MDPs beyond the environments we used. Also, note that, despite being generally low-dimensional, tasks from Myriad are based on existing dynamical systems and realistic applications such as cancer treatment, and could be considered by some readers as compelling as robotics locomotion, if not more.\n\n[1] Zheng, Qinqing, Amy Zhang, and Aditya Grover. \"Online decision transformer.\" international conference on machine learning. PMLR, 2022.\n\n[2] Ni, Tianwei, et al. \"When do transformers shine in rl? decoupling memory from credit assignment.\" arXiv preprint arXiv:2307.03864 (2023)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2774/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700337740500,
                "cdate": 1700337740500,
                "tmdate": 1700520835759,
                "mdate": 1700520835759,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "59D8MEBHMB",
                "forum": "Jj8AAlNobk",
                "replyto": "KfGXJnVUw4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2774/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2774/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Additional Results on Decision Transformers"
                    },
                    "comment": {
                        "value": "**Edit 20/11/2023**: We have included additional results on the online decision transformer on the Myriad environments. Please see Appendix F.3 and the general response. We hope that these results, along with our extended related work, clears up any confusion that our work does not differ enough from the line of work on decision transformers."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2774/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700520896135,
                "cdate": 1700520896135,
                "tmdate": 1700531948223,
                "mdate": 1700531948223,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "k2G8OROQ70",
            "forum": "Jj8AAlNobk",
            "replyto": "Jj8AAlNobk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2774/Reviewer_KDm5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2774/Reviewer_KDm5"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a model-based deterministic policy gradient method for finite-horizon MDPs with deterministic and differentiable transition kernel. In this case the cumulative reward is deterministically determined given a deterministic policy, and its gradient with respect to the policy parameters, i.e. the policy gradient, can be computed by differentiating the transition kernel. Since the true transition kernel is unknown, a baseline solution is to learn a one-step Markovian transition model, then computing the policy gradient by differentiating this learned Markovian transition model. This paper however proposes to not learn one-step model, but to learn a multi-step transition model which takes a sequence of actions as input and predicts a sequence of resulted states as output. Such a multi-step transition model is called Action Sequence Model (ASM) in this paper. The policy gradient is then computed by differentiating over the learned ASM. Despite the Markovian property of the true transition kernel, the paper argues that learning such an multi-step ASM model is a better choice than learning a one-step Markovian model, for the sake of gradient based policy optimization. \n\nIt should be noted that the policy gradient discussed in this paper is limited to \"open-loop policies\" that generate actions without taking the observed states into account. For the more general class of close-loop policies,  the so-called \"open-loop policy gradient\" as defined and discussed in this paper is not the true policy gradient, but is related to the true gradient in the sense that we can obtain the former if ignoring the $\\partial a/\\partial s$ terms in the latter."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "In general, I think it is an very interesting topic to explore whether in model-based RL it's benefitable to learn not directly the underlying MDP model but another form of model. The experiment part of the paper made several good points to this end. For example, the Chaotic experiment in Section 4.2 nicely illustrates a case where even perfectly learning the Markovian transition model can lead to chaotic policy gradients while ASMs can smooth out the gradients and therefore lead to better policy optimization. It is also quite intriguing to see that, in Section 4.3, ASMs lead to better policy optimization than models with full history info including the states (although I'm not sure about the explanation about this phenomenon provided in the paper)."
                },
                "weaknesses": {
                    "value": "**(a)** I am not sure that the theory part (Section 3) well support the claim that action-sequence models are better choice than one-step Markovian models. Only the norm of the gradients induced by two special cases of these two classes of models are compared, but in practice the estimated gradient is often normalized so the norm is less important, in my impression. On the other hand, the accuracy of the gradient direction may be a more important factor, I suspect, but is not analyzed at all in the theory part. Also see my Question 1~4 below for several soundness concerns about the theory part.\n\n**(b)** I am not sure if the baselines in the experiment part (Section 4) are strong enough to establish the advantage of ASM over one-step models. In particular, the one-step model tested in the experiment seems to be a very simple one. See my Question 5~6 below for the detailed concerns.\n\n**(c)** The current results of this paper seem to have limited applicability scope: it seems to mainly applicable to environment that is deterministic, differentiable, with fixed episode length, where open-loop policies are sufficient for the environment. In this special case, the RL problem degenerates to a simple black-box optimization problem where we maximize an unknown but deterministic objective function over an action-sequence space. It would be more interesting if the paper can discuss more complex situations, such as those that require close-loop control."
                },
                "questions": {
                    "value": "1. Page 4, in the paragraph below Proposition 1, you said it's a \"fundamental fact\" that PG with Markovian model is \"fundamentally ill-behaved\", and you said this fact is \"analyzed in-depth in this section\". I don't quite understand this sentence and am not sure about the analysis either. By \"ill-behaved\" do you mean the exponential upper bound in Corollary 1.1? But Corollary 1.1 applies only to a special Markovian model, the model with linear units. What about other Markovian models? In what sense can we conclude that Corollary 1.1 is due to the Markovian property, instead of to the linearity or other limits of the model under consideration?\n\n2. How do we know that the upper bound in Corollary 1.1 is tight? Without tightness, an upper bound like Corollary 1.1 is not enough to support your claim that the gradient of RNN models will \"explode exponentially fast\". To support such a claim, we typically need a *lower bound* result.\n\n3. Even though it really could be proved that the gradient of Markovian model with true transition kernel grows exponentially with the horizon length -- even though we suppose this were true in this question -- this means the *unbiased* policy gradient optimization is unstable and we perhaps should not use the model-based policy gradient method at all in this case, isn't it? Importantly, although the gradient with Transformer is better bounded in this case, since we know that it's *not* the true policy gradient (because the true policy gradient has larger norm), how do we know that the gradient from transformer is different from an arbitrary small-but-biased gradient, in terms of its effectiveness to power the policy optimization?\n\n4. Page 5, in the paragraph below Corollary 1.1, you said \"Corollary 1.1 explains both the difficulties ... and the limitations ...\". What are exactly the difficulties and limitations here? Why does your upper bound result indeed *explain* them (rather than just coincident with them)? The argument here is not self-contained so it's hard to evaluate its soundness.\n\n5. In your experiments, what's the difference between the \"ASM(RNN)\" model and the \"One-step Model\"? Are they use the same linear transition kernel given by Eq.2, that is, is ASM(RNN) equivalent to unrolling one-step model for H steps? While you upgrade the ASM models from simple RNN to LSTM and Transformer, did you also try to upgrade the one-step model from a simple linear model to more sophisticated ones?\n\n6. In Section 4.3, are the environments here partially observable? Does a state info $s_t$ give a Markovian state or only the partial observation of the full state? I am not sure that the capability to see the additional state info for \"History Transformer\" can really account for its bad performance, given that the attention modules in Transformer can be trained to simply ignore the state info if they are not helpful. On the other hand, one-step Markovian models are just not appropriate for POMDP environments, so I'm not sure they should be included as baselines in this experiment."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2774/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699299967505,
            "cdate": 1699299967505,
            "tmdate": 1699636220120,
            "mdate": 1699636220120,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wY0AybaDrr",
                "forum": "Jj8AAlNobk",
                "replyto": "k2G8OROQ70",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2774/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2774/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response"
                    },
                    "comment": {
                        "value": "Thank you for the detailed feedback, we appreciate the thoroughness of your review! \n\n> in practice the estimated gradient is often normalized so the norm is less important, in my impression.\n\nClipping exploding gradients often results in biased gradients and does not always provide a benefit [1]. In fact, we tried to normalize gradients for all of our experiments, for one-step and true models, via both clipping and normalization, with no signficant gain in performance. \n\n>   On the other hand, the accuracy of the gradient direction may be a more important factor, I suspect, but is not analyzed at all in the theory part.\n\nIndeed, for some environments, accuracy can play an important role, but it is difficult to analyze in theory, since it would require accurately characterizing the training dynamics and generalization of a neural network. Instead, we provide empirical evidence to complement and confirm our theory: experiments in Section 4.1 show improved gradient accuracy for well-behaved systems, and the double-pendulum experiment in Section 4.2 demonstrates a case where using the gradient coming from a smooth but possibly inaccurate model leads to better policy optimization compared to using the real policy gradient. \n\nThis highlights a highly desirable property of using architectures such as Transformers as Action-Sequence Models. On the one hand, they are able to provide an accurate gradient when the dynamics of the environment is smooth enough; on the other hand, when the dynamics is non-smooth and thus not amenable to policy optimization, they will tend to approximate it with a smooth version of it, possibly surpassing the original dynamics in terms of ease of optimization via backpropagation through time.\n\n>  It would be more interesting if the paper can discuss more complex situations, such as those that require close-loop control.\n\nNote that our current analysis and experiments employ a closed-loop **policy**, but simply use an open-loop **policy gradient**, identical to the gradient used in state of the art model-based methods such as [2], [3] and [4]. The open-loop **policy gradient** does not necessarily induce an open-loop **policy**, and, despite its effect is yet to be well understood in the literature, it has been shown to lead to successful policies in complex tasks.\n\n> By \u201cill-behaved\u201d do you mean the exponential upper bound in Corollary 1.1?\n\nYes, Corollary 1.1 formally establishes the possibility of exploding gradients under a Markovian model.\n\n> But Corollary 1.1 applies only to a special Markovian model, the model with linear units. What about other Markovian models?\n\nIn reality, the analysis provided by Corollary 1.1 considers a non-linear network $x_t=\\sigma(W_x x_{t-1}) + W_a a_{t-1} + b$. The linearity with respect to the action quickly becomes non-linear for all future state predictions due to the recursive nature of $x_t$. The analysis can thus be extended to the more traditional non-linear formulation $x_t = \\sigma(W_x x_{t-1} + W_a a_{t-1} + b)$, just as done in the seminal work by Pascanu [5].\n\n> Without tightness, an upper bound like Corollary 1.1 is not enough to support your claim that the gradient of RNN models will \"explode exponentially fast\".\n\nIn our paper, we only claim \"*the policy gradient [...] with an RNN [...] **can** explode exponentially fast [...]*\".\n\nWe agree that a tight bound is important in order for our statement to be relevant: for this reason, we have added a justification in Appendix A.1, explaining why the bound is indeed tight. The justification is similar to the ones used to explain exploding gradients in RNNs in supervised learning [1] [5]. \n\n> Even though it really could be proved that the gradient of Markovian model with true transition kernel grows exponentially with the horizon length, [...] this means the unbiased policy gradient optimization is unstable and we perhaps should not use the model-based policy gradient method at all in this case.\n\nThis is a good observation. However, the fact that the policy gradient under the true transition kernel does not lead to successful policy optimization does not imply, surprisingly, that we should not use model-based policy gradients at all. Our experiments on the double-pendulum environment in Section 4.2 exactly demonstrate this point: the learned gradient from a better sequence model is highly preferable to the true gradient, and amenable to policy optimization. \n\n> how do we know that the gradient from transformer is different from an arbitrary small-but-biased gradient, in terms of its effectiveness to power the policy optimization\n\nWhile we cannot prove this theoretically, we show empirically in Section 4.2 that the learned well-behaved gradients from a Transformer are indeed effective for policy optimization, leading to high-perfoming policies compared to other baselines."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2774/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700337405335,
                "cdate": 1700337405335,
                "tmdate": 1700337405335,
                "mdate": 1700337405335,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EhxvaepU9x",
                "forum": "Jj8AAlNobk",
                "replyto": "k2G8OROQ70",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2774/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2774/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response (cont.)"
                    },
                    "comment": {
                        "value": "> you said \u201cCorollary 1.1 explains both the difficulties \u2026 and the limitations \u2026\u201d. What exactly are the difficulties and limitations here?\n\nPrior works in model-based policy gradients have noted the difficulties of unrolling models for long horizons, and Corollary 1.1 justifies this difficulty due to the exploding gradients. \n\n> Why does your upper bound result indeed explain them (rather than just coincide with them)?\n\nThese exploding gradients are subsequently shown in Figure 5 to result in poor policies in Figure 4.\n\n> In your experiments, what's the difference between the \"ASM(RNN)\" model and the \"One-step Model\"?\n\nFundamentally, we note in Section 3.1 that they differ only in their training objectives: *\"When training an ASM as an RNN with teacher forcing, we are essentially training its recurrent cell as a one-step model\"*. Experimentally, the ASM(RNN) is detailed in Appendix E: it is modeled as a 2-layer RNN with ReLU activations. Notably, we have added the architecture details of the one-step model, which also uses a 2-layer MLP with non-linear ReLU activations.\n\n> did you also try to upgrade the one-step model from a simple linear model to more sophisticated ones\n\nYes, all experiments use a non-linear 2 layer MLP for the one-step model. No experiment uses one-step linear models.\n\n> In Section 4.3, [...] are the environments here partially observable? [...] one-step Markovian models are just not appropriate for POMDP environments.\n\nNo, as specified in our background section, all the environments we employ in our paper are fully-observable and Markovian.\n\n> I am not sure that the capability to see the additional state info for \"History Transformer\" can really account for its bad performance, given that the attention modules in Transformer can be trained to simply ignore the state info if they are not helpful.\n\nThe poor performance of the History Transformer does not come from the inability of a transformer to ignore uninformative state information. Instead, for most environments, a transformer conditioned on the entire history will find the state information particularly useful for predicting the next states. As a notable example, in a Markovian case, the transformer can actually learn to just use the last state and action to predict the next state: in this case, unrolling the transformer will create a computational graph that would be similar to the one of an unrolled Markovian model, and thus potentially lead to ill-behaved gradients. For its particular structure, an Action-Sequence Model is instead constrained to directly attend to the actions at its input, thus directly propagating gradients from rewards to actions without unnecessarily chaining next-state predictions.\n\n\n[1] Metz, Luke, et al. \"Gradients are not all you need.\" arXiv preprint arXiv:2111.05803 (2021).\n\n[2] Hafner, Danijar, et al. \"Mastering atari with discrete world models.\" arXiv preprint arXiv:2010.02193 (2020).\n\n[3] Hafner, Danijar, et al. \"Mastering diverse domains through world models.\" arXiv preprint arXiv:2301.04104 (2023).\n\n[4] Ghugare, Raj, et al. \"Simplifying model-based rl: learning representations, latent-space models, and policies with one objective.\" arXiv preprint arXiv:2209.08466 (2022).\n\n[5] Pascanu, Razvan, Tomas Mikolov, and Yoshua Bengio. \"On the difficulty of training recurrent neural networks.\" International conference on machine learning. Pmlr, 2013.\n\n[6] Heess, Nicolas, et al. \"Learning continuous control policies by stochastic value gradients.\" Advances in neural information processing systems 28 (2015)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2774/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700337526030,
                "cdate": 1700337526030,
                "tmdate": 1700337526030,
                "mdate": 1700337526030,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]