[
    {
        "title": "ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities"
    },
    {
        "review": {
            "id": "pRO8iugy4b",
            "forum": "9Klj7QG0NO",
            "replyto": "9Klj7QG0NO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4516/Reviewer_DHvY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4516/Reviewer_DHvY"
            ],
            "content": {
                "summary": {
                    "value": "This work proposed a extensible multi-modal model named ONE-PEACE. The architecture of ONE-PEACE consists of multiple modality adapters which extract unified features from different raw signals, and a modality fusion encoder which facilitate information extraction between and within different modalities. To pretrain ONE-PEACE, this work used cross-modal contrastive learning and intra-modal denoising contrastive learning. The experimental results on different tasks across various modalities shows the advantages of the model."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The experiments in this work are very comprehensive, including extensive experiments on downstream tasks, ablation experiments, and visual results.\n+ The experimental results in the paper unquestionably demonstrate the superior performance of the model. The excellent fine-tuning and zero-shot performance across various downstream tasks in the visual, language, and audio modalities makes this model an outstanding three-modal universal model.\n+ The model has a relatively straightforward overall architecture. The functions of each module are easy to comprehend."
                },
                "weaknesses": {
                    "value": "- As an engineering project, this work is exceptional, with the proposed model demonstrating superior performance and good reproducibility. However, as an academic research, this work does not bring interesting findings or questions. It appears more like a fusion of various well-established and effective techniques, like hMLP,  Sub-LayerNorm and LayerScale. The contribution of this work should be reconsidered.\n- Experiments solely on vision, language and audio modalities cannot prove that the model can generalize to \"unlimited\" modalities. Many heterogeneous modalities are hard to collect paired data and align with a existing modality, such as sensors, tables or even proprioception [a]. An experiment on a more heterogeneous modality like IMU should be conducted at least.\n- I also wonder why the AVQA dataset is merely used for AQA task? The model is trained on paired data of two modalities, thus the performance of the model on a task with all three modalities is important. This experiment should be conducted.\n\n\n[a] P. P. Liang, Y. Lyu, X. Fan, J. Tsaw, Y. Liu, S. Mo, D. Yogatama, L.-P. Morency, and R. Salakhutdinov, \u201cHigh-modality multimodal transformer: Quantifying modality & interaction heterogeneity for high-modality representation learning,\u201d Transactions on Machine Learning Research, 2022."
                },
                "questions": {
                    "value": "The authors should provide a better exposition of the contributions of this work, especially the problems that the model addresses, rather than solely emphasizing its superior performance. The above weaknesses should be concerned.\n\nFigure. 1: Adaptor -> Adapter"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4516/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698641377697,
            "cdate": 1698641377697,
            "tmdate": 1699636428440,
            "mdate": 1699636428440,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QtFbNMxkO5",
                "forum": "9Klj7QG0NO",
                "replyto": "pRO8iugy4b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4516/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4516/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DHvY (1/2)"
                    },
                    "comment": {
                        "value": "Thanks for your time reviewing and providing the helpful suggestions for our paper. We address open questions and remarks below:\n\n> **Clarify our contributions**\n\nWe appreciate your recognition of our work as an exceptional engineering project with superior performance and good reproducibility. In the last paragraph of the introduction section, we outline the contributions of our work, including: 1) a highly extensible model with scalable-friendly architecture and modality-agnostic tasks that have the potential to expand to unlimited modalities; 2) the first representation model to achieve new SOTAs across a wide range of uni-modal and cross-modal tasks; 3) strong emergent retrieval capabilities for aligning unpaired modalities, eliminating the need for paired data collection across different modalities.\n\nHere we elaborate on the first point to emphasize the technical contributions of ONE-PEACE:\n\n1. Scalable-friendly architecture. ONE-PEACE processes different modalities through shared attention layers and separate Adapters/FFNs. When introducing a new modality, it only requires the addition of new adapters and FFNs. While previous works like VLMO [1] and BEIT-3 [2] have also explored this design, they primarily concentrate on enhancing model performance rather than highlighting the utilization for modality expansion.\n\n2. Modality-agnostic tasks. We combine two modality-agnostic pretraining objectives: cross-modal contrast and intra-modal denoising contrast, enabling the model to achieve outstanding zero-shot retrieval performance and fine-tuning performance simultaneously. We also analyzed the impact of different denoising losses in Table 11. We found that denoising contrastive loss is more compatible with cross-modal contrastive loss than other losses, it can further improve the model's performance. These experiments have not been explored well in previous works.\n\n3. With the help of scalable-friendly architecture and modality-agnostic tasks, we design a two-stage training method. It allows us to add new modalities (audio) into ONE-PEACE without affecting its performance on existing modalities (vision and language), and even enables the achievement of SOTA results in tasks related to the new modalities. This modality extension method is a significant contribution of our paper, and to the best of our knowledge, it has not been explored in previous works.\n\n> **Experiment on more heterogeneous modalities**\n\nThe design of our architecture and tasks can effectively handle heterogeneous modalities with a lack of paired data.\n\n1. Modality adapters are solely responsible for transforming the raw input into vectors, allowing us the flexibility to choose its architecture, such as CNN, RNN, and MLP, guided by previous works (e.g., HighMMT, ImageBind [3]). Therefore, this design is very friendly to heterogeneous modalities. In addition, the training curves in Figure 4 indicate that the model architecture of ONE-PEACE helps accelerate the convergence speed of the model and achieve better results.\n\n2. Denoising contrastive loss is a modality-agnostic task that can be used on any modality, and it does not rely on paired data. The results in Table 10 show that combining denoising contrast and cross-modal contrast pre-training tasks can achieve better modality alignment effects, making this design suitable for modalities that lack paired data.\n\nWe agree that experiments on more heterogeneous modalities can further demonstrate the potential of ONE-PEACE, but due to constraints during the response period, we have included a discussion in the Appendix. Additionally, we have also discussed related works, such as HighMMT, in this section.\n\n```text\n[1] Bao H, Wang W, Dong L, et al. Vlmo: Unified vision-language pre-training with mixture-of-modality-experts[J]. Advances in Neural Information Processing Systems, 2022, 35: 32897-32912. \n[2] Wang W, Bao H, Dong L, et al. Image as a foreign language: Beit pretraining for all vision and vision-language tasks[J]. arXiv preprint arXiv:2208.10442, 2022.\n[3] Girdhar R, El-Nouby A, Liu Z, et al. Imagebind: One embedding space to bind them all[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 15180-15190.\n```"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4516/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700497536019,
                "cdate": 1700497536019,
                "tmdate": 1700500568319,
                "mdate": 1700500568319,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "p8xV9YAjq7",
                "forum": "9Klj7QG0NO",
                "replyto": "pRO8iugy4b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4516/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4516/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DHvY (2/2)"
                    },
                    "comment": {
                        "value": "Thanks for pointing this out. We have re-collected the AVQA dataset, conducted experiments, and updated the results in Table 6. As shown in Table 6, ONE-PEACE also achieves excellent results in this task. Thus, we have validated the ability of ONE-PEACE in all combinations of modalities, including vision-language (visual grounding, image-text retrieval), vision-audio (video-audio classification), audio-language (audio-text retrieval, audio question answering), and vision-audio-language (AVQA).\n\n| Model              | Ensemble  |   AVQA   |\n|:-------------------|:---------:|:--------:|\n| HME [1]            | HAVF [8] |   85.0   |\n| PSAC [2]           | HAVF [8] |   87.4   |\n| LADNet [3]         | HAVF [8] |   84.1   |\n| ACRTransformer [4] | HAVF [8] |   87.8   |\n| HGA [5]            | HAVF [8] |   87.7   |\n| HCRN [6]           | HAVF [8] |   89.0   |\n| PSTP-Net [7]      |     -     |   90.2   |\n| **ONE-PEACE**      |     -     | **92.2** |\n\n```text\n[1] Fan C, Zhang X, Zhang S, et al. Heterogeneous memory enhanced multimodal attention model for video question answering[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019: 1999-2007.\n[2] Li X, Song J, Gao L, et al. Beyond rnns: Positional self-attention with co-attention for video question answering[C]//Proceedings of the AAAI conference on artificial intelligence. 2019, 33(01): 8658-8665.\n[3] Li X, Gao L, Wang X, et al. Learnable aggregating net with diversity learning for video question answering[C]//Proceedings of the 27th ACM international conference on multimedia. 2019: 1166-1174.\n[4] Zhang J, Shao J, Cao R, et al. Action-centric relation transformer network for video question answering[J]. IEEE Transactions on Circuits and Systems for Video Technology, 2020, 32(1): 63-74.\n[5] Jiang P, Han Y. Reasoning with heterogeneous graph alignment for video question answering[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2020, 34(07): 11109-11116.\n[6] Le T M, Le V, Venkatesh S, et al. Hierarchical conditional relation networks for video question answering[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020: 9972-9981.\n[7] Li G, Hou W, Hu D. Progressive Spatio-temporal Perception for Audio-Visual Question Answering[C]//Proceedings of the 31st ACM International Conference on Multimedia. 2023: 7808-7816.\n[8] Yang P, Wang X, Duan X, et al. Avqa: A dataset for audio-visual question answering on videos[C]//Proceedings of the 30th ACM International Conference on Multimedia. 2022: 3480-3491.\n```"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4516/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700497610477,
                "cdate": 1700497610477,
                "tmdate": 1700501849493,
                "mdate": 1700501849493,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "w6MQdyktDy",
                "forum": "9Klj7QG0NO",
                "replyto": "m6WbnxCJRD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4516/Reviewer_DHvY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4516/Reviewer_DHvY"
                ],
                "content": {
                    "comment": {
                        "value": "Sorry, for this comment by authors. I think it's a little rude to directly ask for raising score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4516/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729344858,
                "cdate": 1700729344858,
                "tmdate": 1700729344858,
                "mdate": 1700729344858,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GLmBwAbrCY",
                "forum": "9Klj7QG0NO",
                "replyto": "p8xV9YAjq7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4516/Reviewer_DHvY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4516/Reviewer_DHvY"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for providing the response. I will keep my score, currently."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4516/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729549629,
                "cdate": 1700729549629,
                "tmdate": 1700729549629,
                "mdate": 1700729549629,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PoMk88NKPe",
            "forum": "9Klj7QG0NO",
            "replyto": "9Klj7QG0NO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4516/Reviewer_9Evr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4516/Reviewer_9Evr"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduce ONE-PEACE, a simple but effective model for tri-modality representation learning. The proposed model use two stage training to align the visual acoustic and linguistic representation and it generalize well to downstream tasks. The paper is well written and the proposed method is reproduciable."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- the paper is intuitive, straight-forward and working as expected.\n- The paper is well written and easy to follow. The paper provides enough details to reproduce the results.\n- The results on downstream tasks are solid and convincing. Although not the SOTA as for now, but still strong enough."
                },
                "weaknesses": {
                    "value": "1. The proposed method use the two stage training method, the idea behind it is to align the visual and acoustic information with linguistic representation. This is a practical way to pre-train the model but may lead to representation mis-alignment between visual and acoustic modalities. Consider to add more results to backup the visual-acoustic feature alignment quality.\n2. The experimental results section is sufficient with different downstream results, but lacks the insights on the comparison against other LMMs, especially the ones with different designs."
                },
                "questions": {
                    "value": "1. Please further discuss if the two stage training is a compromise of dataset and data quality, the training resources or it is designed intentionally.\n2. I actually have hands on experience with ONE-PEACE. seems the visual-acoustic alignment is on and off, is this because of the dataset and data quality or the model design?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4516/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698805728175,
            "cdate": 1698805728175,
            "tmdate": 1699636428360,
            "mdate": 1699636428360,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NTkRYptav7",
                "forum": "9Klj7QG0NO",
                "replyto": "PoMk88NKPe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4516/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4516/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9Evr (1/2)"
                    },
                    "comment": {
                        "value": "Thanks for your time reviewing and providing helpful suggestions for our paper. First of all, we would like to clarify that, to the best of our knowledge, ONE-PEACE is still the SOTA in a lot of datasets, such as ADE20K, RefCOCO+, VGGSound, FSD50K, etc.\n\nWe address open questions and remarks below:\n\n> **W1: Add more results to backup the visual-acoustic feature alignment quality**\n\nSome of the experiments done in the paper, as well as our newly added experiments, can address your concerns:\n\n1. We conducted experiments on the VGGsound dataset, which requires the model to predict the category of samples based on the video and audio information. The results are listed in Table 6. It can be seen that ONE-PEACE achieved SOTA results on this task, even outperforming previous video pretraining models.\n\n| Model          | VGGSound |\n|:---------------|:--------:|\n| CAV-MAE [1]      |   65.9   |\n| MMT [2]          |   66.2   |\n| MAViL [3]      |   67.1   |\n| **ONE-PEACE**  |   68.3   |\n\n2. Following the suggestion of Reviewer DHvY30, we conducted experiments on the AVQA dataset. This dataset consists of video, audio, and text modalities, and it requires the model to answer questions based on the video and audio information. We have updated the results in Table 6. ONE-PEACE still achieves leading results on the dataset.\n\n| Model         | AVQA  |\n| :------------ | :---: |\n| PSTP-Net [4]  | 90.2  |\n| **ONE-PEACE** | 92.2  |\n\nIn addition to the above experiments, we also provide the multi-modal retrieval cases in Figure 3 and Figures 7-9 to demonstrate the visual-acoustic feature alignment quality of ONE-PEACE.\n\n> **W2: Lacks insights on the comparison against other LMMs**\n\nIn fact, the baselines compared in the paper cover various model architectures (encoder-only, dual-encoder, encoder-decoder, etc.) and pre-training tasks (contrastive loss, masked prediction, etc.). However, since these works utilize different pre-training datasets and techniques, we focus our analysis on the ablation study to ensure a fair comparison between different architectures and pre-training tasks.\n\n1. In Section 5.4, we conduct a comparative analysis of the ONE-PEACE architecture against alternative frameworks. We hypothesize that the shared attention layer facilitates the effective integration of multimodal data, while the separated FFNs enhance the model's ability to extract modality-specific information. This hypothesis is substantiated by the findings presented in Table 9. The training curves in Figure 4 also indicate that the design of the shared attention layer and separate FFN layer can lead to faster convergence speed and better convergence performance.\n\n2. We further analyze the impact of various pre-training tasks. In comparison to exclusively utilizing cross-modal contrastive learning, the inclusion of denoising loss enables the model to learn fine-grained information within modalities, thereby leading to better fine-tuning and zero-shot retrieval performance. The results in Table 10 confirm this. Table 11 also provides a further comparison of different forms of denoising loss and validates that the denoising contrastive loss used in this paper is more compatible with cross-modal contrastive loss.\n\nMoreover, we also compared with OFA[5], a generative model based on the encoder-decoder structure in Figure 5. It can be seen that ONE-PEACE exhibits better visual grounding ability on out-of-domain pictures.\n\n```text\n[1] Gong Y, Rouditchenko A, Liu A H, et al. Contrastive audio-visual masked autoencoder[C]//The Eleventh International Conference on Learning Representations. 2022.\n[2] Zhu W, Doshi K, Yi J, et al. Multiscale Multimodal Transformer for Multimodal Action Recognition[J]. 2022\n[3] Huang P Y, Sharma V, Xu H, et al. MAViL: Masked Audio-Video Learners[J]. arXiv preprint arXiv:2212.08071, 2022.\n[4] Yang P, Wang X, Duan X, et al. Avqa: A dataset for audio-visual question answering on videos[C]//Proceedings of the 30th ACM International Conference on Multimedia. 2022: 3480-3491.\n[5] Wang P, Yang A, Men R, et al. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework[C]//International Conference on Machine Learning. PMLR, 2022: 23318-23340.\n```"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4516/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700497099236,
                "cdate": 1700497099236,
                "tmdate": 1700500819492,
                "mdate": 1700500819492,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "W9LItnhGjD",
                "forum": "9Klj7QG0NO",
                "replyto": "PoMk88NKPe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4516/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4516/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9Evr (2/2)"
                    },
                    "comment": {
                        "value": "> **Q1: About the two-stage pre-training**\n\nWe designed the two-stage pre-training intentionally to demonstrate that ONE-PEACE has the potential to expand to unlimited modalities. In the first pre-training stage (VL pre-training), vision and language representation are aligned from paired VL data. In the second pre-training stage (AL pre-training), we want the new modality audio to align with language and not influence the VL-aligned feature so we freeze the text-branch parameters. This approach proves that ONE-PEACE can introduce new modalities without influencing the performance of the existing modalities, and even achieve excellent performance in the new modality. If we directly conduct pre-training for the visual, textual, and audio modalities simultaneously, we would not be able to demonstrate the scalability of ONE-PEACE to new modalities.\n\n> **Q2: About using ONE-PEACE**\n\nWe appreciate your experience with ONE-PEACE and the feedback you provided. The following points should be considered when using ONE-PEACE:\n\n* ONE-PEACE has a maximum audio duration limit of 15 seconds. Any audio longer than this will be cut off. Therefore, you should ensure that the input audio is not too lengthy and that its crucial information is within the first 15 seconds.\n* The audio training data for ONE-PEACE solely consists of environmental sound datasets (such as bird chirping, alarm clock ringing, etc.), the accuracy of the other types of audio may be somewhat compromised.\n* If you use our demo, please note that the demo only uses 60,000 images as the candidate set (ImageNet val set + MSCOCO val set), so there may be some audio clips without corresponding images in the candidate set."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4516/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700497221092,
                "cdate": 1700497221092,
                "tmdate": 1700497221092,
                "mdate": 1700497221092,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "B0qP8E21gs",
            "forum": "9Klj7QG0NO",
            "replyto": "9Klj7QG0NO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4516/Reviewer_zvUR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4516/Reviewer_zvUR"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a model with 4B parameters that aligns and integrates representations across vision, audio, and language modalities. Two pertaining tasks, cross-modal aligning contrast and intra-modal denoising contrast are developed to align the semantic space of different modalities."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "the paper is well-written, and the experiments are thorough. The problem of unifying representations from multiple modalities is significant and the proposed approaches showed some potential in this direction."
                },
                "weaknesses": {
                    "value": "The paper presents an ambitious effort to amalgamate multiple modalities into a singular embedding space, a concept previously explored in works such as ImageBindm (encompassing images, language, audio, depth, thermal, and IMU modalities), CLAP (audio and language), ULIP (3D, image, and language), and Chatbridge (audio, video, image, and language), but seems not thoroughly discussed and compared. Notably, this study posits the advantage of a scaling-friendly architecture, purportedly capable of incorporating an unlimited array of modalities. While this is a compelling proposition, the reviewer suggests that the paper could better substantiate this claim by integrating and examining a broader range of modalities. Such an expansion would more robustly demonstrate the architecture's potential and scalability, thereby providing a more comprehensive understanding of its capabilities in handling diverse and complex multimodal datasets."
                },
                "questions": {
                    "value": "see the weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4516/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698910709015,
            "cdate": 1698910709015,
            "tmdate": 1699636428290,
            "mdate": 1699636428290,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "POTK4oJLHP",
                "forum": "9Klj7QG0NO",
                "replyto": "B0qP8E21gs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4516/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4516/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer zvUR"
                    },
                    "comment": {
                        "value": "Thanks for your time reviewing and providing helpful suggestions for our paper. We address open questions and remarks below:\n\n> **Discuss and compare with related works**\n\nSome of the works have been discussed in the related work section (ImageBind) or compared in the experiments section (CLAP). Due to the page limit, we added a new discussion section in the appendix to discuss the other works. Here, we briefly discuss the differences between ONE-PEACE and these works:\n\n* ImageBind aligns multiple separated models with a pre-trained visual model to integrate different modalities into a unified semantic space. In contrast, ONE-PEACE uses a single model to align multiple modalities and allows them to interact through the self-attention mechanism, thus being more effective in handling fine-grained multimodal tasks like visual grounding. We updated the zero-shot results of ImageBind on ESC-50 in Table 6, and it can be seen that ONE-PEACE achieves better performance on this dataset.\n\n| Model         | ESC-50 (zero-shot) |\n|:--------------|:------------------:|\n| ImageBind     |        66.9        |\n| **ONE-PEACE** |      **91.8**      |\n\n* CLAP uses contrastive learning to align audio and language. We compare ONE-PEACE and CLAP in Table 5 and Table 6, ONE-PEACE achieves better performance in all the audio-related tasks. \n\n* ULIP introduces the 3D modality through contrastive learning. However, since ONE-PEACE has not incorporated the 3D modality, we cannot directly compare it with ULIP. We discuss the related works in the newly added discussion section. \n\n* Chatbridge provides multimodal capabilities for LLM by incorporating representation models. Our paper mainly compares ONE-PEACE with representation models, where generative models that combine LLMs and representation models have not been included in the baselines yet. As a general representation model, ONE-PEACE can also be integrated into LLM to provide multimodal capabilities. We discuss this in the discussion section. \n\n> **Integrating and examining more modalities**\n\nit is noteworthy that ONE-PEACE has demonstrated remarkable performance on various video-related datasets, including K400, VGGsound, and AVQA. These results validates its capability across four modalities: image, audio, language, and video. We agree incorporating additional modalities such as 3D data and IMU can further demonstrate the potential of ONE-PEACE. However, due to time constraints during the response period, we added a discussion in the discussion section and left this for future research."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4516/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700496482179,
                "cdate": 1700496482179,
                "tmdate": 1700498191546,
                "mdate": 1700498191546,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KvThGMHZh1",
                "forum": "9Klj7QG0NO",
                "replyto": "B0qP8E21gs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4516/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4516/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you again for taking the time to review our paper. We appreciate your feedback and hope that we were able to address your concerns in our response. As the deadline is nearing, please let us know if you have any further questions before the discussion period ends. We are glad to address your concerns."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4516/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673410240,
                "cdate": 1700673410240,
                "tmdate": 1700673410240,
                "mdate": 1700673410240,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dFNg3hWOeH",
                "forum": "9Klj7QG0NO",
                "replyto": "KvThGMHZh1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4516/Reviewer_zvUR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4516/Reviewer_zvUR"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response. I've read other reviews and rebuttals."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4516/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709010049,
                "cdate": 1700709010049,
                "tmdate": 1700709010049,
                "mdate": 1700709010049,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "m6WbnxCJRD",
                "forum": "9Klj7QG0NO",
                "replyto": "B0qP8E21gs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4516/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4516/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for the reply. We would like to ensure that we have addressed your concerns. If you have no further questions, would you kindly consider raising the score?"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4516/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713563749,
                "cdate": 1700713563749,
                "tmdate": 1700713563749,
                "mdate": 1700713563749,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]