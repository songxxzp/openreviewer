[
    {
        "title": "Tackling Underestimation Bias in Successor Features by Distributional Reinforcement Learning"
    },
    {
        "review": {
            "id": "nVQR0EVWMp",
            "forum": "prLvQWc5YH",
            "replyto": "prLvQWc5YH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission79/Reviewer_mAvf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission79/Reviewer_mAvf"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors identify an issue with underestimation bias when employing successor features with generalized policy improvement (GPI). They apply distributional RL to successor features to obtain a distributional GPI that they claim alleviates this issue."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* Connecting successor features and multi-objective RL. These two concepts are intimately related, yet I\u2019m unaware of any published work making a direct connection. \n* Identifying the issue of underestimation with SFs. The intuition behind this makes sense but I found the explanation hard to follow."
                },
                "weaknesses": {
                    "value": "Overall, I found the paper hard to follow as the math is a little too loose to make sense of what the authors are claiming. I'd suggest the authors go over the text with a careful eye and fix some of the notational issues. I\u2019ll detail the major weaknesses and ask more direct questions about notation and other issues in the questions section below.\n\n* I believe they are learning the wrong object with their proposed distributional SF algorithm. They should be learning the features' joint distribution, but they treat each feature as independent. This is implicitly done in how the author's sample $\\tau$; to learn the proper distribution, you'd need the quantile of a multivariate random variable.\n* The paper is poorly positioned in the literature regarding multi-dimensional reward functions in distributional RL. My above point on learning the wrong object has been solved by [1] and [2], where they learn the correct joint distribution. Furthermore, I expected a more in-depth discussion about [3] as they also learn \u201cdistributional SFs\u201d (although they also aren\u2019t learning the joint distribution).\n* Assumption 1 seems dubious; this should be impossible with stochastic transitions; a single application of the Bellman operator will construct a mixture distribution, so, at the very least, you\u2019d expect the target to be a mixture of Gaussians. \n* No justification is given for the additive noise model (Equation 6).\n* It\u2019s hard to judge the effectiveness of the approach as the empirical results don\u2019t differentiate much between distributional GPI and regular GPI. I would have had to see a better-executed empirical study to be convinced.\n\n---\n\n[1] Pushi Zhang, Xiaoyu Chen, Li Zhao, Wei Xiong, Tao Qin, Tie-Yan Liu. Distributional Reinforcement Learning for Multi-Dimensional Reward Functions. NeurIPS 2021.\n\n[2]  Dror Freirich, Tzahi Shimkin, Ron Meir, Aviv Tamar. Distributional Multivariate Policy Evaluation and Exploration with the Bellman GAN. ICML 2019.\n\n[3] Michael Gimelfarb, Andre Barreto, Scott Sander, and Chi-Guhn Lee. Risk-Aware Transfer in Reinforcement Learning using Successor Features. NeurIPS 2021."
                },
                "questions": {
                    "value": "- In Section 3.1, the prediction $\\Psi(s', a', \\theta_i)$ should be $\\Psi(s, a, \\theta_i)$? Only the TD target should have the next state-action term. This error is propagated from this point forward, e.g., the gradient term is wrong, and eq (5) contains the same error.\n- In Section 3.1, why is there an expectation over s\u2019 in the loss? Aren\u2019t we trying to write down the stochastic approximation algorithm for learning SFs via TD? Citing equation 1 makes it seem like that\u2019s what we\u2019re trying to do.\n- Theorem 1, why is $s\u2019$, $a\u2019$ defined as input to $\\Delta$ but then $s'$ appears in the expectation? Also, why do we have an expectation over $s'$ again?\n- Theorem 3 compares the expected return of the optimal policy with a risk measure of the estimated policy. Why? If you're being risk-sensitive, the goal is not to learn the mean-optimal policy.\n- Algorithm 1, where is $\\tau_e$ being used? Shouldn't it be used in Line 6 when computing the greedy action?\n- Algorithm 1, quantiles are treated implicitly in some cases; this makes it hard to decipher what's going on."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission79/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission79/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission79/Reviewer_mAvf"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission79/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698788520347,
            "cdate": 1698788520347,
            "tmdate": 1699635932510,
            "mdate": 1699635932510,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0K7F3kZJ0x",
                "forum": "prLvQWc5YH",
                "replyto": "nVQR0EVWMp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission79/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission79/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Reviewer mAvf 1"
                    },
                    "comment": {
                        "value": "1.*``I believe they are learning the wrong object with their proposed distributional SF algorithm. They should be learning the features' joint distribution, but they treat each feature as independent. This is implicitly done in how the author's sample\u00a0$\\tau$; to learn the proper distribution, you'd need the quantile of a multivariate random variable. \u2019\u2019*\n\n2.*``The paper is poorly positioned in the literature regarding multi-dimensional reward functions in distributional RL. My above point on learning the wrong object has been solved by [1] and [2], where they learn the correct joint distribution.\u2019\u2019*\n\n4.*``Assumption 1 seems dubious; this should be impossible with stochastic transitions; a single application of the Bellman operator will construct a mixture distribution, so, at the very least, you\u2019d expect the target to be a mixture of Gaussians.\u2019\u2019*\n\n**Answer.** Thank you for your valuable suggestion. As in [3], we will consider a more general case elliptical distribution. \n\n3.*``Furthermore, I expected a more in-depth discussion about [3] as they also learn \u201cdistributional SFs\u201d (although they also aren\u2019t learning the joint distribution).\u2019\u2019*\n\n**Answer.** Thank you very much for the insightful references. We will add [3] to the related work section. Specifically, Gimelfarb et al. proposed risk-aware successor features (RaSF) to realize policy transfer between tasks with shared dynamics by incorporating risk-awareness into the decision-making. Differently, our proposed concept of DSFs considers multiple different risk measures, which consider more scenarios of the return distribution and are more general. \n\n\n[1] Zhang et al.. Distributional reinforcement learning for multi-dimensional reward functions. NeurIPS 2021.\n\n[2] Freirich et al.. Distributional multivariate policy evaluation and exploration with the bellman GAN. ICML 2019.\n\n[3] Gimelfarb et al.. Risk-aware transfer in reinforcement learning using successor features. NeurIPS 2021."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission79/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491256455,
                "cdate": 1700491256455,
                "tmdate": 1700492730069,
                "mdate": 1700492730069,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9mCCkHNeba",
                "forum": "prLvQWc5YH",
                "replyto": "nVQR0EVWMp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission79/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission79/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Reviewer mAvf 2"
                    },
                    "comment": {
                        "value": "5.*``No justification is given for the additive noise model (Equation 6).\u2019\u2019*\n\n**Answer.** Actually, we have claimed the justification of the additive noise model before Eq. (6). Specifically, in practical applications, SFs-estimation usually incorporates random errors, which presumably stem from function approximation and are induced by source tasks.\n\n9.*``Theorem 1, why is\u00a0$s\u2032,\u00a0a\u2032$\u00a0defined as input to\u00a0$\\Delta$\u00a0but then\u00a0$s\u2032$\u00a0appears in the expectation? Also, why do we have an expectation over\u00a0$s\u2032$\u00a0again?\u2019\u2019*\n\n**Answer.** Thank you for your valuable suggestion. We approve that $\\Delta$ is not a function of $s\u2019,a\u2019$, and we will correct it in our further revision. \n\n10.*``Theorem 3 compares the expected return of the optimal policy with a risk measure of the estimated policy. Why? If you're being risk-sensitive, the goal is not to learn the mean-optimal policy. \u2019\u2019*\n\n**Answer.** Actually, the estimated policy is trained by risk-sensitive RL algorithms,  while the optimal policy is determined by the MDP independent with aleatoric risk measures. Thus, the estimated policy and the optimal policy should be considered in different scenarios, where the former (inherent) is objective and the latter (trained) is risk-sensitive. \n\n11.*``Algorithm 1, where is\u00a0$\\tau_{e}$\u00a0being used? Shouldn't it be used in Line 6 when computing the greedy action?\u2019\u2019*\n\n12.*``Algorithm 1, quantiles are treated implicitly in some cases; this makes it hard to decipher what's going on. \u2019\u2019*\n\n**Answer.** Thank you for your careful review. We will elaborate on how to use various kinds of quantiles in our further revision. Specifically, indeed, $\\tau_{e}$ is used in Line 6 when computing the greedy action. $\\tau_{j}$ and $\\tau_{k}$ are used to compute the loss $J_{Z}(\\boldsymbol{\\theta})$. All these issues will be clarified."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission79/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491357168,
                "cdate": 1700491357168,
                "tmdate": 1700492788750,
                "mdate": 1700492788750,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7L7hI2YUtB",
                "forum": "prLvQWc5YH",
                "replyto": "9mCCkHNeba",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission79/Reviewer_mAvf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission79/Reviewer_mAvf"
                ],
                "content": {
                    "comment": {
                        "value": "I believe my main concerns were still not addressed in the rebuttal, specifically points (1) and (2) above. Furthermore, my confusion around comparing the optimal policy w.r.t. the expected return and the optimal policy w.r.t. a risk measure still remains. That said, I'd still like to keep my score as is, I believe it's too late to adequately address the issues with this paper for ICLR."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission79/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578559441,
                "cdate": 1700578559441,
                "tmdate": 1700578559441,
                "mdate": 1700578559441,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "n2COEqnaqK",
                "forum": "prLvQWc5YH",
                "replyto": "nVQR0EVWMp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission79/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission79/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the time and effort that you dedicated to providing feedback on our manuscript and are grateful for the insightful comments on our paper."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission79/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700583298044,
                "cdate": 1700583298044,
                "tmdate": 1700583413548,
                "mdate": 1700583413548,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "09CNPiA8np",
            "forum": "prLvQWc5YH",
            "replyto": "prLvQWc5YH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission79/Reviewer_3ktK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission79/Reviewer_3ktK"
            ],
            "content": {
                "summary": {
                    "value": "In this work, the authors aim to address underestimation when using Successor Features and Generalized Policy Iteration (GPI). A common technique often used to prevent overestimation in the Q-values is by using the min operation with double Q-functions, which may in turn result in underestimation. Motivated by this insight,  the authors rely on theoretical analyses to show a similar trait is observed when updating the parameters of the successor features. This is induced by a mismatch of the parameters of the successor features between one that depends on a changing policy distribution and the other being the optimal policy. The authors proposed replacing successor features with its distributional form in order to limit or reduce the underestimation bias."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "I have to be honest. It is difficult for me to identify what to write with regards to the strength of the paper. I can see that a lot of work has been done. However, the presentation and writing is not doing justice to the authors\u2019 effort."
                },
                "weaknesses": {
                    "value": "1. First of all, the writing and the overall presentation is not very clear and does not flow well at times. Despite reading the paper a couple of times, it still seems confusing and overly complicated. Lastly, some of the sentences in the paper do not even make sense and makes me wonder if they were generated by LLMs. Here are some examples: \n  a. \u201cExplosively, we take an impressive TRL method - successor features (SFs) (Barreto et al., 2017; 2018; Carvalho et al., 2023) as an example, to study the underlying overestimation/underestimation bias.\u201d \n  b.\u201cThey enrich our concepts mutually.\u201d\n  c.\u201cExtensive quantitative evaluations support our analysis.\u201d\n2. It seems that the research question about addressing underestimation was motivated by RL/ But at the moment, I fail to understand the need of using risk-sensitive frameworks and multi-objective RL. What is the main motivation for considering these frameworks and theories? Furthermore, the lack of clarity from the section on bridging successor features and multi-objective RL does not help the cause.\n3. Eq 4. Is y the target that you are regressing towards? It is confusing if that is not the case. \n4. It is very hard to read the paper when there are a large portion of different concepts and their corresponding theorems and equations. I would recommend moving most of these items into the appendix and use the main portion of the paper to explain what these different concepts are and how they are related to the research question that you are attempting to address. You can also move the pseudocode for Algorithm 1 into the appendix. This will also allow you to make more space for the section for conclusion and discussion. \n5. The overall paper structure should be re-visited. The fact that a whole chunk of related work is in your appendix is a missed opportunity for the readers that they can follow along. \n6. Although the author did provide the theoretical proofs showing the existence of the underestimation bias in the SF & GPI framework, this point will make a stronger case with empirical evidence as well."
                },
                "questions": {
                    "value": "1. What is the purpose of analyzing using the risk-sensitive framework? \n2. What is the purpose of considering multi-objective RL which only further complicates the study?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission79/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699138323260,
            "cdate": 1699138323260,
            "tmdate": 1699635932425,
            "mdate": 1699635932425,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LJbPDx4prm",
                "forum": "prLvQWc5YH",
                "replyto": "09CNPiA8np",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission79/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission79/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Reviewer 3ktK 1"
                    },
                    "comment": {
                        "value": "1.*``First of all, the writing and the overall presentation is not very clear and does not flow well at times. Despite reading the paper a couple of times, it still seems confusing and overly complicated. Lastly, some of the sentences in the paper do not even make sense and makes me wonder if they were generated by LLMs. Here are some examples: a. \u201cExplosively, we take an impressive TRL method - successor features (SFs) (Barreto et al., 2017; 2018; Carvalho et al., 2023) as an example, to study the underlying overestimation/underestimation bias.\u201d b.\u201cThey enrich our concepts mutually.\u201d c.\u201cExtensive quantitative evaluations support our analysis.\u201d\u2019\u2019*\n\n**Answer.** Thank you for your careful review. Actually, we do not use LLMs while writing the paper, and we are sorry for all these unclear expressions. We will update these sentences in our further revision. The specifics are as follows. \n\nExplosively, we take an impressive TRL method - successor features (SFs) (Barreto et al., 2017; 2018; Carvalho et al., 2023) as an example, to study the underlying overestimation/underestimation bias. -> Typically, we study the underlying overestimation/underestimation bias in TRL by taking a representative method - successor features (SFs) (Barreto et al., 2017; 2018; Carvalho et al., 2023) as an example. \n\nThey enrich our concepts mutually -> We will delete this meaningless sentence in our further revision. \n\nExtensive quantitative evaluations support our analysis -> Extensive quantitative evaluations demonstrate the Q-value underestimation phenomenon in SFs transfer framework and the effect of DSFs mitigating such underestimation. \n\n2.*``It seems that the research question about addressing underestimation was motivated by RL/ But at the moment, I fail to understand the need of using risk-sensitive frameworks and multi-objective RL. What is the main motivation for considering these frameworks and theories? Furthermore, the lack of clarity from the section on bridging successor features and multi-objective RL does not help the cause.\u2019\u2019 \n\n7.``What is the purpose of analyzing using the risk-sensitive framework?\u2019\u2019\n\n8.``What is the purpose of considering multi-objective RL which only further complicates the study?\u2019\u2019* \n\n**Answer.** \n\n**The main motivation for risk-sensitive frameworks.**\n\nAs mentioned in [1], \u2018risk\u2019 refers to uncertainty over possible outcomes, and risk-sensitive policies are those that depend upon more than the mean of the outcomes. Meanwhile, [2] claims that using the expected return as a measure of optimality could still lead to undesirable behavior such as excessive risk-taking, since low-probability catastrophic outcomes with negative reward and high variance could be underrepresented. For this reason, risk awareness is becoming an important aspect of practical RL. With the aid of the risk-sensitive framework, a series of RL works have been successfully conducted [3, 4]. In our paper, we leverage this framework to relieve the underestimation problem in the scenario of SFs. \n\n**The main motivation for MORL.**\n\nIncorporating MORL will mitigate the underestimation, and subsequently ensure possessing the optimal policy. Specifically, GPI/DGPI can only be used to construct a policy that performs no worse than the existing one but brings about the limitation of the policies' optimality. The prior knowledge of the set of SFs/DSFs is crucial for the precise and efficient learning of optimal policies using GPI/DGPI. To tackle this issue, MORL literature has extensively studied the problem of constructing an excellent prior, convex coverage set (CCS). For detailed literature, please refer to [5, 6]. \n\nActually, we should also conduct experiments on separate SFs. However, due to program limitations, we only conduct experiments by combining SF with MORL. We aim to address this limitation in the future and include experiments on separate SFs to improve our paper. \n\n**The lack of clarity from the section on bridging successor features and multi-objective RL does not help the cause.**\n\nThank you for your valuable suggestion. We will complement the purpose of bridging SFs and MORL in Section 2.2 of our revision. \n\n\n[1] Dabney et al.. Implicit quantile networks for distributional reinforcement learning. ICML, 2018. \n\n[2] Gimelfarb et al.. Risk-aware transfer in reinforcement learning using successor features. NeurIPS 2021. \n\n[3] Ma et al.. DSAC: Distributional soft actor critic for risk-sensitive reinforcement learning. arXiv preprint arXiv: 2004.14547.2020. \n\n[4] Zhou et al.. Distributional generative adversarial imitation learning with reproducing kernel generalization. Neural Networks, 2023. \n\n[5] Yang et al.. A generalized algorithm for multi-objective reinforcement learning and policy adaptation. NeurIPS 2019. \n\n[6] Hayes et al.. A practical guide to multi-objective reinforcement learning and planning. AAMAS, 2022."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission79/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491092982,
                "cdate": 1700491092982,
                "tmdate": 1700492597830,
                "mdate": 1700492597830,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PXxEPOpS1o",
                "forum": "prLvQWc5YH",
                "replyto": "09CNPiA8np",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission79/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission79/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Reviewer 3ktK 2"
                    },
                    "comment": {
                        "value": "4.*``It is very hard to read the paper when there are a large portion of different concepts and their corresponding theorems and equations. I would recommend moving most of these items into the appendix and use the main portion of the paper to explain what these different concepts are and how they are related to the research question that you are attempting to address. You can also move the pseudocode for Algorithm 1 into the appendix. This will also allow you to make more space for the section for conclusion and discussion. \u2019\u2019*\n\n5.*``The overall paper structure should be re-visited. The fact that a whole chunk of related work is in your appendix is a missed opportunity for the readers that they can follow along.\u2019\u2019* \n\n**Answer.** Thank you for your valuable suggestion. We will reorganize the whole paper for a more clarified presentation."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission79/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491154264,
                "cdate": 1700491154264,
                "tmdate": 1700492633748,
                "mdate": 1700492633748,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wMyugP0jxU",
                "forum": "prLvQWc5YH",
                "replyto": "LJbPDx4prm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission79/Reviewer_3ktK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission79/Reviewer_3ktK"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the response to my questions and concerns, as well as sharing the resources on bridging SFs and multi-objective RL. Given that there is still substantial amount of work to be done, I will keep my score as it is. I hope that the authors will incorporate the suggestions that other reviewers and myself have provided in order to improve the quality of the paper."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission79/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582793322,
                "cdate": 1700582793322,
                "tmdate": 1700582793322,
                "mdate": 1700582793322,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fjEg23YFHM",
                "forum": "prLvQWc5YH",
                "replyto": "09CNPiA8np",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission79/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission79/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the time and effort that you dedicated to providing feedback on our manuscript and are grateful for the insightful comments on our paper."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission79/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700583314457,
                "cdate": 1700583314457,
                "tmdate": 1700583428617,
                "mdate": 1700583428617,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aBc5uzK6RB",
            "forum": "prLvQWc5YH",
            "replyto": "prLvQWc5YH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission79/Reviewer_dNHm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission79/Reviewer_dNHm"
            ],
            "content": {
                "summary": {
                    "value": "This paper theoretically studies the underestimation phenomenon in successor features and generalized policy improvement. The paper introduces distributional RL into the SF/GPI framework so as to mitigate underestimation and theoretically analyzes its generalization bounds. The experiments are run on multi-objective RL environments (testing transferabiity with GPI) in Mujoco. They compare the performance of their distributional variants to the standard SF/GPI variants."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper does indeed seem to show theoretically that the underestimation phenomena occurs in SF & GPI.\n\nThe paper does indeed seem to show that distributional SFs has a lower generalization bound than the original SFs."
                },
                "weaknesses": {
                    "value": "My primary concerns have to do with clarity, as well as well as the experimental results.\n\nThere are many typos or awkward wording, including some that impact the reader's understanding.\n\nSome examples include:\n- \"The results indicate that both the two DSFs-based algorithms (RDSFOLS and DGPI-WCPI)\". The latter isn't even in Figure 2? I am assuming the latter refers to \"WCDPI+DGPI\", but this should be clarified.\nAwkward wording:\n- \"resulting in a \u201czero-shot\u201d somewhat fantastical\"\n- \"For a new task w_{n+1}, it is practicable to evaluate all policies\". Perhaps you mean practical?\n- \"standpoint to expose the mystery of underestimation\". The word 'mystery' seems akward here.\n- \"Due to the disorder of exploration\": I don't know what \"disorder of explanation refers to\"\n- \"is lack of stability\"\n- \"DSFs exhibits\" -> \"exhibit\"\n- \"We remark that \u03b4_\u03c6 > 0 makes no focus\". I didn't understand what was meant by \"focus\".\n- \"if the set of DSFs enough close\"\n\nThere are many more beyond what was mentioned, and this does indeed negatively impact the readability of the paper.\n\nThe results in Figure 2 do not appear to be very compelling. It seems the proposed method does not significantly outperform the baselines."
                },
                "questions": {
                    "value": "- Can you describe again the y-axis in Figure 2?\n- Did you look at Q-value predictions of the agents to show/demonstrate lower underestimation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission79/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission79/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission79/Reviewer_dNHm"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission79/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699156724966,
            "cdate": 1699156724966,
            "tmdate": 1699635932360,
            "mdate": 1699635932360,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6jRw7jABdc",
                "forum": "prLvQWc5YH",
                "replyto": "aBc5uzK6RB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission79/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission79/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Reviewer dNHm"
                    },
                    "comment": {
                        "value": "1. *``The results indicate that both the two DSFs-based algorithms (RDSFOLS and DGPI-WCPI)\". The latter isn't even in Figure 2? I am assuming the latter refers to \"WCDPI+DGPI\", but this should be clarified. \u2019\u2019*\n\n**Answer.** Thank you for pointing this out. The label \"WCDPI+DGPI\" should be \"DGPI-WCPI\". We are sorry for this typo, and we will correct it in our revised paper. \n\n2.*``Awkward wording.*\n\n*\"resulting in a \u201czero-shot\u201d somewhat fantastical\"*\n\n*\"For a new task $w_{n+1}$, it is practicable to evaluate all policies\". Perhaps you mean practical?*\n\n*\"standpoint to expose the mystery of underestimation\". The word 'mystery' seems akward here.*\n\n*\"Due to the disorder of exploration\": I don't know what \"disorder of explanation refers to\"*\n\n*\"is lack of stability\"*\n\n*\"DSFs exhibits\" -> \"exhibit\"*\n\n*\"We remark that $\\delta_{\\phi}>0$ makes no focus\". I didn't understand what was meant by \"focus\".*\n\n*\"if the set of DSFs enough close\"\u2019\u2019*\n\n**Answer.** Thank you for your careful review and suggestions. We will make updates to these linguistic issues. The specifics are as follows. \n\nFor a new task $w_{n+1}$, it is practicable to evaluate all policies -> For a new task $w_{n+1}$, it is practical to evaluate all policies\n\nstandpoint to expose the mystery of underestimation -> standpoint to expose the phenomenon of underestimation\n\nDue to the disorder of exploration -> Due to the randomness of exploration\nis lack of stability -> is not stable\n\nDSFs exhibits -> exhibit, and we will correct this issue throughout the whole paper.\n\nThe agent will attain near-optimal performance, if the set of DSFs enough close to task $w_{n+1}$.  -> The agent will attain near-optimal performance in task $w_{n+1}$, if it has solved a similar task before.\n\n4.*``Can you describe again the y-axis in Figure 2?\u2019\u2019*\n\n**Answer.** In Figure 2, the y-axis represents the expected return (value) obtained from each method when evaluated over a test set of 60 tasks uniformly sampled from $\\boldsymbol{W}^{\\boldsymbol{\\phi}}$."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission79/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700490897931,
                "cdate": 1700490897931,
                "tmdate": 1700491859760,
                "mdate": 1700491859760,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]