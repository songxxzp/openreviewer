[
    {
        "title": "Simultaneous Dimensionality Reduction: A Data Efficient Approach for Multimodal Representations Learning"
    },
    {
        "review": {
            "id": "AWWp7yBuhb",
            "forum": "4SrzKsJocx",
            "replyto": "4SrzKsJocx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6299/Reviewer_KsZw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6299/Reviewer_KsZw"
            ],
            "content": {
                "summary": {
                    "value": "Authors of this paper studies the variation preservation and covarying structure in dimensionality reduction (DR) methods for multimodal datasets through a generative linear model with known variance and covariance. The findings show that PSL and rCCA are preferred to OCA when detecting covariation is more important than variation preservation"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Empirical studies from various perspectives of the simulated data are performed where the data are sampled from a generative model including both self-signal and shared-signal."
                },
                "weaknesses": {
                    "value": "The presented model is formulated based on strong independent assumption for all variables in the linear model, so it doubts that the data generated from this model can align well with real data and the findings are informative. The scope of this study is limited to a small set of models based on the generative linear model, and the findings may not be properly extended to other methods. The study of this paper is mainly numerical, so it is unclear if there exists theoretical result to explain the findings."
                },
                "questions": {
                    "value": "This study is limited to specified methods, PCA, PLS and CCA, all of which are variance- or covariance-based models. It is unclear how the findings in this paper can be extended for broad family of dimensionality reduction methods.\n\nIn Section 2.2, authors present models (1) and (2) for each modality. It seems that all are random variables. Is every element in the random matrix i.i.d. sampled from a Gaussian with 0 mean and specified variance? Due to the strong assumptions used in (1) and (2), it is unknown how they align well with the generation process of real data. Authors should refer to the existing work like probabilistic PCA or probabilistic CCA for properly defining the generative linear model.\n\nBach, Francis R., and Michael I. Jordan. \"A probabilistic interpretation of canonical correlation analysis.\" (2005).\n\nIn Section 3, authors mentioned that training and test data sets are generated according to (1) and (2). Does it mean that all random variables are sampled accordingly to generate a sample pair X and Y? Due to some confusing in the definition of the presented models, it is better to describe the generation process in detail. For example, all samples may be generated with fixed U_X, U_Y and P. \n\nAs this paper concentrates on the empirical evaluation of existing models on the data sampled from the presented generative linear model. The evaluation metric can be important. Authors introduce the so-called reconstructed correlations RC\u2019, which is described in Appendix A.2. It is the scaled correlation of projected points in low-dimensional spaces obtained by corresponding models. The correlation values are within [-1, 1]. It is unclear why (15) should be in [0, 1]. And the measure RC_0 is introduced because the ideal uncorrelation is not achievable if the sample is few. But RC_0 is computed based on multiple random trials. That is to say, the evaluation metric is not deterministic. \n\nIn experiments, figures with gamma_self and gamma_shared are generated. How do the two parameters are generated to form a grid? Both parameters are functions of other three variances. \n\nAll the findings are concluded from the reconstructed correlations RC\u2019, which is biased to CCA for maximizing the shared signals. This may not be new. Moreover, the conclusion or suggestion made by authors can be strong. It is possible that rCCA works better than PCA, but it is unclear SDR works better than IDR."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6299/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698643896065,
            "cdate": 1698643896065,
            "tmdate": 1699636691945,
            "mdate": 1699636691945,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IY4MwtfZIZ",
                "forum": "4SrzKsJocx",
                "replyto": "AWWp7yBuhb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6299/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6299/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "$\\textbf{Part 1}$\n\nWe appreciate your comprehensive and insightful feedback on our manuscript. Responses to this feedback have strongly improved the revised version of this work. \n\nIn reference to the variance or covariance-based nature of the methods investigated, it is crucial to note that our main focus is in examining the interplay between variation and covariation rather than solely emphasizing variance-covariance distinctions. Although the emergence of this distinction in scenarios involving covariation or variation might not be immediately evident, we aim to address this through two lines of analysis. Firstly, to provide further evidence beyond our model-based simulations, we have included in the appendix an additional experiment involving noisy MNIST, a nonlinear dataset not derived from our generative model. This independent assessment reinforces our findings, demonstrating the robustness of our conclusions. Secondly, we are concurrently exploring these notions in a nonlinear information theory approach in our other ongoing work (also under review by ICLR (https://openreview.net/forum?id=ZhY1XSYqO4)). This parallel study serves to fortify the intuitive aspects we explained in this current work. The deliberate choice to confine ourselves to linear scenarios using linear methods in the current manuscript was made to ensure the lucidity and comprehensibility of our message.\n\nAdditionally, while it's feasible to formulate this model using the probabilistic approach as in Bach and Jordan (2005), or more recently in Murphy (2023), our formulation is instead within the framework of random matrix theory. This design choice allows for a broader range of simulations and analytical treatments to explore diverse structural patterns and sampling scenarios. It's important to note that, while Gaussian distributions were utilized to structure individual matrices in our model, this choice doesn't imply the entire observation matrix follows a Gaussian distribution."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6299/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700545246470,
                "cdate": 1700545246470,
                "tmdate": 1700545297158,
                "mdate": 1700545297158,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VRDEAkthsT",
                "forum": "4SrzKsJocx",
                "replyto": "AWWp7yBuhb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6299/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6299/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "$\\textbf{Part 2}$\n\nIn our effort to provide a clear understanding of the data generation process, we've expanded on our explanation in Section 2.2 to outline the specifics of how the data is generated. Additionally, we direct attention to the Appendix's implementation section, which offers a more comprehensive explanation of what we did. To generate the matrices $X$ and $Y$, we follow the following process: we generate a set of projection matrices $Q$s and $V$s, which remain fixed throughout the entirety of the simulation. Subsequently, we generate the matrices $R$s, $U$s, and $P$, and then appropriately multiply these matrices with the fixed $Q$s and $V$s to derive the complete matrices $X$ and $Y$. Each of these matrices employed in generating $X$ and $Y$ comprises a defined number of dimensions. These dimensions represent observations generated from Gaussian variables with a mean of 0 and their respective variance. Our aim with this detailed explanation is to eliminate any ambiguity concerning the data generation methodology and ensure a comprehensive understanding.\n\nFor the evaluation metric, we would like to clarify that the scaling of Eq. (15) within [0, 1] is derived from the Frobenius norm of the correlation matrix, which is the square root of the sum of the squares of the singular values of a correlation matrix, which is by construction a positive number, and after normalization, it becomes a value between 0 and 1. Also, while acknowledging the nondeterministic nature of $\\mathcal{RC}_{0}$, we reiterate that it is computed consistently across multiple trials, and as shown in Figure 7 in the appendix, it has a very small variance.\n\nThe lack of clarity in specifying the generation process of parameters $\\gamma_{\\text{self}/\\text{shared}}$ in terms of the three variances has been rectified in the revision by providing explicit details on their derivation from the three variances in question. For completeness, the way we generate these matrices is by fixing the variance of the noise and projection matrices, and varying the signal one.\n\nWhile we acknowledge the potential bias towards CCA (or SDR methods in general) in our findings, this inclination is quite reasonable in setups where multiple modalities of the data are present, and the quest is for identifying covariation. However, the prevalent practice among many practitioners and papers often leans towards PCA for dimensionality reduction, followed by regression or correlation calculations. Our primary aim is to showcase that, while PCA might be effective in certain scenarios, its performance can significantly lag behind in others. We don't explicitly favor one method over another; instead, we highlight their efficacy based on the questions they seek to address.\n\nWhile we show that rCCA outperforms PCA, extending this claim to imply that SDR is universally superior to IDR requires more careful considerations. However, this is a suggestion based on evidence gathered from various sources that we cited before, such as 'Data efficiency, dimensionality reduction, and the generalized symmetric information bottleneck' by K. Michael Martini and Ilya Nemenman (2023), or as was brought to our attention by reviewers, such as 'Supervised Dimensionality Reduction for Big Data' by Vogelstein et al. (2021), as well as our ongoing research mentioned previously. Our collective findings in numerous scenarios indicate that SDR tends to outperform IDR when exploring covariation, a phenomenon that's seemingly expected, but not extensively explored or commonly practiced within the field."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6299/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700545280970,
                "cdate": 1700545280970,
                "tmdate": 1700545280970,
                "mdate": 1700545280970,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "z1P2ozEMXU",
            "forum": "4SrzKsJocx",
            "replyto": "4SrzKsJocx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6299/Reviewer_di89"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6299/Reviewer_di89"
            ],
            "content": {
                "summary": {
                    "value": "This paper compares two approaches of dimensionality reduction for bimodality, namely, those methods independence between the modalities versus those assuming that some shared signal exist.\nThese methods are interested in different sub-blocks of a grand, unknown covariance matrix.\nThe authors provide a clear and well thought empirical comparison of both types of approaches.\nThe paper is mostly empirical and focusing on an artificial, fully-controlled framework for testing and comparing methods of each type."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well written, well organized, clearly structured, not missing anything with respect to its claims (which are reasonable, limitations are stated clearly as well).\nThe message, (limited) scope, contribution, and limitations are well described and clearly stated."
                },
                "weaknesses": {
                    "value": "The technicality of the contribution is present but rather limited, as the paper is an empirical comparison of well established methods (PCA, CCA, etc.).\nThe limited scope makes it a pleasant paper to read, not too dense; the price to pay is that the novelty is weak and, as said, purely empirical and not unexpected knowing the intrinsic assumptions of the two different approaches.\nSome parts could be clarified, like when discussing the variance in 2.2 (an identical variance uniformly appleid to all entries of the matrices?it seems so but the sentence comes a bit late) and the figure captions (the first figure caption could spend some more sentences describing the elements of the figure).\nThe paper would gain in extending the experimental section to real data."
                },
                "questions": {
                    "value": "None at this stage."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6299/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698677092984,
            "cdate": 1698677092984,
            "tmdate": 1699636691838,
            "mdate": 1699636691838,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9GxNHzpena",
                "forum": "4SrzKsJocx",
                "replyto": "z1P2ozEMXU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6299/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6299/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are grateful for your insightful and constructive feedback, and we incorporated your feedback in the revised version of the manuscript.\n\nFirstly, we addressed your request for $\\textit{additional clarification}$ in certain sections, particularly Section 2.2 and the figure captions. \n\nFurthermore, we agree with your proposal to $\\textit{extend our empirical evaluation to real-world datasets}$. We applied the same analysis to the Noisy MNIST dataset, which provides tangible insights into scenarios not governed by the proposed model.\n\nRegarding the $\\textit{novelty and significance}$ of our work, we point out that we intended to underscore the distinction between self and shared signals, developing a clear understanding of why certain approaches excel while others falter. To render these complex notions accessible to a wider audience requires doing work in easier-to-understand scenarios."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6299/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700544817413,
                "cdate": 1700544817413,
                "tmdate": 1700544817413,
                "mdate": 1700544817413,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "R0eh0VSoOv",
            "forum": "4SrzKsJocx",
            "replyto": "4SrzKsJocx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6299/Reviewer_7S2y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6299/Reviewer_7S2y"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript studies dimensionality reduction (DR) methods (PCA, PLS, CCA) for multimodal representation learning. To investigate these methods, the manuscript synthesizes data by introducing a generative linear model with known variance and covariance structures. The investigation explores whether the DR method extracts the relevant shared signal and identifies the dimensionality of the shared and self-signals from noisy, undersampled data. Based on investigation the manuscript suggests to prefer Simultaneous DR methods such as regularized CCA to recover covariance structures."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- Synthetic experiments for multiple cases"
                },
                "weaknesses": {
                    "value": "**Novelty**:\n\n- The manuscript proposes a generative linear model for multimodal data. However, the model is known and can be found in the literature. For example, it can be found in the probabilistic form (Murphy et al., 2022; Klami et al., 2012).\n- The manuscript suggests preferring SDR methods over IDR methods to recover the shared signal between different modalities. However, I do not think this is novel knowledge. See Borga et al.: \"A Unified Approach to PCA, PLS, MLR, and CCA.\" PCA, PLS, MLR, and CCA can be unified under a generalized eigenproblem. Figure 2 and Figure 3 in Borga et al. show that all the dimensionality reduction methods recover different solutions, which is expected since they have different inductive biases by construction.\n\n**Technicality**: The experiments are very limited to synthetic data, and it is not clear how these insights will generalize to different settings. Specifically, suppose you read literature on neural networks like Deep CCA or DCCAE. In that case, they all use layer-wise unimodal pretraining or autoencoder for training, respectively, and the CCA is used only afterwards. Hence, only SDR won't be enough to model multimodal data. \n\n**Rigor**: The experiments do not show the solutions' variability since they have not been run over multiple initializations.\n\n**Significance**: The significance to me is not clear. \n\nMurphy, Kevin P.  *Probabilistic machine learning: an introduction*. MIT press, 2022.\n\nBorga, Magnus, and Tomas Landelius Hans Knutsson. \"A Unified Approach to PCA, PLS, MLR and CCA.\"\n\nKlami, Arto, Seppo Virtanen, and Samuel Kaski. \"Bayesian exponential family projections for coupled data sources.\" *arXiv preprint arXiv:1203.3489* (2012)."
                },
                "questions": {
                    "value": "Overall, I do not think these results demonstrate new, relevant, and impactful knowledge."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6299/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698797930495,
            "cdate": 1698797930495,
            "tmdate": 1699636691708,
            "mdate": 1699636691708,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fBppaSJkTy",
                "forum": "4SrzKsJocx",
                "replyto": "R0eh0VSoOv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6299/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6299/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are grateful for your insightful comments on our manuscript. \n\nAddressing your concerns, $\\textit{we acknowledge the significant literature in probabilistic modeling}$, particularly in works by Murphy et al. [1], Klami et al. [2], and the unified approach presented in Borga et al. [3]. Our model, indeed, draws upon the concepts elucidated in these papers, focusing on similar latent variable structures for multimodal data representation, albeit with distinct goals and under different parameter setups.\nWhile these references are relevant to our work, they do not comprehensively cover the scenarios of undersampled data and multiple signal sources. as we aim to in our exploration. These latter questions are very common in many practical situations.\n\n$\\textit{Regarding the technicality of our experiments and generalizability to real-world settings}$, our intention extends beyond synthetic data. While our focus on synthetic data served as a controlled setup, we acknowledge the necessity to demonstrate applicability in real-world scenarios. To address this, we supplemented our findings with experiments on the Noisy MNIST dataset, illustrating the practical implications of our methods on nonlinear datasets.\n\nWe have taken note of your point $\\textit{regarding the limitations of nonlinear Simultaneous Dimensionality Reduction (SDR) approaches}$, as observed in methods like Deep CCA or DCCAE. In response, we would like to underscore two essential aspects. Firstly, the primary focus of our paper was directed towards highlighting the prevalent use of methods such as PCA, PLS, and CCA among practitioners across various scenarios. Our intention was to elucidate the fundamental intuition crucial in method selection and application, which is often overlooked despite its importance. Additionally, we delve into the exploration of nonlinear SDR methods in our other research (also under review by ICLR at (https://openreview.net/forum?id=ZhY1XSYqO4). This concurrent work emphasizes the distinctions between SDR and IDR methods within nonlinear settings. It underscores the criticality of method selection tailored to the pertinent questions being posed, thereby illuminating the significance of aligning methodologies with the inquiry at hand.\n\nFinally, $\\textit{regarding rigor}$, we wish to clarify that our experiments were indeed averaged over multiple initializations, as detailed in Appendix A.3 of our manuscript; we re-emphasize this in the main text of the revision.\n\nIn conclusion, we thank you for pointing out the connections to prior works and their contributions, which indeed form a vital part of our research. However, we believe our work significantly expands the scope of dependencies explored, covering undersampled scenarios and multiple signal sources."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6299/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700544714104,
                "cdate": 1700544714104,
                "tmdate": 1700544714104,
                "mdate": 1700544714104,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0sdkNQTkqQ",
                "forum": "4SrzKsJocx",
                "replyto": "fBppaSJkTy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6299/Reviewer_7S2y"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6299/Reviewer_7S2y"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "Thank you for the effort in addressing my concerns. I decided to keep the score. \n\nThe multi-view MNIST experiment adds more empirical evidence. However, it is still a minimal contribution to the empirical study. Also, I do not think it should be hidden in the appendix.\n\nThe \"across various scenarios\" should cover more challenging cases for the current state of the art. The instances may differ for multi-view, multi-domain, or multi-modal scenarios. At this point, the proposed work is primarily an experiment that explores a model found in Murphy and Klami and methods in Borga.\n\nOverall, I think you have the potential to write an excellent paper, but this project is in a premature state. I would suggest exploring works on intrinsic dimensionality (Facco et al., 2017; Ansuini et al., 2019), a family of multivariate statistics (Kornblith et al., 2019), and self-/shared signals (Von K\u00fcgelgen et al., 2021; Lyu et al., 2022; Liang et al., 2023) to extend the empirical direction of your work: \n- Facco, Elena, et al. \"Estimating the intrinsic dimension of datasets by a minimal neighborhood information.\" Scientific reports 7.1 (2017): 12140.\n- Ansuini, Alessio, et al. \"Intrinsic dimension of data representations in deep neural networks.\" Advances in Neural Information Processing Systems 32 (2019).\n- Kornblith, Simon, et al. \"Similarity of neural network representations revisited.\" International conference on machine learning. PMLR, 2019.\n- Von K\u00fcgelgen, Julius, et al. \"Self-supervised learning with data augmentations provably isolates content from style.\" Advances in neural information processing systems 34 (2021): 16451-16467.\n- Lyu, Qi, et al. \"Understanding Latent Correlation-Based Multi-view Learning and Self-Supervision: An Identifiability Perspective.\" International Conference on Learning Representations. 2022.\n- Liang, Paul Pu, et al. \"Factorized Contrastive Learning: Going Beyond Multi-view Redundancy.\" arXiv preprint arXiv:2306.05268 (2023)."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6299/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673181503,
                "cdate": 1700673181503,
                "tmdate": 1700673181503,
                "mdate": 1700673181503,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "S9LxIa6m6b",
            "forum": "4SrzKsJocx",
            "replyto": "4SrzKsJocx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6299/Reviewer_7ULi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6299/Reviewer_7ULi"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript conducts some numerical experiments comparing PCA to CCA, PLS, and regularized CCA in some linear-Gaussian multivariate settings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The numerical experiments seem straightforward and correct."
                },
                "weaknesses": {
                    "value": "The numerical results are essentially well understood already in the statistics community, though the specific numerics for these specific simulations are not obviously in the literature. PCA will keep the eigenvectors of the top eigenvalues of the data matrix, regardless of their source, whereas (r)CCA and PLS will keep those eigenvectors that span the joint subspace. A paper we wrote several years ago looks at the mathematics of this in some detail, https://www.nature.com/articles/s41467-021-23102-2#Sec12.  Specifically, the appendix explains how the eigenvalues matter, and we also provide theoretical guarantees using Chernoff bounds.  Another paper I like on this topic is https://www.sciencedirect.com/science/article/pii/S0047259X14001201?via%3Dihub. \n\nTo me, this reads like a very nice senior thesis, or graduate level class project, suitable for a workshop, e.g., a Neurips workshop on high-dimensional data analysis.  To warrant publication in ICLR, I would want to see some strong theoretical results, and some results on benchmark data, and/or real world data."
                },
                "questions": {
                    "value": "I think everything the authors wrote is quite clear."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6299/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6299/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6299/Reviewer_7ULi"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6299/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699239960271,
            "cdate": 1699239960271,
            "tmdate": 1700688274645,
            "mdate": 1700688274645,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "a2wzSbhCE8",
                "forum": "4SrzKsJocx",
                "replyto": "S9LxIa6m6b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6299/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6299/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the time and effort you devoted to evaluating our manuscript. Your detailed insights and references to the relevant work have helped us to improve the manuscript.\n\n$\\textbf{The concerns as we understand them:}$\nWe agree that our manuscript focuses on numerical experiments within a linear-Gaussian multivariate setting, comparing various dimensionality reduction approaches. Your remarks about the existing literature and the inherent understanding of PCA, CCA, PLS, and rCCA in the statistics community are duly noted and now incorporated in the revised manuscript. Our aim was to elucidate the differences between IDR and SDR methods in a controlled environment, and your insights into the theoretical underpinnings in this domain align with our intentions.\n\n$\\textbf{Differences in Approach and Applicability:}$\nRegarding the referenced papers, we appreciate the opportunity to delve into their core messages and distinguish our approach. While both papers make significant contributions to the domain, they are complementary to our work, and collectively advance a similar agenda.\n\n$\\underline{Supervised\\ Dimensionality\\ Reduction\\ for\\ Big\\ Data:}$ This paper primarily focuses on supervised dimensionality reduction for binary or limited class classifications. It introduces a distinctive approach where the construction of the projection matrix hinges upon class means and conditional covariance matrices. The central goal revolves around enhancing classification accuracy through a specialized projection strategy. More precisely:\n\n$\\textit{Classification-Centric Approach:}$ The paper predominantly concerns itself with classification tasks involving a limited number of classes. It primarily focuses on scenarios where DR effectiveness is measured against a discrete set of classes, generally lacking extensive exploration in terms of broader signal capture or structure identification. Note that in all of the mentioned scenarios in the paper, the supervising signal, or the second dataset, is a one-dimensional, mostly categorical, variable. This is contrasted with the situation we explore, in which both datasets are real-valued high-dimensional systems, which is common for many other applications.\n\n$\\textit{Limited Application to Self and Shared Signals:}$ While this paper provides valuable insights and analytical treatment into supervised classification tasks, it doesn't extensively explore the dimensionality of self-signals and shared structures, as we investigate in our work. Our research delves deeper into understanding these signal components and their interplay, providing a distinct dimension to the study of linear methods.\n\nBy contrasting our emphasis on self-signals and shared structures within a controlled linear setting, our work augments the existing knowledge by exploring dimensions beyond classification-centric supervised learning scenarios. While our investigation primarily delves into the intrinsic structures of data through correlations, the analytical results presented in the appendix of \"Supervised Dimensionality Reduction for Big Data\" utilizing Chernoff bounds presents an interesting way to quantify the success of different methods.\n\nWe express our gratitude for shedding light on this analytical approach, and we will be considering such techniques in our future work. However, our choice to use correlations as a metric was deliberate, aiming to offer a more intuitive measure understandable to a broader audience, including practitioners. We recognize the significance of various analytical treatments, including those rooted in random matrix theory, and view them as promising avenues for future investigations in this field.\n\n\n$\\underline{Generalized\\ Canonical\\ Correlation\\ Analysis\\ for\\ Classification:}$ This paper concentrates on generalized canonical correlation analysis and classification tasks, showcasing the inclusion of auxiliary data to augment canonical correlation approaches. Conversely, our focus lies in delineating the differences between IDR and SDR methods and their capacity to capture self and shared signals, demonstrating a unique set of questions within the domain of linear methods that has not been extensively explored.\n\n$\\textbf{Proposed Steps to Address Concerns:}$\n$\\textit{Extension to Real-World Data:}$ We acknowledge the importance of validating our findings in real-world scenarios. As part of our revised manuscript, we included an application of the methods to the Noisy MNIST dataset, demonstrating the applicability of the proposed model and the insights from it to nonlinear datasets, fostering broader understanding and relevance.\n\nWe are grateful for your suggestion of relevant prior work, and we have cited it in our revised version."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6299/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700544449414,
                "cdate": 1700544449414,
                "tmdate": 1700544449414,
                "mdate": 1700544449414,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IuK7OBpPNd",
                "forum": "4SrzKsJocx",
                "replyto": "a2wzSbhCE8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6299/Reviewer_7ULi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6299/Reviewer_7ULi"
                ],
                "content": {
                    "title": {
                        "value": "I see what you mean."
                    },
                    "comment": {
                        "value": "I'll increase the score.  I stand by my prior claim that substantial more work is required to warrant publication, but it is a very nice first step, and I encourage the authors to continue pushing along this direction to get more clear and compelling theoretical and empirical results."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6299/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688255311,
                "cdate": 1700688255311,
                "tmdate": 1700688255311,
                "mdate": 1700688255311,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lYEzOaNTbG",
            "forum": "4SrzKsJocx",
            "replyto": "4SrzKsJocx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6299/Reviewer_595z"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6299/Reviewer_595z"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a generative linear model to synthesize multimodal data for comparing the ability to find shared latent (covariance structure) of different dimensionality reduction approaches, including PCA, PLS, CCA, and regularized CCA (rCCA). Through numerical experiments on the synthetic datasets, they find that simultaneous dimensionality reduction (SDR) methods (PLS, CCA, and rCCA)  consistently outperform PCA (as an independent dimensionality reduction (IDR) method). Different configurations have been applied to the experiments, and remarkably, rCCA is significantly better than others when the number of samples is much smaller than the dimensionality of the data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The paper is written in a clear and logical way. Experimental results are well presented and understandable.\n* The metrics provided for comparing different methods are meaningful."
                },
                "weaknesses": {
                    "value": "* The proposed model is just a simple linear model, which is easy to understand but hard to fit any real-world data\n* These analyses are hard to migrate or generalize to real-world experimental data. For example, all results and conclusions in this paper are limited to the proposed generative linear model. At least, no real-world instruction is provided. See questions."
                },
                "questions": {
                    "value": "I think the main drawback of this paper is that the generative linear model is too simple. It seems like it is not something new, but just a linear model for generating a synthetic dataset. Therefore, most conclusions in this paper are drawn from that generative linear model but are hard to generalize to any real-world dataset due to the high nonlinearity in the real-world dataset. Also, the real-world data is generated in a very complicated manner (in addition to nonlinearity). Therefore, the experimental results seem intuitive and easy to me. In other words, I'm not surprised by these results, since we can expect that SDRs are better than IDRs, especially in such a simple synthetic dataset generated from a linear model. Although authors provide detailed analysis with quantitative results (metrics), I still don't see what we can tell more when facing a real-world dataset. While SDRs might still be better than IDRs. However, this seems like a very direct possible result since SDRs are methods that consider correlations/covariances between $X$ and $Y$, but IDRs are not."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6299/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6299/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6299/Reviewer_595z"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6299/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699258762449,
            "cdate": 1699258762449,
            "tmdate": 1699636691451,
            "mdate": 1699636691451,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tCaACDfCnb",
                "forum": "4SrzKsJocx",
                "replyto": "lYEzOaNTbG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6299/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6299/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the thorough review and insightful comments regarding our manuscript.\n\n$\\textbf{The concerns as we understand them:}$\n\nAs we explain in detail in the $\\textit{Common Comments}$, the simplicity of the proposed generative linear model was deliberate, aimed at providing a clear and tractable framework to explore the differences between simultaneous dimensionality reduction (SDR) and independent dimensionality reduction (IDR) methods. While the model's simplicity may limit its direct application to highly nonlinear real-world datasets, it was intended as a controlled environment to elucidate the conceptual differences between IDR and SDR techniques. However, we concur that demonstrating applicability to real-world scenarios would be useful.\n\n$\\textbf{Proposed Steps to Address Concerns:}$\n\n$\\textit{Extension to Noisy MNIST:}$ We expanded our experimental scope to include the application of linear IDR and SDR methods to the Noisy MNIST dataset (please see the Appendix). Results are consistent with our observations for the linear model.\n\n$\\textit{Generalization Beyond Linear Models:}$ While beyond the scope of this work, as it is aimed to be a clear separation and explanation of the linear methods, our other work, currently under review at ICLR as well (https://openreview.net/forum?id=ZhY1XSYqO4), delves into nonlinear dimensionality reduction methods. Our results there reveal conclusions consistent with the current work."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6299/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700543953779,
                "cdate": 1700543953779,
                "tmdate": 1700545439831,
                "mdate": 1700545439831,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XeWbzNFpiQ",
                "forum": "4SrzKsJocx",
                "replyto": "tCaACDfCnb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6299/Reviewer_595z"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6299/Reviewer_595z"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors,\n\nThanks for your response. I agree with the correctness and practical insights of this paper. However, I have the same feeling as Reviewer 7ULi that the contribution is not adequate. Therefore, I would like to keep my score.\n\nThanks again for your response."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6299/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700545119487,
                "cdate": 1700545119487,
                "tmdate": 1700545119487,
                "mdate": 1700545119487,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]