[
    {
        "title": "LeanFlex-GKP: Advancing Hassle-Free Structured Pruning with Simple Flexible Group Count"
    },
    {
        "review": {
            "id": "jaYFnjLNfn",
            "forum": "YhNXGWVH1N",
            "replyto": "YhNXGWVH1N",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7227/Reviewer_zM1b"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7227/Reviewer_zM1b"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes LeanFlex-GKP, a new structured pruning method that builds on recent work in grouped kernel pruning (GKP). This method is a one-shot, post-train, and data-agnostic. The key idea is to make the number of groups flexible across layers rather than having dynamic operations during filter grouping or pruning stages."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper clearly identifies limitations of existing GKP methods in terms of complexity from dynamic operations and proposes a sensible alternative via flexible group counts.\n- The method delivers empirical results across a wide range of model architectures and datasets.\n- As a one-shot, post-train, data-agnostic technique with minimal hyperparameters, LeanFlex-GKP is far easier to use out-of-the-box compared to many existing methods."
                },
                "weaknesses": {
                    "value": "- The contribution of this paper is limited. This paper seems an incremental work of TMI-GKP, and the main difference between this work and TMI-GKP is the group count evaluation.\n- It would be better if the authors can provide the experimental settings, such as hyperparameters they used for different models.\n- From my understanding, LeanFlex-GKP also includes dynamic choices of clustering schemes in each of its convolutional layers as TMI-GKP. I would appreciate it if the author can give results of the performance of LeanFlex-GKP and TMI-GKP. I can only get limited information from Table 1 to Table 5."
                },
                "questions": {
                    "value": "- Could the benefits of dynamic group counts extend to other structured pruning granularities like filter or channel pruning?\n- Is there an optimal strategy for setting group counts or do they need to be exhaustively evaluated?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7227/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698596637707,
            "cdate": 1698596637707,
            "tmdate": 1699636860600,
            "mdate": 1699636860600,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2EmNfWye86",
                "forum": "YhNXGWVH1N",
                "replyto": "jaYFnjLNfn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7227/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7227/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks & Initial Response to Reviewer zM1b (1/3)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for finding our method to be sensible, well-motived, and user-friendly, as well as recognizing our effort spent on proving comprehensive empirical evaluations and delivering a clear background introduction. This is almost everything we aimed for, and we are glad to learn our message is well-received. Here, we address your raised concerns and questions.\n\n### **[W1 - Incremental to TMI-GKP with only different being group count evaluation]: Our method's technical procedure differs from TMI-GKP in every possible stage, and can be applied in scenarios where TMI-GKP can't.**\n\nWe respectively disagree with this assessment, though we understand this potential \"misunderstanding\" might be due to the lack of community recognition of GKP [1] for being a recently proposed pruning granularity awaiting future improvements. We disagree with this assessment as LeanFlex-GKP has a vastly different technical procedure to TMI-GKP; yet, it can be applied in scenarios where TMI-GKP can't (or is not ideal) with many handy characteristics and advantages.\n\nFrom a technical perspective \u2014 as illustrated in [Figure 2](https://openreview.net/pdf?id=YhNXGWVH1N#page=3) of our paper \u2014 every Grouped Kernel Pruning (GKP) method has the following stages: *1) Filter grouping, 2) Grouped kernel pruning, and 3) Reconstruction of the pruned model*. **LeanFlex-GKP utilizes a totally different filter grouping and grouped kernel pruning approaches than TMI-GKP.** It considers aspects TMI-GKP does not consider (group count) and proposes an integral optimization to make the connection between stages that TMI-GKP solves individually. **Such designs lead to many performance and efficiency gains**, as demonstrated in [Table 3, 4](https://openreview.net/pdf?id=YhNXGWVH1N#page=17), and [Table 7](https://openreview.net/pdf?id=YhNXGWVH1N#page=18). **Other reviewers also tend to recognize and appreciate the technical novelty of our proposed method** ([R `SmAM`](https://openreview.net/forum?id=YhNXGWVH1N&noteId=uyaRnx6ga1): *\"the authors ... exploits GKP with self-designed clustering and pruning methods\"*; [R `HKS2`](https://openreview.net/forum?id=YhNXGWVH1N&noteId=ymbBiH0Ih6): The *\"idea of making Conv2d flexible under an integral optimization is interesting.\"*)\n\nThe only similarity between the two methods is they both prune grouped kernels, and their pruned model is reconstructed to a grouped convolution format \u2014 which are common characteristics shared by almost all proposed methods following the GKP granularity. **We respectfully argue that improving upon a proposed pruning granularity/framework is not incremental in nature.** There are thousands of methods published under the filter pruning granularity/framework [2, 3], bringing impact to the framework and providing massive contributions to the efficiency community. We argue that GKP, being a promising but newly proposed granularity/framework with only a few established methods under its wing, could also use some healthy advancements, as we provided here. \n\nOn the practical application aspect, unlike TMI-GKP, LeanFlex-GKP does not require access to model training checkpoints, **making it applicable when such access is restricted (e.g., pruning a pretrained model, which is a pretty common pipeline)**. Also, because we get to \"lean down\" on the grouping and pruning stages (and especially to avoid the expensive dimensionality reduction operations like k-PCA, as analyzed in [Section 2.1](https://openreview.net/pdf?id=YhNXGWVH1N#page=4)), the efficiency advantage makes LeanFlex-GKP applicable to wide models under a practical context. E.g., with 64-core of EPYC 7742 and an 80G A100 allocated, it takes more than 50 hours to prune just half of the ResNet101 w.r.t. TMI-GKP. In contrast, we can prune full ResNet101 w.r.t. LeanFlex-GKP in just under 2 hours. \n\n**The reviewer has already recognized the adaptability/usability advantage of our method in [*Strength 3*](https://openreview.net/forum?id=YhNXGWVH1N&noteId=jaYFnjLNfn); we hope the above comparative discussion can put such advantage into the context concerning TMI-GKP.**\n\n\nWe emphasize that our work adopts different approaches to almost all stages of GKP to TMI-GKP (though still towards the same goals to leverage the GKP granularity) with significant efficiency, adaptability, and usability improvement by solving a common pain point of exiting GKP implementations (dynamic operations). We consider this a worthy contribution and hope the reviewer would appreciate it too.\n\n\n\n\n### **[W2 - Add experiment setting (e.g., hyperparameters) clarification]: Sure!**\n\nWe have now added such a table in [Appendix C.2](https://openreview.net/pdf?id=YhNXGWVH1N#page=17). Given that this information is mostly used for result replications, we will also open source the experiment setting files and model checkpoints should our manuscript be accepted."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7227/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700140767090,
                "cdate": 1700140767090,
                "tmdate": 1700477730994,
                "mdate": 1700477730994,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "u3JRZidmLZ",
                "forum": "YhNXGWVH1N",
                "replyto": "jaYFnjLNfn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7227/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7227/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial Response to Reviewer zM1b  (2/3)"
                    },
                    "comment": {
                        "value": "### **[W3.1 - LeanFlex-GKP also includes dynamic choices of clustering schemes in each of its convolutional layers as TMI-GKP]: Not really, its clustering operation is deterministic.**\n\nTMI-GKP applies dynamic clustering schemes, as each of its convolutional layers will go through various combinations of dimensionality reduction and clustering techniques (then one clustering result is chosen according to its *tickets magnitude increase (TMI)* score; see [Section 2.1](https://openreview.net/pdf?id=YhNXGWVH1N#page=4) of our paper or [Section 3.2.1 & 3.2.2](https://openreview.net/pdf?id=LdEhiMG9WLO#page=4) of TMI-GKP [1]). In LeanFlex-GKP, we apply the same *KPP-aware filter grouping procedure* illustrated in [Figure 5](https://openreview.net/pdf?id=YhNXGWVH1N#page=7) for every conv layer, so there is no *\"dynamic choice of clustering schemes.\"*\n\nTo provide a more faithful rebuttal, we suspect the reviewer perceives LeanFlex-GKP to be dynamic in terms of filter grouping because we'd consider different `Conv2d(groups)` settings, which obviously influence the clustering/grouping results. While we do consider such settings, `Conv2d(groups)` is just one (and the only method-related) hyperparameter to the same grouping operation, instead of multiple other unique filter grouping approaches, as in TMI-GKP. \n\nThis should now be crystal clear with our above [*response to [W2]*](https://openreview.net/forum?id=YhNXGWVH1N&noteId=2EmNfWye86); we thank the reviewer for suggesting this clarification. We have further provided an early definition & example of dynamic operation in [Section 1](https://openreview.net/pdf?id=YhNXGWVH1N), as well as a pseudocode walkthrough of our method in [Appendix B.3](https://openreview.net/pdf?id=YhNXGWVH1N#page=16) to facilitate the digestion of our research motivation and proposed procedure.\n\n### **[W3.2 - Give more performance comparision between LeanFlex-GKP and TMI-GKP]: We have already compared them under 8 (now 11) different setups.**\n\nThere were more comparisons done on the two methods in other tables placed in [Appendix D](https://openreview.net/pdf?id=YhNXGWVH1N#page=19). Here, we provide a gap table for your viewing convenience; **we also added three new results** since our initial submission to make a more comprehensive comparison:\n\n\n> $\\Delta$Acc gap between TMI-GKP and LeanFlex-GKP in %, `+` indicates LeanFlex-GKP is better.\n> \n| Model-Dataset            | $\\Delta$Acc Gap |  Source | \n| - | :-: | :-: |\n| ResNet50-ImageNet        | +0.11                                      | [Table 9](https://openreview.net/pdf?id=YhNXGWVH1N#page=20) |\n| ResNet56-TinyImageNet    | +3.65                                      | [Table 10](https://openreview.net/pdf?id=YhNXGWVH1N#page=20) |\n| ResNet101-TinyImageNet   | +1.57                                      | Table 10 |\n| ResNet20-CIFAR10         | +0.31                                      |  [Table 16](https://openreview.net/pdf?id=YhNXGWVH1N#page=23) |\n| ResNet32-CIFAR10         | +0.02                                      | Table 16 |\n| ResNet56-CIFAR10         | +0.05                                      | Table 16 |\n| ResNet110-CIFAR10        | +0.02                                      | Table 16 |\n| VGG16-CIFAR10            | +0.11                                      |  [Table 12](https://openreview.net/pdf?id=YhNXGWVH1N#page=21) |\n| ResNet56-CIFAR100 (new)  | +0.32                                      |  [Table 14](https://openreview.net/pdf?id=YhNXGWVH1N#page=21) |\n| ResNet110-CIFAR100 (new)  | +0.63                                      | Table 14 |\n| DenseNet40-CIFAR10 (new) | +0.35                                      | [Table 13](https://openreview.net/pdf?id=YhNXGWVH1N#page=21) |\n\n**It is clear that LeanFlex-GKP has a performance advantage to TMI-GKP across 11 different experiment settings.** We'd say the performance between the two methods is indeed close under some particular setups (e.g., ResNet32/56/110-CIFAR10). But, as we noted in [*Section 5 - Discussion and Conclusion*](https://openreview.net/pdf?id=YhNXGWVH1N#page=8), these BasicBlock CifarResNets experiments are getting saturated, as methods with significant performance gaps on more difficult model-dataset combinations tend to show little difference upon them. Our gap table above also supports this observation with much more meaningful gaps observed under harder settings. \n\nAt the risk of being redundant (as the reviewer already recognizes it in [*Strength 3*](https://openreview.net/forum?id=YhNXGWVH1N&noteId=jaYFnjLNfn)), we again emphasize that performance improvement is just one aspect of our contribution. LeanFlex-GKP has significant efficiency, adaptability, and usability advantages over TMI-GKP, as detailed in our [*[W1] response*](https://openreview.net/forum?id=YhNXGWVH1N&noteId=2EmNfWye86) above; yet, our dynamic group count design may inspire all future GKP implementations."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7227/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700140819174,
                "cdate": 1700140819174,
                "tmdate": 1700477753275,
                "mdate": 1700477753275,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RT4LWmRCrE",
                "forum": "YhNXGWVH1N",
                "replyto": "jaYFnjLNfn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7227/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7227/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial Response to Reviewer zM1b (3/3)"
                    },
                    "comment": {
                        "value": "### **[Q1 - Can dynamic group counts benefits filter/channel pruning?]: Depending on how you want them to work together.**\n\nSince group count `Conv2D(groups)` is a hyperparameter specific to the grouped convolution format, it is incompatible with the standard filter/channel pruning method where the pruned model is not in a grouped convolution format. However, we can do it the other way around, e.g., we may first prune filters to obtain a smaller layer tensor, then apply GKP (with or without dynamic group counts) to the reduced layer tensor. \n\nWhile this will require a much different pipeline beyond the scope of our paper, we are confident that dynamic group count will provide benefits, as a filter-pruned model is similar to a smaller (unpruned baseline) model, and our dynamic group count design has showcased improvements across different setups, as demonstrated in [Appendix C](https://openreview.net/pdf?id=YhNXGWVH1N#page=16).\n\n### **[Q2 - Is there an optimal strategy for setting group counts, or do they need to be exhaustively evaluated?]: It must be post-hoc evaluated, but we don't need to find the absolute optimal.**\n\nGiven different group count settings will alter the compute graph of the model, within the train-prune-finetune pipeline, the optimal group count can only be trialed and confirmed with post-hoc evaluations. Considering the data-agnostic setting, we believe the reliance on post-hoc evaluation is always necessary.\n\nHowever, at the risk of being obvious, we'd note the actual task is not to find the absolute optimal group count setting per different layers (which is costly to search in nature since one cannot find out the best setting in a layer by layer manner even with fine-tuning, as it is a combinatorial problem). We just need to make a reliable guess of which group count setting is better via indicators obtained with cheap operations. Our method does precisely that with lightning-fast data-agnostic group/prune procedures, and it is empirically better at finding the suitable group count than many other potential solutions (see [Appendix C](https://openreview.net/pdf?id=YhNXGWVH1N#page=16)).\n\n---\n[1] Zhong et al., Revisit Kernel Pruning with Lottery Regulated Grouped Convolutions. ICLR 2022   \n[2] Zhou et al., Less is More: Towards Compact CNNs. ECCV 2016    \n[3] Li et al., Pruning Filters for Efficient ConvNets. ICLR 2017"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7227/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700140852656,
                "cdate": 1700140852656,
                "tmdate": 1700477787266,
                "mdate": 1700477787266,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wcqlG084Gf",
                "forum": "YhNXGWVH1N",
                "replyto": "jaYFnjLNfn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7227/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7227/Authors"
                ],
                "content": {
                    "title": {
                        "value": "An invite to discussion; as well as a digested summary of our rebuttal."
                    },
                    "comment": {
                        "value": "Right now, with `555` (which are borderline rejections for ICLR), we are hoping to receive some engagements given we posted our rebuttal 4 days ago with the discussion deadline coming close (**Nov 22, [no second stage](https://iclr.cc/Conferences/2024/AuthorGuide)** this year). **We especially wish to receive a response from you**, as all other reviewers' questions are mostly factually rooted and can be objectively addressed. But [your W1](https://openreview.net/forum?id=YhNXGWVH1N&noteId=jaYFnjLNfn) touches on the technical novelty/contribution aspect of our paper, which is a multifaced issue that we believe can only be addressed via a faithful discussion.\n\nWe understand that the reviewing pressure is heavy, and we all have personal matters to attend to \u2014 **which is possibly especially the case for ICLR, given many of us are authors and reviewers at the same time,** \u2014 but please excuse us for urging, as we want to make sure your concerns are adequately addressed. To respect your time, here we provide a contextual summary for your digestion convenience.\n\n---\n\nWe believe it is fair to say that, despite a rate of `5`, **your [feedback](https://openreview.net/forum?id=YhNXGWVH1N&noteId=jaYFnjLNfn) are plenty supportive: as you find our proposed method to be *technically sound*** (*\"proposes a sensible alternative...\"*), ***well-motivated*** (*\"the paper clearly identifies limitations of...\"*), ***performant*** (*\"delivers empirical results across a wide range of model architectures and datasets\"*), **and *user-friendly*** (*\"LeanFlex-GKP is far easier to use out-of-the-box...\"*) \u2014 which are basically everything we hope to strike for; **we appreciate your recognition.**\n\n\n\nHere, we venture to categorize your concerns as the following tri-fold: 1) technical novelty/contribution of LeanFlex-GKP over TMI-GKP. 2) performance comparsion between the two, and 3) clarifications regarding LeanFlex-GKP's grouping procedure, its compatibility with filter/channel pruning, and evaluation procedure for optimal group count.\n\n\n---\n**We believe we have addressed your *concerns #2 & #3* in a head-on manner** with a [gap table](https://openreview.net/forum?id=YhNXGWVH1N&noteId=u3JRZidmLZ) filled with 11 comparative results (where 3 of them are newly added) and some detailed clarifications (on [grouping](https://openreview.net/forum?id=YhNXGWVH1N&noteId=u3JRZidmLZ) and [the rests](https://openreview.net/forum?id=YhNXGWVH1N&noteId=RT4LWmRCrE)). For *concern #1*, we illustrate the **technical procedure of LeanFlex and TMI-GKP actually differs at every possible stage**. Yet, from a practical standpoint, **LeanFlex-GKP can be utilized in scenarios where TMI can't** (e.g., w/o access to training checkpoint, models with wide layers) and is generally a lot more user-friendly due to several hassle-free designs.\n\nThe reviewer is indeed correct that both methods do prune at the same granularity and deliver their pruned models in a grouped convolution format; though, we emphasize that this is the general norm for methods following the GKP granularity/framework. We note \u2014\u00a0though we believe the reviewer is sure aware \u2014 **that advancements under the same framework are common and nonetheless contributive** to the community, which is especially the case given GKP's recency (coined in ICLR 2022) compared to established granularities/frameworks like filter pruning.\n\nOn this note, we advocate our observations and investigations on dynamic operations, as well as our proposed integral optimization to make connections on previously separated GKP stages, should provide reference to and inspire future structured pruning works.\n\nWe hope the reviewer will appreciate our above analysis. Thanks in advance!\n\nSincerely,   \nPaper7227 Authors"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7227/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700520692178,
                "cdate": 1700520692178,
                "tmdate": 1700626453240,
                "mdate": 1700626453240,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ymbBiH0Ih6",
            "forum": "YhNXGWVH1N",
            "replyto": "YhNXGWVH1N",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7227/Reviewer_HKS2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7227/Reviewer_HKS2"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes that the best practice to introduce the dynamic operations to GKP is to make Conv2d flexible under an integral optimization, and proposes a one-shot, post-train, data-agnostic GKP method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The writing is clear.\n2. The background about Different Structured Pruning Granularities, Grouped Kernel Pruning, and Dynamic Structure Pruning is clearly introduced.\n3. The idea of making Conv2d flexible under an integral optimization is interesting."
                },
                "weaknesses": {
                    "value": "1. The evaluation lacks comprehensiveness.\n\n- Baselines are restricted. A lot of related works about iterative structured pruning [1-10] and one-shot pruning [11-15] are not compared.\n\n- Most of the evaluation focuses on the ResNet and VGG architectures. It will be better if more model architectures are evaluated, especially small models like MobileNet, EfficientNet, and ShuffleNet.\n\n- In Section 4, it will be better if some insights can be provided, not only listing numbers.\n\n2. The Ablation Study is missing.\n\n3. The structure can be improved. The Introduction occupies too much space. The first 3.5 pages are all about the Introduction. \n\n4. The discussion about the related works is insufficient, such as iterative structured pruning [1-10], one-shot pruning [11-14], Grouped Kernel Pruning [15], and automatic pruning (with little-to-no hyper-parameter tuning) [2, 3].\n\n[1] EigenDamage: Structured Pruning in the Kronecker-Factored Eigenbasis\n\n[2] Automatic Attention Pruning: Improving and Automating Model Pruning using Attentions\n\n[3] Amc: Automl for model compression and acceleration on mobile devices\n\n[4] Layer-adaptive sparsity for the magnitude-based pruning\n\n[5] Chipnet: Budget-aware pruning with heaviside continuous approximations\n\n[6] Provable filter pruning for efficient neural networks\n\n[7] Accelerate CNNs from Three Dimensions: A Comprehensive Pruning Framework\n\n[8] EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning BT\n\n[9] DMCP: Differentiable Markov Channel Pruning for Neural Networks\n\n[10] GDP: Stabilized Neural Network Pruning via Gates with Differentiable Polarization\n\n[11] Only train once: A one-shot neural network training and pruning framework\n\n[12] Evolutionary multi-objective one-shot filter pruning for designing lightweight convolutional neural network\n\n[13] One-shot layer-wise accuracy approximation for layer pruning\n\n[14] One-shot Network Pruning at Initialization with Discriminative Image Patches\n\n[15] Group-based network pruning via nonlinear relationship between convolution filters"
                },
                "questions": {
                    "value": "1. Can authors compare the proposed method with more related pruning works [1-14] (See above)?\n\n2. Can some ablation study be provided?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7227/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698972236349,
            "cdate": 1698972236349,
            "tmdate": 1699636860491,
            "mdate": 1699636860491,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mrbcwq5SxY",
                "forum": "YhNXGWVH1N",
                "replyto": "ymbBiH0Ih6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7227/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7227/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks & Initial Response to Reviewer HKS2 (1/2)"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer for recognizing the clarity of our delivery and finding our proposed solution to be interesting. Here, we address the concerns and questions you raised. \n\n(Additionally, while it is our understanding that the reviewer provided the extensive list of related work without noticing the comparative evaluation we have done in [Appendix D.2](https://openreview.net/pdf?id=YhNXGWVH1N#page=19), we still want to send our gratitude to the reviewer for the time and effort spent \u2014 thank you for taking much care in the reviewing process!)\n\n\n### **[W1 & Q1 - Evaluation lacks comprehensiveness]: We have already compared with 16 popular structured pruning methods. But sure, we can add 16 more.**\n\n\nWe kindly direct the reviewer's attention to [Appendix D.2](https://openreview.net/pdf?id=YhNXGWVH1N#page=19), where we have compared 16 structured pruning methods by the time of initial submission (listed as the first part of [Table 8](https://openreview.net/pdf?id=YhNXGWVH1N#page=19)). To our understanding, this is already a pretty comprehensive coverage that is probably more extensive than most (if not all) pruning arts we compared and suggested by Reviewer `HKS2`.\n\n**As the reviewer is interested, we now add comparisons to 16 *more* methods.** 10 of them are from the reviewer's suggestions, yet 6 additional methods are also included during the expansion exploration (respectively listed as the second and the third portion of [Table 8](https://openreview.net/pdf?id=YhNXGWVH1N#page=19)). We also kindly note that some of the suggested methods by the reviewer are incomparable due to reasons like being under a different paradigm (e.g., LAMP [4], for being unstructured) and not having comparable experiment setups (e.g., EMOFP [12], which is MNIST-focused); yet, methods like 3D [7] and OTOv2 [11] are already compared in our initial submission. **We also added MobileNetV2 results in [Table 10](https://openreview.net/pdf?id=YhNXGWVH1N#page=20)**, as requested by the reviewer.\n\n**Now, we are comparing LeanFlex-GKP against 32 methods and evaluating it under 20 different settings.** To the best of our knowledge \u2014 and with many of our experiments running upon an identical baseline under a controlled pipeline \u2014 we believe our evaluation is considered the most comprehensive in the structured pruning field. \n\nWe guess the reviewer was left with the impression that our evaluation lacks comprehensiveness because of the limited results posted in the main text of our initial submission. We now provide an abbreviated table in the main text ([Table 1](https://openreview.net/pdf?id=YhNXGWVH1N#page=9)), as well as the above-mentioned Method Overview table ([Table 8](https://openreview.net/pdf?id=YhNXGWVH1N#page=19)) to illustrate all compared methods. As the reviewer suggested, we also expanded the discussion and provided more insight in [*Section 5 - Discussion and Conclusion*](https://openreview.net/pdf?id=YhNXGWVH1N#page=8) regarding the results posted in [Section 4.1](https://openreview.net/pdf?id=YhNXGWVH1N#page=8). We hope the reviewer may find the new additions helpful. \n\n\n### **[W2 & Q2 - Ablation study is missing]: It is in Appendix C. We now expand its coverage with better description.**\n\nWe kindly direct the reviewer's attention to [Appendix C](https://openreview.net/pdf?id=YhNXGWVH1N#page=16), where we provided a full-scale ablation study at the time of initial submission. For better readability, we have expanded the ablation study coverage, added more descriptions, and placed an anchor in bold text in [*Section 4 - Experiments*](https://openreview.net/pdf?id=YhNXGWVH1N#page=8) to help navigate among related sections."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7227/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700138419681,
                "cdate": 1700138419681,
                "tmdate": 1700477666606,
                "mdate": 1700477666606,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XzCBeDDTF3",
                "forum": "YhNXGWVH1N",
                "replyto": "ymbBiH0Ih6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7227/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7227/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial Response to Reviewer HKS2 (2/2)"
                    },
                    "comment": {
                        "value": "### **[W3 - Paper structured can be improved with shorter Intro]: Sure, we changed it up a little!**\n\nWe agree that our paper has a (possibly overly) prolonged intro & motivation section ([Section 1 and 2](https://openreview.net/pdf?id=YhNXGWVH1N)). However, given our method is built upon specific observations of the recently proposed GKP granularity [16] with only limited exposure, we feel like we owe it to our readers to provide a clear and unified introduction of the GKP framework and its adaptations without asking them to jump among different papers. Should we remove too much of it, our readers might get confused about GKP and none of our contributions matter. **All reviewers, including you, happen to appreciate the clarity of our background introduction**; so we don't want to drastically modify these sections, if possible.\n\nThat being said, we have cut off some repeated material in [Section 1 and 2](https://openreview.net/pdf?id=YhNXGWVH1N), added more connections between our methodology and contribution claims in [Section 3](https://openreview.net/pdf?id=YhNXGWVH1N#page=6) (suggested by [Reviewer `SmAm`](https://openreview.net/forum?id=YhNXGWVH1N&noteId=uyaRnx6ga1)), and added more result discussions in [Section 5](https://openreview.net/pdf?id=YhNXGWVH1N#page=8) (suggested by you).\n\n### **[W4 - Discussion of related work is insufficient]: Agreed, we have now added them.**\n\nAs noted in our response to [*[W1 & Q1]*](https://openreview.net/forum?id=YhNXGWVH1N&noteId=mrbcwq5SxY), we have already provided comprehensive coverage of different pruning methods, but we haven't properly discussed them. We have referenced and added such proper discussion in [Appendix A](https://openreview.net/pdf?id=YhNXGWVH1N#page=14) for all compared methods, as well as some landmark work and important aspects of structured pruning.\n\nWe were probably too focused on getting the GKP intro right (and from the feedback, we did) and, therefore, lost the big picture a little. We thank the reviewer for pointing this out and directing us to provide a more zoom-out view of the structured pruning field to our readers.\n\n---\n(here, we follow the citation index initiated by the reviewer for better consistency)     \n[4] Lee et al., Layer-adaptive sparsity for the Magnitude-based Pruning. ICLR 2021    \n[7] Wang et al., Accelerate CNNs from Three Dimensions: A Comprehensive Pruning Framework. ICML 2021    \n[11] Chen et al., OTOv2: Automatic, Generic, User-Friendly. ICLR 2023 \u2014 this is `v2` of the reviewer suggested method [11].    \n[12] Wu et al., Evolutionary Multi-Objective One-Shot Filter Pruning for Designing Lightweight Convolutional Neural Network. Sensors 2021     \n[16] Zhong et al., Revisit Kernel Pruning with Lottery Regulated Grouped Convolutions. ICLR 2022"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7227/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700138481493,
                "cdate": 1700138481493,
                "tmdate": 1700477686220,
                "mdate": 1700477686220,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wcdZLMIOjw",
                "forum": "YhNXGWVH1N",
                "replyto": "ymbBiH0Ih6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7227/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7227/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Your concerns are rather factually rooted; we have met them with head-on responses \u2014 mind confirming if everything is resolved?"
                    },
                    "comment": {
                        "value": "Dear Reviewer `HKS2 `,\n\nIf we may, we'd venture to categorize your concerns as the following: *1) More baselines; 2) More model architectures; 3) Need ablation studies; 4) Writing structure improvement; and 5) More related work discussion.*\n\nWe believe we have \n* **met your *concern #1, #2, and #3* head-on by pointing to the rich comparison (and ablation studies) we conducted at the time of submission, as well as the newly added results.**\n    * Now, with [32 methods compared](https://openreview.net/pdf?id=YhNXGWVH1N#page=19) and [20 results](https://openreview.net/pdf?id=YhNXGWVH1N#page=18) of LeanFlex-GKP reported, we believe we are among the very top of the evaluation department \u2014 if not the best \u2014 within the structured pruning field. \n    * We also conducted **new experiments on [MobileNetV2](https://openreview.net/pdf?id=YhNXGWVH1N#page=20)** as the reviewer suggested, and improved **the presentation of (of our now expanded) ablation studies** and [abbreviated experiment results](https://openreview.net/pdf?id=YhNXGWVH1N#page=9) in the main text \u2014\u00a0two reviewers have missed our results previously; this new update should help.\n* addressed your *concern #4* by expanding [Section 3](https://openreview.net/pdf?id=YhNXGWVH1N#page=6) to make more connections to our claimed contributions and cutting down some repetitive material in Section 1 & 2.\n* **added a [one-page walkthrough](https://openreview.net/pdf?id=YhNXGWVH1N#page=14)** of different aspects of structured pruning in the related work sections (*concern #5*).\n\n\nAs mentioned in the title, we believe **your concerns are rather *factually rooted* \u2014 as in, there is a minimum-to-zero chance of us disagreeing whether the provided rebuttals have addressed the raised concerns.** We, of course, understand the reviewing pressure [16]. However, please excuse us for urging, as we want to ensure your concerns are adequately addressed.\u00a0**Would you be so kind as to leave us a quick confirmation and maybe consider improving your rating?**\n\nThanks in advance, and please do let us know if there's more we can answer.\n\n---\n\n[16] We believe this year's ICLR schedule is a bit non-optimal, as a real-time system combined with a short discussion stage means the authors are motivated to post their rebuttals asap. Meanwhile, the reviewers \u2014 who are likely also authors \u2014 are also rushing their rebuttals, rendering low engagement despite everyone working around the clock. We are sorry to urge you in such a situation, but we hope the reviewer would understand, given we have a borderline scoring with most raised concerns that can be (and have been) objectively addressed."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7227/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687221237,
                "cdate": 1700687221237,
                "tmdate": 1700687333900,
                "mdate": 1700687333900,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uyaRnx6ga1",
            "forum": "YhNXGWVH1N",
            "replyto": "YhNXGWVH1N",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7227/Reviewer_SmAM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7227/Reviewer_SmAM"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose a fine-grained pruning approach, which exploits grouped kernel pruning (GKP) with self-designed clustering and pruning methods, corresponding to high performance and general efficient inference speed. The authors identify that the existing methods correspond to coarse-grained pruning with inferior accuracy performance. In addition, they propose a grouping method to enable exploiting general infrastructure with the pruned model. Then, they propose a L-2 geometric method-based grouped kernel pruning method to perform the pruning operation. Furthermore, they exploit a post-pruning group count evaluation to evaluate the pruned model. They conducted experimental comparison with 5 baseline approaches, which demonstrates the advantages of the proposed approach."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The authors propose a find-grained pruning method that can have higher accuracy performance.\n2. The introduction section is detailed with the presentation of the background explanation.\n3. The experimental results seems promising."
                },
                "weaknesses": {
                    "value": "1. The structure of the paper can be improved. Sections 1 and 2 are too detailed that Section 3 corresponds to only a small part, which is not enough to explain the major contribution.\n2. 5 baseline approaches are compared while some lossless approaches or other baselines can be added as baseline approaches.\n3. It is not clear whether the proposed method can achieve lossless pruning. Some theoretical analysis may be beneficial to the paper.\n4. Many grammar errors, e.g., an higher ***, they can be run, with in, is determine by, with lowest etc.\n5. Dependable experience may be independable experience."
                },
                "questions": {
                    "value": "1. I wonder if \"dependable experience\" should be independable experience. \n2. I wonder if the proposed approach is lossless. In addition, I wonder if the proposed approach can be applied to other structures.\n3. I wonder if the authors can compare the proposed approach with lossless pruning methods."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7227/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699162827215,
            "cdate": 1699162827215,
            "tmdate": 1699636860381,
            "mdate": 1699636860381,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QUebbwQTfu",
                "forum": "YhNXGWVH1N",
                "replyto": "uyaRnx6ga1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7227/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7227/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks & Initial Response to Reviewer SmAM (1/2)"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer for recognizing the evaluation and the performance aspect of our proposed method, as well as our detailed background illustration. Here, we address the concerns and questions you raised.\n\n### **[W1 - Paper structured can be improved (Sec 1&2 are too long and Sec 3 should be expanded)]: Sure, we changed it up a little.**\n\nWe agree that our paper has a (possibly overly) prolonged intro & motivation section ([Section 1 and 2](https://openreview.net/pdf?id=YhNXGWVH1N)). However, given our method is built upon specific observations of the recently proposed GKP granularity [1] with only limited exposure, we feel like we owe it to our readers to provide a clear and unified introduction of the GKP framework and its adaptations without asking them to jump among different papers. Should we remove too much of it, our readers might get confused about GKP and none of our contributions matter. **All reviewers, including you, happen to appreciate the clarity of our background introduction**; so we don't want to drastically modify these sections, if possible.\n\nThat said, we agree with the reviewer that *[Section 3 - Proposed Method](https://openreview.net/pdf?id=YhNXGWVH1N#page=6)* should be expanded to better connect our claimed contribution and actual methodology. **We added such connections in [Section 3](https://openreview.net/pdf?id=YhNXGWVH1N#page=6) and cut off some repetitive material in [Section 1 and 2](https://openreview.net/pdf?id=YhNXGWVH1N)**; we thank the reviewer for suggesting this. \n\n### **[W2.1 - More than 5 baselines should be compared]: We have in fact compared 16 (now 32) pruning methods. We now provide such results with better presentation for improved readability.**\n\n\nWe kindly direct the reviewer's attention to [Appendix D.1](https://openreview.net/pdf?id=YhNXGWVH1N#page=19), where we have compared 16 structured pruning methods by the time of submission (first part of [Table 8](https://openreview.net/pdf?id=YhNXGWVH1N#page=19)). **Now**, after including the recommended work by [reviewer `HKS2`](https://openreview.net/forum?id=YhNXGWVH1N&noteId=ymbBiH0Ih6), **we are comparing LeanFlex-GKP against 32 pruning methods** (see [Table 8](https://openreview.net/pdf?id=YhNXGWVH1N#page=19)). To the best of our knowledge \u2014 and with many of our experiments conducted with the identical baseline under a controlled pipeline \u2014 we believe our evaluation is considered the most comprehensive in the structured pruning field.\n\n\nWe guess the reviewer was left with the impression that we only compared with 5 baselines because of the limited results posted in the main text of our initial submission. We now provide an abbreviated table in the main text ([Table 1](https://openreview.net/pdf?id=YhNXGWVH1N#page=9)), as well as a Method Overview table ([Table 8](https://openreview.net/pdf?id=YhNXGWVH1N#page=19)) to illustrate all compared methods. We hope the reviewer may find them helpful.\n\n\n### **[W2.2 & W3.1 & Q2.1 & Q3 - Is the proposed method \"lossless\"?]: It depends on your definition of \"lossless.\" Good accuracy retention? \u2014 Yes; Exact output? \u2014 No.**\n\nAlmost all pruning methods will alter the output of the original model (unless under some very special and often trivial setups). Thus, if by \"lossless,\" the reviewer means \"exact,\" like FlashAttention [2], then our method is not \"lossless.\"\n\nThis conclusion is rather obvious, given we reported $\\Delta$Acc in all our experiments, indicating the difference between the unpruned and prune models. To make the rebuttal more faithful, we suspect the reviewer meant to ask if our method has good accuracy retention (maybe even an improvement after prune). **The answer to this question is model-dataset-setting dependent, but we'd say our method can generally provide good accuracy retention under a reasonable setup**. This is evidenced by 17 out of 20 reported results of LeanFlex-GKP showing improvements after pruning, where no other method can provide a $\\uparrow$ $\\Delta$Acc under the three exception setups."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7227/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700136580533,
                "cdate": 1700136580533,
                "tmdate": 1700659422747,
                "mdate": 1700659422747,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OATWGj1oPM",
                "forum": "YhNXGWVH1N",
                "replyto": "uyaRnx6ga1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7227/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7227/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial Response to Reviewer SmAM (2/2)"
                    },
                    "comment": {
                        "value": "### **[W3.2 - Lack of theoritcal analysis]: Unfortunately, this is beyond the currently avaliable instruments.**\n\n\nAs we are sure the reviewer is well aware, most theoretical analyses done on unpruned NNs require settings that are far distant from the actual models used in pruning tasks (e.g., two-layer ultra-wide MLP [3] vs ResNets). The analysis of pruned NNs is arguably harder. To the best of our knowledge, there is no theoretical analysis for data-agnostic pruning methods following the classic train-prune-finetune pipeline, even for the long-proposed popular filter pruning framework [4, 5] \u2014 which was introduced almost a decade ago and has thousands of follow-ups.\n\nGiven GKP future complicates the process by alternating the compute graph of the original model (from standard conv to grouped conv), providing a faithful theoretical analysis is, unfortunately, beyond the currently available instruments. **Alternatively**, we have provided a rather comprehensive procedural-based ablation study in [Appendix C.1](https://openreview.net/pdf?id=YhNXGWVH1N#page=16), where we controllably swap in/out components of LeanFlex-GKP with other possible procedural recipes, to facilitate the understanding of method. We hope that would also satisfy the reviewer.\n\n\n### **[W4 - Typos]: Fixed, and thank you for the close read!**\n\n### **[W5 & Q1 - Should \"dependable experience\" be \"independable experience\"?]: Will change to \"predictable\" for disambiguation.]**\n\nAt the end of [Section 2](https://openreview.net/pdf?id=YhNXGWVH1N#page=6), we note that LeanFlex-GKP's pruned size/compute is directly estimable by multiplying the pruning rate by the readings of the original model, thus *\"making the whole pruning procedure a standardized and [dependable] experience.\"* Here, we use the term \"dependable\" to indicate our method is reliable and predictable in terms of pruned model size/compute estimation (e.g., in contrast to methods like GAL [6], where one needs to trial-and-error different hyperparameter combinations to reach the desired reduction). So, we indeed meant \"dependable.\" \n\nHowever, we realize this might cause confusion, as \"dependable\" can imply hard coupling, which is often not a favorable character. Thus, we will change the term to \"predictable\" for disambiguation. We thank the reviewer for catching this small but vital detail.\n\n### **[Q2.2 - Can the proposed approach applied to other structures?]: Again, it depends on your definition of \"structure.\" With filter/channel pruning? \u2014 Yes; Outsides CNNs? \u2014 No.**\n\nSuppose the reviewer is referring to \"structures\" as model architectures; given that the convolutional kernel is a concept specific to CNN-like architectures, our method cannot apply to other architectures (e.g., transformers), and neither do most filter/channel pruning methods. However, if the reviewer refers to \"structures\" as pruning structures, then yes: LeanFlex-GKP can work in conjunction with a filter/channel pruning method \u2014 e.g., first remove some filters deemed unimportant, then apply GKP to the pruned model.\n\n---\n[1] Zhong et al., Revisit Kernel Pruning with Lottery Regulated Grouped Convolutions. ICLR 2022    \n[2] Dao et al., FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. NeurIPS 2022    \n[3] Du & Zai et al., Gradient Descent Provably Optimizes Over-parameterized Neural Networks. ICLR 2019    \n[4] Zhou et al., Less is More: Towards Compact CNNs. ECCV 2016    \n[5] Li et al., Pruning Filters for Efficient ConvNets. ICLR 2017    \n[6] Lin et al., Towards Optimal Structured CNN Pruning via Generative Adversarial Learning. CVPR 2019"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7227/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700136614393,
                "cdate": 1700136614393,
                "tmdate": 1700658771187,
                "mdate": 1700658771187,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6fnYY64bIj",
                "forum": "YhNXGWVH1N",
                "replyto": "uyaRnx6ga1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7227/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7227/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Your concerns are rather factually rooted; we have met them with head-on responses \u2014 mind confirming if everything is resolved?"
                    },
                    "comment": {
                        "value": "Dear Reviewer `SmAM `,\n\nIf we may, we'd venture to categorize your concerns as the following: *1) More baselines; 2) Clarification on whether the proposed method is lossless and appliable to other structures 3) Cosmetic/structures suggestions regarding typos fixing, Section 3 expansion, and the use of the term \"dependable\"; 4) Lack of theoretical analysis.*\n\n**We believe we have:**\n* **met your *concern #1* head-on by pointing to the rich comparison we made at the time of submission, as well as the newly added results.** \n    * Now, with [32 methods compared](https://openreview.net/pdf?id=YhNXGWVH1N#page=19) and [20 results](https://openreview.net/pdf?id=YhNXGWVH1N#page=18) of LeanFlex-GKP reported, we believe we are among the very top of the evaluation department \u2014 if not the best \u2014 within the structured pruning field. \n* **provided a detailed walkthrough** on different potential interpretations **of your *concern #2***; we believe our clarifications are easy to understand and support our work.\n\t* In short, our method has good accuracy retention but is not exact/equivalent to the unpruned network. It can work in conjunction with filter/channel pruning but not on non-CNNs. \n   \n* **made direct adjustments** following your *concern #2* by expanding [Section 3](https://openreview.net/pdf?id=YhNXGWVH1N#page=6) to make more connections to our claimed contributions, fixing typos, and replacing the ambiguous \"dependable\" term with \"predictable.\"\n* **rationally illustrated** why a faithful theoretical analysis of GKP is beyond the currently available instruments (*concern #4*). \n    * Evidenced by the lack of theoretical support for the long-proposed data-agnostic filter pruning framework.\n\nAs mentioned in the title, we believe **your concerns are rather *factually rooted* \u2014 as in, there is a minimum-to-zero chance of us disagreeing whether the provided rebuttals have addressed the raised concerns.** We, of course, understand the reviewing pressure [7]. However, please excuse us for urging, as we want to ensure your concerns are adequately addressed.\u00a0**Would you be so kind as to leave us a quick confirmation and maybe consider improving your rating?**\n\nThanks in advance, and please do let us know if there's more we can answer.\n\n---\n\n[7] We believe this year's ICLR schedule is a bit non-optimal, as a real-time system combined with a short discussion stage means the authors are motivated to post their rebuttals asap. Meanwhile, the reviewers \u2014 who are likely also authors \u2014 are also rushing their rebuttals, rendering low engagement despite everyone working around the clock. We are sorry to urge you in such a situation, but we hope the reviewer would understand, given we have a borderline scoring with most raised concerns that can be (and have been) objectively addressed."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7227/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682761377,
                "cdate": 1700682761377,
                "tmdate": 1700687305052,
                "mdate": 1700687305052,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]