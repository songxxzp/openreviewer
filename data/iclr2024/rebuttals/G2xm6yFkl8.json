[
    {
        "title": "BenthIQ: a Transformer-Based Benthic Classification Model for Coral Restoration"
    },
    {
        "review": {
            "id": "UZfODEMQj9",
            "forum": "G2xm6yFkl8",
            "replyto": "G2xm6yFkl8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3815/Reviewer_cnzW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3815/Reviewer_cnzW"
            ],
            "content": {
                "summary": {
                    "value": "This paper shows a semantic segmentation model applied to coral aerial photo segmentation. The model is based on a U-net shaped architecture with Swin transformer as backbone. The authors releases a new dataset, on which they made a comparison between state-of-the-art semantic segmentations models and their model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The results are surpassing other models, yet not necessarily largely. \nThe paper is well-written, it proposes an interesting application and a new dataset."
                },
                "weaknesses": {
                    "value": "The model that was proposed as 'new' is actually a well-known model, so there is no methodological novelty. Unless the authors prove me wrong on the following points (in particular the first one), I don't think it is worth publishing in ICLR.\n1) By reading the paper, it looks as if the model proposed was constructed by the authors, while it is an exact copy of the 'Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation', Cao et al. 2022 (cited 1300 times). This paper is not even cited. Please comment and explain.\n2) It would be important to also compare your model on existing databases and thus existing benchmarks. Is there a reason why you did not?\n3) The release of data is not fully clear. It is said that it will be released at the paper acceptance, but on another part of the paper it is said that it will be released 'by TNC upon completion of the database': will that be at the paper acceptance?"
                },
                "questions": {
                    "value": "cf. section weakenesses.\n\nQ1) As I understand, the dataset is new release coming with the paper. From my perspective, it can have an \ninteresting value, but you would need to better explain the advantage of this database compared to existing ones, and also how was the labelling performed, and with which uncertainty?\n\n\nTypos:\n- of the these images\n- We share and our code"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3815/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698670504868,
            "cdate": 1698670504868,
            "tmdate": 1699636338821,
            "mdate": 1699636338821,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "G2lrKUMv9J",
            "forum": "G2xm6yFkl8",
            "replyto": "G2xm6yFkl8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3815/Reviewer_G8Fd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3815/Reviewer_G8Fd"
            ],
            "content": {
                "summary": {
                    "value": "This work addresses the semantic segmentation on benthic images. The designed BenthIQ is a U-shaped neural network with Swin Transformer and skip connections for the semantic segmentation of benthic images. The collected dataset contains benthic images labeled with four pixel-level categories of sand, coral, algae, and rock. The authors analyzed the performance of BenthIQ on the collected dataset in terms of ablation study and benthic classification."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- It is meaningful to collect real-world benthic images with semantic labels for the study of coral reef restoration and protection."
                },
                "weaknesses": {
                    "value": "- The novelty of this work is limited. The designed method, namely BenthIQ, is a general architecture that is commonly used in many fields, thus it has very limited contributions about the network. Although the collected dataset is helpful, the authors should provide more details about this dataset as well as why and how to build this dataset for the study of coral reef restoration and protection.\n- The experiments are insufficient. The analysis is mainly about how some parameters influence the performance, and the comparison doesn't consider many cutting-edge semantic segmentation methods. Actually, it might be meaningful to dive deep into the real-world applications of this study for coral reef restoration and protection, and it might be better if the authors consider to firstly build a benchmark with state-of-the-art semantic segmentation methods and then design a new method to improve the performance further for meeting the requirements of real-world applications on coral reef restoration and protection."
                },
                "questions": {
                    "value": "This work is still preliminary on semantic segmentation of benthic images, and the authors are suggested to dive deep into the topic of coral reef restoration and protection with cutting-edge artificial intelligence techniques."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3815/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3815/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3815/Reviewer_G8Fd"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3815/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698717043475,
            "cdate": 1698717043475,
            "tmdate": 1699636338750,
            "mdate": 1699636338750,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "Vb3VGKyUUC",
            "forum": "G2xm6yFkl8",
            "replyto": "G2xm6yFkl8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3815/Reviewer_pc9S"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3815/Reviewer_pc9S"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the crucial task of monitoring coral reef health, highlighting the current trade-off between spatial coverage and resolution in existing methods. The authors introduce BenthIQ, a novel multi-label semantic segmentation network that leverages the hierarchical Swin Transformer within a U-shaped encoder-decoder architecture for underwater substrate classification. This fusion allows the model to effectively learn both local and global semantic features. Through a case study in French Polynesia, they convincingly show that BenthIQ surpasses both traditional CNN-based and attention-driven models in pixel-wise classification of shallow reef imagery."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The work has great potential to contribute to oceanography, leveraging ViT. \n2. Good writeup style, in terms of grammar and readability."
                },
                "weaknesses": {
                    "value": "1. Research questions are not specified. It is better to specify it in bullet points, like you did in mentioning contributions. \n2. It is not mentioned how the parameters and/or hyperparameters of different architectures are dealt with.\n3. No limitation is mentioned here."
                },
                "questions": {
                    "value": "1. Table 1,2: The results are not clear to me. I mean I understand the mIOU part, but about Sand, Coral, Algae, Rock, what are the numbers of? Is it accuracy? Or which metric exactly? This is not identifiable at a first glance. Please indicate that and add that in the caption of Table 1. This will be beneficial, especially for new researchers in this field.\n2. The architectures compared in this work are diverse enough and have a lot of different hyperparameters, some which are commonly shared and others which are not. Could you please add more discussions about what steps you took to ensure that comparisons were fair and not biased by something which you can actively avoid?\n3. Which part of your network specializes in this Coral Detection task if it is just a Swin Transformer?\n4. Future Work should not be in the discussion section. You can update the section \u201cConclusion\u201d to \u201cConclusion and Future Work\u201d. \n5. You have written that you implemented \u201cSwin Transformer\u201d as the backbone in abstract, but in the conclusion, it is written as \u201cViT\u201d as the backbone. It is better to specify that you used a variation of ViT in the conclusion."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3815/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3815/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3815/Reviewer_pc9S"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3815/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698811979141,
            "cdate": 1698811979141,
            "tmdate": 1699636338618,
            "mdate": 1699636338618,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]