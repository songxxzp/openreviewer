[
    {
        "title": "Reservoir Transformer at Infinite Horizon: the Lyapunov Time and the Butterfly Effect"
    },
    {
        "review": {
            "id": "eC0LDoOt9N",
            "forum": "f3NLRksLiZ",
            "replyto": "f3NLRksLiZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8012/Reviewer_Md14"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8012/Reviewer_Md14"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript proposes a transformer architecture for chaotic time series prediction, in particular addressing the challenge of supplying very long input sequences by routing the inputs first though an ensemble of (fixed weight) reservoir networks, from which the transformer then takes a lower-dimensional, non-linear readout. Performance comparisons to related models for chaotic time series prediction are shown.\n\nUnfortunately I could not perform a suitably deep review of this manuscript due to time constraints. I apologize to the authors and the area chair. My comments are to be considered as low-confidence opinions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Mostly clear presentation, promising empirical validation results"
                },
                "weaknesses": {
                    "value": "(see questions)"
                },
                "questions": {
                    "value": "- From a theoretical standpoint, it is not obvious to me why using reservoir networks should be more effective than employing other fixed nonlinear feature maps which map from an infinite time-horizon to a finite-dimensional state and might be computationally cheaper. In order to demonstrate that reservoir RNNs are specifically well suited, it would be useful to compare to several other input maps, while keeping the transformer architecture on top the same.\n- Could it be useful to consider also some well-controlled synthetic time-series prediction tasks such as the dynamics of the Mackey-Glass equation or the Kuramoto-Shivashinsky system? In these tasks the Lyapunov time and dimensionality (in the KS case) can be varied in a controlled manner, facilitating a more detailed analysis of the model predictions and performance. \n- Maybe some of the statements are overly strong.. (e.g. \"significant departure from heuristic based assumptions\", \"transformative phase for the transformer model\")"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8012/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697828895590,
            "cdate": 1697828895590,
            "tmdate": 1699636988342,
            "mdate": 1699636988342,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "uoYeuj0zhj",
            "forum": "f3NLRksLiZ",
            "replyto": "f3NLRksLiZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8012/Reviewer_8rjM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8012/Reviewer_8rjM"
            ],
            "content": {
                "summary": {
                    "value": "Authors, present resevoir transformers, which modify the original echo state networks (ESN), to include self-attention in the readout layer, an ensemble of reservoirs and a transformer that takes as input the processed reservoir state $z$ to produce the output $y$. Unlike reservoir transformers (Shen et.al (2020)), they endow existing ESN with transformers, instead of imposing the same training paradigm of ESN on transformers. \n\nTheir method is benchmarked against state-of-the-art transformer architectures for time-series forecasting in both regression and classification settings, where they show superior performance. Additionally, they study their model's performance by predicting the lyapunov exponent which is characteristic of a dynamical system. Finally, LIME is used to explain the readout layer's decision."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Competitive performance against time-series baselines on standard benchmarks\n- Outperforms NLinear on very long sequence forecasting\n- Motivates the use of datasets by assessing their chaotic behavior\n- Used LIME to explain readout layer's decision\n- Good motivation of the resevoir ensemble, as showcased in the sensitivity analysis, in the appendix section"
                },
                "weaknesses": {
                    "value": "- The use of the transformer (and self-attention in general) is not very well motivated :\n   - In section 3.1, you want to replace the linear readout by a transformer stating that for large $m$, the transformer training is slow, yet, in the previous section, you mention that for a linear readout, ESN is optimized using linear regression which is less costly and easier to perform, so why replace it with self-attention? A two-layer MLP could be enough since the self-attention doesn't act on a sequence but a vector in this case\n   - The use of transformer after (that is on $z(t+1)$) is unclear. It is used on which sequence? From the algorithm in the appendix, the transformer is applied on $z(i+1)$ at each timestep $i$, so it's a vector not a sequence of vectors. Unless you mean the transformer is applied on a sequence $z(1), z(2), \\dots, z(i+1)$ in which case, you should clarify that in the paper.\n- Apart from the reservoir ensemble study and related to the above point, no ablation study is performed to justify the changes made to the original ESN. Hence, self-attention and transformer should be ablated separately as well to assess their contributions.\n- You present a method that improves over vanilla ESN, yet, no ESN-derived baselines are included. You should at least compare against a vanilla ESN and the reservoir transformer (which is the closest to your method) (Shen, et.al (2020)).\n-"
                },
                "questions": {
                    "value": "- In eq. 3, there's an $a$, perhaps you meant $\\alpha$?\n- I don't understand on what time-series does the transformer act on ? It takes $z(t+1)$ as input but that's just a single vector.\n- Is the lyapunov exponent given for each point in the timeseries? My original understanding of lyapunov exponents is that it's computed over the whole sequence, so Figure 2 is (as well as the whole LE experiment) is unclear to me."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8012/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8012/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8012/Reviewer_8rjM"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8012/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698683805833,
            "cdate": 1698683805833,
            "tmdate": 1699636988217,
            "mdate": 1699636988217,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "dEBirpeM2X",
            "forum": "f3NLRksLiZ",
            "replyto": "f3NLRksLiZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8012/Reviewer_cN3c"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8012/Reviewer_cN3c"
            ],
            "content": {
                "summary": {
                    "value": "This paper combines Deep Reservoir Computing and transformers for long time series prediction, which can present chaotic behaviors. The method can handle arbitrarily input length sequences. The architecture consists of an ensemble of reservoirs, which readout is non-linear and modeled by attention mechanisms, thus allowing large expressive power. The ensembling technique is used to improve performances but also to limit the impact of sensitivity to initial conditions. The experiments include long-term time series forecasting on various datasets as well chaotic time series forecasting. Additionally, LIME algorithm is used to have some degree of post-processing interpretability."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The method is interesting and novel, combining the expressive power of transformers with the properties of reservoir computing.\n- The model is able to handle efficiently long input length sequences.\n- The experimental results at very-long term forecasting on the ETT dataset are interesting, showing that the model manages to overall capture the trend, even though variations are still not well captured. This shows interesting improvement compared to the NLinear baseline."
                },
                "weaknesses": {
                    "value": "The presentation of the paper could be better for three reasons:\n\n- The message is not always clear. Indeed, studying the butterfly effect would include analysis on the sensitivity to initial conditions and study of completely chaotic systems such as the Lorentz equation. Moreover, one of the main tables of results consists of long-term time series forecasting, not really highlighting the ability to predict chaotic time series. This is interesting to have but is presented as a main argument whereas, in my opinion, it should be less important. This does not help clarifying the message. \n- The details of the model and Deep Reservoir Computing are not always clearly explained, thus leading less experienced readers to go to external resources to really understand the basic concepts.\n- There is plenty of notations problems and typos, which does not help the reading and lower the quality of the presentation. For instance in Eq.3, it should probably be $\\alpha a$ instead of $a$, and $a$ is not defined, notations on the look-back window and the look-back window size are the same, $s$,  the phrase at the end of page 5, a sentence is not finished, etc...\n\nThe experiments are not completely extensive:\n- State-of-the-art transformer for time series forecasting, PatchTST [1], is not included in the related work nor the experimental baselines, where it should be.\n- The size of the look-back window for the experiments is not specified, which is crucial for baseline performances in long-term time series forecasting. \n- More experiments on chaotic time series could be undertaken, since this is the main point of the article. All the baselines could be implemented for Table 3 and other the other datasets used in Table 2 could be used all well for Table 3. Especially since Table 3 presents the same underlying phenomena, whereas the other datasets present other features.\n\n ### Remarks:\nWhen describing the basic Leaky Integrator ESN, [2] should be cited, since this is the main reference for this architecture.\n\nFigure 1 is not very clear. It would probably be beneficial to combine the two ideas and have a single figure showing both the non-linear readout and the ensembling. As in the article, the link between Figure 1a and Figure 1b is not explicit, thus the figure is not self-explained.\n\n\n[1]: Nie, Y., Nguyen, N. H., Sinthong, P., Kalagnanam, J. (2022). A time series is worth 64 words: Long-term forecasting with transformers. arXiv preprint arXiv:2211.14730.\n\n[2]: Jaeger, H., Luko\u0161evi\u010dius, M., Popovici, D., Siewert, U. (2007). Optimization and applications of echo state networks with leaky-integrator neurons. Neural networks, 20(3), 335-352."
                },
                "questions": {
                    "value": "- How did you choose the different model parameters ? If a grid search was performed, it could be interesting to show a few ablation studies to better understand the training of the model and which choices are important for its performances.\n- In Eq. 12, have you tried transforming the output $z$ with something else than a transformer ?\n- Is it possible to perform a meaningful analysis of the significant features that influence model choices when applying LIME interpretation? Or do the features have no meaning or possible interpretation in the ETT case?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8012/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8012/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8012/Reviewer_cN3c"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8012/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698750767458,
            "cdate": 1698750767458,
            "tmdate": 1699636988097,
            "mdate": 1699636988097,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "F4TScJgAaa",
            "forum": "f3NLRksLiZ",
            "replyto": "f3NLRksLiZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8012/Reviewer_rA4y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8012/Reviewer_rA4y"
            ],
            "content": {
                "summary": {
                    "value": "The work proposes a Reservoir Transformer network for time series forecasting specifically targeted towards arbitrarily long input sequences such that it could predict \"chaotic\" time series (i.e with positive Lyapunov exponents). The proposed network achieves this by processing the arbitrarily long input by a reservoir network with nonlinear output implemented by self-attention mechanism. The output of the reservoir network along with current time is passed to a transformer. To increase effectiveness of the approach an ensemble of reservoir networks is trained in parallel and their nonlinear output is then merged. The approach is tested on multiple (10) time series prediction tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The work proposes a novel approach of extracting \"historical\" features from time series by reservoir network which then further can be used to transfer that information to a time-series transformer. This allows the transformer to consider arbitrary long inputs.\n\n2. The approach proposes a novel nonlinear readout in terms of self-attention for the reservoir network. \n\n3. A possibility of merging features from multiple reservoir networks to enhance prediction effectiveness is presented."
                },
                "weaknesses": {
                    "value": "1. The approach accuracy at forecasting is compared with other transformer configurations, however does not compare with other network architectures designed for sequences such as RNNs and their variants (while M-RNNs are listed quantitive comparison is not presented), Graph Neural Networks, etc.\n\n2. The reason for the choice of reservoir computing as feature extractor is not explained and ablations studies with other potential extractors, eg. auto encoders have not been considered. Similarly, the choice of self-attention output vs other projections have not been investigated.\n\n3. Figure 3 and compared models are not well explained in the text."
                },
                "questions": {
                    "value": "1. How would the approach apply to non-chaotic time-series and what would be the accuracy then? In other words, is chaos required for the forecasting?\n\n2. Would the approach apply to solve chaotic differential equations? \n\n3. Force Learning and its extensions (Full Force, R-FORCE) was proposed for optimization of Echo state networks. Would such optimization be applicable for RT?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8012/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8012/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8012/Reviewer_rA4y"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8012/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699595919666,
            "cdate": 1699595919666,
            "tmdate": 1699636987929,
            "mdate": 1699636987929,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]