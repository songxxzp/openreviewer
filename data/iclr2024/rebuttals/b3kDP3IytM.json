[
    {
        "title": "KITAB: Evaluating LLMs on Constraint Satisfaction for Information Retrieval"
    },
    {
        "review": {
            "id": "bOaPeX8mkB",
            "forum": "b3kDP3IytM",
            "replyto": "b3kDP3IytM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1731/Reviewer_dWav"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1731/Reviewer_dWav"
            ],
            "content": {
                "summary": {
                    "value": "This paper develops a dataset and associated dataset construction framework for the evaluation of LLMs on _constraint satisfaction information retrieval_ tasks (eg, \"celebrities who died in 2022\"). This problem is interesting because many it covers many important and useful factual query tasks, but LLMs have significant accuracy difficulties. The authors experiment against existing LLMs (GPT-3.5 and GPT-4) while varying key dimensions of the task, and conduct performance and error analyses on the results."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "A crisp task definition and associated evaluation dataset can be a high-leverage research contribution that enables and accelerates other work, so this is a worthwhile goal. It's also quite promising that the task seems to be intuitively straightforward yet is still challenging for state-of-the-art proprietary models like GPT-4.\n\nThe citations and engagement with IR research going back to 1989, 1999, 2009, etc is good to see and contextualizes the work well. \n\nThe exploration of problem setting parameters (task settings, author popularity, selectivity) is systematic and illuminating. The research questions are clearly defined and answered. The explanations of the dataset development and scoring are clear and thorough. The \"extensible\" nature of the methodology for dataset construction is useful to \"future proof\" the work.\n\nSome of the detailed error analysis was intriguing and suggestive of mechanisms behind the failure modes which is very exciting for future work:\n* ends-with vs starts-with constraints\n* \"sharp phase transition\" for failure rate vs author popularity\n* fabrication for \"list all books by author\""
                },
                "weaknesses": {
                    "value": "For the open datasets (Open Library and WikiData), is there some precautions taken to \"snapshot\" these to a specific point in time for reproducibility purposes? Also, the procedures uses a few proprietary tools to develop the dataset: Azure Cognitive Services NER and Language API, Geonames in the dataset preparation. If the framework code release is open-sourced, future developers could substitute open or more reproducible components in these places, but this means that it would be difficult to create a new version of the dataset \"out of the box\". Likewise for GPT-3.5 and -4: are all the queries and responses captured somehow for reproducibility of the experiments and analysis? Some of these reproducibility concerns potentially undercut the usefulness and extensibility of the framework.\n\nFor controlled experimentation purposes it is useful to focus on a single domain such as book  authorship. However, it seems like there is some risk that the findings or behaviors may not transfer to other constrained IR tasks. How easy or difficult would it be to adapt the framework to generate similar dataset-task pairings in other domains as mentioned in the paper (movies, restaurants, etc)? \n\nCan fine-tuned LLMs do well on KITAB? The results and dataset would be more compelling with another FINE-TUNED setting, even if it had to be done with an OSS model."
                },
                "questions": {
                    "value": "What does KITAB stand for? I couldn't find a definition. \n\nTable 3: why are some numbers red?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1731/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698675713707,
            "cdate": 1698675713707,
            "tmdate": 1699636101962,
            "mdate": 1699636101962,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7gI9mXVYnR",
                "forum": "b3kDP3IytM",
                "replyto": "bOaPeX8mkB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1731/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1731/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dWav: Part 1 of 2"
                    },
                    "comment": {
                        "value": "*[part 1 of 2]*\n\nWe thank the reviewers for their detailed feedback and for finding our work to be a valuable worthwhile contribution. Next, we respond to each of the comments and questions. We\u2019d also be happy to follow up with further clarifications.\n\n**For the open datasets (Open Library and WikiData), is there some precautions taken to \"snapshot\" these to a specific point in time for reproducibility purposes?** - Indeed, for the sake of reproducibility, we follow two key precautions:\n- *Inclusion of Raw Data and Metadata*: In our published dataset, in addition to our clean processed book data associated with each query, we have included the raw snapshot pulled from OpenLibrary. We have also included the metadata (birth year and popularity metric) associated with each author. This is integrated alongside the main KITAB dataset.\n- *Provision of Code Scripts*: We provide code scripts not only for the evaluation of KITAB, but also for every step of the construction of the dataset itself, allowing users to refresh and rebuild the dataset as needed and also generate a different version with a different set of authors if necessary.\n\n**The procedures uses a few proprietary tools to develop the dataset: Azure Cognitive Services NER and Language API, Geonames in the dataset preparation.** - We experimented with a few options to identify an effective and inclusive entity recognition tool and found that Azure Cognitive Services provided the most reliable and comprehensive results for our dataset. However, if users would like to replicate or update KITAB, and would like to adhere to non-proprietary tools, they can easily adapt our published code scripts to replace Azure's API with any open-source NER software they see fit. Additionally, we used Geonames, which is already an open-source tool, making it readily accessible for those who wish to use or adapt our methodology. For reference, you can check out Geonames - All Cities with a population > 1000 \u2014 Opendatasoft. \n\n**Likewise for GPT-3.5 and -4: are all the queries and responses captured somehow for reproducibility of the experiments and analysis? Some of these reproducibility concerns potentially undercut the usefulness and extensibility of the framework.** -\nThis feedback is very insightful; we have opted not to release our model responses along with KITAB, primarily due to the dynamic nature of LLMs, especially GPT models and the inconsistencies that can arise from different endpoints, leading to potential confusion. However, we have thoroughly documented all of our experiments, capturing both the raw and processed outputs from the models. To facilitate research and extend the utility of our work, we plan to include a note in our published dataset, encouraging researchers to contact the authors directly if they wish to access these detailed records. This approach is intended to maintain the integrity and reproducibility of our research while acknowledging the evolving landscape of LLMs."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1731/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700075444037,
                "cdate": 1700075444037,
                "tmdate": 1700075751098,
                "mdate": 1700075751098,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "f8a9Ymj9jF",
                "forum": "b3kDP3IytM",
                "replyto": "HsS2EWXTbI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1731/Reviewer_dWav"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1731/Reviewer_dWav"
                ],
                "content": {
                    "title": {
                        "value": "Author response"
                    },
                    "comment": {
                        "value": "Thank you for the response. The additional steps around reproducibility address some of my concerns on that front.\n\nIt would be nice to put a footnote about the origin/definition of kitab :)"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1731/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700270844637,
                "cdate": 1700270844637,
                "tmdate": 1700270844637,
                "mdate": 1700270844637,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SaCDhHD8rw",
            "forum": "b3kDP3IytM",
            "replyto": "b3kDP3IytM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1731/Reviewer_XGyb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1731/Reviewer_XGyb"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new dataset, KITAB that evaluates how well LLMs  perform in answering constraint satisfaction queries for information retrieval."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Overall I think this new dataset is a well posed and timely work.\n\n* The dataset seems well constructed and reasonable\n* Incorporating constraint satisfaction for LLMs appears to be somewhat understudied, and this dataset will fit nicely into the existing LLM evaluation framework\n* The choice of model evaluation on the data (ChatGPTs) is reasonable\n* The differing frames for the data (with context, without, etc) makes sense\n* The dataset construction is well documented, with design choices explicitly described\n* The metrics and evaluations are well constructed"
                },
                "weaknesses": {
                    "value": "The dataset is somewhat limited, being about books, years and authors. I would prefer if the dataset included a wider portfolio of constraint tasks, perhaps around geography or movies.\n\nIt would also be nice if at least one opensource LLM was used in evaluation (i.e. LLAMA or LLAMA 2).\n\nSome of the constraints seem a little artificial: authors born in a specific year. Some of the constraint questions are out of scope of the design of current LLM's tokenization, \"Book title starts with the letter v\". These constraints don't accurately reflect user interaction with a LLM, I feel."
                },
                "questions": {
                    "value": "Can you enumerate all the constraints? It's not clear from the text what exactly the single and double constraints general architecture is."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1731/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698872312131,
            "cdate": 1698872312131,
            "tmdate": 1699636101879,
            "mdate": 1699636101879,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "H9nhbdM4kT",
                "forum": "b3kDP3IytM",
                "replyto": "SaCDhHD8rw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1731/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1731/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "*[part 1 of 2]*\n\nWe thank the reviewers for their elaborate feedback and for finding our work to be well-constructed and a good fit. Next, we respond to each of the comments and questions. We\u2019d also be happy to follow up with further clarifications.\n\n**The dataset is somewhat limited, being about books, years and authors. I would prefer if the dataset included a wider portfolio of constraint tasks, perhaps around geography or movies.** - As mentioned in the general response, we chose the book domain as a domain that has sufficient public data to work with, but we believe that the problem of LLMs having difficulties in answering such queries is more ubiquitous. Based on research and usage evidence, similar issues have been observed when models are required to answer queries of the same nature in the scientific research domain or encyclopedia-like questions by models fabricating titles of papers that do not exist on a given topic (e.g. https://twitter.com/IttefaqM/status/1715392338535563594?s=20). Most prominently, even the earlier hallucination observed in Bard (see https://www.theverge.com/2023/2/8/23590864/google-ai-chatbot-bard-mistake-error-exoplanet-demo) was related to a similar query that also contained constraints which were not satisfied by the model (i.e., \u201cWhat new discoveries from the James Webb Space Telescope can I tell my 9 year old about?\u201d with the model including a discovery that did *not* actually involve the James Webb Space Telescope). While we hope that more datasets of a similar nature evolve in the community covering other domains, we hope that KITAB opens the path for such work and shows the value of being able to collect and leverage clean and complete data for similar evaluations. Note that one of the main challenges with constructing datasets in a similar vein is related to data cleaning and completeness. Even for domains around movies, geography, celebrities etc., well-known repositories are far from clean and require the integration of several data sources and robust cleaning approaches as we further discuss in the paper and supplementary material.  In addition, we also wanted to provide future users with a way to create more evaluation data dynamically by using public APIs, given rising concerns around data contamination.\n\n**It would also be nice if at least one opensource LLM was used in evaluation (i.e. LLAMA or LLAMA 2)** -  This is a very important point and we very much agree with the reviewer\u2019s observation. Unfortunately, at the time of investigation models in the llama family (including chat versions) did not perform well in terms of following formatting instructions, which resulted in challenges with reliable and scalable evaluation. More recently, and in particular the official deployments by Meta, are more receptive to formatting instructions, which we hope will enable us and future studies to overcome the challenges that we faced initially. At the same time, even with the most recent deployments and releases, we still notice that for this particular task model quality is so low that it is not clear whether it is possible to extract useful quantitative or qualitative insights, in particular when it comes to comparing many different open source models or performance on different constraint types. This concern was valid even for the most popular authors in the dataset, but for the mid-range and less popular authors anecdotally we see that model output may not even have a single satisfactory item. We will include a discussion on this very important topic in the paper and hope to do more of these tasks in the future for either more simplified versions of our dataset or for future open source model releases that perhaps show stronger emerging capabilities."
                    },
                    "title": {
                        "value": "Response to Reviewer XGyb: Part 1 of 2"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1731/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700075111427,
                "cdate": 1700075111427,
                "tmdate": 1700075195618,
                "mdate": 1700075195618,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "R1d4XUXsc8",
                "forum": "b3kDP3IytM",
                "replyto": "SaCDhHD8rw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1731/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1731/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer XGyb: Part 2 of 2"
                    },
                    "comment": {
                        "value": "*[part 2 of 2]*\n\n**Some of the constraints seem a little artificial: authors born in a specific year. Some of the constraint questions are out of scope of the design of current LLM's tokenization, \"Book title starts with the letter v\". These constraints don't accurately reflect user interaction with a LLM, I feel.** - Your observation about certain constraints seeming artificial, like specifying authors' birth years and questions about book titles starting with a specific letter, is indeed insightful. The reasoning we\u2019ve arrived at behind these constraints, however, comes from a practical standpoint.\nAs multiple authors could share the same name, we have chosen to add the birth year in our prompts to help in distinguishing them. This approach is aimed at reducing confusion and improving the accuracy of responses, especially in cases where authors might be less well-known or have common names. We indeed saw this issue happening in practice and merely wanted to add more information to help the model and provide a better chance for it to be correct as it did in fact help for several examples that we investigated manually.\nRegarding constraints such as book titles starting with a specific letter, while it is indeed valid that these type of tokenization-dependent tasks might be out of scope for the LLM's architecture, we have decided to include them to assess the model's performance under various scenarios, keeping in mind that average users may not be aware of the technical limitations of language models and might still trust or expect accurate responses from the model in such cases. Another consideration is that currently users are building applications and multi-agent systems on top of existing LLMs and they are using them for tasks that traditionally we had not anticipated them for, even searching with a given string (or letter). While current LLMs do not perform well on such tasks, even if they are out-of-scope for their design, it is probably still a good idea to quantify and increase awareness as results remain unreliable.\n\n **Can you enumerate all the constraints? It's not clear from the text what exactly the single and double constraints general architecture is** - Certainly, we used two main types of constraints in our study:\n- Author Constraints: This specifies the author's name and their birth year. For example, \"books by Gabriel Garc\u00eda M\u00e1rquez (born 1927)\".\n- Book Constraints: These are of six different types (mentioned in Table 2) to test for lexical, entity-based, and temporal aspects. An example is \"Book contains a human name in its title\". Please refer to Table 2 for details on the distribution of book constraints of different types across KITAB queries containing one and two book constraints (in addition to an author constraint), as well as Figures 5 and 6 for a visualization of the latter across author popularity and constrainedness.\n\nIn our experiments, for every query, we use one author constraint and a varied number of book constraints from zero to two. We then evaluate how well the models met all these constraints per query through a variety of metrics described in detail in section 4.1."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1731/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700075258502,
                "cdate": 1700075258502,
                "tmdate": 1700077219597,
                "mdate": 1700077219597,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TMf6AeI5Iy",
            "forum": "b3kDP3IytM",
            "replyto": "b3kDP3IytM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1731/Reviewer_ZHUe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1731/Reviewer_ZHUe"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes KITAB, a dataset for evaluating how large language models perform on information retrieval tasks with constraint satisfaction. The author(s) describe the dataset constructions and settings in detail. Evaluated on the proposed dataset, observations from different perspectives indicate that KITAB is still a challenging task for LLMs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Timely study of LLMs on constraint satisfaction for information retrieval.\n2. The overall paper is well-written and easy to follow.\n3. Detailed descriptions of the data construction are provided.\n4. The author(s) also promise to release the dataset, as well as code to the community.\n5. The author(s) comprehensively analyzed the results to draw several interesting observations."
                },
                "weaknesses": {
                    "value": "1. Concerns of the submission being out-of-scope. The paper undoubtedly stands out as a great resource that makes contributions through its new dataset and the accompanying empirical studies. It certainly serves as a valuable asset for the research community. However, I'm concerned about its suitability as a full paper for ICLR, given the conference's typical focus.\n\n2. The author(s) have indeed conducted a thorough consideration of multiple facets in constructing the dataset, such as the number of constraints, the variety of constraints, and unsatisfiability. Nevertheless, the scope of the proposed dataset is fundamentally narrow, being confined to the domain of books. This domain-specific focus could limit its applicability to a broader range of IR research. It could be better if data from a variety of domains can be included.\n\n3. The evaluations presented are primarily focused on the LLM services provided by OpenAI. While these are undoubtedly among the leading services in the field, the inclusion of open-source LLMs, such as LLaMA 2 chat, in the evaluation process could provide a more comprehensive view of the landscape."
                },
                "questions": {
                    "value": "Please refer to \"Weaknesses\"."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1731/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1731/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1731/Reviewer_ZHUe"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1731/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699083594479,
            "cdate": 1699083594479,
            "tmdate": 1699636101805,
            "mdate": 1699636101805,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BYbsAOzg0t",
                "forum": "b3kDP3IytM",
                "replyto": "TMf6AeI5Iy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1731/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1731/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer ZHUe"
                    },
                    "comment": {
                        "value": "We thank the reviewer for finding our study timely, comprehensive, and well presented!  Next, we respond to each of the comments and questions. We\u2019d also be happy to follow up with further clarifications.\n\n**Venue fit. Concerns of the submission being out-of-scope.** - It would be great if the reviewer could further clarify their concerns on scope. If the concern is related to the fact that the dataset focuses on factual IR queries rather than learning problems, it is worth noting that even though IR queries traditionally have not been considered in the ML scope, given the increasing abilities and most importantly usage of LLMs to answer these queries, it is important to share these results and artifacts with the ML community through this ICLR track focused on benchmarks and datasets. Indeed, pure LLM or LLM-based solutions are currently being used worldwide to answer exactly these types of queries, even more so in the retrieval augmented generation setting (BingChat and ChatGPT more recently), which we simulate in one of our experimental conditions. \n\n**Limited Scope. \u2026 The scope of the proposed dataset is fundamentally narrow, being confined to the domain of books.** - As mentioned in the general response, we chose the book domain as a domain that has sufficient public data to work with, but we believe that the problem of LLMs having difficulties in answering such queries is more ubiquitous. Based on research and usage evidence, similar issues have been observed when models are required to answer queries of the same nature in the scientific research domain or encyclopedia-like questions by models fabricating titles of papers that do not exist on a given topic (e.g. https://twitter.com/IttefaqM/status/1715392338535563594?s=20). Most prominently, even the earlier hallucination observed in Bard (see https://www.theverge.com/2023/2/8/23590864/google-ai-chatbot-bard-mistake-error-exoplanet-demo) was related to a similar query that also contained constraints which were not satisfied by the model (i.e., \u201cWhat new discoveries from the James Webb Space Telescope can I tell my 9 year old about?\u201d with the model including a discovery that did *not* actually involve the James Webb Space Telescope). While we hope that more datasets of a similar nature evolve in the community, we hope that KITAB paves the path for such work and shows the value of being able to collect and leverage clean and complete data for similar evaluations. Note that one of the main challenges with constructing datasets in a similar vein is related to data cleaning and completeness. Even for domains like movies, celebrities etc., well-known repositories are far from clean and require the integration of several data sources and robust cleaning approaches as we further discuss in the paper and supplementary material. In addition, we also wanted to provide future users with a way to create more evaluation data dynamically by using public APIs, given rising concerns around data contamination.\n\n**Inclusion of Open Source Models** - This is a very important point and we very much agree with the reviewer\u2019s observation. Unfortunately, at the time of investigation models in the llama family (including chat versions) did not perform well in terms of following formatting instructions, which resulted in challenges with reliable and scalable evaluation. More recently, and in particular the official deployments by Meta, are more receptive to formatting instructions, which we hope will enable us and future studies to overcome the challenges that we faced initially. At the same time, even with the most recent deployments and releases, we still notice that for this particular task model quality is so low that it is not clear whether it is possible to extract useful quantitative or qualitative insights, in particular when it comes to comparing many different open source models or performance on different constraint types. This concern was valid even for the most popular authors in the dataset, but for the mid-range and less popular authors anecdotally we see that model output may not even have a single satisfactory item. We will include a discussion on this very important topic in the paper and hope to do more of these tasks in the future for either more simplified versions of our dataset or for future open source model releases that perhaps show stronger emerging capabilities."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1731/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700074905572,
                "cdate": 1700074905572,
                "tmdate": 1700074905572,
                "mdate": 1700074905572,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "g2qmYTTNqS",
                "forum": "b3kDP3IytM",
                "replyto": "BYbsAOzg0t",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1731/Reviewer_ZHUe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1731/Reviewer_ZHUe"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comprehensive and thoughtful responses to my concerns. I appreciate your time and effort. Overall, I will maintain my rating.\n\nFor the weakness \"Venue fit. Concerns of the submission being out-of-scope.\", thank you for your detailed response clarifying the scope and relevance of your submission to ICLR. I realize now that the ICLR this year includes a specific track for benchmarks and datasets, which indeed makes your paper a suitable fit for the conference. I apologize for my oversight in this regard."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1731/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641338993,
                "cdate": 1700641338993,
                "tmdate": 1700641338993,
                "mdate": 1700641338993,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mBtigrMBzl",
            "forum": "b3kDP3IytM",
            "replyto": "b3kDP3IytM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1731/Reviewer_pbw5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1731/Reviewer_pbw5"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a dataset KITAB for testing on how LLMs can satisfy constraints in IR. A set of books are collected. Constraints can be set on authors, titles, dates, etc. and can be combined. The evaluation of the results by GPT-3.5 and GPT-4 are evaluated in terms of irrelevance, completeness, etc. Three test conditions are set using different prompts: no-context, self-context and context, providing (or not) the list of books written by an author.\nThe paper examines how GPT-3.5 and GPT-4 perform on different queries. A wide range of experimental results are reported."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The problem examined - constraint satisfaction - is not a widely studied problem. To some extent, this investigation extends the traditional IR to a special type of IR.\nThe data collection and experimental setting are well described.\nThe results may reveal that GPT models are not able to process this type of queries very well, and in particular, those with some constraints.\nThe authors will release the dataset."
                },
                "weaknesses": {
                    "value": "The targeted problem is very special. The authors consider it as an IR problem. It resemble more database querying problem. Indeed, once the book references are transformed into structured database, the problem may be much simpler. Howere, as I understand, the intent of the author is not to examine how GPT models can handle such a search problem, but try to use this to test the ability of LLMs to handle queries containing constraints. While I appreciate this effort, I still question on the appropriateness of thFor e task. Instead of expression general constraints in a language, the constraints examined are very specific, and sometimes special. For example, constraints asking a title to end with a letter `v', or titles to be of 4 words, seem very particular. It is expected that a general LLM may not perform the task well because they have not been trained for the task. The main concern about the work is whether the types of constraints used can really test the ability of LLMs to satisfy constraints, or if the tests are on LLMs ability to understand the constraints and to execute filterings of books accordingly in this very special case. Even if LLMs are able to do the job well, would one be able to draw interesting conclusions? The experiments shown in the paper may not allow to draw general conclusions on LLM's ability to satisfy constraints.\nGiven the very special characteristics of the dataset, I question about the value of the KITAB dataset for the purpose of examining LLM's capability of constraint satisfaction in search.\nWhile the ground truth is determined correctly, there may be some problem in the measure of irrelevance and completeness, as some fuzzy matches are allows. As the authors admit, there may be some overestimation of the performance. So the experimental results are only indicative, with some possible rate of errors."
                },
                "questions": {
                    "value": "For the estimation of irrelevance or partial satisfaction, one case of match is when one is a string subset of another: For each ki, we check if there exists a book in the ground truth set of books by that author which is either a string subset match for ki (in both directions),... Do you also apply a threshold on the percentage of the substring, or a substring of any length is accepted?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1731/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699421819962,
            "cdate": 1699421819962,
            "tmdate": 1699636101736,
            "mdate": 1699636101736,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ToBgyNTw2x",
                "forum": "b3kDP3IytM",
                "replyto": "mBtigrMBzl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1731/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1731/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer pbw5"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback and for appreciating our contributions! Next, we respond to each of the comments and questions. We\u2019d also be happy to follow up with further clarifications.\n\n**Generality of the dataset. Can the presented constraint types really test the ability of LLMs to satisfy constraints?** - We consider 6 different constraint types to test for *lexical*, *entity-based*, and *temporal* constraints. Indeed, the pretrained models are not directly trained to satisfy these constraints, but that is the case for many emergent abilities of interest. Yet, we\u2019d like to understand and measure model performance on these abilities given that current models do show non-trivial (albeit non satisfactory) performance in such tasks. \n\nEven amongst the constraint types we consider, models may have received direct or indirect training signals. For example, entity recognition (human/city name) is a task that has been largely studied in NLP and may therefore have more explicit training data used for pretraining, while the model\u2019s ability to apply string operations is less studied and perhaps requires different skills. This is why we disaggregate our analysis and show how performance varies across different constraints. While it is true that some of these constraints can be easily satisfied through SQL, our goal here is to test the model\u2019s direct linguistic abilities as at the application-level not all user queries are or can be rewritten as SQL queries. Some of the constraint types we consider, can actually be mapped to concrete *librarian tasks* such as retrieving an author\u2019s work (List all books by an author), indexing or searching by a given constraint (List all books by an author whose title starts with a letter), finding characters mentioned in the title (or content in more advanced versions) etc. While our goal isn\u2019t to completely solve the library domain, we use this domain as an illustration of a much larger problem in factual inaccuracy when models are not able to follow constraints. \n\n**Evaluation procedure. While the ground truth is determined correctly, there may be some problem in the measure of irrelevance and completeness, as some fuzzy matches are allowed.** - Thanks for raising this concern, it is indeed important for the measurement to be realistic. We view performance on KITAB as an \u201cupper bound\u201d on the LLM\u2019s actual ability for these types of simple information retrieval tasks. Our main qualitative result is that existing LLMs already struggle to obtain good performance on KITAB, and therefore, this casts doubt on their ability to perform these sorts of constraint satisfaction tasks in general settings. This is why we are willing to overestimate model performance; indeed, our main result demonstrates that even with all of these relaxations, state-of-the art LLMs still struggle immensely with this elementary (but important) task.\n\nIn addition, relaxing the evaluation process is crucial for testing constraint satisfaction abilities in the language domain in a realistic manner without undermining model capabilities. For example, the evaluation should not over penalize the model if the model is asked to output a title that starts with the letter \u201cc\u201d and includes the title \u201cThe Citadel\u201d in its results. Indeed, librarian practices do include such titles as this categorization is most useful for users. Similarly, if the model outputs the following title \u201cG\u00f6del, Escher, Bach\u201d as a book by Douglas R. Hofstadter but does not mention its full title (\u201cG\u00f6del, Escher, Bach: An Eternal Golden Braid\u201d) this should be considered a correct answer as the book is known with both titles. Similar reasoning goes to accounting for potential typos in the model\u2019s answer or in our data by using fuzzy matching. In fact, we consider the evaluation process together with the appropriate implemented relaxation process as a practical contribution of the paper and will open source the artifacts to support reproducibility to this end.\n\nAs mentioned in the paper (Section 3.1 - Book collection) and supplementary material (C.1), we also run a manual annotation to separately evaluate how irrelevance and completeness is impacted by perhaps books that we were not able to retrieve with the sources used (OpenLibrary and WikiData) and find that only 5% of the GPT4 answers contain at least one book that exists on the internet from the same author but is not captured from KITAB. The impact of this on actual irrelevance and completeness is a lot lower because the number of non-found books per query is usually less than 2 while the satisfactory set is larger.\n\n**Do you also apply a threshold on the percentage of the substring, or a substring of any length is accepted?** - Let\u2019s take the example of 2 possible titles for the same book t1= \u201cG\u00f6del, Escher, Bach: An Eternal Golden Braid\u201d and t2= \u201cG\u00f6del, Escher, Bach\u201d. We would consider this a match if t2 \\in t1 or if t1 \\in t2. Therefore, we only consider full substrings as a match."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1731/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700073015224,
                "cdate": 1700073015224,
                "tmdate": 1700077133548,
                "mdate": 1700077133548,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "F8fSFyKKIW",
                "forum": "b3kDP3IytM",
                "replyto": "ToBgyNTw2x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1731/Reviewer_pbw5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1731/Reviewer_pbw5"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the explanation and answer.\n\nThe task still seems very special, which may not be well suited for testing the general ability of LLM for constraint satisfaction.\nThe task is more suitable for SQL-like querying system than a LLM.\n\nDespite the fact that 6 types of constraints are considered, they are very specific for the task. Therefore, their representativity of the general constraints is limited."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1731/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700614204459,
                "cdate": 1700614204459,
                "tmdate": 1700614204459,
                "mdate": 1700614204459,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]