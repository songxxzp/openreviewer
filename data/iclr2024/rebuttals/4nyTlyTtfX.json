[
    {
        "title": "Heterogeneous Decision Making towards Mixed Autonomy: When Uncertainty-aware Planning Meets Bounded Rationality"
    },
    {
        "review": {
            "id": "gjWuWqvsuk",
            "forum": "4nyTlyTtfX",
            "replyto": "4nyTlyTtfX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8909/Reviewer_Sgxf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8909/Reviewer_Sgxf"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers heterogeneous decision-making by human-driven and automated vehicles in 2 player game scenarios. The paper introduces two main lines of inquiry. The first hypothesis (H1) is understanding the relationship between learning performance and HV\u2019s bounded rationality and AV\u2019s planning horizon. The second hypothesis (H2) relates the impact of decision\u2013making strategies on the overall learning performance. To evaluate H1, the paper provides a theoretical analysis through a derivation of the upper bound of the regret. To evaluate H2, the authors provide an empirical ablation over hyperparameters."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The paper provides a formalization of a problem that has not had extensive theoretical analysis.\n+ The introduction shows this has a useful application despite being a smaller setup (2-player).\n+ There is a clear description of the method assumptions and limitations throughout the paper."
                },
                "weaknesses": {
                    "value": "-The main argument of the paper assumes that identifying and formalizing this problem is a significant contribution. \n\n-Since there are only two agents, there may be methods outside of autonomous driving that may be relevant. [1] may be a good starting point to find such literature.\n[1] Natarajan, M., Seraj, E., Altundas, B., Paleja, R., Ye, S., Chen, L., ... & Gombolay, M. (2023). Human-Robot Teaming: Grand Challenges. Current Robotics Reports, 1-20.\n\n-There are many bounds proved, but it is unclear what the intuition is. Is this bound good? Could it be better? The paper is quite difficult to understand the takeaways from these bounds. This weakness would no longer be an issue after clarifying the writing."
                },
                "questions": {
                    "value": "How should these theoretical results be interpreted? It would be beneficial to the reader to add some intuition regarding whether this bound is tight enough for performance.\n\nWhat is the main takeaway from the empirical performance? There exists a set of parameters for a setting to minimize the regret, but then how should a practitioner use these results? Can they be extended beyond a 2-player setting or do these hold empirically with humans? Many of these results ask more questions (due to the nature of a paper formalizing a problem). Thus, I would expect much of the paper to discuss the implications of these results, which it is currently lacking.\n\nSome minor points:\nThe metaphor in line 1 was a bit confusing.\n\nMany of the abbreviations in the introduction should be introduced before using the abbreviated form. What is \u201cNHTSA\u201d, \u201cHV\u201d (only defined in abstract, may do well to just repeat once in the introduction such as Automated Vehicle (AV).\n\nIn Section 5, line 2, I believe \u201cAv\u201d should be \u201cAV\u201d\n\nPlease link to the proofs in the appendix from the main document."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8909/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8909/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8909/Reviewer_Sgxf"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8909/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698700367810,
            "cdate": 1698700367810,
            "tmdate": 1699637121126,
            "mdate": 1699637121126,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sOkdZNGiuC",
                "forum": "4nyTlyTtfX",
                "replyto": "gjWuWqvsuk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Sgxf (1/2)"
                    },
                    "comment": {
                        "value": "We thank reviewer Sgxf for your thorough review and useful feedback. We address your concerns as follows. \n\n**(Weakness)**\n\n**W1. Significance - Why our problem formulation is different from others?**\n\n**A1.** In this work, we aim to understand the complicated interactions between human and machine decision-making and develop a *fundamental understanding* of the impact of *different decision making strategies* on the system efficiency (in terms of dynamic regret). To this end, our problem formulation considers  different learning approaches  for each agent's decision making. Specifically, we compare the major differences with other relevant problem formulations, e.g., Dec-POMDP, as follows.\n\n\n- Many MARL formulations for  Dec-POMDP, often assume all agents   use the *same* RL algorithms to 'maximize' the rewards. Our work aims to study the case where AV and HV  use different learning methods, i.e., longer-horizon look-ahead planning and myopic decision making, to achieve their objectives. \n- Further,  in our setting, HV makes decision  with bounded rationality at each time step, which *deviates from reward maximization*. \n- (Theoretical Results) In our setting, we study the regret dynamics of the system (cf. $\\mathcal{R}\\_{A-H}$  Section 4) such that the impact of different learning strategies on the system performance is characterized. Specifically, we show how HV's bounded rationality, AV's planning horizon and function approximation error have impact on the overall system dynamics. We remark this is different from the analysis in MARL formulation, where often times the objective is to characterize the Nash Equilibrium. Thus, the analysis method used in our work is very different from previous MARL formulations. \n\n\n**W2. Relevant works on two agents interaction**\n\n**A2.** We thank the reviewer for pointing out the very interesting paper on Human-Robot Teaming [R1]. In what follows, we compare our work with [R1] and the reference therein  and outline the new ideas beyond the literature.\n\n\n- (Bounded Rationality of HV's Decision Making.) In the same spirit as in [85-86, 104-108], we consider the human's decision making to be sub-optimal (cf. Section 3.2). More importantly, we study the impact of different bounded rationality levels on the overall system performance (cf. Figure 3(c)). \n- (Theoretical Study on HV-AV Interaction.) Despite the rich empirical results [89,154],   the theoretical analysis on the interaction between AV and HV is still lacking, especially considering their different decision making.   Our work aim to obtain a fundamental understanding of the heterogeneous decision making in the interplay, especially the impact of HVs' decision making with bounded rationality on AVs' performance, which is crucial for achieving efficient mixed autonomy. \n- (Problem Formulation) Our problem formulation considers AV's modeling of HV's behaviors [86-90] (ref.~Equation (2)). Meanwhile, in our work, we use regret dynamics as a metric to \"evaluate the human experience\" [298-301], i.e., we show how does HV's learning strategies have impact on the overall learning performance by characterizing the system regret $\\mathcal{R}\\_{A-H}$ (ref. Section 4).\n\n[R1] Natarajan et al., \"Human-Robot Teaming: Grand Challenges\". Current Robotics Reports, 1-20. (2023)\n\n**W3. What is the intuition of the bounds? Is this bound good? Could it be better?**\n\n**A3.** To improve the presentation, we reorganize the description and analysis of our theoretical results in our revision; and the changes are marked in blue. We summarize the main changes as follows. We provide the intuition of these bounds in our answer to Question 1 below.\n\n- (Lemma 1 and Lemma 2.) We clarify that the derived performance gaps are tight.\n\n- (Theorem 3.) We provide a more detailed explanation of each parameter in the upper bound of the regret and clarify the bound is tight.\n\n- (Theorem 4.) We add the analysis of the upper bound for HV's regret and clarify the bound is tight.\n- (Corollary 5) We provide a more detailed discussion on the coupling between AV's decision making and HV's decision making."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700073445858,
                "cdate": 1700073445858,
                "tmdate": 1700073445858,
                "mdate": 1700073445858,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YCalYjb2Q3",
                "forum": "4nyTlyTtfX",
                "replyto": "gjWuWqvsuk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Sgxf (2/2)"
                    },
                    "comment": {
                        "value": "**(Questions)**\n\n**Q1. How to interpret those theoretical results?**\n\n**A1.** We clarify \"the intuition regarding whether this bound is tight enough for performance\" as follows.\n\n- (Lemma 1 and Lemma 2.) Both bounds on the performance gap are tight. It can be seen that the upper bound equal to zero when we set $\\mu\\_A = \\sigma_A=0$ (such that AV's prediction on HV's action is accurate), the upper bound is only related to the function approximation error term $\\gamma^L\\mu\\_{v,t}$. Furthermore, by setting $L \\to \\infty $ (such that the Q-function does not rely on the terminal cost approximation), the upper bound equal to zero which indicating there is no performance gap. \n- (Theorem 3.) We clarify the upper bound for AV's regret is tight, i.e., by setting $\\mu_A = \\sigma\\_A=0$, the residual term $(\\gamma^L/T) \\Gamma \\mu\\_{v,0}$ is only related to the function approximation error, which will be close to zero as we increase $L$. \n- (Theorem 4.) We clarify the bound is tight since the upper bound will be zero once we set $\\mu\\_H = \\sigma\\_H=0$. Intuitively, HV's regret is solely due to its bounded rationality.\n- (Corollary 5) The upper bound is tight. We first set $\\mu\\_A = \\sigma\\_A=\\Psi\\_v = 0$, then the residual terms are only relevant to  HV's bounded rationality $\\Psi\\_H,\\Xi\\_H$. Now let $\\mu\\_H = \\sigma\\_H=0$, we obtain the zero regret of the whole system. \n\n\n\n**Q2. What is the main takeaway from the empirical performance? How should a practitioner use these results?**\n\n**A2.** In what follows, we summarize the main takeaway, as supported by the empirical experiments illustrated in Figures 1-4.\n\n- (**How to adjust look-ahead length** $L$?). In Figure 1 and Figure 2(a), we show the Goodhart's Law in AV's learning performance and it indicates that  increasing the planning horizon will initially help   the learning performance until a critical point.  In practice, we remark that adjusting the look-ahead length (e.g., through grid search) is essential to help AV achieve the desired performance.  \n- (**How to choose discounting Factor** for AV?) In Figure 2(b), we show that the discounting factor should be set in a way that considers the function approximation error. For instance, if the function approximation error is more dominant than the prediction error, the small discounting factor tends to help to reduce the regret. \n- (**Priority of training**: function approximation v.s. prediction model?) In Figure 3(a-b), we show the impact of different function approximation error ($\\mu\\_{v,0}$) and prediction error ($\\mu\\_A$) on the system regret. It can be seen that by reducing the prediction error from 0.4 to 0.2, the regret have significant change from 4000 to 1800 (-$30\\%$). While reducing the function approximation error from 100 to 50, the regret changes from 51.41 to 51.38 (-$0.06\\%$). The empirical results indicate that optimizing over prediction model tends to help us get more improvement on regret. \n\n\n**Q3. Extend to more than two-player settings? Or do these hold empirically with humans?**\n\n**A3.** We remark that it is feasible to extend to more than one AV and one HV setting and  we share some preliminary thoughts as follows. Assume there are $N\\_H$ HVs and $N\\_A$ AVs in the mixed traffic system. With abuse of notations, we define the action vector for AVs and HVs as follows, at time step $t$,\n$$u_H(t) = [u_{H,1}(t), u_{H,2}(t),\\cdots, u_{H,N_H}(t)]$$\n\n$$u_A(t) = [u_{A,1}(t), u_{A,2}(t),\\cdots, u_{A,N_A}(t)]$$\n\n\nBy defining the prediction error as in Equation (4) and HVs' bounded rationality as in Section 3.2, our analysis framework still can be applied. We remark that the dimenion of the approximation error term and the bounded rationality term is thus $N\\_A$ and $N\\_H$ times higher than the two-agent case. Hence, the resulting regret in Theorem 3 and Theorem 4 are $N\\_A$ and $N\\_H$ times higher than the two-agent case. \n\n\n**Q4. Minor points: metaphor in line 1 was confusing? Abbreviations issue. Link to the appendix.**\n\n**A4.** We thank the reviewer to point out those minor issues and our revision has changed in the following ways,\n\n- We add reference for the metaphor in line 1.\n- We correct the abbreviation issues in the paper.\n- We add the link to the appendix in the main paper."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700073500641,
                "cdate": 1700073500641,
                "tmdate": 1700073500641,
                "mdate": 1700073500641,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KxDsWAntxj",
                "forum": "4nyTlyTtfX",
                "replyto": "gjWuWqvsuk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank Reviewer Sgxf for your review"
                    },
                    "comment": {
                        "value": "Thank you Reviewer Sgxf again for your detailed review. \n\nSince the final stage of the discussion between reviewers and authors will end soon, please let us know if you have any further comments on our response to your concerns, we will be more than happy to answer your questions."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700345483399,
                "cdate": 1700345483399,
                "tmdate": 1700345601967,
                "mdate": 1700345601967,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5yNYVG6yQm",
                "forum": "4nyTlyTtfX",
                "replyto": "TUlixIa8kN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8909/Reviewer_Sgxf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8909/Reviewer_Sgxf"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the detailed responses. While I disagree regarding the 2-agent vs 3+ agent significance, I make my decision based on the clarity of the 2-agent case. Despite substantial effort on the author's behalf, I find the overall paper writing still very confusing. I agree with reviewer dAQk's claims about clarity and will keep my score as is."
                    }
                },
                "number": 29,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700676647086,
                "cdate": 1700676647086,
                "tmdate": 1700676647086,
                "mdate": 1700676647086,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kReBZXJqg0",
            "forum": "4nyTlyTtfX",
            "replyto": "4nyTlyTtfX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8909/Reviewer_DRuU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8909/Reviewer_DRuU"
            ],
            "content": {
                "summary": {
                    "value": "The paper delves into the challenges and advancements associated with mixed autonomy in traffic scenarios, where both automated vehicles (AVs) and human-driven vehicles (HVs) coexist. \nThe primary objective of the study is to gain insights into the interactions between HVs and AVs by framing these interactions as a process wherein AVs predict the decisions made by HVs. \nRegret analysis serves as a metric for evaluating the learning performance of both systems, and the authors conduct a comprehensive analysis to unveil how the overall system performance is influenced by HVs' bounded rationality and AVs' planning horizon."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The reviewer has limited familiarity with the field of autonomous driving research. Based on the reviewer's best judgment: \n\n- The paper offers a novel perspective on mixed autonomy, emphasizing the diverse decision-making processes involving both human and machine behaviors. \n\n- The paper's introduction and problem formulation are well-defined and effectively presented. The paper's structure is logical and facilitates easy comprehension. \n\n- The formulation of interactions between human-driven vehicles (HVs) and automated vehicles (AVs) appears innovative and of significant relevance. The regret analysis applied to both systems is robust, and it is intriguing to observe how the results reveal the overall system performance's dependence on HVs' bounded rationality and AVs' planning horizon."
                },
                "weaknesses": {
                    "value": "The study relies on a set of assumptions such as: \n\n- Both HVs and AVs operate within the same action space;\n\n- HVs plan actions over a shorter time horizon, whereas AVs adopt a longer horizon for decision-making; \n\nThe practicality of these assumptions raises questions. \n\nThe formulation of HV-AV interactions leans on Goodhart\u2019s law to evaluate AV performance and incorporates a model wherein HVs react to AVs based on bounded rationality. At this stage, I am unable to comment on the robustness of such a formulation. \n\nSome comparative experiments and analysis would strengthen the paper's contributions.  \n\nI will refine my viewpoint upon reviewing the feedback from other reviewers. \n\n\nMinor: \n\nPlease use the ICLR2024 template."
                },
                "questions": {
                    "value": "Could the authors elaborate on the foundational assumptions made for the formulation presented in the paper\uff1f"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8909/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8909/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8909/Reviewer_DRuU"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8909/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698799872794,
            "cdate": 1698799872794,
            "tmdate": 1699637120987,
            "mdate": 1699637120987,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Wd1mLjxhCP",
                "forum": "4nyTlyTtfX",
                "replyto": "kReBZXJqg0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DRuU (1/2)"
                    },
                    "comment": {
                        "value": "We thank reviewer DRuU for your thorough review and positive comments. We address your concerns as follows. \n\n**(Weaknesses)**\n\n**W1. The practicality of the assumptions.**\n\n**A1.** We clarify a few points in  our study as follows.\n\n- (Action Space for HV and AV.) We clarify that we do not assume both vehicles have the same action space. As can be seen in Section 2, we denote the action space for AV and HV to be $\\mathcal{U}\\_A$ and  $\\mathcal{U}\\_H$, respectively. We do assume that both vehicles' action space have the same cardinality. Intuitively, though different vehicles can drive forward, backward, parking (the cardinality of the action space are both 3), the corresponding actions (Action space) can be different, e.g., the wheel angle, forces on the brake, etc. \n- (What is the intuition of \"HV plans actions over a shorter time horizon, whereas AVs adopt a longer horizon for decision-making\"? ) We clarify that the difference between human decision making and  AVs decision making stems from the inherent differences such as cognition systems and learning/computing capabilities. Our problem formulation of human decision making is related to the studies in psychology and  behavior theory [R1,R2,R3], where the main discovery on human's decision making process is that \"human individuals have constraints in both their understanding of their surroundings and their computational capacities\" (ref. Paragraph 2 in Introduction and Section 2.1). To this end, we model human as a \"myopic\" decision maker by only plan over $N$-step time horizon. We remark that we do not have constrains on the maximum value of $N$ in our analysis. Meanwhile, the AVs are generally equipped with sensing, storage and computing system on board such that AVs are able to handle complex optimization problem in the real-time, e.g., combing the information from camera, radar to make prediction of the other vehicles' future actions. The advantage of the computation capability can also help with long-term planning. For example, by taking advantage of the traffic information (e.g., interactive maps with congestion prediction, road closure, etc.), AVs are able to plan a longer trajectory than HVs, who generally are making the decision with limited local information [R4]. \n\n[R1] Herbert A Simon. Models of man; social and rational. 1957.\n\n[R2] Herbert A Simon. Rational decision making in business organizations. The American economic review, 69(4):493\u2013513, 1979.\n\n[R3] Daniel Kahneman. Maps of bounded rationality: Psychology for behavioral economics. American economic review, 93(5):1449\u20131475, 2003.\n\n[R4] Song et al. \"Pip: Planning-informed trajectory prediction for autonomous driving.\" ECCV, 2020."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700073329724,
                "cdate": 1700073329724,
                "tmdate": 1700073329724,
                "mdate": 1700073329724,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sLfKnzWJ6n",
                "forum": "4nyTlyTtfX",
                "replyto": "kReBZXJqg0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DRuU(2/2)"
                    },
                    "comment": {
                        "value": "**W2. Robustness of the problem formulation?**\n\n**A2.** We thank the reviewer's interest in the robustness of our problem formulation. We clarify that our problem formulation is general and considers the practical interaction between HVs and AVs.\n\n- (**Adaptive Interaction between HVs and AVs.**) In this work,  we consider both AV and HV adapt their policies during the interaction. More specifically, as shown in Section 2.1 Equation (2) and Equation (3) (and the paragraph below), both AV and HV's policy is conditioned on each other and will update its policy when the other agent has changed its policy. We remark that this formulation corroborates with the real-world human-autonomous vehicle interaction. \n- (**General Formulation for AV and HV's Decision Making** (cf. Equation (2), Equation (3)).) In Equation (2), we consider AV's decision making can be either model free (by setting $L=1$) or model-based (by setting $L>1$). Meanwhile, we do not constrain the method on deriving the policy $\\pi$ such that HV's learning method can be either $Q-$learning (greedy policy) or Actor-Critic (by using policy gradient). Meanwhile, in Equation (3), we consider AV's decision making to be $N$-step planning while we do not impose any constrains on the length of $N$. In particular, when $N\\to \\infty$, the decision making of HV is related to dynamic programming (assume the model is available) and otherwise, the decision making of AV is in the same spirit of Model Predictive Control (MPC). \n- (**Beyond Two-agent Case**) We remark that it is feasible to extend to more than one AV and one HV setting and  we share some preliminary thoughts as follows. Assume there are $N\\_H$ HVs and $N\\_A$ AVs in the mixed traffic system. With abuse of notations, we define the action vector for AVs and HVs as follows, at time step $t$, $u\\_H(t) = [u\\_{H,1}(t), u\\_{H,2}(t),\\cdots, u\\_{H,N_H}(t)]$ and $u\\_A(t) = [u\\_{A,1}(t), u\\_{A,2}(t),\\cdots, u_{A,N\\_A}(t)]$. By defining the prediction error as in Equation (4) and HVs' bounded rationality as in Section 3.2, our analysis framework still can be applied. We remark that the dimension of the approximation error term and the bounded rationality term is thus $N\\_A$ and $N\\_H$ times higher than the two-agent case. Hence, the resulting regret in Theorem 3 and Theorem 4 are $N\\_A$ and $N\\_H$ times higher than the two-agent case.  \n\n\n**W3. Comparative experiments and analysis**\n\n**A3.** We thank the reviewer for the suggestions on strengthen this paper's contribution. In our revision, we make the following changes in this regard:\n\n- (Comparison with conventional multi-agent problem formulation.) We further clarify the difference between our problem formulation and other multi-agent formulation (e.g., MARL, Dec-POMDP, Two-player game) and emphasize the robustness of our formulation in Section 1 (related work) and Section 2. \n- (Detailed analysis on the theoretical results.) We  add  insight on the theoretical results and focus on the impact of each key elements of the decision makings (e.g., look-ahead length $L$, prediction error, etc.) in Section 3 and Section 4.\n- (Thorough analysis on the empirical results.) We emphasize the comparison among different parameter setting in Figures 1-3 and clarify the main takeaway   for  algorithm design from a practitioner perspective, in Section 4."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700073361365,
                "cdate": 1700073361365,
                "tmdate": 1700073361365,
                "mdate": 1700073361365,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TUlixIa8kN",
                "forum": "4nyTlyTtfX",
                "replyto": "kReBZXJqg0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank Reviewer DRuU for your review"
                    },
                    "comment": {
                        "value": "Thank you Reviewer DRuU again for your detailed review. \n\nSince the final stage of the discussion between reviewers and authors will end soon, please let us know if you have any further comments on our response to your concerns, we will be more than happy to answer your questions."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700345561056,
                "cdate": 1700345561056,
                "tmdate": 1700345561056,
                "mdate": 1700345561056,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Odo1aj1XL8",
            "forum": "4nyTlyTtfX",
            "replyto": "4nyTlyTtfX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8909/Reviewer_dAQk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8909/Reviewer_dAQk"
            ],
            "content": {
                "summary": {
                    "value": "This paper is concerned with a setting where autonomous vehicles must deal with humans that have bounded rationality. Under this scenario, this work provides a theoretical upper bound on the resulting regret of a planning-based autonomous vehicle with a finite planning horizon. The upper bound is then utilized to analyze the impact of various factors (e.g. the human's bounded rationality, the autonomous vehicle's finite planning horizon, the discount rate used during planning, etc.) towards the regret upper bound produced by the human-autonomous vehicle interaction. Finally, the authors claimed that they use their proposed upper bound formulation to analyze the impact of different learning strategies in human-autonomous vehicle interaction."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "**Major Strength - Originality - Novelty of Regret Analysis**\n\nTo the best of my limited knowledge of the subject, I have not seen a regret analysis in a human-autonomous vehicle interaction where humans have bounded rationality. While the proposed analysis seems to be based on very limiting assumptions, it could provide an important basis for future works in human-autonomous vehicle interaction. Perhaps from a multiagent systems researcher's perspective, the closest work to this paper is from Loftin & Oliehoek (2022), who also explored regret bounds when dealing with adaptive partners (that can be humans). Nonetheless, this work does not account for a human partner's bounded rationality in their theoretical analysis and is limited to fully cooperative settings.\n\n**Major Strength - Clarity**\n\nIn general, I find the paper to be clearly written. I especially appreciate the authors pointing out which factors of the human or autonomous vehicle's decision-making each theorem/analysis addressed. That helped the readers to stick with the overall flow of the paper, which is always challenging with papers proposing lots of theoretical analysis.\n\nCitations:\n\nLoftin & Oliehoek. 2022. \"On the Impossibility of Learning to Cooperate with Adaptive Partner Strategies in Repeated Games\". ICML 2022."
                },
                "weaknesses": {
                    "value": "**Minor Weakness - Quality - Missing Citations to Related Works**\n\nWhile not limited to their applications to autonomous vehicles, there are works in multiagent systems concerned with interaction against partners (including humans) whose decision-making processes (which encompass many things, such as their beliefs, goals, policies, bounded rationality, etc.) are unknown. These works can usually be found in the literature on ad hoc teamwork [1], zero-shot coordination [2], or opponent modelling [3].  Following how human-autonomous vehicle interaction can be modelled using the formulation described in these research areas, there should be a reference to these works in the paper.\n\n**Minor Weakness - Quality - Problem Formulation**\n\nThe decision to model the interaction as an MDP is also questionable. It specifically goes against the original formulation of MDPs to have the environment output two reward functions (one function each for the human and autonomous vehicle). In this case, would it not have been better to model the problem as a Dec-POMDP (or any other multiagent systems formulation)?\n\n**Major Weakness - Quality - Non-Adaptive Policy Assumption**\n\nAs highlighted by the authors themselves, a major weakness of this paper lies in the assumption that human and autonomous vehicle policies do not change/adapt during interaction. This assumption plays an important role in the analysis where teammate actions are assumed to be fixed when establishing the regret upper bounds of the autonomous vehicle's decision-making process. But this, overall, seems like an unrealistic assumption in real-world human-autonomous vehicle interaction.\n\nIn general, I highly recommend the authors to consider the concept of adaptive regret [4] in their analysis. Unlike the notion of regret used in this work, adaptive regret accounts for the possible change in a partner's decision-making process during interaction.\n\n**Minor Weakness - Quality - Misrepresented Contribution**\n\nIn the final sentence in the introduction, the authors claimed that they investigated the effects of different learning strategies on the learning performance achieved through the human-autonomous vehicle interaction. Unfortunately, the analysis in Section 4 did not exactly achieve that and instead settled with analyzing the effects of an autonomous vehicle's approximation errors on the overall regret. If the authors wanted to stick with the original claim, it would have been better if they formulated different types of learning algorithms and analyzed the effects on the regret bounds (see Section 5 of [4]). Otherwise, the authors should adequately adjust the reader's expectations in the introduction by being more direct about which factors' effects on the regret bounds are analyzed.\n\n**Major Weakness - Significance**\n\nI am concerned with the significance of this work. First, it seems based on highly restrictive assumptions that would not hold in real-world scenarios, where humans will adapt to the autonomous vehicle's policy. While this could have been fine had there been no previous theoretical analysis on dealing with adaptive partners, Loftin and Oliehoek (2022) [4] have introduced concepts for this kind of analysis. To improve the significance of this work for the community, I would expect the authors to build their analysis from the adaptive regret concept [4].\n\nAt the same time, there isn't a sufficient analysis of the effects of different autonomous vehicle learning algorithms on the overall regret in this paper. This generally limits the usefulness of this work for other people working on learning algorithms for autonomous vehicles. All in all, I doubt this work can significantly influence future work in this field.\n\nCitations:\n\n[1] Mirsky et al. (2022). \"A Survey of Ad Hoc Teamwork Research\". EUMAS 2022\n\n[2] Hu et al. (2020). \"Other-Play for Zero-Shot Coordination\". ICML 2021.\n\n[3] Albrecht et al. (2017). \"Autonomous Agents Modelling Other Agents: A Comprehensive Survey and Open Problems\". AIJ\n\n[4] Loftin & Oliehoek. 2022. \"On the Impossibility of Learning to Cooperate with Adaptive Partner Strategies in Repeated Games\". ICML 2022."
                },
                "questions": {
                    "value": "1. What are the challenges to extending the existing theoretical analysis to an adaptive setting where both human and autonomous vehicle change their policies during an interaction?\n\n2. Can this analysis be extended to other learning strategies that are not model-based?\n\n3. Why is it appropriate to model the system as an MDP? Doesn't the environment output two rewards at each timestep (which goes against the definition of MDPs)?\n\n4. Can you explain why the non-linear model (Section 3) is suitable to model human-autonomous vehicle interaction?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "I do not have ethical concerns over the paper"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8909/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698837503208,
            "cdate": 1698837503208,
            "tmdate": 1699637120878,
            "mdate": 1699637120878,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6UOt6aRctG",
                "forum": "4nyTlyTtfX",
                "replyto": "Odo1aj1XL8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dAQk (1/4)"
                    },
                    "comment": {
                        "value": "We thank reviewer dAQk for your thorough reading and constructive criticism. We answer your questions and address your concerns as follows.\n\n**(Weakness)**\n\n**W1. Minor Weakness-Missing Citations**\n\n**A1.** We thank the reviewer for pointing out references [1-3], and we have added a comparison between these works and ours in Section 1 (Related Work), which is outlined below.\n\n\n- Ad-hoc teamwork problem [1] mainly focused on the cooperative case,  whereas our setting does not impose assumptions on the cooperation of agents.\n\n- Zero-shot coordination [2] focused on the robustness of the self-play, whereas our work aims to understand the interaction between two agents with different decision makings strategies.\n\n- Opponent modeling [3] addressed the methods on modeling the opponents, whereas our work focuses on characterizing the impact of such modeling errors on the learning performance.    \n\n\n**W2. Minor Weakness-Problem formulation goes against MDP? Why not Dec-POMDP?**\n\n**A2.** In fact,  our  formulation can be viewed as a  MDP in the following sense: From the whole system's perspective, let $u=[u\\_A,u\\_H]$ to be the action vector which consists the action of both AV and HV. Let $r=[r\\_A,r\\_H]$ be the reward vector that consists the reward of both vehicles. In this case, we have the system MDP as $\\langle \\mathcal{S},\\mathcal{U}, P, r, \\gamma \\rangle$.  In a nutshell, our MDP formulation considers a multi-agent RL system with independent learners [R1].  We  outline a few new ideas beyond the conventional Dec-POMDP  as follows. \n\n- Many MARL formulations for  Dec-POMDP, often assume all agents   use the same RL algorithms to 'maximize' the rewards. Our work aims to study the case where AV and HV  use different learning methods, i.e., longer-horizon look-ahead planning and myopic decision making, to achieve their objectives. \n\n- Furthermore,  in our setting, HV makes decision  with bounded rationality at each time step, which deviates from reward maximization. \n\n[R1] Zhang et al., \"Multi-agent reinforcement learning: A selective overview of theories and algorithms.\" Handbook of reinforcement learning and control (2021): 321-384."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700072987806,
                "cdate": 1700072987806,
                "tmdate": 1700073068406,
                "mdate": 1700073068406,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6y2wakLvZ6",
                "forum": "4nyTlyTtfX",
                "replyto": "Odo1aj1XL8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dAQk (2/4)"
                    },
                    "comment": {
                        "value": "**W3. Major Weakness-AV and HV's policy is not adaptive?**\n\n**A3.** We thank the reviewer for sharing the very interesting paper on adaptive regret  [4].  We clarify that in fact, both AV and HV  **adapt their policies** during the interaction. More specifically, as shown in Section 2.1 Equation (2) and Equation (3) (and the paragraph below), both AV and HV's policies are conditioned on each other and each agent will update its policy when the other agent has changed  its policy, summarized below:\n\n\n- (HV adapts its policy via $N$-step planning.) In the same spirit as in Model Predictive Control (MPC), HV chooses the action by planning ahead for $N$ steps (cf. Equation (3)), i.e., at each time step $t$, HV's (optimal) policy is given by $$\\pi\\_{H}(\\cdot \\vert x(t),u\\_A(t)): ~u\\_H^{\\*}(t):=\\arg\\max\\_{u\\_{H}(t)}\\max\\_{u\\_{H}(t+1),\\cdots, u\\_{H}(t+N-1)} \\Phi\\_H(x(t),u\\_A(t),u\\_H(t)),$$ \n  where the policy is conditioned on (predicted) AV's action $u\\_A(t)$ from time $t$ to $t+N-1$. Along the line of (Sadigh et al. (2016,2018)), we remark that predicting a short-term sequence of controls is manageable for human, i.e., $u\\_A(t)$ is the true action taken by AV. At the next time step $t+1$, HV's policy is further adapted to AV's policy through its prediction of AV's future action $u\\_A(t+1),\\cdots,u\\_A(t+N)$.\n\n- (AV adapts its policy via $L$ -step lookahead RL.) Similarly, AV's policy is derived by maximizing the $L$ -step lookahead planning objective (ref. Equation (2)), i.e., at each time step $t$, \n  $$\\hat{\\pi}\\_A^t = \\hat{\\pi}\\_A(\\cdot \\vert x(t),u\\_H(t)) = \\arg \\max\\_{u\\_A(t)} \\max\\_{u\\_A(t+1),\\cdots,u\\_A(t+L-1) } \\hat{Q}\\_{t}(x(t),u\\_A(t),\\hat{u}\\_H(t)).$$\n  We note that AV's policy is conditioned on (predicted) HV's action $\\hat{u}\\_H(t)$, which is a noisy prediction of the underlying true action ${u}\\_H(t)$ (ref. Equation (4)). In this way, AV's policy will adapt to HV's policy change. \n\n**(Relation to Adaptive Regret)** Moreover, we clarify that in fact our regret analysis  uses **adaptive regret** which \"accounts for the possible change in a partner's decision-making process during interaction\". For instance, in our regret definition for HV (ref. Section 3.2), let ${\\pi}\\_A^t = {u}\\_A(t)$ and $\\pi\\_H^t=u\\_H(t)$ be the policies followed by AV and HV at time $t$. By abuse of notations, \n$$\\Phi\\_H^{\\*}(t) := \\Phi\\_H(x(t),\\pi\\_H^{\\*}(t),\\pi\\_A(t)) = \\max\\_{\\pi\\_H \\in \\Pi\\_H}\\Phi\\_H(x(t),\\pi\\_H,\\pi\\_A(t)).$$\n\nThen we obtain the regret of HV as \n$$\\mathcal{R}\\_H(T\\vert \\pi\\_A) = \\frac{1}{T} \\mathbf{E}\\_{x}[\\sum\\_{t=1}^T (\\max\\_{\\pi\\_H \\in \\Pi\\_H} \\Phi(x,\\pi\\_A^t,\\pi\\_H)-\\Phi(x,{\\pi}\\_A^t, \\pi\\_H^t))].$$\nNotice that at each time step $t$, AV does not follow a fixed policy. Meanwhile, HV's optimal policy is determined by choosing a policy $\\pi\\_H^{\\*}(t)$ from policy space $\\Pi\\_H$ such that $\\Phi(x,\\pi\\_A^t,\\pi\\_H)$ is  maximized. We remark that the definition of the regret given in our work is related to the one given in Equation (2)[4]. We clarify the relation between the two definitions in our revision. \n\n**(Regret Dynamics)** Moreover, in **Section 4**, we present the analysis of the regret dynamics, i.e., how does the regret change over interaction time $t$ (ref. Fig. 4). It can be seen from Fig. 4 that the regret dynamics of the overall system, i.e., $\\mathcal{R}\\_{A-H}$, is changing during the interaction. Note that this is different from the the external regret (e.g., Equation (3) [4]), where $G(a\\_n,b\\_n),n=1,2,\\cdots,s$ is the one-step payoff. In our case,  $Q$ and $\\Phi$ are the multi-step look-ahead or planning objective function when HV follows policy $\\pi_H^t$ and AV follows policy $\\pi\\_A^t$, $t=1,2,\\cdots,T$."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700073028558,
                "cdate": 1700073028558,
                "tmdate": 1700073119786,
                "mdate": 1700073119786,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "25K0g8olyB",
                "forum": "4nyTlyTtfX",
                "replyto": "Odo1aj1XL8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dAQk(3/4)"
                    },
                    "comment": {
                        "value": "**W4. Minor Weakness-Misrepresented Contribution?**\n\n**A4.** We clarify that by saying \"different learning strategies\", we refer to the setting where *AV's learning strategy* (or \"learning algorithm\") is different from *HV's* during the interaction, i.e.,\n\n- AV uses $L$-step look-ahead planning (cf. Eqn. (2))\n- HV uses $N$-step planning (cf. Eqn. (3)). \n\nIn Section 4, we show the impact of AV and HV's learning strategies have impact on the overall system performance in terms of the regret of the whole system $\\mathcal{R}\\_{A-H}$. In particular, \n\n\n- **Impact of AV's $L$-step look-ahead planning** (related to $\\Psi\\_A,\\Psi\\_v$ and look-ahead length $L$ in Corollary 5). Notably, $\\Psi\\_A$ and $\\Psi\\_v$ represent not only the impact of AV's function approximation error (ref. Figure 3(a)) but also AV's prediction error on HV's action (ref. Figure 3(b)). Moreover, $L$ plays a critical role in AV's planning algorithm and at time $t$, the look-ahead length is not directly relevant to AV's function approximation error $\\hat{Q}\\_{t-1}$ (it will have impact on the function approximation error at next time step). \n- **Impact of HV's $N$-step planning** (related to $\\Psi\\_H, \\Xi\\_H$ in Corollary 5).  HV's decision making hinges heavily on its bounded rationality level (i.e., $\\mu\\_H$). To this end, the $\\Psi\\_H, \\Xi\\_H$ terms in the upper bound characterize the impact of the rationality level on the overall system performance. Further more we demonstrate the impact of HV's rationality level in Figure 3(c).\n- **AV-HV's Impact on Each Other**.  The coupling term $\\gamma^l \\Psi\\_H(l)$  shows that AV's decision making (e.g., $L$) can directly affect HV's impact on the system performance, e.g., either weaken or intensify HV's impact on system performance.\n\n**W5. Major Weakness-Significance: Assume HV and AV have fixed policy? Consider different autonomous vehicles learning algorithms?**\n\n**A5.** As clarified in A3 above,  our HV-AV interaction setting (cf. Equation (2), Equation (3)) indeed considers the case where both vehicles adapt to the other's policy. \n\nNext, we remark on the rationale of choosing $L$-step look-ahead planning as AV's learning method in our analysis, i.e.,\n\n- Why model-based planning? In the context of AV-HV interaction, the system model represents the state (e.g., locations of two vehicles) transition, which follows the physics laws and can be learned by using offline dataset. Comparing with model-free learning, model-based method is more data efficient and thus more practical for real-world applications [R1]. Moreover, one of the advantage of model based RL is the prediction capability, i.e., predict the future state by rolling out the model. The prediction capability of autonomous vehicles has been proved to be useful for safety [R2].  \n- Why the learning method defined in Equation (2) is in fact general?  We clarify that Equation (2) is related to many commonly used RL algorithms, for instance,\n  - (Model-free Case) Set $L=1$, Equation (2) is the model-free Q-function update and our regret analysis still holds. \n  - (Actor-Critic Case) Let $Q$-function and policy $\\pi$ be parameterized by $\\theta$ and $\\phi$, respectively, Then Equation (2) can be learned by using Actor-Critic, i.e., in the actor step,  $\\theta$ is updated by maximizing the $L$-step look-ahead objective and $\\phi$ is updated using policy gradient. Note that in this case, the approximation error in both Actor and Critic update can be encapsulate into $\\epsilon\\_{v,t}$ as in Assumption 1.  Our proof of the regret carries over.\n\n[R1] Kiran, B. Ravi, et al. \"Deep reinforcement learning for autonomous driving: A survey.\" IEEE Transactions on Intelligent Transportation Systems 23.6 (2021): 4909-4926.\n\n[R2] Ren, Xuanchi, et al. \"Safety-aware motion prediction with unseen vehicles for autonomous driving.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700073156059,
                "cdate": 1700073156059,
                "tmdate": 1700073156059,
                "mdate": 1700073156059,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZCExy00tvI",
                "forum": "4nyTlyTtfX",
                "replyto": "Odo1aj1XL8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dAQk(4/4)"
                    },
                    "comment": {
                        "value": "**(Questions)**\n\n**Q1. What is the challenge to consider the adaptive setting?**\n\n**A1.** We clarified in A3 above that our HV-AV interaction setting (cf. Equation (2), Equation (3)) indeed considers the case where both vehicles adapt to the other's policy.\nOne main objective of this work is to characterize regret dynamics and understand the impact of different decision making strategies on learning performance.\n\n**Q2. Non model-based analysis?**\n\n**A2.** We clarify that the learning method defined in Equation (2) reduces to model-free Q-learning by setting $L=1$ (ref. A5 in the Weakness part above).\n\n**Q3. Why model the system as MDP?**\n\n**A3.** We clarify that our MDP formulation our MDP formulation considers a multi-agent RL model with independent learners (ref. A2 in the Weakness part above).   \n\n**Q4. Why non-linear model is suitable to model the interaction?**\n\n**A4.** We note that the non-linear model is common in the real world traffic flow. For instance, the well known intelligent driver model (IDM) is initially proposed to demonstrate the general approach on smoothing nonlinear mixed traffic flow [R3].\n\n[R3] Gazis, Denos C., Robert Herman, and Richard W. Rothery. \"Nonlinear follow-the-leader models of traffic flow.\" Operations research 9.4 (1961): 545-567."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700073183326,
                "cdate": 1700073183326,
                "tmdate": 1700073183326,
                "mdate": 1700073183326,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eY3viqfHlI",
                "forum": "4nyTlyTtfX",
                "replyto": "Odo1aj1XL8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank Reviewer dAQk for your review"
                    },
                    "comment": {
                        "value": "Thank you Reviewer dAQk again for your detailed review. \n\nSince the final stage of the discussion between reviewers and authors will end soon, please let us know if you have any further comments on our response to your concerns, we will be more than happy to answer your questions."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700345438874,
                "cdate": 1700345438874,
                "tmdate": 1700345588767,
                "mdate": 1700345588767,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pOelW8PY4Y",
                "forum": "4nyTlyTtfX",
                "replyto": "Odo1aj1XL8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8909/Reviewer_dAQk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8909/Reviewer_dAQk"
                ],
                "content": {
                    "title": {
                        "value": "Response to Author Rebuttal (1/4)"
                    },
                    "comment": {
                        "value": "**W1. Minor Weakness-Missing Citations**\n\nThank you for adding these new citations. I believe this would really help position the authors' work with respect to other related research problems. \n\nOne minor note is that the goal of zero-shot coordination is not limited towards evaluation against agents trained via self-play. More recent works in ZSC/AHT also evaluate against agents with different decision-making strategies or even with humans. Thus, I see no difference between this work's problem setup and ZSC/AHT. Nonetheless, I would encourage the authors to clearly emphasize the theoretical aspect of their work as opposed to most ZSC/AHT works that tend to be more empirical in nature.\n\n**W2. Problem Formulation**\n\nThe assertion that Dec-POMDP is limited to model agents using the same algorithm is **undoubtedly wrong**. Existing problem formulations in MARL (such as Dec-POMDPs or Stochastic Games) are agnostic towards the learning algorithm (i.e. including whether they are independent or centralized). As someone who has authored and reviewed papers in MARL, these models simply formalize the interaction between agents and the way their actions affect the state of the environment/other information the agents perceive (i.e. their rewards or observations included).\n\nNow, the use of MDPs to model interaction between agents in a multiagent system is simply incorrect (including the book referred to by the authors). This is because there is no \"system\" viewpoint in an MDP. MDP has always been designed for single-agent RL where each agent observes the entire state and gets one reward scalar (i.e. not multiple scalars like the authors' formulation) with the transition and reward function assumed to be Markovian. This Markovian and the single-agent assumption simply makes MDP unsuitable for modelling MARL.\n\nHowever, what the authors describe is more like a Stochastic Game [1] (importantly, not a Dec-POMDP since agents may have different reward functions). I highly urge the authors to reformulate their problem as this. I also encourage the authors to refer to the book from Shoham & Leyton-Brown (2008) to find different problem formulations that are more suitable for a multiagent setting.\n\nCitations: \n\n[1] Shapley, L.S. Stochastic Games. Proceedings of the National Academy Of Sciences, 39(10):1095\u20131100, 1953.\n\n[2] Shoham, Y. and Leyton-Brown, K. Multiagent Systems: Algorithmic, Game Theoretic, and Logical Foundations. Cambridge University Press, 2008."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582002926,
                "cdate": 1700582002926,
                "tmdate": 1700582083148,
                "mdate": 1700582083148,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JAI3f4AjO8",
                "forum": "4nyTlyTtfX",
                "replyto": "Odo1aj1XL8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8909/Reviewer_dAQk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8909/Reviewer_dAQk"
                ],
                "content": {
                    "title": {
                        "value": "Response to Author Rebuttal (2/4)"
                    },
                    "comment": {
                        "value": "**W3. Major Weakness-AV and HV's policy is not adaptive?**\n\nI understand the authors' arguments. However, there seems to be a misunderstanding about what **adaptive** means. By adaptive, like Loftin and Oliehoek (2022), I am referring to agent policies that **change according to other agents' observed behaviour (i.e. based on observation-action history) from the past**. Formally, such adaptive policies can be formalized as $\\pi: \\mathcal{H} \\rightarrow \\Delta(A)$ (see background section from Loftin and Oliehoek (2022)).\n\nIn comparison to the above concept **adaptive policies**, the policies used in this work's analysis are not adaptive since the decision-making of either human or AI agent is only based on the current state ($x_{t}$) and not the sequence of states and agent actions observed in the past (see that both agent and human objective when choosing actions are not conditioned on past information from timestep 0 to $t-1$). While there has been an effort to address this in the revision by conditioning on $u_{H}$, it is not clear how these previous actions translate into the objective for action selection (Expression 2) since they're not involved in this expression at all. Meanwhile, the human policy is not even assumed to depend on the previous actions of the agent.\n\nI believe it is important for the theoretical analysis to incorporate analysis of agents/humans that adapt their behaviour (i.e. having a policy that is dependent on the previous action of other agents). After all, humans change their driving policy according to the previous actions displayed by other drivers. For instance, if you see a driver who displays a tendency to be reckless, you may want to drive carefully around them. Meanwhile, you may want to overtake a driver that in the recent past tends to be slow and conservative. Without modelling the effects of different past actions to the agent's regret, it feels as if the applicability of the work will be very limited.\n\n(Adaptive Regret) Thanks for clarifying that the work is using adaptive regret. However, I think the writing has to be improved based on what I mentioned above. If $\\pi_{A}^{t}$ is not defined as a policy conditioned on observation-action history, then its value on both terms in the expression will be the same. In that case, there will be no need to use adaptive regret at all (since the agent will not adapt to different policies when interacting with $\\pi_{H}$ or $\\pi_{H}^{t}$ ). In this sense, there has to be better clarity on how $u_{H}$ affects the policy of the agent in Expression 2. Again, currently, it does not appear as if $u_{H} $ affects anything in the RHS of Expression 2."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700606884228,
                "cdate": 1700606884228,
                "tmdate": 1700607387626,
                "mdate": 1700607387626,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4snuLyNVux",
                "forum": "4nyTlyTtfX",
                "replyto": "25K0g8olyB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8909/Reviewer_dAQk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8909/Reviewer_dAQk"
                ],
                "content": {
                    "title": {
                        "value": "Response to Author Rebuttal (3/4)"
                    },
                    "comment": {
                        "value": "**W4. Minor Weakness-Misrepresented Contribution?**\n\nI would recommend a more specific term to describe the variation of decision-making strategies compared in the analysis. Perhaps maybe changing \"learning strategies\" to \"number of steps for lookahead planning\". \n\nThe problem here is that learning strategies, especially in the context of non-stationary teammates, can be mistaken for different ways an agent/human learns/changes their policy following the observed actions of other agents from the past. Being more specific should help avoid this misunderstanding and improve overall clarity.\n\n**W5. Major Weakness-Significance: Assume HV and AV have fixed policy? Consider different autonomous vehicles learning algorithms?**\n\nWhen you claim that your analysis is wrt adaptive agents, how come the model-based planning only predicts the future state of the environment? With adaptive teammates, different actions taken by the planning agent during rollout would change the policies of the other agents in different ways (after all, they see the agent playing different actions). Do the authors also model the change in teammate policies during planning (apart from just modelling the change in states)?"
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700608155201,
                "cdate": 1700608155201,
                "tmdate": 1700608155201,
                "mdate": 1700608155201,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "33Z3QP45RO",
                "forum": "4nyTlyTtfX",
                "replyto": "4snuLyNVux",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8909/Reviewer_dAQk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8909/Reviewer_dAQk"
                ],
                "content": {
                    "title": {
                        "value": "Response to Author Rebuttal (4/4)"
                    },
                    "comment": {
                        "value": "Overall, I am keeping my scores as it is. \n\nThe problem with this paper is the clarity regarding how other agents/humans' past actions affect the policy/planning process of a controlled agent. Despite the authors claiming that this work is analyzing co-adaptive agents, the proposed agent/human objective together with the model-based planning method does not clearly indicate that other agents' past actions can affect a controlled agent's objective/planning process. All things considered, there has to be significant improvement in terms of clarity, even if perhaps the results from the authors are correct."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700608486496,
                "cdate": 1700608486496,
                "tmdate": 1700608486496,
                "mdate": 1700608486496,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "I3AA5LSg9e",
                "forum": "4nyTlyTtfX",
                "replyto": "Odo1aj1XL8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer dAQk's response (2/4)"
                    },
                    "comment": {
                        "value": "**W3. AV and HV's policy is not adaptive?**\n\n**A3.** We thank the reviewer for clarifying your meaning of  \"adaptive policy\" in the recent comments. \"Adaptive policy\" can take many different forms, and our proposed adaptive policy'' is a widely used one in the literature (e.g., [R1, R2]). \n\n- (\"Policies only depend on current state and not the sequence of states\"?) First, the history state information has already embodied in the current state. Meanwhile, the polices depends **not only on** the current state $x\\_t$ but also on the (predicted) other's actions (cf. Equation (2) and Equation (4)). Further, the policy would depend (indirectly) on the sequence of states and agent actions observed in the past through the current state. We model both AV and HV as an underactuated system such that the action of AV will have impact on HV, and vice versa. For instance, we have clarified in our original response that HV's policy depends on her prediction on AV's action $u\\_A(t)$, $t=t+1,t+2,\\cdots$.  \n    \n- (\"Policies are not adaptive since it does not depend on the sequence of states and agent actions observed in the past\"?) Our polices are indeed adaptive and follow the standard definition as in adaptive control and reinforcement learning in general.  We verbatim copy the definition from some of these papers below.\n\n    - (Adaptive Control) In the well-known textbook [R1] \"adaptive control\" by Karl J. Astrom et al., the adaptive controller is defined as \"an adaptive controller is a controller with adjustable parameters and a mechanism to adjust parameters (in response to changes in dynamics)\"\n    - (Reinforcement Learning) In the suvery paper [R1], \"reinforcement learning refers to a class of methods that enable the design of adaptive controllers that learn online, in real time.\". In particular, adaptation in reinforcement learning, is more about staying ahead of evolving challenges. \n\n     In our work, the adaptation of the agent's policy is achieved through the prediction on the future state (and actions of other agent). Clearly, the prediction of the future state can use end-to-end offline learning method  and/or previous collected history data. However, designing \"an algorithm to predict the future state\" and determine \"what is the needed  information\" are not the focus of our work. \n\n     In response to your suggestions on using history observations, we point out that it is well-known that solely using history information can be harmful in RL. For instance, the distribution shift issue in offline RL and the state-action visitation distribution shifts when using history polices in model-based RL. \n    \n- (\"Applicability of the work will be very limited\"?) Our work is not restricted to any specific model on *how to use the history observation to predict the future*,  and in fact, it is a general setting that can serve as a basis for future studies. For instance, AV's prediction error of HV's future state by using historical data can be encapsulated in the term $\\epsilon\\_A$. In our work, we assume $\\epsilon\\_A$ to follow a Gaussian distribution. With explicit prediction model, it is feasible to derive the prediction error distribution conditioned on the history information. \n- (\"agent will not adapt to different policies when interacting with $\\pi\\_H^t$\"?) This is incorrect. For instance, in the defined reward function of AV, i.e., $  r_A(x,u\\_A,u\\_H)= f\\_A(x,u\\_H) + u\\_A^{\\top}S\\_Au\\_A$ (cf. Assumption 2), where the first term $f\\_A$ is directly related to HV's policy. Based on this definition, AV's policy is derived by optimization the $L-$step lookahead planning, i.e., the summation of the future reward. \n\n[R1] \"Adaptive Control\" by Karl J. Astrom et al.\n \n[R2] Lewis, Frank L., Draguna Vrabie, and Kyriakos G. Vamvoudakis. \"Reinforcement learning and feedback control: Using natural decision methods to design optimal adaptive controllers.\" IEEE Control Systems Magazine 32.6 (2012): 76-105."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675926204,
                "cdate": 1700675926204,
                "tmdate": 1700675939034,
                "mdate": 1700675939034,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KgoyulGFps",
            "forum": "4nyTlyTtfX",
            "replyto": "4nyTlyTtfX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8909/Reviewer_BMkw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8909/Reviewer_BMkw"
            ],
            "content": {
                "summary": {
                    "value": "This is a theory paper, analyzing a heterogeneous system of an automated vehicle (AV) and a human vehicle (HV) modeled as a joint MDP with continuous actions. The AV's model of the HV is N-step look ahead optimization plus Gaussian noise. Authors derived upper bounds for the following cases:\n1. AV's performance Gap with linear dynamics\n2. AV's performance Gap with non-linear dynamics\n3. AV's Regret with non-linear dynamics\n4. HV's Regret with non-linear dynamics\n5. Joint HV-AV's Regret with non-linear dynamics"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ Great theoretical analysis that lays foundation for mixed human and autonomous systems\n+ In addition to the upper bound derivation, authors plotted the upper bound for several conditions for better explanation of their impact"
                },
                "weaknesses": {
                    "value": "- Generalizability: Assumptions 1 and 2 can limit the applicability of the theory in practice. While I understand Assumption 2 has been widely used in the past, it would be great to discuss limitations these two assumptions would play in practice. \n- Writing and clarity: there are writing issues with the paper (see minor comments for examples). Also I could not find the values for certain setups (e.g. settings 1-5).\n\nMinor comments:\nIn definition of Q^\u03c0(x,u_A,u_H), the expectation needs \\pi,\noptimization on an proxy object => on a proxy\nLet Let => Let\net al. (2022)Sadigh et al. (2016) => add ,"
                },
                "questions": {
                    "value": "Can elaborate on the limitations of your assumptions in practice?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8909/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698992517579,
            "cdate": 1698992517579,
            "tmdate": 1699637120642,
            "mdate": 1699637120642,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "w3dnZD0adM",
                "forum": "4nyTlyTtfX",
                "replyto": "KgoyulGFps",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer BMkw"
                    },
                    "comment": {
                        "value": "We thank reviewer BMkw for your thorough reading and positive feedback. \n\n**Q1. Limitations of Assumption 1 and Assumption 2**\n\n**A1.** We summarize the limitations of two assumptions as follows:\n\n- (**Assumption 1**: Function Approximation Error) In practice, since the underlying optimal value function is unknown, a commonly used approach to estimate the function approximation error $\\epsilon\\_{v,t}$ and $\\mu\\_{v,t}$ is to compare the difference between the rollout of the current policy (to estimate $\\hat{V}\\_t$) and Monte-Carlo Tree Search (MCTS) (to estimate $V^{\\*}$) [R1]. However, in order to get an accurate estimation of the optimal value, MCTS need to be applied  to different policies and it can be time-consuming if the state space and action space are large. One of the promising approach is to leverage an offline dataset with the interaction history between the HVs and AVs [R2].\n-  (**Assumption 2:** Reward Structure) In order to obtain the reward parameters $S_H$ and $S_A$ for HVs and AVs, various factors may be taken into considerations, e.g., safety, speed, comfort. In practice, the reward function design is a long-standing open question and highly depends on the problem of interest. For instance, [R3] also considers reference path deviation to avoid the vehicles to drive out of the lane. Handcraft all the factors that matter to the questions can be challenging. A promising way to efficiently learn such a reward signal can be achieved by Inverse Reinforcement Learning (IRL) based on HVs and AVs driving data. \n\n[R1] Chen et al. \"Randomized Ensembled Double Q-Learning: Learning Fast Without a Model.\" ICLR. 2020.\n\n[R2] Fu et al. \"D4rl: Datasets for deep data-driven reinforcement learning.\" arXiv preprint arXiv:2004.07219 (2020).\n\n[R3] Ran et al. \"Safety assurances for human-robot interaction via confidence-aware game-theoretic human models\". ICRL. 2022. \n\n**Q2. Writing issues.**\n\n **A2.** We thank the reviewer for pointing out the typos and the missing parameters setup and we have corrected them in our revision (all the revisions are marked in blue)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700072567195,
                "cdate": 1700072567195,
                "tmdate": 1700072567195,
                "mdate": 1700072567195,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qiIwOS8JHN",
                "forum": "4nyTlyTtfX",
                "replyto": "KgoyulGFps",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8909/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank Reviewer BMkw for your Review"
                    },
                    "comment": {
                        "value": "Thank you Reviewer BMkw again for your detailed review. \n\nSince the final stage of the discussion between reviewers and authors will end soon, please let us know if you have any further comments on our response to your concerns, we will be more than happy to answer your questions."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700345382461,
                "cdate": 1700345382461,
                "tmdate": 1700345577419,
                "mdate": 1700345577419,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]