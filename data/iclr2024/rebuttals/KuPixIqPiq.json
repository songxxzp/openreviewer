[
    {
        "title": "Teaching Large Language Models to Self-Debug"
    },
    {
        "review": {
            "id": "PmWIUwC6XY",
            "forum": "KuPixIqPiq",
            "replyto": "KuPixIqPiq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4261/Reviewer_AdYB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4261/Reviewer_AdYB"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a few-shot prompting based approach to help LLM's to self-correct errors made in code generation tasks. The approach works as follows:\n\nGiven a code task prompt, when LLM produces an output code, the system would first test the code on test input/output to decide if the code is correct or not. If the code is incorrect, the system call the LLM again to perform self-repair of the code, and produce a revised code as the output.\n\nThe paper various multiple ways to generate self-repair feedback: Simple (binary feedback), UT (when unit tests are available as part of task prompt), Expl (self-generated explanation of error) and Trace (execution trace when UT is available). The paper shows that different versions of feedback information have their own strength in different models and tasks; but in general, Expl shows most promising results, especially in Spider dataset.\n\nThe contribution of the paper is clear: it provides a comprehensive approach for leveraging self-repair to fix errors of code generation models, and perform ablation studies to compare their effectiveness in different tasks and different models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* systematic study of the effectiveness of self-repair\n* thorough evaluation on most recent models, tasks and variations of repair methods."
                },
                "weaknesses": {
                    "value": "The paper right now has quite a big flaw in my opinion: the unfair comparison with existing techniques and baseline due to different access to test input/output pairs to decide correctness of code; thus, the reported improvement number on top of baseline or existing work is meaningless.\n\nMore concretely: the paper relies on an oracle to decide whether the code is correct or not to proceed into repair round. And this information is not part of the input from reranking / LEVER / Baseline / Reviewer (note: these baselines used test correctness at training time, but not in the evaluation inference time). As such, the self-repair framework has additional information gain to improve its performance (e.g., even if self repair only correct 1/100 case, it will be a straight improvement over the baseline). This different setup from existing work should be clearly mentioned, especially in the evaluation session; other readers could be confused on the reported \"accuracy up to 12%\" as claimed in the abstract.\n\nBecause I agree this paper is valuable to our community, I would like to propose two solutions to address this issue:\n1. In self-repair, instead of using test I/O to decide if the code would be feed into the self-repair process, the authors can let the model to decide correctness of the code itself with only unit tests (for MBPP and transcoder) and task prompt. In this case, the model may make mistakes in classifying correct program as incorrect program, but it does not use the external power of test oracle. This approach would make the technique directly comparable with existing approaches and baseline no-repair model, because all of these approaches have same access of information when running evaluation.\n\n2. Alternatively, modify the baseline to be able to access to the test oracle: when the baseline generate code that fails on test oracle, simply resample a solution. In this case, the baseline would also benefit from oracle, and the resample will be guaranteed to improve the test accuracy (but probably not as much as self-repair approach). This will also be a fair comparison and the numbers would be meaningful to the readers.\n\nAs of now, I don't think it is appropriate to accept the paper because the experiments are comparing apples to pineapples. But I would vote for an acceptance if this issue can be addressed given the paper is well written and innovative."
                },
                "questions": {
                    "value": "Please address my concern on \"comparison with baseline\" clearly."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4261/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4261/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4261/Reviewer_AdYB"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4261/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698451353518,
            "cdate": 1698451353518,
            "tmdate": 1700606555710,
            "mdate": 1700606555710,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0gYN50zKhd",
                "forum": "KuPixIqPiq",
                "replyto": "PmWIUwC6XY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4261/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4261/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for raising the concern. We appreciate that you consider our work as innovative and well-written. We would like to clarify our experimental setup as follows.\n\n### Self-debugging does not have different test input/output pairs to decide the code correctness compared to baselines ###\n\nWe want to emphasize that self-debugging does not utilize extra input-output pairs compared to the baselines. Specifically, for text-to-SQL generation, there is no unit test for both self-debugging and all other baselines. Therefore, the LLM needs to infer the code correctness completely by itself. As described in Section 4.3, on MBPP, we follow the baseline approaches to include 1 unit test in the problem description, and other hidden unit tests are only used for the final evaluation on the code correctness. Therefore, the LLM also needs to infer the code correctness, since passing the given unit test does not necessarily mean that the predicted code is fully accurate and can pass the hidden unit tests. Note that during the self-debugging process, the hidden unit tests are never used, and the generated code is only executed on the 1 given unit test that already shows up in the problem description for generating the initial code. For code translation, we consider the setting where all unit tests are available, since the ground truth execution results can already be obtained by executing the code in the source programming language, which also follows the evaluation setup in prior work.\n\n### We compared to baselines that utilize code execution ###\nAs described in Section 2 and Section 5 (page 5), we compared self-debugging to baselines that generate multiple programs, and use code execution to select the final response.\n\nOn the Spider benchmark for text-to-SQL generation, since the unit tests are not available, the baseline takes the majority vote of execution results as the final prediction. As shown in Figure 4 (a), self-debugging from greedy decoding matches the performance of the execution-based baseline that uses 16 samples, and self-debugging consistently improves the sample efficiency. For Transcoder, our baseline utilizes all unit tests to determine the code correctness. Note that the performance of this baseline is equivalent to the reviewer\u2019s second suggested solution, since we consider the prediction from this baseline to be accurate if any one of the generated samples is correct. Again, we show that self-debugging from greedy decoding matches the baseline that generates 10 samples, and we presented the full analysis in Appendix E.1.\n\n### The usage of richer execution information is a strength of our self-debugging design ###\n\nFirst, we would like to note that the reranking techniques LEVER and MBR-Exec also utilize code execution to select the response. The main difference between our work and these execution-based reranking techniques lies in the way we utilize the execution results. Specifically, the baseline approaches generate multiple samples all at once, thus the model can generate the same wrong code multiple times despite observing the execution errors. On the other hand, self-debugging can leverage past predictions and code execution to reduce the redundant cost of resampling wrong predictions, which is an advantage over existing reranking techniques.\n\nFurthermore, the key motivation of our self-debugging design is to provide complementary performance gain on top of all kinds of existing code generation methods. As shown in Figure 1 and described in Section 3, the first step of each self-debugging turn is code generation, which can be implemented as any existing baseline such as greedy decoding and execution-based reranking. Therefore, the subsequent debugging step leverages richer execution information than the first step of code generation by design. We demonstrate in Table 1, Figure 4 (a) and Appendix E.1 that self-debugging does not only improve over greedy decoding, but also consistently improves the performance when the baseline utilizes multiple samples.\n\nWe hope the above discussion addresses your concerns. Please do not hesitate to contact us for any further questions or clarifications."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4261/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700222102055,
                "cdate": 1700222102055,
                "tmdate": 1700222102055,
                "mdate": 1700222102055,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iqwGiKlnWE",
                "forum": "KuPixIqPiq",
                "replyto": "0gYN50zKhd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4261/Reviewer_AdYB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4261/Reviewer_AdYB"
                ],
                "content": {
                    "title": {
                        "value": "Great response!"
                    },
                    "comment": {
                        "value": "Thanks a lot for the clarification. It's indeed my misunderstanding about whether evaluation test cases are used in deciding whether a solution would be repaired or not.\n\nJust a little quick follow up:\n\n1. In SQL case, since it is LLM's job to decide whether a solution is correct or not to proceed to self-debug phase, (1) what percentage of *initially correct solutions* are determined as \"incorrect\" and proceeded to the self-debug phase, and (2) whether these solutions are self-debugged into a wrong solution or remains correct? \n(I ask this because I guess that unlike cases with unit tests, LLM may not have 100% accuracy of deciding whether a solution is correct so it may fix a small amount of correct solutions into wrong solutions)\nBtw, I don't have any further question on SQL dataset, since it quite clear that self-debugging over-performs the results of majority voting based on 16 solutions, given that self-debugging only draws 2 samples (1 greedy + 1 self debug output). [Can you confirm if my understanding is right?]\n\n2. For MBPP and transcoder, despite (greedy) baseline models have access to unit tests in the same way as self-debug, they don't test it on them. I'm wondering if it is possible to provide a quick enhanced baseline: when a initially sampled program p is incorrect (when evaluated on unit tests), resample a second program and use it towards calculating accuracy.\n(I ask this because it would interesting to understand how much gain in self-debug comes from \"having a second chance\" v.s. \"the generated self-debug feedback\". This enhanced baseline is better than the current baseline because it only has one chance to generate the final solution without access to evaluation on unit tests.) Further clarification:\n\ndef current_baseline(prompt, test_case):\n   p = LLM(prompt)\n   return p\n\ndef enhanced_baseline(prompt, test_case):\n   p = LLM(prompt)\n   if p passes test_case: return p\n   else: return LLM(prompt)\n\ndef self_debug(prompt, test_case):\n  p = LLM(prompt)\n  if p passes test_case: return p\n  else: return LLM_with_self_debug(prompt, p, test_case)\n\nI'm wondering if the authors agree that enhanced_baseline may be a better baseline to highlight the effect of self debug feedback? Or, did I mistake again here that enhanced_baseline is already the baseline presented in table 2?\n\nBesides, I fully agree that the way how unit tests are used is an important contribution of the paper. Thus I'd like to understand the above two points. I will update my score since SQL cases clearly show an improvement."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4261/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700245273738,
                "cdate": 1700245273738,
                "tmdate": 1700245273738,
                "mdate": 1700245273738,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HMRAO8eIT1",
                "forum": "KuPixIqPiq",
                "replyto": "BdTXEIYJNJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4261/Reviewer_AdYB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4261/Reviewer_AdYB"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks a lot for clarification, I'm satisfied with explanation on Spider dataset, and hopefully these discussions and results will be integrated into the paper (updated my score based on this).\n\nFor this one:\n\nTranscoder: greedy decoding = 80.4%, enhanced baseline = 84.5%, self-debugging = 92.5%\nMBPP: greedy decoding = 61.4%, enhanced baseline = 65.4%, self-debugging = 70.8%\n\n80.4% seems to be Codex performance on baseline Transcoder, and self-debugging = 92.5% seems to be the results from GPT-3.5. What's the enhanced baseline number based upon? I'm wondering if it is possible to add enhanced baseline result into Table 2 for easy comparison (for both MBPP and Transcoder)?"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4261/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700524528814,
                "cdate": 1700524528814,
                "tmdate": 1700524528814,
                "mdate": 1700524528814,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Pbfp486pxp",
                "forum": "KuPixIqPiq",
                "replyto": "ZYGh76CPNH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4261/Reviewer_AdYB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4261/Reviewer_AdYB"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks a lot. The revision addresses my concern. I will further update my score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4261/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700606528237,
                "cdate": 1700606528237,
                "tmdate": 1700606528237,
                "mdate": 1700606528237,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nAtrCKkwJH",
            "forum": "KuPixIqPiq",
            "replyto": "KuPixIqPiq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4261/Reviewer_UhXy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4261/Reviewer_UhXy"
            ],
            "content": {
                "summary": {
                    "value": "This paper propose a method called self-debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations.The intuition is that human programmers also write programs in an iterative way where they modify the code through debugging. Their method achieves state-of-the-art performance on several code generation benchmark."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The intuition and introduction of this paper is quite clear. The proposed method is simple, effective, and can be applied to any LLM model. Their method achieves promising performance improvement in code generation tasks."
                },
                "weaknesses": {
                    "value": "1. Although the introduction of this paper is clear, the methodology part is not the case. There are many components in the proposed approach: code execution, code explanation, inferring code correctness, etc. Figure 1 is helpful but still not clear enough. It would be better if there is a diagram with concrete example in the main text.\n2. Although the paper claims that they improve sample efficiency, I am still doubtful about this as the self-debugging approach does require generating much more tokens for debugging. I think comparing different approaches by the average the number of tokens generated for solving each problem is more fair."
                },
                "questions": {
                    "value": "1. How do you compare your approach to Reflexion [1]?\n2. The paper has shown that self-debugging is effective in fixing grammar and semantic error. Is it effective in fixing logic error?  For example, the original implementation is correct but does not actually solve the problem. Will self-debugging be able to identify and fix such type of error?\n\n[1] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4261/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698568312815,
            "cdate": 1698568312815,
            "tmdate": 1699636393504,
            "mdate": 1699636393504,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6ShAmY9wJh",
                "forum": "KuPixIqPiq",
                "replyto": "nAtrCKkwJH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4261/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4261/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the encouraging comments.\n\n### Concrete example of self-debugging ###\n\nThanks for the feedback. We have added Figure 2 and 3 in the main paper to show the self-debugging workflow, which are abbreviated versions of Figures 6 and 7 in Appendix F. The 2 figures demonstrate concrete running examples of how self-debugging works with different prompt formats.\n\n### Sampling budget comparison ###\n\nCompared to the sampling cost for obtaining the initial code for debugging, self-debugging requires the LLM to generate more tokens by design. For example, in Table 2, self-debugging generates more samples than the greedy decoding baseline. This extra cost is also needed for other related work, e.g., self-refine and Reflexion.\n\nOn the other hand, as discussed in Section 5.2.1, we demonstrate that to improve the accuracy over greedy decoding, self-debugging achieves better performance with a lower inference cost. To compare the cost in terms of the number of tokens, we consider two settings: (1) when unit tests are not available; e.g., on Spider for text-to-SQL generation; and (2) when unit tests are available; i.e., on Transcoder and MBPP for Python code generation.\n\nOn the Spider benchmark, with explanation feedback, on average a full debugging turn generates 5.5 times more tokens than simply generating the SQL query. By comparing the performance of self-debugging starting from greedy decoding (80.8% accuracy) to the baseline that samples 16 programs without self-debugging (80.7% accuracy), we can see that self-debugging also reduces the number of generated tokens compared to the baseline.\n\nOn Transcoder and MBPP, due to the availability of unit tests, the improvement with explanation feedback is generally less significant than on Spider. In this case, the unit test feedback is a better option to reduce the sampling budget. On average, a full debugging turn with the unit test feedback generates 2.5 times more tokens than simply generating the initial code, thus self-debugging is again more sample-efficient in terms of the token cost.\n\nFurthermore, despite the fact that generating explanation and trace feedback increases the number of generated tokens, we note that one benefit of such feedback is to provide more interpretable rationales on how LLMs infer the code correctness, and how the fixes are conducted. Such natural language feedback helps us to investigate the models\u2019 weaknesses, and potentially can be used to finetune the model and improve its overall debugging and coding ability.\n\n### Comparison to Reflexion ###\nThe Reflexion paper evaluates their approach on AlfWorld and HotPotQA, which are decision-making tasks and question-answering tasks. For both benchmarks, they assume that the oracle reward is available, which tells the LLM whether the current state is successful or the current answer is correct.\n\nOn the contrary, our work focuses on code generation tasks, where the LLM does not always have access to the oracle reward about the code correctness. For example, on text-to-SQL generation where the unit tests are not available, the LLM needs to determine completely by itself when the debugging process should terminate. Furthermore, we proposed different prompt formats to enable self-debugging for diverse tasks and LLMs, and we demonstrated that the self-debugging ability can be taught with few-shot prompting, even if the model is not specially tuned for zero-shot debugging. For example, with code-davinci-002 that is not instruction tuned, we showed that self-debugging with few-shot prompting achieves similar or better performance gain than GPT-4.\n\n### Questions on fixing logic errors ###\n\nFor all tasks, our evaluation metric is the execution accuracy, which considers the predicted code to be correct when it passes the given unit tests or its outputs match those of the ground truth code. Therefore, when we only use unit tests to determine whether to continue the self-debugging process, e.g., for Transcoder experiments where all unit tests are available, programs that pass the unit tests but have other logic errors would not be detected. On the other hand, when we request the LLM to determine the correctness of the generated code, e.g., on Spider and MBPP, the LLM can correct such logical errors to some extent. In our experiments, we manually investigated tens of correct predictions from each benchmark, and we did not notice such false positives."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4261/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700222016926,
                "cdate": 1700222016926,
                "tmdate": 1700222016926,
                "mdate": 1700222016926,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TrfQEH6gqr",
                "forum": "KuPixIqPiq",
                "replyto": "6ShAmY9wJh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4261/Reviewer_UhXy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4261/Reviewer_UhXy"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for your response to my questions. I would like to keep my positive evaluation."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4261/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634228358,
                "cdate": 1700634228358,
                "tmdate": 1700634228358,
                "mdate": 1700634228358,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "n3hMAyMgy3",
            "forum": "KuPixIqPiq",
            "replyto": "KuPixIqPiq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4261/Reviewer_hHB2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4261/Reviewer_hHB2"
            ],
            "content": {
                "summary": {
                    "value": "Authors propose Self-Debugging approach in which LLMs identify code generation mistakes by investigating the execution results and explaining the generated code. This approach can improve code generation quality. Authors have performed experiments on several code generation benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Proposed Self-Debugging approach based on code self-explanation ( rubber duck debugging ) and test execution result investigation.\n- Some improvement over baseline results."
                },
                "weaknesses": {
                    "value": "- Improvement over baseline for Spider benchmark is only 2-3% which is not shown to be statistically significant. It could be accidental result of prompt change.\n- Same issue with code explanation without debugging for TransCoder and MBPP. \n- It seems that Self-Debugging without unit tests executions has very limited and possibly statistically insignificant improvements. \n\nThe following weaknesses have been fixed in the paper update by the authors:\n- Section 4 is very hard to read. It constantly refers to Appendixes. Even with limited space, it is possible to present material much better so that paper would be self-contained and would not require readers to read appendixes to follow a whole section 4. In my opinion, if this is not fixed, this is a very serious issue with the paper.\n- There are presentation issues with result tables. See Questions.\n\nI have increased my rating from 3 to 5.\nBased on further responses from the authors, I increased my rating to 6."
                },
                "questions": {
                    "value": "- In table 1, what does line \"Codex\" represent? It is in the section \"Self-Debugging (this work)\", but the result is different from other lines in Self-Debugging section. Your paper does not explain what this line shows. Is this baseline without Self-Debugging techniques? If so, why is it presented in Self-Debugging section?\n\n- In table 2, Codex results for Spider correspond to Codex results for Spider in Table 1. However, Codex results for MBPP do not correspond to Codex results for MBPP in Table 1. Why?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4261/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4261/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4261/Reviewer_hHB2"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4261/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698727259105,
            "cdate": 1698727259105,
            "tmdate": 1700700938818,
            "mdate": 1700700938818,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "re7QpDLrFs",
                "forum": "KuPixIqPiq",
                "replyto": "n3hMAyMgy3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4261/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4261/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the constructive feedback.\n\n### Significance of the Spider improvement ###\n\nWe would like to note that 2-3% improvement on Spider is non-trivial. As shown in Figure 4 (a), without self-debugging, a 3% improvement from greedy decoding requires 16 samples. In particular, Figure 4 (b) shows that self-debugging improves the accuracy on the hardest SQL tasks by 9%, which is a significant boost.\n\n### Significance of the code explanation on Transcoder and MBPP ###\n\nFirst, we would like to clarify that code explanation results in Tables 1-3 are accompanied with self-debugging. The evaluation of code explanation without self-debugging is not our primary focus, but we presented the results in Figure 1 of Appendix E.1, where code explanation consistently improves the performance by 2-4%.\n\nOn the other hand, due to the availability of unit tests, we acknowledge that the improvement with explanation feedback on Trancoder and MBPP is generally less significant than on Spider. However, one benefit of explanation feedback is to provide more interpretable rationales on how LLMs infer the code correctness, and how the fixes are conducted. Such natural language feedback helps us to investigate the models\u2019 weaknesses, and potentially can be used to finetune the model and improve its overall debugging and coding ability.\n\nFurthermore, we would like to emphasize that one core contribution of our work is to conduct a comprehensive evaluation of self-debugging performance with different LLMs, applications, prompting methods and feedback formats, and demonstrate the importance of each factor. Specifically, both code explanations and traces are also commonly used for human debugging, which do not require any extra input from other human programmers. We show that stronger LLMs for code (code-davinci-002 with few-shot prompting and gpt-4) benefit more from such richer self-generated feedback, which can motivate future work on improving LLMs for investigating and understanding the semantic meaning of code.\n\n### Importance of unit test execution ###\n\nAs discussed in Section 5.2.2, we acknowledge that the improvement of self-debugging is less significant without unit test execution. However, even without unit tests, sometimes LLMs can still improve their performance with self-generated feedback. For example, as shown in Table 3b:\n* With code-davinci-002, self-debugging without unit test execution improves the performance by up to 5%;\n* GPT-4 without unit test execution improves the MBPP accuracy by 3.6%.\n\nFurthermore, we would like to note that utilizing unit tests whenever applicable is also part of our self-debugging approach, and we consider the ability of leveraging execution of past generated programs as a key advantage of our self-debugging approach.\n\n### Comments on paper writing ###\nThanks for the feedback about Section 4\u2019s readability. We have added Figure 2 and 3 in the main paper to show the self-debugging prompt formats, which are abbreviated versions of the corresponding figures in the appendix. We hope this makes the section easier to read.\n\nRegarding other questions:\n\n* Codex in Table 1 represents the baseline without self-debugging. We put it under the section \u201cSelf-Debugging (this work)\u201d to denote that the following self-debugging results are built upon initial programs generated from this baseline. We added a divider around \u201cCodex (baseline)\u201d in Table 1 to make it clearer that these are the baseline results before self-debugging.\n* The Codex results in Table 2 were produced from greedy decoding, which demonstrate that self-debugging improves the performance with a higher sample efficiency. On the other hand, Codex baseline results in Table 1 were obtained with multiple samples, which demonstrate that self-debugging outperforms other baseline methods that leverage multiple samples. We updated the captions of Table 1 and 2 to clarify the settings.\n\nWe hope our paper revision and above discussion address your concerns. Please do not hesitate to contact us for any further questions or clarifications."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4261/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700221940073,
                "cdate": 1700221940073,
                "tmdate": 1700221940073,
                "mdate": 1700221940073,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2n46MgySPT",
                "forum": "KuPixIqPiq",
                "replyto": "re7QpDLrFs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4261/Reviewer_hHB2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4261/Reviewer_hHB2"
                ],
                "content": {
                    "title": {
                        "value": "Thanks to authors for answers and paper changes"
                    },
                    "comment": {
                        "value": "Thanks to authors for answers and paper changes.\nI have updated my review and paper score based on paper changes."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4261/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700354737931,
                "cdate": 1700354737931,
                "tmdate": 1700354737931,
                "mdate": 1700354737931,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "a1S78nQYEO",
                "forum": "KuPixIqPiq",
                "replyto": "n3hMAyMgy3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4261/Reviewer_hHB2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4261/Reviewer_hHB2"
                ],
                "content": {
                    "title": {
                        "value": "2-3% improvement significance"
                    },
                    "comment": {
                        "value": "I think you may have misunderstood my comment regarding 2-3% improvements. What I am wondering about is whether simple prompt changes may result in improvements of similar range. \n\nWe know that prompt changes affect results.\nWe don't know if baselines and self-debugging use best possible prompts, so-so prompts, or not very good prompts. It is possible that the result ranges of baseline and self-debugging overlap if the prompt ranges are considered. We don't know how the results would compare if the best \"oracle\" prompts would be used.\n\nDetermining reasonable prompt variability range may not be simple. Authors naturally pick the best prompt that they have seen so far. Different authors may spend less or more effort to find the best prompt. \n\nHave you tried different prompts for self-debugging? What variability range did you see?\n\nThank you"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4261/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700431464481,
                "cdate": 1700431464481,
                "tmdate": 1700431500569,
                "mdate": 1700431500569,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vjngdPLj9W",
                "forum": "KuPixIqPiq",
                "replyto": "n3hMAyMgy3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4261/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4261/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Improvement from self-debugging is complementary to the improvement from baseline prompt changes"
                    },
                    "comment": {
                        "value": "Thanks for your followup response! We agree that changing the baseline prompt can also change the performance. However, we have done our best to optimize the prompts and construct strong baselines. Please let us know if you have any better baseline prompts in mind, and we are more than happy to evaluate them.\n\nOn the other hand, the improvement from self-debugging is complementary to the improvement from baseline prompt changes. As supporting evidence, for text-to-SQL generation, we evaluated a variant of our full prompt where we removed the demonstration of one data row per table; i.e., we deleted the INSERT INTO statements in Appendix H. With greedy decoding, the accuracy of this baseline prompt is 75.6%, which is 1.9% lower than our full prompt. From this baseline, self-debugging also improves the accuracy by 2.1%. Note that removing INSERT INTO statements weakens both the baseline and the self-debugging prompt, but we still see a consistent improvement from the self-debugging process.\n\nRegarding different self-debugging prompts, we have shown results of integrating different feedback information in Section 5, especially Table 2. During the development of our approach, we have also done extensive evaluation on different wordings of each feedback type, but we did not observe much performance differences that change the relative order when comparing different feedback types.\n\nWe hope the above discussion and followup experiments address your concerns. Please let us know your thoughts, and we are more than happy to answer any further questions."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4261/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512117400,
                "cdate": 1700512117400,
                "tmdate": 1700513306633,
                "mdate": 1700513306633,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "83Uqlqoj02",
                "forum": "KuPixIqPiq",
                "replyto": "pjLhRrNE15",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4261/Reviewer_hHB2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4261/Reviewer_hHB2"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for authors response"
                    },
                    "comment": {
                        "value": "Thank you for your follow up. It addresses my concerns about the results demonstrated in the paper."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4261/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687121580,
                "cdate": 1700687121580,
                "tmdate": 1700687121580,
                "mdate": 1700687121580,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rrPe5x9VYv",
            "forum": "KuPixIqPiq",
            "replyto": "KuPixIqPiq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4261/Reviewer_urzn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4261/Reviewer_urzn"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a novel approach called SELF-DEBUGGING that enables a large language model to debug its own predicted program via few-shot demonstrations. The approach achieves state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation. The paper also discusses the possibilities of leveraging feedback messages and reusing failed predictions to improve the sample efficiency."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Proposing a novel approach called SELF-DEBUGGING that enables a large language model to debug its own predicted program via few-shot demonstrations.\n2. Demonstrating that SELF-DEBUGGING can teach the large language model to perform rubber duck debugging, i.e., identifying its mistakes by investigating the execution results and explaining the generated code in natural language.\n3. Achieving state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation.\n4. Discuss the ethics of using large language models for automated code generation and the importance of analyzing their capabilities and limitations before deploying them for real-world programming applications."
                },
                "weaknesses": {
                    "value": "I do not find obvious weaknesses in this work. I only have a concern about the proposed approach. Since I'm not an expert in the code generation field, please correct me if I have some misunderstandings. This kind of \"self-debug\" or \"self-refine\" requires LLMs to inspect their own outputs based on the unit test results and generate some explanation in an autoregressive manner. So a concern is the additional latency in the inference time, especially for extremely large language models. This extra computational time is not reported or discussed in this paper."
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4261/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4261/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4261/Reviewer_urzn"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4261/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698808875189,
            "cdate": 1698808875189,
            "tmdate": 1699636393323,
            "mdate": 1699636393323,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5474ZmyIuV",
                "forum": "KuPixIqPiq",
                "replyto": "TlOH1PzyO2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4261/Reviewer_urzn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4261/Reviewer_urzn"
                ],
                "content": {
                    "comment": {
                        "value": "Hi thanks for the insightful comments. \n\nI've checked the publication date of this paper and Self-refine on arxiv and find that they are almost in the same month. So I consider them as concurrent work, and both propose the \"self-eval, self-refine, self-debug\" idea. I agree that the authors can compare with these baselines to make the results more solid. \n\nFor the overhead of this approach, maybe quantitative results, quantifying the increase in both overhead and the overall performance, are needed to show whether this overhead is really worthwhile."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4261/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699851922400,
                "cdate": 1699851922400,
                "tmdate": 1699851922400,
                "mdate": 1699851922400,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]