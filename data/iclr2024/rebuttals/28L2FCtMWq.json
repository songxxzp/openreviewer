[
    {
        "title": "Ground-A-Video: Zero-shot Grounded Video Editing using Text-to-image Diffusion Models"
    },
    {
        "review": {
            "id": "RZizOHM0sn",
            "forum": "28L2FCtMWq",
            "replyto": "28L2FCtMWq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1291/Reviewer_NYGw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1291/Reviewer_NYGw"
            ],
            "content": {
                "summary": {
                    "value": "this paper aims to address the challenge of multi-attribute editing in video editing. By incorporating the outputs of grounding models like GLIP and employing a designed attention mechanism, the proposed approach enables precise and temporally consistent video attribute editing. The authors conducted comparative experiments with recent methods, demonstrating superior performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper addresses a highly significant problem of achieving consistent fine-grained attribute editing in videos. The introduction of grounding conditions proves to be a direct and effective approach.\n2. In addition to the design of grounding conditions, the authors propose mechanisms such as noise smoothing and cross-attention."
                },
                "weaknesses": {
                    "value": "This paper lacks substantial technical innovation as its main contribution lies in expanding the experiments on the input conditions, whether it is the depth or grounding results. The approach used for injecting conditions is based on ControlNet or cross-attention, which is a common practice in stable diffusion and related applications of ControlNet."
                },
                "questions": {
                    "value": "1. The authors provided a visual comparison of the flow smooth effect in Figure 6, but it only includes a single example. Are there additional examples and comparisons with baseline and other methods (excluding flow smooth) demonstrating their effects under the same prompt?\n2. during the smoothing process, thresholds are introduced. Are there objective metric comparisons for different thresholds and experiments to evaluate the robustness of the thresholds?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1291/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1291/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1291/Reviewer_NYGw"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1291/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698494137191,
            "cdate": 1698494137191,
            "tmdate": 1700740657420,
            "mdate": 1700740657420,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UlCVj6g5E9",
                "forum": "28L2FCtMWq",
                "replyto": "RZizOHM0sn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1291/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1291/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer NYGw"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer for the thoughtful comments.\n\n---\n>**W1.** This paper lacks substantial technical innovation as its main contribution lies in expanding the experiments on the input conditions, whether it is the depth or grounding results. The approach used for injecting conditions is based on ControlNet or cross-attention, which is a common practice in stable diffusion and related applications of ControlNet.\n\nWe sincerely thank the reviewer for the insightful review, but gracefully disagree with you in terms of importance of our contribution.\n\nOne of the key objectives of our work is to achieve multi-attribute video editing in a '*training-free*' manner. To accomplish this, we leverage the foundational Text-to-Image (T2I) model, Stable Diffusion [1]. \nConsequently, we opt for input conditions commonly used in image editing methods, where all input conditions are derived towards the goal of attaining temporal consistency. For example, the groundings are incorporated by our proposed Cross-Frame Gated Attention to assure groundings of each frame contribute to the global appearance consistency across frames within the generated video. \n\nAdditionally, in response to concerns that our work may be perceived as an extension of Stable Diffusion or ControlNet-based input conditions, we have added *comparisons with straightforward (per-frame) applications of T2I editing methods* in **Figure 9**. \nAs the results demonstrate, injecting input conditions directly leads to significant appearance inconsistencies across generated frames, even when text-image alignment is maintained on individual frames. \nWe believe that our methods (Cross-Frame Gated Attention, Inflated ControlNet, Modulated Cross-Attention, Optical Flow guided Latents Smoothing) to solve these inconsistencies are important contribution to the video generative research community, where temporal consistency is the most critical metric in the tasks.\n\nLastly, we wish to emphasize the novelty of the Modulated Cross-Attention mechanism, which enables interactions between context vectors across frames. This approach has not been explored previously even in a similar context.\n\n---\n>**W2.** The authors provided a visual comparison of the flow smooth effect in Figure 6, but it only includes a single example. Are there additional examples and comparisons with baseline and other methods (excluding flow smooth) demonstrating their effects under the same prompt?\n\nIn response to the reviewer's concern, we have incorporated additional examples showcasing the proposed optical flow smoothing in **Figure 6 (Left)**. \nWhile we did not identify any prior work specifically addressing the smoothing of inverted latents before the denoising process, we have demonstrated the effectiveness of our flow smoothing by applying it within the Tune-A-Video [3] framework, as depicted in the newly added **Figure 10**. \nIn comparison to scenarios where flow smoothing is absent, the application of this technique results in improved consistencies within static regions and the elimination of artifacts.\n\nWe also kindly remind the reviewer that our proposed smoothing is applicable to any video translation framework that initiates denoising process from inverted latents.\n\n---\n>**W3.** during the smoothing process, thresholds are introduced. Are there objective metric comparisons for different thresholds and experiments to evaluate the robustness of the thresholds?\n\nTo assess the robustness of the thresholds, we have included a quantitative analysis in the Optical Flow Smoothing Subsection of **Section 4.3**. \nThrough empirical experimentation, we determined that the optimal threshold for smoothing falls within the range of 0.1 to 0.5 and the algorithm is quite robust within this ranges. Consequently, in our quest for the ideal value, we computed frame consistencies using thresholds of 0.2, 0.3, and 0.4 with the same video dataset employed in our experiments.\n\n| Threshold | 0.2   | 0.3   | 0.4   |\n|-----------|-------|-------|-------|\n| Frame-Con | 0.970 | 0.968 | 0.964 |\n\n---\n[1] Rombach, Robin, et al. High-resolution image synthesis with latent diffusion models. In CVPR, 2022.\n\n[2] Zhang, Lvmin, et al. Adding conditional control to text-to-image diffusion models. In ICCV, 2023. \n\n[3] Wu, Jay Zhangjie, et al. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In ICCV, 2023."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1291/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700409076718,
                "cdate": 1700409076718,
                "tmdate": 1700409076718,
                "mdate": 1700409076718,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5gGZCXvKQC",
                "forum": "28L2FCtMWq",
                "replyto": "RZizOHM0sn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1291/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1291/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Dear Reviewer NYGw"
                    },
                    "comment": {
                        "value": "As the deadline for the Reviewer-Author discussion phase is fast approaching (there is only a day left), we respectfully ask whether we have addressed your questions and concerns adequately."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1291/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700571103806,
                "cdate": 1700571103806,
                "tmdate": 1700571173623,
                "mdate": 1700571173623,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3VFFD0M7iI",
                "forum": "28L2FCtMWq",
                "replyto": "RZizOHM0sn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1291/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1291/Authors"
                ],
                "content": {
                    "title": {
                        "value": "[Reminder] Summarization of our rebuttal"
                    },
                    "comment": {
                        "value": "Dear reviewer NYGw,\n\nWe believe that we have addressed the concerns that you have raised. Specifically,\n\n1. We clarified our pipeline design and its components, aimed at achieving *training-free* multi-attribute *video* editing.\nTo accomplish this goal, we leverage the pretrained Stable Diffusion model [1], employ input conditions used in image editing methods, and repurpose these conditions, all with the important aim of *ensuring temporal consistency in the generated videos*.\n\n2. To further address your concern that our work may be perceived as an extension of Stable Diffusion or ControlNet [2] -based input conditions, we have added comparisons with straightforward (per-frame) applications of T2I editing methods in **Figure 9**. \nAs the results illustrate, direct injection of input conditions leads to significant appearance inconsistencies across generated frames, even when maintaining text-image alignment on individual frames.  \nWe firmly believe that our proposed methods (Cross-Frame Gated Attention, Inflated ControlNet, Modulated Cross-Attention, Optical Flow guided Latents Smoothing) to solve these inconsistencies are valuable contributions to the field of video generative research, especially in the context where temporal consistency is of paramount importance.\n\n\n3. In response to the reviewer's concern regarding our proposed Optical Flow Smoothing, we have incorporated additional examples showcasing the proposed algorithm in **Figure 6 (Left)**.\n\n\n4. To the best of our knowledge, no prior work has attempted any form of smoothing operations on inverted latent representations prior to the denoising process.  \nHowever, in response to the reviewer's request for further validation of our proposed smoothing, we have showcased the effectiveness of our flow smoothing by integrating it into the Tune-A-Video [3] framework, as depicted in the newly introduced **Figure 10**.\nComparing scenarios with and without flow smoothing, the application of this technique leads to enhanced consistencies within static regions and the removal of artifacts.\n\n5. To illustrate the robustness of our Optical Flow Smoothing threshold, we have introduced a quantitative analysis in the Optical Flow Smoothing subsection of **Section 4.3**.\nThrough empirical experimentation, we have determined that the optimal smoothing threshold lies within the range of 0.1 to 0.5, and the algorithm exhibits robustness within this range. In pursuit of the ideal threshold value, we computed frame consistencies using thresholds of 0.2, 0.3, and 0.4 with the same video dataset used in our experiments.\n\n[1] Rombach, Robin, et al. High-resolution image synthesis with latent diffusion models. In CVPR, 2022.\n\n[2] Zhang, Lvmin, et al. Adding conditional control to text-to-image diffusion models. In ICCV, 2023. \n\n[3] Wu, Jay Zhangjie, et al. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In ICCV, 2023.\n\n---\n\nWe would like to gently remind you that the **end of the discussion period is imminent**. We would appreciate it if you could let us know whether our comments addressed your concerns.\n\nBest regards, \nAuthors"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1291/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714967054,
                "cdate": 1700714967054,
                "tmdate": 1700714967054,
                "mdate": 1700714967054,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MNnMg31e4v",
                "forum": "28L2FCtMWq",
                "replyto": "3VFFD0M7iI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1291/Reviewer_NYGw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1291/Reviewer_NYGw"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the author's response. Most of my concerns have been addressed. My remaining reservations primarily pertain to the novelty of the proposed methodology, which resembles an assembled machine, although it has indeed yielded satisfactory results. After careful consideration, I have decided to raise my rating."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1291/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740670196,
                "cdate": 1700740670196,
                "tmdate": 1700740670196,
                "mdate": 1700740670196,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "coB6uPkEv4",
            "forum": "28L2FCtMWq",
            "replyto": "28L2FCtMWq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1291/Reviewer_TXKT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1291/Reviewer_TXKT"
            ],
            "content": {
                "summary": {
                    "value": "Ground-A-Video proposes a training-free framework for grounded multi-attribute editing. The grounding ability is achieved by introducing the pretrained GLIGEN gated self-attention module into existing video editing pipelines. The paper also proposes several techniques including \"cross-frame attention\" and \"flow-guided latent smoothing\" to improve temporal consistency."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Grounded video editing is a novel task. Bounding box grounding allows users to accurately select the regions to edit. It is a useful feature that allows better location controllability and content disentanglement over the existing popular sentence-level video editing works.\n\n2. The overall framework is training-free. The proposed framework is built upon several pretrained models like Stable Diffusion, Control Net, GLIGEN, and training-free inversion techniques like Null-text Inversion, so the framework itself requires no additional training on video data.\n\n3. The framework effectively associates editing prompts with the grounded areas. The results in the paper show better text alignment compared to previous sentence-level video editing approaches like Tune-A-Video, and ControlVideo."
                },
                "weaknesses": {
                    "value": "1. Although under the same framework, the technical contributions are quite disconnected from one another and some of them might not be closely related to the grounded video editing task. Two main technical contributions (Modulated Cross-Attention and Flow-guided Latent Smoothing) in the paper lie in finding temporally smooth and faithful latent noise during inversion, which serves as a preparatory step and seems to be less relevant to the grounded editing task. On the other hand, the grounded editing capability mainly comes from the pretrained GLIGEN gated self-attention module, which brings limited addition to the previous image grounding task. \n\n2.  Although the proposed framework is training-free, it is still noteworthy that the per-frame null-text inversion requires gradient-based optimization on the null-text embeddings and could be time-consuming for longer videos. Moreover, the new Modulated Cross-Attention mechanism requires jointly optimizing all frames, which requires large memory.\n\n3. I am not very clear about the flow-guided smoothing after reading the description: does it only work on static areas? If not, why there are not any warping operations mentioned? I also find the terms \"spatially continuous\" and \"spatially discrete\" a bit confusing and hard to understand what continuous and discrete refers to.\n\n*For reproducibility, the code has not been released in the provided link at the point of this review."
                },
                "questions": {
                    "value": "Please kindly address the questions in the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "n.a."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1291/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698727108455,
            "cdate": 1698727108455,
            "tmdate": 1699636056008,
            "mdate": 1699636056008,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rJcrDgxRjk",
                "forum": "28L2FCtMWq",
                "replyto": "coB6uPkEv4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1291/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1291/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer TXKT (1/2)"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer for the insightful comments.\n\n---\n>**W1.** Although under the same framework, the technical contributions are quite disconnected from one another and some of them might not be closely related to the grounded video editing task. Two main technical contributions (Modulated Cross-Attention and Flow-guided Latent Smoothing) in the paper lie in finding temporally smooth and faithful latent noise during inversion, which serves as a preparatory step and seems to be less relevant to the grounded editing task. On the other hand, the grounded editing capability mainly comes from the pretrained GLIGEN gated self-attention module, which brings limited addition to the previous image grounding task.\n\nWe sophisticatedly designed the technical contributions (Gated Self-Attention into Cross-Frame Gated Attention, Cross-Attention into Modulated Cross Attention and introduction of inflated ControlNet [2] and Optical Flow Smoothing), all connected towards the *common goal of ensuring temporal consistency even in complex editing scenarios*. As the reviewer stated, the grounded editing capability primarily comes from the GLIGEN [1] gated self-attention module, however if directly applied in the Image editing manner, significant inconsistencies are introduced. Please refer to the added **Figure 9**. \n\nWe would like to emphasize that our work focuses on multi-attribute \u2018*video*\u2019 editing, where even if complex editing is performed successfully on each frame, if the appearance consistency (temporal consistency) is broken between frames, the edited video is considered as failure. We have also included a detailed quantitative analysis in **Table 2**, illustrating the effect of each component in our pipeline on achieving temporal consistency. \n\n---\n>**W2.** Although the proposed framework is training-free, it is still noteworthy that the per-frame null-text inversion requires gradient-based optimization on the null-text embeddings and could be time-consuming for longer videos. Moreover, the new Modulated Cross-Attention mechanism requires jointly optimizing all frames, which requires large memory.\n\nAs the reviewer stated, the per-frame null-text inversion requires gradient-based optimizations. In our work, the null-text inversion consumes 40 seconds. \nFurthermore, we would like to inform you that the denoising stage comprising 50 steps takes 1minute and 50 seconds. \nAlthough, the gradient-based null-text inversion requires non-trivial time, we believe that accelerating this inversion by parallel batch processing (putting the video frames on the batch dimension) is possible. \nAlthough we haven\u2019t yet implemented the parallel null-text inversion for videos due to the limitation on computational resource, we will include the inversion on our code release as soon as we implement it.\n\nTo further discuss the memory consumption related to the Modulated Cross-Attention, we would like to kindly inform you that the GPU vRAM consumption is not increased from the Modulated Cross-Attention. The main vRAM bottleneck in our framework comes from computation of Cross-Frame Gated Attention, which consumes 20 GB, which is available on a single Quadro RTX 6000 GPU, where we conducted all our experiments."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1291/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700408963592,
                "cdate": 1700408963592,
                "tmdate": 1700408963592,
                "mdate": 1700408963592,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uVrMQptItX",
                "forum": "28L2FCtMWq",
                "replyto": "coB6uPkEv4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1291/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1291/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer TXKT (2/2)"
                    },
                    "comment": {
                        "value": ">**W3.** I am not very clear about the flow-guided smoothing after reading the description: does it only work on static areas? If not, why there are not any warping operations mentioned?\n\nThe proposed Optical Flow guided Latents Smoothing works on static regions, the regions with relatively small motion difference, which is why we utilized RAFT optical flow estimator [3]. To enhance understanding of the algorithm, we revised a few notations in **Algorithm 1**. \n\nIn the previous algorithm, the obtained mask denotes dynamic regions $map_{mask}^{i} \\xleftarrow{}  map_{mag}^i>M_{thres}$,   \nwhich performs smoothing on static regions by ${z}^i_T = ({z}^i_T - {z}^{i-1}_T) \\* map\\_{mask}^i + {z}^{i-1}_T$ . \n\nIn the revised algorithm, the obtained mask denotes static regions $map_{mask}^{i} \\xleftarrow{}  map_{mag}^{i}<M_{thres}$,   \nwhich performs smoothing on static regions by ${z}^i_T = {z}^{i-1}_T \\* map\\_{mask}^i + {z}^{i}_T * (1-map\\_{mask}^i)$. \n\n\nWe would like to inform the reviewer that both the previous algorithm and the revised algorithm perform the exact same operations. The former is designed to be 'code-friendly,' while the latter is optimized for reader-friendliness. Additionally, our framework does not involve any warping operations. We are happy to revise our algorithm to address the reviewer\u2019s concern and we would like to provide more clarifications if needed.\n\n---\n>**W4.** I also find the terms \"spatially continuous\" and \"spatially discrete\" a bit confusing and hard to understand what continuous and discrete refers to.\n\nSpatially-continuous conditions refer to the spatial conditions that explicitly provide structural information by itself, such as edge maps and depth maps. These conditions are strictly spatially aligned with the generated output image. Also, these conditions are stored in a continuous spatial format, such as a 2D image (C x H x W) for depth maps.\nIn contrast, Spatially-discrete conditions do not require strict spatial alignment with the output image. In our work, the grounding conditions does not enforce particular structural constrains inside the bounding boxes, but rather determines the positions of entities enclosed by the bounding box within the overall image. While our work visualizes groundings as bounding boxes in each image (e.g., Figure 1), these bounding boxes are stored in a discrete spatial format, represented by the four coordinates that define each bounding box.\nWe believe the visualization on Figure 13 will facilitate understanding the explanation above.\nWe kindly refer the Reviewer to **Section F.1** Discrete and Continuous Conditions and **Figure 13** in the Appendix.\n We would be happy to engage in further clarifications if needed.\n\n\n---\n>For reproducibility, the code has not been released in the provided link at the point of this review.\n\nWe regret the delay in releasing the code. We understand the importance of open-source work and we assure that the code will be made publicly available. Also, in the abstract, we have updated the statement \u201ccodes are provided at our project page\u201d to \u201ccodes will be released.\u201d \n\n---\n[1] Li, Yuheng, et al. Gligen: Open-set grounded text-to-image generation. In CVPR, 2023. \n\n[2] Zhang, Lvmin, et al. Adding conditional control to text-to-image diffusion models. In ICCV, 2023.\n\n[3] Mokady, Ron, et al. Null-text inversion for editing real images using guided diffusion models. In CVPR, 2023.\n\n[4] Teed, Zachary, and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In ECCV, 2020."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1291/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700408988473,
                "cdate": 1700408988473,
                "tmdate": 1700409028671,
                "mdate": 1700409028671,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XjtVqvI6Qb",
                "forum": "28L2FCtMWq",
                "replyto": "coB6uPkEv4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1291/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1291/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Dear Reviewer TXKT"
                    },
                    "comment": {
                        "value": "As the deadline for the Reviewer-Author discussion phase is fast approaching (there is only a day left), we respectfully ask whether we have addressed your questions and concerns adequately."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1291/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700571084268,
                "cdate": 1700571084268,
                "tmdate": 1700571164079,
                "mdate": 1700571164079,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rZxjV8qT0T",
            "forum": "28L2FCtMWq",
            "replyto": "28L2FCtMWq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1291/Reviewer_vcyz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1291/Reviewer_vcyz"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents the \ufb01rst grounding-driven video editing framework, which is intended to solve the problem of neglected editing, wrong element editing and temporal-inconsistency in context of the complexities of multi-attribute video editing scenarios. Moreover, the proposed method is training-free which overcomes the obstacle of excessive computational cost on video tasks. Spatial-Temporal Attention, Cross-Frame Gated Attention and Modulated Cross Attention are introduced to further enhance consistency, depth map is used as an additional condition to better preserve structure and 3D information and the binary mask calculated through the optical \ufb02ow map can help maintain the consistency of the background area. E\ufb00ectiveness has been proven by su\ufb03cient experiments and convincing qualitative results."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "S1. This paper presents the \ufb01rst training-free grounding-driven video editing framework, which is relatively innovative.\n\nS2. The proposed method and experimental results are consistent with their motivation and e\ufb00ectively solve the problem of multi-attribute video editing in complex scenes.\n\nS3. The method is clearly stated, and the details are comprehensive."
                },
                "weaknesses": {
                    "value": "W1. Since the introduced depth map and optical \ufb02ow map both re\ufb02ect pixel-level structural information, will this cause the structure before and after editing to be too consistent, so that if the foreground is replaced by an object with inconsistent structure, the editing result will be poor and lack of \ufb02exibility? e.g. replace the \u201crabbit\u201d in the phrase \"A rabbit is eating a watermelon on the table\" with an animal without long ears.\n\nW2. For similar reasons, this may also limit the editing method to the task of adding or deleting objects."
                },
                "questions": {
                    "value": "See weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1291/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1291/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1291/Reviewer_vcyz"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1291/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698759176138,
            "cdate": 1698759176138,
            "tmdate": 1699636055937,
            "mdate": 1699636055937,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bQV8I71wlq",
                "forum": "28L2FCtMWq",
                "replyto": "rZxjV8qT0T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1291/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1291/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer vcyz"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful points.\n\n---\n>**W1.** Since the introduced depth map and optical \ufb02ow map both re\ufb02ect pixel-level structural information, will this cause the structure before and after editing to be too consistent, so that if the foreground is replaced by an object with inconsistent structure, the editing result will be poor and lack of \ufb02exibility? e.g. replace the \u201crabbit\u201d in the phrase \"A rabbit is eating a watermelon on the table\" with an animal without long ears.\n\nOur proposed optical flow smoothing make the static regions in the generated consecutive frames to be similar, whereas shapes of the foreground objects are mainly affected by **ControlNet's guidance** (depth).\nAs the reviewer stated, ControlNet [1] injects the pixel-level structural information, where the structural preservation between input image and output is the core advantage, yet weakness of ControlNet. \nSince we employ the ControlNet (Inflated ControlNet), our framework also share the advantage and weakness of the Image ControlNet.\n\nHowever, in the process of injecting the ControlNet's output features to the main Stable Diffusion [2] UNet's transformer blocks, *we multiply the ControlNet's output features by a scaling term, \u2018**ControlNet Scale**\u2019*, before they are added to the main UNet's output features. \nWe depict this process in the revised **Figure 3 (Right)** and added the description in **Section 3.3** Inflated ControlNet and **Section 5** Conclusion (Limitation).\nBy adjusting this scaling hyperparameter, ranging from 0 to 1, the control over the structural flexibility is gained. \nWe also added a visual ablation of it on **Figure 8 and 24**, where we experiment replacing the \u201crabbit\u201d with an animal without long ears.\n\n---\n>**W2.** For similar reasons, this may also limit the editing method to the task of adding or deleting objects.\n\nWe kindly refer the reviewer to the following examples in our manuscript:\n - \u2018Mountains\u2019 are removed and completely differently-shaped \u2018sunset\u2019 or \u2018fireworks\u2019 are added in the second video case of **Figure 9**.\n - Background \u2018trees\u2019 successfully removed in the last row of **Figure 21**.\n - The \u201clake\u201d is successfully added in the last four rows of **Figure 23**.\n\nWe would be happy to engage in further requests or clarifications if needed. Again, we appreciate the reviewer for the sharp observation.\n\n---\n[1] Zhang, Lvmin, et al. Adding conditional control to text-to-image diffusion models. In ICCV, 2023.\n\n[2] Rombach, Robin, et al. High-resolution image synthesis with latent diffusion models. In CVPR, 2022."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1291/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700408920450,
                "cdate": 1700408920450,
                "tmdate": 1700408920450,
                "mdate": 1700408920450,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TxTa7szRVk",
                "forum": "28L2FCtMWq",
                "replyto": "rZxjV8qT0T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1291/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1291/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Dear Reviewer vcyz"
                    },
                    "comment": {
                        "value": "As the deadline for the Reviewer-Author discussion phase is fast approaching (there is only a day left), we respectfully ask whether we have addressed your questions and concerns adequately."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1291/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700571021362,
                "cdate": 1700571021362,
                "tmdate": 1700571021362,
                "mdate": 1700571021362,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "n7zhcGcGsg",
            "forum": "28L2FCtMWq",
            "replyto": "28L2FCtMWq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1291/Reviewer_F2zK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1291/Reviewer_F2zK"
            ],
            "content": {
                "summary": {
                    "value": "The paper focuses on an interesting problem, namely multi-attribute editing in the video domain. The proposed method relies on recent techniques, such as GLIP, and ControlNet, and integrates the grounding information to perform a sequence of editing operations. The aim is also to propose a training-free pipeline by using pre-trained models in a zero-shot setting. For video-level editing, a stable diffusion backbone is extended for video data with DDIM inversion, and optical flow and depth maps are integrated as conditional to improve video editing quality. The evaluation is conducted on a set of videos and the evaluation is performed qualitatively with a comparison to SOTA and quantitatively by a user study with 28 participants."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The topic of paper, multi attribute editing in videos, is a challenging and interesting problem. \n\nThe paper seems mostly the integration and extension on available recent T2I pipelines for video domain. But, the selected models are recent and they fit well within the proposed pipeline. Moreover, the pipeline avoids additional training stages that is good for zero-shot pipeline."
                },
                "weaknesses": {
                    "value": "The paper is a well-designed integration of mostly existing techniques for video editing pipeline. The authors explain the steps in subsections with details. However, I found the overall text flow confusing as the stable-diffusion model and the layers of it are explained in multiple  sections, e.g. 3.2 and 3.4, rather than a whole. Additionally, the integrations of the controlNet and optical flow into the whole pipeline are not clear. I think a revised figure with math representations consistent with the text may help to explain the model better. \n \nA more detailed qualitative evaluation of the model could be presented in the experimental section. For instance, to assess the impact of a particular component on the pipeline, the ablation section (Section 4.3) only includes the edited video outputs generated using with and without this component. However, this evaluation could also be conducted quantitatively (as in Table 1) to see the impact of some evaluated components on the pipeline."
                },
                "questions": {
                    "value": "What is the video set used for comparison and evaluation in Table 1? \n\nHow similar are the paper's evaluation metrics with CAV (Chen et.al 2023)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1291/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1291/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1291/Reviewer_F2zK"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1291/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698909230254,
            "cdate": 1698909230254,
            "tmdate": 1699636055801,
            "mdate": 1699636055801,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "at91kKP3AK",
                "forum": "28L2FCtMWq",
                "replyto": "n7zhcGcGsg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1291/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1291/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer F2zK"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer for very thoughtful and constructive comments.\n\n---\n>**W1.** I found the overall text flow confusing as the stable-diffusion model and the layers of it are explained in multiple sections, e.g. 3.2 and 3.4, rather than a whole.\n\nWe revised the composition of **Section 3** accordingly. The revised composition is as follows:\n- [Section 3.1] Ground-A-Video: Overview\n- [Section 3.2] Inflated Stable Diffusion Backbone\n    - [Subsection 1] Attention Inflation with Spatial-Temporal Self-Attention\n    - [Subsection 2] Per-frame Inversion with Modulated Cross-Attention\n    - [Subsection 3] Video Groundings with Cross-Frame Gated Attention\n- [Section 3.3] Inflated ControlNet\n- [Section 3.4] Optical Flow Guided Inverted Latents Smoothing\n\n---\n>**W2.**  Additionally, the integrations of the controlNet and optical flow into the whole pipeline are not clear. I think a revised figure with math representations consistent with the text may help to explain the model better.\n\nTo clarify the integrations of the pipeline components, we revised a figure with consistent math representations in **Figure 3**. We split the overall pipeline into *Input Preparation* and *Denoising Process* and changed the caption of the figure. We would be happy to engage in further clarifications if needed.\n\n--- \n>**W3.** A more detailed qualitative evaluation of the model could be presented in the experimental section. For instance, to assess the impact of a particular component on the pipeline, the ablation section (Section 4.3) only includes the edited video outputs generated using with and without this component. However, this evaluation could also be conducted quantitatively (as in Table 1) to see the impact of some evaluated components on the pipeline.\n\nTo assess the individual impact of each component within the pipeline, we conducted a quantitative analysis presented in **Table 2**. We evaluated text alignment and frame consistency under the condition of omitting each component. \n|                    | Text-Align | Frame-Con |\n|--------------------|------------|-----------|\n| w/o Modulated CA   | 0.835      | 0.967     |\n| w/o Groundings     | 0.802      | 0.960     |\n| w/o Cross-Frame GA | 0.829      | 0.956     |\n| w/o ControlNet     | 0.823      | 0.948     |\n| Full components    | **0.837**  | **0.970** |\n\n\nMoreover, quantitative analysis on Optical Flow Smoothing component is added in the corresponding paragraph in **Section 4.3**.\n| Threshold | 0.2   | 0.3   | 0.4   |\n|-----------|-------|-------|-------|\n| Frame-Con | **0.970** | 0.968 | 0.964 |\n\n\nCoupled with the above suggestions on reorganizing the sections and revising the figure, we believe the reviewer\u2019s feedback added a lot of value to our work and sincerely appreciate the time devoted to deeply analyzing our manuscript.\n\n--- \n>**Q1.** What is the video set used for comparison and evaluation in Table 1?\n\nWe used a subset of 20 videos from DAVIS [1] dataset. We kindly refer the reviewer to **Section 4.1** Implementation Details. We would like to further clarify the video clips used in our experiments if needed.\n\n---\n>**Q2.** How similar are the paper's evaluation metrics with CAV (Chen et.al 2023)?\n\nCAV [2] employed XCLIP [3] video encoder to obtain video embeddings then computed cosine similarity between the targe text embeddings. In contrast, our work employed CLIP [4] image encoder to obtain embeddings of each frame, computed cosine similarities between the target text embeddings, then averaged the cosine similarities.\n\n---\n[1] Pont-Tuset, Jordi, et al. The 2017 davis challenge on video object segmentation. In arXiv preprint, 2017.\n\n[2] Chen, Weifeng, et al. Control-A-Video: Controllable Text-to-Video Generation with Diffusion Models. In arXiv preprint, 2023.\n\n[3] Ni, Bolin, et al. Expanding language-image pretrained models for general video recognition. In ECCV, 2022.\n\n[4] Radford, Alec, et al. Learning transferable visual models from natural language supervision. In ICML, 2021."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1291/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700408868057,
                "cdate": 1700408868057,
                "tmdate": 1700410261359,
                "mdate": 1700410261359,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1Inzx1GHR4",
                "forum": "28L2FCtMWq",
                "replyto": "n7zhcGcGsg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1291/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1291/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Dear Reviewer F2zK"
                    },
                    "comment": {
                        "value": "As the deadline for the Reviewer-Author discussion phase is fast approaching (there is only a day left), we respectfully ask whether we have addressed your questions and concerns adequately."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1291/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700571055296,
                "cdate": 1700571055296,
                "tmdate": 1700571228892,
                "mdate": 1700571228892,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fJa6CdckiO",
                "forum": "28L2FCtMWq",
                "replyto": "n7zhcGcGsg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1291/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1291/Authors"
                ],
                "content": {
                    "title": {
                        "value": "[Reminder] Summarization of our rebuttal"
                    },
                    "comment": {
                        "value": "Dear reviewer F2zK,\n   \nWe believe that we have addressed the concerns that you have raised. Specifically,\n\n1. We have restructured **Section 3** (Method) in response to the your feedback regarding the text flow. \nSpecifically, we have consolidated the explanation of the inflated Stable Diffusion model [1] and its layers into a cohesive presentation in Section 3.2. Additionally, we have provided elucidation of the Inflated ControlNet [2] in Section 3.3 and introduction  of Optical Flow-guided Smoothing in Section 3.4, respectively. \n\n2. In response to the your suggestions, we have made revisions to **Figure 3** (Overview of Ground-A-Video pipeline) to ensure consistent mathematical representation and enhance the clarity of the pipeline component integrations. More specifically, we have subdivided the entire pipeline into the *Input Preparation Process* and *Denoising Process*, and we have adjusted the figure's caption to reflect these changes.\n\n3. We have expanded our qualitative evaluation of Ground-A-Video in **Table 2**.\nIn order to assess the effects of various components, including Modulated Cross Attention (vs Cross Attention), Video Groundings guidance (vs Absence of Groundings guidance), Cross-Frame Gated Attention (vs Gated Self-Attention), and Inflated ControlNet guidance (vs Absence of ControlNet guidance), on our pipeline, we conducted evaluations of text alignment and frame consistency under each of these conditions. \nFurthermore, we have introduced quantitative analysis of our proposed Optical Flow-guided Inverted Latents Smoothing in the relevant section of Section 4.3.\n\n4. We have clarified the video dataset used for the comparison and evaluation. Additionally, we have provided an explanation of the similarities and differences between our Frame Consistency metric and the Frame Consistency metric used by CAV [3].\n\n\n[1] Rombach, Robin, et al. High-resolution image synthesis with latent diffusion models. In CVPR, 2022.\n\n[2] Zhang, Lvmin, et al. Adding conditional control to text-to-image diffusion models. In ICCV, 2023.\n\n[3] Chen, Weifeng, et al. Control-A-Video: Controllable Text-to-Video Generation with Diffusion Models. In arXiv preprint, 2023.\n\n---\n\nWe would like to gently remind you that the **end of the discussion period is imminent**. \nWe would appreciate it if you could let us know whether our comments addressed your concerns.\n\nBest regards, \nAuthors"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1291/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713632270,
                "cdate": 1700713632270,
                "tmdate": 1700713632270,
                "mdate": 1700713632270,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1Cp654z7VO",
                "forum": "28L2FCtMWq",
                "replyto": "fJa6CdckiO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1291/Reviewer_F2zK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1291/Reviewer_F2zK"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your detailed rebuttal which addresses many points I raised. But, I still have one major concern that the current format of the submission may require substantial revision particularly in the text part. I will evaluate your rebuttal again and your answers to other reviewer's comments."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1291/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700735127452,
                "cdate": 1700735127452,
                "tmdate": 1700735127452,
                "mdate": 1700735127452,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QiBhzw1e0k",
                "forum": "28L2FCtMWq",
                "replyto": "0358ZsJKOl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1291/Reviewer_F2zK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1291/Reviewer_F2zK"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for asking the clarification. I meant by \"substantial\" the revision over the \"initial\" submission. I will carefully check your revised version before finalizing my decision."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1291/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740657924,
                "cdate": 1700740657924,
                "tmdate": 1700740657924,
                "mdate": 1700740657924,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]