[
    {
        "title": "Adversarial Imitation Learning from Visual Observations using Latent Information"
    },
    {
        "review": {
            "id": "Qu55vEtnVw",
            "forum": "kA7vZQG34x",
            "replyto": "kA7vZQG34x",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5754/Reviewer_UdfL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5754/Reviewer_UdfL"
            ],
            "content": {
                "summary": {
                    "value": "This paper has proposed a visual imitation learning approach, where the agent learns from expert observations, but is not accessible to expert actions. To deal with the high-dimensional visual observations, the imitative rewards are defined in a latent space, and the latent state space is learned with the minimizing TV divergence objective. This paper is theoretically justified, and the proposed approach is evaluated in the mujoco domain."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper has a sound theoretical analysis."
                },
                "weaknesses": {
                    "value": "Comparing the sample efficiency and the convergent return, the proposed approach has not shown much strength superior to the baseline methods.\n\nThis paper has missed important related works, which aims to define imitative rewards with sinkhorn distance, which is beyond the GAIL framework.\n\n[1] Dadashi R, Hussenot L, Geist M, et al. Primal Wasserstein Imitation Learning[C]//ICLR 2021-Ninth International Conference on Learning Representations. 2021."
                },
                "questions": {
                    "value": "Does the latent representation require pretraining? Or is it learned end-to-end with the policy network and Q network?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5754/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698575168910,
            "cdate": 1698575168910,
            "tmdate": 1699636603995,
            "mdate": 1699636603995,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0HPvFMwvq1",
                "forum": "kA7vZQG34x",
                "replyto": "Qu55vEtnVw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5754/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5754/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## General\n\nThank you for taking the time to review our paper. In the following, we have carefully answered all of your concerns. We humbly request that you please consider updating your review scores based on the clarifications we have provided. Once again, we want to express our appreciation for your input.\n\n### W1, W2\n\n>#### Comparing the sample efficiency and the convergent return, the proposed approach has not shown much strength superior to the baseline methods.\n\nWe believe that our experiments demonstrate the benefits of our proposed approach. Our algorithm achieves similar or improved asymptotic performance compared to the baselines, while demonstrating significant improvements in terms of computational efficiency. These benefits are highlighted in Table 2, Table 3, and Figure 4.\n\nIn the visual imitation from observations setting, the bottom row of Table 2 shows that our algorithm achieves 75% of expert performance significantly faster than PatchAIL [a]: 5 times faster in walker walk, hopper stand, and hopper hop, 4 times faster in walker stand and walker run, and 2 times faster in cheetah run.  \n\nIn the visual imitation learning setting, Table 3 shows that our algorithm outperforms the baseline VMAIL [b] in terms of both final performance and computational efficiency.\n\nFinally, Figure 4 shows how our algorithm can leverage expert videos to improve efficiency in RL from pixels\n.\n[a] Liu, Minghuan, et al. \"Visual Imitation Learning with Patch Rewards.\" The Eleventh International Conference on Learning Representations. 2023.\n\n[b] Rafailov, Rafael, et al. \"Visual adversarial imitation learning using variational models.\" Advances in Neural Information Processing Systems 34 (2021): 3016-3028.\n\n>#### This paper has missed important related works, which aims to define imitative rewards with sinkhorn distance, which is beyond the GAIL framework.\n[1] Dadashi R, Hussenot L, Geist M, et al. Primal Wasserstein Imitation Learning[C]//ICLR 2021-Ninth International Conference on Learning Representations. 2021.\n\nThank you for sharing this paper with us. We have added this to our Related Work section in the updated version of the paper.\n\n### Q1\n\n>#### Does the latent representation require pre-training? Or is it learned end-to-end with the policy network and Q network?\n\nThe latent representation is entirely learned end-to-end and no pre-training is involved at any stage."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5754/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700168177262,
                "cdate": 1700168177262,
                "tmdate": 1700168177262,
                "mdate": 1700168177262,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4QXdjT3BE7",
            "forum": "kA7vZQG34x",
            "replyto": "kA7vZQG34x",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5754/Reviewer_f9wd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5754/Reviewer_f9wd"
            ],
            "content": {
                "summary": {
                    "value": "The paper tackles the problem of \"Visual Imitation from Observations\" (V-IfO), where the only learning source is a set of RGB observations of a task. The theoretical contribution is establishing a new upper bound on the learner's suboptimality based on its divergence to the expert's state-transitions as encoded in some latent. Methodology-wise, the authors propose a new algorithm \"Latent Adversarial Imitation from Observations\" (LAIfO), which combines existing methodology from inverse RL (IRL) with observation stacking and data-augmentation from recent off-policy RL algorithms. Empirically, the authors show their algorithm trains in less wall-clock time while retaining the same performance to other recent state-of-the-art imitation algorithms on six DeepMind Control tasks. Moreover, they also show that incorporating demostrations with off-policy learning and rewards can speed up existing off-policy RL algorithms on three of the more challenging DeepMind Control tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Overall, the paper is well-written. In particular, the authors make an appreciated effort to clearly define notation and assumptions before delving into the analysis.\n\n- The methodology is clear and simple and mostly reproducible.\n\n- I appreciate the purpose of the paper, visual imitation is a relevant problem."
                },
                "weaknesses": {
                    "value": "1. The proposed algorithm combines the adversarial imitation loss with gradient penalties from DAC [1] with the off-policy algorithm, stacking and  data-augmentation strategy from DrQv2 [2]. While both this papers are cited in text, the way the methodology is introduced in Section 5 never makes these connection explicit. As a consequence, I feel the way the algorithm is prevented can be quite misleading to an unfamiliar reader. Hence, I believe changing Section 5 to clarify which components come from DAC, which ones come from DrQv2 and that the novelty lies in *combining* them, would be extremely important before this work can be published.\n\n2. I found the novelty of the theoretical analysis and methodology to be quite limited. While I believe this is not a mandatory aspect for a good paper, especially if the resulting algorithm is effective, I found the quality of the empirical evaluation insufficient to make such assessment (see point 3).\n\n3. There are several aspects of the evaluation that left me unsatifsfied with its quality. First, the comparison with PatchAIL-W and VMAIL is only carried out on six tasks from three environments from the DeepMind Control (DMC) suite, while the comparison with DrQv2 and Dreamer-v2 is only carried out in three tasks from a single environment. I would have appreciated seeing a wider variety (e.g., including other complex environments from DMC such as quadruped/jaco arm and from alternative benchmarks e.g., car racing, claw rotate as considered in VMAIL). Furthermore, the current ablation seems very much limited as it could consider studying the effect of performance of many additional design choices (e.g. spectral norm v gradient penalty for Lipshitzeness/number of stacked frames/type of data augmentation...). Additionally, I think that reporting results also for a simple behavior cloning baseline with the same data-augmentation/architecture/optimization would help understand the contribution from the introduced IRL methodology. Most worryingly, however, when comparing LAlfO with Dreamerv2 and DrQv2 the performance of the baselines is considerably lower than what reported in prior work (e.g. see [2]). Even after 10x10^6 milion steps, the gains from incorporating expert demonstrations seem marginal at best (if any) when using the results from DrQv2. I would really appreciate it the authors could clarify this inconsistency. (also given that DrQv2 shares the data that produced their reported learning curves)\n\n4. Again, related to the evaluation Section, I find some of the claims to be quite misleading. E.g. in connection to the humanoid results the authors state \"we solve these tasks by using only 10^7 interactions\" However, the reported performance on 2/3 tasks (walk and run) is still extremely low, and I would refrain from referring to any of these tasks as solved. Furthermore, I think to make the comparison fairer I would have also appreciated seeing results for DrQv2/Dreamerv2 adding the expert demonstrations to their respective replay buffer.\n\nMinor:\n\nI believe the visual imitation problem setting described is a special simpler case of the visual third-person/observational imitation learning setting tackled by prior methods [3 as cited, 4, 5]. Yet, in contrast to what stated in Related Work (\"All of the aforementioned works consider fully observable environments\"), also this line of work deals with visual observation. Hence, I believe there should be a clearer explicit connection.\n\n[1] Kostrikov, Ilya, et al. \"Discriminator-actor-critic: Addressing sample inefficiency and reward bias in adversarial imitation learning.\" arXiv preprint arXiv:1809.02925 (2018).\n\n[2] Yarats, Denis, et al. \"Mastering visual continuous control: Improved data-augmented reinforcement learning.\" arXiv preprint arXiv:2107.09645 (2021).\n\n[3] Stadie, Bradly C., Pieter Abbeel, and Ilya Sutskever. \"Third-person imitation learning.\" arXiv preprint arXiv:1703.01703 (2017).\n\n[4] Okumura, Ryo, Masashi Okada, and Tadahiro Taniguchi. \"Domain-adversarial and-conditional state space model for imitation learning.\" 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2020.\n\n[5] Cetin, Edoardo, and Oya Celiktutan. \"Domain-robust visual imitation learning with mutual information constraints.\" arXiv preprint arXiv:2103.05079 (2021)."
                },
                "questions": {
                    "value": "- Where can I find detail regarding the expert data (is it taken from a standard benchmark? Was it collected with any particular protocol?) I cannot find this important information in the main text.\n\n- Can the authors provide learning curves for an increased number of steps for the Humanoid tasks, if available? (or at least also show the DrQv2 learning curves for the full 3x10^7 steps, shared in their repository to provide a refence for Figure 4)\n\nIn conclusion, while I mostly appreciate the nature of the contribution, the direction, and the presentation of the paper, I believe there are some current major flaws that make it not, yet, ready for publication. For this reason, I am currently leaning towards rejection. However, I am willing to change my score, in case the authors manage to properly address my criticism and questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5754/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698706883875,
            "cdate": 1698706883875,
            "tmdate": 1699636603905,
            "mdate": 1699636603905,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jQL4xDRaXZ",
                "forum": "kA7vZQG34x",
                "replyto": "4QXdjT3BE7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5754/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5754/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## General\n\nThank you for reviewing our paper. We appreciate your questions,  and we have carefully answered them in order to address your concerns. We kindly request that you please consider increasing your review scores in light of the responses we provided.  Once more, we sincerely appreciate your contribution, which has helped to improve the quality of our work.\n\n### W1, W2\n>#### The proposed algorithm combines the adversarial imitation loss with gradient penalties from DAC [1] with the off-policy algorithm, stacking and data-augmentation strategy from DrQv2 [2]. The way the methodology is introduced in Section 5 never makes these connections explicit. I believe changing Section 5 to clarify which components come from DAC, which ones come from DrQv2 and that the novelty lies in combining them, would be extremely important before this work can be published.\n\nWe have updated the beginning of Section 5 to cite all the literature inspiring our algorithm, including DAC and DrQ v2, in order to improve clarity and transparency. \n\nWe want to emphasize that the combination of the different elements from DAC and DrQ v2 is well-motivated by the novel theoretical analysis in Section 4, which is instrumental for the formulation of our algorithm and of other possible variants. \n\n>#### I found the novelty of theoretical analysis and methodology to be quite limited. While I believe this is not a mandatory aspect for a good paper, especially if the resulting algorithm is effective, I found the quality of the empirical evaluation insufficient to make such an assessment (see point 3).\n\nThank you for the opportunity to further discuss this point. We believe that important insights come both from our theoretical analysis and our experiments. \n\nTo the best of our knowledge, theoretical analysis of the imitation from observations problem in a partially observable environment does not appear in previous work. The analysis in Section 4 sheds light on the assumptions needed between expert and agent in order to perform visual imitation from observations, and motivates the idea of latent variable inference from high dimensional observations. Note that this is in contrast with PatchAIL in [a] where imitation is directly performed on the pixel space. \n\nThe Experiments section shows the practical implications of performing imitation on a lower dimensional space rather than on the pixel space. This results in significant improvements in computational efficiency, as emphasized by the wall-clock time ratio in Table 2. \n\n[a] Liu, Minghuan, et al. \"Visual Imitation Learning with Patch Rewards.\" The Eleventh International Conference on Learning Representations. 2023.\n\n### W3, W4\n>#### The comparison with PatchAIL-W and VMAIL is only carried out on six tasks from three environments from the DeepMind Control (DMC) suite, while the comparison with DrQv2 and Dreamer-v2 is only carried out in three tasks from a single environment. I would have appreciated seeing a wider variety (e.g., including other complex environments from DMC such as quadruped/jaco arm and from alternative benchmarks e.g., car racing, claw rotate as considered in VMAIL). \n\nThe goal of our Experiments section is to answer questions (1)-(4) as introduced at the beginning of Section 6 and not be exhaustive. We agree with the reviewer that our paper could benefit from additional experiments, but we do not think that the main message of the section will change if more environments or tasks are added. \n\nMoreover, in terms of quantity and difficulty of the tested environments, our paper is aligned with the convention established by the previous literature. Specifically, for the comparison with PatchAIL-W [a] and VMAIL [b], we test 3 easy tasks and 3 medium tasks from the DMC suite. In [a], 4 easy tasks and 2 medium tasks from the DMC suite are tested. In [b], 5 environments are tested.\n\nFor the comparison with DrQ v2 and Dreamer v2, we carry out experiments only in tasks which are considered hard to solve. In these types of tasks, leveraging expert videos makes more sense and the impact of our solution can be better appreciated as DrQ v2 and Dreamer v2 are quite inefficient.\n\n[b] Rafailov, Rafael, et al. \"Visual adversarial imitation learning using variational models.\" Advances in Neural Information Processing Systems 34 (2021): 3016-3028."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5754/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700167598356,
                "cdate": 1700167598356,
                "tmdate": 1700167598356,
                "mdate": 1700167598356,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mhj1ZI828Q",
                "forum": "kA7vZQG34x",
                "replyto": "G0k3UaWHji",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5754/Reviewer_f9wd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5754/Reviewer_f9wd"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response."
                    },
                    "comment": {
                        "value": "Unfortunately, while I appreciate the effort to respond to raised concerns, the modifications currently made to the text are minimal and I believe all my criticism still stands.\n\nIn particular, while the novelty lies in combining the existing practices from prior work, the way the work is presented never makes this clear, even in the current version (I did not find the small changes made to page 5 very helpful in this regard). \n\nMoreover, I still found the evaluation to be unsatisfactory for the reasons listed in my original review. In their response, the authors note that, for instance, VMAIL also tests for 5 environments. However, I would like to point out that these environments involve a much higher degree of diversity: they including continuous control locomotion, car racing, and complex robotic manipulation tasks. Moreover, VMAIL's experimental section considers several ablations considers several ablations and auxiliary experiments to better understand their algorithm. \n\nWhile contributions with relatively limited conceptual novelty and analysis can be impactful if they show consistent improvements, I believe they require a clear presentation and some breadth in their empirical evaluation for validation. Currently, I believe the paper is lacking in precisely these aspects, and I would the encourage the authors in considerations the provided criticism in future revisions of their work."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5754/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666663424,
                "cdate": 1700666663424,
                "tmdate": 1700666663424,
                "mdate": 1700666663424,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DzqESH1LKI",
            "forum": "kA7vZQG34x",
            "replyto": "kA7vZQG34x",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5754/Reviewer_Eep1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5754/Reviewer_Eep1"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers the problem of imitation learning from visual demonstrations, where not only the actions are unobservable, but also---due to partial observability---the underlying state. An upper bound is presented, that shows that the suboptimality of the learner can be bounded based on the TV-distance of the respective distributions over transitions in a latent space that compresses a history of observations and is assumed to be a sufficient statistic of the complete history (including actions). Motivated by this bound, a method is presented that performs imitation learning by applying GaifO (GAIL with a state-transition discriminator) using the latent representations instead of the states. The latent representations are learned during imitation learning by backpropagating the Q-function loss of DDPG through the encoder (the Q-function is expressed as $Q(z(x\\_{t^{-}:t}), a)$, with observation history $x_{t^{-}:t}$). No other losses (e.g. policy or discriminator loss) are backpropagated through the encoder.\n\nThis method is compared to the baseline PatchAIL in the \"Visual imitation from Observeration\" (V-IfO) setting and to LAIL in the visual imitation learning (VIL) setting, where expert actions are observed and their history is used for computing the embedding. In both settings, the proposed method LAIFO/LAIL compares favorable to the baseline methods in terms of stability, final performance and training time. Furthermore, the paper investigates the RL from demonstration setting, where the discriminator reward is augmented with a known reward function to guide exploration using demonstrations for vision-based locomotion tasks, which significantly improves performance compared to methods that do not make any use of demonstrations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Soundness\n------------------\n- The overall approach of learning a latent representation and imitating the expert with respect to latent transitions is sound.\n\n- The derived Theorems seem to be correct.\n\n- The claims are substantiated, and the main weaknesses (e.g. that expert and learner act in the same POMDP) are clearly communicated.\n\n2. Relevance\n-----------------\n- Imitation learning from (actual) observations is an important problem. Although I agree that learning under dynamic mismatch is still a key limitation, I think that the considered problem setting is still a useful step towards this objective.\n\n3. Novelty\n-------------\n- The proposed method seems to be novel.\n\n4. Presentation\n--------------------\n- The method was very well presented. The paper was a very read for me, which, however, is also partially due to the fact that the method is very straightforward.\n\n5. Related work\n---------------------\n- I'm not very familiar with the particular problem setting of imitation learning in POMDPs with unobserved actions, so I am not sure that no important baseline is missing. But the paper certainly does discuss several important relevant works. I am only of a recent work by Al-Hafez et al. (2023) that performs imitation learning for locomotion without action observations, but does not consider partial observability due to visual observations.\n\nAl-Hafez, F., Tateo, D., Arenz, O., Zhao, G., & Peters, J. (2023). LS-IQ: Implicit reward regularization for inverse reinforcement learning. (ICLR)."
                },
                "weaknesses": {
                    "value": "1. Experiments\n--------------------\n- The results presented in Table 2 do not seem to be statistically significant. I think it is misleading to highlight the best final performance in bold despite overlapping confidence intervals.\n\n- The experiments in the imitation learning from demonstration setting are not fair as none of the baseline makes use of the expert demonstrations. It would be better to compare to methods that focus on this problem setting.\n\n2. Originality\n-----------------\nWhile I think that the method is novel, it also very straightforward and simple. While I do believe that simple methods are good, I could not get many new insights from the paper (the theorems are also relatively straightforward variations of previous theorems that bound suboptimality based on TV distance in IL and RL)."
                },
                "questions": {
                    "value": "How can adding a reward objective to the imitation learning objective be justified? Can the detrimental effects of one objective on the other be bounded in some way?\n\nIt is common to not backpropagate through the actor in representation learning, and I also think that for similar reasons it makes sense to not backpropagate through the discriminator in the adversarial IL setting. However, did you consider addional (or alternate) methods to learn better representations? For example, many representation learning methods use additional objectives, e.g. contrastive losses, maximizing predictive information, which can significantly improve the downstream performance, in particular in RL from images."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5754/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698741041811,
            "cdate": 1698741041811,
            "tmdate": 1699636603763,
            "mdate": 1699636603763,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QRIE53Tofo",
                "forum": "kA7vZQG34x",
                "replyto": "DzqESH1LKI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5754/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5754/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## General\n\nThank you for reviewing our paper. We appreciate your questions and in response we have carefully answered each of them. Thank you once again for your input, which has contributed to improving our work.\n\n### W1: Experiments\n\n>#### The results presented in Table 2 do not seem to be statistically significant. I think it is misleading to highlight the best final performance in bold despite overlapping confidence intervals.\n\nWe understand the concern raised by the reviewer. We chose to highlight the best final performance in bold based on conventions established in previous literature (refer to [1,2] for instance). \n\n[1] Yang, Chao, et al. \"Imitation learning from observations by minimizing inverse dynamics disagreement.\" Advances in neural information processing systems 32 (2019).\n\n[2] Liu, Minghuan, et al. \"Visual Imitation Learning with Patch Rewards.\" The Eleventh International Conference on Learning Representations. 2023\n\n>#### The experiments in the imitation learning from demonstration setting are not fair as none of the baseline makes use of the expert demonstrations. \n\nThe main goal of Figure 4 and of the paragraph entitled \u201cImproving RL using expert videos\u201d is to show how our algorithm can leverage videos of experts to improve the efficiency of standard RL from pixels (see question (4) at the beginning of the Experiments section). As a result, in our view it makes sense to propose the comparison as in Figure 4. We also want to emphasize that we only assume access to visual observations of the expert in our algorithm and not direct knowledge of expert states, actions, and rewards since such information cannot be easily obtained when using videos of the expert. \n\nIn all the remaining paragraphs within the Experiments section, our algorithm and the baselines are compared in the same setting, i.e., either visual imitation learning or visual imitation from observations.\n\n### W2: Originality\n\n>#### While I do believe that simple methods are good, I could not get many new insights from the paper.\n\nThank you for the opportunity to further discuss this point. We also agree that simple methods are good, and we believe that interesting insights come both from the Theoretical Analysis and the Experiments sections. \n\nThe Theoretical Analysis section sheds light on the assumptions needed between expert and agent in order to perform visual imitation from observations, and motivates the idea of latent variable inference from high dimensional observations. Note that this is in contrast with PatchAIL where imitation is directly performed on the pixel space. \n\nThe Experiments section shows the practical implications of performing imitation on a lower dimensional space rather than on the pixel space. It is also clear from the Experiments section that our straightforward approach to latent variable estimation leads to strong performance in practice, without requiring more complicated inference procedures.\n\n### Q1\n\n> #### How can adding a reward objective to the imitation learning objective be justified? Can the detrimental effects of one objective on the other be bounded in some way?\n\nBy assumption, we consider both the expert and the agent acting on the same POMDP. As a result, in this specific case, the reward objective represents the reward, possibly sparse, which is maximized by the expert. The imitation learning objective, instead, provides a dense reward to the learning agent. Adding the imitation learning objective to this sparse reward yields the improved efficiency illustrated in Figure 4.\n\nWe believe that considering the impact of suboptimal data and/or relaxing the aforementioned assumption represent interesting directions for future work. \n\n### Q2\n\n>#### Did you consider additional (or alternate) methods to learn better representations? For example, many representation learning methods use additional objectives, e.g. contrastive losses, maximizing predictive information, which can significantly improve the downstream performance, in particular in RL from images.\n\nThank you for asking. We have tested different methods for latent variable inference including variational inference [3], as well as different auxiliary contrastive losses for the feature extractor $\\phi$ [4, 5]. The simple and effective approach presented in this paper outperformed these more complicated methods in the described setting.\n\n[3] Lee, Alex X., et al. \"Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model.\" Advances in Neural Information Processing Systems 33 (2020): 741-752.\n\n[4] Chen, Ting, et al. \"A simple framework for contrastive learning of visual representations.\" International conference on machine learning. PMLR, 2020.\n\n[5] Grill, Jean-Bastien, et al. \"Bootstrap your own latent-a new approach to self-supervised learning.\" Advances in neural information processing systems 33 (2020): 21271-21284."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5754/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700167257419,
                "cdate": 1700167257419,
                "tmdate": 1700167257419,
                "mdate": 1700167257419,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4MkBiDJCEU",
                "forum": "kA7vZQG34x",
                "replyto": "QRIE53Tofo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5754/Reviewer_Eep1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5754/Reviewer_Eep1"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your reply"
                    },
                    "comment": {
                        "value": "Thank you for your response. I still believe that the weaknesses I initially raised are valid, namely:\n\n1. The insights are limited\n2. The use of boldface in Table 2 is misleading."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5754/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661292198,
                "cdate": 1700661292198,
                "tmdate": 1700661292198,
                "mdate": 1700661292198,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]