[
    {
        "title": "Adapt On-the-Go: Behavior Modulation for Single-Life Robot Deployment"
    },
    {
        "review": {
            "id": "ScsaZTBPK6",
            "forum": "xTFgpfIMOt",
            "replyto": "xTFgpfIMOt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6577/Reviewer_YRpi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6577/Reviewer_YRpi"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an approach for selecting a skill from a fixed repertoire of pre-trained behaviors. Though the general approach is mainly studied in the light of test-time distribution shift, the selection scheme is general and could potentially be applied to other problems, e.g., task selection in long-horizon tasks. At a high level, the methodology adds a regularization function to the advantages of the task repertoire to increase the tasks's value function according to the state visitation frequency (the more a policy observes that state, the higher the value is going to be). At test time, the action with the highest value is selected. Experiments in simulation and real world show the effectiveness of the approach over a set of baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The problem this paper is trying to solve is important: skill selection in previously unseen scenarios is challenging. Using values for selection is not novel (See, for example, Chen et al., Sequential Dexterity: Chaining Dexterous Policies for Long-Horizon Manipulation), but the way the overall selection method is novel to me.\n2. While the case study is focused on adaptation to distribution shifts, the approach could be generalized to other skill selection problems, e.g., long-horizon task execution.\n3. The experiments show a good margin over other baselines, e.g. a naive high-level controller for skill selection trained with supervised learning."
                },
                "weaknesses": {
                    "value": "The major methodological weakness in the problem formulation is the bias induced by the proposed cross-entropy term. As proven by Theoreom 4.2, the increase in the value function is proportional to the state visitation frequency. This is a problem because the high-level policy will select the low-level policy, which mostly visited a state, not necessarily the best available policy. For example, a policy that resets almost immediately will visit a low neighborhood of the initial state and, therefore, will be pushed up and, possibly, preferred to a policy with a higher value but visits the same region of the state space much more infrequently. I don't see how this can be prevented without an extra normalization term on the state visitation frequency.\nAnother problem, though less structural than the previous one, is that there might be multiple policies with similar values and the high-level policy switching between them at random. This could lead to suboptimal behavior and possibly lead to failures. I don't see that there is any measure preventing this in the current approach.\n\nI am also not convinced by the experimental setup. I don't think I understood why policies trained in the real world are used for evaluation. This seems to be very interesting but orthogonal to the paper's contribution. This is a problem, in my opinion, because the policies can barely walk and keep balance, even without any pull forces (this can be seen at the beginning of the first video). This confounds the current experiments since the evaluation metrics are speed and stability. The gait and stability just can't be compared to the policies obtained via sim2real. It would be important to look into this and check whether this gap is still there after upgrading to a better policy. In addition, it is challenging to see whether a similar effect is happening in simulation without any visualization."
                },
                "questions": {
                    "value": "Is there any difference in the policies on their original MDP after fine-tuning? Or, in other words, does the cross entropy have any effect on the policy performance?\nIs there a way to quantify whether none of the available skills are good enough? (e.g., thresholding values)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6577/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697671796646,
            "cdate": 1697671796646,
            "tmdate": 1699636746882,
            "mdate": 1699636746882,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LQ6QNKdrRK",
                "forum": "xTFgpfIMOt",
                "replyto": "ScsaZTBPK6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6577/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6577/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comments. We include additional empirical analyses and answers to individual concerns below and have revised the paper accordingly. \n\n> The major methodological weakness in the problem formulation is the bias induced by the proposed cross-entropy term. As proven by Theorem 4.2, the increase in the value function is proportional to the state visitation frequency. This is a problem because the high-level policy will select the low-level policy, which mostly visited a state, not necessarily the best available policy\u2026\n\nThe cross-entropy term is not the only criterion for selecting a behavior; it is a regularizer. A behavior with higher value is still preferred if its visitation frequency is not too low. In theory, there could be situations where the best behavior is not selected if it has not seen the state before, but if a particular skill has not been trained in a particular state, we have no way of knowing what its value is. So preferring not to select such skills leads to a conservative strategy. This is a common approach in RL (e.g., methods like LCB-VI and CQL in offline RL). Such methods could also be argued to prefer choosing familiar actions, but this does not stop them from being widely used in practice and having appealing theoretical properties. The more significant challenge is the overestimation of value in infrequent or out-of-distribution states, which our method aims to address, as it is the real problem that we face. \n\nIn practice, the issue you describe does not seem to occur; ROAM does not exclusively always select the most high-frequency behavior. To show this, we ran an additional experiment, where in the simulated stiffness suite, we held out most of the data from one of the buffers corresponding to one of the behaviors, leaving only 5k transitions compared to the original 40k, and evaluated the agent at test time in an environment suited for that behavior. We find that even with only 5k transitions (compared to 40k for all other behaviors), ROAM still selects this less-frequent but suitable behavior the majority of the time, leading to similar overall performance.\n\n| # Transitions | % Timesteps Chosen | Avg # Steps |\n|---------------|---------------------|-------------|\n| 5k            | 53.2            | 591.3       |\n| 40k           | 78.4            | 573.8       |\n\n\n>  Another problem, though less structural than the previous one, is that there might be multiple policies with similar values and the high-level policy switching between them at random. This could lead to suboptimal behavior and possibly lead to failures. I don't see that there is any measure preventing this in the current approach.\n\nWe did not find that frequent switching between behaviors causes any problems. In scenarios where multiple policies have similar values, often these policies are all adept at handling the given state. We find that in practice, the presence of multiple policies with similar values does not negatively affect performance. In fact, this can be an advantage: The ability to switch between these policies at each timestep allows the agent to adapt to new and unforeseen situations for which no single behavior is optimally suited.\n\nEmpirically, we measured how often behaviors were switched and tried to see if frequency of behavior switches correlates with failure. We found no such correlation. Below, we show the percent of timesteps where the agent decides to switch behaviors, and more frequent switching does not lead to a higher average number of steps needed to complete the task. \n\n|      | Dynamic Friction       |                        | Dynamic Stiffness      |                        |\n|------|------------------------|------------------------|------------------------|------------------------|\n| beta | Avg # Steps            | Frequency of Switching | Avg # Steps            | Frequency of Switching |\n| 0.01  | 7610 +- 854            | 17.20%                 | 2698 +- 844            | 2.92%                  |\n| 0.1    | 2082 +- 382            | 15.63%                 | 1331 +- 263            | 8.25%                  |\n| 0.5   | 772 +- 179             | 11.85%                 | 628 +- 19              | 12.35%                 |\n| 0.9  | 1466 +- 534            | 9.36%                  | 735 +- 54              | 13.36%                 |"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6577/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700175145472,
                "cdate": 1700175145472,
                "tmdate": 1700175145472,
                "mdate": 1700175145472,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GhQ1Q7nSqT",
                "forum": "xTFgpfIMOt",
                "replyto": "JxyuJ01kYj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6577/Reviewer_YRpi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6577/Reviewer_YRpi"
                ],
                "content": {
                    "title": {
                        "value": "Possible misunderstanding, but issues are not addressed."
                    },
                    "comment": {
                        "value": "I thank the author for their response. However, there appears to be a misunderstanding.\n\n```\nThe cross-entropy term is not the only criterion for selecting a behavior; it is a regularizer. A behavior with higher value is still preferred if its visitation frequency is not too low. In theory, there could be situations where the best behavior is not selected if it has not seen the state before, but if a particular skill has not been trained in a particular state, we have no way of knowing what its value is. So preferring not to select such skills leads to a conservative strategy. This is a common approach in RL (e.g., methods like LCB-VI and CQL in offline RL). Such methods could also be argued to prefer choosing familiar actions, but this does not stop them from being widely used in practice and having appealing theoretical properties. The more significant challenge is the overestimation of value in infrequent or out-of-distribution states, which our method aims to address, as it is the real problem that we face.\n```\nI did not mean that the problem is in the states you never saw before. Obviously, in such states, you can't do anything better than guessing; a low value is undoubtedly helpful. My issue is with states that are visited but with different frequencies by different policies. Assume $\\pi_1$ and $\\pi_2$ both visit state $s_1$, but $V_1=kV_2$, for some $k>0$, and that $P_{\\pi_2} (s_1)>P_{\\pi_1}(s_1) $. Given $P_{\\pi_2} (s_1),P_{\\pi_1}(s_1)$, it is possible to find a $\\beta$ so that the proposed regularizer will increase the value of $\\pi_2$, resulting in $V_2^*>V_1^*$. This results in the worst policy being constantly selected. This situation is not uncommon. For example, if all policies start from the same state, the \"bad\" policies will reset more and end up visiting more often the initial part of the MDP.\nI believe that the issue is not observed _in practice_ in the current setup. However, would this work on different tasks? If the paper's point is mainly empirical, I believe more evidence is required. A very similar argument stands for the problem I mentioned about switching behaviors. The fact that it does not seem to be a problem in this task does not mean the issue does not exist. A simple example would be finding a state where two policies with equivalent values move at opposite speeds. Constantly switching between them would let the robot stay in place.\n\nThank you for adding the new results in the simulation, where the policy appears to be quite good. It would be nice to have a qualitative example of the other baselines as well and see how they fail and if these failures correlate with the real-world ones. I still think it is easier to train these policies in simulation and transfer them in the real world (so that the latter correlation point comes almost for free), but I agree (as I wrote in my original review) that this is orthogonal to the paper's contribution. The problem now (as mentioned by other reviewers as well) is in the small number of samples in the real world (3). With these samples, it is challenging to draw conclusions. The fact that the gait does not (visually) look stable could add quite a lot of noise in the statistics."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6577/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630563287,
                "cdate": 1700630563287,
                "tmdate": 1700630563287,
                "mdate": 1700630563287,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uT5RfknCBL",
            "forum": "xTFgpfIMOt",
            "replyto": "xTFgpfIMOt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6577/Reviewer_TYha"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6577/Reviewer_TYha"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a method for policy adaptation to different tasks. Instead of relying on a high level-controller to select the task appropriate behavior, they sample from the softmax distribution derived from the behavior policies\u2019 value functions. Specifically, they add a regularization cross-entropy term (equation 1) to artificially raise the value function in the encountered states of a behavior policy while lowering the value of other behavior policies (section 4.1). In this way, they assert that the propensity for the value function to over-estimate out-of-distribution states is reduced. This facilitates selection of the appropriate or closest in-distribution behavior policy to the encountered state at run-time.\n\nIn section 4.2, theoretical analysis is provided. They present a modified Bellman operator and show that it is a contraction (lemma 4.1). Theorem 4.2 also asserts that - with appropriately selected hyperparameters \u2013 the value function of the in-distribution behavior policy should be lower.\n\nEvaluation is done on a legged quadruped robot in both simulation and the real world where their proposed method outperforms baselines in data efficiency (Figure 3 for simulation) and general performance (Table 1 for real world results). They further validate that their approach selects the appropriate behavior for the current situation with high accuracy (Figure 4) and show how fine-tuning with the additional cross-entropy loss causes the gap in value functions of in-distribution versus out-of-distribution policies to become more apparent (Figure 5)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- State-of-the-art performance compared to recent baseline methods.\n- Theoretical analysis included.\n- Simulation and real-world experiments conducted. \n- Ablation study included.\n- The work is well written and clear."
                },
                "weaknesses": {
                    "value": "- The approach introduces an additional hyperparameter $\\beta$ that must be tuned. I am also unaware of how sensitive the approach is to this hyperparameter (whether most selected values will work well and beat baselines or whether only a small handful are appropriate).\n- The approach somewhat changes the definition of the Bellman operator such that it also contains a notion of the policies propensity to have encountered a given state instead of being based solely on the expected cumulative reward. Moreover, some values of $\\beta$ appear to have potentially odd behaviors. For example, selecting $\\beta=1$ appears to make the Bellman operator no longer depend on the reward signal?\n- There is perhaps some issues of fairness compared to RMA and HLC baselines. The authors use a state-of-the-art RLPD actor-citric method for their base learning approach. If RMA and HLC baseline methods also use actor-critic agents, were they also updated to use RLPD? If not, I would perhaps be concerned that the performance benefits reported may be in part due to RLPD instead of the author\u2019s proposed method.\n- Only a small number of real-world trials (3) are done and no confidence interval / variance is reported with the results (Table 1)."
                },
                "questions": {
                    "value": "Questions are in part copied from the weaknesses section: \n- How sensitive is the approach is to the $\\beta$ hyperparameter (whether most selected values will work well and beat baselines or whether only a small handful are appropriate)?\n- Some values of $\\beta$ appear to have potentially odd behaviors. For example, selecting $\\beta=1$ appears to make the Bellman operator no longer depend on the reward signal. Can the authors clarify this?\n- There is perhaps some issues of fairness compared to RMA and HLC baselines. The authors use a state-of-the-art RLPD actor-citric method for their base learning approach. If RMA and HLC baseline methods also use actor-critic agents, were they also updated to use RLPD? If not, I would perhaps be concerned that the performance benefits reported may be in part due to RLPD instead of the author\u2019s proposed method."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6577/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6577/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6577/Reviewer_TYha"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6577/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698828089332,
            "cdate": 1698828089332,
            "tmdate": 1699636746733,
            "mdate": 1699636746733,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SkC5xhFY4g",
                "forum": "xTFgpfIMOt",
                "replyto": "uT5RfknCBL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6577/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6577/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for your thoughtful comments and positive assessment of our work. Below we provide answers to your questions and concerns.\n\n> How sensitive is the approach to the $\\beta$ hyperparameter (whether most selected values will work well and beat baselines or whether only a small handful are appropriate)?\n\nWe ran ROAM with 4 different values (0.01, 0.1, 0.5, 0.9) of $\\beta$ in each simulated suite and show the performance below. For both evaluations, 3 out of 4 of these values (all except 0.01) outperform all the other baselines. \n\n|      | Dynamic Friction | Dynamic Stiffness |\n|------|------------------|-------------------|\n| beta | Avg # Steps      | Avg # Steps       |\n|------|------------------|-------------------|\n| 0.01  | 7610 +- 854      | 2698 +- 844       |\n| 0.1    | 2082 +- 382      | 1331 +- 263       |\n| 0.5  | 772 +- 179       | 628 +- 19         |\n| 0.9  | 1466 +- 534      | 735 +- 54         |\n\n> Some values of $\\beta$ appear to have potentially odd behaviors. For example, selecting $\\beta=1$ appears to make the Bellman operator no longer depend on the reward signal. Can the authors clarify this?\n\nThanks for bringing this up, and we apologize for any confusion. In practice, it makes sense to use ROAM only with values of $\\beta < 1$, and we only use such $0 < \\beta < 1$ in our experiments. \n\n> There is perhaps some issues of fairness compared to RMA and HLC baselines. The authors use a state-of-the-art RLPD actor-citric method for their base learning approach. If RMA and HLC baseline methods also use actor-critic agents, were they also updated to use RLPD? If not, I would perhaps be concerned that the performance benefits reported may be in part due to RLPD instead of the author\u2019s proposed method.\n\nThanks for this question, and we apologize for any confusion. We built all methods, including ROAM, RMA, and HLC on top of the same state-of-the art implementation of SAC as the base learning approach. For all comparisons, we additionally use a high UTD ratio, dropout, and layernorm, following DroQ (Hiraoka et al. 2022), and for any method that does fine-tuning, we use 50/50 sampling following RLPD (Ball et al. 2022). We have revised the paper to clarify this. \n\nAgain, thank you for your thoughtful review. If you have any remaining questions, please let us know."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6577/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700175487198,
                "cdate": 1700175487198,
                "tmdate": 1700175487198,
                "mdate": 1700175487198,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nHNnR5NJzr",
                "forum": "xTFgpfIMOt",
                "replyto": "SkC5xhFY4g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6577/Reviewer_TYha"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6577/Reviewer_TYha"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for responding to my questions. However, I still have some confusion. In the appendix section it states that \"For ROAM, we tuned $\\beta$ with values 1, 10, 100, 1000.\" (page 17) and notably $\\beta=1$ in real world experiments (page 18). Yet, you assert that $0<\\beta<1$ for experiments in your reply. Can this be clarified?"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6577/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700176198423,
                "cdate": 1700176198423,
                "tmdate": 1700176198423,
                "mdate": 1700176198423,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OOCVa7x72b",
            "forum": "xTFgpfIMOt",
            "replyto": "xTFgpfIMOt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6577/Reviewer_K9rY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6577/Reviewer_K9rY"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the problem of robot adaptation on the fly to unfamiliar scenarios and proposes a method for robust autonomous modulation (ROAM) that dynamically selects and adapts pre-trained behaviors to the situation."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The problem of adaptation on the fly is important for robotics.\n\n+ The proposed approach seems novel and is well-justified to address on-the-fly robot adaptation.\n\n+ Experiments using real robots are a strength and well demonstrate the proposed method.\n\n+ Comparison with existing methods is clear in the related work section."
                },
                "weaknesses": {
                    "value": "- Figure 1 motivates the problem using examples of facing various terrain and robot failure (e.g., damaged leg), but no experiments were performed on real robots in these scenarios.\n\n- Showing on-the-fly adaptation across different scenarios (beyond dynamic payloads) could make the experiments more convincing, for example, in a scenario when a robot with a heavy payload suddenly steps on icy terrain."
                },
                "questions": {
                    "value": "Please see the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6577/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698886023346,
            "cdate": 1698886023346,
            "tmdate": 1699636746586,
            "mdate": 1699636746586,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Vfn6ydirwc",
                "forum": "xTFgpfIMOt",
                "replyto": "OOCVa7x72b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6577/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6577/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for your thoughtful review and appreciate your acknowledgment of many strengths of our work. We aim to build upon our experimental validation to include more diverse and challenging conditions in future works, as you suggested, to further demonstrate the versatility and robustness of our method."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6577/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700175725945,
                "cdate": 1700175725945,
                "tmdate": 1700175725945,
                "mdate": 1700175725945,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]