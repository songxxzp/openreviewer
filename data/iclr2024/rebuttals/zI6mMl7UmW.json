[
    {
        "title": "Rethinking Spectral Graph Neural Networks with Spatially Adaptive Filtering"
    },
    {
        "review": {
            "id": "aU1HIVQ5dk",
            "forum": "zI6mMl7UmW",
            "replyto": "zI6mMl7UmW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4497/Reviewer_k2v7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4497/Reviewer_k2v7"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the connection between spectral and spatial domains for graph neural networks (GNN). Starting from a graph optimization problem, the authors can interpret an adapted new graph (used for propagation) based on the graph filter. Due to the inversion operation applied to the graph filter and the range of the graph filter\u2019s eigenvalues, this new graph can express an infinite series of the graph Laplacian. Motivated by this advantage against truncated polynomial approximators, the authors design a novel paradigm of GNN, named SAF, which utilizes BernNet and attends to the local and global information. Extensive experiments show the advantages of SAF on various datasets and under different settings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThis paper is well-written. I can pick the core idea effortlessly.\n2.\tThe proposed paradigm SFA is theoretically motivated, and its advantages seem to be well attributed to the expressivity of the adapted new graph.\n3.\tThe empirical studies are comprehensive. It makes the results convincing to evaluate related methods on those recently introduced node classification datasets. The results in the ablation study confirm the source of end2end advantages is the more expressive spectral filtering."
                },
                "weaknesses": {
                    "value": "1.\tIn my opinion, the most salient limitation of this work is the necessity to compute spectral decomposition. Although there have been many acceleration methods to conquer the O($N^3$) complexity (as the authors have also mentioned), such complexity will make SFA inapplicable to even moderate-sized graphs such as ogbn-products. Besides, there have been many more theoretical yet less practical graph filtering methods, where what limits their applications is mainly the necessity of spectral decomposition.\n2.\tI am a little confused with the so-called supervised setting. It is obvious that the split ratio is different from that of a semi-supervised setting. However, it seems that the node features of test nodes are also accessible to the compared methods during the training stage, which is the same as the semi-supervised setting.\n3.\tTheoretically, as the adapted new graph can express the power series of graph Laplacian, the most straightforward advantage of SFA, against existing spectral GNNs that use truncated polynomial approximation, is the capability of fitting specific graph filters. Thus, it is desirable to include such experiments in the main context of this paper as BernNet."
                },
                "questions": {
                    "value": "Could you explain figure 1 for me? I cannot figure out what the x-axis means as well as how these figures imply your conclusion."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4497/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4497/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4497/Reviewer_k2v7"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4497/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697703526549,
            "cdate": 1697703526549,
            "tmdate": 1700555158208,
            "mdate": 1700555158208,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Gqm5FgJKBz",
                "forum": "zI6mMl7UmW",
                "replyto": "aU1HIVQ5dk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4497/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4497/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Q.1  Limitations regarding spectral decomposition in this work.\n\nThank you for your feedback on our use of spectral decomposition. We have addressed this shared concern in our \"global\" response.\n\n\n> Q.2 I am a little confused with the so-called supervised setting.\n\nThank you for your inquiry about the \"supervised setting\" in our experiments. \n\n**(1)** In line with established experimental protocols in transductive node classification, as referenced in prior works [1,2,3], our approach utilizes all node features (train, validation, and test) while only incorporating training node labels during the training phase. This method is a standard practice in the field, aimed at evaluating GNNs in node-level tasks. \n\n**(2)** To further evaluate models comprehensively under varying levels of supervision, we implemented two distinct data splitting strategies: a sparse splitting (2.5%/2.5%/95%) and a dense splitting (60%/20%/20%) for the training, validation, and testing sets, respectively. Consistent with the terminology used in [3], we refer to these tasks as \"semi-supervised\" and \"full-supervised\" node classification. This distinction is based on the degree of supervision each setting entails.\n\n[1] Chien, Eli, et al. \"Adaptive universal generalized pagerank graph neural network.\" ICLR, 2021.\n\n[2] He, Mingguo, et al. \"Bernnet: Learning arbitrary graph spectral filters via bernstein approximation.\" NeurIPS, 2021.\n\n[3] He, Mingguo, et al. \"Convolutional neural networks on graphs with chebyshev approximation, revisited.\" NeurIPS, 2022.\n\n\n\n> Q.3  Theoretically, as the adapted new graph can express the power series of graph Laplacian, the most straightforward advantage of SAF, against existing spectral GNNs that use truncated polynomial approximation, is the capability of fitting specific graph filters.\n\n\nThanks for your constructive feedback and the insightful suggestion to incorporate experiments on fitting specific filters. We appreciate this opportunity to clarify the scope and focus of our work.\n\n**(1) Essence of the Adapted New Graph:** In our theoretical framework, the adapted new graph is fundamentally anchored in the spatial domain. This spatial orientation plays a pivotal role in making spectral GNNs interpretable. While it is true that our new graph expresses the infinite power series of the graph Laplacian (more so of the original adjacency matrix $\\hat{\\mathbf{A}}$), this aspect mainly highlights its non-local nature in the vertex domain. This non-locality is crucial for our approach to capture complex node relations, but it does not intrinsically boost expressiveness in the spectral domain.\n\n**(2) Enhancement of Spectral GNNs by SAF:** Our SAF framework is designed to address a key limitation of existing spectral GNNs -- the oversight of global vertex dependencies caused by truncated polynomials. By employing spatial aggregation on the non-local new graph, SAF adeptly captures long-range dependencies between nodes, a feature we have rigorously demonstrated in our experiments. These findings demonstrate SAF's effectiveness in handling complex graph structures, particularly in scenarios like heterophilic graphs where global interactions are essential.\n\n**(3) Comparison in Fitting Specific Filters:** Addressing the spectral domain challenge of fitting specific filters, it's noteworthy that both our SAF and BernNet utilize Bernstein polynomials as their spectral filter functions. This commonality implies that any experimental results in this regard would likely be identical for both frameworks. Hence, we chose not to include this specific task in our experiments. The congruence in the outcomes also underscores that our framework aligns with the capabilities of existing approaches in the spectral domain.\n\n\n> Q.4 Could you explain figure 1 for me?\n\nThanks for your question regarding Figure 1 in our paper. We are pleased to provide further clarification on its content and how it supports our conclusions.\n\n**(1)** In Figure 1, the x-axis represents the geodesic distance between node pairs in the original graph. This distance is calculated as the number of hops in the shortest path between the vertices. Concurrently, the y-axis quantifies the frequency of node pairs (of various types) being connected in the new graph.\n\n**(2)** Using the Minesweeper dataset in Figure 1, where the longest geodesic distance is 99, as an example, we observe that node pairs from the original graph, regardless of their distance, are often directly connected in the new graph (indicated by a frequency greater than 0). This finding is crucial as it demonstrates the capability of the new graph to establish direct links among nodes that originally require multiple hops to do so.\n\n**(3)** To enhance clarity, we have revised the caption of Figure 1 as \"Distributions of adjacent nodes in the new graph based on their geodesic distance in the original graph. It is seen that nodes, distant in the original graph, can be connected in the new graph.\"."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4497/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700239546688,
                "cdate": 1700239546688,
                "tmdate": 1700239546688,
                "mdate": 1700239546688,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LCtOv0FSEx",
                "forum": "zI6mMl7UmW",
                "replyto": "aU1HIVQ5dk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4497/Reviewer_k2v7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4497/Reviewer_k2v7"
                ],
                "content": {
                    "title": {
                        "value": "Discussion"
                    },
                    "comment": {
                        "value": "Thanks for your detailed explanation! I also read your general response. I appreciate the analysis and the connections established in this paper. However, when it comes to instantiate a concrete GNN architecture, it requires the eigen-decomposition, which is unaffordable for even graph of moderate size. This is not a severe issue for a research paper in my opinion, but there would be a lot of interesting design/variants for spectral GNNs if such a computation is allowed. In this sense, I tend to keep the rating unchanged, while raising contribution to 3."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4497/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700555077596,
                "cdate": 1700555077596,
                "tmdate": 1700555142715,
                "mdate": 1700555142715,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NSQKhlSbVC",
            "forum": "zI6mMl7UmW",
            "replyto": "zI6mMl7UmW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4497/Reviewer_wm9b"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4497/Reviewer_wm9b"
            ],
            "content": {
                "summary": {
                    "value": "The paper examines spectral GNNs from a spatial perspective, uncovering their interpretability in the spatial domain. It reveals that spectral GNNs implicitly transform the original graph into an adapted version, incorporating non-locality and signed edge weights for label consistency. Based on these insights, the authors propose a Spatially Adaptive Filtering framework that enhances spectral GNNs by leveraging the adapted graph structure for non-local aggregation. The SAF framework addresses issues related to long-range dependencies and graph heterophily by considering global node similarity and dissimilarity. Extensive experiments on 13 node classification benchmarks demonstrate the superiority of the proposed SAF framework over existing models. This work provides valuable understanding of spectral GNNs in the spatial domain and offers a promising approach for improving graph representation learning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tConnect spatial and spectral GNNs. Spectral filters modify the original graph into a new graph showing nice non-locality.\n2.\tIt proposes a novel Spatially Adaptive Filtering (SAF) framework. SAF can balance between spectral and spatial features. As a result, it can mitigate two problems of GNNs: long-range dependencies and graph heterophily.\n3.\tIt provides an analysis about the newly adapted graph. There are two properties: Non-locality and Discerning Label Consistency, which alleviates the long-range dependency and heterophily problems in graphs.\n4.\tComprehensive experiments show that the proposed SAF can promote the performance of BernNet on various datasets."
                },
                "weaknesses": {
                    "value": "1.\tSAF is only compatible with BernNet due to its assumption of non-negative filtering. As a result, it raises questions about whether the observed good performance is attributable to BernNet or SAF?\n\n2.\tWhen compared to other spectral methods, SAF requires eigen decomposition, which incurs a time complexity of O(n^3). This is a computational cost that exceeds that of other baseline methods that does not need eigen decomposition (BernNet, ChebNetII)."
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4497/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698643722873,
            "cdate": 1698643722873,
            "tmdate": 1699636425719,
            "mdate": 1699636425719,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TiLgyfVkBl",
                "forum": "zI6mMl7UmW",
                "replyto": "NSQKhlSbVC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4497/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4497/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Q.1  SAF is only compatible with BernNet due to its assumption of non-negative filtering. As a result, it raises questions about whether the observed good performance is attributable to BernNet or SAF?\n\n\nThank you for your thoughtful feedback regarding the compatibility of our SAF framework with BernNet. We appreciate the opportunity to clarify and expand on this important aspect of our work.\n\n**(1)** As you rightly pointed out, SAF indeed requires non-negative graph filters, which we discuss in Section 5 of our paper. However, we would like to highlight that SAF's compatibility extends beyond BernNet. **Theoretically, SAF is also applicable to other models like ChebNetII [1], a recent Spectral GNN variant based on Chebyshev interpolation, by reparameterizing its learnable parameters to be non-negative.** To substantiate this theoretical proposition, we have conducted additional experiments with ChebNetII, incorporating our SAF framework. These experiments yielded encouraging results as shown in the following table, proving the potential of SAF in enhancing models other than BernNet. \n\nFull-supervised node classification accuracies (\\%)\n|              |   Cham.            |   Squi.            |   Texas            |   Corn.            |   Actor            |   Cora             |   Cite.            |   Pubm.            |\n|--------------|--------------------|--------------------|--------------------|--------------------|--------------------|--------------------|--------------------|--------------------|\n|   ChebNetII  |   71.37$\\pm$1.01   |   57.72$\\pm$0.59   |   93.28$\\pm$1.47   |   92.30$\\pm$1.48   |   41.75$\\pm$1.07   |   88.71$\\pm$0.93   |   80.53$\\pm$0.79   |   88.93$\\pm$0.29   |\n|   SAF-Cheb   |   74.97$\\pm$0.66   |   64.06$\\pm$0.59   |   94.43$\\pm$1.81   |   92.62$\\pm$2.13   |   42.65$\\pm$1.01   |   89.56$\\pm$0.64   |   80.68$\\pm$0.68   |   91.27$\\pm$0.34   |\n|   Improv.    |   3.60             |   6.34             |   1.15             |   0.32             |   0.90             |   0.85             |   0.15             |   2.34             |\n\n**(2)** While SAF, as detailed in our paper, does build upon the foundational structure of BernNet, **its distinct feature is the inclusion of non-local aggregation, aimed at capturing long-range dependencies and addressing graph heterophily.** This aspect is crucial for advanced graph learning and is not a native feature of BernNet. Our experimental results, as detailed in Tables 1, 2, and 4, clearly show that the enhancements brought by SAF are distinct and significant, with performance improvements of up to 15.37\\%. These results underscore the unique contributions of SAF, reaffirming its significance beyond the inherent capabilities of BernNet.\n\n[1] He, Mingguo, et al. \"Convolutional neural networks on graphs with chebyshev approximation, revisited.\" NeurIPS, 2022.\n\n> Q.2  When compared to other spectral methods, SAF requires eigen decomposition, which incurs a time complexity of O(n 3). This is a computational cost that exceeds that of other baseline methods that does not need eigen decomposition (BernNet, ChebNetII).\n\nThanks for raising the question regarding our explict use of spectral decomposition. As this concern was shared among multiple reviewers. we've addressed it in the \"global\" response for a thorough explanation."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4497/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700239048504,
                "cdate": 1700239048504,
                "tmdate": 1700239048504,
                "mdate": 1700239048504,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LdZ9KVAI2Z",
                "forum": "zI6mMl7UmW",
                "replyto": "TiLgyfVkBl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4497/Reviewer_wm9b"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4497/Reviewer_wm9b"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your rebuttal. Most of my concerns have been solved. I will keep my overall score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4497/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710072402,
                "cdate": 1700710072402,
                "tmdate": 1700710072402,
                "mdate": 1700710072402,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NxLrq1MiSm",
            "forum": "zI6mMl7UmW",
            "replyto": "zI6mMl7UmW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4497/Reviewer_WZnB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4497/Reviewer_WZnB"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores the significance of spectral graph neural networks (GNNs) in the spatial domain, highlighting their role in capturing non-local information and label consistency. To address limitations associated with fixed-order polynomials, the authors introduce the Spatially Adaptive Filtering (SAF) framework, enabling flexible integration of spectral and spatial features. SAF overcomes issues related to truncated polynomials, enhancing the model's ability to handle long-range dependencies and graph heterophility. Experimental results demonstrate substantial improvements, with SAF outperforming other spectral GNNs on average across various benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) In this paper, the concept of bridging spectral GNNs and spatial GNNs via a graph optimization framework is a novel and innovative idea.\n2) The SAF framework introduced in this paper enhances the performance of BernNet, surpassing the performance of other baseline models in scenarios involving heterophilic graphs.\n3) Interestingly, the presence of negative edges in the new graph can provide insights into label consistency within heterophilic graphs.\n4) The experiments conducted in this paper are impressively comprehensive, encompassing a wide array of datasets, baseline comparisons, ablation studies, and visualizations."
                },
                "weaknesses": {
                    "value": "1. My main concern is the use of explicit eigndecomposition, which I believe is unacceptable in spectral GNNs.\n- The time and space complexity of eigendecomposition for an $n$-dimensional square matrix is $O(n^3)$ and $O(n^2)$, respectively, severely limiting scalability in practice. Even as a preprocessing step, it becomes challenging to execute on graphs with millions of nodes and edges.\n- The primary advantage of spectral GNNs based on polynomials is their ability to avoid the need for eigendecomposition. If eigenvectors were readily available, the design of spectral GNNs would become trivial, rendering polynomial-based approaches unnecessary.\n- While certain works, such as Specformer, have used eigendecomposition, I believe their practical significance is quite limited.\n- Although the authors have conducted practical complexity experiments, preprocessing time and space complexity have not been included.\n\n2. Opting for the Bernstein basis as the backbone may not represent the optimal choice. On the one hand, ChebNetII can also guarantee filter non-negativity, and on the other hand, Chebyshev interpolation boasts lower complexity in comparison to Bernstein approximation."
                },
                "questions": {
                    "value": "1) Please refer to the aforementioned weaknesses.\n2) My main concern is the practicality of explicit eigendecomposition, and I would welcome a discussion between the author and other reviewers on this issue."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4497/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4497/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4497/Reviewer_WZnB"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4497/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698656738968,
            "cdate": 1698656738968,
            "tmdate": 1700488611738,
            "mdate": 1700488611738,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KAwafsWkvl",
                "forum": "zI6mMl7UmW",
                "replyto": "NxLrq1MiSm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4497/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4497/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Q.1  Computational Concerns about the use of explicit eigndecomposition.\n\nThank you for raising this important question. We acknowledge that this query was shared among multiple reviewers. To provide a comprehensive response, we have elaborated on this topic in our \"global\" response section.\n\n\n\n> Q.2  Opting for the Bernstein basis as the backbone may not represent the optimal choice. On the one hand, ChebNetII can also guarantee filter non-negativity, and its Chebyshev interpolation boasts lower complexity in comparison to Bernstein approximation.\n\nThank you for your insightful comments regarding our choice of the Bernstein basis in our SAF framework. Your comment has prompted a valuable extension of our analysis, which can be found in the Appendix G of our revised manuscript.\n\n**(1) Experimenting with ChebNetII as Base Model:** We sincerely apologize for the initial omission in not including experiments with ChebNetII as a base model for SAF. We have now conducted additional experiments with ChebNetII as a base model for SAF (SAF-Cheb), alongside our original implementation using BernNet (SAF-Bern). The results, which we present in the subsequent table, illustrate that SAF indeed enhances ChebNetII\u2019s performance.\n\nFull-supervised node classification accuracies (\\%)\n|              |   Cham.            |   Squi.            |   Texas            |   Corn.            |   Actor            |   Cora             |   Cite.            |   Pubm.            |\n|--------------|--------------------|--------------------|--------------------|--------------------|--------------------|--------------------|--------------------|--------------------|\n|   BernNet    |   68.53$\\pm$1.68   |   51.39$\\pm$0.92   |   92.62$\\pm$1.37   |   92.13$\\pm$1.64   |   41.71$\\pm$1.12   |   88.51$\\pm$0.92   |   80.08$\\pm$0.75   |   88.51$\\pm$0.39   |\n|   SAF-Bern   |   75.30$\\pm$0.96   |   63.63$\\pm$0.81   |   94.10$\\pm$1.48   |   92.95$\\pm$1.97   |   42.93$\\pm$0.79   |   89.80$\\pm$0.69   |   80.61$\\pm$0.81   |   91.49$\\pm$0.29   |\n|   Improv.    |   6.77             |   12.24            |   1.48             |   0.82             |   1.22             |   1.29             |   0.53             |   2.98             |\n|   ChebNetII  |   71.37$\\pm$1.01   |   57.72$\\pm$0.59   |   93.28$\\pm$1.47   |   92.30$\\pm$1.48   |   41.75$\\pm$1.07   |   88.71$\\pm$0.93   |   80.53$\\pm$0.79   |   88.93$\\pm$0.29   |\n|   SAF-Cheb   |   74.97$\\pm$0.66   |   64.06$\\pm$0.59   |   94.43$\\pm$1.81   |   92.62$\\pm$2.13   |   42.65$\\pm$1.01   |   89.56$\\pm$0.64   |   80.68$\\pm$0.68   |   91.27$\\pm$0.34   |\n|   Improv.    |   3.60             |   6.34             |   1.15             |   0.32             |   0.90             |   0.85             |   0.15             |   2.34             |\n\n**(2) Performance Comparison:** Interestingly, SAF-Bern slightly outperforms SAF-Cheb in most datasets, and the improvement margin with SAF-Bern is notably larger. This may be due to the $g_ \\phi(\\lambda) \\leq 1$ constraint in our SAF (see Section 5), where filter functions need to be rescaled by their maximum values on the graph spectrum. For Bernstein polynomials, this maximum is simply the maximum polynomial coefficient ($\\max \\{\\phi_k\\}_ {k=0}^K$), as per our Proposition 3. In contrast, the best theoretical upper bound for Chebyshev polynomials is $\\sum_ {k=0}^K |\\phi_ k|$, which is less precise. This distinction potentially affects the new graph's construction, thereby limiting model performance. We plan to address this issue in future work.\n\n**(3) Empirical Complexity Studies:** We acknowledge that Chebyshev interpolation boasts greater efficiency over Bernstein approximation in theory. However, our empirical studies indicate that BernNet and SAF-Bern generally demand less training time compared to ChebNetII and SAF-Cheb, as evidenced in the table below. This could be due to ChebNetII requiring additional computations for the values w.r.t. $(K+1)^2$ Chebyshev nodes.\n\nAverage total running time (s).\n|               |   Cham.  |   Squi.  |   Texas  |   Corn.  |   Actor  |   Cora   |   Cite.  |   Pubm.  |\n|---------------|----------|----------|----------|----------|----------|----------|----------|----------|\n|   BernNet     |   8.36   |   13.74  |   3.92   |   4.16   |   4.88   |   5.24   |   5.52   |   6.06   |\n|   ChebNetII   |   22.82  |   30.73  |   11.47  |   9.64   |   14.88  |   19.96  |   16.14  |   36.91  |\n|   SAF-Bern    |   11.55  |   18.78  |   4.38   |   4.70   |   5.36   |   6.04   |   6.12   |   18.43  |\n|   SAF-Cheb    |   19.29  |   19.87  |   8.31   |   7.97   |   13.27  |   10.65  |   19.22  |   39.94  |"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4497/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700238918409,
                "cdate": 1700238918409,
                "tmdate": 1700238918409,
                "mdate": 1700238918409,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KOY7yKJNpX",
                "forum": "zI6mMl7UmW",
                "replyto": "KAwafsWkvl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4497/Reviewer_WZnB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4497/Reviewer_WZnB"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for the author's responses and the extra experimental results. I believe the method proposed in this paper is effective and sound. My main concern is whether it is reasonable to promote the use of eigendecomposition in current graph convolutional networks, considering the challenges of applying these methods to graphs with over a million nodes. However, given that many previous works have already employed eigendecomposition and taking into account the contributions made by this paper, I am inclined to increase my rating to 6 and lean towards accepting this paper."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4497/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700488587221,
                "cdate": 1700488587221,
                "tmdate": 1700488587221,
                "mdate": 1700488587221,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LUQZff6t8g",
            "forum": "zI6mMl7UmW",
            "replyto": "zI6mMl7UmW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4497/Reviewer_2DDa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4497/Reviewer_2DDa"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigated the relationship between spectral graph filters and spatial GNNs. The theoretical results suggest that the graph filter obtained in spectral space is able to construct a non-local new graph that contains global connectivity information. By combining the representation aggregating from this adapted new graph with the spectral embedding obtained from polynomial graph filters, a new framework SAF is proposed for node classification. \nThe experiments revealed what the adapt adjacency learned in spatial space. The ablation study shows the effectiveness of spectral filtering, and the node classification performance of SAF is competitive."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Overall, this paper is theoretically solid. The relationship between spatial and spectral is clearly uncovered with theoretical proofs.\n2. The experimental analysis of SAF can effectively demonstrate its capability for capturing heterophilous edges and punishing them with negative weights.\n3. The motivation for finding the correlation between spatial and spectral and combining them is clear and intriguing.\n4. The analysis of attention trends is consistent with intuition, showing the effectiveness of the proposed SAF."
                },
                "weaknesses": {
                    "value": "Although the paper includes impressive theoretical analyses, there are some weaknesses as below:\n\n1. The correlation between spectral and spatial domain graph learning has been exposed by several previous works although they do not emphasize this. For example, the work cited in your paper, Interpreting and unifying graph neural networks with an optimization framework, has derived both spectral-domain graph filters and spatial-domain GNN frameworks from the denoising optimization target; some other works like GCNII have also endeavored to analyze their spatial GNN model in the spectral domain.\n\n2. The authors claim that the polynomial filters are limited in K hop, but they do not discuss how this problem is addressed by the proposed model. In my view, it is hard to understand why the combination of graph filter and adapted adjacent also derived from this graph filter can address this limitation.\n\n3. The ablation study should exclude each component.\n\n4. This paper lacks of parameter sensitive analysis regarding $\\tau, \\eta, \\epsilon$, and the experimental analysis regarding different $L$ and $K$."
                },
                "questions": {
                    "value": "Please try to fix the questions stated above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4497/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4497/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4497/Reviewer_2DDa"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4497/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698861577366,
            "cdate": 1698861577366,
            "tmdate": 1699636425562,
            "mdate": 1699636425562,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hpmscTuPxI",
                "forum": "zI6mMl7UmW",
                "replyto": "LUQZff6t8g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4497/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4497/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (1/3)"
                    },
                    "comment": {
                        "value": "> Q.1  Previous works on the correlation between spectral and spatial domain graph learning.\n\nThanks for your insightful comments. We are glad to further clarify our work's position in relation to previous studies on the correlation between spectral and spatial domain graph learning.\n\n**(1)** In our original submission, we actually have acknowledged these pioneering researches, including but not limited to works [1,2,3], in exploring the congruencies between spectral and spatial GNNs. This discussion is present in the second paragraph of Section 1 and the \"Unified Viewpoints for GNNs\" subsection of Section 2. **Our work, distinct from these previous efforts, represents the first endeavor to delve into the interpretability of spectral GNNs from the spatial domain, examining the theoretical interplay between spectral filtering and spatial aggregation.** \n\n**(2)** Regarding GCNII [4], which utilizes spectral analysis to validate its anti-oversmoothing capability, we regret its omission in our earlier discussion and have now included it in Section 2 (highlighted in blue).\n\n**(3)** In our experiments, we have compared our SAF framework against relevant models including GNN-LF [1], GNN-HF [1], ADA-UGNN [2], FE-GNN [3], and GCNII [4], as detailed in Tables 1 and 2 of our manuscript. **These comparisons demonstrate that SAF significantly outperforms these models (including the two as you kindly mentioned)**, thereby underscoring our contribution to the existing literature in not only theoretical but also practical terms.\n\n[1] Zhu, Meiqi, et al. \"Interpreting and unifying graph neural networks with an optimization framework.\" WWW, 2021.\n\n[2] Ma, Yao, et al. \"A unified view on graph neural networks as graph signal denoising.\" CIKM, 2021.\n\n[3] Sun, Jiaqi, et al. \"Feature expansion for graph neural networks.\" ICML, 2023.\n\n[4] Chen, Ming, et al. \"Simple and deep graph convolutional networks.\" ICML, 2020.\n\n> Q.2 Why the combination of polynomail graph filter and the adapted adjacent also derived from this filter can address its limitation?\n\nThanks for your insightful comments. We appreciate the opportunity to clarify the capabilities of our SAF framework in addressing the limitations of spectral GNNs.\n\n**(1)** Most spectral GNNs are limited by their reliance on fixed-order polynomials for graph filter approximation, confining their effective propagation to within $K$ hops and thus restricting long-range dependency capture. Our research, however, has unveiled an intriguing aspect. We discovered that spectral filtering, while performed in the spectral domain, implicitly crafts a non-local new graph in the spatial domain. This new graph, free from the constraints of the original topology, uncovers deeper, previously unobserved node connections. **Despite this, domain barriers impede spectral GNNs from exploiting this non-locality existing in the spatial domain.** In response, our SAF framework is designed to enhance spectral filtering with auxiliary non-local aggregation. This combination effectively overcomes the truncated polynomials' limitations, enriching long-range dependency capture and graph information exploration.\n\n**(2)** This concept is extensively discussed in the last paragraph of Section 4.1 of our paper, where we elaborate on the distinct mechanics of the adapted new graph compared to conventional spectral filtering. Moreover, in Section 6, particularly in the \"Full-supervised Node Classification\" subsection, we provide empirical evidence and analytical insights that demonstrate SAF's superiority over other spectral GNNs, substantiating our methodology."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4497/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700238498351,
                "cdate": 1700238498351,
                "tmdate": 1700238498351,
                "mdate": 1700238498351,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bHBYM0tliz",
                "forum": "zI6mMl7UmW",
                "replyto": "LUQZff6t8g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4497/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4497/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (2/3)"
                    },
                    "comment": {
                        "value": "> Q.3 The ablation study should exclude each component.\n\nThank you for your valuable suggestion to augment our ablation analysis. Your guidance has been instrumental in enhancing the comprehensiveness of our study, and we have added detailed analysis in the revision (see Appendix F.3).\n\n**(1)** Our SAF framework primarily comprises three modules: \"Non-negative Spectral Filtering\" (Spec.), \"Non-local Spatial Aggregation\" (Spat.), and \"Node-wise Prediction Amalgamation\" (Amal.). We have already tested our SAF without the Spec. module in the \"Ablation Study\" subsection of Section 6. Additionally, the configuration of SAF w/o Spat. is effectively equivalent to the BernNet model, of which the performance can be observed in Tables 1 and 2.\n\n**(2)** Following your kind suggestion, we have now included experiments that ablate the Amal. module. We implemented this by removing the attention mechanism and equally blending predictions from different domains. To provide a comprehensive view, all ablation results are presented in the following table. These results illustrate that the omission of any module generally results in a noticeable decrease in performance, affirming the effectiveness of our design. Interestingly, for the Amal. module, some datasets like Cham. and Squi. exhibit only a modest performance decline. This observation aligns with our observation that their optimal attention values are close to an even split, as suggested in Figures 3(b) and 8(a).\n\nAblation study of SAF framework regarding \u201cNode-wise Prediction Amalgamation\u201d (Amal.), \u201cNon-local Spatial Aggregation\u201d (Spat.) and \u201cNon-negative Spectral Filtering\u201d (Spec.) modules.\n|                  |   Cham.  |   Squi.  |   Texas  |   Corn.  |   Actor  |   Cora   |   Cite.  |   Pubm.  |\n|------------------|----------|----------|----------|----------|----------|----------|----------|----------|\n|   SAF-$\\epsilon$  |   74.84  |   64.00  |   94.75  |   93.28  |   42.98  |   89.87  |   81.45  |   91.52  |\n|   SAF            |   75.30  |   63.63  |   94.10  |   92.95  |   42.93  |   89.80  |   80.61  |   91.49  |\n|   SAF w/o Amal.  |   75.01  |   62.62  |   89.18  |   86.07  |   41.53  |   88.80  |   80.37  |   91.24  |\n|   SAF w/o Spec.  |   73.55  |   55.70  |   90.49  |   88.20  |   41.06  |   88.03  |   78.87  |   90.12  |\n|   SAF w/o Spat.  |   68.53  |   51.39  |   92.62  |   92.13  |   41.71  |   88.51  |   80.08  |   88.51  |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4497/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700238652917,
                "cdate": 1700238652917,
                "tmdate": 1700238652917,
                "mdate": 1700238652917,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vdVUBuj0nt",
                "forum": "zI6mMl7UmW",
                "replyto": "LUQZff6t8g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4497/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4497/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (3/3)"
                    },
                    "comment": {
                        "value": "> Q.4 This paper lacks of parameter analysis regarding $\\tau, \\eta, \\epsilon$, $L$ and $K$.\n\nThanks for your insightful feedback regarding the sensitivity analysis of hyper-parameters in our model. We regret the initial omission of this crucial analysis in our experiments. To address this, we have now conducted a comprehensive analysis of our model's behavior across a wide range of hyper-parameter choices, specifically for $\\tau, \n\\eta, \\epsilon$, and $L$. More detailed results and analysis have been added in the revision (see Appendix H).\n\n**(1)** Regarding the parameter $K$, which represents the number of polynomial orders in the spectral filter, **we aligned with existing literature and fixed it at 10 in our SAF model.** This choice reflects our focus on the novel aspects of SAF rather than on parameters well-explored in previous works.\n\n**(2)** To provide a clear illustration of our findings, we have included results from two datasets, Texas and Cora, in the following tables as an typical example. **As can be observed, our SAF model exhibits robust stability across a broad range of parameter values.** For instance, promising performance can be obtained by selecting both $\\tau$ and $\\eta$ values from {$0.1, 0.2, ..., 1$}. This interval aligns with the parameter tuning strategy we described in Appendix E.3.\n\n**(3)** For the non-local aggregation layer number $L$, a noticeable decline in model performance is observed when $L$ exceeds 10. This is attributed to the non-local nature of our new graph, which facilitates efficient information exchange between nodes. Exceeding a certain number of layers may potentially lead to oversmoothing, where there is an overemphasis on global information, thus degrading model performance. However, **choosing the number of layers within a reasonable range generally ensures consistent and impressive model performance**, as verified in Figures 9p-9t of our revised manuscript.\n\nSensitivity analysis of parameter $\\tau$ in node classification accuracy\n|          |   1e-3  |   1e-2  |   0.1    |   0.5    |   1      |   10     |   100    |\n|----------|----------|----------|----------|----------|----------|----------|----------|\n|   Texas  |   90.98  |   91.80  |   92.95  |   94.59  |   93.61  |   91.64  |   88.52  |\n|   Cora   |   80.84  |   81.41  |   88.57  |   89.82  |   89.57  |   85.24  |   38.54  |\n\nSensitivity analysis of parameter $\\eta$\n|          |   1e-3  |   1e-2  |   0.1    |   0.5    |   1      |\n|----------|----------|----------|----------|----------|----------|\n|   Texas  |   90.00  |   91.80  |   93.11  |   94.75  |   94.59  |\n|   Cora   |   88.72  |   88.93  |   89.18  |   89.80  |   89.52  |\n\nSensitivity analysis of parameter $\\epsilon$\n|          |   0      |   1e-5  |   1e-4  |   1e-3  |   1e-2  |   0.1    |\n|----------|----------|----------|----------|----------|----------|----------|\n|   Texas  |   94.10  |   94.10  |   94.26  |   94.75  |   93.93  |   93.61  |\n|   Cora   |   89.80  |   89.72  |   89.87  |   89.69  |   89.41  |   89.11  |\n\nSensitivity analysis of parameter $L$\n|          |   1      |   5      |   10     |   20     |   30     |\n|----------|----------|----------|----------|----------|----------|\n|   Texas  |   93.44  |   94.75  |   92.95  |   91.80  |   91.15  |\n|   Cora   |   89.06  |   89.87  |   88.77  |   86.04  |   83.61  |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4497/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700238746430,
                "cdate": 1700238746430,
                "tmdate": 1700238746430,
                "mdate": 1700238746430,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uSfKruBrdo",
                "forum": "zI6mMl7UmW",
                "replyto": "LUQZff6t8g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4497/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4497/Authors"
                ],
                "content": {
                    "title": {
                        "value": "As the discussion period is drawing to a close, we would be appreciative to receive your feedback."
                    },
                    "comment": {
                        "value": "Dear Reviewer 2DDa,\n\nWe are deeply grateful for your time and effort in reviewing our paper. As the discussion period is drawing to a close, we sincerely hope that our rebuttal has carefully addressed each of your comments.\n\nIn particular, we have clarified the unique position of our work in relation to prior studies on the correlation between spectral and spatial domain graph learning. Besides, we have elaborated on how our SAF framework overcomes certain limitations of current spectral GNNs. As you kindly suggested, additional experimental results, focusing on ablations and the sensitivity analysis of parameters, are also provided to enable a comprehensive analysis of SAF.\n\nIf you have any other comments or questions, we are happy to engage and provide further clarification. Thank you for your invaluable feedback and attention to our submission.\n\nBest regards,\n\n4497Authors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4497/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668714070,
                "cdate": 1700668714070,
                "tmdate": 1700668714070,
                "mdate": 1700668714070,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]