[
    {
        "title": "Self-Supervised Contrastive Forecasting"
    },
    {
        "review": {
            "id": "j0G6qvSjr9",
            "forum": "nBCuRzjqK7",
            "replyto": "nBCuRzjqK7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1868/Reviewer_hiP7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1868/Reviewer_hiP7"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses challenges of long-term forecasting by utilizes contrastive learning and an enhanced decomposition architecture specifically designed to address long-term variations. The key idea considers the global autocorrelation within the entire time series, enabling the creation of positive and negative pairs in a self-supervised manner. Experiments demonstrate that this approach outperforms baseline models on nine established long-term forecasting benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Contribution:\n\n- The authors deliver an intuitive and easy to implement technique based on newly defined \u2018global autocorrelation\u2019. A plus point for this technique also relates to its fast computing time and no extensive memory requirement.\n- The method is extensible and applicable to both uni and multi-variate datasets.\n- Well-rounded experiments are conducted with multiple datasets of different patterns (different autocorrelation patterns - Fig. 5).\n- The authors are well aware with the weakness of autocorrelation technique - which capture only linear variations.\n\nPresentation:\n\n- Nice visual explainations (e.g. Fig.1, 3)."
                },
                "weaknesses": {
                    "value": "- For circumstance of nearly stationary input sequence, the autocorrelation will produce stationary result and cause the collapse of Constrastive Learning framework.\n- The increasing complexity for long-term branch of decomposition framework might contribute to the better result of the whole proposed pipeline. A potential verification could be an additional ablation case of that architecture with a linear layer like the work of Zeng (2023).\n- While the authors are aware of the linear assumption of autocorrelation and suggest the potential use of different techniques for high-order one, this suggestions might be inapplicable for the lengthy global input sequences with densely use frequency. Any approaches investigating these non-linear correlation can consume much more resources compare to the current autocorrelation.\n- Some grammar and typos (e.g. TiemsNet - page 2)\n\nReference:\n\nAiling Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series\nforecasting? In Proc. the AAAI Conference on Artificial Intelligence (AAAI), 2023"
                },
                "questions": {
                    "value": "Please address my comments on weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1868/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1868/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1868/Reviewer_hiP7"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1868/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698514915154,
            "cdate": 1698514915154,
            "tmdate": 1700530421763,
            "mdate": 1700530421763,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VSgaTca2zN",
                "forum": "nBCuRzjqK7",
                "replyto": "j0G6qvSjr9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1868/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1868/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### W1: Collapse of our contrastive framework with stationary time-series\nWe thank reviewer hiP7 for the insightful comment regarding the potential challenges posed by nearly stationary input sequences in our contrastive learning framework.\nWe understand your concern that \"the nearly stationary sequences\" might lead to stationary autocorrelation results, which could, in turn, affect the contrastive learning mechanism.\n\nHowever, even in the nearly stationary sequences, the autocorrelation does not produce stationary results since stationary results in autocorrelation are theoretically possible only in the case of infinitely long time series.\nIn practice, we deal with finite samples from time series. In such cases, autocorrelation converges to zero, decreasing from 1 to 0 as the lag $\\tau$ increases even in \"the nearly stationary time series\". In other words, autocorrelation exhibits a non-stationary trend and shows a decreasing pattern. This is because the covariance with lagged series $Cov(S_{t+\\tau}, S_t) $ decreases as $\\tau$ increases when numerically calculating the autocorrelation.\nFor example, as shown in M-Figure 5, the autocorrelation results of four datasets converge regardless of their stationarity. \n\nAdditionally, the Weather time series, which is considered the most stationary among the tested time series according to the Augmented Dick-Fuller (ADF) test [[1]](https://www.nber.org/papers/t0130) as shown in statistics [[2]](https://proceedings.neurips.cc/paper_files/paper/2022/hash/4054556fcaa934b0bf76da52cf4f92cb-Abstract-Conference.html), also converges to 0 in a similar manner.\nNaturally, such stationarity implies small (or absence of) long-term variation.\nTherefore, even if our method is applied, as described in Dataset Analysis of Section 4.1, performance improvement could be marginal (3% reduced MSE).\nHowever, we believe there is an unequivocal difference between gaining only a slight performance improvement and \"collapse of contrastive learning\".\n(If reviewer hiP7 was trying to convey a different meaning than \"collapse\", we would be happy to further discuss this matter.)\n\n### W2: Increasing complexity of long-term branch in the decomposition architecture\nIncreasing the complexity of the long-term branch is essential for learning long-term representations, but it is not the only reason for the superior performance of our methodology.\nIn other words, even with the increased complexity, capturing long-term variation is not an easy task if the current framework only uses the forecasting loss.\nIt is essential to use the AutoCon loss to effectively learn long-term variation that leads to performance improvement.\nWe have already demonstrated the need for AutoCon loss through ablations in M-Figure 6 and M-Table 3.\nHowever, to fully address the reviewer's concerns, we conducted two additional ablation studies: increasing complexity in DLinear [[3]](https://ojs.aaai.org/index.php/AAAI/article/view/26317) and in our model.\n\nFirst, DLinear utilizes only a single linear layer for both long-term and short-term branches.\nWe increase the complexity of the long-term branch by stacking linear layers with an activation function.\nHowever, as evident in R-Table 3 below, even when stacking layers in the long-term branch, performance tends to decrease or remain similar.\nThis demonstrates that merely increasing the complexity of the long-term branch is not effective in the existing decomposition architecture.\n\n**R-Table 3: Increasing Complexity of Long-term Branch in DLinear in input-336-output-720 univariate setting**\n\n| **Model** | **\\# of Layers** | **ETTh1 (MSE)**   | **ETTh1 (MAE)**   | **ETTh2 (MSE)**   | **ETTh2 (MAE)**   | **Electricity (MSE)** | **Electricity (MAE)** |\n|------------|-------------------|-------------------|-------------------|-------------------|-------------------|-----------------------|-----------------------|\n| DLinear    | 1                 | **0.1780\u00b10.0054** | **0.3466\u00b10.0063** | **0.2929\u00b10.0140** | **0.4362\u00b10.0087** | 0.3067\u00b10.01544        | 0.4125\u00b10.0109         |\n|            | 2                 | 0.1993\u00b10.1964     | 0.3638\u00b10.2191     | 0.3170\u00b10.0488     | 0.4581\u00b10.0404     | 0.3775\u00b10.01574        | 0.4618\u00b10.0081         |\n|            | 3                 | 0.2793\u00b10.0678     | 0.4594\u00b10.0682     | 0.3008\u00b10.0046     | 0.4451\u00b10.0035     | **0.3057\u00b10.00902**    | **0.4097\u00b10.0065**     |\n|            | 4                 | 0.2760\u00b10.0663     | 0.4564\u00b10.0683     | 0.3015\u00b10.0031     | 0.4455\u00b10.0024     | 0.3165\u00b10.03838        | 0.4183\u00b10.0279         |\n\n* In the case of the 2 layers, there exist linear layers along with the temporal axis only (*i.e.,* the outputs shapes as follows: (B, I, C) > (B, D, C) > (B, O, C) where B is batch size, I is input length, D is hidden dimension, and O is output length)"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1868/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700471112985,
                "cdate": 1700471112985,
                "tmdate": 1700478083939,
                "mdate": 1700478083939,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Q3FePfM4zk",
                "forum": "nBCuRzjqK7",
                "replyto": "j0G6qvSjr9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1868/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1868/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Second, the following R-Table 4 demonstrates the change in performance based on the complexity of the long-term branch in our decomposition architecture. Without Autocon, our model may be slightly better or comparable to the second-best model.\nThe highest performance is achieved only when the AutoCon loss is employed. This further underscores the necessity of the AutoCon loss we proposed in order to accurately capture long-term variation.\n\n**R-Table 4: Increasing complexity of long-term branch of our model over three datasets in output-720 univariate setting. Other setting is the same to experiments as shown in M-Table 1.**\n\n| **Model**                 | **\\# of layers** | **AutoCon** | **ETTh1 (MSE)**  | **ETTh1 (MAE)**   |\n|---------------------------|-------------------|-------------|------------------|-------------------|\n| TimesNet (baseline best)  | 2                 | -           | 0.0834\u00b10.0024    | 0.2310\u00b10.0023     |\n| Ours                      | 1                 | X           | 0.0837\u00b10.0185    | 0.2372\u00b10.0294     |\n| Ours                      | 2                 | X           | 0.0910\u00b10.0188    | 0.2360\u00b10.0257     |\n| Ours                      | 3                 | X           | 0.0876\u00b10.0240    | 0.2351\u00b10.0303     |\n| Ours                      | 4                 | X           | 0.0918\u00b10.0194    | 0.2381\u00b10.0241     |\n| Ours (best)               | 1                 | O           | **0.0787\u00b10.002** | **0.2226\u00b10.0023** |\n\n\n| **Model**                 | **\\# of layers** | **AutoCon** | **ETTh2 (MSE)**   | **ETTh2 (MAE)**   |\n|---------------------------|-------------------|-------------|-------------------|-------------------|\n| TimesNet (baseline best)  | 2                 | -           | 0.2074\u00b10.0113     | 0.3703\u00b10.0155     |\n| Ours                      | 1                 | X           | 0.2023\u00b10.0230     | 0.3594\u00b10.0261     |\n| Ours                      | 2                 | X           | 0.2020\u00b10.0098     | 0.3525\u00b10.0078     |\n| Ours                      | 3                 | X           | 0.2036\u00b10.0265     | 0.3573\u00b10.0224     |\n| Ours                      | 4                 | X           | 0.2087\u00b10.0167     | 0.3605\u00b10.0130     |\n| Ours (best)               | 3                 | O           | **0.1771\u00b10.0393** | **0.3441\u00b10.0366** |\n\n\n\n| **Model**                 | **\\# of layers** | **AutoCon** | **Electricity (MSE)** | **Electricity (MAE)** |\n|---------------------------|-------------------|-------------|-----------------------|-----------------------|\n| DLinear (baseline best)   | 1                 | -           | 0.3067\u00b10.0154         | 0.4125\u00b10.0109         |\n| Ours                      | 1                 | X           | 0.2928\u00b10.1369         | 0.3978\u00b10.1009         |\n| Ours                      | 2                 | X           | 0.2889\u00b10.0330         | 0.3927\u00b10.0209         |\n| Ours                      | 3                 | X           | 0.2975\u00b10.0458         | 0.4049\u00b10.0481         |\n| Ours                      | 4                 | X           | 0.3089\u00b10.4737         | 0.4115\u00b10.0587         |\n| Ours (best)               | 2                 | O           | **0.2753\u00b10.0224**     | **0.3861\u00b10.0166**     |\n\n### W3: Applying our AutoCon with high-order and non-linear correlation\n\nAs reviewer hiP7 mentioned, thoroughly calculating high-order correlation across a very long time series can be computationally challenging.\nWhat we have claimed and demonstrated in this work, however, is the necessity of leveraging global autocorrelation for long-term predictions.\nThese two are independent topics, and we find it difficult to understand why the former is pointed out as a \"weakness\" of the latter.\nEven if one decides to combine the two somehow in the future (*e.g.* extend our work to incorporate high-order global correlation), there are many possibilities to explore. For example, one could try to reduce the computational complexity by leveraging domain knowledge of the specific dataset or the task (*e.g.* ignore the correlation between certain variables or certain time durations).\nAlternatively, one could propose to capture the global high-order correlation in a completely different manner than how we capture the global autocorrelation.\nTherefore, we find it unconvincing that the reviewer would use an independent problem as a weakness for the current submission.\n\n\n### W4: Typos\nThank the reviewer for pointing out the typos. We have corrected them in the revised manuscript.\n\n### References\n[[1]](https://www.nber.org/papers/t0130) Efficient tests for an autoregressive unit root, 1992    \n[[2]](https://proceedings.neurips.cc/paper_files/paper/2022/hash/4054556fcaa934b0bf76da52cf4f92cb-Abstract-Conference.html) Non-stationary Transformers: Exploring the Stationarity in Time Series Forecasting, NeurIPS'22  \n[[3]](https://ojs.aaai.org/index.php/AAAI/article/view/26317) Are Transformers Effective for Time Series Forecasting?, AAAI'23"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1868/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700471132244,
                "cdate": 1700471132244,
                "tmdate": 1700478104053,
                "mdate": 1700478104053,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EqQ6pu3j9n",
                "forum": "nBCuRzjqK7",
                "replyto": "Q3FePfM4zk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1868/Reviewer_hiP7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1868/Reviewer_hiP7"
                ],
                "content": {
                    "title": {
                        "value": "Additional Comment from Reviewer hiP7"
                    },
                    "comment": {
                        "value": "Dear Authors,\n\nThank you for making clarifications to my concerns.\nI can tell the Authors have made ample clarifications for my two first concerns.\n\nRegarding the third one, I agree that *leveraging global autocorrelation for long-term predictions* and *calculating high-order correlation* is two independent topics. However, what I want to clarify what I meant is that the way the Authors use linear autocorrelation for now in this work might limit the scopes of applications in the circumstance of that non-linear correlations represent.\n\nIn general, I quite satisfy with the answer, together with the extra experiment the authors conduct.\nI have updated my scores.\n\nBest,"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1868/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700530403679,
                "cdate": 1700530403679,
                "tmdate": 1700530403679,
                "mdate": 1700530403679,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3vsrBdpjkf",
            "forum": "nBCuRzjqK7",
            "replyto": "nBCuRzjqK7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1868/Reviewer_MDze"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1868/Reviewer_MDze"
            ],
            "content": {
                "summary": {
                    "value": "This paper points out existing approaches fail to capture the long-term variations that are partially caught within the short window. Based on the finding, this paper presents a novel approach for long-term time series forecasting by employing contrastive learning and an enhanced decomposition architecture. The contrastive loss incorporates global autocorrelation held in the whole time series, which facilitates the construction of positive and negative pairs in a self-supervised manner. The authors conducted extensive experiments on nine benchmarks and achieved superior performance compared to baseline models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* It is a good observation that long-term forecasting data exists long-term autocorrelations and existing methods fail to take into account.\n\n* By adding autocorrelation constrained contrastive loss, it reaches long-term variation consistency between windows, making it more explainable.\n\n* Extensive experiment results and ablation studies show the effectiveness of the method."
                },
                "weaknesses": {
                    "value": "* Capturing long-term variation via the contrastive loss part is separated from the forecasting mechanism. Good finding but have no further design for architectures based on this finding. The forecasting module is a simple dual-head forecaster and already achieves good performance. \n\n* Lack of performance comparisons of different self-supervised objectives. \n\n* 14 baselines comparison is claimed in the abstract, only 7 are found.The performance improvement seems marginal. Extending the forecasting window too much has little practical meaning."
                },
                "questions": {
                    "value": "Could you provide training protocols and the default value of the contrastive loss weight hyperparameter?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics review needed."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1868/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698810390575,
            "cdate": 1698810390575,
            "tmdate": 1699636117060,
            "mdate": 1699636117060,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "84k5pdI0H0",
                "forum": "nBCuRzjqK7",
                "replyto": "3vsrBdpjkf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1868/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1868/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### W1: The forecasting module is a simple dual-head forecaster and already achieves good performance\n\nIn response to the reviewer's concern that \"The forecasting module is a simple dual-head forecaster and already achieves good performance\", we have already demonstrated through an ablation study that the performance decreases when AutoCon is not used as shown in M-Table 3 and (b) of M-Figure 6. \nTherefore, without our contribution, the AutoCon loss, the dual-head forecaster fails to achieve good performance in long-term forecasting. \n\nAnd the reviewer claimed, \"Capturing long-term variation via the contrastive loss part is separated from the forecasting mechanism\", but our long-term encoder is jointly optimized by both forecasting and the AutoCon loss.\nThis learning approach aligns with our initial proposal of the two-stream architecture, where the linear structure empowers short-term (motivated by recent linear models [[1]](https://ojs.aaai.org/index.php/AAAI/article/view/26317)), and the contrastive loss to capture the long-term variations.\nEmpirically, considering the performance decline observed in the ablation studies when excluding the AutoCon loss, we find it unconvincing that reviewer MDze claims the contrastive loss operates independently from the forecasting mechanism.\n\nFinally, our deliberate model design has indeed demonstrated superior long-term prediction performance compared to baseline models.\nTherefore, as desired by the reviewer, we can incorporate even more complex architectures based on these findings in future work."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1868/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470904678,
                "cdate": 1700470904678,
                "tmdate": 1700470904678,
                "mdate": 1700470904678,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8sM9zdHoqJ",
                "forum": "nBCuRzjqK7",
                "replyto": "3vsrBdpjkf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1868/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1868/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### W2:Lack of performance comparisons of different self-supervised objectives\nSince we used self-supervised learning (SSL) loss in addition to the forecasting loss, it seems reasonable to compare our methodology with approaches utilizing different self-supervised objectives.\nWe fully agree with this point and have already presented comparisons with three recent methodologies that employ various self-supervised losses in Section 4.3. However, to comprehensively address reviewer MDze's concern, we have designed and provided results for two possible self-supervised objectives based on HierCon [[2]](https://ojs.aaai.org/index.php/AAAI/article/view/20881) and SupCon [[3]](https://proceedings.neurips.cc/paper/2020/hash/d89a66c7c80a29b1bdbab0f2a1a94af8-Abstract.html) that can be incorporated into our two-stream model structure.\nHierCon induces the representations of two partially overlapped windows to be close to each other, while SupCon encourages the encoder to learn close representations for the windows with the same month label. \n\nThe two SSL objectives were tested with our model architecture, replacing only the AutoCon loss. As shown in R-Table 2 (we also added this in Appendix C.3), compared to the one without any SSL loss, HierCon shows a slight improvement in performance on the ETTh1 and ETTh2 for short-term predictions with a length of 96, but it performs worse in the long-term prediction, as it emphasizes only temporal closeness.\nSupCon leveraged monthly information beyond the window length, leading to performance improvements even in the long-term prediction.\nHowever, SupCon can only learn a single predefined periodicity, unlike AutoCon.\nConsequently, SupCon shows lower performance than AutoCon in learning the periodicities existing in the time series through autocorrelation.\nWe additionally demonstrated the performance of well-known SSL baselines that are compatible with our model architecture to address the concerns of reviewers as much as possible.\nHowever, if there are specific SSL methods that reviewer MDze wishes to see, we would appreciate any suggestions or proposals from reviewer MDze.\n\n**R-Table 2: Comparison with different self-supervised objectives in our redesigned architecture. Another setting is the same to the experiments as shown in M-Table 1**\n\n|                     | Dataset | ETTh1     |  |  |  | ETTh2     |  |  |  | Electricity |           |           |           |\n|---------------------|---------|-----------| --- | --- | --- |-----------| --- | --- | --- |-------------|-----------|-----------|-----------|\n| **Model**           | **Output**  | **96**        | **720** | **1440** | **2160** | **96**        | **720** | **1440** | **2160** | **96**          | **720**       | **1440**      | **2160**      |\n| **Ours w/o SSL**    | **MSE** | 0.061     | 0.082 | 0.096 | 0.130 | 0.147     | 0.214 | 0.213 | 0.236 | 0.206       | 0.289     | 0.363     | 0.419     |\n|                     | **MAE** | 0.190     | 0.226 | 0.245 | 0.286 | 0.285     | 0.375 | 0.365 | 0.372 | 0.322       | 0.393     | 0.460     | 0.503     |\n| **Ours w/ HierCon** | **MSE** | 0.059     | 0.090 | 0.121 | 0.145 | 0.132     | 0.221 | 0.221 | 0.263 | 0.223       | 0.313     | 0.380     | 0.500     |\n|                     | **MAE** | 0.187     | 0.240 | 0.276 | 0.306 | 0.279     | 0.379 | 0.378 | 0.407 | 0.339       | 0.407     | 0.474     | 0.544     |\n| **Ours w/ SupCon**  | **MSE** | 0.056     | 0.082 | 0.092 | 0.091 | 0.125     | 0.185 | 0.199 | 0.215 | 0.209       | 0.279     | 0.351     | 0.408     |\n|                     | **MAE** | 0.184     | 0.229 | 0.242 | 0.237 | 0.270     | 0.349 | 0.360 | 0.371 | 0.326       | 0.388     | 0.451     | 0.489     |\n| **Ours w/ AutoCon** | **MSE** | **0.056** | **0.079** | **0.079** | **0.074** | **0.124** | **0.177** | **0.176** | **0.198** | **0.196**   | **0.275** | **0.338** | **0.380** |\n|                     | **MAE** | **0.182** | **0.223** | **0.225** | **0.215** | **0.269** | **0.344** | **0.340** | **0.358** | **0.313**   | **0.386** | **0.441** | **0.481** |"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1868/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470929054,
                "cdate": 1700470929054,
                "tmdate": 1700478048205,
                "mdate": 1700478048205,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YPMCx4TiCH",
                "forum": "nBCuRzjqK7",
                "replyto": "3vsrBdpjkf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1868/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1868/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### W3:Experiment setting and results\n* 1: 14 baselines, but seven are found  \nIn our study, we analyzed 14 distinct baseline models, which are distributed across different sections of our results due to the breadth of our analysis.\nSpecifically, we evaluated seven models in the context of univariate forecasting, as detailed in M-Table 1.\nAdditionally, four models were examined in the multivariate forecasting domain, as shown in M-Table 2, which includes models such as Crossformer and N-HITS that are specifically tailored to learning inter-variable dependencies.\nFurthermore, we included three time-series representation methods in our analysis, the results of which are presented in M-Table 4.\nWe apologize for causing confusion by stating the aggregate number as 14 in the abstract. In the revised version, we added the clue ``in multiple experiments'' to the abstract to prevent confusion.\n\n* 2: marginal performance improvement  \nWe understand that the reviewer's concerns about marginal performance improvement primarily stem from short-term prediction (Output-96) and the Weather dataset. \nHowever, our method contributes to capturing long-term variations beyond the window scope by leveraging global autocorrelation, thereby enhancing the performance of long-term predictions.\nTherefore, marginal performance improvement in the short-term setting is reasonably anticipated.\nIn relation to the Weather dataset, we already elaborated in Dataset Analysis of Section 4.1 on how performance improvement varies based on the degree of long-term variation in the time series.\nIn that analysis, we have already demonstrated that weather exhibits less long-term variation compared to other datasets, and we have explained that this could inevitably limit the performance improvement of our model. This analysis, rather, serves as significant evidence that our model is indeed operating as intended, validating our intuition that focused on the importance of long-term autocorrelation. In other words, it confirms that our model has behaved according to our intentions, affirming the accuracy of our emphasis on global autocorrelation.\nMost importantly, we believe stating \"Performance improvement seems marginal\" based on only a few numbers does not accurately capture the overall statistics.\nAs shown in M-Table 1, our method achieved the top performance 42 times across all benchmarks.\nOn average, it achieved a 12\\% reduction in MSE and a maximum reduction of 34\\% in MSE. \n\n\n* 3: practical meaning of extending window  \nContemporary time-series communities continue to propose numerous models and methods aimed at predicting the more distant future, with a focus on improving long-term performance.\nFor instance, Informer [[4]](https://ojs.aaai.org/index.php/AAAI/article/view/17325), in response to the real-world need for long-term forecasting, extended the forecasting length beyond the conventional 48 output length\u2014primarily evaluated in M4 [[5]](https://www.sciencedirect.com/science/article/pii/S0169207018300785?casa_token=d5VexiwRFBAAAAAA:pGKzZoJWfqMrB8ktzblyinqxUnZVjLu3JodaoHPihDy522ZTlhk1Hq2eYPiL2CraUhOepO60iHA); \nfurther expanded it to 720. To achieve both performance and computational efficiency in predicting a sequence of length 720, a huge number of models and methods have been proposed [[6](https://proceedings.neurips.cc/paper/2021/hash/bcc0d400288793e8bdcd7c19a8ac0c2b-Abstract.html), [7](https://proceedings.mlr.press/v162/zhou22g.html), [8](https://arxiv.org/abs/2210.02186), [9](https://arxiv.org/abs/2211.14730), [10](https://openreview.net/forum?id=zt53IDUR1U),].\nAt the same time, ReVIN [[11]](https://openreview.net/forum?id=cGDAkQo1C0p) has successfully extended the prediction length up to 960, demonstrating that longer predictions become feasible when addressing and resolving the issue of distribution shift.\nMost recently, a new concurrent effort [[12]](https://openreview.net/forum?id=y08bkEtNBK) has emerged with the aim of further increasing the output length up to 2880.\nGiven the collective efforts of numerous researchers, extending our input to 2160 is not merely a setting to demonstrate superiority in performance but is rooted in genuine practical significance and necessity.\nIf reviewer MDze, however, can present a concrete argument as to why all these efforts have little practical meaning, we are more than happy to further discuss this matter.\nLong-term prediction is our core research interest, and if there is any convincing counter-argument we were unaware of, we would like to take it into account in defining our future research."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1868/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470990141,
                "cdate": 1700470990141,
                "tmdate": 1700470990141,
                "mdate": 1700470990141,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vqXrYXVe1D",
                "forum": "nBCuRzjqK7",
                "replyto": "3vsrBdpjkf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1868/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1868/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Q1:Implementation details\n* 1:Training protocols  \nOur training protocol is identical to conventional training methods, except for the inclusion of AutoCon as an additional loss, in addition to the forecasting loss. To clarify this further, we also present the [algorithm](https://tinyurl.com/training-algorithm) and added it to the Appendix A.1 of revision.\n\n* 2:Weight hyperparameter of AutoCon  \nWe already have shown the analysis of the optimal values and sensitivity for the contrastive loss weight hyperparameter for each dataset. However, due to the page limit, we included it in M-Figure 9 of Appendix A.6. We would appreciate it if the reviewer could refer to the appendix once again. \n\n\n### References\n[[1]](https://ojs.aaai.org/index.php/AAAI/article/view/26317) Are Transformers Effective for Time Series Forecasting?, AAAI'23  \n[[2]](https://ojs.aaai.org/index.php/AAAI/article/view/20881) TS2Vec: Towards Universal Representation of Time Series, AAAI'22  \n[[3]](https://proceedings.neurips.cc/paper/2020/hash/d89a66c7c80a29b1bdbab0f2a1a94af8-Abstract.html) Supervised Contrastive Learning, NeurIPS'20    \n[[4]](https://ojs.aaai.org/index.php/AAAI/article/view/17325) Informer: Beyond efficient transformer for long sequence time-series forecasting, AAAI'21    \n[[5]](https://www.sciencedirect.com/science/article/pii/S0169207018300785?casa_token=d5VexiwRFBAAAAAA:pGKzZoJWfqMrB8ktzblyinqxUnZVjLu3JodaoHPihDy522ZTlhk1Hq2eYPiL2CraUhOepO60iHA) The M4 Competition: Results, findings, conclusion and way forward, International Journal of Forecasting'18   \n[[6]](https://proceedings.neurips.cc/paper/2021/hash/bcc0d400288793e8bdcd7c19a8ac0c2b-Abstract.html) Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting, NeurIPS'21  \n[[7]](https://proceedings.mlr.press/v162/zhou22g.html) FEDformer: Frequency enhanced decomposed transformer for long-term series forecasting, ICML'22  \n[[8]](https://arxiv.org/abs/2210.02186) TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis, ICLR'23  \n[[9]](https://arxiv.org/abs/2211.14730) A Time Series is Worth 64 Words: Long-term Forecasting with Transformers, ICLR'23  \n[[10]](https://openreview.net/forum?id=zt53IDUR1U) MICN: Multi-scale Local and Global Context Modeling for Long-term Series Forecasting, ICLR'23  \n[[11]](https://openreview.net/forum?id=cGDAkQo1C0p) Reversible Instance Normalization for Accurate Time-Series Forecasting against Distribution Shift, ICLR22  \n[[12]](https://openreview.net/forum?id=y08bkEtNBK) WITRAN: Water-wave Information Transmission and Recurrent Acceleration Network for Long-range Time Series Forecasting, NeurIPS'23"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1868/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700471014839,
                "cdate": 1700471014839,
                "tmdate": 1700471014839,
                "mdate": 1700471014839,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iEssvD8GKG",
            "forum": "nBCuRzjqK7",
            "replyto": "nBCuRzjqK7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1868/Reviewer_eBaK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1868/Reviewer_eBaK"
            ],
            "content": {
                "summary": {
                    "value": "This paper mainly presents a new contrastive objective in self-supervised representation learning in time series forecasting. Specifically, the paper argues that the trends extracted from a moving window are often long-term seasonal variations that cannot be captured by the window of smaller size. Therefore, the representations of windows within a mini-batch that are more similar are explicitly induced to get closer compared to other samples with lower correlations. The proposed contrastive loss is then added to a decomposition-based forecasting model as a regularization term. Experiments show impressive performance improvements over existing SOTA models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Overall the paper is well-written and easy to follow. The idea of global autocorrelation and decomposition is well-motivated and sufficiently grounded, and the argument that short-term trends are long-term variations is quite convincing.\n2. The empirical results on univariate long-term forecasting is very impressive.\n3. Extensive analysis support the effectiveness of the proposed method with abundant evidences"
                },
                "weaknesses": {
                    "value": "1. Most of the ambiguity comes from the global information in the proposed representation. The authors should elaborate on the details of the encoder to highlight how the similarity of representations $v_i, v_j$ differ from the linear autocorrelation $r(i, j)$ in order to justify the contrastive objective (3). See question (1). \n2. The plots of baseline models in figure 2 are not quite convincing. While the baseline models do not consider long-term correlations, it's counter-intuitive that the similarity of moving windows with various patterns stays almost constant as shown in the chart."
                },
                "questions": {
                    "value": "1. How is the timestamp-based features derived and incorporated into the encoder?\n2. Which dataset is used in Figure 6? Do you observe the same result on other datasets?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1868/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698903260897,
            "cdate": 1698903260897,
            "tmdate": 1699636116993,
            "mdate": 1699636116993,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TtF4W83Qe2",
                "forum": "nBCuRzjqK7",
                "replyto": "iEssvD8GKG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1868/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1868/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### W1:Details of the encoder to understand representation similarity\nWe sincerely apologize for any ambiguity caused by an insufficient explanation. We mainly focused on experiments and visualization results to enhance understanding, which unfortunately led us to neglect the encoder details that should have been included for clarity. Following the reviewer eBaK's helpful guidance, we first clarify how the timestamp-based features are derived and incorporated into the encoder (Question 1).\n\n* Q1: How are the timestamp-based features derived and incorporated into the encoder?  \nThe timestamp signifies the moment at which data is observed, and this moment is expressed in units of 'year,' 'month,' 'day,' and 'hour.'\nThe timestamp provides a forecasting model with periodicity on a yearly, monthly, and daily basis as input, aiding the model in learning various business cycles.\nMost models use timestamps for long-term forecasting, commonly adding numerical values of periodic functions aligned with each unit's cycle from the timestamp as features to the input sequence.\nSpecifically, if we use four units (*i.e.,* month, day, weekday, and hour), we get a timestamp-based feature $f_{time} \\in \\mathbb{R}^{I \\times 4} $, which is indexed by $\\mathcal{T}$ and is aligned with input sequence $\\mathcal{X} \\in \\mathbb{R}^{I \\times c}$ where $I$ denote the input sequence length and $c$ is the number of variables.\nThen, with a projection layer $W_{time}$ and $b_{time}$, the time embedding $e_{time} = W_{time} \\cdot f_{time} + b_{time} \\in \\mathbb{R}^{I \\times d}$ is added to value embedding $e_{value} = W_{value}\\cdot \\mathcal{X}+b_{value}$.\nFinally, the temporal encoder (*e.g.,* RNN, TCN, and Attention)  takes the embedding sequence $e = e_{value} + e_{time} \\in \\mathbb{R}^{I \\times d}$ to learn temporal dependency.\nThis approach of utilizing the timestamp information was initially introduced in Informer [[1]](https://ojs.aaai.org/index.php/AAAI/article/view/17325), and our model, along with recent models [[2](https://proceedings.neurips.cc/paper/2021/hash/bcc0d400288793e8bdcd7c19a8ac0c2b-Abstract.html), [3](https://proceedings.mlr.press/v162/zhou22g.html), [4](https://arxiv.org/abs/2210.02186)], adopts the same approach.\n\nNow, we clarify the rationale for our AutoCon objective in relation to this timestamp information. In the baselines used in M-Figure 2, TimesNet, FEDformer, and our model all obtained representations using these timestamps, incorporating them into the input sequences, while PatchTST does not utilize the timestamps.\nAn important observation is that both TimesNet and FEDformer do not effectively capture the annual cyclic patterns, despite utilizing timestamps as the same as our model.\nThese failures are noteworthy, particularly considering the Electricity time series, which displays a yearly-long periodicity.\nThese results show that it is challenging for the model to learn yearly patterns even when given input sequences and timestamps, solely relying on the existing forecasting loss.\nTherefore, this result demonstrates the necessity of our AutoCon loss.\nFurthermore, to better demonstrate the importance of our loss in capturing long-term periodicities, we conducted an additional experiment where we compared two models: Ours with and without AutoCon loss.\nAs shown in [R-Figure1](https://tinyurl.com/wo-autocon-repr-sim), compared to the one with AutoCon loss, the one without AutoCon loss demonstrates only a hint of periodicity (not too different from other baselines).\n\n\n### W2:Constant similarity of baseline models even with various patterns\nAs pointed out by reviewer eBaK, M-Figure 2 indeed seems to suggest that the baselines failed to learn even the short-term correlations, not to mention long-term correlations.\nThis is actually due to the smoothing we applied to the representation similarity, in order to help the readers better understand that none of the baselines are able to capture long-term correlations.\nWe apologize for any confusion caused by not explicitly mentioning this process and have denoted it in the caption of M-Figure 2 in the revision.\nWe also provide original representation results, which are enlarged for each baseline, without smoothing out the short-term fluctuations as shown in [R-Figure2](https://tinyurl.com/baselines-repr-sim).\nAs anticipated by reviewer eBaK, the three baseline models are indeed learning short-term correlations within the window, although they are not learning long-term correlations.\nWe hope that additional visualizations clarify the ambiguity caused by the smoothed representation similarity plots."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1868/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470808882,
                "cdate": 1700470808882,
                "tmdate": 1700470808882,
                "mdate": 1700470808882,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TgTD2rUSZ4",
                "forum": "nBCuRzjqK7",
                "replyto": "iEssvD8GKG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1868/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1868/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Q2:Additional M-Figure 6 results over other datasets\nWe thank reviewer eBaK for pointing out and addressing our oversight. M-Figure 6 presents the experimental results for the ETTh2 dataset. We explicitly identified it in the description of M-Figure 6 of our revised paper. Additionally, we provide results for ETTh1 ([R-Figure3](https://tinyurl.com/figure6-ETTh1)) and Electricity ([R-Figure4](https://tinyurl.com/figure6-ecl)), which show the long-term variations. Although there are some differences in magnitude, the overall trends are similar across the three datasets. We have also added these additional results to the Appendix B.2. We sincerely appreciate the reviewer`s constructive and helpful comments once again.\n\n### References\n[[1]](https://ojs.aaai.org/index.php/AAAI/article/view/17325) Informer: Beyond efficient transformer for long sequence time-series forecasting, AAAI'21  \n[[2]](https://proceedings.neurips.cc/paper/2021/hash/bcc0d400288793e8bdcd7c19a8ac0c2b-Abstract.html) Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting, NeurIPS'21  \n[[3]](https://proceedings.mlr.press/v162/zhou22g.html) FEDformer: Frequency enhanced decomposed transformer for long-term series forecasting, ICML'22  \n[[4]](https://arxiv.org/abs/2210.02186) TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis, ICLR'23"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1868/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470837410,
                "cdate": 1700470837410,
                "tmdate": 1700470837410,
                "mdate": 1700470837410,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tAWQIYW6La",
            "forum": "nBCuRzjqK7",
            "replyto": "nBCuRzjqK7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1868/Reviewer_pLfj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1868/Reviewer_pLfj"
            ],
            "content": {
                "summary": {
                    "value": "The paper is about self-supervised contrastive forecasting in long sequences. Existing methods perform poorly on capturing long-term variations beyond the window (outer-window variations). The proposed method, AutoCon, learns a long-term representation by constructing positive and negative pairs accross distant windows in a self-supervised manner. The authors have perform extensive experiments on 9 datasets, using 14 state of the art models, and MSE and MAE evaluation metrics to show that the models with AutoCon loss outperform the rest in most cases, achieving performance improvements of up to 34%."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well-written and well-structured. The authors have done a great job with providing the related work and the limitations, giving the details to the proposed methodology and an extensive experimental evaluation. The images and tables are well-designed and show attention to detail.\n- The paper is about an interesting problem, long-term time-series forecasting via contrastive learning. This is topic that has gained a lot of attention recently.\n- The methodology is described with enough details.\n- The extensive experiments, not only capture different cases in a various datasets and with various state of the art comparison methods, but they also show that the proposed methodology outperforms in most cases the other models."
                },
                "weaknesses": {
                    "value": "- The novelty of the work is incemental. While the authors focus on an interesting problem, the proposed methodology is combination of existing components.\n- It would be useful to add a table with the dataset statistics summarized (in Appendix if it does not fit in the main paper). How does the proposed methodology performs in imbalanced data?\n- It would be interesting and more convincing on the performance if the authors would add results on more evaluation metrics, e.g., Accuracy, AUROC, AUPRC, etc.\n- It would be interesting to show what is the runtime for each method/dataset and time complexities."
                },
                "questions": {
                    "value": "- There is no discussion about making the code available publicly.\n- The authors can respond on my comments 2-4 in the weaknesses section above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethic concerns."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1868/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699344655086,
            "cdate": 1699344655086,
            "tmdate": 1699636116874,
            "mdate": 1699636116874,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "usHatIMKc4",
                "forum": "nBCuRzjqK7",
                "replyto": "tAWQIYW6La",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1868/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1868/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### W1: Novelty of the work\nWe appreciate reviewer pLfj's thoughtful feedback regarding the novelty. We would like to respectfully clarify our perspective on this matter.\n\nOur primary novelty lies in the introduction of global autocorrelation as a foundational element for long-term prediction. Using global autocorrelation in long-term forecasting, to the best of our knowledge, has not been previously explored. While we acknowledge the potential for future refinement and the introduction of more intricate mathematical models, we believe the core novelty of our work is in emphasizing the significance of autocorrelation-based contrastive learning, AutoCon, within the sliding window-based long-term forecasting. \n\nTo incorporate AutoCon, we have renovated a decomposition architecture that is suited for representation learning. While our model can be viewed as a combination of existing components, its architecture is specifically designed to emphasize the significance of the long-term branch. As we mentioned in Section 3.2, this aspect has often been overlooked by even complex existing models, yet our approach maintains simplicity while being effectively structured for long-term representation.\n\nWe understand that perspectives on novelty may vary, and we respect alternative opinions. However, it is our strong belief that the incorporation of autocorrelation-based contrastive learning into the long-term forecasting framework, as demonstrated in our work, constitutes a meaningful advancement in the field.\n\n### W2: Dataset Statistics and Extension to imbalanced data\n\n* 1: Dataset Statistics  \nFor reviewer pLfj's valuable suggestions, we added M-Table 5 summarizing dataset statistics in Appendix A.3. The dataset statistics will provide readers with a quick reference to understand the diverse nature of the datasets used in our experiments.\n\n* 2: Extension to imbalanced data  \nRegarding reviewer pLfj's query on the performance of our methodology in imbalanced data scenarios, we appreciate the opportunity to clarify this aspect. As our focus is on time series forecasting, which is typically treated as a regression task, the concept of data imbalance manifests differently compared to classification tasks. In time series forecasting, data imbalance might refer to scenarios where certain patterns or trends are underrepresented. While our current study does not explicitly address this form of imbalance, our methodology's ability to learn long-term dependencies can be indirectly beneficial in scenarios where certain long-term trends and patterns are less frequent compared to repeated short-term fluctuations within the window. Specifically, AutoCon captures the overall tendency of the time series data across its entire length using the global autocorrelation.\nBy considering the global structure of the data, it helps the model to identify and learn from long-term patterns, including those that may not occur frequently.\nThus, we believe that in imbalanced scenarios, the model can recognize and adapt to these infrequent but potentially significant patterns, leading to more accurate forecasting. We acknowledge the importance of this aspect and will consider investigating this in future work to further enhance the robustness of our forecasting approach. Once again, we thank the reviewer for the constructive feedback."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1868/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470684165,
                "cdate": 1700470684165,
                "tmdate": 1700477856213,
                "mdate": 1700477856213,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "il0Nod4GSY",
                "forum": "nBCuRzjqK7",
                "replyto": "tAWQIYW6La",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1868/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1868/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### W3: Results on other evaluation metrics\nWe concur with reviewer pLfj's recommendation to incorporate additional evaluation metrics, as this will further substantiate the superior performance of our method in comparison to the baselines.\nWhile existing metrics (*i.e.,* MSE and MAE) are standard metrics for long-term forecasting evaluation, they have limitations. Specifically, they may not adequately capture aspects such as the shape and temporal alignment of the time series, which are crucial for a comprehensive evaluation of a forecasting model's performance.\n\nTo address these limitations, we introduced two additional metrics based on Dynamic Time Warping (DTW) [[1]](https://ieeexplore.ieee.org/abstract/document/1163055?casa_token=2XD8Dcxt2fIAAAAA:qR84AD50aWQv7paoubI5peCAiDRousaLO8n6egWNLkc66OF6IbbH9rx1cpvxs5HtveC3ZaB9NQ): Shape DTW and Temporal DTW [[2]](https://arxiv.org/abs/1909.09020). Shape DTW focuses on the similarity of the pattern or shape of the predicted sequence to the actual sequence, providing insight into the model's ability to capture the underlying pattern of the time series. Temporal DTW evaluates the alignment of the predicted sequence with the actual sequence, highlighting the model's accuracy in forecasting the timing of events.\n\nThese additional metrics offer a more nuanced assessment of our model's performance, particularly in areas where MSE and MAE may fall short. Lower values in both Shape DTW and Temporal DTW indicate better performance, signifying lesser distortion between the predicted and actual sequences. As shown in R-Table 1, our method demonstrates superior performance for three datasets, which show obvious long-term variations, not only in MSE and MAE but also in these shape and temporal alignment-focused metrics. \nAlthough it is difficult to measure Accuracy or AUROC due to the absence of labels in the forecasting task, our comprehensive evaluation of the newly used metrics underscores the superiority of our method. Also, these experiments were added to Appendix B.6  in the revised version of the paper.\n\n**R-Table 1: Univariate forecasting results, evaluated using Shape and Temporal DTW in a 720-output setting across three datasets**     \n\n| Dataset            | ETTh1            |                 | ETTh2        |                 | Electricity |  |\n|--------------------|------------------|-----------------|--------------|-----------------|----------------------|--------------------------|\n| **Model \\ Metric** | Shape DTW        | Temporal DTW    | Shape DTW    | Temporal DTW    | Shape DTW | Temporal DTW |\n| **Ours**           | **17.14\u00b11.653** | **59.66\u00b11.739** | **42.38\u00b10.630** | **13.47\u00b11.793** | **80.73\u00b17.7982**    | **0.09\u00b10.014**           |\n| TimesNet           | 25.80\u00b14.349      | 86.86\u00b115.509    | 62.58\u00b15.390  | 51.19\u00b121.834    | 139.83\u00b116.516      | 0.49\u00b10.826               |\n| PatchTST           | 22.21\u00b11.226      | 72.23\u00b14.411     | 65.45\u00b12.976  | 15.23\u00b10.882     | 116.50\u00b129.878      | 0.75\u00b11.674               |\n| MICN               | 37.08\u00b112.393     | 65.70\u00b13.588     | 67.69\u00b18.796  | 22.67\u00b13.168     | 123.93\u00b117.543      | 1.90\u00b11.176               |\n| DLinear            | 58.32\u00b11.955      | 155.21\u00b18.587    | 82.53\u00b13.627  | 24.88\u00b12.403     | 88.70\u00b13.550        | 0.18\u00b10.145  \n\n### W4: Computational Cost\nIn Section 4.4, we have briefly provided an analysis of the computational costs for each model. However, due to space limits, detailed information can be found in Appendix B.4. \nWe measured the computational cost of various baselines for four different output lengths, ranging from 96 to 2160. In summary, our model recorded significantly lower inference times compared to the baselines except for linear models in a single inference process. Additionally, it demonstrated much faster inference speed compared to CNN-based, TimesNet, and Transformer-based models even as the prediction length increased.\nWe would sincerely appreciate it if the reviewer could refer to it once more.\n\n### Q1: Open-sourced code\nWe have already attached our code as supplementary material in the open review system. If this is not made public after the review process, we are also willing to publicly open our code on Github and include the GitHub repository URL in the main paper as either an abstract or a footnote.\n\n### References\n[[1]](https://ieeexplore.ieee.org/abstract/document/1163055?casa_token=2XD8Dcxt2fIAAAAA:qR84AD50aWQv7paoubI5peCAiDRousaLO8n6egWNLkc66OF6IbbH9rx1cpvxs5HtveC3ZaB9NQ) Dynamic programming algorithm optimization for spoken word recognition, IEEE transactions on acoustics, speech, and signal processing'1978  \n[[2]](https://arxiv.org/abs/1909.09020) Shape and Time Distortion Loss for Training Deep Time Series Forecasting Models, NeurIPS'19"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1868/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470722416,
                "cdate": 1700470722416,
                "tmdate": 1700477947059,
                "mdate": 1700477947059,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]