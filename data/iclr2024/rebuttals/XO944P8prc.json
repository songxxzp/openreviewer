[
    {
        "title": "Afterstate Reinforcement Learning for Continuous Control"
    },
    {
        "review": {
            "id": "8KY1UyBD0X",
            "forum": "XO944P8prc",
            "replyto": "XO944P8prc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5463/Reviewer_CtUq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5463/Reviewer_CtUq"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to enhance the critic's imagination in Actor-Critic. The authors proposes two methods (1) learning the utility of action on the environment,  (2) learning the future consequence of the action. The authors verify the effective of their method in standard benchmark."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "This paper proposes a simple method to improve the current RL methods in continuous control tasks. The work present the concept of the Afterstate framework and incorporate this into the critic in the actor-critic framework."
                },
                "weaknesses": {
                    "value": "1. The presentation of the method can be highly improved. (1) The authors should provide detailed description of the motivation of using after-state framework. As it stands, this statement is very heuristic and not supported by any theoretical analysis or evidence.Therefore, this makes the paper significantly less convincing.\n\n2. Experiments lack more advanced baseline algorithms, such as REDQ [1]. \n\n3. Although the author shows the performance of the algorithm in tables, it is more convincing to show the algorithm through curves.\n\n4. The author has made minor changes to the classic Actor-Critic framework. From a technical point of view, the changes are very small and the novelty is not strong. Therefore, if the author can conduct a large number of experiments and beat the current best algorithms, it will strongly prove the importance of considering after-state in RL.\n\n[1] Chen, Xinyue, et al. \"Randomized ensembled double q-learning: Learning fast without a model.\" arXiv preprint arXiv:2101.05982 (2021)."
                },
                "questions": {
                    "value": "Please refer to the above weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5463/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698757285730,
            "cdate": 1698757285730,
            "tmdate": 1699636556658,
            "mdate": 1699636556658,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "D0R8Rv5oSR",
            "forum": "XO944P8prc",
            "replyto": "XO944P8prc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5463/Reviewer_D12j"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5463/Reviewer_D12j"
            ],
            "content": {
                "summary": {
                    "value": "The paper uses an actor-critic method that combines a model to\nestimate the state-action value.  In particular Eq. (3) updates the\npolicy using an actor-critic method with the reward expanded out of\nthe value for one time-step using a learned model. While Eq. (2) defines\na TD-error based on a 2-time-step prediction.\n\nThe method is evaluated on control tasks like the pendulum and range of\nMuJoCo tasks, as well as a painting task. They compare against SAC, MBPO\netc."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- A wide range of evaluations were considered."
                },
                "weaknesses": {
                    "value": "- The idea of expanding out values using a learned model has been widely\nstudied in many past works, e.g., \"Model-Based Value Expansion\nfor Efficient Model-Free Reinforcement Learning\" (ICML2018, Feinberg et al.).\nAlso, methods like MuZero are also essentially performing value expansion\nwith a learned model. These works were not cited or discussed.\n\"Stochastic value gradients\" is another related work that uses a learned model\nto facilitate computation of the policy gradient.\n\n- I am not confident in the evaluations. The benchmarking results for\nSAC are weaker than other results that can be found online, e.g.:\nhttps://github.com/ChenDRAG/mujoco-benchmark\nhttps://spinningup.openai.com/en/latest/spinningup/bench.html\n\n- I found that the clarity was not that great, and I think the manuscript could be polished\nmore.  Also I think some equations seem incorrect (see the bottom of the\nweaknesses section for more details).\n\n\nComments:\nThe asterisk symbol * is not typically used to denote multiplication\nin mathematics or machine learning publications.\n\nPage 2: I think the temporal difference update for V seems wrong.\nFirst, the left-hand side should have a time index for s.\nSecondly, I believe the time index for r and for V(s_{t+2}) should\nbe shifted down by 1 step.\n\nThe notation on the bottom of page 2 for the policy also seems not accurate.\nThe argmax is over $a$, but the equation has $a_t$ with a time index.\nAlso, the left-hand side has $\\pi(s)$, whereas the right-hand side has\na time index for $s_t$.\n\nSimilarly to the previous issue, equation 2 also seems inaccurate.\n$Q_t = r_t + \\gamma V(s_{t+1})$, but the left-hand side has the time-index for\n$r$ wrong."
                },
                "questions": {
                    "value": "Where did the code for the other algorithms such as\nSAC come from, and why do the results not seem to match\nthe results in the existing benchmarks that I pointed out?\n\nWhat is the relationship to value expansion methods?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5463/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698795944353,
            "cdate": 1698795944353,
            "tmdate": 1699636556522,
            "mdate": 1699636556522,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "7Ll0FDnZ9w",
            "forum": "XO944P8prc",
            "replyto": "XO944P8prc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5463/Reviewer_Kkgd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5463/Reviewer_Kkgd"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose an RL algorithm that decouples learning transition dynamics of taking actions from estimating utility of the actions. This involves learning an explicit dynamics model for next state prediction, and training a value function on the next state (as opposed to learning Q functions)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "While sample efficient RL is an important problem, I have concerns regarding the relation of the proposed method to existing approaches in the field, as detailed in the weaknesses section."
                },
                "weaknesses": {
                    "value": "1. Relation to model-based RL approaches \n\nThe claim in the paper hinges on decoupling next state prediction (dynamics learning) from learning utility of actions. The authors then propose learning a separate dynamics model, which is used to optimize the control policy by rolling out the model for one step. Prior work [2,3] do many more rollout steps of the model for optimization, and show far greater sample efficiency than the proposed approach. It is unclear what is the benefit of using the dynamics model in the manner presented in the paper, as opposed to simulating samples from the dynamics model and using those to train the policy[1,2,3]. The authors do compare to MBPO [1], and show much worse sample efficiency. The argument for model-free methods is that dynamics might be hard to learn and unnecessary for the task, but since the presented method already learns a dynamics model, it is unclear why it is being used in the manner presented as opposed to the standard approach of simulating new samples. The paper also doesn't include comparisons to Dreamer and TD-MPC [2,3] which are more recent and much more efficient than MBPO. Furthermore from the experiments included, it seems the proposed approach does worse than its model-free counterparts on the challenging Humanoid environment, which indicates that the approach doesn't scale well to more complex environments. \n\n\n\n\n[1] : Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Modelbased policy optimization.\n[2] : Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models.\n[3] : Hansen, Nicklas, Xiaolong Wang, and Hao Su. \"Temporal difference learning for model predictive control.\""
                },
                "questions": {
                    "value": "1. Why is it better to use the dynamics model in the manner presented, as opposed to simulating samples from the dynamics model as done in [1,2,3], given superior sample efficiency of the latter?\n\n2. Please include learning curve comparisons to [2,3] on the standard dm-control benchmarks \n\n\n\n\n[1] : Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Modelbased policy optimization.\n[2] : Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models.\n[3] : Hansen, Nicklas, Xiaolong Wang, and Hao Su. \"Temporal difference learning for model predictive control.\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5463/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699063050547,
            "cdate": 1699063050547,
            "tmdate": 1699636556402,
            "mdate": 1699636556402,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]