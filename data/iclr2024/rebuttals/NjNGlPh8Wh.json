[
    {
        "title": "The Expressive Power of Transformers with Chain of Thought"
    },
    {
        "review": {
            "id": "FJO5BStRyP",
            "forum": "NjNGlPh8Wh",
            "replyto": "NjNGlPh8Wh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1377/Reviewer_7d6w"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1377/Reviewer_7d6w"
            ],
            "content": {
                "summary": {
                    "value": "This submission studies theoretical expressive power of transformer models equipped with chain of thought reasoning. The authors provide theoretical lower and upper bounds for different decoding step sizez, log (L), linear (Time n^2; Space n), and polynomial (P). Their proof for the lower bounds relies on a construction that enables retrieval across different columns in the transformer (\u201cLayer-norm hash\u201d), which is used to simulate automata and Turing machines in transformer forward passes. The upper bounds are proven by straightforwardly simulating attention via a TM."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Interesting theoretical work on the expressive power of transformers that sheds light on the limitations (and possible capabilities) of transformers for researchers and practicioners alike\n- The theory is well developed and well presented"
                },
                "weaknesses": {
                    "value": "- The paper is not very well self-contained and relies on the reader being familiar with two previous papers\n- The last paragraph in the conclusion should be clearly marked as limitation of this theoretical bounds (the lower bounds and upper bounds might not have any relecance in practice)."
                },
                "questions": {
                    "value": "- For the upper bounds: Doesn\u2019t the same reasoning as for the lower bounds apply? For fixed data distributions, complexity classes are irrelevant. How does this mismatch fit into the theory work?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1377/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698731802304,
            "cdate": 1698731802304,
            "tmdate": 1699636065513,
            "mdate": 1699636065513,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xUnrJxJb2y",
                "forum": "NjNGlPh8Wh",
                "replyto": "FJO5BStRyP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1377/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1377/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your review!\n\n1. **Self-Containedness:** We will add more specific details of the papers we build upon where appropriate. We will also provide a reference to a new comprehensive survey paper on formal language results about transformers (https://arxiv.org/abs/2311.00208).\n2. **Conclusion:** Regarding the conclusion, we will rephrase the part about lower bounds to clarify that it is a limitation. Namely, that just because there exists a transformer that can implement some behavior, it does not mean that that transformer can be learned by gradient descent.\n\n> For the upper bounds: Doesn\u2019t the same reasoning as for the lower bounds apply?\n\nThe reasoning for lower bounds actually does not apply for the upper bounds. The reasoning for the lower bounds is: just because a transformer can express a solution to a problem, it does not mean that gradient descent will find that solution. In the other direction, if we know that a transformer cannot express a solution to a problem, we know that gradient descent will not find a solution to the problem, because no solution exists. We will clarify this."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1377/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700087555634,
                "cdate": 1700087555634,
                "tmdate": 1700087555634,
                "mdate": 1700087555634,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VcWWQdh35U",
                "forum": "NjNGlPh8Wh",
                "replyto": "xUnrJxJb2y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1377/Reviewer_7d6w"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1377/Reviewer_7d6w"
                ],
                "content": {
                    "title": {
                        "value": "Thanks"
                    },
                    "comment": {
                        "value": "I thank the authors for their submission and answer! I would like to keep my score as is. I would appreciate if the authors provide a clear limitation section instead of a sentence in the conclusion. Especially for a more practically oriented audience, the difference between theoretical expressive power and learning in practice might not be clear and need to be made explicit."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1377/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632656074,
                "cdate": 1700632656074,
                "tmdate": 1700632656074,
                "mdate": 1700632656074,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nFSBke6Yr3",
            "forum": "NjNGlPh8Wh",
            "replyto": "NjNGlPh8Wh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1377/Reviewer_a7kJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1377/Reviewer_a7kJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the reasoning ability of transformer models, which underlie some state-of-the-art deep neural networks such as ChatGPT or GPT-4, in solving sequential reasoning problems such as simulating finite-state machines, deciding connectivity of two nodes in a graph, and solving matrix equalities, which are formalized as classical language classes (context free grammars or regular languages in the Chomsky hierarchy) or traditional complexity classes (bounded-depth threshold circuits $TC^0$, logarithmic space $L$, polynomial time $P$).\n\nThe paper shows that the number of intermediate steps (length of Chain-of-Thought, or amount of scratchpad space) is closely related to the sequential reasoning ability of a decoder-only architecture transformer. In particular, Equation 1 shows that the class of problems solvable with $t(n)$ intermediate steps are those solvable by multitape Turing machines using similar time or space."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The connection to traditional complexity classes (Equation 1) effectively refactors out all recurrence/use of scalable memory into the intermediate steps in chain of thought/scrathpad using transformers. This is as expected, and such intuition shows up also in studying traditional complexity classes such as $L$ and $P$ in a natural way, contrasting the need of memory to sequential reasoning.\n\nThe proof of this connection (Equation 1) is also natural. In particular, the simulation of Turing machines by decoder-only transformers using an encoding based on layer-norm hash is also natural."
                },
                "weaknesses": {
                    "value": "The study of log-precision transformer models with $h$ heads, $d$ layers, dimension $m$, and feedforward width $w$, while capturing some of the trends in Machine Learning in recent years, may not apply when the trend changes to other non-transformer architectures in a decade or two.\n\nThe first equivalence between a transformer-defined classes (the chain of thought class) with a standard complexity class ($P$) is not too unexpected, given that both sides allow for a more lenient notion of equivalence/reductions, namely closed under polynomial time reductions instead of more strict notion of reduction such as logarithmic space reductions. Equating under polynomial time reductions are easier targets to hit."
                },
                "questions": {
                    "value": "While some future questions are mentioned (for example to study how different kinds of fine-tuning such as reinforcement learning or improving a model's ability to use chain of thought), how could such learnability problems be approached at a high level? The current paper studies expressive power by simulation results (between chain-of-thought transformers and Turing machines in both directions), which are somewhat standard in the computer science literature and appears unable to answer learnability questions by far."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1377/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698795967611,
            "cdate": 1698795967611,
            "tmdate": 1699636065437,
            "mdate": 1699636065437,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7pyxZhlz5c",
                "forum": "NjNGlPh8Wh",
                "replyto": "nFSBke6Yr3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1377/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1377/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review!\n\n> The study of log-precision transformer models with h heads, l  layers, dimension d, and feedforward width w, while capturing some of the trends in Machine Learning in recent years, may not apply when the trend changes to other non-transformer architectures in a decade or two.\n\nWe agree that this is perhaps an inherent limitation of theory done about transformers. However, we do believe that, even in the world where transformers become obsolete, there could some insight to be gained by analyzing them that might inform the design of future models and the techniques develops might aid in analyzing future models.\n\n> The first equivalence between a transformer-defined classes (the chain of thought class) with a standard complexity class (P) is not too unexpected, given that both sides allow for a more lenient notion of equivalence/reductions, namely closed under polynomial time reductions instead of more strict notion of reduction such as logarithmic space reductions.\n\nCould you clarify what you mean by stricter reductions in this context? The conversion from multitape TMs to transformers is tight (TIME(t) in CoT(t)), and the conversion from transformers to multitape TMs has a quadratic factor (CoT(t) in soft TIME(t^2)).\n\n> While some future questions are mentioned (for example to study how different kinds of fine-tuning such as reinforcement learning or improving a model's ability to use chain of thought), how could such learnability problems be approached at a high level?\n\nThis is a good question. At a very high level, such work would have to define \u201clearnable\u201d by leveraging some condition that makes gradient descent likely to find a solution or some property of transformers that is likely to emerge from gradient descent.\n\nWe believe that recent work characterizing the [simplicity biases of transformers](https://arxiv.org/abs/2211.12316) might be relevant here. Other work identifies [restricted classes of transformers that are learned in practice](https://arxiv.org/abs/2010.09697), and perhaps analyzing the expressive power of such a simplified model of transformers could be a first step towards getting at what is learnable vs. what is expressible. More ambitiously, it might be possible to add constraints to the hypothesis class defined in terms of the geometry of the loss landscape (e.g., what problems can be solved such that the loss function is locally very flat). Some other recent work obtains learnability results for neural networks in the [statistical query learning](https://arxiv.org/abs/2207.08799) and next-token predictors in the [PAC learning frameworks](https://arxiv.org/abs/2309.06979), and perhaps that would be another avenue to pursue with transformer language models."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1377/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700087296489,
                "cdate": 1700087296489,
                "tmdate": 1700659311409,
                "mdate": 1700659311409,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NWUK3ztegY",
                "forum": "NjNGlPh8Wh",
                "replyto": "7pyxZhlz5c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1377/Reviewer_a7kJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1377/Reviewer_a7kJ"
                ],
                "content": {
                    "comment": {
                        "value": "> Could you clarify what you mean by stricter reductions in this context?\n\nBetween these two reductions:\n1. logarithmic space reduction\n2. polynomial time reduction\n(1) is a stricter reduction than (2), that is, every logarithmic space reduction is a polynomial time reduction (but not vice versa or else  $L = P$).\n\nNote that polynomial time reductions are used in both:\na. the definition of $P$, and\nb. the quadratic factor in Equation 1.\n\nSo \"equating $P$ (which is closed under polynomial time reductions) with some other classes (CoT classes) under polynomial time reductions\" is easier than, say, \"equating $L$ (which is closed under stricter reductions) with some other CoT classes under logarithmic reductions\"."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1377/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700444870508,
                "cdate": 1700444870508,
                "tmdate": 1700444870508,
                "mdate": 1700444870508,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tLJdtHOVQu",
            "forum": "NjNGlPh8Wh",
            "replyto": "NjNGlPh8Wh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1377/Reviewer_XLkW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1377/Reviewer_XLkW"
            ],
            "content": {
                "summary": {
                    "value": "A series of recent theoretical work has analysed the limitations of transformers when it comes to solving certain simple sequential problems. In this work, the authors analyse whether these negative results remain when transformers are enriched with a chain-of-thoght. The main results show that these extensions extend the power of transformers to beyond TC^0 and firs-order logic."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The theoretical results are interesting. In particular, the authors have proved that transformers with a linear number of decoding steps have the capacity to regocnize regular languages. Nevertheless they cannot recognised all context sensitive languages."
                },
                "weaknesses": {
                    "value": "The paper focus on the analysis of the expressive power of transformers enriched with chain of thought. For the audience of ICLR, it would be interesting to have some results analysing aspects related to learnability of such models."
                },
                "questions": {
                    "value": "No questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1377/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1377/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1377/Reviewer_XLkW"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1377/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698802014795,
            "cdate": 1698802014795,
            "tmdate": 1699636065349,
            "mdate": 1699636065349,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GELzlhInrK",
                "forum": "NjNGlPh8Wh",
                "replyto": "tLJdtHOVQu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1377/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1377/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your review! We appreciate that you found our theoretical results interesting.\n\nWe agree that questions around learnability are interesting but they lie outside the scope of this work. For future work, we are thinking about whether it may be possible to say something about learnability in the PAC learning framework by extending ideas from [this recent paper](https://arxiv.org/pdf/2309.06979.pdf)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1377/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700086795676,
                "cdate": 1700086795676,
                "tmdate": 1700086795676,
                "mdate": 1700086795676,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PmMBpjLp2U",
            "forum": "NjNGlPh8Wh",
            "replyto": "NjNGlPh8Wh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1377/Reviewer_DD5G"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1377/Reviewer_DD5G"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates how the formal reasoning power of decoder-only transformers change if they are allowed to generate intermediate outputs that can be read in subsequent steps, akin to chain-of-thought or scratchpad reasoning.  The authors show that for an input of size n, adding log n intermediate steps doesn't add much to the power of transformers without intermediate steps.  However, adding a linear number of intermediate steps results in a significant increase in reasoning power -- allowing transformers to recognize languages somewhere between regular and contex-sensitive languages in the Chomsky hierarchy.  The authors also show that using t(n) intermediate steps allows a transformer to mimic t(n) steps of a Turing machine.  Finally, they also show that allowing a transformer poly(n) intermediate steps precisely captures the class of problems solvable in deterministic polynomial time.  The paper presents complexity-theoretic lower and upper bounds of the computational power of transformers with the ability to use t(n) intermediate steps, for various functions t(n)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper establishes both lower and upper bounds on the computational power of transformers with intermediate steps (akin to chain-of-thought or scratchpad reasoning).  The lower and upper bounds are almost tight.  Although I'm not an expert in this area, I find that this work significantly extends state-of-the-art in relating the computational power of transformers to classical complexity classes.  The notion of norm-hash is likely useful in other contexts as well."
                },
                "weaknesses": {
                    "value": "The paper is technically intricate, and may not be accessible to the general ICLR audience.  The core idea of using norm-hash is not illustrated well.  Using examples to explain the intuition will help in improving the readability of the paper.  The paper assumes a lot in terms of prior knowledge of the reader in the formal language theoretic modeling of transformers.  The notation is at times unnecessarily complicated -- I understand this was done for the sake of rigour, but this does hurt readability of the paper."
                },
                "questions": {
                    "value": "1. Please explain the intuition behind norm-hash using examples.  Otherwise, its introduction appears too abrupt.  This is an important concept for the proofs, and hence it should have been explained more clearly with examples.\n2. Why is it necessary to consider a multi-tape Turing machine when you want to show that a move of a Turing machine can be simulated by one move of a transformer with intermediate steps?  Wouldn't showing this for a single tape Turing machine suffice?\n3. The result concerning characterization of P is not adequately explained in the paper.  For example, there is no subsection devoted to it. It is only mentioned as a bullet on page 3.  Please elaborate on this a bit more, like the other results.\n4.  The paper presumes a lot about prior knowledge of the reader in the formal language theoretic modeling of transformers.  A more gentle introduction would help the readability of the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics concerns."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1377/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1377/Reviewer_DD5G",
                        "ICLR.cc/2024/Conference/Submission1377/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1377/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699367850499,
            "cdate": 1699367850499,
            "tmdate": 1700659315980,
            "mdate": 1700659315980,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dttijysYjC",
                "forum": "NjNGlPh8Wh",
                "replyto": "PmMBpjLp2U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1377/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1377/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your review!\n\n# 1) Layer-Norm Hash\n\nWe will add an example problem like the following the better illustrate the layer norm hash. Potentially, if space allows, we would like to have a figure illustrating it visually.\n\n**Problem:** Imagine we describe the state of a 1D grid world with x\u2019s and o\u2019s and then specify a sequence of +/- moves in that world: `xoooxo+-+-++-`. We then want a transformer to add up the +/- moves and return whether the grid cell is `x` or `o`.\n\nThe natural algorithm is to compute the current position with c = #(`+`) - #(`-`) and then attend to that position and see whether it\u2019s x or o. A transformer with causal masking can compute c/n, but it\u2019s messy to use this to retrieve the grid cell at position c because that cell is represented as 1/c. The layer-norm hash lets implement this hard attention cleanly by attending with query phi(c/n, 1/n) = phi_c and key phi(1/c, 1) = phi_c!\n\nWe hope this helps make the idea more concrete, and it captures the way we later use the layer-norm hash for automata and Turing machines in a simpler setting.\n\n# 2) Turing Machine Simulation\n\nRegarding the Turing machine simulation, indeed, you\u2019re correct that it would suffice to simulate a single tape to prove Turing completeness.We consider the multitape model because this is the standard model used to define finegrained complexity classes within P (e.g., TIME(n)). The reason for this is only having just a single tape can add polynomial overhead compared to multitape TM. Thus, if we want to conclude TIME(t) \\subseteq CoT(t) as in Corollary 2.1, we need to use the multitape model.\n\n# 3) Clarifying P Result\n\n> The result concerning characterization of P is not adequately explained in the paper. For example, there is no subsection devoted to it.\n\nThanks for pointing this out. We will add text explaining that this follows from Corollary 2.1 (TIME(t) in CoT(t)) and Theorem 3 (CoT(t) in softTIME(t^2)). Together, these results show poly(n) CoT can express any problem in P and vice versa.\n\n# 4) Reference for Transformers and Formal Languages\n\nFinally, we would like to address your suggestion to give readers more context for formal language results about transformers. Given the space limitations, we think the best way to provide a gentler introduction is to add a reference to a [new survey paper](https://arxiv.org/abs/2311.00208) on transformers and formal language theory."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1377/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700086467395,
                "cdate": 1700086467395,
                "tmdate": 1700086492939,
                "mdate": 1700086492939,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Suhc9pM3HD",
                "forum": "NjNGlPh8Wh",
                "replyto": "dttijysYjC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1377/Reviewer_DD5G"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1377/Reviewer_DD5G"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your responses"
                    },
                    "comment": {
                        "value": "Thanks to the authors for responding to my questions.  My concern about accessibility of the material in this paper to the general ICLR audience remains.  I wish the authors also provided some pointers to practical implications of their results."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1377/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659252549,
                "cdate": 1700659252549,
                "tmdate": 1700659252549,
                "mdate": 1700659252549,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]