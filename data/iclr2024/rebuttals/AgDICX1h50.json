[
    {
        "title": "Large Language Models as Analogical Reasoners"
    },
    {
        "review": {
            "id": "XwCk9v4V8C",
            "forum": "AgDICX1h50",
            "replyto": "AgDICX1h50",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4399/Reviewer_ey56"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4399/Reviewer_ey56"
            ],
            "content": {
                "summary": {
                    "value": "This work presents a novel prompting method inspired by the concept of analogical reasoning in cognitive science. The method is shown to outperform few-shot chain-of-thought methods, while not requiring any few-shot examples."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed method involves an interesting take on concepts from cognitive science, and yields consistent improvements across reasoning benchmarks.\n- The proposed method is simple to implement, and does not require any task-specific prompts or training examples.\n- A qualitative analysis is performed of the generated exemplars and their relationship to downstream performance.\n- The code dataset is limited to recently published problems, thus addressing test set contamination concerns."
                },
                "weaknesses": {
                    "value": "- The paper emphasizes the importance of instructing the LLM to generate *distinct* exemplars, but there is no ablation performed for this specific aspect of the method.\n- It seems unlikely that the generated exemplars are literally retrieved from memory in the sense that they are in human reasoning. It seems more likely that these are novel problems generated by the LLM, based on general statistical knowledge. I don't think this really undermines the usefulness of the approach, but it might be worthwhile to briefly discuss this issue.\n- The caption for Table 1 mentions that an in-context demonstration was used for the davinci models, but I couldn't find any explanation of this description (e.g., does this include an in-context training example, or merely a demonstration of the formatting?). \n\nMinor comments:\n- The authors might consider citing work that analyzes the analogical reasoning ability of LLMs [1,2] (though I should note that I don't think this undermines the contribution of the present work). There are also a few references that would be good to include when introducing the general concept of analogical reasoning and its role in the psychology literature [3,4].\n\n[1] Webb, T., Holyoak, K. J., & Lu, H. (2023). Emergent analogical reasoning in large language models. Nature Human Behaviour.\n\n[2] Hu, X., Storks, S., Lewis, R. L., & Chai, J. (2023). In-Context Analogical Reasoning with Pre-Trained Language Models. arXiv preprint arXiv:2305.17626.\n\n[3] Gentner, D. (1983). Structure-mapping: A theoretical framework for analogy. Cognitive science, 7(2), 155-170.\n\n[4] Holyoak, K. J. (2012). Analogy and relational reasoning. The Oxford handbook of thinking and reasoning, 234-259."
                },
                "questions": {
                    "value": "I am curious to hear the authors thoughts regarding whether the generated exemplars are genuinely retrieved from memory, or are novel problems based on general statistical knowledge. The latter case seems more consistent with the memory mechanisms in LLMs (and with their tendency to generate fabricated but plausible sounding information), but it is not something that has been considered much in the psychology literature on analogical reasoning, and it is somewhat more difficult to understand how it could improve performance."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4399/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698130933225,
            "cdate": 1698130933225,
            "tmdate": 1699636413227,
            "mdate": 1699636413227,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "v8uhWAWgkV",
                "forum": "AgDICX1h50",
                "replyto": "XwCk9v4V8C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4399/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4399/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your review, and our response"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for insightful feedback. We have incorporated all suggestions in our paper. We thank the reviewer for describing that our work offers a novel, interesting, and general-purpose prompting method and conducts careful experiments that mitigate test set contamination. We respond to the reviewer\u2019s concerns and questions below.\n\n> no ablation performed for generating distinct exemplars\n\nThank you for pointing this out. Following is the ablation result on instructing the LLM to generate *distinct* exemplars or not: \n- GSM8K non-distinct vs distinct: 75.9 vs 77.8\n- MATH non-distinct vs distinct: 35.2 vs 37.3\n\n\n> It seems unlikely that the generated exemplars are literally retrieved from memory in the sense that they are in human reasoning. It seems more likely that these are novel problems generated by the LLM, based on general statistical knowledge. I don't think this really undermines the usefulness of the approach, but it might be worthwhile to briefly discuss this issue. / I am curious to hear the authors thoughts regarding whether the generated exemplars are genuinely retrieved from memory, or are novel problems based on general statistical knowledge. \n\nThis is an excellent point and you are correct. We observed that some of the self-generated exemplars resemble the potential training data of the LLM (e.g., GSM8K training set, Geeksforgeeks coding problems), but many of the exemplars are newly generated and appear to be an interpolation of problems seen by the LLM that are related to the target problem. We find this phenomenon very intriguing, suggesting that LLMs may be capable of generating useful exemplars beyond the existent/seen exemplars.\n\n\n> The caption for Table 1 mentions that an in-context demonstration was used for the davinci models, but I couldn't find any explanation of this description.\n\nThank you for pointing this out. We added the description of in-context demonstration in Appendix D.2. It provides an example of formatting the generation of exemplars and solution to the initial problem.\n\n\n> The authors might consider citing work that analyzes the analogical reasoning ability of LLMs [1,2] (though I should note that I don't think this undermines the contribution of the present work). \n\nThank you very much for pointing us to the related works. We have added them to our manuscript."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4399/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700270186057,
                "cdate": 1700270186057,
                "tmdate": 1700270186057,
                "mdate": 1700270186057,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uR1NGyOGRe",
                "forum": "AgDICX1h50",
                "replyto": "v8uhWAWgkV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4399/Reviewer_ey56"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4399/Reviewer_ey56"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thank you to the authors for these responses and clarifications. I enthusiastically support the paper's acceptance."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4399/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700686741607,
                "cdate": 1700686741607,
                "tmdate": 1700686741607,
                "mdate": 1700686741607,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uKtYeLrD1e",
            "forum": "AgDICX1h50",
            "replyto": "AgDICX1h50",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4399/Reviewer_fGoM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4399/Reviewer_fGoM"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces the analogical prompting method. It's a method similar to CoT, but instead of just plaining \"let's think step by step\", it first prompts the model to automatically generate some knowledge and exemplars for solving tasks. Experiments on mathematical reasoning, code generation, and big-bench confirm its validity."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper is well-written, with a detailed depiction of the design principles and implementation details for the proposed analogical prompting method. Experiment results on GSM8K, MATH, codeforces, and a bigbench subset show moderate improvement compared to few-shot CoT, and a noticeable boost compared to other zero-shot prompting methods.\n\n- The analogical prompting method is well-motivated and intuitive enough to follow. Some ablation studies on the scalability, w/o knowledge, and the number of exemplars are good."
                },
                "weaknesses": {
                    "value": "- Limited technical contribution. Although I think this paper is a good example of how to perform prompting engineering when solving zero-shot mathematical reasoning and code generation tasks, it is still more like a trick to an existing method (analogous to CoT->zero-shot CoT, in this case is retrival few-shot -> self-generated few-shot).\n\n- The codeforces dataset only contains 50 questions; perhaps it is too small to make claims on the improvements (~2% is only one more solved question). Any experiments on larger code generation tasks, e.g., HumanEval?"
                },
                "questions": {
                    "value": "There is error analysis for incorrectly solved tasks, how about correctly solved questions? How many generated exemplars are wrong, but the solution to the new question is correct (i.e., it is known that sometimes LLMs can few-shot generalize from wrong exemplars)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4399/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698736572590,
            "cdate": 1698736572590,
            "tmdate": 1699636413155,
            "mdate": 1699636413155,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "83T7cm7uiw",
                "forum": "AgDICX1h50",
                "replyto": "uKtYeLrD1e",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4399/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4399/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your review, and our response"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for constructive feedback. We have incorporated all suggestions in our paper. We are glad that the reviewer describes our work as well-motivated, having good design principles, and offering valuable ablation studies on \u200b\u200bthe scalability and knowledge, etc. We respond to the reviewer\u2019s concerns and questions below.\n\n> Although I think this paper is a good example of how to perform prompting engineering when solving zero-shot mathematical reasoning and code generation tasks, it is still more like a trick to an existing method (analogous to CoT->zero-shot CoT, in this case is retrieval few-shot -> self-generated few-shot).\n\nThank you for your valuable feedback. We would like to highlight that our approach makes significant methodological contributions, beyond prompt engineering or the transition from retrieval few-shot to self-generated few-shot:\n- **New LLM reasoning paradigm**: We introduce a new problem-solving paradigm for LLM, inspired by analogical reasoning in psychology. As Polya's book \"How to Solve It\" (2004) states, when solving a new problem, humans draw from how they solved related problems in the past. We are the first to show that this strategy aids LLMs in solving complex reasoning problems (Section 1).\n- **Design of new techniques**: To ground this strategy in LLM prompting, we analyzed the design space and present effective techniques:\n  - Diversity in generated exemplars (Section 4.1)\n  - Sequential vs independent generation of exemplars (Section 4.1)\n  - Generation of high-level knowledge in addition to low-level exemplars (Section 4.2)\n- **Impact of generation v.s. retrieval**: Our method, based on generation rather than retrieval, offers greater generality and flexibility. It unlocks the generation of various forms of relevant information, such as exemplars, knowledge, formulas, tutorials, etc. (as experimented in Section 4.2). This goes beyond traditional retrieval methods that were confined to a pre-defined set (e.g., GSM8K train set), and empirically yields superior results too (Table 4).\n\n> The codeforces dataset only contains 50 questions; perhaps it is too small to make claims on the improvements (~2% is only one more solved question). Any experiments on larger code generation tasks, e.g., HumanEval?\n\nThis is a great point. The reason we used Codeforces 2023 for evaluation, despite its smaller size, was to prevent test set contamination; larger code generation datasets such as HuanEval and older versions of Codeforces may be used for LLM training, introducing the risk of contamination. To mitigate the issue of the small evaluation set, we repeated the experiments multiple times, each time using different LLM output samples, and then reported the average results (Section B).\n\nWe also evaluated our method on the larger, older Codeforces dataset from AlphaCode, involving 300 coding problems. Our prompting method outperformed the baseline CoT prompting by 4% in this setting as well.\n\n> There is error analysis for incorrectly solved tasks, how about correctly solved questions? How many generated exemplars are wrong, but the solution to the new question is correct (i.e., it is known that sometimes LLMs can few-shot generalize from wrong exemplars)?\n\nThank you for pointing this out. You are right. While not too often, there are cases where generated exemplars are wrong but the solution to the new question is correct. This result aligns with the previous finding that LLMs can few-shot generalize from wrong exemplars. Below are the details of 50 correctly solved problems and 50 incorrectly solved problems from GSM8K+MATH:\n- 50 correctly solved problems:\n  - (15/50) Generated exemplars are incorrect\n  - (35/50) Generated exemplars are correct\n- 50 incorrectly solved problems:\n  - (22/50) Generated exemplars are incorrect\n  - (28/50) Generated exemplars are correct"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4399/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700270099752,
                "cdate": 1700270099752,
                "tmdate": 1700270099752,
                "mdate": 1700270099752,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IHnzuSTvaU",
                "forum": "AgDICX1h50",
                "replyto": "83T7cm7uiw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4399/Reviewer_fGoM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4399/Reviewer_fGoM"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors' response. After reading the new experiment results and discussions from reviewer ExxM, my concerns remain the same. It looks like the correctness of exemplars doesn't matter much, so it is technically not analogical reasoning."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4399/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700592771774,
                "cdate": 1700592771774,
                "tmdate": 1700592771774,
                "mdate": 1700592771774,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6ij8vxvkeY",
            "forum": "AgDICX1h50",
            "replyto": "AgDICX1h50",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4399/Reviewer_ExxM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4399/Reviewer_ExxM"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose the use of analogical prompting to enhance reasoning performance. This method requires LLMs to first self-generate relevant examples or knowledge before attempting to solve a problem. Then, the model provides a response based on the generated concepts."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well-organized and easy to understand. And the proposed method is intuitive.\n2. The experiments demonstrate the effectiveness of the proposed methods across diverse datasets, while the ablation study indicates that the method outperforms the baseline approaches."
                },
                "weaknesses": {
                    "value": "1. Will the language model be easily distracted when the generated examples are irrelevant or incorrect? The experiments in section 6.6 should provide more details. For instance, it should specify how many problems in MATH fail due to incorrect and irrelevant generated examples.\n2. Can current LLMs generate helpful examples for challenging questions? The authors are encouraged to include more examples in the paper.\n3. Can this method be integrated with self-consistency decoding? For instance, could the majority voting result from multiple reasoning chains with the generated knowledge, lead to better outcomes?\n4. The authors are encouraged to include more qualitative examples to compare the behavior between reasoning with a 0-shot prompt and reasoning with generated examples."
                },
                "questions": {
                    "value": "The most important questions are mentioned in the \"Weaknesses\" section. Here is an additional question I am interested in, which may not necessarily be included in this paper.\n\nCan a verification stage be added to filter out incorrect or irrelevant examples to further improve performance?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4399/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4399/Reviewer_ExxM",
                        "ICLR.cc/2024/Conference/Submission4399/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4399/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698804510477,
            "cdate": 1698804510477,
            "tmdate": 1700436581496,
            "mdate": 1700436581496,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XrJPmvYVP6",
                "forum": "AgDICX1h50",
                "replyto": "6ij8vxvkeY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4399/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4399/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your review, and our response"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for insightful feedback. We have incorporated all suggestions in our paper. We thank the reviewer for describing our method as intuitive, well-motivated and demonstrating effectiveness across diverse datasets. We respond to the reviewer\u2019s concerns and questions below.\n\n> Will the language model be easily distracted when the generated examples are irrelevant or incorrect? \n\nGreat question. Below is the analysis of 50 correctly solved problems and 50 incorrectly solved problems from GSM8K+MATH. Overall, when the generated examples are irrelevant or incorrect, the LLM produces more wrong answers (22) than correct answers (15), but the gap is not large, indicating that LLM is not distracted to a critical extent.\n- 50 correctly solved problems:\n  - (6/50) Generated exemplars are irrelevant\n  - (9/50) Generated exemplars are relevant but incorrect\n  - (35/50) Generated exemplars are relevant and correct\n- 50 incorrectly solved problems:\n  - (10/50) Generated exemplars are irrelevant\n  - (12/50) Generated exemplars are relevant but incorrect\n  - (28/50) Generated exemplars are relevant and correct \n\n> Can current LLMs generate helpful examples for challenging questions?\n\nYes, we observed that the LLM generates helpful exemplars for challenging problems. It can generate relevant, simpler problems and their solutions, which help the LLM solve more challenging target problems. For instance, the Codeforces task often presents challenging code problems, and the LLM generates relevant basic problems like typical prefix sum and dynamical programming problems, which help solving the challenging target problems. We have added examples to the appendix D.\n\n> Can this method be integrated with self-consistency decoding?\n\nThis is a great suggestion. Yes, our method can be integrated with self-consistency and this indeed improves the performance:\n- GSM8K\n  - Ours: 77.8\n  - Ours + self-consistency: 85.3\n- MATH  \n  - Ours: 37.3\n  - Ours + self-consistency: 46.0\n\nIn particular, we find that self-consistency reduces the failure case of self-generating irrelevant examplers and getting distracted. This suggests that self-consistency and our analogical prompting can effectively complement each other.\n\n> The authors are encouraged to include more qualitative examples to compare the behavior between reasoning with a 0-shot prompt and reasoning with generated examples.\n\nThank you for your suggestion. We have added examples comparing 0-shot and our method in the Appendix D.\n\n\n> Here is an additional question I am interested in, which may not necessarily be included in this paper. Can a verification stage be added to filter out incorrect or irrelevant examples to further improve performance?\n\nThis is a very interesting idea. We experimented with this idea by sampling multiple exemplars, filtering out incorrect or irrelevant exemplars using GPT-4 as the verifier (i.e. prompting GPT-4), and then re-prompting the original LLM with the remaining exemplars to solve the target problem. We found this improves the task performance (e.g. 2% improvement in the MATH task)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4399/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700269917083,
                "cdate": 1700269917083,
                "tmdate": 1700269917083,
                "mdate": 1700269917083,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "feX0KmvlMr",
                "forum": "AgDICX1h50",
                "replyto": "XrJPmvYVP6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4399/Reviewer_ExxM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4399/Reviewer_ExxM"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response!"
                    },
                    "comment": {
                        "value": "Thank you for the detailed experiments and explanations provided. However, after reviewing the rebuttal, my concerns regarding the efficacy of the generated examples have increased. As a result, **I have adjusted my score from 6 to 5**.\n\nI would like to request further clarification on the following points:\n\n1. In my initial query, I asked for specific numbers regarding the sample distribution: how many samples are from the GSM8K dataset and how many are from the MATH dataset? This information is crucial for a comprehensive evaluation.\n\n2. Regarding unsolved questions, it appears that Language Models (LLMs) can provide relevant and correct examples in 56% of cases. This leads to a critical question: why are LLMs unable to solve these questions? Does this suggest that it is inherently challenging for LLMs to learn from generated examples, indicating that they are not particularly effective at analogical reasoning? If this is the case, could you propose any directions or strategies for achieving further improvements in this area?"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4399/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700436567826,
                "cdate": 1700436567826,
                "tmdate": 1700436567826,
                "mdate": 1700436567826,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "No8PZeAyKq",
                "forum": "AgDICX1h50",
                "replyto": "6ij8vxvkeY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4399/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4399/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your questions, and our clarification"
                    },
                    "comment": {
                        "value": "Thanks for reading our response! \n\n> 1. how many samples are from the GSM8K dataset and how many are from the MATH dataset?\n\nIt is 50%-50% ratio from GSM8K and MATH (paper Section 6.6).\n\n> 2. Regarding unsolved questions, it appears that Language Models (LLMs) can provide relevant and correct examples in 56% of cases. Why are LLMs unable to solve these questions?\n\nBelow is the analysis of these unsolved problems where LLMs provided relevant and correct exemplars (full detail in paper Section 6.6):\n\n- (28/50) Generated exemplars are relevant and correct, but LLM fails to solve the new problem:\n  - (12/50) A generalization gap between the generated exemplars and the new problem. \n  - (8/50) Overreliance on specific exemplars, such as copying.\n  - (8/50) Other issues, such as calculation errors.\n\nRemaining 22/50 unsolved problems:\n- (10/50) Generated exemplars are irrelevant\n- (12/50) Generated exemplars are relevant but incorrect\n\nAs this suggests, among the unsolved problems, the error modes are spread evenly (irrelevant exemplars, incorrect exemplars, correct exemplars with generalization gap, other LLM mistakes). Therefore, we do not think there is a particular weakness such as \"it is inherently challenging for LLMs to learn from generated examples\". \n\nRegarding future directions to achieve further improvements:\n\n- To generate more relevant and correct exemplars: as the reviewer suggested, we can add a verification stage to filter out incorrect or irrelevant examples\n- To generate exemplars with less generalization gap: we can include additional instruction in the prompts so that we generate exemplars sequentially from simpler exemplars to harder exemplars that are closer to the target problem.\n\n**Most importantly**, please note that this analysis is an **error analysis**, which **only looks at unsolved problems** (only a part of the total event space) to see \"among unsolved problems, how often the LLMs provided relevant and correct exemplars\". In the majority of situations, the LLMs provided relevant and correct exemplars, and solved the target problem correctly, i.e., performed analogical reasoning successfully.\n\nWe hope that this response resolves the confusion or concerns the reviewer may have. Please feel free to let us know if you have any further questions!"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4399/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700440398883,
                "cdate": 1700440398883,
                "tmdate": 1700441299522,
                "mdate": 1700441299522,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CDsFkNfCSv",
                "forum": "AgDICX1h50",
                "replyto": "No8PZeAyKq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4399/Reviewer_ExxM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4399/Reviewer_ExxM"
                ],
                "content": {
                    "title": {
                        "value": "More Discussions"
                    },
                    "comment": {
                        "value": "1. I'm interested in separate analyses for GSM8K and MATH.\n\n2. Regarding the table:\n50 correctly solved problems:\n(6/50) Generated exemplars are irrelevant\n(9/50) Generated exemplars are relevant but incorrect\n(35/50) Generated exemplars are relevant and correct\n50 incorrectly solved problems:\n(10/50) Generated exemplars are irrelevant\n(12/50) Generated exemplars are relevant but incorrect\n(28/50) Generated exemplars are relevant and correct\n\nThis indicates exemplars are useful in 56% of unsolved problems and 70% of solved ones. However, in 30% of solved problems, LLMs disregard the exemplars. Additionally, LLMs fail to benefit from exemplars in at least 56% of unsolved problems. This suggests difficulty in altering LLMs' reasoning from their trained biases. Thus, I am not confident that the current LLMs are already good analogical reasoners.\n\nIn the end, I think it is an interesting direction. But current experiments cannot fully convince me of its efficacy. Thus, I will keep the score **5**."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4399/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700457613454,
                "cdate": 1700457613454,
                "tmdate": 1700457613454,
                "mdate": 1700457613454,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Cwf4x4mOpD",
                "forum": "AgDICX1h50",
                "replyto": "6ij8vxvkeY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4399/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4399/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your questions, and our clarification"
                    },
                    "comment": {
                        "value": "These are great questions.\n\n> I'm interested in separate analyses for GSM8K and MATH.\n\nGSM8K:\n- 25 solved problems\n  - (2/25) Generated exemplars are irrelevant\n  - (4/25) Generated exemplars are relevant but incorrect\n  - (19/25) Generated exemplars are relevant and correct\n- 25 unsolved problems\n  - (4/25) Generated exemplars are irrelevant\n  - (5/25) Generated exemplars are relevant but incorrect\n  - (16/25) Generated exemplars are relevant and correct\n    - (7/25) A generalization gap between the generated exemplars and the new problem (e.g., harder).\n    - (5/25) Overreliance on specific exemplars, such as copying.\n    - (4/25) Other issues, such as calculation errors.\n\nMATH:\n- 25 solved problems\n  - (4/25) Generated exemplars are irrelevant\n  - (5/25) Generated exemplars are relevant but incorrect\n  - (16/25) Generated exemplars are relevant and correct\n- 25 unsolved problems\n  - (6/25) Generated exemplars are irrelevant\n  - (7/25) Generated exemplars are relevant but incorrect\n  - (12/25) Generated exemplars are relevant and correct\n    - (5/25) A generalization gap between the generated exemplars and the new problem (e.g., harder).\n    - (3/25) Overreliance on specific exemplars, such as copying.\n    - (4/25) Other issues, such as calculation errors.\n\n> in 30% of solved problems, LLMs disregard the exemplars\n\nThis means that LLM solved the target problems correctly even when the generated exemplars were irrelevant or incorrect. This is reasonable \u2013 it is known that LLMs can few-shot learn even from wrong in-context exemplars (https://arxiv.org/abs/2202.12837).\n\n>  LLMs fail to benefit from correct exemplars in at least 56% of unsolved problems. current experiments cannot fully convince me of its efficacy\n\nWe would like to clarify that this is reasonable. Our work does **not** suggest that when the exemplars are correct, LLMs should always produce the correct answer. For example, few-shot CoT always uses correct exemplars, but can fail to solve the target problem too \u2013 in fact, fails more often than our method (Table 1). **Our main contribution is that we show our method outperforms manual few-shot CoT**, i.e., LLMs benefit more from our self-generated exemplars than hand-written exemplars."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4399/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700486752162,
                "cdate": 1700486752162,
                "tmdate": 1700486824056,
                "mdate": 1700486824056,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yViKAmA6tK",
                "forum": "AgDICX1h50",
                "replyto": "Cwf4x4mOpD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4399/Reviewer_ExxM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4399/Reviewer_ExxM"
                ],
                "content": {
                    "title": {
                        "value": "More comments"
                    },
                    "comment": {
                        "value": "If the paper aims to demonstrate better automatic prompt engineering, then you need to provide more comparisons with existing works, such as \"Large Language Models Are Human-Level Prompt Engineers\" and its follow-up. Or you need to include experiments to demonstrate when LLMs are analogical reasoners and when not. **At this time, the paper is not strong enough to get in.**"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4399/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688151665,
                "cdate": 1700688151665,
                "tmdate": 1700688151665,
                "mdate": 1700688151665,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DmfhHDWi42",
            "forum": "AgDICX1h50",
            "replyto": "AgDICX1h50",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4399/Reviewer_DVcm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4399/Reviewer_DVcm"
            ],
            "content": {
                "summary": {
                    "value": "The authors proposes a new prompting paradigm, which has three phases: \n1. Related knowledge retrieval\n2. Exemplar generation\n3. Answer prompting. \n\nWith this paradigm, the prompting on math and reasoning required tasks has an average accuracy gain of +4%."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Originality: 3/5 \n\nThis paper aims to solve the problem that the few-shot prompting schema requires manually collected examples via a template-based method. \nAlthough there are quite a few previous works on prompt templates and knowledge retrieval, such as recitation-augmented models, this work focuses more on reasoning-based problem-solving. \n\nQuality: 2.5/5\n\nThis work has performed studies on quite a few benchmarks, including GSM8K and MATH, code generation in Codeforces, and other reasoning tasks in BIG-Bench. However, the experiment setup is a bit weird, as some of the studies include the in-context demonstration of generating examples, some do not; some adopt with knowledge paradigm, and some do not. A minor concern is that this study does not include the GPT-4 performance. \n\nClarity: 3.5/5\n\nOverall the paper is well written and easy to follow. The examples are quite illustrative but may be a bit repetitive, as Figure 3 seems to already include all the information that Figure 2 contains. \n\nSignificance: 3/5\n\nThis method proposes a new prompting schema for leveraging the language model as a knowledge base. However, this exemplar generation procedure does not provide an in-depth guarantee or study on the quality of generated examples."
                },
                "weaknesses": {
                    "value": "1. The experiment setup could be better."
                },
                "questions": {
                    "value": "1. It would be nice to conceptually compare the work against the neural symbolic method [1]. \n\n[1] Zhang, Hanlin, et al. \"Improved logical reasoning of language models via differentiable symbolic programming.\" arXiv preprint arXiv:2305.03742 (2023)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4399/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698809936931,
            "cdate": 1698809936931,
            "tmdate": 1699636413010,
            "mdate": 1699636413010,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bkSF7BwlQt",
                "forum": "AgDICX1h50",
                "replyto": "DmfhHDWi42",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4399/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4399/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your review, and our response"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for constructive feedback. We have incorporated all suggestions in our paper. We appreciate that the reviewer describes our work as offering a novel and effective prompting strategy for LLMs. We respond to the reviewer\u2019s concerns and questions below.\n\n> experiment setup is a bit weird, as some of the studies include the in-context demonstration of generating examples and knowledge paradigm. A minor concern is that this study does not include the GPT-4 performance.\n\nWe included the specific setups, such as in-context demonstration and knowledge generation, for the reasons outlined below. We appreciate the reviewer's suggestion on improving our presentation, and we've addressed it in the updated paper.\n- Re: In-context demonstration. We included this setup to experiment with our approach across various LLMs. GPT3.5-turbo and GPT-4 were trained with extensive instruction tuning / RLHF, and they do not need any in-context demonstration to be able to generate exemplars and solutions. On the other hand, Davinci models were trained with less instruction tuning / RLHF, and in such cases, in-context demonstration boosts their performance (Table 1). \n- Re: Self-generation of knowledge. This technique supplements our primary approach of self-generating exemplars. This is especially useful for complex tasks like code generation, where high-level knowledge complements low-level exemplars. We also evaluated this knowledge generation technique in other tasks like GSM8K and obtained performance boosts (0.5%).\n- Re: GPT-4. We reported GPT-4 performance mainly for the challenging Codeforces 2023 task (Table 2), considering that in other tasks like GSM8K, the base GPT-4 performance was already high (e.g. 92% accuracy), possibly because these task datasets were seen during GPT-4 training. Nevertheless, in both cases, our method provides performance boosts over the baseline CoT for GPT-4: 4% in Codeforces and 1% in GSM8K.\n\nIn our updated paper, we present all models (including GPT-4) and setups (exemplar generation and knowledge generation) in a cohesive manner.\n\n> does not provide study on the quality of generated examples.\n\nBelow is the manual analysis of the quality of generated examples, based on 50 correctly solved problems and 50 incorrectly solved problems from GSM8K+MATH:\n- 50 correctly solved problems:\n  - (6/50) Generated exemplars are irrelevant\n  - (9/50) Generated exemplars are relevant but incorrect\n  - (35/50) Generated exemplars are relevant and correct\n- 50 incorrectly solved problems:\n  - (10/50) Generated exemplars are irrelevant\n  - (12/50) Generated exemplars are relevant but incorrect\n  - (28/50) Generated exemplars are relevant and correct \n\nOverall, we find that the majority of the generated exemplars relevant and correct. We have added the detailed analysis to our paper.\n\n\n> It would be nice to conceptually compare the work against the neural symbolic method [1].\n\nThank you for pointing us to the related work. We have added a citation to the neural symbolic work in our paper. Our approach, analogical prompting, is distinct yet complementary to neural symbolic methods. We present a high-level LLM reasoning strategy, demonstrating how generating related exemplars through analogy aids in problem-solving. Neural symbolic methods could further complement the problem-solving stage, which is an interesting avenue for future research."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4399/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700269730470,
                "cdate": 1700269730470,
                "tmdate": 1700269730470,
                "mdate": 1700269730470,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]