[
    {
        "title": "Learning Hierarchical Image Segmentation For Recognition and By Recognition"
    },
    {
        "review": {
            "id": "bN6E0raLce",
            "forum": "IRcv4yFX6z",
            "replyto": "IRcv4yFX6z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6646/Reviewer_GfNL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6646/Reviewer_GfNL"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces CAST (Concurrently Adaptive Segmentation Tokens).\nThis builds on existing work where superpixels are used as tokens in a\nvision transformer. The new contribution is that graph pooling is used to\nadaptively cluster segments to create a hierarchy, allowing segmentation (and\nclassification) at a coarse and fine-grained level. \n\nSEEDS is used to define superpixels via over-segmentation. A CNN produces\ninitial features on the original pixel regular grid. The mean of CNN features\nunder a superpixel define the feature-token for that superpixel. Positional\nencodings are created in a similar way.\n\nSuperpixels are used as leafs of a tree, which is adaptively defined by graph\npooling. This tree has L layers, and starting from the leafs, the next layer is\ndefined by first assuming that there are C coarser segments at the next level,\nand then for each token the network predicts a probability for which coarser\nsegment each finer segment should be assigned to. The argmax of the probability\nis the assignment. The number of coarser segments is defined adaptively via the\nFarthest Point Sampling algorithm.\n\n\nResults are reported on:\n\n* PartImageNet, where CAST is compared to a baseline ViT and SAM-B, where is shows a significant performance improvement over the baseline in most cases and also with lower computational costs.\n\n* DensePose Human part segmentation, and is comparable to the HSG baseline.\n\n* Pascal VOC Segmentation - where CAST and ViT are trained on COCO. Ablation studies are\n  done here. CAST outperforms the baseline in all reported cases.\n\n* Pascal VOC classification - where CAST is compared to ViT and Swin using a linear probing evaluation."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The algorithm design is well motivated.  \n\nThe technique is tested against strong baselines, and shows benefits in both quality and computational metrics. \n\nDesign choices are ablated.\n\nAdditional details and failure cases are given in the appendix."
                },
                "weaknesses": {
                    "value": "The tone is a bit to strong and unscientific at times. Value judgements are\nplaced on things, instead of performing comparisons and reporting results. I\nrecommend rewording in certain areas:\n\n> We assert that our design is the proper approach for true vision transformer, distinct from the text-inspired ViT.\n\nThere is no \"proper\" approach. You can assert it might be a more natural choice in some circumstances, but in the instance where superpixel segmentation fails, then this is very much not the correct approach.\n\n> CAST not only discovers the hierarchy but also enhances flat semantic segmentation, indicating its superiority in learning dense representations compared to ViT.\n\nSuperiority is too strong. It might be ok, but given the overstated tone in the rest of the paper, I think: \"indicating that it learns richer dense representations\" might be a better wording. \n\n> Sec. 4.4. We conducted an ablation study on our design choices, confirming that CAST is better\nthan previous token pooling approaches for ViT (Marin et al., 2021; Sarfraz et al., 2019).\n\nIt's not confirmed. You made a measurement that contributed evidence towards\nthe idea. Confirming is something that someone who replicates your work will\ndo. I recommend changing the word \"confirmed\" to measured in most places."
                },
                "questions": {
                    "value": "> The resulting input segment features are defined as Z0 = [Xclass; Xs] + Epos\n\nI've seen people simply adding positional encodings to the token features directly rather than concatenating them as additional feature channels. I must have missed the work that introduced / ablated this decision. Is there a reference you can point me to on this?\n\n\n> We initiate the next-level coarse regions by sampling centroids from segment tokens Zl\u22121 at level l \u2212 1 and compute Pl based on token similarity, with C representing the sampled centroid indices.\n\nAre these centroids in position space or feature space?\n\n\n> All models are self-supervisedly trained on IN-100 and evaluated using linear probing.\n\nIs there a reference for this? I'm not familiar with this and it seems like a traditional imagenet-style classification benchmark would be a better comparison where there was a defined groundtruth."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6646/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6646/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6646/Reviewer_GfNL"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6646/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698609618880,
            "cdate": 1698609618880,
            "tmdate": 1700693962296,
            "mdate": 1700693962296,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SfC0B904eK",
                "forum": "IRcv4yFX6z",
                "replyto": "bN6E0raLce",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6646/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6646/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GfNL"
                    },
                    "comment": {
                        "value": "Dear reviewer GfNL,\n\nThank you for your valuable feedback and comments. We appreciate your remarks on the motivation, method design, and experimental results of our work. We will address your concerns and questions in the response below.\n\n---\n**[W1] Tone is a bit too strong**\n\nThank you for your suggestions. We revised our manuscript accordingly.\n\n---\n**[Q1] How to incorporate positional encodings?**\n\nWe added the positional encodings instead of concatenating them like standard ViTs. Please note that $E_\\text{pos}$ is added ($+$) to the tokens in the equation on page 4 of our paper.\n\n---\n**[Q2] Centroids from position or feature space?**\n\nThe centroids are sampled based on token similarity in the feature space, though it also reflects the position information through positional encodings. We have clarified this on page 5 of our revised manuscript.\n\n---\n**[Q3] Reference for linear probing on IN-100**\n\nLinear probing is a widely used protocol for evaluating self-supervised learning methods [1-3]. It involves training a linear classifier on top of learned representations to measure classification accuracy. Prior works [4-5] used ImageNet-100 for linear probing, where the top-1 accuracy (~80%) in Table 1 matches that in Table 4 of our paper. The detailed procedure has been explained in Appx. C.4 of our manuscript.\n\n[1] He et al. Momentum Contrast for Unsupervised Visual Representation Learning. CVPR 2020.\\\n[2] Chen et al. A Simple Framework for Contrastive Learning of Visual Representations. ICML 2020.\\\n[3] Caron et al. Emerging Properties in Self-Supervised Vision Transformers. ICCV 2021.\\\n[4] Xiao et al. What Should Not Be Contrastive in Contrastive Learning. ICLR 2021.\\\n[5] Costa at al, Solo-learn: A Library of Self-supervised Methods for Visual Representation Learning. JMLR 2022.\n\n---\nPlease let us know if you have any further concerns.\n\nSincerely,\\\nAuthors"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700363031479,
                "cdate": 1700363031479,
                "tmdate": 1700503153920,
                "mdate": 1700503153920,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0GKjlBh8Ka",
                "forum": "IRcv4yFX6z",
                "replyto": "SfC0B904eK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6646/Reviewer_GfNL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6646/Reviewer_GfNL"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you, I've updated my rating of presentation from 2 to 3. I like the paper and will stick with the score of 8."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700694062834,
                "cdate": 1700694062834,
                "tmdate": 1700694062834,
                "mdate": 1700694062834,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7F5oEIxTmY",
            "forum": "IRcv4yFX6z",
            "replyto": "IRcv4yFX6z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6646/Reviewer_hsyp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6646/Reviewer_hsyp"
            ],
            "content": {
                "summary": {
                    "value": "This work addresses image classification and hierarchical image segmentation. No pixel-wise ground truth is provided in training for segmentation, so the proposed ViT-based approach, CAST, is trained by self-supervision and supervised image classification. CAST replaces the fix-grid patches with superpixels, and uses a graph pooling to aggregate features to sampled cluster centroids. The resulting graph-pooled superpixel tokens are aimed at better incorporating the local-global hierarchical semantics, and can produce image segmentation. CAST is evaluated on multiple datasets, and outperforms SOTA."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper is well-written and easy to follow. The experiments are comprehensive, and the reported results are promising.\n\nThe motivation for using the superpixel token to replace the fixed-grid patch token in ViT is clear and interesting. As the initial superpixels are estimated based on low-level visual cues, the graph-pooled superpixel tokens incorporate appearance and shape details, as well as local-global hierarchical relationships. This seems to help give sharper segmentation masks than in related work."
                },
                "weaknesses": {
                    "value": "Overall novelty seems limited. The proposed graph-pooling for aggregating fine-level tokens into coarse-level tokens is similar to the Multi-stage Token Aggregation in TCFormer (Zeng et al 2022) and the Token Merging in ToMe (Bolya et al 2023). A comparison of CAST with these two methods is missing.\n\nThe method increases complexity relative to using the fixed-grid of patches, since it requires estimation of superpixels and sampling the clusters. Performance seems to be critically dependent on the quality of superpixels and the cluster sampling methods. The paper poorly tests performance wrt varying quality of superpixels and cluster sampling. It is unsatisfying that these two components are not learnable, and that they are not end-to-end integrated with the rest of CAST."
                },
                "questions": {
                    "value": "What is the \"MLP ratio\" in section 4.4 and Table 6.c? Above equation 3, it shows f(a,b) = a + MLP(b), but there is no coefficient there."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6646/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698699216787,
            "cdate": 1698699216787,
            "tmdate": 1699636759521,
            "mdate": 1699636759521,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PVD0tU26o5",
                "forum": "IRcv4yFX6z",
                "replyto": "7F5oEIxTmY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6646/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6646/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer hsyp"
                    },
                    "comment": {
                        "value": "Dear reviewer hsyp,\n\nThank you for your valuable feedback and comments. We appreciate your remarks on the motivation, experimental results, and presentation of our work. We will address your concerns and questions in the response below.\n\n---\n**[W1] Technical novelty over token pooling**\n\nOur graph pooling is just a device for implementing our concept of hierarchical segmentation, and any token pooling method can be used. We emphasize that our goal is to develop a framework that integrates segmentation into the recognition process, unlike previous token pooling methods that focus on ViT efficiency. As a reminder, we have discussed this in the related section, properly mentioning both suggested works.\n\nWe employed graph pooling because it performed the best in our experiments. In particular, we compared various token pooling algorithms in Table 6a, including Token Pooling (Marin et al., 2021) and FINCH (Sarfraz et al., 2019). Additionally, following your suggestion, we conducted further comparisons with state-of-the-art methods, TCFormer (Zeng et al., 2022) and ToMe (Bolya et al., 2023), using the same setup as in Table 6a. For TCFormer, we modified the Clustering-based Token Merge (CTM) module by removing the convolutional layer to apply it to our superpixel tokens. For ToMe, we reduced the number of tokens per layer to 16 to align with the latency of other methods. We stated the implementation details in Appx. A.5.\n\nThe table below presents the results, including both the values from Table 6a and the new results. While both CTM and ToMe perform well, our graph pooling outperforms them. We have updated the table in our revised manuscript.\n\n| Pooling Method       | Accuracy |\n|----------------------|----------|\n| Graph Pooling        | **79.9** |\n| Random Sampling      | 55.8     |\n| K-Means              | 73.9     |\n| K-Medoids            | 72.3     |\n| FINCH                | 63.3     |\n| Token Pooling        | 75.8     |\n| CTM (new results)   | 72.2     |\n| ToMe (new results)   | 78.1     |\n\n---\n**[W2] Ablation study on superpixel and cluster sampling methods**\n\n*Superpixels.*\nWe used SEEDS superpixels for simplicity, as they performed well. However, we note that CAST also performs well with other reasonable superpixel methods. To verify this, we conducted an additional ablation study on superpixel methods. Specifically, we train CAST-S models using SEEDS (Bergh et al., 2012) and SLIC (Achanta et al., 2012) superpixels with MoCo objectives. We report the linear probing accuracy on ImageNet-1K using the models trained on ImageNet-1K and mIoU on Pascal VOC using the models trained on COCO, both before (left) and after (right) fine-tuning.\n\nThe table below compares the classification and segmentation performance of SEEDS and SLIC. Both superpixels performed reasonably well, with SEEDS outperforming SLIC by capturing better boundaries. We included deeper discussions and visual examples of superpixels in Appx. B.8 of our revised manuscript.\n\n| Superpixel | Classification | Segmentation |\n|--------|--------|-------|\n| SEEDS  | 67.0   | 38.4/67.6 |\n| SLIC   | 65.6   | 37.7/65.7 |\n\nNevertheless, we acknowledge that CAST indeed depends on the quality of superpixels. We discussed this limitation in Appx. B.7, illustrating that superpixels may not capture thin structures like light poles. Addressing this issue, perhaps by jointly learning superpixels, would be an interesting future direction.\n\n*Cluster sampling.*\nOur cluster sampling is jointly learned with feature development in CAST. On one hand, our cluster sampling and assignments are determined by the learned feature distances. On the other hand, our clusterings facilitate the final recognition process, guiding feature learning. Thus, cluster sampling and feature learning mutually influence each other in CAST. We note that Farthest Point Sampling (FPS) is just one natural method for partitioning features into clusters, and other reasonable clustering methods that encourage maximum margins between clusters would perform similarly well.\n\n---\n**[Q1] Definition of the MLP ratio?**\n\nThe MLP ratio refers to the multiplier for the dimension of the MLP used in the self-attention block within our graph pooling module. We clarified this in our revised manuscript.\n\n---\nPlease let us know if you have any further concerns.\n\nSincerely,\\\nAuthors"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700363084797,
                "cdate": 1700363084797,
                "tmdate": 1700697734189,
                "mdate": 1700697734189,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Hp7FJjO4AL",
            "forum": "IRcv4yFX6z",
            "replyto": "IRcv4yFX6z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6646/Reviewer_RnRE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6646/Reviewer_RnRE"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a novel method that adopts superpixel as the token for vision transformer. The superpixel tokens are hierarchically grouped together in each layer via graph pooling. This gives rise to a hierarchical segmentation result  using only image classification label. Experimental results demonstrate the usefulness of the proposed method."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- Using superpixel tokens is more appropriate for vision transformers. The proposed method delivers a hierarchical segmentation result using only image-level annotation.  I really appreciate the concept of using hierarchical segmentations as the tokens in different layers. In particular, the proposed method starts with a finest segmentation given by superpixels, and learns automatically the hierarchical segmentation. In my opinion, using different regions in hierarchical segmentation is a more natural and powerful way of image tokenization. \n- The experimental results are quite convincing. The generated hierarchical segmentation is visually impressive. As depicted in Table 1 and 3, the proposed method significantly outperforms some baseline methods (e.g., the powerful segment anything model), while being also efficient."
                },
                "weaknesses": {
                    "value": "- The runtime analysis is missing. Compared with the vanilla ViT, the proposed method involves superpixel generation and graph pooling. Does this require much more extra runtime?\n- Ablation study of using different superpixel methods. Is this proposed method sensitive to the finest segmentation ?"
                },
                "questions": {
                    "value": "Does this require much more extra runtime compared with classical ViT.\nIs this proposed method sensitive to the finest segmentation ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "10: strong accept, should be highlighted at the conference"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6646/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698911862537,
            "cdate": 1698911862537,
            "tmdate": 1699636759396,
            "mdate": 1699636759396,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3zpzIxjKxW",
                "forum": "IRcv4yFX6z",
                "replyto": "Hp7FJjO4AL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6646/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6646/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer RnRE"
                    },
                    "comment": {
                        "value": "Dear reviewer RnRE,\n\nThank you for your valuable feedback and comments. We appreciate your remarks on the motivation, method design, and experimental results of our work. We will address your concerns and questions in the response below.\n\n---\n**[W1] Runtime analysis**\n\nThe computational cost of superpixel generation and graph pooling can be reduced by the decreased number of tokens required for computing self-attention. This advantage becomes more pronounced when using larger models, where self-attention blocks dominate the entire cost. To validate this, we analyze the latency of model inference and superpixel generation. Our system comprises a 32GB Nvidia Titan V GPU card and two Intel(R) Xeon(R) CPU E5-2630 v4 processors, totaling 20 CPU cores. We utilize the PyTorch machine learning framework with 24 workers, a batch size of 64, and an image resolution set to 224x224.\n\nIn our system, CAST-B achieves a lower average inference latency of 217 ms compared to ViT-B with 273 ms. SEEDS takes 73 ms to generate superpixels from the batch of images. However, we remark that the current SEEDS implementation is not fully optimized. Employing GPU implementation or parallelizing the process with more CPU cores can alleviate the bottleneck in superpixel generation. Furthermore, the cost of superpixel generation becomes less significant with larger models, which are commonly used in practice. We have included these discussions in Appx. B.1 of our revised manuscript.\n\n---\n**[W2] Ablation study on superpixel methods**\n\nCAST performs well with reasonable superpixel methods. To verify this, we conducted an additional ablation study on superpixel methods. Specifically, we train CAST-S models using SEEDS (Bergh et al., 2012) and SLIC (Achanta et al., 2012) superpixels with MoCo objectives. We report the linear probing accuracy on ImageNet-1K using the models trained on ImageNet-1K and mIoU on Pascal VOC using the models trained on COCO, both before (left) and after (right) fine-tuning.\n\nThe table below compares the classification and segmentation performance of SEEDS and SLIC. Both superpixels performed reasonably well, with SEEDS outperforming SLIC by capturing better boundaries. We included deeper discussions and visual examples of superpixels in Appx. B.8 of our revised manuscript.\n\n| Superpixel | Classification | Segmentation |\n|--------|--------|-------|\n| SEEDS  | 67.0   | 38.4/67.6 |\n| SLIC   | 65.6   | 37.7/65.7 |\n\nNevertheless, we acknowledge that CAST indeed depends on the quality of superpixels. We discussed this limitation in Appx. B.7, illustrating that superpixels may not capture thin structures like light poles. Addressing this issue, perhaps by jointly learning superpixels, would be an interesting future direction.\n\n---\nPlease let us know if you have any further concerns.\n\nSincerely,\\\nAuthors"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700363227116,
                "cdate": 1700363227116,
                "tmdate": 1700697782562,
                "mdate": 1700697782562,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GtHJBOtrXQ",
            "forum": "IRcv4yFX6z",
            "replyto": "IRcv4yFX6z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6646/Reviewer_5FXt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6646/Reviewer_5FXt"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents CAST - a variant of ViTs that does hierarchical segmentation of inputs and tokens as part of its pipeline. The method is simple - input pixels are partitioned into super-pixels (in contrast to simple patches for ViTs) a convolutional network is applied over the input image and the resulting features are pooled across the superpixels (average pooling) to create the initial set of tokens. \nFrom here the method works by passing the set of tokens through ViT blocks and between them graph pooling layers which group tokens into a coarser set of tokens by sampling centroids and measuring similarity between tokens and centroids. \n\nThe method is trained with a self supervised objective (MoCo, read out from the last layer) and demonstrated to work well in a variety of contexts."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I think this is an interesting paper with good motivation and a simple and effective implementation of core ideas.\n\n* While simple, the use of super pixels and feature pooling to replace the awkward patch based embeddings of ViTs is a nice and original idea.\n* The method is quite elegant and naturally lends itself to several use cases which are demonstrated in the paper in a convincing manner (mostly)\n* Paper is well presented and well executed and was quite easy to follow.\n* A highly applicable method which should be easy to use in several contexts so significant to the community."
                },
                "weaknesses": {
                    "value": "I have some (relatively minor) concerns about parts of this work:\n\n* The major thing which I find missing is more in-depth discussion of the role of super-pixels here - I feel the ablation table is missing one critical row which is a ViT with a super-pixel (+conv net) embedding layer instead of patch extraction. This is also a possible explanation to the results in Table 5. There is a chance that super-pixels are the source of most of the improvements in the paper (quantitatively, at least, clearly there would be no clear hierarchical structure in this case) and this is not discussed enough.\n\n* The positional embedding pooling is a bit odd - super pixels can be with quite arbitrary shapes and there is a chance the resulting average PE over the super pixels would bare little connection to the actual \"position\" of the super pixel. Can the authors comment on that?\n\n* While the authors claim there's a top-down element here (and arguably there is through training) I would argue it is not truly top-down at inference. The super pixels and segmentations do not depend and will not change based on what the top layers infer - the network has top-down pathway to inform them (other than gradients in training). This is of course fine, but it should be discussed.\n\nI hope to see a discussion about the points above and I would be happy to raise my score if these are discussed and addressed to."
                },
                "questions": {
                    "value": "More minor questions:\n\n* How do segmentations are extracted from ViTs in the paper? it's not clear to me how the results in, say, Figure 4, were generated for ViT-S.\n\n* Is the PE added to the CLS token as well?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6646/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6646/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6646/Reviewer_5FXt"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6646/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699362826551,
            "cdate": 1699362826551,
            "tmdate": 1700650235431,
            "mdate": 1700650235431,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mmOsgV2gLw",
                "forum": "IRcv4yFX6z",
                "replyto": "GtHJBOtrXQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6646/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6646/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5FXt"
                    },
                    "comment": {
                        "value": "Dear reviewer 5FXt,\n\nThank you for your valuable feedback and comments. We appreciate your comments on the motivation, method design, presentation, and applicability of our work. We will address your concerns and questions in the response below.\n\n---\n**[W1] Discussion on the role of superpixels**\n\nWe have shown that superpixels are not the sole reason for improvements. Please refer to the discussion in Table 3a, which reports (non-hierarchical) semantic segmentation performance. Here, superpixels enhance segmentation by providing better boundaries, as you suggested. However, combining hierarchical pooling leads to even greater improvements, enabling the model to grasp the structure of entire objects. This supports that both superpixels and pooling are essential, even for applications that do not require explicit hierarchy. We have further clarified these points on page 8 of our revised manuscript.\n\nWe have included the reorganized Table 3a below for your convenience. In this table, we report the mIoU (left) and boundary F-score (right) of segmentation before and after fine-tuning the models. Superpixels improve the original ViT by +5.1% in the boundary F-score. However, hierarchical pooling further enhances ViT + Superpixel by +6.8%.\n\n| Model | Before tuning | After tuning |\n|-------|-------|-------|\n| ViT | 30.9/16.1 | 65.8/40.7 |\n| ViT + Superpixel | 32.2/21.2 | 66.5/46.7 |\n| ViT + Superpixel + Hierarchy (CAST) | **38.4**/**27.0** | **67.6**/**48.1** |\n\n---\n**[W2] Pooling over positional embeddings**\n\nPooling over positional embeddings is as natural as pooling over token features, with the former characterizing spatial location and the latter characterizing appearance, which are two aspects of a segment. In fact, averaged positional embeddings serve as summary statistics for representing pooled segments, even when the corresponding location falls outside of a non-convex shaped segment pool. Prior works have also adopted a similar design choice. For example, SLIC (Achanta et al., 2012) calculates centroids by averaging LAB color features and XY locations within pixel clusters.\n\n---\n**[W3] Top-down recognition during inference**\n\nThank you for the insightful suggestion! As you noted, CAST only considers the top-down pathway during training, guided by the recognition objectives. This knowledge is encoded in the model and reflects the bottom-up pathway during inference. While this enables the model to learn the general top-down elements, the segmentations will not change based on what the top layers predict at inference time.\n\nTo extend CAST by incorporating the top-down pathway during inference, we employ test-time adaptation (TTA) [1], specifically TENT [2], with the classifier trained on top of the CAST backbones. We apply TENT to each sample to adapt the model and maximize its prediction confidence. As a result, CAST refines its object segments to align with its initial belief: If this image depicts the predicted class, which parts contribute to this prediction?\n\nWe have included the detailed approach and experimental results in Appx. D of our revised manuscript. We trained a dog vs. non-dog classifier on PartImageNet and evaluated the evolution of segmentation before and after TTA. Figure 16 showcases visual examples where TTA improves the segmentation of CAST by capturing object shapes more effectively (e.g., not missing legs in rows 1-3) while reducing attention to unnecessary details (e.g., window frames in row 3). The improvement is more substantial for challenging samples that the CAST originally fails on, increasing the mIoU from 69.5% to 76.6% (+7.1%) for samples with an original mIoU of less than 80%.\n\n[1] Sun et al. Test-Time Training with Self-Supervision for Generalization under Distribution Shifts. ICML 2020.\\\n[2] Want et al. Tent: Fully Test-time Adaptation by Entropy Minimization. ICLR 2021.\n\n---\n**[Q1] How to generate segments from ViT?**\n\nWe generate segments from ViT by applying K-Means clustering to the final output tokens. We bilinearly upscale the feature map and then apply K-Means to the pixel-level features to align the segments with the input image resolution. Similar to CAST, we cross-reference clustering assignments across different levels to achieve hierarchical image segmentation. To maintain a consistent cluster hierarchy, we iteratively run K-Means, reducing the number of clusters by grouping the clusters from the previous iteration into coarser clusters. We have clarified this procedure on page 5 and in Appendix A.4 of our revised manuscript.\n\n---\n**[Q2] PE added to the CLS token?**\n\nFollowing MoCo-v3, we used fixed sine-cosine positional embeddings for patches and an all-zero vector for the CLS token. The all-zero vector matches the length of positional embeddings with tokens but has no impact. We have clarified this in the equation on page 4 of our revised manuscript.\n\n---\nPlease let us know if you have any further concerns.\n\nSincerely,\\\nAuthors"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700363278522,
                "cdate": 1700363278522,
                "tmdate": 1700499609265,
                "mdate": 1700499609265,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fFZlsD7Ype",
                "forum": "IRcv4yFX6z",
                "replyto": "mmOsgV2gLw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6646/Reviewer_5FXt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6646/Reviewer_5FXt"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your detailed response!"
                    },
                    "comment": {
                        "value": "I appreciate the time taken to address my and the other reviewers' concerns.\n\nI think the TTA results are really nice and happy these were tried out. I also appreciate the reorganized table 3 - it's definitely clearer now. \nI have raised my score to 8 accordingly.\n\nThanks again!"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700650202733,
                "cdate": 1700650202733,
                "tmdate": 1700650202733,
                "mdate": 1700650202733,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]