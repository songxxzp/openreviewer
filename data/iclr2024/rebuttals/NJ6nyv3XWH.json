[
    {
        "title": "Leveraging Graph Neural Networks to Boost Fine-Grained Image Classification"
    },
    {
        "review": {
            "id": "ZQ5mIkP1tB",
            "forum": "NJ6nyv3XWH",
            "replyto": "NJ6nyv3XWH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4939/Reviewer_yF85"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4939/Reviewer_yF85"
            ],
            "content": {
                "summary": {
                    "value": "- This paper proposes a graph network based fine-grained visual categorization learning framework.\n\n- The key idea is, after feature extraction from a deep neural network, the graph network is embedded to refine the feature representation, so that the contextual information of the feature representation is enhanced.\n\n- Extensive experiments are conducted on some standard FGVC benchmarks and a variety of different backbones, which show the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper is well-written and easy-to-follow.\n\n- The proposed method is technically simple, straight-forward and effectiveness.\n\n- The proposed learning scheme is effective and leads to a performance gain on multiple settings and backbones."
                },
                "weaknesses": {
                    "value": "- The proposed learning scheme seems not to be devised for the task of FGVC.\n\nFrom the reviewer's view, adding a graph neural network to refine the image representation and acquire a stronger contextual representation is versatile to many high-level tasks, from image recognition on such as ImageNet, detection, segmentation, and etc.\nThus, it would be no surprise that such represention learning scheme, as a by-product, can improve the FGVC performance on top of some backbone networks.\n\n- The proposed learning scheme does not have much insight for discerning the fine-grained patterns, which is critical for FGVC.\nOverall, in the FGVC community, one of the most important things for FGVC is to differentiate the fine-grained patterns from the entire image. Indeed the graph representation can improve the contextual information, or the relation between feature embeddings. However, the designed framework does not delve into depth to mine the relation between fine-grained patterns. Instead, it is still implemented through an image-level for contextual enhancement. It is of a style that the reviewer does not appreciate for FGVC and its representing.\n\n- The technical novelties are somewhat incremental and marginal. Directly adding a graph neural network after a backbone is very ordinary and lacks insight.\n\n- The compared methods and related work discussison are both very insufficient. In this work, the comparison and references are mainly from typical visual backbones. In both introduction and related work, more recent FGVC works from the past five years, especially from top-tier conferences such as CVPR and top-tier journals such as PAMI and TIP should be included and discuss. Besides, more comparisons should be made.\n\n- The visualization is not good. The activation map is common for generic image recognition such as on ImageNet. Please follow the visualization in FGVC tasks, to activate the fine-grained patterns by such as GradCAM.\n\n[a] Destruction and construction learning for fine-grained image recognition. CVPR 2019.\n\n[b] Counterfactual attention learning for fine-grained visual categorization and re-identification. ICCV 2021.\n\n[c] Fine-grained object classification via self-supervised pose alignment. CVPR 2022.\n\n[d] Transfg: A transformer architecture for fine-grained recognition. AAAI 2022."
                },
                "questions": {
                    "value": "Q1: Is the proposed method really well devised for FGVC task? Or it is a more general representation learning scheme for high-level tasks, which benefit the contextual representation?\n\nQ2: The proposed framework does not leverage insights on representing fine-grained patterns for FGVC, which is of a style that the reviewer does not appreciate.\n\nQ3: The technical novelties are somewhat incremental and marginal. Directly adding a graph neural network after a backbone is very ordinary and lacks insight.\n\nQ4: More recent works on FGVC should be added for both related work discussion and experimental comparision.\n\nQ5: Please improve the visualization following the FGVC community's convention."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4939/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4939/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4939/Reviewer_yF85"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4939/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697627217623,
            "cdate": 1697627217623,
            "tmdate": 1699636480210,
            "mdate": 1699636480210,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TqDPjCfFGe",
                "forum": "NJ6nyv3XWH",
                "replyto": "ZQ5mIkP1tB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4939/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4939/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for providing thorough feedback. We provide our responses below.\n\nQ1:\nThe proposed method can be applied to general image classification, but its performance improvement is lower than for fine-grained image classification.\n\nQ2:\nThank you for your suggestion. We will consider it for future submissions.\n\nQ3:\nAcknowledged. We will improve in the next submission\n\nQ4:\nWe appreciate your suggestion. We will strive to do better in the next submissions.\n\nQ5:\nThank you for your suggestion. We will consider it for future submissions."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4939/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726884627,
                "cdate": 1700726884627,
                "tmdate": 1700726884627,
                "mdate": 1700726884627,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fsTRBgrRq8",
                "forum": "NJ6nyv3XWH",
                "replyto": "TqDPjCfFGe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4939/Reviewer_yF85"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4939/Reviewer_yF85"
                ],
                "content": {
                    "title": {
                        "value": "Response to Rebuttal from Reviewer#yF85"
                    },
                    "comment": {
                        "value": "Thanks for the rebuttal.\n\nHowever, the arguments, from the reviewer's perspective, are very weak, and the concerns remain.\n\nThus, the reviewer still recommend to clear reject this paper and keep the original score."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4939/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734377516,
                "cdate": 1700734377516,
                "tmdate": 1700734377516,
                "mdate": 1700734377516,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wdNlObkgbr",
            "forum": "NJ6nyv3XWH",
            "replyto": "NJ6nyv3XWH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4939/Reviewer_AgJz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4939/Reviewer_AgJz"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel neural architecture called GNN Post-Hoc (GPH) that uses Graph Neural Networks to improve neural networks for fine-grained image classification. The method consists of using GNNs to aggregate image features over a batch, to improve the features for fine-grained image classification. The authors test their method on 3 different datasets, CUB200-2011, NABirds, and Stanford Dogs, and show how their method improves the baselines\u2019 results."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposed method is simple and improves the performance of different vision backbones for the task of fine-grained image recognition, both for CNN-based and Attention-based backbones.\n- The authors conduct experiments on 3 different datasets and study the effect of using different GPH architecture with a fixed vision encoder, and how different vision encoders perform with a fixed GPH architecture.\n- The authors do ablations to understand the role of the image encoder, the batch configuration and the GNN aggregation method."
                },
                "weaknesses": {
                    "value": "1. The main weakness of the paper is that some of the results do not seem to back up the proposed method. First of all, the main idea of the paper is that by processing the feature vectors of all the images in a batch as a nodes in a graph, GPH can exploit the relationships between the images in the graph to improve the feature embeddings and make them more suitable for fine-grained recognition.\n\n    If that is the case, the proposed method should not be better with lower batch sizes. However, from Table 5 and Figure 3 it looks like models augmented with GPH perform very similarly with batch size of 1. How can a GPH augmented model perform similarly when the batch size is 1? Shouldn\u2019t it perform the same as the baseline, as it is not using any extra information? Additionally, if the best model is ConvNext + GPH, it would be better if the results in Tables 4 and 5 included ConvNext-GPH.\n\n    Another example, the results of DenseNet201-Attention seem really bad with batch size of 1. However, with batch size of 1, the result should be similar to the just DenseNet201, instead of much worse. Why is that the case?\n\n\n2. Another aspect that is not clear is the choice of using GNNs to aggregate information from different images in the graph. The method proposed in the paper uses a fully connected graph, so there isn\u2019t really a structure that the GNN can exploit. All the GNN is doing is aggregating features from a set of images without any relevant structural information, so why not use something like DeepSets [1] or attention?\n\n\n3. Related to the previous point, the authors have used attention instead of GPH as a baseline, but it seems to obtain much worse performance. Why is that the case? From Table 2 the GPH-GCN version seems better than Attention, but I\u2019m not sure that is correct. Considering that the graph is fully connected, GCN will amount to averaging the feature embeddings of all nodes at each layer, therefore it is not clear why that works better than attention. GCN should be equivalent to an attention that assigns $1/degree_i$ to the attention value for each node $i$.\n\n\n4.  The authors also comment on the batch design, i.e how to select which images go in the batch, since the output for one image depends on the other images in the batch. First of all, by using the \"sequential\" option (most images likely coming from the same class), the model is effectively looking at many images from the same class to make a prediction. However, in a real use case scenario, one does not know the class of the images to process, so therefore all validation scores should be reported using the \u201cshuffled\u201d option. It is not clear if the reported validation scores use the \u201csequential\u201d or \u201cshuffled\u201d option. Validation scores using the \u201csequential\u201d option should be invalid, since the method can exploit a pattern in the order of the data.\n\n    Secondly, I would expect the predictions made with \u201csequential\u201d to be much better than the shuffled option, since they are almost two different tasks. The former is easier, as it amounts to classifying the class shared by a group of images, while in the second one there is class variability, so it is harder. However, from Table 4 it looks like both are very similar. How were the values in that table obtained? Changing the method during training and evaluation? Or only during evaluation?\n\n\n5.  Figure 2 lacks details. It is not clear which samples from which dataset have been used, or which base model and GPH version are used to generate the image embeddings, as well as the projection method. \n\nFinally, the authors should cite [2], which is a related paper in which a GNN is used to predict a label for one image considering other images in the batch.\n\n[1] Zaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B., Salakhutdinov, R. R., & Smola, A. J. (2017). Deep sets. Advances in neural information processing systems, 30.\n\n[2] Garcia, V., & Bruna, J. (2018, January). Few-shot learning with graph neural networks. In 6th International Conference on Learning Representations, ICLR 2018."
                },
                "questions": {
                    "value": "Following my comments in the weaknesses section, my questions are:\n\n1) Why does the proposed GPH method perform similarly with batch size of 1, if there aren't other images in the batch to help generate better image features?\n2) What is the benefit of using a GNN over attention if the graphs are fully connected?\n3) Are the result scores reported using the \"sequential\" or \"shuffled\" batch order?\n4) What are the details used to generate Figure 2?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4939/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698662673955,
            "cdate": 1698662673955,
            "tmdate": 1699636480099,
            "mdate": 1699636480099,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5tbyeTEvh3",
                "forum": "NJ6nyv3XWH",
                "replyto": "wdNlObkgbr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4939/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4939/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your time in reviewing this paper. Please find below our responses.\n\nQ1:\nThe ability of the proposed GPH method to achieve high accuracy even with a batch size of 1 can be attributed to the fact that the model was trained using a batch size that fully utilizes the available RAM, as shown in Table 10. This training strategy ensures that the model is exposed to a sufficient number of training examples during the training process, allowing it to effectively learn the underlying relationships between images and extract meaningful features. When the trained model is evaluated with a batch size of 1, it benefits from the knowledge acquired during training with larger batch sizes.\n\nQ2:\nWe acknowledge the omission of a key advantage of using GNNs over attention mechanisms in our previous response. GNNs have the distinct advantage of preserving the original image features while attention mechanisms only focus on the features of other images in the batch. This preservation of original image features is crucial for fine-grained image classification tasks, as it allows the model to retain the essential details that distinguish between fine-grained categories.\n\nQ3:\nThe result scores reported using the \"sequential\" or \"shuffled\" image order in test dataset.\n\nQ4:\nWe employed t-SNE to visualize features for five classes after the GNN layer."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4939/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726794660,
                "cdate": 1700726794660,
                "tmdate": 1700726794660,
                "mdate": 1700726794660,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "olR1itxRtx",
            "forum": "NJ6nyv3XWH",
            "replyto": "NJ6nyv3XWH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4939/Reviewer_nzxZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4939/Reviewer_nzxZ"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces the Graph Neural Network (GNN) block into the deep networks (DNN) to improve the performance of fine-grained image classification. The DNN is to learn the feature embeddings for classification while the GNN is used to encode the relationship embedding among the input samples. The proposed method is evidenced by experiments on CUB-200-2011, Stanford Dogs, and NABirds datasets with both ConvNets and transformers."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The paper is well-written and easy to follow. \n\n+ The proposed method of GNN + DNN is simple and easy to understand. \n\n+ Extensive experiments on several datasets show the effectiveness of the proposed method. Moreover, comprehensive ablation studies of multiple GNN encoders, different batch size configurations, and aggregation functions further illustrate some interesting observations with the proposed method."
                },
                "weaknesses": {
                    "value": "However, there are still some concerns to be addressed:\n\n- The combination of GNN + DNN is quite straightforward and simple, thus, the novelty of this paper may be marginal. \n\n- The authors claim that the proposed method is able to learn contextual information and relationships that are essential for fine-grained categorization. However, looking through the manuscript, it seems that the discussion and evidence are missing.\n\n- The proposed method builds a fully connected graph, which will increase the complexity of the whole model as the training batch size increases. While the authors provide an ablation study of the test batch size, it will be more interesting to provide a detailed analysis of the training batch size, including the number of parameters, wall-clock training time, and performance.\n\n- In Table 2, the optimal model accuracy varies across datasets with different GNN encoders. Please clarify how to choose the encoder if using the proposed method.  \n\n- It is challenging to deploy and evaluate the trained model because its test performance is tied to both the test batch size and the data sampling method. Please clarify."
                },
                "questions": {
                    "value": "Please see the weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4939/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698753600590,
            "cdate": 1698753600590,
            "tmdate": 1699636480001,
            "mdate": 1699636480001,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "PxyuOKISfi",
            "forum": "NJ6nyv3XWH",
            "replyto": "NJ6nyv3XWH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4939/Reviewer_uMuy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4939/Reviewer_uMuy"
            ],
            "content": {
                "summary": {
                    "value": "To address issues in fine-grained image classification, where intra-class variability is high and inter-class distinctions may be subtle, the authors propose extending classical DNN-based image encoder architectures with a GNN, to refine features output by the DNN encoder. A complete graph is constructed using the feature vectors obtained from the DNN encoder, after which a number of GNN layers are applied. The resulting feature vectors are combined with the DNN feature vectors to yield the feature vectors used in classification. The authors show improved performance on a number of fine-grained classification tasks and for different GNN architectures."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The proposed method is simple and consistently outperforms baseline methods over multiple datasets.\n- The authors provide a relatively clear and practical explanation for their approach.\n- The authors provide ablations over different architectures for the GNN decoder."
                },
                "weaknesses": {
                    "value": "- My main concern is the lack of motivation for using a graph-based approach. The manuscript in its current version lacks any clear intuition for why a graph neural network on top of would improve performance in such a consistent way for specifically fine-grained image classification. \n- Second, the contribution this paper makes is fairly limited. The paper proposes a very practical addition to current DNN-based image classification methods, but essentially does not go beyond showing that with additional compute, fine-grained image classification improves. Especially given the intimate connection between message passing on graphs and \"conventional\" convolutions (convolutions are really a special case of message passing), I would very much appreciate a more thorough analysis as to why a graph-based approach is beneficial, i.e. what specific benefits this method has in this setting. Also a more thorough analysis on the types of features that are being learned would be useful; is it really the GPH that leads to a more correct clustering in fig 2? Would simply adding more layers of DNN not yield the same results? These questions have not been adequately addressed in my opinion."
                },
                "questions": {
                    "value": "- What is your motivation for using a graph-based approach for feature refinement? Please also include this motivation in your manuscript.\n- Notation is confusing in section 3.2, you seem to be using the same notation for the features output by the DNN and the GNN. As a result i'm not sure how to interpret eq 1,2.\n- How do you combine the GNN and DNN features? Is it different based on the GNN method you're using? I'm not sure how to interpret eq 2 in this regard. You only mention you \"combine\" the features, not how you go about this.\n- What is your interpretation of the results shown for different GNN architectures in 4.2.1. Is there a clear reason  you see for GraphTransformer outperforming conventional attention? Are these two approaches not identical in the case of fully connected graphs?\n\n---\n\nUpdate after rebuttal: the motivation the authors give for their method is in my opinion quite weak. Also considering the other reviewers concerns regarding novelty and limited motivation and evidence for some claims made in the paper i'm inclined to keep my recommendation; it seems the paper requires substantial modifcations and thus I deem it best to reject this paper for the current venue."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4939/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4939/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4939/Reviewer_uMuy"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4939/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698758504434,
            "cdate": 1698758504434,
            "tmdate": 1700924500126,
            "mdate": 1700924500126,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WO1UHfmcYJ",
                "forum": "NJ6nyv3XWH",
                "replyto": "PxyuOKISfi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4939/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4939/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for providing thorough feedback. We provide our responses below.\n\nQ1: We sincerely apologize for the omission of our motivation in the initial manuscript. Our motivation for employing a graph-based approach for feature refinement lies in the inherent challenges posed by fine-grained image classification. Conventional image classification methods, which process each image independently, often struggle to effectively capture the subtle and intricate details that distinguish between fine-grained categories. To overcome this limitation, we propose a graph-based approach that explicitly models the relationships between images within a batch, enabling the model to leverage the collective knowledge of the batch to refine feature representations and enhance classification accuracy.\n\nQ2: In Equation 2, the combine function represents the skip connection that merges the outputs of the deep neural network (DNN) and the graph neural network (GNN) in Figure 1.\n\nQ3: As Q2\n\nQ4:\n- The experimental results presented in Section 4.2.1 demonstrate that incorporating various graph neural network (GNN) architectures into the proposed framework effectively enhances the performance of the deep neural network (DNN) in fine-grained image classification tasks. \n- While the GraphTransformer architecture does not consistently outperform all other GNNs across various datasets, it achieves state-of-the-art performance on the Stanford Dogs dataset. This observation suggests that the GraphTransformer architecture is particularly well-suited for capturing the intricate relationships between images in the Stanford Dogs dataset, leading to superior classification accuracy.\n- The two approaches share similarities in their ability to focus on relevant features and suppress irrelevant ones"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4939/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726695039,
                "cdate": 1700726695039,
                "tmdate": 1700726695039,
                "mdate": 1700726695039,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]