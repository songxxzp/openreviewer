[
    {
        "title": "In defense of parameter sharing for model-compression"
    },
    {
        "review": {
            "id": "opB7kZnZPK",
            "forum": "ypAT2ixD4X",
            "replyto": "ypAT2ixD4X",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7052/Reviewer_sVzo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7052/Reviewer_sVzo"
            ],
            "content": {
                "summary": {
                    "value": "This paper looks into the problem of enhancing the quality of compression methods that use parameter sharing. It argues that existing parameter sharing methods, such as ROAST, can have instability issues during initialization and suggests a solution to reduce the instability called STABLE-RPS. The main idea of STABLE-RPS is to reshape the blocks into a single array, split it into several chunks, and apply a hashing function to determine which chunks to share parameters. The paper claims that this approach can improve the stability and pareto-continuity. Experiments on ResNet20/VGG on CIFAR10/100/TinyImagenet demonstrate that the proposed method outperforms several existing pruning methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "While parameter sharing has been heavily studied in the past, the paper made an interesting observation of the shortcomings of existing random parameter-sharing methods in terms of stability and Pareto-continuity. Furthermore, the paper proposes techniques to improve the stability and pareto-continuity of parameter sharing methods."
                },
                "weaknesses": {
                    "value": "1. The writing of the paper lacks clarity and needs some major improvements. For example, the problem statement is vague --- \u201cIs pruning the correct method to reduce model parameters?\u201d. However, the authors do not define what are the \u201ccorrect methods\u201d for model compression. Are methods that exhibit better stability and Pareto-continuity considered as correct? Are there other correctness conditions?  The paper also uses unconvincing examples to support its arguments. For instance, the authors try to explain pruning adversely affects model capacity, and use an example \"Consider pruning a n \u00d7 d embedding table. If we prune beyond d\u00d7, we start getting degenerate zero embeddings.\" But what does \"pruning beyond dx\" even mean, and why is not it obvious that pruning reduces model capacity? \n\n2. The paper is narrowly focused on comparing with pruning-based methods, which are only a subset of the model compression techniques. The paper does not justify why pruning methods are the most relevant baselines for the proposed method, which belongs to the parameter sharing paradigm. The paper also ignores other important compression methods, such as quantization, distillation, and low rank factorization.\n\n3. The evaluation setups are weak. Evaluation is done on tiny datasets such as CIFAR10/100 and Tiny-ImageNet and very old architectures such as ResNet-20 and VGG-11 (10-year old), which raise questions on how well the observation from this work can be generalized to larger datasets, such as ImageNet, and more recent architectures, such as vision transformers."
                },
                "questions": {
                    "value": "1. How does STABLE-RPS compare with neural architecture search based methods? Can RPS still outperform search methods in different compression settings?\n\n2. How does the proposed method affect latency? It seems that flattening and chunking blocks would disrupt the original matrix multiplication operations, which could slow down the model execution significantly. The paper should provide latency results for the proposed method and other methods.\n\n3. Why does Theorem 3.2 involve cache line fetches? Cache line fetches are related to execution speed, not memory efficiency. The paper should explain how cache line fetches affect the memory consumption of the proposed method."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7052/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698411376866,
            "cdate": 1698411376866,
            "tmdate": 1699636829655,
            "mdate": 1699636829655,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kuq0xzYpGv",
                "forum": "ypAT2ixD4X",
                "replyto": "opB7kZnZPK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7052/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7052/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your time and efforts to evaluate our paper. We incorporate your suggestions in the updated version and address other concerns below.\n\n**\"Is pruning the correct method to reduce model parameters?\" What does correct mean?**\n\nWe are raising the question that, given an architecture and a parameter memory budget whether \"pruning\" is the best method to reduce parameters in the model to achieve the memory budget. We have reworded the sentence to read, \"Is pruning the optimal method to reduce model parameters?\", where optimality is measured in terms of model memory and quality trade-off.\n\n\n**what does pruning beyond dx mean?**\n\nPruning beyond dx implies we are compressing more than a factor of $d\\times$ ( where 2x means reducing the model to half the parameters ).\n\n**why is not it obvious that pruning reduces model capacity**\n\nThe model capacity is reduced with any model compression technique. We argue that pruning affects the model capacity more adversely than other methods, such as RPS. The embedding table is a perfect example because compression with pruning leads to degenerate embeddings, which will not happen with RPS, even at very high compression. We have also added datasets in experiments on compressing embedding tables, corroborating this argument. \n\n**why pruning baselines, why not other baselines such as low-rank, distillation, quantization, etc?**\nIn this paper, we talk about model compression of a given architecture at the start of training. Pruning is the most advanced and well-studied baseline in this setting.\n\n1. Low-Rank: Low-rank methods for model compression are inferior to global-mask pruning. We have added a low-rank baseline to our new results on graphs to show this.\n\n2. Distillation: In this setting, distillation is not the correct baseline as it needs first to train a big model and then use it to train smaller models.\n\n3. Quantization: Quantization is popularly applied in model compression as either post-quantization or training-aware quantization - both of which are not compression-at-start techniques. To apply quantization at the start, we can train low-precision models, which can be used in conjunction with other techniques, such as pruning and RPS. Also, the compression range of quantization is very limited.\n\n**Diversity of Datasets and models for evaluation**\n1. We added more datasets from different domains : \nWe understand the concern from reviewers about restricted domains of evaluation. To provide more diverse evidence, we have added two more domains of \"Graphs with Attention-based Networks\"  and \"Deep learning Recommendation Models (DLRM) \" in Appendix H. The DLRM models (~540M parameters) and the associated dataset are relatively large (40M samples). The observations are consistent in that RPS outperforms other methods. In fact, pruning is especially bad in DLRM models due to the pruning of embedding tables. Pruning of embedding tables at high compression (beyond d\\times ) will give degenerate embeddings. \n\n\n2. Why not train vit on imagenet?\nCreating a single data point by training a vit model on imagenet needs 300 GPU hours if using A100 GPUs. We do not have access to the kind of computing it would need to create Pareto curves for this experiment in a reasonable time. \n\n\n**How does STABLE-RPS compare with neural architecture search-based methods? Can RPS still outperform search methods in different compression settings?**\n\nAt this stage, we cannot compare the two methods. NAS finds the best architecture given standard modules. STABLE-RPS and pruning methods find the reduced parameters for a given architecture. From works on pruning, it is clear that once a model is decided, it can be sparsified further and still keep matching accuracies in some cases. However, we still need to choose a good model to begin with, which is where NAS is useful. In the future, NAS can be combined with methods such as STABLE-RPS to obtain even better architectures. But, this direction is out of the scope of the current paper. \n\n\n**How does the proposed method affect latency? Why does Theorem 3.2 involve cache line fetches?**\n\n In theorem 3, we proved STABLE-RPS introduces minimal cache-inefficiency over ROAST mapping. We add latency numbers to the appendix section G. We measure the training time for different-sized matrices using different mappings in these experiments. We observe that STABLE-RPS mapping introduces a minimal increase in the training time of the model over ROAST."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7052/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700610565878,
                "cdate": 1700610565878,
                "tmdate": 1700610565878,
                "mdate": 1700610565878,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EtoyBFmOz8",
            "forum": "ypAT2ixD4X",
            "replyto": "ypAT2ixD4X",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7052/Reviewer_L2s3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7052/Reviewer_L2s3"
            ],
            "content": {
                "summary": {
                    "value": "The paper argues towards Randomized Parameter Sharing based models. The authors identified issues and provided solutions in the RPS technique ROAST, regarding stability (ROAST\u2019s sensitivity to initialization hyperparameters, leading to divergence) and Pareto-continuity (ROAST\u2019s inability to recover the accuracy of the original model at zero compression). The authors addressed this by proposing STABLE-RPS. The authors evaluated the method against many SOTA pruning methods and provided a theoretical grounding to their work."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Identification and Resolution of Stability, Pareto-Continuity Issues: The authors have identified key issues with existing techniques and proposed STABLE-RPS is an innovative method to address these issues \n\nRigorous Theoretical Foundation: The authors have also established a strong mathematical foundation to analyze the compression methods. This rigorous approach provides clear insights into how these methods affect vector inner products and under which conditions they perform optimally.\n\nQuality: The work done in the submission is of good quality with clear motivation and clarity. It could be a significant contribution if more empirical evidence is shown by authors."
                },
                "weaknesses": {
                    "value": "Limited Experimental Validation: The paper could benefit more extensive experimental validation to complement the theoretical analysis. The authors only provided experimental evidence on small datasets. Given, that authors claim RPS is the way to go forward it would be good if they can follow up with more experiments\n\nLack of discussion around additional computation overhead in proposed STABLE-RPS method compared to other methods like ROAST. Some datapoints on what is the end to end training speedups that one can expect with this method will also help."
                },
                "questions": {
                    "value": "Diversity of datasets: Can more experiments be done with different types of datasets and model architectures evaluated to assess the proposed method\u2019s performance? \n\nSome discussion around what are there any specific scenarios where the proposed method might not perform well?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7052/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7052/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7052/Reviewer_L2s3"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7052/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698760632730,
            "cdate": 1698760632730,
            "tmdate": 1699636829469,
            "mdate": 1699636829469,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GVTPzw3tmO",
                "forum": "ypAT2ixD4X",
                "replyto": "EtoyBFmOz8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7052/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7052/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for identifying the thoroughness of theory and experiments and the value and soundness of the proposal. Your comment on the work's overall quality, motivation, and clarity is encouraging. \n\nWe have incorporated many of your suggestions, as we describe below:\n\u00a0\n1. **Additional Experimental validation**\n\n     1. We have added more datasets. : We understand the concern from reviewers about restricted domains of evaluation. To provide more diverse evidence, we have added two more domains of \"Graphs with Attention-based Networks\" and \"Deep learning Recommendation Models (DLRM) \" in Appendix H. Specifically, DLRM experiments are relatively large, with the dataset having 40M samples and the model having 540M parameters. The observations are consistent in that RPS outperforms other methods. \n     2. Why not train larger models/datasets such as vit on imagenet? Another reviewer suggested training vit on imagenet. Creating a single data point by training a vit model on imagenet needs 300 GPU hours if using A100 GPUs. We do not have access to the kind of computing it would need to create Pareto curves for this experiment in a reasonable time. \n\n2. **Computation overhead in the proposed STABLE-RPS method compared to other methods like ROAST**\n\nIn theorem 3, we proved STABLE-RPS introduces minimal cache-inefficiency over ROAST mapping. We add latency numbers to the appendix section G.  We measure the training time for different-sized linear layers using different mappings in these experiments. We observe that STABLE-RPS mapping introduces a minimal increase in the training time of the model over ROAST.\n\n3. **Some discussion around what are there any specific scenarios where the proposed method might not perform well?**\n\nThe effectiveness of RPS standalone will depend on how much redundancy exists inside the model given a specific task. For some models, it might be very effective (for example, see the newly added embedding table compression in DLRMs example). In contrast, for some tasks, it will show degradation with compression like the vision tasks we show.\nCompared to methods such as pruning, as pointed out by theory and empirical validation, RPS is superior concerning the memory-accuracy tradeoff."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7052/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700606290393,
                "cdate": 1700606290393,
                "tmdate": 1700608214139,
                "mdate": 1700608214139,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "U3GThaugz7",
            "forum": "ypAT2ixD4X",
            "replyto": "ypAT2ixD4X",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7052/Reviewer_Fae8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7052/Reviewer_Fae8"
            ],
            "content": {
                "summary": {
                    "value": "The authors provide theoretical analysis, algorithmic refinement, and empirical experimentation of randomized parameter sharing (RPS) for model compression, in contrast with the prominent paradigm of parameter pruning for model compression. Guided by their theoretical analysis, the authors\u2019 refined algorithm STABLE-RPS resolves some prior issues with existing RPS methodologies\u2014convergence stability and ability to recover full accuracy at no compression. Further, their large scale empirical analysis reveals that STABLE-RPS can outperform nearly all pruning methods in terms of accuracy-compression tradeoff except for a leading method, lottery ticket rewinding, where STABLE-RPS only outperforms at high compression rates. Research on RPS is motivated by the objective of exploring and improving alternative paradigms for model compression that may be able to deliver better accuracy-compression tradeoff."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "\u2022 Thorough and useful theoretical analysis of existing RPS methods which ultimately inform design of improved RPS alrogithm\n\n\u2022 Design and implementation of STABLE-RPS algorithm which resolves two significant issues with existing RPS techniques\u2014convergence stability and Pareto-continuity\n\n\u2022 Extensive empirical analysis of STABLE-RPS on three datasets and two architectures compared to seven existing pruning or model compression strategies which demonstrate that STABLE-RPS can outperform nearly all pruning methods in terms of accuracy-compression tradeoff"
                },
                "weaknesses": {
                    "value": "\u2022 STABLE-RPS cannot outperform a leading pruning method, lottery ticket rewinding, in low to medium compression regime"
                },
                "questions": {
                    "value": "1. Is STABLE-RPS compatible with parameter quantization (to achieve additional compression)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7052/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7052/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7052/Reviewer_Fae8"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7052/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698802749894,
            "cdate": 1698802749894,
            "tmdate": 1699636829328,
            "mdate": 1699636829328,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mnk7xjCYKq",
                "forum": "ypAT2ixD4X",
                "replyto": "U3GThaugz7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7052/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7052/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for identifying the thoroughness in theory and experiments, and the value and soundness of the proposal.  We would like to let you know that we have added more diverse datasets, latency experiments and validation of theorems to further improve our paper. (details of which are in official comment \"Summary of Rebuttal\")\n\nWe thank you for your encouraging support of the paper."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7052/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700605367739,
                "cdate": 1700605367739,
                "tmdate": 1700605367739,
                "mdate": 1700605367739,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0skhDCmW3S",
                "forum": "ypAT2ixD4X",
                "replyto": "U3GThaugz7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7052/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7052/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Is STABLE-RPS compatible with parameter quantization (to achieve additional compression)?"
                    },
                    "comment": {
                        "value": "Yes Quantization can be used in conjunction with other methods including RPS and Pruning to obtain further compression."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7052/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634318616,
                "cdate": 1700634318616,
                "tmdate": 1700634318616,
                "mdate": 1700634318616,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7t4MXGoVyu",
            "forum": "ypAT2ixD4X",
            "replyto": "ypAT2ixD4X",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7052/Reviewer_YEB1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7052/Reviewer_YEB1"
            ],
            "content": {
                "summary": {
                    "value": "Stable-RPS is an extension of ROAST, which is a method for sharing parameters using hashing. Parameters are mapped from a shared parameter array into each layer, along with a sign and scaling factor. The paper addresses the stability of ROAST and ensuring that the original performance of the model is maintained at 1x compression (Pareto-continuity). Modifications:\n\n1. A gradient scaling function depending on the scaling factors applied when ROAST maps parameters to layers from the global store\n2. A better hash function that ensures Pareto-continuity\n\nThe authors then demonstrate that this method of parameter sharing is competitive with contemporary pruning methods.\nThe authors also include various theoretical results to explain this improved performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper explores missing potential for random parameter sharing in deep neural networks. The early results, such as [Chen et al 2015][hashing], demonstrated competitive performance at the time but the method has received less attention since. It would be valuable for the field to have a comprehensive exploration of the potential of this direction of research.\n\nThe improvements to ROAST presented are well motivated and address clear shortcomings of an existing method. They are a good contribution to the field.\n\nThe theory investigating random parameter sharing presented in 5 theorems is a useful insight into how random parameter sharing works and will be useful to future work in this area. It seems likely that future research may focus on other alternative hashing methods and will benefit by performing similar analysis.\n\n[hashing]: https://arxiv.org/abs/1504.04788"
                },
                "weaknesses": {
                    "value": "Figure 1 fails to explain how ROAST works or how STABLE-RPS relates to ROAST. I don't know what a ROAST array is or what a ROAST++ array is. I don't understand why the resulting Linear array has exactly the same elements (same colors) in both cases.\n\nEquations for ROBE-Z and ROAST are insufficient. I have no way of replicating either method from these descriptions. It looks like integer and modulo division are being used but outside of pseudocode these should be defined with explanation of what they're doing in the equation.\n\nThere are 5 theorems stating results, but these results are not checked by experiment. Experimental verification would free the reader from checking the derivation or trusting that it is correct.\n\nParameter counts for the networks in Figure 3 would be useful, to understand how many parameters the network has at different compression levels."
                },
                "questions": {
                    "value": "Why is there no discussion of the main limitation of random parameter sharing methods versus pruning methods: that they do not reduce the numpy of floating point operations required, while pruning methods do? If pruning methods and RPS perform similarly up to compression factors of 100x then pruning has a significant edge in saving FLOPs. When would RPS really be competitive?\n\nAre there any experimental results on networks other than ResNet-20 or VGG-11? For the same parameter budget there are now many network architectures that perform much better. In other words, where would MobileNet or EfficientNet be placed on Figure 3? Unfortunately, as the compression ratio increases the network quickly enters regions where the accuracy is not worthwhile.\n\nExperimental results are demonstrated on CIFAR-10, CIFAR-100 and Tiny-ImageNet. Would it be possible to explore this method on a contemporary large scale model? What would a scaling law for random parameter sharing look like?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7052/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7052/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7052/Reviewer_YEB1"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7052/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698802972858,
            "cdate": 1698802972858,
            "tmdate": 1699636829175,
            "mdate": 1699636829175,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ow0tDDtpil",
                "forum": "ypAT2ixD4X",
                "replyto": "7t4MXGoVyu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7052/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7052/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for identifying the thoroughness in theory and experiments, and the value and soundness of the proposal. We have incorporated your suggestions into the updated version of the paper.\n\n**Experimental verification for theorems**\n\nThanks for the great suggestion. We have added empirical verification for theorems in the appendix F. As you can see, the theoretical predictions and empirical computations match in all presented cases.\n\n**Add parameters to the figures** \n\nWe have added parameter counts to both existing and newly added figures.\n\n**Would it be possible to explore this method on a contemporary large-scale model?**\n\n1. We added more datasets from diverse domains: We understand the concern from reviewers about restricted domains of evaluation. To provide more diverse evidence, we have added two more domains of \"Graphs with Attention-based Networks\"  and \"Deep learning Recommendation Models (DLRM) \" in Appendix \"MORE DOMAINS.\"  The DLRM models (~540M parameters) and the associated dataset are relatively large (40M samples). The observations are consistent in that RPS outperforms other methods. In fact, pruning is especially bad in DLRM models due to the pruning of embedding tables which is expected since pruning of embedding tables at high compression (beyond d\\times ) will give degenerate embeddings. \n\n2. Why not train larger models/datasets such as vit on imagenet?\nAnother reviewer suggested training vit on imagenet. Creating a single data point by training a vit model on imagenet needs 300 GPU hours if using A100 GPUs. We do not have access to the kind of computing it would need to create Pareto curves for this experiment in a reasonable time. \n\n **Limitation of random parameter sharing methods versus pruning methods, When would RPS really be competitive?**\n\nWe mention the current limitations of RPS in the section \"Discussions and conclusion,\"  Reducing the number of flops in RPS is currently an open problem. We will also mention it in the introduction to avoid any confusion.\n\nThere are many situations in which RPS can be potentially beneficial right away.\n\n1. Embedding heavy models such as DLRMs. We added new results comparing pruning and RPS, and pruning is terrible when compressing embedding tables. As embedding tables do not have any flops, RPS is equally efficient in computation.\n\n2. MoE models. A mixture of expert models is generally memory-heavy, and their computation is sparsified by design. \n\n3. Other scenarios exist where higher compression can help us eliminate model parallel execution, which can have latency advantages. \n\nHaving said that, this paper focuses on memory-accuracy tradeoff compression techniques. By establishing this, we want to argue for further investigation of RPS to solve existing open problems.\n\n**For the same parameter budget there are now many network architectures that perform much better. In other words, where would MobileNet or EfficientNet be placed on Figure 3?**\n\nGiven a memory budget, finding the best architecture is a challenging problem and comes under NAS. At this stage, we cannot compare RPS on one architecture with other architectures (alternatively, we cannot compare the RPS and NAS) . NAS finds the best architecture given standard modules. STABLE-RPS and pruning methods find the reduced parameters for a given architecture. From works on pruning, it is clear that once a model is decided, it can be sparsified further and still keep matching accuracies in some cases. However, we still need to choose a good model to begin with, which is where NAS is useful. In the future, NAS can be combined with methods such as STABLE-RPS to obtain even better architectures. But, this direction is out of the scope of the current paper. \n\n\n**Unfortunately, as the compression ratio increases, the network quickly enters regions where the accuracy is not worthwhile.**\n\n In practical scenarios, what tradeoff benefits a particular use case depends on the existing resource constraints and quality requirements. Thus, it is impossible to comment on whether RPS will solve the issue to satisfaction. But, in this paper, we want to rigorously compare RPS and pruning paradigms to see which method is better, given the resource constraints, and we find that RPS has better expressivity than pruned models at given memory budget.\n\n**Writing Issues**\n1. We have rewritten the background on parameter sharing to improve the readability of the section with a focus on the ROAST method. In the appendix, we can add details on other methods, such as ROBE and Hashednet, if needed.\n\n2. The colors used in the full model denote the different ROAST chunks in the model, and the color in the linear array shows one instance of the mapping -- mapping of one chunk of ROAST or one fold of STABLE-RPS into the array showing which parameter goes where."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7052/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700609944662,
                "cdate": 1700609944662,
                "tmdate": 1700623847657,
                "mdate": 1700623847657,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]