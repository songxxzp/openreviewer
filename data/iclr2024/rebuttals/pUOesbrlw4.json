[
    {
        "title": "Deep Unlearning: Fast and Efficient Training-free Approach to Controlled Forgetting"
    },
    {
        "review": {
            "id": "h1Q4m6IAJK",
            "forum": "pUOesbrlw4",
            "replyto": "pUOesbrlw4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8026/Reviewer_8Shn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8026/Reviewer_8Shn"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the question of whether we can unlearn one or more classes from a well-trained model if only a few samples are accessible from the training data of a large dataset. To address this question, the authors propose a novel singular value decomposition-based class unlearning method. This work first estimates the retain space and forget space of the model layer by layer based on the singular value decomposition technique, and then removes the shared information between spaces from the forget space to isolate the class-discriminatory feature space for unlearning, and finally, projects the model weights in the orthogonal direction of the class-discriminatory space to obtain the unlearned model. The authors demonstrate the effectiveness of the method on CIFAR-10, CIFAR-100, and ImageNet datasets as well as VGG11, ResNet18, and ViT models. They also display the applicability of their method in two practical scenarios of multi-class unlearning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "**Originality:** As far as I know, this paper is the first to propose the method of decomposing feature space through SVD to solve class unlearning, so this work is novel.\n\n**Quality:** The approach seems reasonable. By decomposing the feature space of each layer of the well-trained model, the features of the unlearn class are eliminated while reducing the impact on the features of the retained classes. The experimental evaluations in the paper also provide evidence to support the claims made in the paper. In particular, there are verifications on the large datasets ImageNet and ViT. However, the method also has some unclear aspects that I mentioned below.\n\n**Clarity:** The paper is generally well-written. However, there is room for improvement as I mention below."
                },
                "weaknesses": {
                    "value": "**Methodology:**\n1. The authors only describe the representations collection of linear and convolutional layers, but how to deal with the transformer layer, BatchNorm, etc.?\n2. The authors perform SVD operation based on the representations matrix of $X_r$ and $X_f$. Do the sample size and sampling strategy of $X_r$ and $X_f$ affect the results of the algorithm?\n3. Why did the authors consider designing importance-base space scaling? Won't this cause the deformation of the forget space and retain space?\n4. The choice of best $\\alpha$ in Eq.(1) seems to be a trick as different datasets give different $\\alpha$ sets. Is there a more reasonable way to choose, like learnable $\\alpha$?\n\n\n**Experiments:**\n1. The authors mention that the experimental results come from 10 different target unlearning classes, and CIFAR-100 is evaluated for every 10th. Does it mean that the [10, 20, \u2026, 100]-th class was used as an unlearning class to conduct the experiment?\n2. Does the \"Original\" represent the well-train model? Are the results reported on all retain and forget classes?\n3. How is the classification head designed? For example, for CIFAR-100, the classification head of the well-train model outputs a 100-dimensional vector. What about the unlearning model?\n4. In Figure 4, the proposed method does not have an advantage in efficiency compared to the Tarun et al. (2023)? Could the authors provide further analysis?\n5. While I think the multi-class forgetting experiment is interesting, what the authors provide is not sufficient. First, simply conducting experiments on CIFAR10 is not convincing. Second, the effects of baselines were not compared. Third, the result analysis is insufficient. For example, why is the original model of resnet18 better in Figure 5a, but not as good as the unlearning model in Figure 5b?"
                },
                "questions": {
                    "value": "1. See weakness for details on methodological and experimental issues.\n2. The authors only mention three related works of literature on class unlearning. I am wondering about the importance and practicality of this problem.\n3. This paper only evaluates the effectiveness of the method based on the classification accuracy of forgetting and retaining classes. Have the authors considered verification metrics with more theoretical guarantees? If the forgetting class is known, the output probability of the forgetting class can be forced to 0 without changing the model parameters to ensure the effect of forgetting and retaining classes, but this approach is obviously contrary to the motivation of unlearning."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8026/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8026/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8026/Reviewer_8Shn"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8026/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698469150663,
            "cdate": 1698469150663,
            "tmdate": 1699636990317,
            "mdate": 1699636990317,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KoYHdbTw4i",
                "forum": "pUOesbrlw4",
                "replyto": "h1Q4m6IAJK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8026/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8026/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 8Shn"
                    },
                    "comment": {
                        "value": "**Response to Weakness:**\n- **Methodology**\n    1. We have added a line in section 4.3 under \u201cDiscussion\u201d to clarify this.  We apply our algorithm to all the linear layers in the transformer architecture. For an attention block, the linear layer weights updated are the Key Weights, Query Weights, Value Weights, and Output weights. We do not perform any unlearning of the normalization layers as the fraction of total parameters for these layers is insignificant and empirically is not seen to affect the performance of our algorithm. \n    2. Samples from each class are necessary to ensure a good estimation of retain space and forget space. We chose $X\\_r$ and $X\\_f$ to be around 1000 to ensure we have at least 1 sample from each class (ImageNet). Empirically we observed that having 1000 samples was enough and we did not increase it to ensure the compute cost of Representation collection and SVD over $X\\_r$/$X\\_f$ is kept low. \n    3. In our algorithm  (line 14, Algorithm 1) we find $P\\_{dis}$, which is the class discriminatory activation space. We want to find a class discriminatory space where forget samples have the \u2018highest\u2019 activations whereas retain samples have the 'lowest'. By removing this space (from the entire space) we perform our unlearning. To assign this connotation of ''highest/lowest' activation with the original $P\\_f$/$P\\_r$ space we need to assign importance - thus scaling was used.  We agree this causes deformation of the space - which is a design choice.\n    4. Our algorithm is gradient free algorithm and obtains the parameter update in a one-shot fashion. As we did not use gradient information it was not straightforward to come up with a learning scheme for $\\alpha$. However, in the future training algorithms can be designed that could learn $\\alpha$ to be used for unlearning if needed. \n- **Experiments**\n    1. For CIFAR100 we use [1, 11,21, \u2026 91 ]. We update the paper stating  \u201cCIFAR10 dataset is accessed for unlearning on each class and CIFAR100 is evaluated for every 10th starting from the first class.\u201d\n    2. Yes, original refers to a well trained model which is trained on $D\\_{train}$. The accuracy numbers in the previous version were the overall accuracy of the network on the samples of retain and forget class. We have updated these numbers to report retain class accuracy and forget class accuracy separately. \n    3. To evaluate the unlearnt model we assume that the unlearning algorithm gives us the unlearnt model without changing the classifier output neurons similar to the baseline SoTA methods Tarun et al and Kurmanji et al. \n    4. We thank the reviewer for pointing this out. We realized that the runtimes depend on the implementation of the algorithm and the background hardware it is running on. We analytically find the compute costs of different algorithms and plot them in Figure 5 and show the details in Appendix A.7. Our algorithm has at least 6.5x lower compute than other methods. \n    5. We present the results for superclass unlearning on CIFAR100 in Figure 6 and have moved the results of CIFAR10 to Appendix A.8. Further, we have added a comparison with baselines in Table 6 in Appendix A8. We see our model performs better on MIA scores and retain accuracy over SoTA Tarun et al. NegGrad+ is seen to outperform our method on ResNet18. We saw Kurmanji et al to have extremely low retain accuracies and hence have not reported them in the table. There was a mistake in the plot of Figure 5b in the original submission (10b in updated). We did not plot the original model\u2019s forget accuracy in the figure. \n\n**Response to Questions:**\n\n2. Class unlearning is important where entire user data belongs to a single class. Consider a scenario where we train a classification model for face recognition. In this case, an entire class belongs to a single person and we would be required to remove this class when on request. Further, in the cases where a collaborative multi-task model is shared by a few companies and one of the companies decides to withdraw its data the one-shot multi-class removal algorithm becomes relevant. \n3. We have added membership inference attack evaluations in our experiments. The results show our method performs well across different datasets and architectures obtaining an average improvement of 7.8% in MIA accuracy over other baselines. \nOur MIA attack has access to the true confidence scores of the model on the correct class. Hence an algorithm that post hoc manipulates the output would fail to obtain good MIA scores.\n\n**Note:** We have added a version of the paper with highlighted changes to supplementary material."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8026/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700384947356,
                "cdate": 1700384947356,
                "tmdate": 1700384947356,
                "mdate": 1700384947356,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6L9Tf77c6w",
                "forum": "pUOesbrlw4",
                "replyto": "h1Q4m6IAJK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8026/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8026/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer 8Shn, \n\nWe appreciate your valuable comments on our work. We have updated the paper as per your suggestion addressing the weaknesses and questions. We look forward to any additional feedback you have for improving the quality of our work further. \n\nThanks\nAuthors"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8026/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700612598838,
                "cdate": 1700612598838,
                "tmdate": 1700612880590,
                "mdate": 1700612880590,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SVX4WKs345",
            "forum": "pUOesbrlw4",
            "replyto": "pUOesbrlw4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8026/Reviewer_FQb4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8026/Reviewer_FQb4"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel method for class unlearning inspired by work in continual learning that uses Singular Value Decomposition as a means for separating \u2018spaces\u2019 containing knowledge of different tasks. In this work, aninstantiation of SVD is used to separate out the \u2018forget space\u2019 (the \u2018space\u2019 in which the activations of samples belonging to the forget set lie) from the \u2018retain space\u2019 (analogously, for retain samples). These spaces are obtained by running SVD on activations (retain or forget) from all layers of the network. The authors show that a small subset of the retain and forget sets suffices for obtaining these activations. Once the forget space is identified, the model weights are updated by a projection to a space that removes forget set information.\nThe authors also use a baseline that they refer to as Stable Ascent (which was proposed in recent work). Interestingly, they show that both this baseline and their proposed method surpass the previous state-of-the-art in the context of class unlearning on some benchmarks, with their proposed method making further progress over Stable Ascent. They investigate empirically different scenarios (different datasets and architectures, removal of one or more classes either in one-go or sequentially) and report both quantitative results (accuracies) and qualitative ones (gradcam heatmaps to visualize feature saliency)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper studies the important problem of unlearning that is attracting increasing attention recently\n- The proposed method is well-motivated and an interesting idea\n- For the most part, the paper is well written (see below for some exceptions)\n- Indeed the Transformer results are the first to my knowledge application of unlearning methods in larger-scale models that are closer to the state-of-the-art, which is really interesting.\n- interesting qualitative analysis of saliency."
                },
                "weaknesses": {
                    "value": "- Motivation: the way that the problem of unlearning is motivated in this paper (data deletion; user privacy) seems at odds with the problem of unlearning classes (as it would correspond to unlearning individual data points that don\u2019t necessarily belong to the same \u2018class\u2019, depending on the definition of class). What are application scenarios for class unlearning? While the authors have motivated the problem of unlearning well, motivation of missing for class unlearning in particular.\n- the authors claim that Stable Ascent is one of the contributions of the paper but this baseline has already been proposed in previous work that the authors did not cite ([A] \u2013 see References below, where it is referred to as NegGrad+ and the authors of that work also find that it is a strong baseline, surpassing previous SOTA in several scenarios)\n- recent unlearning methods are missing from the Related Work section, e.g. [A, B, C, D] (see References below), and it would also significantly strengthen the paper to empirically compare against them too.  \n- ablations are missing. For example, how large is the contribution of the proposed scaling? It would be good to investigate a version of the proposed method without this. Further, how important is it to use activations from all layers for SVD versus just the top layer(s)?\n- also, it would be good to motivate the scaling a bit more. Is such a scaling used in the continual learning literature / related methods? If not, what is different about this application that necessitates it?\n- the evaluation is lacking. While several evaluation metrics are used for unlearning, the authors rely primarily on accuracy metrics. A particularly important class of evaluation techniques that is missing is membership inference attacks (see e.g. the papers by Golatkar et al, which are cited in this work, and also see [A] from the references below)\n- In fact, the evaluation metrics seem to be at odds with the goal of class unlearning that the authors state in the Introduction, namely that \u201cthe unlearning algorithm should produce parameters that are equivalent to those of a model trained without the target class\u201d. Despite this definition, the authors don\u2019t look at proximity in weight space or related metrics and instead rely primarily on accuracy.\n- In section 3, the description of the problem of class unlearning isn\u2019t precise enough. It\u2019s defined as producing a set of unlearned parameters such that two conditions (test retain examples are correctly classified and test forget examples incorrectly classified) are satisfied for \u2018many samples\u2019. But how many samples? Usually, the accuracy on each of those two sets is desired to be just as high/low as it would be for retrain-from-scratch. Is there a reason that this is not the definition used? Also, how come this definition refers only to test examples? Usually it is also desired to have similar conditions hold for the training set (retain and forget partitions).\n- clarity: the algorithm is presented in terms of linear and convolutional layers. But in their experiments, the authors also use Transformers. It\u2019s not directly obvious how the proposed method is used for attention layers.\n- clarity: \u201cfor both linear and convolutional layers, where l is a layer and i is retain sample \u201d \u2013 i was not mentioned in that context. \n\nMinor issues and typos\n==================\n- \u2018produce parameters that are equivalent to those of a model trained without the forget set\u2019 \u2013 not clear what the word \u2018equivalent\u2019 means here.\n- \u2018Generalization on retain samples\u2019 \u2013 this is a little confusing as the retain set is part of the training set, so generalization isn\u2019t an appropriate term (as it refers to held-out samples). \u2018accuracy\u2019 or \u2018performance\u2019 are more appropriate.\n- \u2018we asks the question\u2019 \u2192 \u2018we ask the question\u2019\n- \u2018this section focus\u2019 \u2192\u2018this section focuses\u2019\n- \u2018sorted in by the amount \u2192\u2018sorted by the amount\u2019 \n- \u2018the samples form class to be retained\u2019 \u2192\u2018the samples from the class\u2019\n- In several places, some articles are missing, and there are more typos than these mentioned here. \nPlease proofread the paper and check for grammatical errors.\n\nReferences\n==========\n- [A] Towards Unbounded Machine Unlearning. Kurmanji et al. NeurIPS 2023.\n- [B] Towards Adversarial Evaluations for Inexact Machine Unlearning. Goel et al. 2023. \n- [C] Prompt Certified Machine Unlearning with Randomized Gradient Smoothing and Quantization. Zhang et al. NeurIPS 2022.\n- [D] Unrolling SGD: Understanding Factors Influencing Machine Unlearning. Thudi et al. 2022"
                },
                "questions": {
                    "value": "- What is it about this method that makes it specific to class unlearning? Could one use this method to unlearn a random subset of the training dataset? If not then why not?\n- in figure 4, I was surprised that the method of Tarun et al. is more efficient than the proposed method, since Tarun et al uses SGD (in several phases too) while the proposed method only requires forward passes of a few samples (and no backward passes). Why is that?\n- in figure 5b, why are the confidence intervals so large? Can we draw any meaningful conclusions from this figure?\n- why is the reference point / oracle of Retraining not included in all tables? Is this because it is too computationally expensive to compute this for some models / datasets? Without that reference point, it is not possible to know what the target accuracy/error is for the forget set (because the goal is usually defined as matching / being as close as possible to the accuracy/error of Retrain on the forget set). Could you explain how the results are interpreted given the absence of that reference point?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8026/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698575214407,
            "cdate": 1698575214407,
            "tmdate": 1699636990196,
            "mdate": 1699636990196,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "U0i2wBawLw",
                "forum": "pUOesbrlw4",
                "replyto": "SVX4WKs345",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8026/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8026/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer FQb4"
                    },
                    "comment": {
                        "value": "**Response to Weakness:**\n1. **Motivation for Class Unlearning -** Class unlearning is important where entire user data belongs to a single class. Consider a scenario where we train a classification model for face recognition[1]. In this case, an entire class belongs to a single person and we would be required to remove this class on request. Further, in the cases where a collaborative multi-task model is shared by a few companies where a task is classification over a group of classes and one of the companies decides to withdraw its data, the multi-class removal algorithm becomes relevant. \n2. **NegGrad+ baseline-** We thank the reviewer for pointing this out. We have added the citation and updated the contributions. Further, we have renamed Naive Ascent as NegGrad and Stable Ascent as NegGrad+ to be consistent with the literature. \n3. **Comparisons-**  We add experiments on [kurmanji et al] (or reference [A] in the Reviewers list) as it is a more recent method in the list of suggestions. We have updated the \u201cComparisons\u201d part in Section 5 mentioning this as a baseline and added the results in Table1 and Table2. We see our method has 13.6% better forget accuracy and 45.7% better MIA over [A] on average across all the datasets and networks in Table 1 and Table 2.\n4. **ablations are missing-** \n    -  **Proposed scaling:** Without the proposed scaling the $\\Lambda\\_r$ and $\\Lambda\\_f$ in line 12 of Algorithm 1 become Identity. Now as $U$ matrix obtained through SVD is an orthonormal matrix, $P\\_r$ and $P\\_f$ in line 13 of Algorithm 1 become Identity when $\\Lambda\\_r$ and $\\Lambda\\_f$ are Identity. When $P\\_r$ and $P\\_f$ is Identity $P\\_{dis} = P_f(I-P\\_r)$ computed in line 15 becomes a zero matrix. Projection of weights on $I-P\\_{dis}$ in line 15 would hence cause no change in the weight as $I-P\\_{dis}$ is Identity essentially leading to no unlearning. This necessitates the scaling in line 12 of the algorithm. We have clarified this under the \u201cImportance-base Space Scaling\u201d in Section 4.2.\n    -  **Impact of layer:** We have added a section on \u201cLayer-wise analysis\u201d where our results show that later layers play a more significant role in unlearning. Further, in Figure 8 of Appendix A.6 we see that removing our algorithm from the first 4 layers has a very low impact on forget accuracy.\n5. **Scaling Motivation-** In continual learning, scaling has been used to modulate the gradient descent steps during training so that old tasks are not forgotten and new tasks are learned with high accuracy.  In this scenario, we apply one-shot scaling of the weights on a pre-trained model. Effectively, we apply scaling to restrict the layer-wise activation of the input samples (during inference) to remove activations belonging to class discriminatory space $P\\_{dis}$. Further, we have added a few lines to explain the need for importance scaling under \u201cImportance-base Space Scaling\u201d in Section 4.2.\n6. **MIA attacks-** As per reviewer\u2019s suggestion, we have added membership inference attack evaluations in our experiments. The results show our method performs well across different datasets and architectures obtaining an average improvement of 7.8% in MIA accuracy over other baselines.\n7. **Class Unlearning goal in Introduction-** We meant \u201cThe unlearning algorithm should produce model (parameters) that are functionally equivalent to those of a model trained without the target class.\u201d by the statement and have updated this in the paper. We show such equivalence by our experiment using ACC_r, ACC_f, MIA, confusion matrix, and gradient based saliency maps.\n8. **Problem description in Section 3-**  Thanks for this suggestion. We have modified the writing to address reviewers concern. \nWe clarify the definition in the following places in the updated paper.\nClass Unlearning  ( Section 1 Introduction) \u201cFor a class unlearning setup, the primary goal of the unlearning algorithm is to eliminate information associated with a target class from a pretrained model. This target class is referred to as the forget class, while the other classes are called the retain classes.\u201d Additionally, we have formally presented \u201cclass unlearning\u201d in Section 3: \u201cThe parameters $\\theta^*\\_f$ must be functionally indistinguishable from a network with parameters $\\theta^*$, which is retrained from scratch on the samples of $\\mathcal{D}\\_{train\\_r}$ in the output space. In other words, these parameters must satisfy $f(x\\_i, \\theta^*) \\simeq f(x\\_i, \\theta^*\\_f)$ for $(x\\_i, y\\_i) \\in D\\_{test}$ or $D\\_{train}$.\u201d\n\n**Note:** We have added a version of the paper with highlighted changes to supplementary material. \n\nReference:\n[1] Y. Sun, X. Wang, and X. Tang, \"Deep learning face representation from predicting 10,000 classes.\" pp. 1891-1898."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8026/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700387512768,
                "cdate": 1700387512768,
                "tmdate": 1700387512768,
                "mdate": 1700387512768,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pBox5XK9ja",
                "forum": "pUOesbrlw4",
                "replyto": "SVX4WKs345",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8026/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8026/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer FQb4"
                    },
                    "comment": {
                        "value": "**Response to Weakness(Continued):**\n\n9. **Transfomer Layers -** We have added a line in section 4.3 under \u201cDiscussion\u201d to clarify this.  We apply our algorithm to all the linear layers in the transformer architecture. For an attention block, the linear layer weights are the Key Weights, Query Weights, Value Weights, and Output weights. We do not perform any unlearning of the normalization layers as the fraction of total parameters for these layers is insignificant and empirically is not seen to affect the performance of our algorithm. \n10. **typo-** We updated the line to  \u201c for both linear and convolutional layers, where $l$ is layer.\u201d\n\n**Response to Questions:**\n\n1. Our method relies on SVD which analyzes the underlying statistics of the activation spaces. We believe our method would work on any unlearning problem where there are differences in the data distributions of the forget samples and the retain samples. Class Unlearning is one such case where there is a significant difference in the data distributions, which makes it easier for SVD to figure out distinct spaces. When we assign a random subset of training samples as forget set, SVD can not distinguish well between the activations of forget samples and retain samples. This is because Retain samples are also drawn similarly (Independently Identically Distributed) from the training set. This is why the SVD based method would need further modifications to be fully effective for random subset unlearning problems. \n2. We thank the reviewer for pointing this out. We realized that the runtimes depend on the implementation of the algorithm and the background hardware it is running on. We analytically find the compute costs of different algorithms and plot them in Figure 5 and show the details in Appendix A.7. We see 7.3x lesser compute for our approach than the baseline. \n3. The mean forget accuracy in Figure 5b in the original version (it has been moved to Figure 10 in the updated version) is less than 0.37 on average. In all the experiments we ran for 5 class unlearning the forget accuracy was 0 in 6 of the 10 experiments (the other experiments have forget accuracy of 0.32, 0.7, 0.74, 1.88). In the experiments for 2 class unlearning 9 of the 10 experiments have 0 forget accuracy while one experiment had a forget accuracy of 2.5. These experiments show very low forget accuracies on the model and hence we believe are significant. \n4. Yes, it was computationally expensive to compute these numbers for ImageNet dataset. To address reviewer concern we have updated Section 6 with lines \u201cDue to the training complexity of the experiments, we were not able to obtain retrained models for ImageNet. We observe that the results on CIFAR10 and CIFAR100 datasets consistently show $ACC_f$ to be 0 and the $MIA$ performance to be $100\\%$. We, therefore, interpret the model with high $ACC_r$, $MIA$, and low $ACC_f$ as a better unlearnt model for these experiments.\u201d\n\n**Response to Minor Issues and typos**\n- We thank the reviewer for pointing this out. We went over the document and corrected grammatical errors."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8026/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700388209512,
                "cdate": 1700388209512,
                "tmdate": 1700389689803,
                "mdate": 1700389689803,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HdFGXdwPHI",
                "forum": "pUOesbrlw4",
                "replyto": "pBox5XK9ja",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8026/Reviewer_FQb4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8026/Reviewer_FQb4"
                ],
                "content": {
                    "title": {
                        "value": "thank you for the responses"
                    },
                    "comment": {
                        "value": "Hi authors, thank you for the detailed responses to my comments!\n\nI think the new version is improved due to the clarifications (e.g. the definition of the problem, the importance of the scaling, how the proposed methods is used in the transformer architecture, runtime computation, etc), additional analyses (e.g. layer-wise analysis), experiments (the MIA results in particular). \n\nI have a question about the MIA results. The authors state \"We report the average model predictions on Dtrain_f as the MIA scores in\nour evaluations. A high value of MIA score for a given sample indicates that it does not belong to the training data.\". I'm not sure I understand the rationale behind this way of computing a 'score'. Notably, having the MIA attacker be *certain* that the forget data was not seen is not necessarily the intended behaviour? Intuitively, we may want it to only be as certain about this as the attacker would have been if it was attacking a retrain-from-scratch model.\n\nIn prior works, the goal for MIA evaluations has been phrased as either having the binary MIA attacker have an accuracy of 50% (which, assuming that the set of members and non-members it is queried on are balanced, indicates that the attacker cannot distinguish the forget set from non-members). Could you comment on this difference? Generally, it would be great to precisely describe the MIA setup (on what data exactly was the SVM trained on and what scores we expect the ideal unlearner to have).\n\nI also think the authors should explicitly discuss the limitations of their method in the paper (as they say, by design this approach is perhaps more appropriate for class unlearning / situations with pronounced differences in the distributions of the forget and retain sets)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8026/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700571298869,
                "cdate": 1700571298869,
                "tmdate": 1700571298869,
                "mdate": 1700571298869,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1pbvy8zWYA",
                "forum": "pUOesbrlw4",
                "replyto": "pH82DhaZEn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8026/Reviewer_FQb4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8026/Reviewer_FQb4"
                ],
                "content": {
                    "title": {
                        "value": "thank you for the clarifications, and some follow-up thoughts"
                    },
                    "comment": {
                        "value": "Thank you, the MIA procedure used is now clearer. It would be great to also update the paper with these details.\n\nSome follow-up thoughts on this: if the forget set has a distribution that is quite distinct from the retain set distribution (as is the case for class unlearning for instance), then it is perhaps easier for a \"mia\" classifier to not think that the forget set belongs to the training set (merely because it follows a different distribution!). So, in this particular case, what the \"mia\" classifier may be doing is detecting the distribution change rather than actually performing (solely) membership inference.\nit would be great to explicitly state in this context which classes are part of the retain and test sets that are used to train the binary mia classifier; and which class(es) are part of the forget set that this binary classifier is then queried on, to surface this issue. \n\nWith this in mind, perhaps applying LiRA-like MIAs (\"membership inference attacks from first principles\", Carlini et al) is more appropriate, which operate on a per-example basis, and model the distribution of losses/confidences of *that particular example* when it is included vs excluded from training.\n\nHowever, I do think the experiments that the authors conducted are a great first step in this direction and already strengthen the paper."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8026/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700650966594,
                "cdate": 1700650966594,
                "tmdate": 1700650966594,
                "mdate": 1700650966594,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8YemhXCeVt",
            "forum": "pUOesbrlw4",
            "replyto": "pUOesbrlw4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8026/Reviewer_bRqF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8026/Reviewer_bRqF"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method for unlearning an entire class or a group of classes from a learned model. They estimate internal activations corresponding to all the layer, for the forget and retain set and then compute a forget and retain space. Then they intersect the two spaces and removes it from the forget space, and final project the weights onto the orthogonal space corresponding to this space. They show that such a method is efficient and performs better than the contemporary methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper address the problem of machine unlearning which is an important problem given the recent explosion in large scale models.\n2. The proposed method is easy to use, as it applied layer wise, and only requires layer wise SVD computation.\n3. Stable Ascent as a heurestic based method for unlearning is interesting in the case of linear models, however, in this case it is for non-linear models.\n4. The paper provides empirical results for different datasets and models."
                },
                "weaknesses": {
                    "value": "1. The paper is lacking a clear and precise definition of unlearning. Its is important to show the definition of unlearning that you want to achieve through your algorithm. \n2. The proposed algorithm is an empirical algorithm without any theoretical guarantees. It is important for unlearning papers to provide unlearning guarantees against an adversary.\n3. The approach is very similar to this method (http://proceedings.mlr.press/v130/izzo21a/izzo21a.pdf) applied on each layer, which is not cited.\n4. A simple baseline is just applying all the unlearning algorithm mentioned in the paper to the last layer vs the entire model. This comparison is missing. \n5. All the unlearning verification are only show wrt accuracy of the model or the confusion matrix, however, the information is usually contained in the weights of the model, hence other metrics like membership attack or re-train time after forgetting show be considered. \n6. The authors should also consider applying this method a linear perturbation of the network, as in those settings you will be able to get theoretical guarantees in regards to the proposed method, and also get better results. \n7. Since the method is applied on each layer, the authors should provide a plot of how different different weights of the model move, for instance plot the relative weight change after unlearning to see which layers are affected the most after unlearning."
                },
                "questions": {
                    "value": "Please see the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8026/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8026/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8026/Reviewer_bRqF"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8026/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698795445215,
            "cdate": 1698795445215,
            "tmdate": 1699636990064,
            "mdate": 1699636990064,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kuhQyVWPGe",
                "forum": "pUOesbrlw4",
                "replyto": "8YemhXCeVt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8026/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8026/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer bRqF"
                    },
                    "comment": {
                        "value": "**Response to Weakness:**\n1. Thanks for this suggestion. We have modified the writing to address this. \nWe clarify the definition in the following places in the updated paper.\nClass Unlearning  ( Section 1 Introduction) \u201cFor a class unlearning setup, the primary goal of the unlearning algorithm is to eliminate information associated with a target class from a pretrained model. This target class is referred to as the forget class, while the other classes are called the retain classes.\u201d Additionally, we have formally presented \u201cclass unlearning\u201d in Section 3: \u201cThe parameters $\\theta^*\\_f$ must be functionally indistinguishable from a network with parameters $\\theta^*$, which is retrained from scratch on the samples of $\\mathcal{D}\\_{train\\_r}$ in the output space. In other words, these parameters must satisfy $f(x\\_i, \\theta^*) \\simeq f(x\\_i, \\theta^*\\_f)$ for $(x\\_i, y\\_i) \\in D\\_{test}$ or $D\\_{train}$.\u201d\n2. We pose our work as empirical and evaluate our work on large-scale experiments with various sized dataset (including ImageNet) and SoTA deep neural network models (e.g. ViTs etc\u2019). The SoTA baselines we compare against [Tarun et al] and [kurmanji et al] are also empirically evaluated and the scale of their experiments is limited. Further, We have added experiments for Membership Inference Attacks to demonstrate the efficacy of forgetting. \n3. We thank the reviewer for providing the reference. We have added reference to this work in our paper. We would like to point out the fundamental differences between this work and our work. \n    -  It uses synthetic data and gradient-based optimization/finetuning of the model on these data for unlearning. Whereas we do not use any synthetic data and we don\u2019t need any gradient-based finetuning of the given pre-trained model.\n    - The scale of experiments is very different. In particular, this work proposes data deletion with a particular focus on linear and logistic regression whereas we propose an unlearning method for deep learning models that give SoTA performance. \n    - At its core this work relies on being able to calculate the logits of the retrained model without having access to the weights. Essentially calculating  $f(x\\_i,\\theta^*)$ without knowing $\\theta^*$, where $x\\_i$ is a forgot sample and $\\theta^*$ is retrained model weights. This is a challenging problem to solve in the case of deep learning models and datasets where data might not be very separable which limits the algorithm's application in deep networks. Our algorithm does not rely on the knowledge of logits of the retrained model and hence easily scales for challenging datasets and networks. \n4. We add the results for applying our method to the top layer in Appendix A6. Figure 8 shows applying our approach to the last layer only (indicated by layer=10) has a forget accuracy 65% which indicates that removing information from other layers is also necessary.\n5. We have added membership inference attack evaluations in our experiments. The results show our method performs well across different datasets and architectures obtaining an average improvement of 7.8% in MIA accuracy over other baselines.\n6.  Can the reviewer clarify this point e.g. what is linear perturbation of the network in the context of unlearning? Some reference would be helpful.\n7. We have added this plot in Figure 4. We see that later layers are more affected by unlearning through our approach. This is expected as the later layers are expected to learn complex class discriminatory information while the initial layers learn edges and simple textures [1]. \n\n**Note:** We have added a version of the paper with highlighted changes to supplementary material. \n\nReference:\n\n[1] Chris Olah, Alexander Mordvintsev, and Ludwig Schubert. Feature visualization. Distill, 2017"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8026/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700383499668,
                "cdate": 1700383499668,
                "tmdate": 1700388849539,
                "mdate": 1700388849539,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "D1UBlixwVZ",
                "forum": "pUOesbrlw4",
                "replyto": "8YemhXCeVt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8026/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8026/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to feedback."
                    },
                    "comment": {
                        "value": "Dear Reviewer bRqF, \n\nWe appreciate your valuable comments on our work. We have updated the paper as per your suggestion addressing most of the weaknesses and questions. We look forward to any additional feedback you have for improving the quality of our work further. \n\nThanks\nAuthors"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8026/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700613046757,
                "cdate": 1700613046757,
                "tmdate": 1700613046757,
                "mdate": 1700613046757,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "e4Iriyjq2d",
            "forum": "pUOesbrlw4",
            "replyto": "pUOesbrlw4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8026/Reviewer_ZSgK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8026/Reviewer_ZSgK"
            ],
            "content": {
                "summary": {
                    "value": "In this research endeavor, a novel class unlearning algorithm is introduced, meticulously designed to eliminate an entire class or a group of classes from the acquired model. The algorithm, developed within this study, initiates by estimating two essential spaces: the Retain Space and the Forget Space. These spaces represent the feature or activation spaces corresponding to the samples from classes that need to be retained and unlearned, respectively. The method proposed for obtaining these spaces leverages a unique singular value decomposition-based technique, mandating the collection of network activations at different layers via several forward passes through the network. Subsequently, the shared information between these spaces is computed and selectively removed from the Forget Space, thus isolating the class-discriminatory feature space for the unlearning process. Ultimately, the model weights are projected in the orthogonal direction of the class-discriminatory space, resulting in the derivation of the unlearned model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The method is very simple and elegant.\n2. Provides a strong baseline stable-ascent and also the proposed method beats the stable ascent.\n3. Results are more satisfactory than current SoTA methods."
                },
                "weaknesses": {
                    "value": "1. Requires a few training samples for unlearning. There are methods for zero-shot unlearning."
                },
                "questions": {
                    "value": "1. There is a more fundamental question about the class unlearning setup. The whole point of unlearning is to replicate a completely retrained model in parameter space or in output space  Now for the preliminary section 3 it is mentioned that for an unlearned model the output label of a datapoint belonging to an unlearned class, is not a true label in this case the unlearned class i.e. y_i != f(x_i,\\theta_f). Why is this the case? Is it not that the unlearned model output should be exact/almost the same as the retrained model? If the retrained model gives output as the true label on very few samples why unlearned model can\u2019t give output the same? In other terms,. if the retrained model gives accuracy let's say 3% on the test forget set and the unlearning model also gives similar accuracy on the forget set. The unlearning method is valid. So, I think the y_i != f(x_i,\\theta_f) can be formulated in better probabilistic terms so that it matches the retrained model. The implicit assumption is that for a retrained model the accuracy on the forget set if 0 is not correct. We can set adversarial examples such that the retrained model gives an accuracy of 100% on the forget set."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8026/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698838362238,
            "cdate": 1698838362238,
            "tmdate": 1699636989963,
            "mdate": 1699636989963,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QQiitencrw",
                "forum": "pUOesbrlw4",
                "replyto": "e4Iriyjq2d",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8026/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8026/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer ZSgK"
                    },
                    "comment": {
                        "value": "**Response to Weakness**: \n1. The reviewer is right to point out there are zero-shot unlearning works. However, here zero-shot does not mean that they don't use any samples for unlearning. It means the algorithm first generates a few synthetic samples and then unlearns with those [1], usually with SGD based training/finetuning steps. On the other hand, we propose a novel SVD-based approach where we use only a few samples from the training set to get the forget space and modify the weights, without any training, to forget the desired class. Generating synthetic samples from the trained model is a well-established idea. We believe our SVD-based space estimation can be used on these synthetic data to get the desired forget space. Hence would enable zero-shot unlearning. We leave this exploration for the future.\n\n**Response to Questions**: \n1. We have rewritten the definition in Section 3(Class Unlearning) as \"The parameters $\\theta^*\\_f$ must be functionally indistinguishable from a network with parameters $\\theta^*$, which is retrained from scratch on the samples of $\\mathcal{D}\\_{train\\_r}$ in the output space. In other words, these parameters must satisfy $f(x\\_i, \\theta^*) \\simeq f(x\\_i, \\theta^*\\_f)$ for $(x\\_i, y\\_i) \\in D\\_{test}$ or $D\\_{train}$.\" We thank the reviewer for pointing this out.\n\n**Note:** We have added a version of the paper with highlighted changes to supplementary material. \n\nReference:\n\n[1] Chundawat, Vikram S., et al. \"Zero-shot machine unlearning.\" IEEE Transactions on Information Forensics and Security (2023)"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8026/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700382461773,
                "cdate": 1700382461773,
                "tmdate": 1700382461773,
                "mdate": 1700382461773,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]