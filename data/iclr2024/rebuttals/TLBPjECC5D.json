[
    {
        "title": "Unlearning via Sparse Representations"
    },
    {
        "review": {
            "id": "XCAnOyqODP",
            "forum": "TLBPjECC5D",
            "replyto": "TLBPjECC5D",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7695/Reviewer_ffna"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7695/Reviewer_ffna"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose a compute-free method for achieving class unlearning in deep models. Their method assumes that the model is trained with a Discrete Key-Value Bottleneck (DKVB) which maps input data to key-value pairs. By removing the key corresponding to a certain class, the model is no longer able to provide a correct classification for it."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The method has virtually no computation overhead (just a few inference steps) \n\n- It does not introduce any new parameters\n\n- It provides an effective way for unlearning an entire class from a pretrained classification model"
                },
                "weaknesses": {
                    "value": "- Following Xu et al., 2023, the method falls in the \"Weak Unlearning\" category; meaning that the unlearning only affects the final activations of the models (in this case we can identify it with the DKVB). \n\n- Connected to the point above, weak unlearning does not guarantee that the internal parameters of the model do not still encode information about the forget set after the unlearning. In the context of this paper, given that the backbone of the model is always kept frozen and no finetuning is necessary, one could argue that, if the pretraining is done on the same dataset (Tab E.1), the final model will still contain forget samples encoded in its weights. This of course is a matter of current research, however it also limits the applicability of the proposed method. \n\n- I wonder if this method would be similar to taking a pretrained model (without DKVB) and then just either *i)* retraining the classification layer without the forget class or *ii)* masking out the forget class from the output logits. In both cases, I would find it difficult to call it \"unlearning\" but rather \"obfuscation\" of some kind, although I am not an expert in this field.\n\n- Unlearning an entire class seems perhaps unrealistic in practice; whereas unlearning specific instances should be more relevant. \n\n- I think that the experimental evaluation is a bit lacking and could be expanded to larger datasets such as ImageNet, as the proposed method is almost compute-free"
                },
                "questions": {
                    "value": "- How does your method compare to SCRUB in terms of the categorization provided by Xu et al. ? \n\n- What information about the forget samples remain after removing the keys? \n\n- Could you provide some practical examples in which: 1) whole class unlearning is relevant 2) weak unlearning is acceptable under law requirements (e.g. GDPR) \n\n- Can your method unlearn just a subset of the forget class? e.g. perhaps by using denser keys in the DKVB? If so, some experiments presenting this result would greatly improve the impact of this work. \n\n- Would your method work also if the pretrain dataset is different from the target one? \n\n- Can you expand more on Sec.D of the Appendix, as I think it is relevant from a practical point of view?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7695/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7695/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7695/Reviewer_ffna"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7695/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698757274316,
            "cdate": 1698757274316,
            "tmdate": 1699636936895,
            "mdate": 1699636936895,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IofEO3ktqe",
                "forum": "TLBPjECC5D",
                "replyto": "XCAnOyqODP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7695/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7695/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ffna"
                    },
                    "comment": {
                        "value": "We thank the reviewer for taking the time go through our work and giving a detailed feedback as well as suggestions. We hope to answer the reviewers questions in the rebuttal that follows:\n\n**Regarding the evaluation of the approach at scale**\n\nWe evaluate our approach on a larger scale by training the baseline and DKVB models on the ImageNet-1k and attempting to unlearn class number 1.\n\n* Experimental Setup - We train the DKVB and baseline model (i.e. Frozen backbone + trainable linear layer) on ImageNet-1k up to 68% classification accuracy. ImageNet-1k consists of 1.28 million training examples and 50000 test examples divided into 100 classes. Next, we choose class 1 as the class to be unlearnt, and perform the same set of experiments as performed for the three datasets in Section 5.2 and Section 5.3 in the paper.\n\n* Results - We present a comparative analysis between the baseline and the proposed approach on ImageNet-1k in the table given below. Further, we have also included scatter plots for the retain set accuracy vs forget set accuracy in Appendix I of the paper. We observe that our approach performs equally well on ImageNet as compared to the three datasets already included in the paper, with similar hyperparameters This demonstrates that the proposed approach is equally effective in large-scale settings as well. The given table presents a comparison between the proposed methods and the baselines on ImageNet-1k. We compare the change in performance on the retain and forget test data relative to the originally trained models.\n\n|  | $D_{retain}^{test}$ | $D^{forget}_{test}$ |\n|----------|----------|----------|\n| DKVB via Activations (sec 5.2) | 0.15 % | $\\textbf{-100}$ % |\n| DKVB via Examples (sec 5.2) | -0.03 % | $\\textbf{-100}$ % |\n| Linear Layer + SCRUB | $\\textbf{7.31\\}$ % | $\\textbf{-100}$ % |\n\n**Regarding Instance-Specific Unlearning**\n\nIn this work, we focus on the setting of class unlearning. Instance-specific unlearning (or selective unlearning) would be non-trivial with the proposed approach and we leave investigations regarding that for future work. \n\n**Elaborating on Membership Inference Attacks (Appendix Section D)**\n\nFirst, we would like to clarify that our approach is not a-priori suited for selective unlearning, i.e. the setting where we want the model to forget specific examples or a small subset of examples instead of removing the information about an entire class. The KV bottleneck induces clusters of representation, where the members of a particular cluster correspond to the representations belonging to the same class. When we try to unlearn the representations corresponding to one particular example belonging to a particular class, the KV bottleneck routes the selection to other (key-)representations within the same cluster. Since these representations also contain information about the same class as the examples we intend to unlearn, the model would still predict the class to be unlearnt.\n\nDue to the same reason our approach is also not designed for working against traditional Membership Inference attacks. According to the basic attacks setup as explained in their paper  the objective is to obtain a model that has unlearnt a small subset of specific examples (i.e. selective unlearning) such that the loss of the model on the unlearnt subset of examples should be indistinguishable from examples that the model never saw during training.\n\nNevertheless, we attempt to modify the above setup to that of \"Class Membership Inference Attacks (CMIA)\". In CMIA, the aim is to defend against an attacker whose aim is to determine whether a model that has undergone unlearning ever saw a particular class as a part of its training data. Thus, we want the model to unlearn a particular class such that the losses/performance of the model on the unlearnt class is indistinguishable from a held-out class that the model never saw during its training. We describe the experimental setup and results below:\n\n* Experimental Setup: We perform the experiment for CIFAR10. We divide the dataset into training data ($\\mathcal{D_{Train}}$), validation data ($\\mathcal{D_{Val}}$) and test data ($\\mathcal{D_{Test}}$). Training Data consists of 4000 examples per class; validation and test data consist of 1000 examples per class. We first trained a model on the first 9 classes of CIFAR10. Here, class number 10 is the held-out class. Next, we unlearn class 1 from the model using the Unlearning via Activations approach introduced in the paper. We unlearn the model until the loss on the validation sets of the forget class and the held-out class are similar. In our experiments, we find that we reach this point at approximately $N_a = 240000$. The loss $l(x, y)$ in our case would be the cross-entropy loss."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7695/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689974259,
                "cdate": 1700689974259,
                "tmdate": 1700735730093,
                "mdate": 1700735730093,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LhHayhOcX3",
                "forum": "TLBPjECC5D",
                "replyto": "XCAnOyqODP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7695/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7695/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ffna [Continued]"
                    },
                    "comment": {
                        "value": "* Next, we label the losses corresponding to the validation and test set of the forget class as 1 and those corresponding to the validation and test set of the held out class as 0. We train a binary classifier as done in the \"Basic Attack\" setting in [6] on the validation losses of both sets and test it on the test losses. We follow a similar setting for the baseline model, where we obtain the model suitable for MIA defense by following a procedure similar to SCRUB+R as introduced in [6]. For a successful defense, we would want the accuracy of the classifier to be close to 50%, indicating that it is unable to distinguish between the unlearnt class and the held-out class. Same as Kurmanj [1], we use sklearn.logistic_regression as our attacker (the binary classifier). We call the approach described above Partial UvA (Partial Unlearning via Activations).  We run experiments for 3 random seeds, and the mean of the attacker performance is reported. \n* Observations and Results: We report the results of the experiment described above in the table given below. We observe that although the baseline performs better, the proposed approach performs competitively, even though we have not intended to develop the method for this scenario. \n\n| Approach | Accracy of the attacker |\n|-----------|-----------|\n| Partial UvA | 53.5 |\n| Linear Layer + SCRUB + R | 51.50 |\n\n**Would the method work if the pretrain dataset is different from the target one**\n\nIn all of our experiments, the dataset used for pretraining the backbone is always distinct from the downstream task. For all the experiments in the paper, we use a ViT-B/32 backbone which has been pretained using CLIP (Radford et al., 2021). which uses web-scale Image-Text data for pretraining. In our new experiments with a ResNet50 backbone (see Appendix H) which has been pretrained in a supervised manner on ImageNet and the tasks used for downstream tasks are CIFAR-10, CIFAR-100 and LACUNA-100)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7695/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690010353,
                "cdate": 1700690010353,
                "tmdate": 1700702873571,
                "mdate": 1700702873571,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0EY4p58pex",
            "forum": "TLBPjECC5D",
            "replyto": "TLBPjECC5D",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7695/Reviewer_g9Bk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7695/Reviewer_g9Bk"
            ],
            "content": {
                "summary": {
                    "value": "Nonetheless, every approach to machine unlearning necessitates a substantial increase in computational resources to facilitate the unlearning process. In this scholarly work, the authors posit that neural information bottlenecks offer a highly efficient and precise method for unlearning. Their research demonstrates that this proposed technique effectively unlearns the specified data, causing minimal disruption to the model's performance on the remaining dataset. The researchers assess the effectiveness of their approach in the context of class unlearning, using three distinct datasets: CIFAR-10, CIFAR-100, and LACUNA100. They conduct a comparative analysis between their proposed technique and SCRUB, a state-of-the-art unlearning approach that employs knowledge distillation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Zero Shot Unlearning.\n2. Low computational cost\n3. Model Specific only applicable to models with Discrete Key Value Bottleneck."
                },
                "weaknesses": {
                    "value": "1. Even though the whole motivation of this approach is to implement a new approach with better computational efficiency no experiments on computational cost are done. This approach is not measured with current approaches of class unlearning in terms of computational efficiency."
                },
                "questions": {
                    "value": "1. Is this approach model agnostic? I am not sure if this can applied to other models if the bottleneck is not present."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7695/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698839499691,
            "cdate": 1698839499691,
            "tmdate": 1699636936783,
            "mdate": 1699636936783,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1ZiACppyRm",
                "forum": "TLBPjECC5D",
                "replyto": "0EY4p58pex",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7695/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7695/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer g9Bk"
                    },
                    "comment": {
                        "value": "We thank the reviewer for taking the time to go through our work. We attempt to address the points raised by the reviewer in this rebuttal:\n\n**Regarding Experiments around the computational cost of the approaches discussed**\n\nWe measure the runtimes of the experiments discussed in Section 5.3 and Table 2 as a proxy to the computational costs required by the approaches discussed. The results are presented in the table below. We note that both the proposed approaches are several times faster than the baseline\n\n| | CIFAR-10 | CIFAR-100 | LACUNA-100 |\n|----------|----------|----------|----------|\n| Runtimes (in seconds) | | | |\n| DKVB via Activations (sec 5.2) | $\\textbf{5.02}$ | $\\textbf{1.57}$ | $\\textbf{2.74}$ |\n| DKVB via Examples (sec 5.2) | 13.98 | 6.65 | 13.03 |\n| Linear Layer + SCRUB | 288.80 | 921.56 | 553.31 |\n\nWe find that Unlearning via Activations is consistently the most compute efficient approach while both Unlearning via Activations and Unlearning via Examples being at least ~20x faster than the baseline\n\n**Regarding the approach being model agnostic**\n\nThe reviewer correctly notes that the proposed approaches are conditioned on the presence of a Discrete Key Value Bottleneck. However, it is agnostic to the choice of the frozen backbone and the decoder architectures. This can be supported by evidence from our new experiments with a ResNet50 backbone (see Appendix Section H) which show that the proposed approach also works with backbones other than a CLIP pretrained ViT-B/32."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7695/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688907956,
                "cdate": 1700688907956,
                "tmdate": 1700688907956,
                "mdate": 1700688907956,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0clnBZv7tU",
            "forum": "TLBPjECC5D",
            "replyto": "TLBPjECC5D",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7695/Reviewer_Pr1b"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7695/Reviewer_Pr1b"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of unlearning where the task is to forget a set of concepts learned during training while retaining good performance on other concepts. The paper demonstrates that sparse representations can help decompose the concepts, making post-hoc unlearning amendable. Specifically, they train an architecture with a Discrete Key-Value Bottleneck which specifically induces sparse key-value pairs. Next, given training examples that correspond with a particular concept, they propose pruning away certain key-value pairs that light up the most for these examples. On image classification tasks CIFAR10/100 and and LACUNA-100, they show that this procedure can drop a particular class\u2019s test accuracy to 0% while retaining the same performance on the remaining classes."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper demonstrates strong performance gains, and is computationally inexpensive. The method only requires forward passes since irrelevant key-value pairs are simply pruned unlike previous methods which utilize some form of negative gradients updates or knowledge distillation. \n\nThe paper is generally an easy read and figures are clear."
                },
                "weaknesses": {
                    "value": "In terms of novelty, I think it\u2019s important to point out that this paper is a direct application of Discrete Key-Value Bottleneck (Trauble 2022). \n1. DKVB was proposed for class-incremental learning, and class unlearning is quite literally the inverse task. \n2. The original work also shows improvements on CIFAR10/100, the same benchmarks in this paper. \n3. DKVB improves class-incremental learning since each class can be learned by disjoint key-value pairs, thus the model updates for learning new classes can be localized to certain parameters. Thus, why DKVB would help with unlearning might be quite trivial \u2013 simply deleting the key-value pairs corresponding with a certain class. \n\nThere are also a couple improvements that could be made to improve the quality of the paper.\n\n1. The paper focuses on performance evaluation, but there\u2019s a lack of empirical analysis of their conceptual motivation. For example, how often are key-value pairs shared between classes? Is it close to 0? Can this be visualized? \n2. A more diverse range of benchmarks is necessary to test the unlearning capabilities of DKVB. It\u2019s unclear how successful the unlearning method is because the evaluation benchmarks are quite similar to each other (CIFAR10/100, LACUNA100). It might be interesting to evaluate the method on more diverse benchmarks such as benchmarks with outlier classes that are quite similar to each other, or training to distinguish between superclasses and unlearning a specific class in a superclass."
                },
                "questions": {
                    "value": "1. \u201cZero-shot\u201d is thrown around in the paper, but seems like the wrong terminology here and I am unsure what it is referring to exactly since their method and the baseline method SCRUB both require utilizing training examples. Maybe they mean something closer to \u201czero-order\u201d. \n2. The authors choose to unlearn a class until 0% accuracy. Is this correct, or should you be unlearning up until random chance (10%)?\n3. Section 5.4 was a bit unclear.\n- What do you mean by \u201cimplications of additional compute\u201d? Are you retraining the DKVB models for longer?\n- What do you mean by \u201cretraining\u201d? In Figure 4, you state \u201cretraining\u2026using the proposed methods\u201d (It says \u201cproposed models\u201d but I think you meant to say \u201cproposed methods\u201d?) What does it mean to retrain using SCRUB. Or is the SCRUB\u2019ed model then retrained to learn the forgotten class just by normal supervised training? \n- It seems a bit strange that the performance would ever drop during retraining with SCRUB."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7695/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698899913569,
            "cdate": 1698899913569,
            "tmdate": 1699636936676,
            "mdate": 1699636936676,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Lw11jZMCYb",
                "forum": "TLBPjECC5D",
                "replyto": "0clnBZv7tU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7695/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7695/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Pr1b"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful comments and suggestions. We attempt address the concerns raised by the reviewer below:\n\n**Evaluating the approach on more benchmarks**\n\nWe tested the proposed approach on ImageNet-1k dataset and the results are as follows:\n\n* Experimental Setup - We train the DKVB and baseline model (i.e. Frozen backbone + trainable linear layer) on ImageNet-1k up to 68% classification accuracy. ImageNet-1k consists of 1.28 million training examples and 50000 test examples divided into 1000 classes. Next, we choose class 1 as the class to be unlearnt, and perform the same set of experiments as performed for the three datasets in Section 5.2 and Section 5.3 in the paper.\n* Results - We present a comparative analysis between the baseline and the proposed approach on ImageNet-1k in the table given below. Further, we have also included scatter plots for the retain set accuracy vs forget set accuracy in Appendix I of the paper. We observe that our approach performs equally well on ImageNet as compared to the three datasets already included in the paper, with similar hyperparameters This demonstrates that the proposed approach is equally effective in large-scale settings as well. The table given below provides a comparison between the proposed methods and the baselines on ImageNet-1k. We compare the change in performance on the retain and forget test data relative to the originally trained models.\n\n|  | $D_{retain}^{test}$ | $D^{forget}_{test}$ |\n|----------|----------|----------|\n| DKVB via Activations (sec 5.2) | 0.15 % | $\\textbf{-100}$ % |\n| DKVB via Examples (sec 5.2) | -0.03 % | $\\textbf{-100}$ % |\n| Linear Layer + SCRUB | $\\textbf{7.31\\}$ % | $\\textbf{-100}$ % |\n\n**Complete Unlearning vs Unlearning until random accuracy**\n\nAs discussed in the literature, unlearning completely vs random accuracy depends on the task at hand. For instance, in cases where we care about removing harmful biases from a model (e.g. remove a users data completely), complete unlearning is more desirable. In other scenarios such as Membership Inference Attacks (MIAs), the focus is on user privacy, it is desired that the model unlearns only to an extent such that the performance of the model on the unlearnt class is equivalent to that on a class that it has never seen. For the proposed approaches of Unlearning via Examples and Unlearning via Activations, this \u201cextent of unlearning\u201d can be controlled using the parameters $N_e$ and $N_a$ respectively as shown in Figures 2 and 3. Further, our new experiments of MIAs as discussed in Appendix D. Here, we show that the proposed approach is also useful in cases where complete unlearning is not desired.\n\n**Clarifications regarding Section 5.4**\n\n* Implications of using additional compute - The proposed approaches do not require additional compute for unlearning a class apart from the compute required for inference. The baseline, on the other hand, requires additional compute in the form of training the model and optimizing it w.r.t. a given objective. Thus, we attempt to investigate what would happen if the proposed approach also somehow made use of additional compute. In order to make use of some additional compute, once the DKVB models have undergone unlearning of the forget class, we train them on just the retain set in order to cover up any damage in performance on the retain set that might have occurred due to the unlearning.\n* Use of the term \u201cretraining\u201d - We use the term \u201cretraining\u201d to refer to the above-explained process of training the unlearnt DKVB models on only the retain set in order to improve the performance on the retain set."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7695/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688063648,
                "cdate": 1700688063648,
                "tmdate": 1700730559248,
                "mdate": 1700730559248,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TMm6PbwbNd",
            "forum": "TLBPjECC5D",
            "replyto": "TLBPjECC5D",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7695/Reviewer_GrC6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7695/Reviewer_GrC6"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel approach to machine unlearning, aiming to efficiently remove data from a trained model\u2014a process known as \"forgetting\"\u2014without the significant computational overhead typically associated with such tasks. The authors propose a zero-shot unlearning technique that utilizes a discrete representational bottleneck to erase specific knowledge of a forget set from a model. This technique is claimed to preserve the overall performance of the model on the remaining data."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "**Writing**:\nThe paper's logic is clear, and it is easy to follow."
                },
                "weaknesses": {
                    "value": "- **Incremental Novelty**: While the application of concepts from DKVB [1] to the context of machine unlearning is interesting, it may appear as an incremental advance rather than a groundbreaking innovation. Nonetheless, the practical integration of these ideas to address real-world challenges is acknowledged as valuable and can sometimes be sufficient to make significant contributions to the field. (Minor points)\n\n- **Comparative Analysis**: The paper could benefit from a broader comparison with existing work, particularly the zero-shot unlearning approach presented in [2]. Additionally, for the specific examples of unlearning demonstrated in Figure 3, it would be advantageous to include a wider range of baselines, as referenced in [3-6], to provide a more comprehensive evaluation of the proposed method.\n\n- **Unlearning Scenarios**: The focus on 'complete unlearning' may limit the paper's scope, given the broader spectrum of unlearning scenarios presented in [2-7], where the goal is for the unlearned model to closely resemble a model that has been retrained without the forgotten data. Although the authors claim in Appendix D that their method is applicable to traditional machine unlearning problems, detailed results and discussions within these contexts would strengthen the paper's contributions.\n\n- **Efficiency Metrics**: For claims regarding efficiency, the absence of reported running times is a noticeable omission. Including such metrics would substantiate the claims of the method's efficiency and offer a tangible comparison with other techniques.\n\n- **Model Diversity**: The exclusive use of the ViT model in the experiments may raise questions about the method's generalizability. As DKVB [1] also considers ResNet architectures, a discussion on the application or limitation of the proposed method to other architectures would provide deeper insights into its adaptability and utility across various models.\n\n- **Limited Literature Review**: Given the recent surge in research surrounding machine unlearning, the paper's literature review could be perceived as insufficiently comprehensive. I included some papers published recently in the following references.\n\n>[1] Tr\u00e4uble, Frederik, et al. \"Discrete key-value bottleneck.\" International Conference on Machine Learning. PMLR, 2023.\n>\n>[2] Chundawat, Vikram S., et al. \"Zero-shot machine unlearning.\" IEEE Transactions on Information Forensics and Security (2023).\n>\n>[3] Chen, Min, et al. \"Boundary Unlearning: Rapid Forgetting of Deep Networks via Shifting the Decision Boundary.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n>\n>[4] Warnecke, Alexander, et al. \"Machine unlearning of features and labels.\" arXiv preprint arXiv:2108.11577 (2021).\n>\n>[5] Jia, Jinghan, et al. \"Model sparsification can simplify machine unlearning.\" arXiv preprint arXiv:2304.04934 (2023).\n>\n>[6] Kurmanji, Meghdad, Peter Triantafillou, and Eleni Triantafillou. \"Towards Unbounded Machine Unlearning.\" arXiv preprint arXiv:2302.09880 (2023).\n>\n>[7] Golatkar, Aditya, Alessandro Achille, and Stefano Soatto. \"Eternal sunshine of the spotless net: Selective forgetting in deep networks.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020."
                },
                "questions": {
                    "value": "- Could you consider including the additional baselines identified as pertinent in Figure 3? This would help provide a comprehensive evaluation against a variety of existing methods.\n- Would it be possible to provide more experimental results assessed against the conventional machine unlearning benchmarks, particularly those evaluating the similarity to a model retrained without the forget set?\n- Can a comparison be drawn against the baseline referenced regarding zero-shot unlearning settings, as outlined in the weaknesses?\n- Could you augment your experimental results with additional insights on how your technique performs on models within the ResNet family?\n- Considering the broader discussion in the literature on the role of sparsity in machine unlearning, could you discuss how your approach aligns with or diverges from these concepts, specifically referring to studies [1-2]?\n\n> [1] Mehta, Ronak, et al. \"Deep unlearning via randomized conditionally independent hessians.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n> \n> [2] Jia, Jinghan, et al. \"Model sparsification can simplify machine unlearning.\" also mentions sparsity can help machine unlearning."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7695/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699140519476,
            "cdate": 1699140519476,
            "tmdate": 1699636936557,
            "mdate": 1699636936557,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GVvY2s7MbX",
                "forum": "TLBPjECC5D",
                "replyto": "TMm6PbwbNd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7695/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7695/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Reviewer GrC6"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their detailed and very helpful feedback on our paper. We hope  to address your questions and suggestions in the following.\n\n\n**Regarding Model Diversity**\n\nAs requested by the reviewer, we performed the same set of experiments presented in the paper in Sections 5.2 and 5.3 for CIFAR-10, with a backbone from the ResNet family. \n* We use an ImageNet supervised pretained ResNet50  model as the backbone for our approach as well as the baseline model. \n* The results are as presented in the table below. We have also attached the scatter plots for the retain set accuracy vs forget set accuracy in Appendix H of the paper. We observe that the results with ResNet backbone are fairly similar to the ones with ViT backbone, discounting the difference in absolute performances. Importantly, even in this scaled-up setting we are able to perform unlearning with minimal compute without damaging the performance on the retain set.\n* We compare the change in performance on the retain and forget test data\nrelative to the originally trained models. For the baseline, we report two cases: Case B where the\nmodel unlearns the forget set completely but the retain set performance is not preserved very well,\nand Case A where the retain set performance is not preserved very well, but the model does not\nunlearn the forget set completely\n\n\n|  | $D^{retain}_{test}$ | $D^{forget}_{test}$ |\n|----------|----------|----------|\n| DKVB via Activations (sec 5.2) | 0.04% | $\\textbf{-100}$% |\n| DKVB via Examples (sec 5.2) | -0.07% | $\\textbf{-100}$% |\n| Linear Layer + SCRUB (A) | $\\textbf{-0.44\\}$% | -96.04% |\n| Linear Layer + SCRUB (B) | -11.75% | $\\textbf{-100}$% |\n\n**Regarding comparison of runtimes**\n\nWe measured the runtimes of the experiments discussed in Section 5.3 and Table 2. The results are presented in the table below. We note that both the proposed approaches are several times faster than the baseline\n\n| | CIFAR-10 | CIFAR-100 | LACUNA-100 |\n|----------|----------|----------|----------|\n| Runtimes (in seconds) | | | |\n| DKVB via Activations (sec 5.2) | $\\textbf{5.02}$ | $\\textbf{1.57}$ | $\\textbf{2.74}$ |\n| DKVB via Examples (sec 5.2) | 13.98 | 6.65 | 13.03 |\n| Linear Layer + SCRUB | 288.80 | 921.56 | 553.31 |\n\n**Regarding more Unlearning Scenarios**\n\nFirst, we would like to clarify that our approach is not a-priori suited for selective unlearning, i.e. the setting where we want the model to forget specific examples or a small subset of examples instead of removing the information about an entire class. The KV bottleneck induces clusters of representation, where the members of a particular cluster correspond to the representations belonging to the same class. When we try to unlearn the representations corresponding to one particular example belonging to a particular class, the KV bottleneck routes the selection to other (key-)representations within the same cluster. Since these representations also contain information about the same class as the examples we intend to unlearn, the model would still predict the class to be unlearnt.\n\nDue to the same reason our approach is also not designed for working against traditional Membership Inference attacks. According to the basic attacks setup as explained in Kurmanji et al., 2023, the objective is to obtain a model that has unlearnt a small subset of specific examples (i.e. selective unlearning) such that the loss of the model on the unlearnt subset of examples should be indistinguishable from examples that the model never saw during training.\n\nNevertheless, we attempt to modify the above setup to that of \"Class Membership Inference Attacks (CMIA)\". In CMIA, the aim is to defend against an attacker whose aim is to determine whether a model that has undergone unlearning ever saw a particular class as a part of its training data. Thus, we want the model to unlearn a particular class such that the losses/performance of the model on the unlearnt class is indistinguishable from a held-out class that the model never saw during its training. We describe the experimental setup and results below:\n \n* Experimental Setup: We perform the experiment for CIFAR10. We divide the dataset into training data ($\\mathcal{D_{Train}}$), validation data ($\\mathcal{D_{Val}}$) and test data ($\\mathcal{D_{Test}}$). Training Data consists of 4000 examples per class; validation and test data consist of 1000 examples per class. We first trained a model on the first 9 classes of CIFAR10. Here, class number 10 is the held-out class. Next, we unlearn class 1 from the model using the Unlearning via Activations approach introduced in the paper. We unlearn the model until the loss on the validation sets of the forget class and the held-out class are similar. In our experiments, we find that we reach this point at approximately $N_a = 240000$. The loss $l(x, y)$ in our case would be the cross-entropy loss."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7695/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681274759,
                "cdate": 1700681274759,
                "tmdate": 1700731124807,
                "mdate": 1700731124807,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zKs5OhoYUP",
                "forum": "TLBPjECC5D",
                "replyto": "TMm6PbwbNd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7695/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7695/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GrC6 [Continued]"
                    },
                    "comment": {
                        "value": "* Next, we label the losses corresponding to the validation and test set of the forget class as 1 and those corresponding to the validation and test set of the held out class as 0. We train a binary classifier as done in the \"Basic Attack\" setting in [6] on the validation losses of both sets and test it on the test losses. We follow a similar setting for the baseline model, where we obtain the model suitable for MIA defense by following a procedure similar to SCRUB+R as introduced in [6]. For a successful defense, we would want the accuracy of the classifier to be close to 50%, indicating that it is unable to distinguish between the unlearnt class and the held-out class. Same as Kurmanj [1], we use sklearn.logistic_regression as our attacker (the binary classifier). We call the approach described above Partial UvA (Partial Unlearning via Activations).  We run experiments for 3 random seeds, and the mean of the attacker performance is reported. \n* Observations and Results: We report the results of the experiment described above in the table given below. We observe that although the baseline performs better, the proposed approach performs competitively, even though we have not intended to develop the method for this scenario. \n\n| Approach | Accracy of the attacker |\n|-----------|-----------|\n| Partial UvA | 53.5 |\n| Linear Layer + SCRUB + R | 51.50 |\n\n**Regarding Limited Literature Review**\n\nWe thank the reviewer for pointing us to these important papers. We have discussed these ([1], [3], [4], [5]) works as well as drawn a comparison of the proposed approach to [1] and [5] in the \"Additional Related Works\" section in the Appendix (section G).\n\n > [1] Jia, Jinghan, et al. \"Model sparsification can simplify machine unlearning.\" arXiv preprint arXiv:2304.04934 (2023). \\\n> [2] Chundawat, Vikram S., et al. \"Zero-shot machine unlearning.\" IEEE Transactions on Information Forensics and Security (2023). \\\n> [3] Warnecke, Alexander, et al. \"Machine unlearning of features and labels.\" arXiv preprint arXiv:2108.11577 (2021). \\\n> [4] Chen, Min, et al. \"Boundary Unlearning: Rapid Forgetting of Deep Networks via Shifting the Decision Boundary.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\\\n> [5] Mehta, Ronak, et al. \"Deep unlearning via randomized conditionally independent hessians.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022 \\\n> [6] Kurmanji, Meghdad, Peter Triantafillou, and Eleni Triantafillor> \"Towards Unbounded Machine Unlearning.\" arXiv preprint arXiv:2302.09880 (2023)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7695/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687080817,
                "cdate": 1700687080817,
                "tmdate": 1700730803481,
                "mdate": 1700730803481,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]