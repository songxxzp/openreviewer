[
    {
        "title": "How Over-Parameterization Slows Down Gradient Descent in Matrix Sensing: The Curses of Symmetry and Initialization"
    },
    {
        "review": {
            "id": "NwQ9DKTMBf",
            "forum": "xGvPKAiOhq",
            "replyto": "xGvPKAiOhq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3284/Reviewer_XP68"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3284/Reviewer_XP68"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the matrix sensing problem where one observes y_i = <A_i, M^*> and A_i, and aims to estimate M^*. \nThis problem makes sense for either symmetric or asymmetric matrices. \nThe most significant contribution is that this paper unveils a surprising phenomenon that even for the symmetric version of the problem, introducing asymmetry in the initialization and the parametrization produces qualitatively faster convergence rate."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The matrix sensing problem requires no more motivation (at least to me) and the results in this paper bring further insights to this classical problem. \nThe paper is reasonably well-written and the results are definitely sufficiently interesting for ICLR. \nThe punchlines (the fact that asymmetry helps and why/how this is the case) are clearly addressed within the first 4 pages. \nSec 4.1 is pedagogically helpful. \nSeveral versions of the problems are treated to a reasonably systematic extent."
                },
                "weaknesses": {
                    "value": "I don't see major weaknesses. \nPlease see technical comments below."
                },
                "questions": {
                    "value": "1. All results only assume RIP for A_i. If A_i are i.i.d. Gaussian matrices, is it possible to derive sharper or even asymptotically exact (in the sense of e.g. https://arxiv.org/abs/2207.09660) results?\n\n2. Could the authors comment on how crucially the results rely on the \"linearity\" of the problem? Does it make sense to consider a \"generalized\" matrix sensing problem in which y_i = phi(<A_i, M^*>) for some non-linearity phi? This is somewhat motivated by other models with similar structures such as generalized linear models or single-index models. I guess the information exponent of phi or something like that will play a role in the convergence rate. \n\n3. In Sec 5, an accelerated method is proposed. In particular, step (5.1) should be executed once the iterates are sufficiently close to the optimum. But in practice, how can one verify this neighborhood condition? Note that Sigma is unknown. Please let me know if I missed something simple here. \n\n4. It seems that both the model and the algorithms are deterministic. What happens if the observations are noisy?\n\n5. It's claimed on top of page 6 that the results easily extend to the rectangular case. Could the authors state such results formally (even without formal proofs)? I'm curious to see how the results depend on the aspect ratio n_2 / n_1. In fact, if the matrices are extremely rectangular (e.g. n_2 / n_1 is growing or decaying), I actually doubt if such extensions are so straightforward. Thanks in advance for the clarification. \n\n6. Lemma G.1 assumes x, y are \"random vectors\". Are they actually independent and uniform over the sphere? For generic joint distribution, not much can be said about their angle. Please make the statement more precise. \n\nMinor notational/grammatical issues. \n1. The ground truth is interchangeably denoted by M^* or M^\\star. I suggest stick to M^\\star to avoid conflict with adjoint operator. \n\n2. In the title of Sec 1.2, where is the word \"symmetric\" repeated twice?\n\n3. Statement of Theorem 1.3: t-the iteration --> t-th iteration. \n\n3. Page 4: which we require it to be small --> which we require to be small.\n\n4. Description of Table 1: by \"row\" I think the authors meant \"column\". \n\n5. Right after equation (2.2): definition of A should be A^*."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3284/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3284/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3284/Reviewer_XP68"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3284/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698591892646,
            "cdate": 1698591892646,
            "tmdate": 1699636277044,
            "mdate": 1699636277044,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zafbYCtyEh",
                "forum": "xGvPKAiOhq",
                "replyto": "NwQ9DKTMBf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3284/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3284/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Reviewer XP68"
                    },
                    "comment": {
                        "value": "> 1. All results only assume RIP for A_i. If A_i are i.i.d. Gaussian matrices, is it possible to derive sharper or even asymptotically exact results?\n\n\n\nSince we only use one derived property of RIP, (G.19) in Lemma G.2, the sample complexity might be improved if we use particular properties of Gaussian matrices to prove (G.19). It might be possible to further derive a sharper or even asympototically exact results, but it requires a different set of techniques rather than standard RIP technique in matrix sensing.\n\n> 2. Could the authors comment on how crucially the results rely on the \"linearity\" of the problem? Does it make sense to consider a \"generalized\" matrix sensing problem in which $y_i = \\phi(<A_i, M^*>)$ for some non-linearity $\\phi$? \n\nThis is a good question. In this paper, our analysis heavily relies on the linearity of the problem: We utilize a key property that the imbalanced term $F_t^TF_t-G_t^TG_t$ keeps large during the convergence of the gradient descent. This property is derived by the updating rule that the change is $O(\\eta^2)$.  However, when the problem is nonlinear, this property does not hold and it seems hard to apply a similar technique. We leave it as a future work.\n\n> 3. In Sec 5, an accelerated method is proposed. In particular, step (5.1) should be executed once the iterates are sufficiently close to the optimum. But in practice, how can one verify this neighborhood condition? Note that Sigma is unknown. \n\n\nIn practice, since we can get an estimate of the loss $F_tG_t^T-\\Sigma$ by calculating $\\mathcal{A}^*(y) = \\mathcal{A}(F_tG_t^T-\\Sigma)$, we can set a threshold and apply the modification step when the loss first becomes less than the threshold. We can also apply the modification at the half of the process like the experiment in our paper.\n\n> 4. It seems that both the model and the algorithms are deterministic. What happens if the observations are noisy?\n\nSince our paper mainly focuses on the exact global convergence result, we do not consider the noisy setting. The noisy setting induces an additional error term in the gradient.     We believe that when the variance of the noise is small, our result can be extended to the noisy setting, and the gradient descent finally converges to a statistical error dependent on the noise. \n\n> 5. It's claimed on top of page 6 that the results easily extend to the rectangular case. Could the authors state such results formally (even without formal proofs)? I'm curious to see how the results depend on the aspect ratio n_2 / n_1.\n\nAssume that $F \\in \\mathbb{R}^{n_1\\times k}$ and $G \\in \\mathbb{R}^{n_2\\times k}$. Since we do not use the property that $F$ and $G$ are square matrices, we can replace $n$ by $\\max\\{n_1,n_2\\}$ among all the proof and main theorem of Theorem 4.2, 4.3 and 5.1. \n\n\n\n> 6. Lemma G.1 assumes x, y are \"random vectors\". Are they actually independent and uniform over the sphere? For generic joint distribution, not much can be said about their angle. Please make the statement more precise.\n\nThanks for catching it. Indeed, we assume every element of the random vector $x,y$ is independent and Gaussian. We have updated our paper accordingly.\n\n\n> 7. typos\n\nThanks for pointing out the typos. We have fixed them in our revised version.\n\n*References:*\n\n[1]. Tian Ye, Simon S. Du.  Global convergence of gradient descent for asymmetric low-rank matrix factorization. NeurIPS 2021.\n\n[2]. Jiang et al. Algorithmic Regularization in Model-free Overparametrized Asymmetric Matrix Factorization. SIAM 2023.\n\n[3].  Soltanolkotabi et al. Implicit Balancing and Regularization:\nGeneralization and Convergence Guarantees for Overparameterized Asymmetric Matrix Sensing. COLT 2023.\n\n[4]. Zhuo et al. On the computational and statistical complexity of over-parameterized matrix sensing. preprint 2021."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3284/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700362776754,
                "cdate": 1700362776754,
                "tmdate": 1700366960177,
                "mdate": 1700366960177,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sl1anEbJF2",
                "forum": "xGvPKAiOhq",
                "replyto": "zafbYCtyEh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3284/Reviewer_XP68"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3284/Reviewer_XP68"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thanks to the authors for addressing my questions to a reasonably satisfactory extent. \nMy evaluation remains unchanged."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3284/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700568444881,
                "cdate": 1700568444881,
                "tmdate": 1700568444881,
                "mdate": 1700568444881,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RaeHmkdmAZ",
            "forum": "xGvPKAiOhq",
            "replyto": "xGvPKAiOhq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3284/Reviewer_UZwv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3284/Reviewer_UZwv"
            ],
            "content": {
                "summary": {
                    "value": "In this work, the authors provided the analysis on the different convergence rates when exact-parameterization or over-parameterization are used. They also proposed a new algorithm to avoid the dependence of the convergence rate on the initialization rate for the asymmetric and over-parameterized case."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The results of this paper are novel and should be interesting to audiences in optimization and machine learning fields. The theory provides an explanation for the slow-down of GD in the over-parameterized case, and the paper offered a partial solution to this problem. However, due to the time limit, I cannot check the appendix. So I am not sure about the correctness of the results in this work."
                },
                "weaknesses": {
                    "value": "I can only see a few minor problems with the presentation. For example, the requirement on the sample complexity can be briefly discussed when the informal results are introduced."
                },
                "questions": {
                    "value": "(1) Theorem 1.1: it would be better to say that each entry of X is independently initialized with Gaussian random variable with variance \\alpha^2. Similar comment applies to other theorems.\n\n(2) In Section 1, I think the authors did not mention any requirements on the sample size m. It might be better to briefly mention the requirement on the sample complexity or the RIP constant in Section 1.\n\n(3) For the asymmetric case, I think most convergence results require a regularization term \\|F^TF - G^TG\\|_F^2 to penalize the imbalance between F and G. It would be better to mention the intuition why the regularization term is not required in this work.\n\n(4) After Theorem 1.3: I think it should be \"Comparing Theorem 1.3 and Theorem 1.1\".\n\n(5) Section 1.3: It might be better to also mention the current state-of-the-art results on landscape analysis:\n\nZhang, H., Bi, Y., & Lavaei, J. (2021). General low-rank matrix optimization: Geometric analysis and sharper bounds. Advances in Neural Information Processing Systems, 34, 27369-27380.\n\nBi, Y., Zhang, H., & Lavaei, J. (2022, June). Local and global linear convergence of general low-rank matrix recovery problems. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 36, No. 9, pp. 10129-10137).\n\n\n(6) Section 2: \"Asymmetric Matrix Sensing\"\n\n(7) Theorem 3.1: it seems that the \"ultimate error\" does not appear in Section 3.1.\n\n(8) Also, it might be better to mention that the over-parameterization size k depends on \\alpha and briefly explain what happens if the size k is smaller than this threshold.\n\n(9) In (3.3a), I think T should be T^{(0)}?\n\n(10) Below Theorem 3.1: For the inequality \\|X_tX_t^T - \\Sigma\\|_F^2 \\geq A_t / n, I wonder if it can be improved to \\|X_tX_t^T - \\Sigma\\|_F^2 \\geq A_t?\n\n(11) I wonder if there is a reason that initialization scales are chosen as \\alpha and \\alpha/3? Would it be possible to use, for example, \\alpha and \\alpha / 10 to achieve a better convergence rate?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3284/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3284/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3284/Reviewer_UZwv"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3284/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698727637994,
            "cdate": 1698727637994,
            "tmdate": 1700590531688,
            "mdate": 1700590531688,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8JC0PG6l4s",
                "forum": "xGvPKAiOhq",
                "replyto": "RaeHmkdmAZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3284/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3284/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Reviewer UZwv"
                    },
                    "comment": {
                        "value": "Thanks for the detailed response! Please find our response to your questions below.\n\n> 1. Theorem 1.1: it would be better to say that each entry of X is independently initialized with Gaussian random variable with variance \\alpha^2. A similar comment applies to other theorems.\n\nThanks for your reminder. We have updated our paper accordingly.\n\n> 2. In Section 1, I think the authors did not mention any requirements on the sample size m. It might be better to briefly mention the requirement on the sample complexity or the RIP constant in Section 1.\n\nBelow the Definition 2.1, we state that $m = \\widetilde{\\Omega}(nk/\\delta^2)$, where $\\delta$ is the RIP error parameter. Combining with the constraint of $\\delta$ in Equation 4.8, we can get $m = \\widetilde{\\Omega}(nk^2r)$ is needed, where we ignore the polynomial terms of $\\kappa$. We have added this argument in Sections 1 and 2.\n\n> 3. For the asymmetric case, I think most convergence results require a regularization term $\\|F^TF - G^TG\\|_F^2$ to penalize the imbalance between F and G. It would be better to mention the intuition why the regularization term is not required in this work.\n\n\nRecently, the paper [1] provides the global convergence result for the gradient descent without the regularized term. They mainly focus on the exact-parameterized case, while the over-parameterized case remains open. Some other recent works [2], [3] also provide the convergence guarantee of the gradient descent without the regularization term. However, their convergence is not exact, meaning they only deal with a finite number of iterations.\n\nThe intuition why the regularization term is not required is because the change of the  $||F^TF-G^TG||\\_F^2$ is actually $O(\\eta^2)$, i.e. $||(F_{t+1}^TF_{t+1} - G_{t+1}^TG_{t+1})-(F_t^TF_t-G_t^TG_t)|| = O(\\eta^2)$. Thus when $\\eta$ is small, we can prove that $F_t^TF_t-G_t^TG_t$ will not diverge. Hence, the two matrices $F$ and $G$ will not diverge and be too imbalanced.  \n\n> 4. mention the two current state-of-the-art results on landscape analysis\n\nThanks for pointing out these works. We have mentioned them in the related work section of our revised version.  These two works provide an analysis of the global geometry of general low-rank minimization problems via the Burer-Monterio factorization approach for the exact-parameterized case ($k=r$). The paper [1] obtains less conservative conditions for guaranteeing the non-existence of spurious second-order critical points and the strict saddle property, for both symmetric and asymmetric low-rank minimization problems.  The paper [2] analyzes the gradient descent for the symmetric case and asymmetric case with a regularized loss. They provide the local convergence result using PL inequality, and show the global convergence for the perturbed gradient descent. Compared to them, our paper tends to focus on the over-parameterized case and the vanilla gradient descent.  \n\n> 5. Theorem 3.1: it seems that the \"ultimate error\" does not appear in Section 3.1.\n\n\nIn the previous paper for the symmetric case [4], they prove that the gradient descent will linearly converge to a small error that is dependent on the initialization scale. We want to state that the \"ultimate error\" refers to this small error. Indeed, note that our lower bound in Equation (3.2) also has a $\\alpha^4$. So it is also consistent with the phenomenon that the gradient descent first quickly converges to a small error (which we call \"ultimate error\") that depends on the initialization scale, with a linear convergence rate. Theorem 3.1 characterizes the behavior of the gradient descent after this quick convergence: The convergence speed becomes $\\Omega(1/T^2)$ after the GD converges to a small error linearly.  \n\n\n\n\n> 6.  Also, it might be better to mention that the over-parameterization size k depends on $\\alpha$ and briefly explain what happens if the size k is smaller than this threshold.\n\nThe over-parameterization size $k$ is subject to a lower bound requirement in (3.1) that depends on $\\alpha$. However, since $\\alpha$ appears in the logarithmic term, this requirement is not overly restrictive. We posit that (3.1) may be a result of a proof artifact, and the lower bound of $\\Omega(1/T^2)$ still holds even when $k$ is smaller than this threshold."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3284/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700362525969,
                "cdate": 1700362525969,
                "tmdate": 1700362525969,
                "mdate": 1700362525969,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QOPfusBI7U",
                "forum": "xGvPKAiOhq",
                "replyto": "RaeHmkdmAZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3284/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3284/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Reviewer UZwv"
                    },
                    "comment": {
                        "value": "> 7. Below Theorem 3.1: For the inequality $\\|X_tX_t^T - \\Sigma\\|_F^2 \\geq A_t / n$, I wonder if it can be improved to $\\|X_tX_t^T - \\Sigma\\|_F^2 \\geq A_t$?\n\n We find that the result can be improved. We also need to mention that there is a typo in this paragraph: Equation (3.3) implies $A_t = O(\\alpha^2/t)$. Then since $(X_tX_t^T-\\Sigma)\\_{ii} = \\|x_i\\|^2$, we can have $||X_tX_t^T-\\Sigma||\\_F^2 \\ge \\sum_{i>r}(X_tX_t^T-\\Sigma)\\_{ii}^2 = \\sum_{i>r}\\|x_i\\|^4\\ge A_t^2/n,$ where the last inequality uses the Cauchy's inequality.  Thus the $n^{-2}$ in the final result (Equation (3.2)) can be improved to $n^{-1}$.\n However, We do not find a way to directly prove $\\|X_tX_t^T-\\Sigma\\|_F^2 \\ge A_t^2$. \n \n > 8.  I wonder if there is a reason that initialization scales are chosen as \\alpha and \\alpha/3? Would it be possible to use, for example, \\alpha and \\alpha / 10 to achieve a better convergence rate?\n\nThe selection of the more imbalanced initialization $\\alpha$ and $\\alpha/10$ can make two matrices more imbalanced, thus leading to a faster convergence rate after the gradient descent converges to a local point (Section D.4, D.5 and D.6). However, this might result in a slower convergence rate and impose more stringent requirements on the parameters (up to a constant) at the beginning of the proof, as outlined in Theorem 1 in [3], a pivotal component employed in Section D.3 as the foundation of the proof. To be more specific, a more imbalanced initialization may lead to a looser constant in Lemma B.7 and condition (23) in [3]. \n\n\nWe also want to highlight that a more imbalanced initialization also gives a guarantee $\\|\\Delta_0\\| = O(\\alpha^2)$ where $\\Delta_0 = F_0^TF_0 - G_0^TG_0$. Thus all the changes are only up to a constant. \n\n> 9. typos\n\nThanks for your reminder. We have fixed them in our revised version.\n\n\n*References:*\n\n[1]. Tian Ye, Simon S. Du.  Global convergence of gradient descent for asymmetric low-rank matrix factorization. NeurIPS 2021.\n\n[2]. Jiang et al. Algorithmic Regularization in Model-free Overparametrized Asymmetric Matrix Factorization. SIAM 2023.\n\n[3].  Soltanolkotabi et al. Implicit Balancing and Regularization:\nGeneralization and Convergence Guarantees for Overparameterized Asymmetric Matrix Sensing. COLT 2023.\n\n[4]. Stoger and Soltanolkotabi. Small random initialization is akin to spectral learning: Optimization and generalization guarantees for overparameterized low-rank matrix reconstruction. NeurIPS 2021."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3284/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700362620286,
                "cdate": 1700362620286,
                "tmdate": 1700362620286,
                "mdate": 1700362620286,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "f4U7fHnbwR",
                "forum": "xGvPKAiOhq",
                "replyto": "QOPfusBI7U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3284/Reviewer_UZwv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3284/Reviewer_UZwv"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors' rebuttal"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for their detailed response! I am happy to increase my score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3284/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700590544646,
                "cdate": 1700590544646,
                "tmdate": 1700590544646,
                "mdate": 1700590544646,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bG0kbuMo9b",
            "forum": "xGvPKAiOhq",
            "replyto": "xGvPKAiOhq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3284/Reviewer_Z4Yg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3284/Reviewer_Z4Yg"
            ],
            "content": {
                "summary": {
                    "value": "The reviewed paper is a theoretical investigation of the convergence properties of gradient-descent, and other first-order based methods, for over-parameterized matrix factorization/sensing for symmetric matrices. The specific focus is on the role of using **symmetric** versus **general** Burer-Monteiro factorization as parameterization and how it effects the convergence properties. The unexpected result is that the *symmetricity* versus *imbalance* plays a significant role.\n\nThe main \"positive\" result states that the over-parameterized gradient descent on $FG^T$ factorization is able to achieve linear convergence when the two components are imbalanced in the sense of the spectrum of $\\Delta = F^\\top F - G^\\top G$, and the specific convergence rate depends on this imbalance. The main \"negative\" result shows that there will always exist a positive measure of cases when symmetric parametrization $FF^\\top$ cannot have faster than sublinear convergence.\n\nThe work provides simple, but well explained numerical examples of small matrix sizes ($50 \\times 50, \\mathrm{rank} =3$) that clearly demonstrate this phenomenon.\n\nThe proofs take more than 30 pages in the appendices, they are technically involved and not easy to check in their entirety, but at first sight the result seems correct."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I believe this paper has several very strong points:\n* It presents a novel and surprising result\n* It gives rigorous proofs for the two main statements which together describe a very interesting behaviour\n* The numerical examples corroborate the proven theory\n* The paper is very clearly written, the structure and main message is clear (although the theorems themselves can be a bit complicated to interpret)\n* It gives a very good comparison with existing literature"
                },
                "weaknesses": {
                    "value": "There is not much that I would consider a weakness to this paper. That said, I would like to know, how much the results of the numerical experiments in terms of the neat convergence rate depend on a specific initialisation of the methods and whether these result would also occur for larger ranks and problem sizes."
                },
                "questions": {
                    "value": "1) In Fig 2 we see that for larger $alpha$ the convergence rate is faster. What is the limit of how large $\\alpha$ can be?\n2) Do the numerical results hold also for larger ranks of the true matrix and over-parameterized ranks? Also larger imbalance of ranks, lets say k = 20 and r = 5?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3284/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3284/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3284/Reviewer_Z4Yg"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3284/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698769415741,
            "cdate": 1698769415741,
            "tmdate": 1699636276878,
            "mdate": 1699636276878,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MiBdLdabzJ",
                "forum": "xGvPKAiOhq",
                "replyto": "bG0kbuMo9b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3284/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3284/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Reviewer Z4Yg"
                    },
                    "comment": {
                        "value": "Thanks for the positive review! We have added more experiments according to your suggestions. Please find our response to your questions below.\n> 1. In Fig 2 we see that for larger $\\alpha$ the convergence rate is faster. What is the limit of how large $\\alpha$ can be?\n\nIn practice, a large initialization may not be able to converge at the initialization phase. We run an experiment to show that a large $\\alpha$ fails to converge to a local point. The experiment result is in Appendix I.3\n\nOn the theoretical side, the initialization scale $\\alpha$ has a constraint in Equation (4.7). A large $\\alpha$ does not satisfy the constraint.\n\n\n> 2. Do the numerical results hold also for larger ranks of the true matrix and over-parameterized ranks? Also larger imbalance of ranks, lets say k = 20 and r = 5?\n\nSince we need a relatively small $k$ in Theorem 4.2 $(k<n/8)$, we only run an experiment using $k=10$ and $r=5$. The number of samples is $m=2000$. The experiment results are in Appendix I.4, which shows that a similar phenomenon also holds for a larger rank of the true matrix and a larger over-parameterized rank."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3284/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700362285590,
                "cdate": 1700362285590,
                "tmdate": 1700362302913,
                "mdate": 1700362302913,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "L645h39G1f",
                "forum": "xGvPKAiOhq",
                "replyto": "MiBdLdabzJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3284/Reviewer_Z4Yg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3284/Reviewer_Z4Yg"
                ],
                "content": {
                    "title": {
                        "value": "Response to the rebuttal"
                    },
                    "comment": {
                        "value": "I want to thank the authors for addressing all my questions and remarks. My positive score remains unchanged."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3284/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700591220861,
                "cdate": 1700591220861,
                "tmdate": 1700591220861,
                "mdate": 1700591220861,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xCTvIYW1fR",
            "forum": "xGvPKAiOhq",
            "replyto": "xGvPKAiOhq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3284/Reviewer_MU8J"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3284/Reviewer_MU8J"
            ],
            "content": {
                "summary": {
                    "value": "This paper provides several new results for over-parameterized matrix sensing. First, the authors rigorously prove that with a symmetric parameterization, over-parameterization slows down GD.  In particular, they give a lower bound rate of $\\Omega(1/T^2)$. Second, the authors also show that with an asymmetric parameterization, GD converges at an linear rate depending on the initialization scale. This is in contrast with GD with symmetric parameterization, which has a sublinear rate. Finally, the authors extend their algorithm so that the linear convergence rate is independent of the initialization scale."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Overall I think this is a good paper. The fact that over-parameterization slows down GD for matrix sensing has been observed by quite a few previous papers. However, this is the first paper that I'm aware of to rigorously establish a lower bound. The authors also show that with asymmetric parameterization, GD converges at an exponential rate that depends on the initialization scale. This is somewhat surprising, given that the asymmetric case has traditionally been considered harder due to potential imbalance of the factors."
                },
                "weaknesses": {
                    "value": "My main concern is with the experiments in this paper. I think the paper could benefit from a more thorough experimental section, perhaps in the appendix. \n\nIn the symmetric case, if we use GD with small initialization, then it is often the case that GD goes through an initialization phase where the loss is relatively flat, and then converges rapidly to a small error. However, in the experiments in Figure 2, I do not see this initialization phase in Figure 2b. Instead, linear convergence is observed right from the start, even when a small initialization is used. I wonder why is this the case? For the asymmetric case, is the initialization phase much faster?\n\nAdditional experiments which i think should be nice: on the same plot, compare the convergence of asymmetric versus symmetric parameterization, using the same initialization. Also perform the experiment for different initialization scales. I think the authors should also plot convergence for ill-conditioned versus well-conditioned matrices, as GD with small initialization performs differently based on the eigenvalues. \n\nIn any case, i would like to see a more detailed comparison of symmetric versus asymmetric parameterization, even just using synthetic experiments."
                },
                "questions": {
                    "value": "In Theorem 1.3, the convergence rate depends on the initialization scale $\\alpha$. This is also observed empirically in figure 2b. In practice, does this mean that small initialization has no advantage? One could just set $\\alpha$ to be large to ensure rapid convergence?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3284/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3284/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3284/Reviewer_MU8J"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3284/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698781411166,
            "cdate": 1698781411166,
            "tmdate": 1700600879703,
            "mdate": 1700600879703,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oH5WGDY202",
                "forum": "xGvPKAiOhq",
                "replyto": "xCTvIYW1fR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3284/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3284/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Reviewer MU8J"
                    },
                    "comment": {
                        "value": "Thank you for your review and for thinking our paper is good. We have updated our paper, including more experiments according to your suggestions. Please also find our response to your comments below.\n\n> 1. In the symmetric case, if we use GD with small initialization, then it is often the case that GD goes through an initialization phase where the loss is relatively flat, and then converges rapidly to a small error. However, in the experiments in Figure 2, I do not see this initialization phase in Figure 2b. Instead, linear convergence is observed right from the start, even when a small initialization is used. I wonder why is this the case? For the asymmetric case, is the initialization phase much faster?\n\nIn Figure 2b, our purpose is to show the exact linear convergence. Hence, we run the algorithm with a large number of iterations, 70000 iterations. Thus,  the initialization phase is not clear in Figure 2b. After zooming into the first 5000 iterations of Figure 2b, we find the existence of the initialization phase. That is, the loss is rather flat during this phase. We can also see that the initialization phase is longer when $\\alpha$ is smaller. The experiment results are shown in Appendix I.5.\n\n\n> 2. Additional experiments that I think should be nice: on the same plot, compare the convergence of asymmetric versus symmetric parameterization, using the same initialization. Also, perform the experiment for different initialization scales.\n\nWe add the detailed comparison in Appendix I.1. The comparison shows that the asymmetric parameterization performs better than the symmetric parameterization.\n\n> 3. I think the authors should also plot convergence for ill-conditioned versus well-conditioned matrices, as GD with small initialization performs differently based on the eigenvalues.\n\nWe plot the curve when $\\kappa = 1.5, 3, 10$ with minimal eigenvalue $0.66, 0.33, 0.1$. The experiment results are in Appendix I.2.\nFrom the experiment results, we can see two phenomena:\n* When the minimum eigenvalue is smaller, the gradient descent will converge to a smaller error at a linear rate. We call this phase as the local convergence phase.\n* After the local convergence phase, the curve first remains flat and then starts to converge at a linear rate again. We can see that the curve remains flat for a longer time when the matrix is ill-conditioned, i.e. $\\kappa$ is larger. \n\nThis phenomenon has been theoretically identified by the previous work for incremental learning [1,2], in which GD is shown to sequentially recover singular components of the ground truth from the largest singular value to the smallest singular value.\n\n\n\n> 4. In Theorem 1.3, the convergence rate depends on the initialization scale . This is also observed empirically in figure 2b. In practice, does this mean that small initialization has no advantage? One could just set to be large to ensure rapid convergence?\n\nIn practice, a large initialization may not be able to converge at the initialization phase. We run an experiment to show that a large $\\alpha$ fails to converge to a local point. The experiment result is in Appendix I.3\n\nOn the theoretical side, the initialization scale $\\alpha$ has a constraint in Equation (4.7). A large $\\alpha$ does not satisfy the constraint.\n\n*References:*\n\n[1] Jin et al. Understanding incremental learning of gradient descent: A fine-grained analysis of matrix sensing. ICML 2023.\n\n[2] Jiang et al. Algorithmic Regularization in Model-free Overparametrized Asymmetric Matrix Factorization. SIAM 2023."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3284/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700362227670,
                "cdate": 1700362227670,
                "tmdate": 1700362227670,
                "mdate": 1700362227670,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pCLDqVSoR7",
                "forum": "xGvPKAiOhq",
                "replyto": "4ZZexOmbsb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3284/Reviewer_MU8J"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3284/Reviewer_MU8J"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the additional experiments, which resolves my main concern.  I have increased my score."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3284/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700600854114,
                "cdate": 1700600854114,
                "tmdate": 1700600854114,
                "mdate": 1700600854114,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]