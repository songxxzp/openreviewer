[
    {
        "title": "Combinatorial Optimization via Memory Metropolis: Template Networks for Proposal Distributions in Simulated Annealing applied to Nanophotonic Inverse Design"
    },
    {
        "review": {
            "id": "qpzGlERIy7",
            "forum": "wDE3clrYWR",
            "replyto": "wDE3clrYWR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3402/Reviewer_9yfn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3402/Reviewer_9yfn"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes the Memory Metropolis (MeMe) algorithm, integrating neural networks with simulated annealing (SA) to optimize combinatorial problems on 2D binary grids. By leveraging a unique class of network architecture termed \"template networks,\" the method directs convergence towards states of structurally clustered patterns. This approach challenges conventional practices by intentionally violating the Markov property and is applied to nanophotonic inverse design, highlighting its potential in finding clustered design patterns."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The introduction of \"template networks\" and the Memory Metropolis approach presents a fresh perspective in the realm of optimization.\n\n* Combining elements from Markov Chain Monte Carlo optimization, neural networks, and reinforcement learning is interesting."
                },
                "weaknesses": {
                    "value": "* The technical contribution is not strong. The proposal is generally mired in complexity which may make it inaccessible for readers not deeply familiar with all the integrated disciplines. Intentionally violating the Markov property without substantial justification is concerning. Further evidence or theoretical underpinnings are needed to support this decision.\n\n* Rewriting for clarity can make the paper more accessible to a broader audience.\n\n* The method of reward maximization and the process of determining detrimental actions is not explained in depth.\n\n* Abstract is too lengthy\n\n* The conclusion should reiterate the major findings, their implications, and potential future work in a more detailed manner."
                },
                "questions": {
                    "value": "* A detailed side-by-side comparison with existing SA and regularized SA methodologies is required.\n\n* Delving deeper into the reasons for violating the Markov property and the potential implications can make the proposal more convincing.\n\n* Authors are suggested to discuss the broader applicability of the MeMe algorithm, beyond the specific case study presented.\n\n* The paper doesn't clarify whether the approach is generalizable outside of the specific domain it was applied to."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3402/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3402/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3402/Reviewer_9yfn"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3402/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698740065012,
            "cdate": 1698740065012,
            "tmdate": 1700735806475,
            "mdate": 1700735806475,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aNV4nMCEFS",
                "forum": "wDE3clrYWR",
                "replyto": "qpzGlERIy7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3402/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3402/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your feedback.\nWe updated the manuscript regarding abstract, methods and conclusion and by this made it more clear and accessible.\nBy this we incorporated your suggestions listed under _Weaknesses_, except (1) which we are answering as part of your questions:\n\n1. Since we do not only seek to maximize our devices fitness $f$, but also minimize the granularity $g$ we can not formulate a single objective which can be compared easily e.g. in a table, unfortunately.\n  Instead, we chose to depict both objectives in Fig. 4 for a direct side-by-side comparison.\n  In Fig. 4A SA as well as a pure policy gradient RL approach are given by the border cases of $T_S=\\infty$ and $T_M=\\infty$, respectively.\n  As indicated, regularized SA is a suitable alternative approach to MeMe, we thus include a comparison in Fig. 4B and further show, that MeMe outperforms regularized SA for any regularization factor $\\lambda$ in Sec. A.3.\n2. We agree, that the markov property should not be violated if one seeks to directly sample from the underlying distribution (here i.e., the Boltzmann distribution of fitness values).\n  However, we are only interest in finding a final state of high fitness while clustered structures are preferred.\n  Thus, violating the Markov property is not of big concern in our case which we state now more clearly in our text.\\\n  Furthermore, we added a detailed analysis of the consequences of the violation of the Markov property in Sec. A.15.\n  We there conclude, that the violation actually results in a new Markov process, which generates samples from the product of the Boltzmann distribution of fitness values and the bias term $B = \\exp{2Q/T_S}$ where Q is the learned template after the template matching operation (TMO).\n  We think this theoretical analysis substantially strengthen our motivation for violating the Markov property and hope this answers the question on the implications. \n3. / 4. While we focused on an in-depth analysis of the problem of nanophotonic inverse design here, many other applications exist where similar structures are sought.\n  This is especially true for engineering where optimization of structures for various objectives and under specific constraints exist.\n  However, while many of these applications exist, they are ofentimes rather specific and require a detailed problem introduction as we did for the nanophotonic inverse design."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3402/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700243243135,
                "cdate": 1700243243135,
                "tmdate": 1700245581812,
                "mdate": 1700245581812,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ktqVi3cyeI",
                "forum": "wDE3clrYWR",
                "replyto": "aNV4nMCEFS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3402/Reviewer_9yfn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3402/Reviewer_9yfn"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors' efforts in addressing my previous concerns. However, I still have reservations regarding the technical contribution of this work, particularly in light of the absence of sufficient baseline comparisons. It is essential to establish the novelty and effectiveness of the proposed approach by comparing it with more (advanced) existing methods, and the absence of such comparisons is a notable limitation. \n\nAdditionally and importantly (as far as I am concerned), the potential applicability of this work to a broader audience at ICLR raises concerns about its impact and relevance to the ICLR venue.\n\nGiven these remaining doubts about the overall contribution of the paper, I have decided to revise my score only to '5: marginally below the acceptance threshold.'"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3402/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736267070,
                "cdate": 1700736267070,
                "tmdate": 1700736267070,
                "mdate": 1700736267070,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iUHX813lfa",
            "forum": "wDE3clrYWR",
            "replyto": "wDE3clrYWR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3402/Reviewer_SEFL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3402/Reviewer_SEFL"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel MCMC algorithm Memory Metropolis (MeMe), to tackle the combinatorial optimization problem in the context nanophotonic inverse design. The problem involves finding specially constrained patterns on a binary grid, with applications in creating high-performance devices for nanophotonic integrated circuits. MeMe involves the use of a neural network to build transition proposal distributions in Simulated Annealing (SA). The key contribution is 'template networks', a new class of network architectures designed to learn a template for constructing a proposal distribution for state transitions. MeMe violates the Markovian property as it uses past states to craft transition proposals. The template network is trained on the evaluation results of intermediate states of a single optimization run, which results in an architecture that does not require an input layer. Additional inductive biases are incorporated in the form of layers with limited local connectivity, which encourages the emergence of structural clusters. This biases the target distribution towards cluster formation. MeMe is also linked to deep RL, where the optimization objective of the Metropolis algorithm is viewed as a reward maximization problem. The policy is constructed using the discrepancy between the template and the current state, allowing the template network to find high-reward template-patterns. MeMe is evaluated empirically via application to combinatorial optimization in nanophotonic inverse design where it demonstrates significant improvements over standard SA."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The paper studies the interesting problem appearing in the context of nanophotonic inverse design. The problem is described and formalized clearly, well motivated and presents a unique interesting challenge for machine learning approaches. This isn't the first instantiation of using sampling approaches for combinatorial optimization but is quite well executed. \n* MeMe leverages advances from deep learning in the form of the template networks to craft effective proposal distributions within simulated annealing to model the biased target distribution to get high scoring candidates. \n* The experiments on the nanophotonic design task is described in ample detail and thoroughly analysed."
                },
                "weaknesses": {
                    "value": "* A major weakness in my opinion is that it is unclear how much of the method is generally applicable to other problem settings. It appears that the design of the template networks requires quite a bit careful engineering and domain knowledge and can be potentially challenging on other tasks. The paper's narrow focus on a specific application also makes it somewhat poorly positioned for the audience at ICLR, even though the domain is introduced appropriately. I encourage the authors to consider alternative venues where the particular application is a focus. \n* Another major shortcoming is the lack of baselines - the authors only compare the apporach to simulated annealing but it would be good to have other baselines for instance some standard RL methods like PPO."
                },
                "questions": {
                    "value": "* Can you provide more details on the computational cost of MeMe? How does it scale with the problem size?\n* There is a potential issue of overfitting in the training of the template network? If so, how is it addressed?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3402/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698901841601,
            "cdate": 1698901841601,
            "tmdate": 1699636291317,
            "mdate": 1699636291317,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jACXWuIKNO",
                "forum": "wDE3clrYWR",
                "replyto": "iUHX813lfa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3402/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3402/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "***Weaknesses***\n1. We agree that our application is focused on nanophotonic inverse design.\n  However, while other applications exist, they are oftentimes rather specific and require a detailed problem introduction as we did for the nanophotonic inverse design in our work.\\\n  Furthermore, we think our proposed algorithm might be of interest to the ICLR audience from a conceptional perspective, which can only hardly be adresses in a more application focused venue.\n  The concept of input-less template networks is, to the best of our knowledge, new to deep RL in general and offers new perspectives especially for optimization applications. \n2. We agree that comparisons to additional algorithms would be interesting, however, to the best of our knowledge there are no promising alternative algorithms for the problem we studied in our work.\n  When analyzing the effect of the sampling temperature $T_S$ and the metropolis temperature $T_M$ in detail, multiple baseline algorithms emerge in their borderline cases.\n  Next to SA and greedy-exploration, this also includes a direct policy gradient apporach if $T_M=\\infty$.\n  Furthermore, we also use loss clipping, which is a key part of PPO.\n  As training our template networks differs from classical deep RL in many aspects, we many components which where developed for other RL agents like PPO, are missing motivation for our proposed algorithm.\n  This includes value functions, lower discounts, rollouts of multiple samples, multiple policy update steps (as in PPO objective maximization).\n  Instead, we seek to update the learned template as direct and fast as possible with newly encountered feedback from the environment.\n  We thus included features of e.g. PPO which are transferable to our application while think that more sophisticated RL methods are not well motivated as a baseline for our application.\n\n\n***Questions***\n1. The computational cost of our algorithm is dominated by the FDFD-simulations.\n  So an empirical evaluation would only observe the scaling of these (which scales quadratic with the system size).\n  E.g. for our experiments in Fig. 4 computations took $\\approx 20$h on 26 Intel Xeon Gold 6140 2.30 GHz CPU cores per optimization.\\\n  Since updates of the template network are only propagated to a limited local field of view, training as well as updates of the proposal distribution are scaling linear with the number of pixels (instead of quadratic as expected for e.g. FC layers).\n2. Indeed, we intend to fully fit our networks to single optimization runs as a key concept of template networks.\n  This distinguises template networks from most other architectures used in deep RL.\n  In this regard out networks share a lot with e.g. NeRFs which are also fitted to single scenes.\n  Thus overfitting is not of concern."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3402/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700243102882,
                "cdate": 1700243102882,
                "tmdate": 1700243102882,
                "mdate": 1700243102882,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oHPlJGWEWz",
                "forum": "wDE3clrYWR",
                "replyto": "jACXWuIKNO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3402/Reviewer_SEFL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3402/Reviewer_SEFL"
                ],
                "content": {
                    "title": {
                        "value": "Response to Rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the response and clarification! \n\n> The concept of input-less template networks is, to the best of our knowledge, new to deep RL in general and offers new perspectives especially for optimization applications.\n\nIt would be very helpful for me if the authors could discuss some other potential applications where the template networks could be applicable. That remains the primary concern I have about the paper."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3402/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700596300706,
                "cdate": 1700596300706,
                "tmdate": 1700596300706,
                "mdate": 1700596300706,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NhB97vaZCC",
            "forum": "wDE3clrYWR",
            "replyto": "wDE3clrYWR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3402/Reviewer_UWqi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3402/Reviewer_UWqi"
            ],
            "content": {
                "summary": {
                    "value": "The paper suggests a combination of neural network-based approach and deep RL to the problem of combinatorial optimization with the simulated annealing algorithm. The proposed algorithm utilizes the RL approach to construct the proposal particles in the modification of Metropolis-Hastings scheme. The authors also provide a results on physical simulations demonstrating the efficiency of their approach compared to the vanilla simulated annealing scheme."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The topic of combining RL with discrete optimization is challenging, and the experimental results of the submission are spectacular, especially in the term of quite large problem dimension."
                },
                "weaknesses": {
                    "value": "The relation of the proposed algorithm to the RL setting is not clearly explained in the current submission. Current submission lacks the detailed MDP description with the tuple of state space, action space, and reward, and the reward description for the particular optimization problem. The writing of section 3, and especially section 3.1 is hard to follow. The choice of extremely discounted RL problem (with $\\gamma = 0$) is also rather questionable for an empirical paper, and requires additional experimental verification. Moreover, there already were papers, e.g. [Beloborodov et al, 2020], [Mills et al, 2020], which already provided a framework for treating SA as an MDP and applied RL for solving it. That is why, I suggest the authors to better indicate the novelty of their approach.\n\n[Beloborodov et al, 2020] Beloborodov, D., Ulanov, A. E., Foerster, J. N., Whiteson, S., & Lvovsky, A. I. (2020). Reinforcement learning enhanced quantum-inspired algorithm for combinatorial optimization. Machine Learning: Science and Technology, 2(2), 025009.\n[Mills et al, 2020] Mills, Kyle, Pooya Ronagh, and Isaac Tamblyn. \"Finding the ground state of spin Hamiltonians with reinforcement learning.\" Nature Machine Intelligence 2.9 (2020): 509-517."
                },
                "questions": {
                    "value": "I would suggest the authors to add more structure to the current version of section 3, adding more details on how the considered problem falls into the RL formalism. \n\nMoreover, I would like the authors to elaborate the novelty of their suggested algorithm. For example, RL approach to simulated annealing  was recently considered, e.g. in [Correia et al, 2023], and references therein. Thus I would suggest the authors to better highlight the novelty of their approach compared to the ones discussed in the previous papers.\n\nReferences:\n[Correia et al, 2023] Correia, Alvaro HC, Daniel E. Worrall, and Roberto Bondesan. \"Neural simulated annealing.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2023. \n[Beloborodov et al, 2020] Beloborodov, D., Ulanov, A. E., Foerster, J. N., Whiteson, S., & Lvovsky, A. I. (2020). Reinforcement learning enhanced quantum-inspired algorithm for combinatorial optimization. Machine Learning: Science and Technology, 2(2), 025009.\n[Mills et al, 2020] Mills, Kyle, Pooya Ronagh, and Isaac Tamblyn. \"Finding the ground state of spin Hamiltonians with reinforcement learning.\" Nature Machine Intelligence 2.9 (2020): 509-517."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3402/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699452438782,
            "cdate": 1699452438782,
            "tmdate": 1699636291258,
            "mdate": 1699636291258,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9Q0HKazoOu",
                "forum": "wDE3clrYWR",
                "replyto": "NhB97vaZCC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3402/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3402/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your feedback and suggesting the additional references.\\\nWe added the references to our related work.\nCorreia et al. indeed propose a similar approach by combining RL and SA to generate new state proposals.\nHowever, they do not use the learned proposal distribution to deliberately bias the optimization to converge to a constrained state.\nActually they also do not apply a sampling bias correction (as discussed in Sec. A.9) resulting in uncontrolled biases of converged states, which might offer an explanation for their comparably bad results to SOTA algorithms.\nFurthermore, they rely on hand-crafted features as inputs to FC layers to parameterize the proposal distribution, where we propose input-less template networks, which is a central contribution of our work which we now indicate more directly in the main text.\nWhile we show that a simple policy gradient algorithm can be used to effectively train the template network, Correia et al. have to fall back to less interpretable evolution strategies to train their fully connected networks to reach sufficient results in many of their experiments.\\\nWe restructured and updated section 3, including a precise definition of the MDP accordingly, and added Sec. A.16 where we summarize our notation, thanks for the helpful suggestions."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3402/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700242979129,
                "cdate": 1700242979129,
                "tmdate": 1700242979129,
                "mdate": 1700242979129,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]