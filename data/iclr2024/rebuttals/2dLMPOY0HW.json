[
    {
        "title": "When Do MLPs Excel in Node Classification? An Information-Theoretic Perspective"
    },
    {
        "review": {
            "id": "t6PueZbff8",
            "forum": "2dLMPOY0HW",
            "replyto": "2dLMPOY0HW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6120/Reviewer_zabj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6120/Reviewer_zabj"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of node classification given node features and the graph structure information. The author propose a crude estimate on the extent to which node features cover graph structure information. Based on the idea of maximizing the mutual information between node embeddings and the graph structure, the authors introduce a novel regularization-based MLP model termed InfoMLP. InfoMLP consists of a preprocessing step and a learning step. Only the preprocessing step requires utilizing the graph structure information. Once the raw node features are preprocessed, InfoMLP has the same efficiency as a standard MLP during both the training and testing phases. Empirically, the proposed method demonstrates competitive performance over a few small benchmark datasets, while being much more efficient than existing graph neural networks."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Efficiency is a major problem for the application of conventional GNNs in the industry, where practitioners often have to deal with graphs of massive size. This paper proposes an alternative and efficient method for node classification while utilizing both the raw node features and the graph structure.\n\n- The idea to construct node embeddings during a preprocessing stage by explicitly maximizing the mutual information between node embeddings and the graph structure information is interesting, although not surprising. I liked this perspective.\n\n- The empirical results over the selected benchmark datasets are promising, as shown throughout Section 4 in the paper.  (The additional results in the appendix, however, are on the pessimistic side, more on this later.)"
                },
                "weaknesses": {
                    "value": "- A number of claims in Section 3.1 are not well justified by either theoretical or empirical evidence. Here are some examples:\n  - At the end of page 3, the authors claim that \"We hypothesize that this is due to the high degree of overlap between the information conveyed by node features X and the graph structure A\", and then proceed with an \"analysis\" using the cartoons in Figure 1. First of all, I do not think that the hypothesis is correct in general. As a very simple example, consider a balanced stochastic block model with 2 equal-sized classes, inter-class edge probability q=0 and intra-class edge probability p=1. That is, consider a graph that consists of two complete subgraphs, with one subgraph corresponds to class 0 and the other subgraph corresponds to class 1. Furthermore, assign node IDs 1,2,...,N to the nodes uniformly at random, and let the node features be the one-hot encodings of node IDs. This makes I(X;A) = 0 since X and A are independent. However, it should be clear to see that, even in this case, a strong Laplacian regularization would make an MLP to perform nearly as good as, e.g., GCN. In my example above I am omitting details regarding the representation (which concerns with the ability to approximate/represent certain functions)  and generalization (which concerns with sample complexity) capabilities of MLP and GCN. I welcome the authors' feedback on this example. In any case, I would recommend the authors be careful about stating a hypothesis, in particular, when certain assumptions are required for a claim to be true.\n  - I found that the authors were very vague in their discussion about comparing MLPs with GNNs. For example, in the second paragraph on page 4, when discussing Figure 1(b), the authors claim that \"Thus, MLPs are expected to have the same capacity as GNNs in this context\". The term capacity is a very vague term, here, do the authors refer to representational power, or do the authors refer to generalization capability? Note that when an MLP has a worse test accuracy than a GNN, it could be due to either expressiveness or generalization or both. Overall, I really could not appreciate the discussion in Section 3.1. I am not convinced by the authors' conclusion that the performance gap between MLP and GNN is determined by the mutual information. I would recommend the authors constrain the problem context by explicitly stating the required assumptions and focus on specific data generative models. Otherwise, the current claims feel very unsupported.\n  - As another example, at the top of page 4, the authors claim that \"..., a well-trained MLP model might implicitly derive an estimated graph structure matrix from the outset. It could then implicitly leverage both the estimated graph structure and the raw node features to generate node embeddings, thereby achieving performance similar to that of GNNs.\" This claim is neither theoretically justified nor empirically verified in this paper. Even in the extreme case where one considers augmented feature matrix X' = [X; A], so that the graph structure is completely covered by X', it is unclear if an MLP acting on X' can achieve similar performance to that of GNNs.\n  - To be honest, I think the paper might be better if the authors simply delete Section 3.1. Alternatively, the authors should be very careful in the writing. Clearly state all necessary assumptions on the data/model/architecture, clearly define and use proper technical terms, and avoid using vague terms. Otherwise, I find it very hard to be convinced by the messages which the authors try to convey in Section 3.1.\n\n- There is a clear gap between $H(A|X)$ and $H(A|\\hat{A})$ in equation 1, and the paper does not provide a bound on $|H(A|X)-H(A|\\hat{A})|$. Therefore, it is difficult to determine how useful the proposed metric $H(A|\\hat{A})$ is. The authors should at least discuss the reason to choose the $\\ell_2$ distance for $\\hat{A} = f(X)$.\n\n- Although the empirical results over small datasets in Section 4 look promising, the additional results over larger datasets in the appendix are not very good. Since one of the major advantages of MLPs over GNNs is efficiency, the performance of InfoMLP over larger datasets is more relevant and important. I think the authors should provide results on larger datasets in the main text, and place the results over small datasets in the appendix. However, over larger datasets, InfoMLP is outperformed by simple GNN architectures such as GCN or even SGC which does not suffer from efficiency issues."
                },
                "questions": {
                    "value": "- In Theorem 1, why do we necessarily have that $H(A|X) = \\inf_f H(A|\\hat{A})$ (i.e. equality holds)? Based on the proof I think it should be $H(A|X) \\le \\inf_f H(A|\\hat{A})$?\n\n- For training and testing time of SGC reported in Table 5: I think that once preprocessing is done, SGC is basically a liner logistic regression. Why does it have slower training and testing time than InfoMLP?\n\n- At the end of page 6, the authors claim that \"Furthermore, the node embeddings $Z_{aug}$ are utilized to predict the node labels.\" Based on the description of the method, shouldn't $Z_{mlp}$ be utilized to predict the node labels? In any case, I think the authors should report the performance achieved by both $Z_{aug}$ and $Z_{mlp}$ for comparison purposes."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6120/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698241549627,
            "cdate": 1698241549627,
            "tmdate": 1699636662347,
            "mdate": 1699636662347,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nok4tUlKKF",
                "forum": "2dLMPOY0HW",
                "replyto": "t6PueZbff8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6120/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6120/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer zabj  (1/3)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful comments and constructive suggestions. Below are our answers to the weaknesses and questions.\n\n> W1: Claims in Section 3.1 are not well justified.\n\n**The data generation process behind our assumption.**\n\nThe conclusion in Section 3.1 is indeed based on an assumption of graph-structured data generation. Specifically, our hypothesis posits that for a graph $G = (X, A)$, its generation process involves generating raw node features X for the nodes, and the graph structure A is generated based on the node features and additional confounder factors. This assumption is adopted by the research in the study of generative models for graphs [1]. In addition, many real-world graphs can be considered to be generated based on this assumption. For example,\n- **Citation networks**: Consider citation networks where the node features are the raw text of papers, and the edges represent citation relationships. In this case, we can hypothesize that the textual content of papers is generated first. Then, based on the content of the papers and authors' actions, citation relationships are formed.\n- **Social networks**: Consider social networks where the node features are user profiles, including attributes like occupation and interests, and the edges represent friendship relationships. We posit that users are initially generated with their individual attributes. Subsequently, friendship relationships are formed among users based on shared interests, real-world interactions, and other factors.\n\nTherefore, our assumption regarding the generation process of graph data is reasonable. Based on this assumption, we provide the analysis in Section 3.1. We will provide a detailed explanation of this assumption in the revised paper.\n\n**The balanced stochastic block model.**\n\nThe first example provided by the reviewer, on the one hand, does not align with the generation process we consider for graph-structured data. On the other hand, this synthetic dataset is too specific and may not represent real-world graph datasets. Furthermore, **in the examples provided by the reviewer, it cannot be demonstrated that MLPs can achieve the same performance as GNNs**. The reviewer seems to consider only the transductive setting, where all the nodes (including the testing nodes) are involved in the training process. However, in the inductive setting, once testing nodes are not involved in the training process, for a new testing node with features represented as one-hot IDs, the MLP model clearly cannot make correct predictions, while the GNN model can easily classify it based on which block it belongs to\n\n**We are comparing the generalization ability of MLP and GNN.**\n\nRegarding the second point, we are indeed considering the generalization ability of MLP, and we appreciate the reviewer's input. In fact, the expressive power of MLP can hardly be enhanced because its input information and MLP's structure are fixed. The reason why GNN performs so well in semi-supervised learning is largely due to its utilization of graph structural information to enhance its generalization ability to non-training data, as mentioned in recent literature [2]. \n\n**The performance of $X' = [X, A]$ under MLP classifiers.**\n\nThanks for the suggestions. We agree with the reviewer that our claim needs to be empirically verified. Therefore, we follow the reviewer's suggestions and try learning an MLP classifier over the augmented node features $X' = [X, A]$. In addition, we can consider other simpler examples. On the one hand, SGC can be seen as an MLP model that takes the convoluted node features as input, and in this case, SGC has achieved performance similar to GNN. On the other hand, we can also use node structural embeddings (e.g., derived from DeepWalk) as an additional input for the MLP. In the table below, we provide the performance using the reviewer's example $X' = [X, A]$, as well as the performance with structural embeddings as input for the MLP model:\n\n| Model                 | Cora | Citeseer | Pubmed | Arxiv |\n|----------|------------|--------------|---------|---------\n|   $X' = [X, A]$    |       $81.5\\pm0.4$   |  $71.9\\pm0.3$  | $79.9\\pm0.4$  | OOM   |\n|  Structural embedding    |   $82.2\\pm0.5$    | $71.7\\pm0.4$  | $80.5\\pm0.5$  | $70.28\\pm0.19$   |\n\nAs presented in the table, various approaches that use structural information as input for MLPs can enable them to achieve performance comparable to GNNs.\n\nWe appreciate the constructive criticism provided by the reviewer regarding Section 3.1 of our paper. The feedback has allowed us to better introduce the background and implications of our research. We hope that our response addresses your concerns.\n\nReferences:\n\n[1] Ma, Jiaqi, et al. \"A flexible generative framework for graph-based semi-supervised learning.\" NIPS 2019.\n\n[2] Yang, Chenxiao, et al. \"Graph Neural Networks are Inherently Good Generalizers: Insights by Bridging GNNs and MLPs.\" ICLR. 2023."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6120/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700649045960,
                "cdate": 1700649045960,
                "tmdate": 1700649353101,
                "mdate": 1700649353101,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PTLLxfWlA6",
                "forum": "2dLMPOY0HW",
                "replyto": "t6PueZbff8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6120/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6120/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer zabj  (2/3)"
                    },
                    "comment": {
                        "value": "> W2: Gap between $H(A|X)$ and $H(A|\\hat{A})$\n\nThe gap between $H(A|X)$ and $H(A|\\hat{A})$ is determined by how the function $\\hat{A} = g(X)$ is formulated. Since $|H(A|X) - H(A|\\hat{A})| = I(X;A|\\hat{A}) = I(X;A) - I(X;A;\\hat{A}) = {\\rm const} - I(A;\\hat{A})$. We cannot provide a specific bound because the constant value $I(X;A)$ is non-tractable. Luckily, since we do not care about the absolute value between the gaps, we can resort to the mutual information between the original graph adjacency matrix and the estimated graph structure matrix $\\hat{A}$, i.e., $I(A;\\hat{A})$ for reference. To be detailed, to make this bound tighter, we only need the estimated adjacency matrix to be very close to the true adjacency matrix.\n\nWe choose the $\\ell_2$ distance primarily motivated by the well-known graph homophily theory, which suggests that nodes with similar features are more likely to be connected than nodes with dissimilar features. Therefore, the $\\ell_2$ distance between two nodes based on their features can, to some extent, be used to infer whether two nodes are connected.\n\nThe greatest advantage of this approach is that $g(X)$ is non-parametric, so it can be used as a plug-and-play method for any dataset without the need to train a separate model. Even with such a simple formulation, it is already sufficient to capture the graph structure information and the correlation between node features in graph datasets. It can also explain why MLP-typed models exhibit drastically different performance on different graph datasets, as presented in Table 2 and Figure 2. For instance, on datasets with smaller $H(A|\\hat{A})$ (e.g., Cora, Citeseer, Pubmed, and Coauthor-CS), the performance of MLP-typed models is evidently closer to powerful GNN models than datasets of larger $H(A|\\hat{A})$ (e.g., Amazon-Computer).\n\n> W3: Empirical results on large graphs are not satisfying\n\nThank you for carefully checking our results on the large graphs. Our response is divided into two parts: 1) The suboptimal performance of InfoMLP on some large-scale graphs is due to the significant information gap between node features $X$ and the graph adjacency matrix $A$. 2) MLP-typed methods have an unparalleled advantage over GNNs in cold-start scenarios.\n\n**The suboptimal performance of InfoMLP on some large-scale graphs is due to the significant information gap between $X$ and $A$.**\n\nCritically, the suboptimal performance of InfoMLP is not caused by the large graph size but by the information gap between node features and graph structures, which is endemic to the quality of data. At the end of Section 3.2, we have concluded three representative cases of the scale of $H(A|\\hat{A})$. The third case (large $H(A|\\hat{A})$) has pointed out that when the node features $X$ fail to contain adequate information about the graph structure $A$, the performance of the MLP model can hardly be improved to the level of GNNs. This is the common case for large-scale graphs since $N\\times N \\gg N \\times d$ when $N$ is large while $d$ is small. For example, Arxiv is a citation network with over 160k nodes, while the node attributes consist of 128-dimensional encoded features from raw textual features using skip-gram. The resulting information gap endemic to the data hampers the ability to train an MLP model with enough expressive power comparable to GNNs.\n\nThe empirical results in Figure 5, Appendix E.4, in the original paper also support our arguments. As depicted in Figure 5, the pos/neg distributions are heavily overlapped, leading to large conditional entropy $H(A|\\hat{A})$. It is precisely because the node features $X$ do not contain enough information to recover the graph structure $A$ that MLP-typed methods fail to compete with GNNs\n\n**MLP-typed methods have an unparalleled advantage over GNNs in cold-start scenarios.**\n\nAs explained in the second paragraph of Sec. 1, our study is motivated by addressing the two-fold limitations of conventional GNNs: 1) the efficiency issues and 2) the challenges in achieving good performance in cold-start scenarios. While it is true that shallow GNNs can partially alleviate the first issue, they are less preferable than MLPs in cold-start scenarios. We conducted additional experiments on Reddit and Arxiv datasets for cold-start scenarios and presented the results in the table below.\n\n|  Cold-start | Reddit | Arxiv |\n| ------- | ------- | ------- |\n| GCN | $59.7\\pm1.2$  | $56.7\\pm0.5$ | \n| InfoMLP | $66.3\\pm1.4$ | $58.2\\pm0.4$ | \n\nCompared to the results in the transductive scenario, GCN experiences a significant performance drop in the cold-start scenario, falling far behind our InfoMLP. So, we believe that even though our proposed InfoMLP may not be as competitive as GNNs in some datasets in the transductive scenario, its advantages in the cold-start scenario demonstrate its value."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6120/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700649157014,
                "cdate": 1700649157014,
                "tmdate": 1700649157014,
                "mdate": 1700649157014,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Z6J1Fo4Dnc",
                "forum": "2dLMPOY0HW",
                "replyto": "t6PueZbff8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6120/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6120/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer zabj (3/3)"
                    },
                    "comment": {
                        "value": "> Q1: Theorem 1: Why the equality can hold.\n\nIf $\\hat{A} = f(X)$ is a fully free function, the equality can be obtained when $H(X|\\hat{A}) = 0$, since we can simply let $\\hat{A} = X$ (when $N = d$) or let $\\hat{A} = {\\rm concat}(X, 0)$ (when $N > d$). We understand the reviewer's concern might be that the above formulation of $\\hat{A}$ is very special and unusual, while we think $H(A|X) = \\inf\\limits_{f}H(A|\\hat{A})$ is more accurate. (The equation suggested by the reviewer is correct as well)\n\n> Q2: Testing time of SGC\n\nThe training/testing time of SGC includes the graph convolution step and, therefore, is larger than other MLPs. If we view the graph convolution as a preprocessing step (which is reasonable), the training/testing time will be the same as MLPs. We've revised it in the updated paper, and thanks a lot for pointing it out.\n\n\n> Q3: Comparison of the performance of $Z_{\\rm aug}$ and $Z_{\\rm mlp}$\n\nThank you for the valuable comments. The sentence you pointed out is a typo, and has been revised to be \"Furthermore, the node embeddings $Z_{\\rm mlp}$ are utilized to predict the node labels\". \nIn fact, $Z_{\\rm aug}$ cannot be directly compared to $Z_{\\rm mlp}$ because in our model, the cross-entropy loss for downstream classification tasks is applied to $Z_{\\rm mlp}$ rather than $Z_{\\rm aug}$. To meet the reviewer's request, we independently trained a classification model based on $Z_{\\rm aug}$. In the table below, we provide the performance obtained with $Z_{\\rm aug}$ in the transductive setting and compare it to $Z_{\\rm mlp}$.\n\n|  Embedding | Cora  | Citeseer | Pubmed |\n| ------- | ------- | ------- |  ------- |\n| $Z_{\\rm aug}$ | $81.9\\pm0.5$  | $72.5\\pm0.5$ |  $80.2\\pm0.4$ | \n| $Z_{\\rm mlp}$ | $83.8\\pm0.3$ | $73.7\\pm0.3$ | $83.2\\pm0.9$ | \n\nAs we can see, $Z_{\\rm aug}$ generally performs worse on these datasets compared to $Z_{\\rm mlp}$. We suspect that this is because the regularization loss employed by InfoMLP not only helps $Z_{\\rm mlp}$ learn graph structural information but also includes a decorrelation loss, which encourages different dimensions of the embedding to be relatively independent. This, in turn, helps $Z_{\\rm mlp}$ to obtain an embedding space that is more distinguishable and beneficial for classification.\n\nFurthermore, since $Z_{\\rm aug}$ itself utilizes graph structural information as an input to the encoder, it is natural that it can achieve good performance when node features alone do not contain sufficient graph structural information. For example, on the Arxiv dataset, $Z_{\\rm aug}$ effortlessly achieved a classification accuracy of $70.82\\pm 0.16$."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6120/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700649291059,
                "cdate": 1700649291059,
                "tmdate": 1700649291059,
                "mdate": 1700649291059,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BkFembsG0H",
                "forum": "2dLMPOY0HW",
                "replyto": "Z6J1Fo4Dnc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6120/Reviewer_zabj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6120/Reviewer_zabj"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for providing a detailed response and carrying out additional experiments. However my biggest concern with respect to the analyses in this paper remains. Here are some suggestions:\n- The authors should clearly state all the required assumptions with respect to the data generation process and the learning setting. For example, indicate that the paper assumes homophily in both graph structure and node features, that is, nodes from the same class not only have similar features as measured by Euclidean distance, but also are more likely connected with one another. In addition, indicate if the paper focuses on the transductive setting or the inductive setting or both. These contexts are not very clear in the current paper. It's difficult to judge the soundness of the claims without a clear context. For example, MLP can be better than a simple GCN if the node features are strongly homophilous but the graph structure is complete noise.\n- Based on the author's response, the equality $H(A|X) = \\inf_f H(A|\\hat{A})$ does not hold in general. Please use $\\le$ instead. The authors should discuss the gap between  $H(A|X)$ and $\\inf_f H(A|\\hat{A})$, or better, provide an upper bound on $H(A|\\hat{A})-H(A|X)$ when $f$ is the $\\ell_2$ metric."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6120/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666593138,
                "cdate": 1700666593138,
                "tmdate": 1700666593138,
                "mdate": 1700666593138,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FBuHzgdDRH",
            "forum": "2dLMPOY0HW",
            "replyto": "2dLMPOY0HW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6120/Reviewer_YPT6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6120/Reviewer_YPT6"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the question of how to incorporate structural information to MLPs. \nIn other words, the latent question is: How structured must be MLPs? \nQuoting the authors: \"In this work, we first aim to understand the reasons behind the successes \nof previous MLP-structured models for learning node representations\". \nThe main idea is to quantify the overlap between the node features and graph structures (it is \nwell known that MLPs perform better than GNNs when the node feaures are uncorrelated with \nthe graph structure/adjacency). As a result, the authors propose to maximize the mutual information \nbetween node embeddings and adjacency as a pre-processing step. \n\nInfoMLP, the proposed method, works as follows: \"1) the generation of a graph-augmented node\nfeature matrix that encapsulates extensive graph structure information, and 2) the maximization of\nmutual information between node representations learned from the original node features and the\ngenerated graph-augmented node features.\". \n\nConditional probability between existing edges and feature correlations feeds an entropy estimator \nthat proves to illustrate the overlap between feature distribution and structure (links) in well-known \ndatasets. This leads to an upper bound of the H(A|X): conditional entropy of A given X.  \n\nThen, 1) is addressed in order to facilitate information maximization. This leads to $X_{aug}$ as a \nfunction of powers of the transition matrix. 2) is addressed through standard MI maximization. \nHowever, for the sake of efficiency, the MI Loss during training is simply an Euclidean loss. Overall, \n\"InfoMLP has the same complexity as a vanilla MLP in both training and testing\".\n\nExperiments show that InfoMLP is good in cold-start settings and outperforms significantly \nthe state-of-the-art only in Pubmed. It fails in Cora."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* Nice methodology for comparing/integrating MLPs-GNNs.\n* Good analysis of mutual information."
                },
                "weaknesses": {
                    "value": "* Not good results in standard datasets (e.g. Cora). \n* This is probably due to the terse definition of the augmented features. Multi-hop diffusion seems to me not sufficient even when $K$ is adapted properly."
                },
                "questions": {
                    "value": "* How critical is the augmented representation? \n* How critical is the MI simplified Loss? (please see R\u00e9nyi estimators). \n* Entropy estimation is very terse (positive vs negative distribution). Please, measure the overlap between positive and negative distribution using the Chernoff bound, for instance."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6120/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698749615590,
            "cdate": 1698749615590,
            "tmdate": 1699636662196,
            "mdate": 1699636662196,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lkLDvaNAjA",
                "forum": "2dLMPOY0HW",
                "replyto": "FBuHzgdDRH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6120/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6120/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer YPT6 (1/3)"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer's insightful comments and suggestions for improvement. However, we also noticed that some of the questions might arise from a potential misunderstanding of the setting studied by our paper. We address these points in the following answers.\n\n> W1 & W2: Not good results in standard datasets (e.g. Cora).\n\nWe'd like to first clarify upfront the three different experimental settings we considered, where in each case, the models should be compared in different manners given their different input information.\n\n> 1) In the ***transductive setting***, all nodes and edges are observed during both the training and testing phases.\n> 2) In the ***inductive setting***, testing nodes and their related edges are unobserved during training but are accessible when testing.\n> 3) ***Cold-start setting*** is an extreme case of the inductive setting, wherein the edges related to the testing nodes are inaccessible during testing as well. (We call it cold-start setting since this is similar to the cold-start recommendation problem where the historical interactions are inaccessible for new users)\n\nThese different settings pose different levels of learning difficulties for GNN-based and MLP-based approaches since both model classes use different information as input. Let $X_{train}$ and $X_{test}$ be the training/testing node attributes, and let $E_{train}$ and $E_{test}$ be the edges related to the training and testing do, respectively. The following tables compare the observed information for two model classes.\n\n|  |Transductive GNN  | Transductive MLP  |\n| ------- | ------- | ------- |\n| Train | $X_{\\rm train}, X_{\\rm test}, E_{\\rm train}, E_{\\rm test}$ | $X_{\\rm train}, X_{\\rm test}, E_{\\rm train}, E_{\\rm test}$ |\n| Test | $X_{\\rm train}, X_{\\rm test}, E_{\\rm train}, E_{\\rm test}$ | $X_{\\rm train}, X_{\\rm test}, E_{\\rm train}, E_{\\rm test}$ |\n\n|  |Inductive GNN  | Inductive MLP  |\n| ------- | ------- | ------- |\n| Train | $X_{\\rm train}, E_{\\rm train}$ | $X_{\\rm train},  E_{\\rm train}$ |\n| Test | $X_{\\rm train}, X_{\\rm test}, E_{\\rm train}, E_{\\rm test}$ | $X_{\\rm train}, X_{\\rm test}, E_{\\rm train}$ |\n\n|  |Cold-start GNN  | Cold-start MLP  |\n| ------- | ------- | ------- |\n| Train | $X_{\\rm train}, E_{\\rm train}$ | $X_{\\rm train}, E_{\\rm train}$ |\n| Test | $X_{\\rm train}, X_{\\rm test}, E_{\\rm train}$ | $X_{\\rm train}, X_{test}, E_{\\rm train}$ |\n\nBased on the experimental settings, we next illustrate how our achieved results serve to validate the effectiveness of our model.\n\n> Table 3 in our paper presents the results for **transductive setting** (where MLP and GNN use the same information), and the proposed InfoMLP significantly outperforms all GNN baselines in such a setting.\n\n> Table 4 in our paper reports the results for **inductive and cold-start settings**. Please notice the differences between these two settings. In the inductive setting, GNNs can leverage additional information ($E_{\\rm test}$ during the testing phase) over MLP-based models, which allows GNN to utilize the connection information of testing nodes. Hence, it is natural for GNN to outperform the MLP model in the inductive setting. But for cold-start settings, as shown in Table 4, our proposed InfoMLP yields significant improvements.\n\nTo sum up, when both model classes use the same input information and are compared in a fair setting, our InfoMLP exhibits clear superiority."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6120/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700647994495,
                "cdate": 1700647994495,
                "tmdate": 1700648150804,
                "mdate": 1700648150804,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ccVAbsyH4I",
                "forum": "2dLMPOY0HW",
                "replyto": "FBuHzgdDRH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6120/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6120/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer YPT6 (2/3)"
                    },
                    "comment": {
                        "value": "> Q1: How critical is the augmented representation?\n\nThe graph-augmented node feature matrix $X_{\\rm aug}$ plays an important role in the proposed InfoMLP, since we aim to minimize $H(A|X_{\\rm aug})$ such that $X_{\\rm aug}$ contains enough information about the graph structure. In our paper, we assume $X_{\\rm aug} = g(X, A) = \\sum\\limits_{k=1}^K \\gamma_k \\tilde{A}^{k}X$, and for simplicity we let $\\gamma_k = 1/K$. In this way, we only need to tune $K$ to obtain a proper $X_{\\rm aug}$.\n\nIn Figure 3 of the original paper, we studied the impacts of different instantiations of $X_{\\rm aug}$ on InfoMLP's performance by adjusting $K$. Our observation is that there is a positive correlation between InfoMLP's performance and $H(A|X_{\\rm aug})$ estimated by our estimators. Additionally, we observe that when $K=1$, $H(A|X_{\\rm aug})$ is in general, larger (and InfoMLP has suboptimal performance). However, as $K$ increases to 2 and beyond, $H(A|X_{\\rm aug})$ quickly converges to smaller values, and at this point, InfoMLP's performance also approaches the optimum. This indicates that, for the graphs studied in Figure 3, combining the information of 2-5 order neighborhoods with node attributes is sufficient for $X_{\\rm aug}$ to reconstruct the original graph structure $A$.\n\nIn addition, we supplement more discussions on the impact of $\\gamma_k$ on the performance of InfoMLP. We let $\\gamma_k = \\alpha(1-\\alpha)^k$ such that the definition of $X_{\\rm aug}$ recovers the classical Personalized PageRank embeddings[1]. This specification leads to the property that with increasing $K$, the weight of each hop is $(1-\\alpha)$ times the weight of the previous layer, thereby gradually reducing the importance of higher-order neighbors when $0 < \\alpha < 1$. We let $\\alpha_k = 0.2$, and present the performance of InfoMLP on Cora dataset in the transductive setting w.r.t. to $K$ in the table below. \n\n|  K | 1 | 2 | 3 | 4 | 5 | 6 | 7 |\n| ------- | ------- | ------- |------- | ------- | ------- |------- | ------- | \n| InfoMLP | $80.2\\pm 0.6$ | $82.5\\pm 0.6$ | $83.1\\pm 0.5$ | $83.5\\pm 0.5$ | $83.6\\pm 0.5$ | $83.5\\pm 0.5$ | $83.5\\pm 0.5$ \n\nWe have similar observations and results as in Figure 3. This indicates that InfoMLP can adapt to different instantiations of $X_{\\rm aug}$.\n\n> Q2: How critical is the MI simplified loss?.\n\nThanks for the nice suggestion that can help to improve our work. The R\u00e9nyi estimator is indeed an important method for measuring mutual information between two variables. However, as shown in Equation 3, our learning objective is to maximize $I(Z_1, Z_2)$, which will be used to update the model's trainable parameters. This requires the MI estimator to be differentiable for backpropagation. Unfortunately, the R\u00e9nyi estimator does not meet this requirement, making it unsuitable for our context. \n\nIn addition to the loss used in this paper, based on the reviewer's suggestion, we supplement new experiments with the InfoNCE MI estimator [2] and the Mutual Information Neural Estimator (MINE) [3]. They have both been proven to have lower bounds on MI[4], making it suitable as the learning objective (the MI can maximized by maximizing the estimator). In the following table, we present the mean accuracy of InfoMLP with these estimators:\n\n|  MI Estimator | Cora | Citeseer | Pubmed | \n| ------- | ------- | ------- |------- |\n| MINE | $82.7\\pm 0.9$ | $71.9\\pm 0.8$ | $80.8\\pm0.9$ |  \n| InfoNCE | $84.0\\pm 0.4$| $73.4\\pm0.4$ | $82.9\\pm0.3$ |\n\nThe InfoNCE MI estimator provides competitive performance as the feature-decorrelation-based loss used in our paper (even better on some datasets), while MINE in general, yields inferior performance. This is because MINE has a very large variance in MI estimation, which results in less accurate estimates (see discussions in [4]). Although InfoNCE provides strong results, its computational complexity is O(N^2d), which makes it time and memory-consuming to calculate the InfoNCE loss on medium-sized graphs. Considering these, we use the estimator, which is efficient and produces competitive results in our paper.\n\n\nReferences:\n\n[1] Gasteiger, Johannes, Stefan Wei\u00dfenberger, and Stephan G\u00fcnnemann. \"Diffusion improves graph learning.\" Advances in neural information processing systems 32 (2019).\n\n[2] Oord, Aaron van den, Yazhe Li, and Oriol Vinyals. \"Representation learning with contrastive predictive coding.\" arXiv preprint arXiv:1807.03748 (2018).\n\n[3] Belghazi, Mohamed Ishmael, et al. \"Mutual information neural estimation.\" International conference on machine learning. PMLR, 2018.\n\n[4] Poole, Ben, et al. \"On variational bounds of mutual information.\" International Conference on Machine Learning. PMLR, 2019."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6120/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700648135736,
                "cdate": 1700648135736,
                "tmdate": 1700719276565,
                "mdate": 1700719276565,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gUjGcy1B8c",
                "forum": "2dLMPOY0HW",
                "replyto": "FBuHzgdDRH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6120/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6120/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer YPT6 (3/3)"
                    },
                    "comment": {
                        "value": "> Q3: Entropy estimation is very terse (positive vs negative distribution). \n\nThank you for the constructive suggestion. To ensure our results are interpreted in a precise way, we'd like to point out the difference between the estimated values of $H(A|\\hat{A})$ (in the titles of Figure 2) and the divergence of positive/negative edge distribution. **The calculation method of the conditional entropy $H(A|\\hat{A})$ used in this paper is rigorously derived through the definition of conditional entropy (see Equation 1 and Appendix D in the submitted paper)**. It is merely in the derivation process that we found its calculation is closely related to the positive/negative edge distributions. Therefore, the plotting of distributions in Figure 2 is only an illustrative representation.\n\nWe agree with the reviewer's suggestion that computing the numerical metrics for measuring the distance between two distributions can add value to our paper. Therefore, we add new experiments calculating the relative entropy (KL-divergence) to measure the difference between the distributions of positive edges and negative edges. The table below shows the difference between the distributions of positive edges and negative edges measured by KL-divergence.\n\n|  Divergence | Cora | Citeseer | Pubmed |  Computer  | CS | \n| ------- | ------- | ------- |------- |------- | ------- | \n| KL-Divergence  | 0.4843 | 1.0093| 0.7928 |  0.1462 | 1.3617 | \n\nA larger KL-divergence value indicates a more significant difference between the two distributions. As expected, the KL-divergence values exhibit a clear negative correlation with the estimated values of $H(A|\\hat{A})$ in Figure 2, which is consistent to our results. E.g., Computer dataset has the largest $H(A|\\hat{A})$ while the smallest $\\mathcal{D}\\_{kl}(A|\\hat{A})$. By contrast, CS dataset has the smallest $H(A|\\hat{A})$ while the largest $\\mathcal{D}_{kl}(A|\\hat{A})$."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6120/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700648198549,
                "cdate": 1700648198549,
                "tmdate": 1700648231568,
                "mdate": 1700648231568,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "g3rE59dBzE",
            "forum": "2dLMPOY0HW",
            "replyto": "2dLMPOY0HW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6120/Reviewer_o93U"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6120/Reviewer_o93U"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies an Information-Theoretic Perspective on node classification on graphs, specifically, it focuses on the problem of when and why MLP can achieve good performances in graph representation learning tasks. The conclusion drawn from this paper is that when there is significant structural information contained within node features, MLP can achieve relatively good performances, since it does not require additional operators to introduce topological information. Motivated by this finding, the authors proposed a solution named InfoMLP, which aims to learn a node embedding that maximizes the mutual information between node feature and graph structure."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. The most notable contribution of this paper is it explained how and why MLP can achieve relatively good performances in node classification on certain graph datasets, the mutual information justification is intuitive and serves as a strong argument.\n\n2. Based on the observation that, MLP can only achieve good performances when graph structure information is partially contained in node features, authors proposed a mutual-information maximization approach to enhance the performance of MLP on the graph, which is novel and interesting.\n\n3. The proposed method exhibits strong improvements over baselines, notably, this improvement is valid on both homophilous and heterophilic graphs."
                },
                "weaknesses": {
                    "value": "1. Despite the insights and meaningful findings of this paper, the information-theoretic perspective is still rather empirical (solely based on observations), and lacks theoretical guarantee.\n\n2. What are the differences between $Z_{aug}$ and $Z_{mlp}$? The term \"ideal\" node representation is vague, a more rigorous definition might be needed here. \n\n3. The maximization of $H(A|X)$ is not clear, I can only see a non-parametric mapping that somehow fuses the representation of $X$ and $A$ together into a single representation. However, there is no guarantee nor intuitive explanation of why and how $X_{aug}$ maximizes $H(A|X)$. If no theoretical justification can be provided, authors should try to explain this from an intuitive standpoint, supported by an ablation study that can illustrate or verify the mutual information contained within $X_{aug}$."
                },
                "questions": {
                    "value": "No questions at the moment."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6120/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6120/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6120/Reviewer_o93U"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6120/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698831250788,
            "cdate": 1698831250788,
            "tmdate": 1699676403370,
            "mdate": 1699676403370,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Cj9dbBvSio",
                "forum": "2dLMPOY0HW",
                "replyto": "g3rE59dBzE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6120/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6120/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer o93U"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive comments and insightful suggestions.\n\n> What are the differences between $Z_{aug}$ and $Z_{mlp}$? The term \"ideal\" node representation is vague; a more rigorous definition might be needed here.\n\n**Differences between $Z_{\\rm aug}$ and $Z_{\\rm mlp}$**\n\n$Z_{\\rm mlp}$ and $Z_{\\rm aug}$ are the embedding matrix outputted from the same encoder MLP model, while with the different input information. Specifically, let ${\\rm MLP}\\_{\\theta}$ be the MLP encoder parameterized by $\\theta$, $Z_{\\rm mlp}$ is ${\\rm MLP}\\_{\\theta}$'s output using the raw node features as input, e.g., $Z_{\\rm mlp} = \\rm{MLP}\\_{\\theta} (X)$, while $Z_{\\rm aug}$ is ${\\rm MLP}\\_{\\theta}$'s output using the graph-augmented node features as input, e.g., $Z_{\\rm aug} = \\rm{MLP}\\_{\\theta} (X_{\\rm aug})$. Since $X_{aug}$ is defined as a function of both the vanilla node features and the graph adjacency matrix, i.e., $X_{\\rm aug} = g(X, A)$, $Z_{\\rm aug}$ naturally encodes the graph structure information into the embeddings, while $Z_{\\rm mlp}$ does not.\n\n\n**A more rigorous definition of \"ideal\" node representations**\n\n\nThe term \"ideal\" node representations in the beginning of Section 3.3 refers to the node representations that, given the conditions presented in Figure 1, would perform satisfactorily in downstream tasks. To improve the clarity, we have updated the description \"the node representations that are expected to perform well in downstream tasks given the condition depicted in Figure 1\" in the revised paper.\n\n\n>The maximization of $H(A|X)$ is not clear, I can only see a non-parametric mapping that somehow fuses the representation of $X$ and $A$ together into a single representation. However, there is no guarantee nor intuitive explanation of why and how  $X_{\\rm aug}$ maximizes $H(A|X)$.\n\nThanks for the question. The reviewer should be referring to the minimization of $H(A|X_{\\rm aug})$ (rather than maximization). The minimization of $H(A|X_{\\rm aug})$ is achieved via tuning the optimal $\\gamma_k$ and K in Equation 2. We provide a detailed explanation below.\n\nSince $X_{aug}$ is defined as the function output of the vanilla node features $X$ and the graph adjacency matrix $A$, i.e., $X_{\\rm aug}(X, A)$, we can formulate the optimization problem as\n$$\n    \\min\\limits_{X_{\\rm aug}} H(A|X_{\\rm aug}) = \\min\\limits_{g}H(A|g(X,A)).\n$$\nHowever, the search space for $g$ is very large, as $g$ can be any function. Therefore, we choose to reduce the searching space for g using the form of Eq. 6, which is the popular graph diffusion mechanism for aggregating multi-hop neighborhood information.\n$$\n    g(X, A) = X_{\\rm aug} = \\sum\\limits_{k=1}^K \\gamma_k \\tilde{A}^kX .\n$$\nIn this way, the optimization problem is transformed into selecting the optimal $K$ and $\\gamma_k$ such that $H(A|X)$ is minimized, which is much easier to solve."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6120/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700647836484,
                "cdate": 1700647836484,
                "tmdate": 1700647873950,
                "mdate": 1700647873950,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qIIg0vxQp6",
                "forum": "2dLMPOY0HW",
                "replyto": "Cj9dbBvSio",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6120/Reviewer_o93U"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6120/Reviewer_o93U"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors, thanks for addressing my questions. Overall, my side of the questions has been concluded.\n\nHowever, after reading other reviewers' comment, I feel that reviewer zabj and YPT6 are perhaps more knowldegable in this area, I recommend prioritizing the addressing of their concerns. Currently, I will maintain my existing score and confidence level. However, upon the satisfactory resolution of the questions raised by these other reviewers, I am open to considering an increase in the confidence score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6120/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700649300142,
                "cdate": 1700649300142,
                "tmdate": 1700649300142,
                "mdate": 1700649300142,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]