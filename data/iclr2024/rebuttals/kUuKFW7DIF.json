[
    {
        "title": "Multi-resolution HuBERT: Multi-resolution Speech Self-Supervised Learning with Masked Unit Prediction"
    },
    {
        "review": {
            "id": "WTBQGRPci4",
            "forum": "kUuKFW7DIF",
            "replyto": "kUuKFW7DIF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2381/Reviewer_nPyR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2381/Reviewer_nPyR"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed to improve the self-supervised learning (SSL) method for speech signal by applying the multiple resolution processing. The motivation is to capture varying levels of information present at different resolution of speech signal. Specifically, a hierarchical Transformer is incoperated into the HuBERT-style masked prediction based model architecture. Experimental results on LibriSpeech, SUPERB and ML-SUPERB demonstrateds superior performance compared to the original HuBERT method."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1, This paper proposed new innovative method to deal with a classical problem. Self-supervised learning (SSL) has been widely studied in the past several years to leverage the unlabeled data for deep learning. This paper tackled the SSL probelm from the model architcture aspect that performed the task of multi-resolution masked units prediction. As speech signal carries both short-term and long-term characterstics, e.g., semantic level, acoustic level, etc., applying multi-resolution processing is indeed a reasonable way to analyze speech signal. While it's been applied in other domain of speech area, this paper applies it on SSL for the first time. Their contribution not only comply with this paper, but also opens a door for more potential work in this area in the future.\n\n2, The experiments are comprehensive, clearly demonstrating the effectivness of the proposed method."
                },
                "weaknesses": {
                    "value": "The writing/presentation of the paper could be improved."
                },
                "questions": {
                    "value": "1, Have you or are you considering to evaluate your method on the ASR accuracy on the real conversational data, e.g., AMI, ICSI, Ali-meeting, etc. ? If you have already done with these evaluations, how is the performance compared with the original HuBERT method?\n2, In the paper, it seems two types of resolution are adopted in the method. What the performance will be if you increaes the resolution types to 3 or more?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2381/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2381/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2381/Reviewer_nPyR"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2381/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698618901066,
            "cdate": 1698618901066,
            "tmdate": 1699636173352,
            "mdate": 1699636173352,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Jv6m1EKxYV",
                "forum": "kUuKFW7DIF",
                "replyto": "WTBQGRPci4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2381/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2381/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nPyR"
                    },
                    "comment": {
                        "value": "Dear Reviewer nPyR,\n\nThank you for your supportive feedback on our work. We are pleased to see the alignment in our perspectives on the potential future trajectories of our research. In terms of writing and presentation, we are diligently integrating the collective advice from all reviewers, including notation linkage between figure and content, rectifying formulation errors, fixing missed descriptions, and elucidating experimental setups. Should you have any particular recommendations for further improvement, we would be eager to hear them.\n\nAddressing your queries:\n\n- We have not yet evaluated our system with real conversational data as it was beyond the scope of our initial focus. Indeed, compared to WavLM, which benefits from additional and noisy data augmentation, our system might not currently perform as well. We recognize the significance of this direction and intend to explore it in subsequent research.\n- Concerning your question about multiple resolutions, we direct you to Appendix B.2. Here we assessed the impact of incorporating three resolutions. The breadth of the investigation necessary to thoroughly examine this, including the consideration of more aggressive utterance-level embeddings and finer 10ms resolutions, would be vast. Given this, we have earmarked the exploration of diverse resolutions as a future direction, as noted in Appendix F.\n\nWe are grateful for your thoughtful review and hope that our responses shed light on the aspects you've inquired about. We remain committed to advancing our manuscript with the insights gained from your input."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2381/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699724377043,
                "cdate": 1699724377043,
                "tmdate": 1699724377043,
                "mdate": 1699724377043,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "53ieuJAhXU",
            "forum": "kUuKFW7DIF",
            "replyto": "kUuKFW7DIF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2381/Reviewer_hKsL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2381/Reviewer_hKsL"
            ],
            "content": {
                "summary": {
                    "value": "The authors investigate a multi-resolution evolution of HuBERT, MR-HuBERT, which augments the HuBERT architecture by integrating a lower time resolution (40 ms) transformer into the model, in addition to the standard higher resolution (20ms) processing (c.f. Figure 1).\n\nExtensive experiments on the well known LibriSpeech, SUPERB, and ML-SUPERB datasets are conducted, and indicate that MR-HuBERT generally performs on-par or better than HuBERT, and significantly better on LibriSpeech when the amount of labelled data is very limited (1 hour).\n\nIn addition, inference speed in terms of Multiply-Add Cumulations (MACs) is reduced by 9% and 13% by the base and large variants of MR-HuBERT relative to HuBERT."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- To the best of my knowledge, MR-HuBERT is the first approach to explicitly address the integration of multi-resolution information into the pre-training of a single model, as claimed.\n- Solid performance relative to HuBERT, with strong gains over appropriate baselines when limited task data is available.\n- Extensive evaluations on important datasets, at multiple operating points in terms of masked pre-training (e.g. 60K vs 960 hrs) and labeled data (e.g. 1,10,100 hrs of Librispeech).\n- Extensive appendix with detailed additional results and ablations.\n- Code and models will be publicly released."
                },
                "weaknesses": {
                    "value": "- Somewhat lower in ML novelty, as a more straightforward evolution of HuBERT.\n- As acknowledged (limitations, appendix E), MR-HuBERT was not trained on augmented data like WavLM, leaving this as future work, and so performance lags behind WavLM. \n- The important section on MR-HuBERT's architecture (3.2) could be improved. The processing steps are adequately described, and Figure 1 is for the most part clear enough, but with several functions f and outputs H, the relation between figure 1 and the description could be better---is equation 2 correct? Also, the operators in (eq. 2) and (eq. 3) should be introduced, as should $\\phi$. Are the high resolution encoders in figure 1 the same?\n- MACs are not usually a strong indication of inference speed on GPUs. This should be quantified further and/or perhaps de-emphasized in the abstract, as appropriate.\n- minor: speech Self-Supervised Learning (SSL) models -> Self-Supervised Learning (SSL) speech models\n- minor: we evaluate MR-HuBERT at two resolutions -> we evaluate a two resolution variant of MR-HuBERT"
                },
                "questions": {
                    "value": "See previous section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2381/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699118615345,
            "cdate": 1699118615345,
            "tmdate": 1699636173181,
            "mdate": 1699636173181,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "C16dt3sAWb",
                "forum": "kUuKFW7DIF",
                "replyto": "53ieuJAhXU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2381/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2381/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer hKsL"
                    },
                    "comment": {
                        "value": "Dear Reviewer hKsL,\n\nThank you for the encouraging remarks on our work and for your constructive feedback.\n\nIn response to your points:\n\n- We concur that the core concept behind our approach is straightforward. Nevertheless, we are thrilled about the introduction of MR-HuBERT to the research community. We believe that it can unlock numerous novel pathways in the field of speech representation learning, as thoroughly discussed in Appendix F of our paper.\n\n- With regards to the performance comparison with WavLM, we acknowledge that since we have not replicated the WavLM setups precisely, including data augmentation and the use of additional datasets such as Gigaspeech&Voxpopuli, a direct comparison is not feasible at this stage. We aim to explore these enhancements in our future research.\n\n- We appreciate your attention to detail concerning the notation in our formulation. We will make the necessary corrections to our figures to improve clarity, including the reversal of $f_1^q$ and $f_2^q$. We also apologize for the oversight of not including the definition of $\\phi$ in the manuscript. This will be corrected in our revised edition. Regarding the high-resolution encoders depicted in Figure 1, we will clarify that different $f_1^q$ and $f_3^q$ functions represent them, which should resolve any confusion.\n\n- Regarding the usage of MACs, your observation is astute. Our empirical findings did indicate that the actual execution time (measured by tokens_per_second in fairseq) aligns with the MACs calculations. However, due to the inherent variability in real-time measurements, we opted to present MACs as they provide a theoretical and more consistent metric. Taking your suggestion into account, we will include the real inference time from our Librispeech fine-tuning experiments in an appendix. For your immediate reference, here is a preliminary overview of the speed improvements on Librispeech dev-clean inference:\n\n| Model         | Tokens_per_second (the higher the better) |\n|---------------|-------------------|\n| hubert-base   | 5841.6            |\n| hubert-large  | 2226.2            |\n| mono-base     | 6332.3            |\n| mono-large    | 2522.1            |\n\n- Finally, we are grateful for your suggestions regarding writing improvements. These will be addressed in our forthcoming submission.\n\nYour feedback has been instrumental in enhancing the quality of our manuscript and thank you again for your effort in reviewing this paper."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2381/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699722507972,
                "cdate": 1699722507972,
                "tmdate": 1699722532832,
                "mdate": 1699722532832,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "anpc4b89S5",
                "forum": "kUuKFW7DIF",
                "replyto": "C16dt3sAWb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2381/Reviewer_hKsL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2381/Reviewer_hKsL"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response authors. These updates will resolve my concerns."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2381/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700093579179,
                "cdate": 1700093579179,
                "tmdate": 1700093579179,
                "mdate": 1700093579179,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9cVzck24xY",
            "forum": "kUuKFW7DIF",
            "replyto": "kUuKFW7DIF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2381/Reviewer_vPyr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2381/Reviewer_vPyr"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose an extension of HuBERT armed with multi-resolution perception and understanding capability. The authors show the proposed methods generally outperform HuBERT with sizable improvements in, if not full stack, a wide range of speech tasks."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The idea of multi-resolution encoder makes a lot of sense as acoustic concepts typically happen with different rates, and different speech tasks also require features with diverse granularity. According to the self-contained review on related works, this work, if not the first one, is among the early explorations on using multi-resolution encoder for self-supervised speech representation learning. Some bullet points:\n\n1. The proposed method achieved sizable improvement in the SuperB evaluation series. \n\n2. The proposed method exhibits computational efficiencies, specifically a 9-13% reduction in computation.\n\n3. The authors conducted extensive ablation studies to understand the effectiveness of different components. Detailed hyper parameters are also shared for reproducing purposes."
                },
                "weaknesses": {
                    "value": "My major concerns are:\n\n1. The ASR performance is not really better comparing to HuBERT.\n\n-- According to Table one, the proposed method is better than HuBERT when the hours of labeled speech is no more than 10; However, as we scaled up the labeled speech, HuBERT shows better performance in dev. \n\n-- According to SuperB evaluation, HuBERT large is still better (though not much) compared to the proposed method. \n\n\n2. One major invention, the sampling module that employs a blend of upsampling and downsampling to achieve flexible ratio between any two resolutions, do not show clear benefits when compared to just simple sampling module. \n\n-- According to Table 10, the proposed sampling strategy only becomes more powerful when using 100 hour of labeled speech, and the benefit is not significant. When only using 1 and 10 hour of labeled speech, the simple sampling strategy is actually doing better in terms of ASR. \n\n-- According to Table 18, it shows different design choices and configurations would clearly affect the downstream performance, and there is on clear winner."
                },
                "questions": {
                    "value": "I agree that the multi-resolution ideas is interesting, and the authors\u2019 model do achieve very promising performance according to SuperB evaluation. My main questions have been posted in 'Weakness' section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2381/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2381/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2381/Reviewer_vPyr"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2381/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699141641077,
            "cdate": 1699141641077,
            "tmdate": 1700968850432,
            "mdate": 1700968850432,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0clkqOBGlO",
                "forum": "kUuKFW7DIF",
                "replyto": "9cVzck24xY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2381/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2381/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer vPyr,\n\nWe are grateful for your positive evaluation and the attention to detail you have demonstrated in reviewing the appendix results of our paper.\n\nRegarding the issues you have raised:\n\n- Concerning ASR performance, we acknowledge that MR-HuBERT does not unequivocally surpass the original HuBERT in every test case, although it does in the majority. As we mentioned in the abstract, MR-HuBERT \"exhibits superior or comparable performance to the original HuBERT.\" \n\n  - Specifically, with the Librispeech experiments, we have observed a marginal degradation in the dev_other set compared to the publicly available pre-trained HuBERT, yet MR-HuBERT shows better performance across the remaining sets. It is essential to recognize the inherent randomness in HuBERT training, especially from K-means clustering, which can lead to variations in performance. For a fair comparison, we have also pre-trained a \\texttt{HuBERT-large}* using identical K-means clusters as MR-HuBERT. The results in Table 1 indicate that our \\texttt{mono-large} model outperforms \\texttt{HuBERT-large}* in all aspects except for a negligible difference in the dev_clean set.\n\n  - In the SUPERB benchmark, we have noticed the ASR performance is not as robust compared to the original HuBERT models. We have elaborated on potential reasons for this in Appendix C.3 (\\textbf{ASR} bullet), and we regard this as an avenue for future improvements to MR-HuBERT. The comprehensive explanation provided in the appendix should serve to clarify this point further.\n\n- We are thankful for your scrutiny of our detailed ablation analysis. While there is some divergence in model performance across the base setting in Librispeech experiments, the flexible up/downsampling module demonstrates more consistent results in large-scale experiments and the SUPERB benchmark. We encourage you to refer to Appendix B.8 (\\textbf{Simplified sampling is not recommended} bullet) and the results depicted in Table 18 for models \\texttt{(B.8)-i} and \\texttt{(B.8)-j}. Based on these observations, we maintain that our proposed flexible up/downsampling module is the more reliable choice over its simplified counterpart.\n\nWe appreciate your thorough examination of our ablations and insightful feedback. We trust that the explanations provided will satisfactorily address your concerns and contribute to a better understanding of our work."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2381/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699719577493,
                "cdate": 1699719577493,
                "tmdate": 1699719617227,
                "mdate": 1699719617227,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tjViRAquXU",
            "forum": "kUuKFW7DIF",
            "replyto": "kUuKFW7DIF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2381/Reviewer_MxEg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2381/Reviewer_MxEg"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel approach to multi-resolution pre-training for speech, designed to leverage information at various resolutions effectively. The authors demonstrate performance improvements in several downstream tasks, such as speech recognition, when compared to the prior work HuBERT."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper proposes a novel method for pre-training speech data at multiple resolutions within one model. The improvements on several downstream tasks are significant when using unlabelled data at different scales. The figures and tables are also well presented."
                },
                "weaknesses": {
                    "value": "There're two problems need to be solved before acceptance.\n1. The positions of $f_1^q$ and $f_2^q$ are reversed in equation 2. According to your description, the $\\tilde{H}_0$ is first processed by $f_1^q$.\n2. In HuBERT, there's only one output sequence, so it is sent to a CTC layer when fine-tuning. However, in MR-HuBERT, there're two output sequences. How are the two sequences of different lengths combined and sent to CTC? I didn't find the details in the paper."
                },
                "questions": {
                    "value": "See the two problems listed in the above weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2381/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2381/Reviewer_MxEg",
                        "ICLR.cc/2024/Conference/Submission2381/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2381/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699303148337,
            "cdate": 1699303148337,
            "tmdate": 1699732825398,
            "mdate": 1699732825398,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9mjoQ5tmKx",
                "forum": "kUuKFW7DIF",
                "replyto": "tjViRAquXU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2381/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2381/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer MxEg"
                    },
                    "comment": {
                        "value": "Dear Reviewer MxEg,\n\nThank you for your effort and for your positive feedback on our work.\n\nRegarding your observations:\n\n- We acknowledge the reversal error between $f_1^q$ and $f_2^q$ that you have identified. We will fix this mistake in our forthcoming revised manuscript.\n- Your second point has brought to our attention the need for greater clarity in describing the use cases of pre-trained MR-HuBERT. To address this:\n  - For the fine-tuning setting, we indeed utilize the hidden states from the last layer for downstream models, followed with a shallow linear CTC decoder. It is important to note that the lower-resolution layers are not directly engaged in this process. The intention is to demonstrate the effectiveness of the pre-trained MR-HuBERT as a comprehensive encoder. We will enhance the manuscript by including an additional sentence to elucidate this for the benefit of our readers.\n  - Regarding the frozen setting (SUPERB and ML-SUPERB), we employ a weighted summation of hidden representations from all layers. To synchronize the varying resolutions, we repeat-upsample the time axis of the lower-resolution representations, ensuring alignment across all layers. This approach is in line with established practices found in prior work, which can be reviewed at https://arxiv.org/abs/2306.01084.\n\nWe hope that these clarifications satisfactorily address your comments and enhance the understanding of our proposed methodology."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2381/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699715008984,
                "cdate": 1699715008984,
                "tmdate": 1699715008984,
                "mdate": 1699715008984,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xtqEuiXuzk",
                "forum": "kUuKFW7DIF",
                "replyto": "tjViRAquXU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2381/Reviewer_MxEg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2381/Reviewer_MxEg"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. I think the paper can be accepted after the two issues are solved."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2381/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699732623784,
                "cdate": 1699732623784,
                "tmdate": 1699732968671,
                "mdate": 1699732968671,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]