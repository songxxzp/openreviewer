[
    {
        "title": "Bridging Sequence and Structure: Latent Diffusion for Conditional Protein Generation"
    },
    {
        "review": {
            "id": "Gfa8UtHjSy",
            "forum": "DP4NkPZOpD",
            "replyto": "DP4NkPZOpD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8453/Reviewer_b3vX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8453/Reviewer_b3vX"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors present OMNIPROT, a novel generative modeling technique to capture the joint distribution between protein sequence and three-dimensional structure."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The method appears to be reasonable.\n2. The paper is well-organized.\n3. The research problem is meaningful for researchers in the computational biology field."
                },
                "weaknesses": {
                    "value": "1. There are style issues, such as: `\"FOr example, In drug development, ...\"`, where some words have inconsistent capitalization.\n2. The experiments are not comprehensive:\n   - While the paper in the method section claims that the proposed method can support various protein-related tasks, this is not adequately demonstrated in the experimental section.\n   - For the tasks that have been compared, the baselines are not comprehensive. For example, in the protein-protein docking task, traditional protein docking methods are not compared, and for the inverse folding task, only ProteinMPNN is compared.\n3. The paper mentions that the method supports \"protein-protein docking with contact information\" tasks, which may not be very practical, as in real-world applications, blind docking is more common.\n4. The method's innovativeness is limited. This method essentially involves multi-modal data input and the use of latent diffusion."
                },
                "questions": {
                    "value": "1. The paper mentions that the encoder is trained separately, but the authors do not explain the reason for doing so. Is it because training the encoder and diffusion module together would be challenging due to the complex model architecture?\n2. I hope the authors will compare their method with more baselines and also validate the model's performance on protein folding tasks.\n3. For the docking task, for methods based on generative models, such as the method proposed in this paper and the compared baseline DiffDock, the paper reports \"oracle\" statistics by selecting the prediction with the lowest RMSD from the ground truth among the 20 generated results. I believe it would be valuable to also present the mean and variance of these 20 samples on each metric. In real-world applications, we do not have ground-truth complex structures as a reference."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8453/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697598551161,
            "cdate": 1697598551161,
            "tmdate": 1699637054503,
            "mdate": 1699637054503,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6RhrkkOV5Z",
                "forum": "DP4NkPZOpD",
                "replyto": "Gfa8UtHjSy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8453/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8453/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Your time and constructive comments are sincerely appreciated in our efforts to improve our paper.\n\nPlease see the general response above regarding additional empirical evaluations and baselines.\n\n[_Docking with contact information may not be very practical._] We emphasize that our approach also readily performs blind docking, as explained in section 4 in the paper, and evaluated in the experiments section. While contact information may not always be available in practical applications, there are experimental cases where it may be (e.g. a given pocket is known). We believe that studying how the method can use this additional information and how it affects its performance to be an interesting direction, and that\u2019s why we include it in addition to blind docking.\n\n[_Limited innovativeness._] While we respect the reviewer's perspective, we respectfully disagree and remain confident that our paper offers valuable insights and innovative contributions to the field. The unique combination of the components of OmniProt and the seamless application to diverse biological tasks like protein-protein docking and inverse folding is novel and innovative. This is aligned with a lot of work in this domain, which also applies diffusion models (over euclidean spaces or manifolds) for different protein generation tasks. We believe our work has the potential to shift the field by demonstrating the versatility of latent diffusion in addressing complex challenges in structural biology.\n\n[_Why is the autoencoder trained separately?_] We did not explore training the autoencoder jointly with the latent diffusion. Training separately is the commonly adopted choice in practice (e.g. [1]), and appears to lead to better empirical results and simpler training. While joint training of autoencoder and latent diffusion has been explored (e.g. [2]) this requires some care during training, and to our knowledge is not that commonly used in practice. Additionally, training separately reduces memory requirements (both the encoder and the score network contain triangular updates from AlphaFold, which have high memory requirements.) \n\n[1] Rombach et al. \u201cHigh-Resolution Image Synthesis with Latent Diffusion Models\u201d\n\n[2] Vahdat et al. \u201cScore-based Generative Modeling in Latent Space\u201d.\n\n\n[_Docking benchmark use of oracle metrics, mean and variance._] Using oracle statistics to compare generative models is common practice (see, for instance, DiffDock) as it offers insights into the upper limit of a method\u2019s prediction accuracy, particularly when comparing to other baselines, ensuring a fair assessment of each method. For real-world generative modeling processes, it is common to introduce discriminative oracles as a substitute for known true conformations."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711530670,
                "cdate": 1700711530670,
                "tmdate": 1700711530670,
                "mdate": 1700711530670,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uxQQEHfmEG",
                "forum": "DP4NkPZOpD",
                "replyto": "6RhrkkOV5Z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8453/Reviewer_b3vX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8453/Reviewer_b3vX"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the authors' response"
                    },
                    "comment": {
                        "value": "Thank you for the authors' responses, but some of my concerns have not been effectively addressed.\n\nI believe the article should be more self-contained. If you have only performed docking and inverse folding, please do not claim that your method can effectively support various tasks, such as mentioning folding tasks without conducting experiments to support it.\n\nRegarding the generative modeling, the article can generate diverse docking complex structures. The experimental results presented in the article are selected from multiple results, with the best-scoring complex relative to the ground-truth metrics. What I mean is that when biologists perform practical operations, there is no such thing as a ground-truth. So how can we choose a good result? Therefore, I hope the authors can show the mean and variance of multiple results to indicate whether the generated results are generally of high quality or if the quality varies significantly.\n\nI feel that the experiments are not detailed enough, and many baseline comparisons have not been provided by the authors. There is a lack of a baseline for inverse folding, and ProteinMPNN is not state-of-the-art. Another reviewer mentioned PiFold, which was accepted by the previous ICLR, but the authors did not provide a comparison. Furthermore, for the results on inverse folding, OMNIPROT did not demonstrate superiority.\n\nRegarding the docking task, existing ML-based papers have mostly reported results from traditional docking software, such as HDock. However, the authors have not supplemented this part of the experiment during the rebuttal period.\n\nTherefore, based on the authors' current response, I will maintain my original score for now."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700716466725,
                "cdate": 1700716466725,
                "tmdate": 1700716466725,
                "mdate": 1700716466725,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "y7Wf2ZXVUF",
            "forum": "DP4NkPZOpD",
            "replyto": "DP4NkPZOpD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8453/Reviewer_ZFyz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8453/Reviewer_ZFyz"
            ],
            "content": {
                "summary": {
                    "value": "This paper constructs a unified latent diffusion framework for generating both protein sequences and structures. It leverages the proposed model for protein docking and inverse folding tasks. Experiments show that the proposed framework outperforms protein-protein docking baselines on intermediate-accuracy metrics, though not on highly-accurate metrics. It achieves similar performance to ProteinMPNN for the inverse folding task."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The work presents a unified protein generation framework capable of performing multiple tasks.\n- The paper is well-constructed, and it provides relatively comprehensive implementation details for the model and the experimental tasks.\n- In Appendix F.1, the authors point out the potential data leakage issues in the commonly used DIPS dataset, which can be one of the contribution."
                },
                "weaknesses": {
                    "value": "Main concerns are listed as follows:\n\n- Although the proposed model is claimed to \"bridge sequence and structure,\" Table 3 shows that the proposed method underperforms the inverse folding baseline on all metrics. This raises questions about the effectiveness and motivation of the unified framework by modeling both seq-structure as claimed by the authors. In the docking evaluation, OMNIPROT lags behind the baseline DockGPT by a significant margin for high-quality metrics (25th quantile for RMSD and \u2265high for DockQ). This suggests that OMNIPROT struggles to recall the most accurate \"hits\" but generates many just \"acceptable\" docked poses. In traditional docking, candidate poses are searched and ranked by free energy, with only a few top (low energy) poses selected for downstream analysis or tasks. Therefore, the high-quality metrics for docking should be given the utmost consideration during evaluation, as the results weaken OMNIPROT's performance. The authors are welcome to argue against this if they disagree.\n- The paper lacks an explanation for the design choices made for the OMNIPROT architecture. There is also no ablation study on the proposed framework, encompassing the encoder, decoder, and denoising network. Additionally, the authors should clarify why they adopted latent diffusion instead of a vanilla VAE or other sampling strategies used in VAE, providing corresponding evidence or support, such as an ablation study.\n- While Section 5 mentions that the training data were collected \"without any quality-based filtering,\" the authors should clarify whether they performed redundancy removal for the newly curated docking dataset. Since the docking task leverages both sequence and structure features, proper filtering for the training set (it is mentioned in the paper as \"consists of the remaining data\") based on sequence (and structure) similarity is required to prevent the potential data leakage.\n- A related work, PROTSEED [1] (I found it already published in ICLR 2023, thus cannot be counted as concurrent work), also employs the IPA module of AF2 to decode both sequence and structures for multiple tasks, including inverse folding. The authors should clarify how OMNIPROT differs from PROTSEED and provide a better context for the reference. Failing to show significant improvement over the related works can weaken the contribution of the proposed model.\n- The comparison with inverse folding baselines is insufficient, particularly with the introduction of the new metric \"sc-RMSD.\" The authors should consider contextualizing more baselines that handled this task: PROTSEED[1], ESM-IF [2], PiFold [3], and the gradient-based AlphaDesign [4] to name a few. To establish the state-of-the-art (SOTA) performance, the authors should also refer to the PiFold, especially since it was claimed to outperform ProteinMPNN. An incomplete baseline comparison on a well-studied task may reduce the credibility of the results, and additional experiments are encouraged.\n- Regarding the inverse folding evaluation, what is the rationale for using sc-RMSD instead of perplexity, where the latter is more commonly evaluated for inverse folding task? The sc-RMSD/TM are typically used for backbone (structure) generation models like FrameDiff [5]. Since the authors claim the proposed model to be \"generative,\" the perplexity (ppl) on the test set is encouraged to be reported. The ppl can be more important in reflecting the performance of the inverse folding model compared to the flawed recovery metric.\n- It is unclear to me whether the mentioned two tasks in this paper involves independent model training or if some (or all) of the model parameters are shared across tasks. If any parameters are shared, the authors could specify which ones. (please point to the related text if already mentioned)\n- What is  the \"geometrically-structured\" latent space compared to the canonical latent space as mentioned in the abstract and introduction sections?\n\nMISC:\n- Section 1 Introduction - line 7, FOr \u2192 For.\n- Equation (3), x1 \u2192 z1\n- Section 3 deserves a further refinement.\n- Section 4.4.1, I do not think this should be a subsection of 4.4.\n- I found the paragraph in section H (appendix) incomplete.\n\n[1] Shi, Chence, Chuanrui Wang, Jiarui Lu, Bozitao Zhong, and Jian Tang. \"Protein sequence and structure co-design with equivariant translation.\"\u00a0*arXiv preprint arXiv:2210.08761*\u00a0(2022).\n\n[2] Hsu, Chloe, Robert Verkuil, Jason Liu, Zeming Lin, Brian Hie, Tom Sercu, Adam Lerer, and Alexander Rives. \"Learning inverse folding from millions of predicted structures.\" In\u00a0*International Conference on Machine Learning*, pp. 8946-8970. PMLR, 2022.\n\n[3] Gao, Zhangyang, Cheng Tan, Pablo Chac\u00f3n, and Stan Z. Li. \"PiFold: Toward effective and efficient protein inverse folding.\"\u00a0*arXiv preprint arXiv:2209.12643*\u00a0(2022).\n\n[4] Jendrusch, Michael, Jan O. Korbel, and S. Kashif Sadiq. \"AlphaDesign: A de novo protein design framework based on AlphaFold.\"\u00a0*Biorxiv*\u00a0(2021): 2021-10.\n\n[5] Yim, Jason, Brian L. Trippe, Valentin De Bortoli, Emile Mathieu, Arnaud Doucet, Regina Barzilay, and Tommi Jaakkola. \"SE (3) diffusion model with application to protein backbone generation.\"\u00a0*arXiv preprint arXiv:2302.02277*\u00a0(2023)."
                },
                "questions": {
                    "value": "Please address and clarify the questions and concerns in the Weaknesses section above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8453/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697854703823,
            "cdate": 1697854703823,
            "tmdate": 1699637054379,
            "mdate": 1699637054379,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "U0giwt2LGg",
                "forum": "DP4NkPZOpD",
                "replyto": "y7Wf2ZXVUF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8453/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8453/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for dedicating your time and sharing extensive comments to improve our paper.\n\nPlease see the general response above regarding additional empirical evaluations.\n\n[_Metrics, inverse folding and docking._] While we acknowledge the observed underperformance of our proposed model on some metrics, we believe it is crucial to highlight the specific design philosophy behind OmniProt, as a single model to tackle multiple tasks in the protein generation space. Unlike methods focused on a single \"hit\", OmniProt is designed as a generative model with the intent of providing a diverse set of poses. The emphasis here is on capturing the broad spectrum of feasible solutions that could be relevant in certain biological contexts rather than solely optimizing for the most accurate single pose. As can be seen in the results, it produces more acceptable and medium poses, which means that it is able to find good poses for cases where DockGPT fails. That being said, we believe OmniProt metrics may further improve once we train it with the updated autoencoder. Also, it is worth emphasizing that OmniProt significantly outperforms the previous state of the art diffusion for docking (DiffDock-PP).\n\n[_Ablation study for architecture._] The combination of individual components in OmniProt is unique, yet, the choices for both the autoencoder and diffusion model are fairly standard and extensive ablation studies are costly. Please see the general response for a more detailed discussion.\n\n[_Redundancy removal._] Thank you for pointing this out. By using the Foldseek score for clustering, which linearly combines both 3D-based structure and sequence substitution scores (Appendix F), our approach combines sequence and structure-based similarity metrics and restricts them specifically to interfaces, to provide a robust approach for a protein-protein-interaction benchmark dataset creation. As the test clusters are separate from the train clusters, the train dataset is inherently deleaked against the test dataset.\n\n[_Differences w.r.t. PROTSEED._] Thank you for pointing this out, we will add this to related work. While PROTSEED is also jointly encoding sequence and structure with conditioning it does so without employing diffusion. The formulation behind OmniProt uses diffusion models which were originally designed to sample distributions; in the context of protein design to sample ensembles of protein conformations. The use of diffusion leads to a different overall formulation in terms of training objective and sampling schemes. However, we do believe that PROTSEED provides a relevant baseline and we are working on adding it to our empirical evaluation. \n\n[_IF perplexity metric._] While more metrics may always yield valuable insights, we believe perplexity does not give any indication whether a protein sequence will fold into the desired structure, which is why we chose to report sc-RMSD, an in-silico proxy for designability.\n\n[_Model shared or re-trained for different tasks._] The autoencoder is trained once for each dataset, and corresponding diffusion models are trained for each autoencoder and dataset.\n\n[_Geometrically structured latent space vs. canonical._] The abstract and intro both refer to the geometrically structured latent space. We use \u201cgeometrically structured\u201d because the representations are invariant to rigid body transformations of the input. As we explain in 4.4.1, this property means that OmniProt\u2019s performance on conditional generative tasks is invariant to rigid body transformations of the input structures.\n\n[_Refining section 3 (related work)._] We are happy to expand or modify the related work section (section 3). We will add some missing papers raised in the reviews, any other comments and suggestions are welcome.\n\n[_Incomplete paragraph in Section H._] Thanks for bringing this up. We apologize for this oversight, we will fix the incomplete paragraph."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711404940,
                "cdate": 1700711404940,
                "tmdate": 1700711404940,
                "mdate": 1700711404940,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5a9cqXZRwX",
            "forum": "DP4NkPZOpD",
            "replyto": "DP4NkPZOpD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8453/Reviewer_cpz6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8453/Reviewer_cpz6"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce OmniProt, a method for conditional protein sequence and structure generation leveraging a pre-trained autoencoder and latent diffusion. By capturing the joint distribution of sequence and structure and controlling the conditioning, OmniProt can be used for protein-protein docking, folding, and inverse folding. The authors report competitive performance for both protein-protein docking and inverse folding, compared to popular baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The key idea of performing conditional diffusion in the latent space of an autoencoder trained jointly on sequence and structure allows for a truly multitask generative model that encompasses many protein design capabilities."
                },
                "weaknesses": {
                    "value": "There are some questions remaining about the capabilities (folding, co-generation) that are repeatedly highlighted but not evaluated in the paper. It is unclear if the autoencoder approach introduces a fundamental limitation to the method, or if there are straightforward paths to improving the decoder. The authors show one example of ablating the effects of joint training on sequence and structure, showing improvement in inverse folding metrics when training with structure-based features; their approach affords unique insight into the value of joint training and this point could be expanded upon with further experiments."
                },
                "questions": {
                    "value": "1. Is there any particular reason why protein folding and co-generation have not been evaluated? The flexibility of the method is its major advantage, and even if it does not outperform purpose-built methods like AlphaFold2 (folding) and ProteinGenerator (co-generation), it would be interesting to see demonstrations of these capabilities. If the authors believe this is out of scope, it may be beneficial to narrow the scope of the claims and discussion in the paper to the two tasks that are considered in detail.\n2. The authors repeatedly mention that improvements to the autoencoder might improve overall OmniProt performance. Do the authors believe that this is a scaling / expressivity problem, or architectural?\n3. The authors highlight that OmniProt inverse folding samples are generated using reverse diffusion, rather than the randomized autoregressive scheme used in ProteinMPNN. This does intuitively seem like a major advantage, but no metrics are reported to support this claim. Can the authors provide some evidence beyond sequence recovery that more directly relates to sample quality?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8453/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698763127119,
            "cdate": 1698763127119,
            "tmdate": 1699637054263,
            "mdate": 1699637054263,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "x9WW9Cwllz",
                "forum": "DP4NkPZOpD",
                "replyto": "5a9cqXZRwX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8453/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8453/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for your valuable assistance in refining this work. We are grateful for the time and effort you dedicated to providing feedback. \n\nPlease see the general response above regarding additional empirical evaluations.\n\n[_Is the autoencoder a fundamental limitation? Ablation of architecture._] No, we do not believe it is. We are currently exploring changes to the autoencoder, and were able to improve its performance significantly. Please see the general answer for further details about updated performance. Since OmniProt was working at the limit of the autoencoder performance, we believe this will improve results (e.g. high-quality metrics for protein-protein docking). The main changes to the autoencoder were in the decoder, where we added pair feature updates and increased the number of layers.\n\n[_Experiments for further insights regarding joint seq+structure training._] We explored this for inverse folding. However, for docking all methods rely on the respective sequences as input, so it is not obvious to us how to decouple them. We agree studying this further is quite interesting, and appreciate any suggestions to do so.\n\n[_Metrics beyond sequence recovery._] We believe the main advantage of OmniProt comes from the flexibility in controlling the number of steps when generating samples. For diffusion models the number of steps is independent of the protein length, while autoregressive methods require one step per residue in the protein."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711199731,
                "cdate": 1700711199731,
                "tmdate": 1700711199731,
                "mdate": 1700711199731,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XE6H34UaNX",
            "forum": "DP4NkPZOpD",
            "replyto": "DP4NkPZOpD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8453/Reviewer_rb4f"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8453/Reviewer_rb4f"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces OmniProt, a framework for generative protein design as well as a range of tasks such as folding, inverse folding, and docking.\nThe model consists of an autoencoder, that maps protein sequences and structures to a latent space, and a diffusion model that operates in that latent space. The autoencoder is trained using roto-translational invariant features used in DockGPT. The tasks considered of inverse folding or docking are then solved through conditioning of the diffusion model with the relevant structural or sequence information, and a decoding of the generated latent space samples.\nThe model shows good performance on inverse folding and state-of-the-art performance on docking compared to existing ML models."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "This is an interesting study that takes a new approach to solve several related problems in protein design in a coherent way. The methods that are being compared, such as ProteinMPNN or DockGPT, are task-specific models and is quite remarkable that with a latent diffusion model, the authors are able to approach the performance of state-of-the-art models on both tasks. The OmniProt model is quite general and seems easily extendable into e.g. a de novo design framework."
                },
                "weaknesses": {
                    "value": "A more in-depth benchmarking of the autoencoder, and how its performance varies as a function of structure and sequence complexity would be beneficial. In benchmarks of inverse folding, ProteinMPNN is used as reference but more recent models such as LM-Design or Knowledge-Design have shown improved performance. In the docking benchmark, it would be useful to see comparisons with traditional docking tools such as Zdock or Haddock."
                },
                "questions": {
                    "value": "The authors should include a citation to 2305.04120 which has explored a relatively comparable approach of latent space diffusion for protein design."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8453/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698853059212,
            "cdate": 1698853059212,
            "tmdate": 1699637054152,
            "mdate": 1699637054152,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Oo7bP2RbEC",
                "forum": "DP4NkPZOpD",
                "replyto": "XE6H34UaNX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8453/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8453/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We thank you for carefully reviewing our work and providing comments to help improve the paper.\n\n[_More in depth benchmarking of the autoencoder._] As we described in the general response, we are currently exploring changes to the autoencoder, and were able to improve its performance significantly. Please refer to the general response for details.\n\n[_Cite 2305.04120, also latent diffusion for protein design._] Thank you for pointing this out, we will add this in the related work. In brief, the approach from \u201cA Latent Diffusion Model for Protein Structure Generation\u201d (2305.04120) differs significantly from OmniProt, as it only generates backbone coordinates (only C-alpha, while OmniProt also provides side-chain coordinates), and it does not generate sequences (while OmniProt does). Additionally, to our understanding, the method can only perform unconditional generation."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8453/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711096987,
                "cdate": 1700711096987,
                "tmdate": 1700711096987,
                "mdate": 1700711096987,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]