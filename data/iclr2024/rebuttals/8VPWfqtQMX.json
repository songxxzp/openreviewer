[
    {
        "title": "Context is Environment"
    },
    {
        "review": {
            "id": "qWw8kl6I3d",
            "forum": "8VPWfqtQMX",
            "replyto": "8VPWfqtQMX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1022/Reviewer_cBZm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1022/Reviewer_cBZm"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a domain generalization (DG) method called In-Context Risk Minimization (ICRM).\nICRM uses a transformer architecture that takes multiple datapoints as input and predicts their labels.\nIn DG, inputs come from different environments for which the distribution of inputs and labels change.\nDG assumes that the environment index is known for each datapoint at training and test time, i.e. we can group datapoints by their environment.\nIn ICRM, each set of 'in-context' inputs belongs to the same environment.\nAt test time, this allows ICRM to learn _in-context_ to adapt to a particular set of test inputs.\nThe authors argue this is advantageous to previous work in DG, which either learns models that aggregate test points into a single embedding or learn invariant predictors that ignore dependencies between test observations.\n\nThe authors provide theoretical evidence that ICRM can achieve better loss in iid and ood settings than naive context-free empirical risk minimization.\nThey provide experiments on FEMNIST, Rotated MNIST, WILDS Camelyon17, and Tiny ImageNet-C where they show ICRM outperforms the following methods: ARM, TENT, and ERM."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I am convinced that ICRM is advantageous to methods that do not rely on context or aggregate all context points into a single embedding.\n\nThe linear regression toy example they provide nicely illustrates the advantages of ICRM over invariant methods.\n\nThe paper is largely well-written and the description of the ICRM method is clear."
                },
                "weaknesses": {
                    "value": "Unfortunately,  I cannot recommend acceptance of the submission in its current state.\n\nA) My biggest concern is the strong resemblance the proposed approach bears to neural processes (NPs) (Garnelo et al., 2019), and in particular attentive neural processes NPs (Kim et al., 2019); ANPs), as well as other related work, e.g. from M\u00fcller et al. (2022) and Kossen et al. (2021).  This line of related work is not discussed at all in the current draft. Chiefly, these works are related because, excluding the original NP paper, they all propose transformer architectures that process multiple datapoints at the same time to improve predictions. Neural Processes are framed around 'function learning', where 'functions' are analogous to 'environments' for ICRM. For a set of context points $\\{(x_i, y_i)\\}_{i\\in\\mathcal{C}}$ drawn from a 'function' they make predictions for the labels $y_t$ of test points $x_t$, $x\\in \\mathcal{T}$ of that function. In the attentive neural process, attention is used to predict test labels by attending to the context points \u2013\u00a0strikingly similar to ICRM! \n\nI would strongly encourage the authors to highlight ANPs (and related work) as important prior work, and dial down claims of novelty when appropriate.\nI do believe that the application of ANP-like architectures to domain generalization is interesting and sufficiently novel, however, the current omission of relevant prior work is not acceptable.\n\nNote that there are some differences between prior work and ICRM.\nFor ANPs, the $x_i$ are often single features, e.g. the pixels of an image or (multi-dimensional) observations drawn from a Gaussian Process.\nHowever,  ICRM itself does not process high-dimensional inputs itself, and instead relies on ConvNet/ResNet-50 embeddings: these could also be used as pre-processing with ANPs.\n(Note that Neural Processes have been scaled to image classification before by Requeima et al. (2019))\n\nB) Perhaps the biggest difference between prior work and ICRM is, that ICRM does not use labels of examples as input.  This could be a restriction of ICRM: it can only model p(X|C) of the context. If there are any shifts in p(Y|X, C) between environments, ICRM cannot model these. For ANPs, this is not a problem, as inputs are (x, y) pairs. I would argue, ANPs are a generalisation of the proposed ICRM architecture.\nDoes domain generalization always preclude access to (even a few) of the test environment labels? Relatedly, does DG always assume that p(Y|X, E) does not shift? Even if both of these are true,  ANPs can still be used without test-context labels as these can be iteratively predicted. Conversely, if either is not true, then it seems like ANP-like architectures are clearly advantageous.\n\n\n* Kim, Hyunjik, et al. \"Attentive neural processes.\"  ICLR (2019). https://openreview.net/forum?id=SkE6PjC9KX.\n* Garnelo, Marta, et al. \"Neural processes.\" ICML 2018 workshop on Theoretical Foundations and Applications of Deep Generative Models. https://arxiv.org/abs/1807.01622\n* M\u00fcller, Samuel, et al. \"Transformers can do bayesian inference.\" ICLR (2022). https://openreview.net/forum?id=KSugKcbNf9.\n* Kossen, Jannik, et al. \"Self-attention between datapoints: Going beyond individual input-output pairs in deep learning.\" NeurIPS (2021). https://openreview.net/forum?id=wRXzOa2z5T.\n* Requeima, James, et al. \"Fast and flexible multi-task classification using conditional neural adaptive processes.\" NeurIPS (2019).\n\n\nC) My second biggest concern is with the results of the experimental evaluation. Figure 5 and Table 2 show that ICRM does not consistently improve performance when increasing the number of in-context demonstrations. In particular, for WILDS Camelyon17 and Tiny ImageNet-C the results are concerning. For Tiny ImageNet-C performance only improves very slightly when including additional in-context observations: crucially, the zero-shot ICRM performance outperforms all competing methods at _any_ number of in-context observations. Most concerning is WILDS Camelyon17: here, performance _decreases_ when including in-context examples \u2013\u00a0performance is maximal when the context is empty! \n\nIt seems to me as though the main selling point of ICRM, improving performance by including test-time examples in the context, only comes true for FEMNIST. The benefits of including datapoints in the context with ICRM are not as large as the authors proclaim. I did not see any discussion of this in the paper, and would ask the authors to provide one.\n\nD) Thanks for providing the ERM+ baseline, which uses \"an identical architecture to ICRM, but without context\". Comparing the numbers of Tables 3 and 2, I was surprised to see that ICRM at context size 0 significantly outperforms ERM+ across tasks. Do you have any insights as to why this would be the case? What differences are left between ICRM and ERM+?\n\nE) \"[...] even in the absence of test context. Specifically for both WILDS Camelyon17 and Tiny ImageNet-C, ICRM outperforms baselines despite not leveraging any context from the test environment. This is because ICRM training still benefits from contexts as to find contextual features that ERM ignores\" --> This is unclear to me \u2013\u00a0how can ICRM benefit from context if there is no context? Why does ICRM outperform the other baselines in the zero-shot setting?\n\nF) I think your definition of 'in-context learning' is misleading throughout the draft and a significant departure from the meaning of in-context learning (ICL) as introduced by Brown et al. (2020). This is a recurrent problem throughout the submission. For one, ICRM is about learning from _unlabelled_ examples. ICL is about learning novel tasks from a few-shot set of _labelled_ examples. For example, the 'poem' example the authors give on page 4 is inappropriate: neither does it provide multiple examples, nor does it provide labels. A more appropriate example for ICL for poem writing would be a set of input-output pairs, e.g. a list of target audiences and matching poems for them.\n\nG) \"The key to our answer resides in a recently discovered emergent ability of next-token predictors, namely, in-context learning.\"  I agree with you that an interesting aspect of ICL in LLMs is that it seems to 'emerge', at least the model is not knowingly trained to learn ICL. However, for ICRM, you _train_ the model to rely on the context, i.e. very similar to the setup of Neural Processes that predates ICL. Thus, I find it highly misleading to suggest ICRM relies on any emergent abilities.  Further, ICRM does _not_ preform auto-regressive next-token prediction: your inputs are images and your outputs are labels, opposed to input tokens at time t and output tokens at time t+1. One could even argue that what ICRM does, does not meet common expectations for in-context learning, as there are no labelled in-context examples.\n\nH) You frequently write that ICRM can  \"fully exploit data in natural order\". Are you using causal attention or positional embeddings? If not,  your Transformer architecture should be equivariant to the order of the inputs. Further, the prediction on the test input should then be _invariant_ to the order of the context examples. I therefore find your statements about 'nature does not shuffle data' misleading. In fact, I believe that, often, the order of the data should not affect predictions (cf. exchangeability). On the other hand, if you _are_ using causal attention or positional embeddings (which are part of the GPT-2 default architecture), I believe these might negatively affect your performance, and would like to see an ablation with them removed.\n\nI) \"The maximum context length, or support, is fixed at 100 for all algorithms\" \u2192 Do you sample uniformly between [0, 100] examples at training time, or do you train different ICRM models, one for each context length? Relatedly, you write that you train your model to minimize the 'auto-regressive loss'. Does this mean that, for each training input, you minimize the predictive loss for each of the training inputs, i.e. you perform 'teacher forcing', just like in LLM training?\n\nJ) Your highlighting of low and high attention scores in Figure 2 seems arbitrary. The caption suggests you highlight all low/high attention scores. In addition to not explaining how you decide what is a low/high attention score, you also clearly miss some low/high scores, e.g. 0.00, 0.00, and 0.05 in row 1 should be marked as low if 0.11 is, and 0.20 in row3 should be high if 0.15 is. It seems like you highlight only those scores that fit your narrative, e.g. the pizza in row three is not highlighted, but the train and bus matching the query 'train' are. I would suggest you highlight scores consistently. Alternatively, make clear you highlight only some examples: those which you discuss in the main text.\n\nK) Why is the performance on WILDS Camelyon17 the same for worst and average case? Does this mean the performance is exactly equal on all environments? Why is this the case?\n\n## Nits\n\nL) \"We attain significant out- of-distribution abilities in a multitude of specific tasks \u2014such as writing poems\" --> Why are you sure that writing poems out of distribution for large language models\u00a0\u2013\u00a0even if conditioned on some context?\n\nM) Figure 1b): It seems like it should be $x_i^e$ instead of $x_{i,1}^e$?\n\nN) The sentence before Eq. 5 does not make sense to me. Perhaps move the 'estimates the conditional expectation P (Y | X, C)' to after the definition in equation 5?"
                },
                "questions": {
                    "value": "O) \"Define \u03b4e to be a permutation of \u03b3e that swaps its two components\" \u2192 What are the two components (it seems to me there are three?) and what does it mean to swap them?\n\nP) How do the computational costs of ICRM and the baselines compare?\n\nQ) \"minimizing the worst risk across\" \u2192 Later you write that you sample environments at random during training. Does that not mean that you are actually minimizing the average risk across environments?\n\nR) \"to minimize the auto-regressive loss\" --> Why is this loss auto-regressive? It does not depend on the model's past predictions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1022/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1022/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1022/Reviewer_cBZm"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1022/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698401203888,
            "cdate": 1698401203888,
            "tmdate": 1700506020082,
            "mdate": 1700506020082,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BkNJ1lmg0U",
                "forum": "8VPWfqtQMX",
                "replyto": "qWw8kl6I3d",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1022/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1022/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Reviewer cBZm"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their detailed, thoughtful, and critical review.  We are glad you appreciated the novelty of our framework and its benefits, and we believe this could form the basis for a positive review. We believe that there are a few confusions that have resulted in the low rating. We have tried our best to clarify them below and in the paper. The relevant changes are highlighted in magenta in the main body and the Appendix.\n\n---\n\n> On Neural processes and attentive neural processes\n\nThere are a few important points worth emphasizing:\n- The key challenge of domain generalization is to build learners that succeed at test time in new situations without any access to the labeled data. If we have access to labeled data, it falls in the realm of few-shot learning, an important, related, and complementary area of research. We hope that the reviewer appreciates the challenge of domain generalization. \n\n- We agree that the existing works on NPs, ANPs, which have now been cited in the paper, and the more recent proposals studying ICL have similar architectures. However, as rightly pointed out in the review, there is a critical distinction \u2013 they all rely on labeled data in the context. Our work explores the potential of these context-based transformer architectures in the unlabeled data setting. We believe this is an important departure that embraces the essence of domain generalization. As mentioned in the review, this endeavor is deemed sufficiently novel. That said, our method is not constrained to operate on solely $x$\u2019s. As shown below and in Section D.4, ICRM can be easily adapted to training on labeled sequences. However, this approach (Supervised ICRM) is unsuitable for domain generalization settings where data labels are unavailable at inference.\n\nDataset |  |  | Avg.* |  |  |  |  | WG.*  |  |  |\n|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|\n**FEMNIST** | 0 | 25 | 50 | 75 | 100 | 0 | 25 | 50 | 75 | 100 |\nICRM       |  78.7  | 87.2| 87.4 | 87.5|  87.8|  59.8| 69.3 | 70.6 | 70.6| 70.6\nSupervised ICRM | **79.0** | **87.8** | **87.7** | **88.2** | **87.9** |**61.2** | **72.2** | **73.5** | **74.5** | **74.9** |\n| | | | | | | | | |\n**Rotated MNIST** | 0 | 25 | 50 | 75 | 100 | 0 | 25 | 50 | 75 | 100 |\nICRM       |  **93.6** |   96.1|  96.2| 96.2 | 96.2|  **82.5**|  88.5|  88.5| 88.8| 88.8\nSupervised ICRM | 93.3 | **96.3** | **96.3** |**96.3** | **96.3** | 82.0 | **89.0** | **89.0** | **89.1** | **89.3** |\n\n*Here \u201cAvg.\u201d and \u201cWG.\u201d respectively denote the average and the worst group accuracy across test environments. \n\n- Our work also lays down the theory that shows when unlabeled data can help the trained models perform better than standard ERM in different settings \u2013 short and long contexts and in and out-of-distribution evaluations. The theoretical analysis in this setting poses a different challenge than labeled data. This is another reason we strongly believe our work should not be viewed as a special case of ANPs. The theory from existing works would not translate and explain the success of ICRM. \n\n---\n\n> On the gains achieved using unlabeled examples\n\nWe would like to highlight three key points here. Firstly, in three of the four datasets, we observe notable improvements through adaptation. Please also refer to the worst-group accuracy in Table 2. These datasets are particularly challenging, and even a 1-2 percent gain is significant. Secondly, Theorem 2 characterizes scenarios under which increasing unlabeled context can monotonically improve the performance of the trained model. Thirdly, we present results with comprehensive hyperparameter tuning for all baselines for two additional datasets, Imagenet-R and CIFAR 10 C, demonstrating the advantages of incorporating context. \n\nDataset |  |  | Avg.* |  |  |  |  |  | WG.*  |  |  |\n|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|\nCIFAR10-C | 0 | 25 | 50 | 75 | 100 | | 0 | 25 | 50 | 75 | 100 |\nARM   | 65.9 | 66.0 | 66.0 | 66.0 | 66.0  | | 39.3 | 39.3 | 39.4 | 39.3 | 39.4\nERM   | 66.1 | 66.1 | 66.1 | 66.1 | 66.1 | | 39.8 | 39.8  | 39.8  | 39.8  | 39.8\nICRM  | **70.6** | **71.9** |  **71.9** | **71.9** | **71.9** | | **54.6** | **56.0** | **55.8** | **55.8** | **55.9**\n|  |  |  |  |  |  |  |  |  |  |\nImageNet R | 0 | 25 | 50 | 75 | 100 | | 0 | 25 | 50 | 75 | 100 |\nARM   | 56.3 | 58.1 | 58.8 | **59.8** | 59.0  | | 47.4 | 45.3 | 47.2 | **49.8** | 47.4\nERM   | **58.9** | 58.9 | 58.9 | 58.9 | 58.9 | |**48.0** | **48.0**  | **48.0**  | 48.0  | 48.0 \nICRM   | 57.4 | **59.7** |  **59.6** | 59.4 | **60.5** | | 45.4 | **48.0** | 47.2 | 46.9 | **50.6**\n\n*Here, \u201cAvg.\u201d and \u201cWG.\u201d respectively denote the average and the worst group accuracy across test environments."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1022/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700078546235,
                "cdate": 1700078546235,
                "tmdate": 1700078546235,
                "mdate": 1700078546235,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "78WjXoayYy",
                "forum": "8VPWfqtQMX",
                "replyto": "qWw8kl6I3d",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1022/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1022/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Performance decreases with context in WILDS Camelyon17. \n\nIn the Camelyon dataset, standard ERM often encounters a failure mode where the model exploits shortcut features such as luminosity. Consider a model that only takes the current query as input but has an effective featurizer that eliminates luminosity. Such a model can perform well on Camelyon-17. We posit that ICRM learns a similar featurizer by observing context during training. Consequently, the ICRM model has to learn to ignore additional queries provided during test time.  In some cases, these additional queries may have acted as distractors, causing a 1.2 percent drop in performance.\n\n\n---\n\n\n> On the ERM+ baseline\n\nERM+ only operates on the current query and there is no context input to it. However, ICRM accepts the current query and context as input. At test time during zero shot evaluation, it is true that both ICRM and ERM+ do not have any context. However, during train time ICRM is trained with contexts and that can lead to a model that is better than ERM. The context allows the model to selectively augment the current query and train a better featurizer as also explained in the previous answer. \n\n---\n\n> On the definition of in-context learning\n\nThe first and most popular conception of In-context learning [1]  involves providing the model with contextual information, typically a few sample $(x, y)$ pairs that represent a specific \u201ctask\u201d. While it is common to leverage additional information through labeled data, it is also possible to use unlabeled data to provide additional information, as demonstrated by our work. As shown in [2], labels do not play a significant role in several in-context learning tasks.\n\n[1] Brown, Tom, et al. \"Language models are few-shot learners.\" Advances in neural information processing systems 33 (2020): 1877-1901.\n\n[2] Work, What Makes In-Context Learning. \"Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?.\"\n\n---\n\n\n> On the role of emergence\n\nThere appears to be some confusion here. We are referring to the emergent ability of LLMs, and nowhere do we claim that this ability is emergent for our method\n\n---\n\n> On the role of natural order\n\nWe employ both causal attention and positional embeddings. Causal attention is used to allow the trained model to only make predictions by attending to the observed data thus far. It is also a popular choice to study in-context learning [1]. In our current setup, we randomly sample data sequences from a domain. However, there is a partial order imposed by the environments, which we have explained in greater detail in the revised manuscript. Our intention was to convey that if the data had been collected with a natural order, our approach can accommodate it by processing the inputs accordingly.\n\nAs the review correctly highlights, using positional encodings for our datasets, where predictions are invariant to the order of data in a sequence, may adversely affect performance. The table below illustrates the performance of ICRM with and without positional embeddings. Clearly, both the model achieves similar performance access benchmarks with an exception of FEMNIST, where the model without positional embeddings exhibits 5\\% gains in the worst group accuracy. \n\nDataset |  |  | Avg.* |  |  |  |  | WG.*  |  |  |\n|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|\n**FEMNIST** | 0 | 25 | 50 | 75 | 100 | 0 | 25 | 50 | 75 | 100 |\nICRM (with pos) | 78.4 | 86.6 | **87.3** | 87.0 | 87.3 | **56.0** | **72.3** | 69.3 | 70.8 | 70.0\nICRM (w/o pos) | **79.1** | **86.9** | 86.6 | **87.4** | **88.0** | 55.0 | 70.0 | **71.0** | **72.0** | **75.0**\n| | | | | | | | | |\n**Rotated MNIST** | 0 | 25 | 50 | 75 | 100 | 0 | 25 | 50 | 75 | 100 |\nICRM (with pos) | **92.7** | **95.8** | **95.7** | **95.8** | **95.9** | **81.1** | **87.8** | **87.0** | **87.5** | **87.8**\nICRM (w/o pos) | 92.1 | 95.3 | 95.5 | 95.5 | 95.6 | 80.1 | 86.0 | 86.3 | 86.8 | 86.9\n| | | | | | | | | |\n**Tiny ImageNet-C** | 0 | 25 | 50 | 75 | 100 | 0 | 25 | 50 | 75 | 100 |\nICRM (with pos) | 38.3 |  39.2 |  39.2 |  39.2 | 39.2 | 18.8  | 19.2  |  19.5 | 19.5 | 19.4 \nICRM (w/o pos) | **38.8** | **39.7** | **39.7** | **39.7** | **39.7** | **19.3** | **20.5** | **20.5** | **20.5** | **20.4**\n\n*Here \u201cAvg.\u201d and \u201cWG.\u201d respectively denote the average and the worst group accuracy across test environments. These reported numbers are not obtained through a rigorous hyper-parameter sweep. \n\n[1] Garg, Shivam, et al. \"What can transformers learn in-context? a case study of simple function classes.\" Advances in Neural Information Processing Systems 35 (2022): 30583-30598."
                    },
                    "title": {
                        "value": "Response to the Reviewer cBZm (Continued.)"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1022/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700078595224,
                "cdate": 1700078595224,
                "tmdate": 1700078667583,
                "mdate": 1700078667583,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZlLU7of6ZW",
                "forum": "8VPWfqtQMX",
                "replyto": "bPv711aCbB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1022/Reviewer_cBZm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1022/Reviewer_cBZm"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer Response to Rebuttal"
                    },
                    "comment": {
                        "value": "I thank the authors for their detailed response to my review, which have clarified a number of my concerns. I will increase my score to a 5 but will consider a further score increase if they can clear up my remaining concerns.\n\n\nBefore moving on to technical points, I would just like to note that I find your response hard to parse. It would have been great if you could have used the letters (a-z) that I used to enumerate the points in my original review.\n\n> On Neural processes and attentive neural processes\n\nThanks for your clarifications regarding domain generalization and for adding a discussion of neural processes to the paper.\n\n\n> On training of ICRM models with maximum context length 100\n\nThanks for your reply here!\n\nCan you provide ICRMs performance at all context sizes between 1 \u2013\u00a025 for FEMNIST, Rotated MNIST, and ImageNet-C? Given that you obtain predictions at all context sizes automatically, I would hope this is computationally doable for you.\n\nICRM performance often stays relatively constant between 25-100 examples but there is a big jump when going from 0-25 examples. I would like to see the behavior in that region in more detail. \n\nApologies for asking for another experiment. If you can provide these numbers, I will consider an increase in score.\n\n\n> Consequently, the ICRM model has to learn to ignore additional queries provided during test time. In some cases, these additional queries may have acted as distractors, causing a 1.2 percent drop in performance.\n\nIf ignoring queries works at training time, why would it cause a drop of performance at test time?\n\n> On the definition of in-context learning\n\nI nevertheless think it would be good to highlight this difference in between your setup and conventional in-context learning in LLMs.\nThis really is a bit tangential, but the results of [2] do not really seem to hold up to scrutiny, see https://arxiv.org/abs/2307.12375.\n\n\nI also still think your poem example is not representative of in-context learning in LLMs.\n\n\n> On the role of natural order\n\n\nThanks for the additional experiments.\n\nWhy are the numbers for ICRM (with pos) not the same as the ICRM numbers in the main paper? Is this variance from different random seed?\n\nI find the results for rotated MNIST particularly interesting. Do the inputs have a natural order here? \n\nDo inputs have a natural order for any of your experiments?\n\nIf not, then I find your repeated highlighting of this somewhat inappropriate/misleading.\n\n\n> Images following the query have scores of 0.0 due to causal attention, where the model ignores future tokens. Thus, highlighting such inputs doesn\u2019t add value to the observations.\n\nThat makes sense. Thanks for explaining. I think you should make clear that attention scores here are zero due to causal attention. Or maybe just not show images following the query at all. (Or maybe add arrows to illustrate the direction of attention.)\n\n\n> A threshold of 0.07 is chosen across sequences to distinguish high and low scores.\n\nI think you should mention this. Also, why 0.07?\n\n> Rather than categorizing all image scores as high/low, we aim to uncover non-trivial patterns learned by ICRM.\n\nFor the 'volleyball' query, why is high attention to 'academic gown' more interesting than high attention to 'hog'?\n\nI find your labelling here arbitrary and you should _at least_ clarify that you are making a decisions which high/low points you highlight.\n\n> However, there is a partial order imposed by the environments, which we have explained in greater detail in the revised manuscript.\n\n\nI cannot find any information on 'partial order' in the draft. Could you point me to the relevant section?"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1022/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700153143039,
                "cdate": 1700153143039,
                "tmdate": 1700153143039,
                "mdate": 1700153143039,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "k8ufbkZ3lp",
                "forum": "8VPWfqtQMX",
                "replyto": "EtUbEIEjWa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1022/Reviewer_cBZm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1022/Reviewer_cBZm"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer Response"
                    },
                    "comment": {
                        "value": "Thanks for your quick answer!\n\n> In Figure 5 of Section D.1 in the Appendix, we present the performance of ICRM, ARM, ERM, and TENT across various context lengths ranging from 0 to 50.\n\nThanks, the worst case results for FEMNIST are convincing: ICRM (a) starts off at similar performance as the other methods and then (b) improves progressively as more in-context examples are observed.\nThis is the behavior I would expect to see to support the claim that ICRM improves over baselines because of its 'in-context learning' abilities. \nI am not convinced there is a reason ICRM should significantly outperform other baselines without any context, hence, my expectation to see (a) as well. \n\n\nHowever, I do not find the results on the other datasets sufficiently supportive of the authors claims that improving context at test time helps ICRM outperform baselines.\n\nFor rotated MNIST, (a) is the case but (b) is much less clear, as there is really only one jump in performance from 0-5 examples (this jump could also be at one in-context example).\n\nFor Camelyon, (a) and (b) are not the case.\nFor Tiny ImageNet-C, (a) is not the case and the effect (b) is very small, perhaps insignificantly so.\n\nDo you explain somewhere how the standard deviations are calculated? Are they just over the three seeds (in which case, I think you should plot min/max instead as three is not an appropriate number of samples to compute a standard deviation).  Also, for the average test accuracy, it would also be important to consider the combined standard error over tasks seeds. I believe this would be the uncertainty to consider when evaluating if ICRM outperforms the baselines.\n\nDo you also average over different orders of including the in-context examples in each environment?  (If so, the variance over this would also be relevant.)\n\n\nI'm happy to discuss the above points further. Until now though, I would say there is only one dataset where ICRM clearly delivers on its promise, which is not enough for me to meet the bar of acceptance.\n\n\n> High attention to a \"hog\" is intriguing but challenging to interpret due to limited image pixels. Not all decisions made by an imperfect classifier are interpretable. It is even difficult to definitively classify this as an error, as there could be features in the image of a hog that are relevant to predicting volleyball. On the other hand, the high attention to individuals wearing academic gowns is more interpretable and suggests a semantic understanding of the image.\n\n\nI still don't buy this. Why would attention to an academic gown be more interpretable than attention to the hog, a walking stick, or a plate? I think this interpretation of your figure is cherry picked.\n\n\n> [All other points]\n\nThanks for your answers and actions here. They alleviate my concerns."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1022/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700222140155,
                "cdate": 1700222140155,
                "tmdate": 1700222140155,
                "mdate": 1700222140155,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "T2d1Dw2QiQ",
                "forum": "8VPWfqtQMX",
                "replyto": "op7ybRY0bC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1022/Reviewer_cBZm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1022/Reviewer_cBZm"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer Response"
                    },
                    "comment": {
                        "value": "Thanks for your response!\n\nIf you say that zero-shot/(k=small)-shot gains are also important to you, it would potentially be okay with me, if the (k=large)-shot gains are not always impressive with ICRM.\nHowever, I have to say that (a) I find this to be misleading given the current motivation and presentation of the paper focuses heavily on few-shot in-context learning gains, and (b) I remain not convinced why 0-shot predictions with ICRM should be better (see below).\nAgain, I appreciate your continued discussion on this topic.\n\n\n> ImageNet-R and CIFAR 10-C. Table 8 in Appendix explicitly shows in-context gains\n\nThanks for the pointer. I think the ImageNetR and CIFAR-10C results also mostly demonstrate (k=small)-shot gains, i.e. gains when going from zero-shot to few-shot, especially when considering the standard errors of the results.\nThe performance at 25 samples with ImageNetR is better than the performance at 75 samples. This makes me think that, either ICRM is not working as intended or the true variance is higher than you report, where I think the latter is more likely. Similarly, for CIFAR-10C you obtain the best performance at 25 samples. Importantly, performance at k=25 might be the same as performance at k=1,2,5,10,... which are not reported!\n\n\n> The example in equation (7) and its extensions in Appendix A.6 explicitly constructs a linear example for which ICRM exhibits better out-of-distribution performance than ERM without any contextual information at test time. Since the model is trained over an extended feature space, it can learn the right invariances for inputs in the original feature space.\n\n\nI think this example might be misleading. It _assumes_ ICRM already has access to the improved feature space. In a more realistic setup, you would actually need to show that ICRM can infer $\\mu_\\epsilon^2$ from the context set. For many context-points it makes sense to me that ICRM could infer this term from the context. However, how would ICRM improve zero-shot predictions here? Without any context points, it cannot infer $\\mu_\\epsilon^2$! \n\nOne may even expect that there is some trade-off here where ICRM has to find weights that work well for all possible k-shot predictions. Where the optimum for the zero-shot prediction should still be the ERM optimum.\nIn other words, if you optimize ICRM over 100 context points, the loss is an average over all the 1--100 predictions the model makes. If the model can rely on the existence of context points for most of these points, why would it learn a model that works well when there are no context points?\n\nAlso, would this example not be solvable by ERM if you would add, as is standard, a learnable constant term to the regression model? (I'm just saying this feels a bit artificial, even for a pedagogic example.)\n\n\n> By attending to images that the model finds useful for prediction, ICRM can end up learning \u201cin weights\u201d a higher-quality featurizer. That is, ICRM models may \u201cdepend\u201d on context to learn environmental spurious features while burning \u201cin-weights\u201d the invariant features across environments. \n\nI get the intuitive idea of 'the environment varies per context so this is what ICRM will use attention for; the invariant features do not vary per context, so this is what ICRM will use the weights for'.\n\nHowever, I do find this a little bit vague. Perhaps we can discuss this with an example?\n\nWhy should ICRM improve 0-shot performance over related work (in particular the ERM+ method) for RotatedMNIST?\n\nIt makes sense to me that, for multiple inputs, ICRM can infer the rotation of the environment and improve predictions. However, for zero-shot predictions, I do not see what 'higher-quality' 'invariant' features ICRM could learn that standard methods cannot learn."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1022/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700478151279,
                "cdate": 1700478151279,
                "tmdate": 1700478151279,
                "mdate": 1700478151279,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aBK5svkQs1",
                "forum": "8VPWfqtQMX",
                "replyto": "qWw8kl6I3d",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1022/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1022/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the latest reply by Reviewer cBZm"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their continued discussion. We hope that our clarifications below can help with the concerns they raised and help see our work in a positive light. \n\n---\n\n\n> Regarding misleading motivations\n\nIn their response, they state \u201cI find this to be misleading given the current motivation and presentation of the paper focuses heavily on few-shot in-context learning gains\u201d. We are not clear what they mean here. Both zero-shot and few-shot gains are at the heart of domain generalization, which is the primary focus of our paper.  We do not see how the paper is misleading. Nowhere in our paper do we assert that gains are exclusively expected with large context lengths and not attainable with small context lengths. In fact, achieving gains at small context lengths is practically significant. Our theoretical results focus on in-context gains in the limit of both large and small context lengths (even one context length), as demonstrated in Theorems 1 and 2, respectively.\n\n---\n\n\n> Regarding the issue of performance drop in CIFAR10-C\n\nWe use a toy example to explain the source of the performance drop. Consider a sequence $x_1, \\cdots, x_n$ and corresponding labels $y_1, \\cdots, y_n$.  Suppose the label are generated as  $y_i = f(x_{i},x_{i-1})$, where $x_i$ is the current query, $x_{i-1}$ is the previous query and $y_i$ is the label for the current query. We contrast learning a function with context length 1 and context length 2. The functions with context length 1 take as input $x_i, x_{i-1}$ and the functions with context length 2 take as input $x_i, x_{i-1}, x_{i-2}$.  Observe that the function class of context length 2 contains the function class of context length 1. As a result, the optimal predictor (in terms of the prediction error over the training distribution) in length 2 class is at least as good as that in length 1 class. However, when it comes to learning the ideal function $f(x_{i},x_{i-1})$ with a finite number of samples, learning the ideal function from length 1 class should require fewer samples than learning the ideal function from length 2 class. In other words, with a fixed number of samples, the generalization error of the length 1 class should be smaller than length 2 class.  In our experiments, these finite sample-driven learning-based issues could lead to performance drops.\n\n---\n\n\n> Regarding the performance of CIFAR 10 C at intermediate points\n\nWe would be happy to add performance at all intermediate points for CIFAR 10 C and Imagenet R. And at the risk of repetition even if the performance was the same for all $k=1, 2, \u2026, 25$, it is acceptable and not in contradiction with desiderata or the theory. \n\n---\n\n\n> Regarding zero shot gains\n\n You stated, \u201cI remain not convinced why zero-shot predictions with ICRM should be better\u201d. We have provided empirical evidence that zero-shot gains are achievable. We have provided a solid theoretical analysis for context lengths 1 and beyond. We also provide initial insights into zero-shot gains through examples in equation (7) and the selective augmentation perspective. Transforming these initial insights into a solid full-blown theory to explain \u201cwhy ICRM improves zero-shot predictions\u201d is an important future work, which we also stated in our previous response.  We hope that the reviewer appreciates that it is not possible to explain all aspects of the experiments through the lens of theory in one paper.\n\n---\n\n\n> Regarding the comparison of ERM vs ICRM at zero shot in the example from equation (7) \n\nIn Proposition 3 in the Appendix, we showed that for the example under consideration, the error of ERM grows in variance of the first feature. In the paragraph that follows Proposition 3, we show that the error of ICRM without any context is independent of the variance of the first feature. In the absence of any context, we assume ICRM uses a fixed value of zero as an estimate for the mean.  Zero is an arbitrary choice and qualitatively the result stays the same even if some other fixed value is used. If the variance of the first feature is sufficiently high in the test environment, then the error of ERM is larger than the error of ICRM (without context).\nOn the point about adding a fixed constant to ERM, this will also not work as the mean $mu_e^2$ varies across environments.\n\n---"
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1022/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700497700621,
                "cdate": 1700497700621,
                "tmdate": 1700498031746,
                "mdate": 1700498031746,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0a48YdvVGK",
                "forum": "8VPWfqtQMX",
                "replyto": "GeEl67kyYO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1022/Reviewer_cBZm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1022/Reviewer_cBZm"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer Response"
                    },
                    "comment": {
                        "value": "Thanks for your continued engagement. I am sufficiently satisfied by your responses and will increase my score to a 6.\n\nI would encourage the authors to include some of this discussion in the updated version of the paper.\n\nFor example, I think the current abstract strongly implies that the gains of ICRM are due to in-context learning (\"We argue that context is environment, and posit that in-context learning holds the key to better domain generalization.\")\nThis suggests that test-time observations of environment are the reason why ICRM improves over baselines.\nHowever, half the datasets in Figure 5 show that zero-shot gains dominate the improvements obtained from ICRM.\nThe authors convincingly explain why learned features would be different for ICRM, but not why they would be better."
                    }
                },
                "number": 28,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1022/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700506004551,
                "cdate": 1700506004551,
                "tmdate": 1700506004551,
                "mdate": 1700506004551,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3WJVoBcvGn",
            "forum": "8VPWfqtQMX",
            "replyto": "8VPWfqtQMX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1022/Reviewer_NswY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1022/Reviewer_NswY"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new method for domain generalization called In-Context Risk Minimization (ICRM), a method which generalizes to a new domain given a sequence of unlabeled examples from that domain. The authors motivate ICRM by referring to the LLM literature, in particular their emergent ability to perform well OOD. They prove that ICRM can \"zoom-in\" on good/optimal risk minimizers given a sample/infinite data. Empirically, they demonstrate that ICRM outperforms baselines on several DG datasets and perform supplemental experiments to ablate and interpret these results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- I think this is an ambitious paper and connects two interesting areas within OOD-type research - there are definitely interesting insights gained from the new setup and the idea of receiving a sequence of new-domain examples\n- empirical results are strong, demonstrate a good use of unlabelled data\n- experiments are pretty thoroughly done, I appreciate the supplementary studies in Table 3, Fig 2, and Fig 5\n- the new method is described clearly and the theoretical results seem useful: in particular Theorem 3 and its extension seem important in terms of plugging this work into the iVAE work. I also like the specification of the limitations in terms of Voronoi cells of the training environments as a concept"
                },
                "weaknesses": {
                    "value": "- I find myself somewhat confused by the analogy between LLMs and ICRM - the paper makes it seem as those these should map 1:1 but I can't quite make it clear to myself, perhaps the authors can clarify. It's not clear what the sequence of Xs that arrive correspond to in LLMs, since they are listed as being selected at random at training time: this means they can't be language tokens, and if they are unrelated sequences I don't see how they correspond with the notion of context laid out in the \"gravitational fields poem\" example (these are also correlated - context sequences in the LLM space are usually not thought of as random wrt what follows them). I think ICRM is interesting nonetheless but I have trouble getting the intuition straight because of this\n- I think that the relationship to various types of distribution shift should be highlighted a little bit more here: for instance, it seems like ICRM will be vulnerable to shifts in P(Y | X) across environments, since it only receives unlabelled examples from the new domain at test time. I think this is true of many DG methods, and I think this is what the \"Voronoi cells\" specification in Thm 3 is constraining, which I think is nice. However, I think this weakness is somewhat obscured by the discussion of context and the analogy to LLMs (e.g. gravitation fields poem example): in LLMs the \"context\" as it is generally considered is quite rich and can help with generalization to compound ie Y | X shifts, but ICRM as I understand it does not have that capability.\n- in Sec 5, I don't find the example particularly clarifying: I may be missing something but it seems unfair to give ICRM access to the \\mu variables, and not give them to ERM. It's possible this is a reasonable comparison to make but I think it needs to be explained a bit more in that case.\n- I don't understand how ICRM allows models to take advantage of \"natural order\", since the examples are drawn randomly at training time, correct?\n- I find it a little puzzling that the performance of ICRM decreases with added context in Camelyon17 - is there an explanation for this?\n- It's strange to me that ICRM would outperform other methods even at 0 context: isn't the advantage of the method the ability to use context?\n\n\nSmaller questions:\n- Intro: Minor point but important I think - the Angwin et al paper (Propublica COMPAS investigation) is not an example of an investigation of the impact of out of distribution model failures, but rather the impact of unbalanced model error rates. There are many studies of societal impacts of OOD failures and I think it would be better to cite one of those\n- Thm 3: \"there exists an ICL algorithm\" - what is an ICL algorithm? I don't think this is defined as a mathematical object\n- end of Sec 5: I think you mean that lung cancer is \"deterministic\" in the single smoker case, rather than it \"invariably follows\""
                },
                "questions": {
                    "value": "- Some people would argue that LLMs don't actually perform well OOD - rather, they are trained on a large enough dataset that instances where they have to perform truly OOD are much rarer. This might be good to address in the motivation/intro\n- In general, I find the two concrete examples in the paper more confusing that clarifying (gravitational fields poem, ICRM example in Sec 5): I think clarifying these would go a long way to my understanding of the method and paper"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1022/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698676352428,
            "cdate": 1698676352428,
            "tmdate": 1699636028214,
            "mdate": 1699636028214,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mPlLK33AaG",
                "forum": "8VPWfqtQMX",
                "replyto": "3WJVoBcvGn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1022/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1022/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Reviewer NswY"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their thought provoking review and encouraging words of appreciation for our work. We have incorporated the changes which can be found in magenta in the main body and the Appendix.\n\n---\n\n\n> On the analogy between LLMs and ICRM \n\nWe agree that there is not an exact one-to-one correspondence between all aspects of an LLM and ICRM. However, a few important aspects bear a strong parallel in our implementation. Firstly, both LLM and ICRM condition on the entire sequence of experience so far. Secondly, both typically exploit attention mechanisms to select the examples to pay attention to. However, there are differences, as rightly pointed out in the review. For us, the examples in a context are unlabeled inputs sampled at random from the environment, and thus the order of the examples does not matter. This is a facet specific to datasets we consider and, hence is different from standard language modeling. However, the proposal can handle datasets with some natural ordering based on time (e.g., video) and space (e.g. text). In such a scenario, we preserve the natural ordering instead of shuffling it. \n\n\n---\n\n\n\n> On the nature of distribution shifts \n\nWe agree that ICRM cannot tackle arbitrary shifts in $P(Y|X)$, and the Voronoi cells are precisely meant to constrain that. However, there is an interesting side to it. The constraint imposed by the Voronoi cell is more relaxed than the standard covariate shift assumption where $P(Y|X)$ does not vary. This is afforded to ICRM by the fact that it operates on an extended feature space that takes both current input $X$ and distribution $p(X)$ from the test environment as input, while standard learners (based on ERM, IRM) operate solely on $X$. \n\n---\n\n\n> On the fairness of comparison between ICRM and ERM in Section 5\n\nThis example is meant to bring to light how a key difference between the nature of ICRM and ERM translates to an important difference in the learned model. ERM operates on the current query $x$, and ICRM in its standard form operates on $x$ and $p(x)$. In this example, we make an additional assumption that ICRM operates on $x$ and $\\mu(p(x))$, which we have stated. Under this assumption, we show that ICRM learns the right invariances. It is a pedagogical example to show that \u201cif one could use distributional features as additional inputs, one could reveal the invariance of interest\u201d. Further, our experiments are empirical support for ICRM having such capability for distributional feature discovery.\n\n---\n\n\n> On the issue of ICRM exploiting \u201cnatural order\u201d. \n\nThis point is absolutely correct. In the current setup, we randomly sequence data and do not exploit natural order. We were alluding to possible future work where, if the data had some natural order, namely, say a video where you classify each frame, our ICRM  proposal extends without change and exploits natural order. In sum, our remark about \u201cnatural order\u201d is made as an attempt to motivate future data collectors to retain this precious time-metadata, so methods such as ICRM can be deployed in full force.\n\n---\n\n\n> On the drop in Camelyon-17. \n\nIn the Camelyon dataset, standard ERM often encounters a failure mode where the model exploits shortcut features such as luminosity. Consider a model that only takes the current query as input but has an effective featurizer that eliminates luminosity. Such a model can perform well on Camelyon-17. We posit that ICRM learns a similar featurizer by observing context during training. Consequently, the ICRM model has to learn to ignore additional queries provided during test time.  In some cases, these additional queries may have acted as distractors, causing a 1.2 percent drop in performance.\n\n---"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1022/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700077588717,
                "cdate": 1700077588717,
                "tmdate": 1700077588717,
                "mdate": 1700077588717,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Sk57qop7rq",
                "forum": "8VPWfqtQMX",
                "replyto": "elOiKl2ayR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1022/Reviewer_NswY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1022/Reviewer_NswY"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thanks for this rebuttal - mostly my opinion on the paper has remained stable although I appreciate some clarifications. Some thoughts:\n\n- I think the claim of the paper seems to imply that the analogy is pretty tight between ICRM to LLMs - I'd recommend backing off some of this language if LLMs are mostly inspiration. It's actually quite different to be operating on p(x) as an input than it is to do sequence prediction. I think this is where my confusion comes from - I think it would be much better to have examples which are *not* sequence prediction but nonetheless show the desired behavior\n- On that note, I think describing this model as \"operating on p(x) as an input\" is nice language and would find that clarifying in the paper\n- citations for societal impacts: I haven't read the top two, they seem fairly general. The bottom seems closer to what we're talking about here - not sure if you're interested in distinguishing between a subgroup failure and an OOD failure."
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1022/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700499309575,
                "cdate": 1700499309575,
                "tmdate": 1700499309575,
                "mdate": 1700499309575,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xs6P222RJm",
            "forum": "8VPWfqtQMX",
            "replyto": "8VPWfqtQMX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1022/Reviewer_1GJP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1022/Reviewer_1GJP"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new method for domain generalization that utilizes data previously seen in the test set as contextual information to improve generalization accuracy."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The writing in the article flows smoothly, and the experiments seem to yield very promising results."
                },
                "weaknesses": {
                    "value": "The combination of domain generalization and in-context learning is an ambitious idea. However, the theoretical and experimental discussions in this paper are not sufficient. \n\nI find that the motivation for integrating these two concepts does not fully convince me. It appears that this paper only utilizes information from observed data samples x, which I believe is not entirely consistent with the current concept of in-context learning in LLM because a sequence formed solely from observed training data x may not be sufficient as a demonstration to the model. Authors could consider providing sequences of x, y pairs as context, rather than just a sequence of x. \n\nWhile this paper seems to get promising results, its practical applicability may be limited because it does not account for the possibility of the testing set data samples being a mixture of unknown domains. To address this issue, it might be worth exploring the identification of domain-specific information or introducing additional information as prompts (context). \n\nIn fact, the method proposed in this paper is quite similar to the example presented in Figure 1.b, with the key difference being that this paper's method does not involve averaging."
                },
                "questions": {
                    "value": "* The method proposed in this paper was inspired by some ideas in in-context learning, but its relationship with in-context learning in LLM is not significant. The description in the third paragraph of the introduction can easily lead to misunderstandings.\n\n* A concern regarding Theorem 1: Why does the inequality hold? Even though we assume that data can be independently sampled from x, it doesn't necessarily imply that the distribution of the sequence formed by the samples is independent of the variables. Therefore, The assumption in Theorem 1 seems not correct. Are there any relevant references that can confirm the existence of this independence?\n\n* Does function h have different parameters for different environments? Theorem 1 indicates that h has different parameters depending on the environment. If this is the case, which set of parameters should be used during testing?\n\n* The current motivation of this article lies in the idea that applying context information in domain generalization can get better generalization results. Furthermore, this paper argues that previous methods did not consider this aspect. However, there is an issue in assuming that testing data comes from the same domain (or we know the domain partition of test data) when discussing the generalization problem. If it samples from different domains, this approach may potentially mislead the model to produce incorrect results, as the function h learned during training somehow combines information from c and x to make decisions.\n\n* Although the experiments have yielded many promising results, they are only compared to ERM and two baselines from three years ago. The baseline's performance is even lower than ERM, although the author initially acknowledges that most methods may not necessarily outperform ERM. However, the choice of baselines appears insufficiently comprehensive."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1022/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1022/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1022/Reviewer_1GJP"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1022/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698819283035,
            "cdate": 1698819283035,
            "tmdate": 1700811187585,
            "mdate": 1700811187585,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EdlFt56ydp",
                "forum": "8VPWfqtQMX",
                "replyto": "xs6P222RJm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1022/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1022/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Reviewer 1GJP"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their critical feedback that helps us improve the work and also for appreciating the ambitious idea that we undertake in this work. We have highlighted the changes in magenta in both the main body and the Appendix of the revised manuscript.\n\n---\n\n> On integration of the two concepts of domain generalization and in-context learning: \n\nWe will clarify that, in order to benefit from in-context learning in domain generalization, the context itself must be a sequence of unlabeled inputs. The first and most popular conception of In-context learning [1]  involves providing the model with contextual information, typically a few sample $(x, y)$ pairs that represent a specific \u201ctask\u201d. In contrast, we introduce an alternative perspective on in-context learning, where unlabeled $x$ inputs act as the contextual backdrop for a task, also known as an 'environment.'  Below, we emphasize the significance of this approach along two key dimensions.\n\n- **Why unlabelled $x$\u2019s:** In domain generalization, at test-time, the learner only has access to the unlabeled x\u2019s from the test environment and not both $(x,y)$ pairs. We embrace this challenge and develop a proposal inspired by in-context learning that shows we can continue benefiting from information in the unlabeled $x$\u2019s. Our theory qualifies the conditions under which the information from unlabelled $x$\u2019s only can help improve upon standard ERM in both in-distribution and out-of-distribution settings.\n\n- **Example of when only $x$\u2019s can provide useful context:** Consider the following example \u2013 there are two types of users, one who has a more cursive handwriting and the other type uses straight edges.  At test time, the classifier may be faced with samples from cursive users. By observing the images ($x$\u2019s) only of the samples drawn by the user without the label, the model can quickly realize that it is dealing with a cursive type user. These intuitions are made formal by our theory. Our method capitalizes on these intuitions to show the empirical benefits. \n\nFurther, our method can be easily adapted to deal with $(x,y)$ pairs as input. We refer to this approach as Supervised ICRM. As shown in the table below and Section D.3 and as anticipated, Supervised ICRM outperforms ICRM but is not suitable for domain generalization settings where data labels are unavailable at inference.\n\nDataset |  |  | Avg.* |  |  |  |  | WG.*  |  |  |\n|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|\n**FEMNIST** | 0 | 25 | 50 | 75 | 100 | 0 | 25 | 50 | 75 | 100 |\nICRM       |  78.7  | 87.2| 87.4 | 87.5|  87.8|  59.8| 69.3 | 70.6 | 70.6| 70.6\nSupervised ICRM | **79.0** | **87.8** | **87.7** | **88.2** | **87.9** |**61.2** | **72.2** | **73.5** | **74.5** | **74.9** |\n| | | | | | | | | |\n**Rotated MNIST** | 0 | 25 | 50 | 75 | 100 | 0 | 25 | 50 | 75 | 100 |\nICRM       |  **93.6** |   96.1|  96.2| 96.2 | 96.2|  **82.5**|  88.5|  88.5| 88.8| 88.8\nSupervised ICRM | 93.3 | **96.3** | **96.3** |**96.3** | **96.3** | 82.0 | **89.0** | **89.0** | **89.1** | **89.3** |\n\nTo further the evidence that our proposal of using unlabeled $x$\u2019s is powerful, we provide experiments on two more datasets Imagenet-R (consisting of renditions of Imagenet classes) and CIFAR10-C dataset. Please see the table below or refer to Section D.2. \n\nDataset |  |  | Avg.* |  |  |  |  |  | WG.*  |  |  |\n|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|\nCIFAR10-C | 0 | 25 | 50 | 75 | 100 | | 0 | 25 | 50 | 75 | 100 |\nARM   | 65.9 | 66.0 | 66.0 | 66.0 | 66.0  | | 39.3 | 39.3 | 39.4 | 39.3 | 39.4\nERM   | 66.1 | 66.1 | 66.1 | 66.1 | 66.1 | | 39.8 | 39.8  | 39.8  | 39.8  | 39.8\nICRM  | **70.6** | **71.9** |  **71.9** | **71.9** | **71.9** | | **54.6** | **56.0** | **55.8** | **55.8** | **55.9**\n|  |  |  |  |  |  |  |  |  |  |\nImageNet R | 0 | 25 | 50 | 75 | 100 | | 0 | 25 | 50 | 75 | 100 |\nARM   | 56.3 | 58.1 | 58.8 | **59.8** | 59.0  | | 47.4 | 45.3 | 47.2 | **49.8** | 47.4\nERM   | **58.9** | 58.9 | 58.9 | 58.9 | 58.9 | |**48.0** | **48.0**  | **48.0**  | 48.0  | 48.0 \nICRM   | 57.4 | **59.7** |  **59.6** | 59.4 | **60.5** | | 45.4 | **48.0** | 47.2 | 46.9 | **50.6**\n\n*Here, \u201cAvg.\u201d and \u201cWG.\u201d respectively denote the average and the worst group accuracy across test environments. \n\n[1] Brown, Tom, et al. \"Language models are few-shot learners.\" Advances in neural information processing systems 33 (2020): 1877-1901."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1022/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700079685898,
                "cdate": 1700079685898,
                "tmdate": 1700079685898,
                "mdate": 1700079685898,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FTSZFdjgVy",
                "forum": "8VPWfqtQMX",
                "replyto": "xs6P222RJm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1022/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1022/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Reviewer 1GJP (Continued)"
                    },
                    "comment": {
                        "value": "> On the assumption that testing data sequences are sampled from the same domain\n\nWhile we did mention this point in the previous draft, we acknowledge that it may not have been sufficiently emphasized. In the revised manuscript, we have reiterated this aspect multiple times. We agree that this is an assumption, if true, shows the success of the proposal. Additionally, we consider extending our approach to settings where this assumption does not hold as an exciting avenue for future research. Nevertheless, it's important to note that the current assumption is practical for many scenarios. In particular, all work in domain generalization (see DomainBed [1], WILDS [2]) follows the same fixed-test-domain protocol.\n\n[1] Gulrajani, Ishaan, and David Lopez-Paz. \"In search of lost domain generalization.\" arXiv preprint arXiv:2007.01434 (2020).\n\n[2] Koh, Pang Wei, et al. \"Wilds: A benchmark of in-the-wild distribution shifts.\" International Conference on Machine Learning. PMLR, 2021.\n\n---\n\n\n> On the difference w.r.t averaging based methods\n\nOur experiments show that a long line of work starting from the seminal works on marginal transfer learning that first appeared in NeurIPS 2011, later improved by adaptive risk minimization (NeurIPS 2021) have adopted a suboptimal choice for capturing contextual information.  Our proposal simplifies such literature in light of recently developed transformer and attention architectures. In a nutshell, we show that state-of-the-art performance is obtained by presenting all of the data as a sequence directly to the model, which can later decide to average in various non-linear ways. Further, as discussed in the paper, the size of the representation $\\phi$ would have to grow linearly with the size of the training data to ensure storing all of the information available in the data\n\n---\n\n\n> On the relationship to in-context learning in LLMs\n\nOur whole journey and thought process in this paper was inspired by LLMs and their ability to tackle OOD tasks through in-context learning. Our approach is inspired by it and is not an exact emulation of the workings of a LLM. We have added additional clarifications on this in the paper.  \n\n---\n\n> On the concern regarding Theorem 1\n\nWe are uncertain about which inequality is being referred to here.\"\n- If the reference is to $I(Y; E|X)>0$, then we emphasize that this simply states that X does not carry all the relevant information to predict Y. There is some extra information to be gained from knowing the environment one is operating in. For instance, if one is driving, the decision to speed or not should depend on the current image $X$ in front of the driver and the operating conditions (crowded city, weather, etc., that form $E$). This assumption is made in all marginal transfer learning papers.\n\n- Or perhaps the reference is to the inequality in equation (10). Perhaps your concern refers to the condition above equation 10 in the Appendix, which states $I(Y; C_t | X, E)=0$. Below, we show how to derive it. Equation 10 rests on the assumption that $X_i, Y_i$\u2019s are independent conditional on $E$. This is a fairly standard data generation assumption in most domain generalization papers (e.g., invariant risk minimization, its precursors, and many works that follow up on it). The assumption just means that within an environment, $X_i, Y_i$ samples are independent of one another. Conditional on the user, if the first digit drawn by a user is 9, that does not impact the choice of the digit it draws next.\n\nConsider two samples $(X_1,Y_1,X_2,Y_2)$ from the environment $E=e$. From conditional independence of the pair it follows that \n$P(X_1=x_1, Y_1=y_1, X_2=x_2, Y_2=y_2 | E=e) = P(X_1=x_1, Y=y_1|E=e) P(X_2=x_2, Y_2=y_2 |E=e)$\n\nWe now take the sum over $y_1$\u2019s on both sides of the above equality to obtain. \n$\\sum_{y_1} P(X_1=x_1, Y_1=y_1, X_2=x_2, Y_2=y_2 | E=e) = \\sum_{y_1}P(X_1=x_1,Y=y_1|E=e) P(X_2=x_2,Y_2=y_2 |E=e)$\n\nThe LHS simplifies to $P(X_1=x_1,X_2=x_2, Y_2=y_2|E=e)$ while the RHS simplifies to $P(X_1=x_1|E=e) P(X_2=x_2,Y_2=y_2|E=e)$. Thus we obtain \n$P(X_1=x_1,X_2=x_2, Y_2=y_2|E=e) = P(X_1=x_1|E=e) P(X_2=x_2, Y_2=y_2|E=e)$ \n\nThe RHS can be simplified further and we obtain \n$P(X_1=x_1,X_2=x_2, Y_2=y_2|E=e) =  P(X_1=x_1|E=e)P(X_2=x_2|E=e)P(Y_2=y_2|X_2=x_2,E=e)$ \n\nFrom the above equality, we obtain that $Y_2$ is independent of $X_1$  conditional on $X_2, E$. Since $X_1$ alone forms the context, $Y_2$ is independent of the context conditional on $X_2, E$. Hence, in this example $I(Y_2;C| X_2,E)=0$. The above argument readily extends to longer sequences, and thus we get $I(Y; C_t | X, E)=0.$"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1022/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700079753673,
                "cdate": 1700079753673,
                "tmdate": 1700079825496,
                "mdate": 1700079825496,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "soHrhhvmme",
            "forum": "8VPWfqtQMX",
            "replyto": "8VPWfqtQMX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1022/Reviewer_7Bph"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1022/Reviewer_7Bph"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce In-Context Risk Minimization (ICRM): an algorithm that leverages techniques from next-token predictors (i.e., \"in-context learning\") to learn a transformer-based prediction model that generalizes at test time to new unseen domains.  The authors present several theoretical results that provide intuition on the similarities and differences between their proposed approach and existing approaches in domain generalization. They also demonstrate the value of ICRM by benchmarking their algorithm against other domain generalization approaches on several datasets. \n\nMore abstractly, the authors argue that the success of their approach provides evidence that \"context is environment\": the intuitive idea considering examples from a similar environment as \"context\" can encourage the learned model to \"zoom in\" to control for environment-specific features, while also learning environment-agnostic invariant features."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "This paper is extremely timely and would be of great interest to researchers on domain generalization and group robustness. I strongly recommend that this paper is accepted.\n\n* The authors clearly motivate, present from first principles, and connect modern research in domain generalization and in-context learning.  Their notation and exposition in Sections 2 and 3 is thoughtful and clear.  Abstract concepts are made clear using examples, such as the self-driving car in Section 2 and sentence examples in Section 3.\n* The authors' proposed algorithm (ICRM) is intuitive to implement, and described clearly.\n* The authors' theoretical results in Section 3 (with the exception of the domain generalization result Theorem 3) are relatively intuitive, and also demonstrate the conditions under which ICRM is equivalent (Prop. 1) vs. beneficial (Theorem 1) over global ERM.\n* The authors provide detailed information about their experimental design and instantiation of each algorithm being benchmarked so that their experiments are clearly reproducible.\n* The authors' proposed method appears to result in significant performance gains over comparable domain generalization algorithms, which suggests its ability to have positive impact in practice.\n* I appreciate the authors' efforts to thoughtfully connect more abstract interpretations of how \"environment is context\" to existing literature from causality that studies similar phenomena (at the end of Section 5).  I found the smoking example to be an especially clear analogy that helped me better understand the motivation behind the approach."
                },
                "weaknesses": {
                    "value": "I am happy to consider raising my score if the authors address the below concerns.\n1. **Choice of datasets for evaluation + including experiments on \"harder\" datasets with spurious correlations that vary across environments**. \n  * My basic sense of the datasets that the authors chose to benchmark the value of their domain generalization method on is that there is arguably not *that* high variance in what semantic features are present across environments (e.g., simple rotations or corruptions are somewhat artificial shifts that are detached from reality). To really buy the author's claim that this approach is relevant for domain generalization, I would love to see experimental results on prominent spurious correlation benchmarks, such as those used in [1] (e.g., CivilComments, WaterBirds, CelebA).  I understand that several of these benchmarks only have 2 total test environments (so are perhaps unsuitable for domain generalization specifically), but can you attempt to include additional results on a spurious correlation benchmark (or justify why it's not appropriate)? I'm curious if on more difficult benchmarks, the model learned by ICRM may undesirably \"zoom-in on toxic spurious correlations\".\n2. **Weakening claims about \"what ICRM has learned\"**.  \n  * Unless the authors attempt to peek under the hood (using post-hoc explanation methods or by designing controlled experiments) to probe at what features the ICRM models have \"learned\", I suggest that the authors make clear that some of their claims that their models learn \"invariant\" or \"contextual features\", are hypotheses/intuition (rather than assertions of the truth). ex: in 6.1, you state that the gains are \"because ICRM training still benefits from contexts as to find contextual features that ERM ignores\".  I think this is a bit too strong (what are the \"contextual\" invariant features for your experiments using real data, for example?).  Perhaps soften to \"we hypothesize that this is because\u2026\"\n3. **More discussion of how to use ICRM in practice**, i.e., the nuances of what test set data to present as belonging to the same \"environment\" (/context) vs. different environments.  IMO this approach is perfect for domain generalization settings where the \"environment\" is literally the institution in which the algorithm is being deployed (ex: I begin using it in a new hospital), but has way more challenges when we consider \"environments\" as naturally occurring subpopulations or time-based shifts.  Can you please include additional discussion within your paper about how one would decide what examples to group together as being in the same \"environment\" in practice? I also thought it was a really cool result that you don't need to observe labels in order to include examples in the same \"context\" that should be emphasized further in such a practical discussion.\n\n\n[1] https://proceedings.mlr.press/v177/idrissi22a/idrissi22a.pdf"
                },
                "questions": {
                    "value": "In addition to the above 3 weaknesses which are most important to my score, I also had several comments, questions, and suggestions about places I got confused when reading the paper that can be made more clear.  \n\n* nit: Why did you choose the title \"context is environment\"? I wonder if \"environment is context\" is more appropriate, as you're aiming to solve domain generalization using in-context learning, and treating environment as context.  \"Context is environment\" to me implies that you are labeling \"context\" as environment to feed as input to domain generalization algorithms. \n* nit, Intro: Is the expectation here that we have environment labels available during training time? \u2013 when you state that \"simple empirical risk minimization\" is a strong baseline, do you mean just training using ERM on the entire dataset in an environment-agnostic way? (As a reader, I would need to dig into the references you've cited here to clarify). EDIT:  I see in Section 2 that you mean a global ERM that \"pools the data together\".  Can you clarify this earlier in the intro?\n* nit, Intro: Can you make more clear, perhaps using an example, what it means to \"reveal\" an \"invariance\"?  By an \"invariance\", do you mean some semantic feature that is predictive of the true label $y_i^e$ across all environments $e$?  By \"reveal\", do you mean that you discover that your in-context learner models actually \"use\" the invariant features to make predictions, or something else?\n* Section 2: \"the size of the representation would have to grow linearly with the size of the training data to describe aspects corresponding to a small group of examples, such as extreme value statistics\". I had trouble interpreting this sentence.  What do you mean by \"size\" here \u2013 do you mean the dimensionality of each $\\phi_i^e$?  Why are more dimensions necessary for the representations to encode relevant information?  Why do we care about \"describing aspects corresponding to a small group of examples\" \u2013 is it because we have fewer samples from some environments relative to others?  Is the problem that we must use the same function $\\phi$ across all the environments?\n* Section 5: \"define $\\delta_e$ to be a permutation of $\\gamma_e$ that swaps its two components\".  What do you mean by \"two components\" \u2013 do you mean swaps the parameters for $y = 0$ and $y = 1$?\n* Section 5: I may be missing something, but I'm not sure if I understand the linear least-squares example given in Equation 7.  Is it fair in this case to compare ICRM to \"global ERM\", which in many cases will fail to learn to add environment-specific averages $\\mu_e^i$? Does this phenom (that the true model can be learned) not hold for simple per-environment ERM too?  I am unsure of how illustrative this example (that ICRM tends to learn the true $\\alpha$ vs. some $\\alpha'$) is for the overarching claim that \"ICRM learns invariant features\".\n* Section 5, last paragraph: I don't really understand the value of including the last sentence here (\"when constraining the environment to only one smoker, the outcome of lung cancer disease invariably follows\").  Are you trying to suggest that when we have fewer environments (a \"smaller diameter\"), that invariance is easier to achieve? Can you provide more practical context around this point?\n* Section 6: I am confused by why in Figure 2, you show images that appear (from my eye) to be identical to the query, and calculate the attention score (e.g., between the two rotated 2s or Js).  Can you clarify exactly how the images on the right of the line are \"augmented\"?  Are you certain that these augmentations were significant enough?  Does it make sense for the model to see two almost identical images in the set of test examples?\n* Section 6: nit: In Figure 2, can you write (in small font) what the true labels are for each image, rather than the integer indices? (or at least for the query images?)\n* Section 7: \"we enable learning machines to fully exploit data in natural order\". I question if you should emphasize this point in your conclusion, as you didn't actually present any of the data \"in natural order\" (i.e., chronologically from its creation date) in any of your experiments."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1022/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1022/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1022/Reviewer_7Bph"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1022/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699541122300,
            "cdate": 1699541122300,
            "tmdate": 1699636028041,
            "mdate": 1699636028041,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "43Jw0DyMKC",
                "forum": "8VPWfqtQMX",
                "replyto": "soHrhhvmme",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1022/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1022/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "First of all, we would like to thank the reviewer for their diligent and insightful review and their very encouraging words. The changes in the manuscript can be found highlighted in magenta. \nWe share our thoughts on the questions asked below. \n\n---\n\n> On the choice of evaluation and including experiments on \u201charder\u201d datasets with spurious correlations\n\n- In the literature, distribution shifts are often divided into two categories: diversity shift and correlation shift [1]. Our work primarily focuses on diversity shift, deferring the exploration of correlation shift to future research. Handling datasets like Waterbirds is challenging, as the typical evaluation assumes test data sampled i.i.d from all environments (subpopulations), making a 3:1 training-to-testing split nonstandard. Further, adapting ICRM to such a setting requires significant modifications to the training pipeline and additional analysis. Therefore, our focus is on domain generalization benchmarks, where all environments except one are used for training, aligning with our assumption of _only one environment during testing_.\n- To test the model in other hard settings, we have included the ImageNet R dataset [2], which consists of challenging renditions of Imagenet R classes, and the CIFAR10-C dataset. In both datasets, we observe consistent improvements offered by ICRM (see table below and also in the Appendix of the edited draft). \n\n\nDataset |  |  | Avg.* |  |  |  |  |  | WG.*  |  |  |\n|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|\nCIFAR10-C | 0 | 25 | 50 | 75 | 100 | | 0 | 25 | 50 | 75 | 100 |\nARM   | 65.9 | 66.0 | 66.0 | 66.0 | 66.0  | | 39.3 | 39.3 | 39.4 | 39.3 | 39.4\nERM   | 66.1 | 66.1 | 66.1 | 66.1 | 66.1 | | 39.8 | 39.8  | 39.8  | 39.8  | 39.8\nICRM  | **70.6** | **71.9** |  **71.9** | **71.9** | **71.9** | | **54.6** | **56.0** | **55.8** | **55.8** | **55.9**\n|  |  |  |  |  |  |  |  |  |  |\nImageNet R | 0 | 25 | 50 | 75 | 100 | | 0 | 25 | 50 | 75 | 100 |\nARM   | 56.3 | 58.1 | 58.8 | **59.8** | 59.0  | | 47.4 | 45.3 | 47.2 | **49.8** | 47.4\nERM   | **58.9** | 58.9 | 58.9 | 58.9 | 58.9 | |**48.0** | **48.0**  | **48.0**  | 48.0  | 48.0 \nICRM   | 57.4 | **59.7** |  **59.6** | 59.4 | **60.5** | | 45.4 | **48.0** | 47.2 | 46.9 | **50.6**\n\n*Here \u201cAvg.\u201d and \u201cWG.\u201d respectively denote the average and the worst group accuracy across test environments. \\\n\n[1] Ye, Nanyang, et al. \"Ood-bench: Quantifying and understanding two dimensions of out-of-distribution generalization.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n\n[2] Hendrycks, Dan, et al. \"The many faces of robustness: A critical analysis of out-of-distribution generalization.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.\n\n---\n\n> On weakening claims about \u201cWhat ICRM has learned\u201d\n\nThis is indeed a great point\u2014we have weakened our exposition to explicitly emphasize that it is our hypothesis that ICRM exhibits a propensity to learn more invariant predictors. The post-hoc analysis of attention maps in Section 6.3 complements it empirically. \n\nTo substantiate this further,  we extract embeddings from the penultimate layer of the model trained using our approach of the data pooled together from training environments. Then, we use these embeddings to train a linear classifier that predicts the corresponding environment index. The results (details in Section D.4) demonstrate our approach's success, achieving accuracy rates exceeding 75% on FEMNIST and 98% on Rotated MNIST. This shows that context can help us infer the environment and hence, reinforces the notion of _context is environment_.\n\n---\n\n> More discussion on how to use ICRM in practice\n\nYou make an excellent point on the type of distribution shifts our method is suited for. In datasets such as Waterbirds, the test domain is a mixture of hidden groups/domains and not separated into domains based on the groups. Extending our method to tackle distribution shifts in slowly-changing test environments and changes in subpopulation such as Waterbirds, CelebA type dataset is an exciting future work. We have added this discussion in the the revised manuscript\n\n---\n\n> On \u201cContext is Environment\u201d vs. \u201cEnvironment is Context''  \n\nWe consider the environment to be a more fundamental primitive. We believe that the data generation process in nature begins with sampling of an environment, followed by the generation of samples from this environment, forming the context. Due to this, it is the context that helps us infer the environment and hence context is environment.\n\n---\n\n> On clarification of ERM \n\nIn our paper, Empirical Risk Minimization (ERM) refers to the training of a single model by minimizing the error across the entire pooled dataset. We have incorporated the recommended clarification in the manuscript.\n \n\n---"
                    },
                    "title": {
                        "value": "Response to the Reviewer 7Bph"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1022/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700076538899,
                "cdate": 1700076538899,
                "tmdate": 1700076666787,
                "mdate": 1700076666787,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "36IgMBc31h",
                "forum": "8VPWfqtQMX",
                "replyto": "soHrhhvmme",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1022/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1022/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Clarification on \u201creveal\u201d an \u201cinvariance\u201d \n\nBy \u201cinvariance\u201d we mean a feature representation inducing a classifier that performs well across environments. By \u201crevealing\u201d we mean that the learner can find and use such an invariance..\n\n---\n\n> On the linear growth in the size of the representation\n\nIn this particular case, we aimed to highlight the limitations of marginal transfer learning proposals that use non-linear averaging on the inputs. By size, we mean the dimensionality of the representation $\\phi(x)$. Appendix Section A.7 presents a family of examples where such marginal transfer learning proposals fail. In particular, we give an example of function classes for which the size of $\\phi(x)$ has to grow in the context length to perfectly match the function class. The overarching intuition here is that one would need an $\\theta(n)$-dimensional $\\phi(x)$ to ensure storing all of the information available in $n$ examples.\n\n---\n\n> Clarifying the line on swapping two components\n\nYes, we mean it swaps parameter tuples corresponding to $y=0$ and $y=1$.\n\n---\n\n> On linear least squares example\n\nWe assume in this example that ICRM directly operates on the features summarizing the means $\\mu$. ERM in the standard form only operates on the current query and not on an extended feature space. This example illustrates how ICRM by operating on an extended feature space can discover invariances for features in the original input space. This is as opposed to a global ERM across environments, or any ERM on a separate environment.\n\n---\n\n> On the example of smoking and invariance\n\nThe smoking example attempts to illustrate the fact that, as we consider growing collections of examples (smokers), we need to mine more and more features to find an invariant map\u2014for instance, consider the enormous amount of inputs that we would need to invariantly predict the outcome of disease for all smokers in the world with one single model. On the other extreme, if considering one smoker only, the invariant rule is constant and equal to the observed disease outcome. This illustrates a tension between how many environments one desires to be invariant across ($|E|$) and how many features are necessary to guarantee such invariance ($|C|$).\n\n---\n\n> Clarification about images in the input sequences in Figure 2\n\nFigure 2 displays four random sequences, each containing images randomly sampled from _real data_ in a test environment. It's important to note that none of the images in these sequences were manually augmented. The key takeaway from this visualization is that ICRM effectively learns to attend to a select few samples in an input sequence. These samples either belong to the same class or exhibit similar features, despite potentially belonging to different classes.\n\n---\n\n> True labels for each image in Figure 2\n \nWe thank the reviewer for this suggestion. Using the label mapping [here](https://gist.github.com/aaronpolhamus/964a4411c0906315deb9f4a3723aac57), we have replaced the class indices with their true labels in Figure 2. Clearly, the third row shows that the model, when presented with a query image of a \"bullet train\", attends not only on other trains but also on a \"school bus\"---indicating a semantic understanding of similarity. Further, in the last row, since the associated image from class \"volleyball\u201d in the query, also features a volleyball player, the model learns to pay attention to other images with individuals, including those wearing academic gowns. \n\n---\n\n> On the natural order of data point in the conclusion\n\nExisting datasets in domain generalization only carry environment-level separation, and the proposal is designed to handle the partial ordering of data dictated by this separation. However, if the data had more refined information (e.g., time-based ordering in video), the proposal can exploit that too. We have clarified this in the revised manuscript. In sum, our remark about \u201cnatural order\u201d is made to motivate future data collectors to retain this precious time metadata, so methods such as ICRM can be deployed in full force.\n\n---"
                    },
                    "title": {
                        "value": "Response to the Reviewer 7Bph (Continued)"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1022/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700076604061,
                "cdate": 1700076604061,
                "tmdate": 1700076687759,
                "mdate": 1700076687759,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NQiEBELuO5",
                "forum": "8VPWfqtQMX",
                "replyto": "soHrhhvmme",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1022/Reviewer_7Bph"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1022/Reviewer_7Bph"
                ],
                "content": {
                    "title": {
                        "value": "Response to the rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your prompt and detailed response to each of my questions and concerns!  Thank you also for updating your draft for clarity, and for also including the additional experimental results on ImageNet-R and CIFAR10-C.  The authors' responses have sufficiently addressed my concerns, and after reviewing the larger discussion at this time I still believe this paper should be accepted."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1022/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700364668684,
                "cdate": 1700364668684,
                "tmdate": 1700364668684,
                "mdate": 1700364668684,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]