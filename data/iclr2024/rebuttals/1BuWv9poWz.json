[
    {
        "title": "Enhancing Transferable Adversarial Attacks on Vision Transformers through Gradient Normalization Scaling and High-Frequency Adaptation"
    },
    {
        "review": {
            "id": "VDJXwQfP95",
            "forum": "1BuWv9poWz",
            "replyto": "1BuWv9poWz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5069/Reviewer_Qpbi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5069/Reviewer_Qpbi"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method to increase the tranferrability of adversarial attacks across Transformer models. The key idea is to attenuate mild gradients and to do frequency adaptive perturbation of the input signal.\n\nAfter rebuttal, I decided to increase my score to reflect the answers."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The comperhensive numerical results in the paper across a number of architectures and attack methods shows the benefits of the approach, consistently achieving very high transferrability scores. The method itself seems quite simple to implement."
                },
                "weaknesses": {
                    "value": "This paper was written in a manner which made it very difficult for me to follow the exact approach. I have a number of questions below which I hope the authors will address. I am not up to date on the latest adversarial literature and therefore it may be that I have missed obvious ideas, but nevertheless I think the authors should write the paper for a general ICLR audience rather than adversarial sub-field experts. I am willing to re-visit my rating if the authors can provide satisfactory answers to the questions below, which are mostly to do with the very opaque setup in the paper."
                },
                "questions": {
                    "value": "Questions:\n\n1. What are \"strong\" and \"mild\" gradients? These terms are assumed to be understood by the reader, but never explicitly defined. At the least, one would expect some informal definition to give the reader some intuition.\n\n2. The definition of \"channels\" in a Transformer model is unclear. For conv nets, it is obvious what this refers to. It seems to refer to the number of attention heads, but it was not too clear.\n\n3. It is unclear why attentuating certain types of gradients (\"mild\") leads to better transferability. Is there an intuition that the authors can provide for this phenomenon?\n\n4. The HFA method was quite unclear to me (and I guess it will be to many readers). \n\n5. How are the GNS and HFA methods combined?  \n\n6. What exactly do the results in Table 1 show us? Is there one model on which the adversarial attacks were generated, and the remaining were the rates of success?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5069/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5069/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5069/Reviewer_Qpbi"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5069/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698623679106,
            "cdate": 1698623679106,
            "tmdate": 1700600178400,
            "mdate": 1700600178400,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2dqFvoE5xR",
                "forum": "1BuWv9poWz",
                "replyto": "VDJXwQfP95",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5069/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5069/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official reply to Reviewer Qpbi (Part 1)"
                    },
                    "comment": {
                        "value": "**Questions:**\n\n1.Due to the internal structure differences between Vision Transformers (ViTs) and Convolutional Neural Networks (CNNs), traditional transferable attack methods designed for CNNs often yield suboptimal results when crafting adversarial samples to attack ViTs. \n\nThe TGR method, as proposed by [1], attributes the limitation of adversarial sample transferability to the overfitting caused by \u2018extreme gradients\u2019 during the backpropagation of ViT models. Specifically, \u2018extreme gradients\u2019 are defined as tokens whose backpropagated gradient magnitudes rank in the top-$k$ or bottom-$k$ among all tokens, with $k$ being a hyperparameter. \n\nTo reduce the variance in gradient backpropagation and enhance adversarial sample transferability, TGR regularizes the \u2018extreme gradients\u2019 corresponding to the top-$k$ or bottom-$k$ magnitudes. \n\nOur experiments, illustrated in **Figure 3 of the paper** and further validated in **Table 1 of the global comments**, demonstrate that, compared to the \u2018extreme gradients\u2019 addressed by TGR, small gradients, referred to as \u2018mild gradients\u2019, have more significant impacts leading to overfitting during gradient backpropagation.\n\nIn this study, we employ the threshold $\\mu+u * \\sigma$ to differentiate between \u2018mild gradients\u2019 and \u2018extreme gradients\u2019, where $\\mu$ represents the average value across $C$ channels, $\\sigma$ represents the standard deviation across $C$ channels, and the hyperparameter $u$ defines the allowed deviation level. Gradients with magnitudes less than $\\mu+u * \\sigma$ are considered \u2018mild gradients\u2019. By adjusting the hyperparameter $u, we can alter the boundary for classifying \u2018mild gradients\u2019. \n\nOn the one hand, our classification of \u2018mild gradients\u2019 implies lower gradient magnitudes compared to the \u2018extreme gradients\u2019 associated with top-$k$ or bottom-$k$ magnitudes. Thus, normalizing and scaling these \u2018mild gradients\u2019 causing overfitting does not significantly impact the success rate of attacks. On the other hand, ablation experiments in Section 5.3 of the manuscript demonstrate that our method is not highly dependent on the choice of the $u$ parameter. After normalizing and scaling the majority of \u2018mild gradients\u2019, our algorithm achieves outstanding performance, providing further evidence that \u2018mild gradients\u2019 are the primary contributors to overfitting. Since removing the vast majority of \u2018mild gradients\u2019 is sufficient, the algorithm's performance remains largely unchanged with variations in the parameter $u$.\n\n2.We are pleased to address the reviewer's inquiries. In ViT, images are initially divided into fixed-size \u2018patches\u2019. Each patch is linearly mapped to a lower-dimensional vector, which serves as the input to the model. In this context, for ViT models employing convolution as Q (queries), K (keys), and V (values), the term \u2018channel\u2019 refers to the number of channels in the convolutional kernel. On the other hand, for ViT models utilizing fully connected layers, \u2018channel\u2019 denotes the number of heads in the multi-head attention mechanism.\n\n\u200b\u200b3. Following we provide an intuitive explanation for the phenomenon of \u2018mild gradients\u2019 causing overfitting. Our goal is to train locally adversarial samples on the ViT model and transfer them to attack other ViT or CNN models effectively. It means that, we would like to see less overfitting occurring to the local surrogate ViT model during the training of adversarial samples. This may happen, if the model can learn largely with the gradients that are not model-specific and unstable. This ensures that transferable adversarial samples can effectively cross the decision boundaries of other ViT or CNN models, thereby misleading model decisions.\n\nGiven the inherent structure differences between ViTs and CNNs, ViTs employ self-attention mechanisms to capture both global and local information in images. This mechanism makes the model highly sensitive to small perturbation in the input during the learning process. If gradients are too small, the update steps will be relatively small, causing the model to be sensitive to smaller perturbation in the training data. This may result in overfitting, subsequently affecting the quality of locally trained adversarial samples.\n\nOn the other hand, \u2018mild gradients\u2019 may cause ViTs to get stuck in local optima during the learning process, making the model more prone to overfitting. For deeper ViT models, \u2018mild gradients\u2019 may also contribute to the vanishing gradient problem, preventing effective updates to the lower-level attention mechanisms thereby impacting the training process for adversarial samples."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5069/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700187251023,
                "cdate": 1700187251023,
                "tmdate": 1700187251023,
                "mdate": 1700187251023,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CJCl9AUpXs",
                "forum": "1BuWv9poWz",
                "replyto": "VDJXwQfP95",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5069/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5069/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official reply to Reviewer Qpbi (Part 2)"
                    },
                    "comment": {
                        "value": "4.Due to the internal structure differences between ViT and CNN models, as illustrated in **Figure 2**, we have demonstrated that ViT, unlike CNN models, tends to focus more on high-frequency information. Therefore, it becomes essential to explore the high-frequency regions in the frequency domain to which ViT models are more sensitive, as these regions often contain crucial features influencing model decisions. Firstly, as shown in Equation 7, we utilize Discrete Cosine Transformation (DCT) [2] to convert the spatial information of the perturbed original image into frequency domain information. We introduce randomness through a normal distribution and use the mask constructed by Equation 6 to ensure the exploration of more high-frequency information. Secondly, we employ Inverse Discrete Cosine Transformation (IDCT) [2] to transform the explored high-frequency information back into spatial information. Subsequently, we perform frequency domain exploration for $N$ iterations. We calculate the average gradient of the $N$ samples after frequency domain exploration using Equation 8. The $sign$ function is then applied to determine the gradient update direction corresponding to the average gradient. Finally, we use High Frequency Adaptation (HFA) to guide and adjust the gradient ascent update process for adversarial samples. The gradient update direction obtained through HFA is tailored to the internal structure of ViT models, ensuring that each update moves the gradient in the most adversarial direction. It is worth noting that we have also demonstrated that GNS-HFA achieves optimal results on traditional CNN models.\n\n5.As mentioned in the previous responses, we observed that in the backpropagation of ViT models, \u2018mild gradients\u2019 are more prone to overfitting compared to \u2018extreme gradients\u2019. Additionally, \u2018mild gradients\u2019 have lower gradient magnitudes compared to \u2018extreme gradients\u2019, so altering \u2018mild gradients\u2019 does not have a significant impact on the adversarial nature of the samples. \n\nWe mitigate the occurrence of overfitting in backward propagation by normalizing and scaling \u2018mild gradients\u2019, thereby enhancing the transferability of adversarial samples trained on the local ViT model. This forms the basis of our Gradient Normalization Scaling (GNS) approach. Building upon GNS, and recognizing that ViT models emphasize high-frequency information more than CNN models, we aim to explore the high-frequency regions of ViT models. \n\nWe further utilize High Frequency Adaptation (HFA) to guide and adjust the update process of adversarial samples by incorporating the explored high-frequency information. This ensures that the generated adversarial examples are more adaptable to the structure of the ViT model, and thus can more stably cross the decision boundary of the model in transferable attacks. \n\nTherefore, we first normalize and scale \u2018mild gradients\u2019 through GNS, and subsequently employ the high-quality gradient information obtained from GNS for frequency domain exploration. This process ultimately leads to the creation of highly transferable adversarial samples.\n\nWe conducted additional ablation experiments for GNS or HFA in **Table 2** of the global comments. The experimental results indicate that, compared to attack methods without either GNS or HFA (average success rate of 42.24%), both GNS and HFA play nearly equally crucial roles in enhancing the transferability of adversarial samples (averaging 62.49% and 62.40%, respectively). Combining GNS and HFA yields the best algorithm performance (average 73.20%), demonstrating that gradient normalization and scaling for 'mild gradients', coupled with frequency-domain exploration, effectively improves the transferability of adversarial samples. The results of the ablation experiments align with our assumptions regarding the roles played by GNS and HFA."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5069/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700187406582,
                "cdate": 1700187406582,
                "tmdate": 1700187406582,
                "mdate": 1700187406582,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ETdmI9fEF7",
                "forum": "1BuWv9poWz",
                "replyto": "VDJXwQfP95",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5069/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5069/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official reply to Reviewer Qpbi (Part 3)"
                    },
                    "comment": {
                        "value": "6.Thanks for highlighting the concern for result discussion. \n\nTable 1 serves as a commonly employed quantitative analysis of the success rates of transferable adversarial attacks, as also discussed in [3] and [4]. In the first column labeled \u2018Surrogate Models\u2019, various methods (such as TGR [1], SSA [4], etc., as indicated in the table) are used to train adversarial samples. These samples are then directly transferred to other ViT or CNN target models to assess the attack success rates. A higher attack success rate implies greater transferability. More information about the experimental settings can be found in [3], [4]. \n\nWe will revise the discussion of the experimental design in the new version accordingly.\n\nReferences:\n\n[1] Zhang, J., Huang, Y., Wu, W., & Lyu, M. R. (2023). Transferable Adversarial Attacks on Vision Transformers with Token Gradient Regularization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 16415-16424).\n\n[2] Ahmed, N., Natarajan, T., & Rao, K. R. (1974). Discrete cosine transform. IEEE transactions on Computers, 100(1), 90-93.\n\n[3] Zhang, J., Wu, W., Huang, J. T., Huang, Y., Wang, W., Su, Y., & Lyu, M. R. (2022). Improving adversarial transferability via neuron attribution-based attacks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 14993-15002).\n\n[4] Long, Y., Zhang, Q., Zeng, B., Gao, L., Liu, X., Zhang, J., & Song, J. (2022, October). Frequency domain model augmentation for adversarial attack. In European Conference on Computer Vision (pp. 549-566). Cham: Springer Nature Switzerland."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5069/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700187493330,
                "cdate": 1700187493330,
                "tmdate": 1700187493330,
                "mdate": 1700187493330,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wAR2UODLNc",
                "forum": "1BuWv9poWz",
                "replyto": "ETdmI9fEF7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5069/Reviewer_Qpbi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5069/Reviewer_Qpbi"
                ],
                "content": {
                    "title": {
                        "value": "My concerns are addressed."
                    },
                    "comment": {
                        "value": "Thank you for a detailed answer to my questions."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5069/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700600142195,
                "cdate": 1700600142195,
                "tmdate": 1700600142195,
                "mdate": 1700600142195,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "waXh4QqsCg",
            "forum": "1BuWv9poWz",
            "replyto": "1BuWv9poWz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5069/Reviewer_6tAt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5069/Reviewer_6tAt"
            ],
            "content": {
                "summary": {
                    "value": "In order to enhance the transferability of adversarial attacks on ViTs, this paper introduces a novel gradient normalization scaling method for fine-grained gradient editing. After calculating the distribution of gradients, tanh is used for scaling the gradients that are considered as prone to cause overfitting and have minimal impact on attack capability. A high frequency adaptation method is proposed to explore the sensitivity of ViTs to adversarial attacks in different frequency regions, on the premise that ViTs shows different attention areas from CNNs in frequency. DCT transformation is conducted to obtain high frequency features, and then reverse transformation is put afterward to feed into the network for backpropagation. From the comparison of Attack Success Rates on ViT and CNN models, this work achieves better performance, enhancing the transferability of adversarial samples on ViTs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper comes up with a novel problem space.\n2. Utilizing normalization, or scaling is innovative.\n3. Achieved better attack transferability performance than SOTA methods."
                },
                "weaknesses": {
                    "value": "1.\tLack of soundness of mild gradients and extreme gradients.\n2.\tNormalization and scaling process is insufficiently demonstrated.\n3.\tMiss rationale of utilizing high frequency. Is frequency transformation for better ViTs or for better attacks?\n4.\tAblation study is insufficient to evaluate the impact of your two components on the final method performance and verify their importance.\n5.\tSome sentences contain grammatical errors, such as missing subjects."
                },
                "questions": {
                    "value": "1.\tIn your Abstract, after a one-sentence introduction to ViT, quickly talk about enhancing the transferability of adversarial attacks on ViTs is somewhat discontinuous. Adding an introduction to adversarial attacks in between would make it smoother. There are many more logical breaks like this. \n2.\tStructure illustration figure can be more detailed/comprehensible. Figure annotations could provide more explanation. \n3.\tThere should be more explanations about how mild gradients and extreme gradients in Figure 3 react to your specific parameters (briefly introduce u here instead of later), and why mild gradients are the easiest to overfit. Besides, why is \u00b5 + u \u2217 \u03c3 the watershed between mild and extreme gradients? \n4.\tNormalization and scaling seem to be one and the same.\n5.\tGive more arguments for tanh and frequency transformation.\n6.\tGNS-HFA is a combination, so does each part fit your hypothesis? What role do they play? Which one contributes more?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5069/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698936767107,
            "cdate": 1698936767107,
            "tmdate": 1699636497165,
            "mdate": 1699636497165,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hSj10Sniux",
                "forum": "1BuWv9poWz",
                "replyto": "waXh4QqsCg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5069/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5069/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official reply to Reviewer 6tAt (Part 1)"
                    },
                    "comment": {
                        "value": "**Weaknesses:**\n\n1.In **Section 4.1 of our paper**, we have demonstrated a distinction from TGR [1], which posits that \u2018extreme gradients\u2019 cause overfitting in the backpropagation process. Differently, we found that the small gradients, referred to as \u2018mild\u2019 gradients, are more prone to overfitting in backpropagation. Normalizing and scaling these \u2018mild\u2019 gradients have a relatively lower impact on sample attackability compared to TGR, which directly normalizes \u2018extreme gradients\u2019.\n\n2.Experimental results on ViTs and CNNs models validate the effectiveness of our method to normalize and scale \u2018mild gradients\u2019 in enhancing the transferability of adversarial samples. In Section 4.1 of the paper, we provide a detailed explanation for the normalization and scaling process. As illustrated in **Figure 3 in our paper**, we observe that \u2018mild gradients\u2019 are more susceptible to overfitting in backpropagation compared to the \u2018extreme gradients\u2019 described by TGR. \n\nTherefore, we normalize 'mild gradients.' Subsequently, we employ the tanh activation function to map bias values. If there is a noticeable bias in the gradient information, it indicates the crucial role of relatively larger gradients in attack capability, which should not be eliminated. Hence, using the tanh activation function achieves adaptive scaling of gradients.\n\n3.As depicted in **Figure 2 of the paper**, we visualized the frequency domain attribution map of the ViT model. Our observation reveals that, in contrast to traditional CNN models, the ViT model tends to focus more on high-frequency information. Therefore, we aim to guide and adjust the gradient update direction of adversarial samples through frequency domain transformation, to be able to construct adversarial samples with higher transferability for black-box attacks.\n\n4.We conducted additional ablation experiments for GNS or HFA in **Table 2 of the global comments**. The experimental results indicate that, compared to attack methods without either GNS or HFA (average success rate of 42.24%), both GNS and HFA play nearly equally crucial roles in enhancing the transferability of adversarial samples (averaging 62.49% and 62.40%, respectively). The strategy to combine GNS and HFA together yields the best algorithm performance (average 73.20%), demonstrating that gradient normalization and scaling for 'mild gradients', coupled with frequency-domain exploration, effectively improves the transferability of adversarial samples. The results of the ablation experiments align with our assumptions regarding the roles played by GNS and HFA.\n\n5.We will thoroughly proof-read the paper.\n\n**Questions:**\n\n1.Currently, Vision Transformers (ViTs) have successfully adapted the self-attention mechanism from Transformers for the generation of high-quality images, playing an increasingly crucial role in computer vision tasks. However, akin to Convolutional Neural Networks (CNNs), ViTs are susceptible to the influence of adversarial samples, raising security concerns in real-world applications. \n\nAs one of the most effective black-box attack methods, transferable adversarial attacks, given their ability to generate adversarial samples on surrogate models and directly transfer them to the target model for attacks without accessing the target model parameters, have become a notable threat. However, due to the distinct internal structures of ViTs and CNNs, adversarial samples constructed by traditional transferable attack methods can not be generalized to ViTS. Therefore, it is imperative to propose more effective transferability attack methods to unveil latent vulnerabilities in ViTs.\n\nWe will refine the narrative structure of the abstract as discussed above.\n\n2.We will revise the structure diagram for better clarity, and improve the representation of legends to provide readers with a clearer understanding of our methodology."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5069/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700187012740,
                "cdate": 1700187012740,
                "tmdate": 1700187012740,
                "mdate": 1700187012740,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WfdcmyeuNU",
                "forum": "1BuWv9poWz",
                "replyto": "waXh4QqsCg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5069/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5069/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official reply to Reviewer 6tAt (Part 2)"
                    },
                    "comment": {
                        "value": "3.We are willing to provide additional ablation experiments on other ViT models to demonstrate the phenomenon that 'mild gradients' are more prone to overfitting compared to 'extreme gradients'. The experiments are presented in **Table 1 of the global comments**. \n\nTo address the question of why \u2018mild gradients\u2019 are more likely to cause overfitting in backpropagation, we conducted experiments on the original ViT model without GNS and HFA modules, evaluating the impact of \u2018mild gradients\u2019 and \u2018extreme gradients\u2019 on the stability of backward propagation. \n\nAs shown in **Figure 2 in the paper**, removing \u2018mild gradients\u2019 results in a greater improvement in attack success rate compared to removing \u2018extreme gradients\u2019, indicating that \u2018mild gradients\u2019 are more prone to overfitting, thereby affecting the training of adversarial samples on the surrogate model.\n\nAn intuitive explanation for the increased susceptibility of \u2018mild gradients\u2019 to overfitting is that, in training highly transferable adversarial samples on ViT models for subsequent attacks on other ViT or CNN models, we aim for minimal overfitting to occur on the local surrogate ViT model. In other words, the model should learn gradients that are not model-specific and unstable to ensure the transferability of samples across decision boundaries of other ViT or CNN models, thereby misleading model outputs.\n\nDue to the differing internal structures of ViTs and CNNs, ViTs employ self-attention mechanisms to capture both global and local information in images. This mechanism renders the model highly sensitive to subtle perturbation in inputs during the training process. If gradients are too small, the update step is relatively small, making the model excessively sensitive to small changes in the training data, thus making overfitting more likely and affecting the quality of locally trained adversarial samples.\n\nOn the other hand, \u2018mild gradients\u2019 may lead ViTs to fall into local minima during the learning process, making the model more prone to overfitting. For deeper ViT models, \u2018mild gradients\u2019 may also contribute to the vanishing gradient problem, preventing effective updates to the lower-level attention mechanisms and thereby affecting the training process of adversarial samples.\n\nRegarding the thresholding between \u2018mild gradients\u2019 and \u2018extreme gradients\u2019, we propose using $\\mu+u * \\sigma$ for this purpose, where $\\mu$ represents the average value across $C$ channels, $\\sigma$ represents the standard deviation across $C$ channels, and the hyperparameter $u$ denotes the allowed deviation level.  We consider gradient values smaller than $\\mu+u * \\sigma$ as \u2018mild gradients\u2019, which should be normalized and scaled. By adjusting the hyperparameter $u$, we can correspondingly adjust the range of \u2018mild gradients\u2019. It is worth noting that the ablation experiments on the parameter $u$ in Section 5.3 demonstrate that our method is not heavily dependent on the choice of $u$. After normalizing and scaling the majority of \u2018mild gradients\u2019, our algorithm achieves excellent performance, providing additional evidence that \u2018mild gradients\u2019 are the primary cause of overfitting. Since removing the vast majority of \u2018mild gradients\u2019 is sufficient, the algorithm's performance remains largely unchanged with variations in the parameter $u$.\n\n4.We would like to clarify that normalization tends to scale gradient values to be within the range of 0 and 1, while scaling does not have such a restriction. The purpose of scaling is to reduce the impact of this portion of features on backpropagation by making the corresponding values smaller.\n\n5.We are pleased to provide the relevant parameters for $tanh$ activation function and frequency domain transformation. Actually, they can be found in the replication package. We use the parameter $self.rho$ to control the range of exploration during frequency domain transformation, and we set it to 0.5. Larger values of $self.rho$ indicate a larger exploration range during frequency domain transformation.\n\n6.Please find our detailed response to Weakness #4.\n\n[1] Zhang, J., Huang, Y., Wu, W., & Lyu, M. R. (2023). Transferable Adversarial Attacks on Vision Transformers with Token Gradient Regularization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 16415-16424)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5069/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700187119331,
                "cdate": 1700187119331,
                "tmdate": 1700187119331,
                "mdate": 1700187119331,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "E4MFd172m8",
                "forum": "1BuWv9poWz",
                "replyto": "AOf4xFJRTb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5069/Reviewer_6tAt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5069/Reviewer_6tAt"
                ],
                "content": {
                    "comment": {
                        "value": "I have read the responses. Generally, the authors tried their best to explain the confusing points in the submission, and part of points can be supported. Yet, it seems that the paper should undergo a major revision for consideration of publication. Hence, I remain the original rating."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5069/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700540089522,
                "cdate": 1700540089522,
                "tmdate": 1700540089522,
                "mdate": 1700540089522,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AOf4xFJRTb",
            "forum": "1BuWv9poWz",
            "replyto": "1BuWv9poWz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5069/Reviewer_98px"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5069/Reviewer_98px"
            ],
            "content": {
                "summary": {
                    "value": "In this submission, the authors proposed a gradient normalization scaling and high frequency adaptation for vision transformers. Specifically, the authors proposed to improve the generalization ability of ViTs by using gradient normalization. Moreover, the authors proposed a high frequency adaptation approach to guide the back-propagation in ViTs. Experimental results on several public datasets for adversarial attack have illustrated the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is easy to follow.\n2. The idea is well motivated and presented."
                },
                "weaknesses": {
                    "value": "The contribution is marginal, since the gradient normalization was demonstrated in [1], please discuss the major differences.\n\n[1] Wu, Y. L., Shuai, H. H., Tam, Z. R., & Chiu, H. Y. (2021). Gradient normalization for generative adversarial networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 6373-6382)."
                },
                "questions": {
                    "value": "Please conduct ablation studies using only GNS or HFA to demonstrate the effectiveness of those two methods."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5069/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698972684625,
            "cdate": 1698972684625,
            "tmdate": 1699636497087,
            "mdate": 1699636497087,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "B392Q5WXYi",
                "forum": "1BuWv9poWz",
                "replyto": "AOf4xFJRTb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5069/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5069/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official reply to Reviewer 98px (Part 1)"
                    },
                    "comment": {
                        "value": "**Weaknesses:**\n\nThanks for the thorough review and insightful comment on our work. \n\nWe would like to clarify that, in comparison to \u2018Gradient normalization for generative adversarial networks\u2019 [1], our approach is different, as well as the aim. \n\nFirstly, due to the difference in internal model structure between Vision Transformers (ViTs) and Convolutional Neural Networks (CNNs), conventional transferable attack methods that are effective on CNNs can not be generalized to ViTs. In particular, TGR [2] has presented that such limitations hindering the transferability of adversarial samples arise from the overfitting issue during the backpropagation process of \u2018extreme gradients\u2019 in ViT models. \n\nSpecifically, a token's back-propagated gradient is considered extreme if it ranks in the top-$k$ or bottom-$k$ gradient magnitudes among all tokens, where $k$ is a hyperparameter. \n\nTherefore, to mitigate the variance in gradient backpropagation and enhance the transferability of adversarial samples, TGR regularizes the \u2018extreme gradients\u2019 associated with the top-k or bottom-k tokens [2]. Through empirical result analysis (Figure 3 in our paper), we demonstrate that, compared to the \u2018extreme gradients'\u2019 addressed by TGR, small gradients, referred to as \u2018mild gradients\u2019, also have a more significant impact on mitigating the overfitting issue during gradient backpropagation.\n\nAn intuitive interpretation of this phenomenon is that, for Vision Transformers (ViTs), self-attention mechanisms are utilized to capture both global and local information within images. This mechanism renders the model highly sensitive to subtle variations in the input during the training process. If gradients become extremely small, the updating steps are relatively diminished, causing the model to be excessively sensitive to small perturbation in the training data. Consequently, this heightened sensitivity can lead to overfitting, impacting the quality of locally-trained adversarial samples. \n\nOn the other hand, \u2018mild gradients\u2019 may lead ViTs to converge to local optimal during the training process, making the model more susceptible to overfitting. Additionally, for a deeper ViT model, \u2018mild gradients\u2019 might exacerbate the issue of gradient vanishing, rendering the lower-level attention mechanisms ineffective in receiving meaningful updates and consequently affecting the training process of adversarial samples. Further quantitative experiments validating these assertions are provided in **Table 1 of the global comment**.\n\nWe aim to mitigate the overfitting issue during backpropagation by normalizing and scaling \u2018mild gradients\u2019, thereby enhancing the transferability of adversarial samples locally trained on Vision Transformer (ViT) models. This forms the basis of our work, Gradient Normalization Scaling (GNS). \n\nBuilding upon GNS, and recognizing ViT models' emphasis on high-frequency information over Convolutional Neural Network (CNN) models, we introduce High Frequency Adaptation (HFA). HFA explores the high-frequency regions of ViT models, utilizing the gleaned high-frequency information to adjust the update process of adversarial samples. This ensures that the generated adversarial examples are more adaptable to the structure of the ViT model, and thus can more stably cross the decision boundary of the model in transferable attacks. Consequently, we utilize the refined gradient information obtained after applying GNS for frequency-domain exploration, ultimately generating adversarial samples with heightened transferability.\n\nWe will include the discussion with [1] in our revision, to provide a more comprehensive analysis."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5069/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700186798408,
                "cdate": 1700186798408,
                "tmdate": 1700186798408,
                "mdate": 1700186798408,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "S1XRpbhwHs",
                "forum": "1BuWv9poWz",
                "replyto": "AOf4xFJRTb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5069/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5069/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official reply to Reviewer 98px (Part 2)"
                    },
                    "comment": {
                        "value": "**Questions:**\n\nWe have conducted additional ablation experiments for GNS or HFA in **Table 2 in the global comments**. The experimental results indicate that, compared to attack methods without either GNS or HFA (average success rate of 42.24%), both GNS and HFA play nearly equally crucial roles in enhancing the transferability of adversarial samples (averaging 62.49% and 62.40%, respectively). The strategy of combining GNS and HFA yields the best algorithm performance (average 73.20%), demonstrating that gradient normalization and scaling for \u2018mild gradients\u2019, coupled with frequency-domain exploration, effectively improves the transferability of adversarial samples. The results of the ablation experiments align with our assumptions regarding the roles played by GNS and HFA.\n\nReference: \n\n[1] Wu, Y. L., Shuai, H. H., Tam, Z. R., & Chiu, H. Y. (2021). Gradient normalization for generative adversarial networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 6373-6382).\n\n[2] Zhang, J., Huang, Y., Wu, W., & Lyu, M. R. (2023). Transferable Adversarial Attacks on Vision Transformers with Token Gradient Regularization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 16415-16424)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5069/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700186862212,
                "cdate": 1700186862212,
                "tmdate": 1700186862212,
                "mdate": 1700186862212,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]