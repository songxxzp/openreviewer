[
    {
        "title": "OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text"
    },
    {
        "review": {
            "id": "SXqlalFtHf",
            "forum": "jKHmjlpViu",
            "replyto": "jKHmjlpViu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5983/Reviewer_aKVX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5983/Reviewer_aKVX"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the author introduces OpenWebMath, a new large-scale dataset for language model mathematical problem-solving. A comprehensive illustration of the dataset construction pipeline is provided and some further analysis of the dataset is conducted."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper provides an opensource large-scale mathematical web text dataset which can benefit the following research.\n2. The detailed dataset construction pipeline is provided."
                },
                "weaknesses": {
                    "value": "1. The advance of OpenWebMath compared with existing datasets such as Proof-Pile is not provided.\n2. My main concern here is that the paper is a dataset construction paper without novel technique contribution provided. I\u2019m not very sure if this kind of paper is suitable for ICLR."
                },
                "questions": {
                    "value": "What is the advance of OpenWebMath compared with existing datasets such as Proof-Pile?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5983/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698074828992,
            "cdate": 1698074828992,
            "tmdate": 1699636639983,
            "mdate": 1699636639983,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "peoOSdtlGX",
                "forum": "jKHmjlpViu",
                "replyto": "SXqlalFtHf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5983/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5983/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer aKVX"
                    },
                    "comment": {
                        "value": "Thank you for your review and comments. \n\n> My main concern here is that the paper is a dataset construction paper without novel technique contribution provided. I\u2019m not very sure if this kind of paper is suitable for ICLR.\n> \n\nLarge language models are state-of-the-art in a large number of natural language and machine learning applications and data is often just as important or more important than other parts of the LLM training pipeline. However, data is famously one of the most closely guarded secrets of the closed labs which create these strong models. We strongly believe that dataset works such as OpenWebMath are crucial for the field\u2019s ability to do good science in this emerging area of research. \n\nTowards the goal of pushing forward the academic study of LLM reasoning, we document as much as possible and present novel techniques throughout our pipeline to deal with the unique challenges posed by the creation of such a dataset. These include but are not limited to our methodology for extracting LaTeX code from Common Crawl files and our self-supervised filtering methods. We believe these contributions along with our analysis and openly available artifacts (filtering models, dataset) make OpenWebMath a valuable contribution to the ICLR community, especially for those who study LLM reasoning or dataset creation in the future.\n\n> The advance of OpenWebMath compared with existing datasets such as Proof-Pile is not provided.\n> \n\nProof Pile is a hand-curated mathematics dataset that mostly ArXiv math papers. OpenWebMath is a web-scale dataset of 14.7B tokens which contains over 100k unique domains. The sources present in Proof Pile are distributed as follows:\n\n- ArXiv.math (13B tokens)\n- Open-source math textbooks and formal mathematics libraries (265M tokens)\n- Math Overflow and Math Stack Exchange (825M tokens)\n- Wiki-style sources (17M tokens)\n- MATH training set (2M tokens)\n\nUnlike OpenWebMath, Proof Pile itself is not a great dataset for math problem solving, likely since the vast majority of its data is arXiv papers which do not contain as many instances of step-by-step problem solving and usually skip basic concepts. OpenWebMath is complementary to Proof Pile and includes almost exclusively data that wasn\u2019t in that dataset. As shown in our experiments, training on a diverse mixture of documents from OpenWebMath and Proof Pile is essential to train the best model.\n\nWe will add a more thorough description of Proof Pile to the paper as well as the following table that illustrates how the two datasets are complementary.\n\n|  | Papers | Formal Math | Web Pages | Openly Available |\n| --- | --- | --- | --- | --- |\n| Minerva Web Math Pages | 21B | 0 | 17.5B | No |\n| ProofPile | 13B | 0.2B | 0.8B | Yes |\n| OpenWebMath | 0 | 0 | 14.7B | Yes |\n\nThank you again for your review. We will update the pdf with the new paper version in the next few days. We hope that our clarifications are useful and that you will consider raising your score."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5983/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700365873550,
                "cdate": 1700365873550,
                "tmdate": 1700365873550,
                "mdate": 1700365873550,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NLQnyjDdZh",
            "forum": "jKHmjlpViu",
            "replyto": "jKHmjlpViu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5983/Reviewer_2Z7u"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5983/Reviewer_2Z7u"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes OpenWebMath, an open dataset of 14.7B high-quality mathematical documents from web text. The authors highlight the importance of pretraining on mathematical content to improve the reasoning abilities of large language models. They mention the success of the Minerva model, which was trained on a curated dataset of mathematical documents. However, existing open-source web datasets do not preserve mathematical notation accurately, limiting their usefulness. OpenWebMath aims to address this gap by providing a dataset of 14.7 billion tokens of mathematical web pages extracted from Common Crawl. The authors describe their method for extracting and filtering web pages for high-quality English mathematical documents. They also conduct experiments showing that models trained on OpenWebMath outperform models trained on larger general language datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper proposes an open high-quality mathematical dataset, which can let models get a good reasoning ability in a lower computation.\n2. This paper proposes a new method for extracting and filtering mathematical text from web pages. This method is worth deeper research."
                },
                "weaknesses": {
                    "value": "1. The authors should provide an example of a dataset in the paper.\n2. The order in which the table appears is inconsistent with the logic of the text.\n3. There are invisible Unicode characters and some text in other languages in the data."
                },
                "questions": {
                    "value": "Why there are some invisible Unicode characters and other language text in sample_dataset.jsonl? For instance the\u00a0 40th and 43th lines of sample_dataset.jsonl."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5983/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5983/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5983/Reviewer_2Z7u"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5983/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698718351122,
            "cdate": 1698718351122,
            "tmdate": 1699636639884,
            "mdate": 1699636639884,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nXHR89utpl",
                "forum": "jKHmjlpViu",
                "replyto": "NLQnyjDdZh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5983/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5983/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2Z7u"
                    },
                    "comment": {
                        "value": "Thank you for your review and comments. We are happy that you find the dataset high quality and we are excited both about the models that this can be used to train and the scientific research that well documented, open datasets can enable.\n\n> The authors should provide an example of a dataset in the paper.\n> \n\nThank you for the suggestion. We will add a figure that shows an example from the dataset to the paper.\n\n> The order in which the table appears is inconsistent with the logic of the text.\n> \n\nWe will rearrange the figures in the paper to be closer to the text discussing them.\n\n> There are invisible Unicode characters and some text in other languages in the data.\n> \n\nThank you for pointing this out. We did not do any processing to remove invisible Unicode characters simply because we were not aware this was a problem. We will add a script in our final release to remove such characters from the dataset. \n\n> Why there are some invisible Unicode characters and other language text in sample_dataset.jsonl?\n> \n\nIn general, we cannot guarantee that every document in OpenWebMath is high quality since there are many documents and our filters, no matter how much they are tuned, will never be 100% accurate. Additionally, we expect a small amount of low-quality documents or a small amount of documents in another language to be present in our dataset by design, since we focus on optimizing for recall over extremely precise filtering in order to preserve more information. Users can always further filter and remove documents from OpenWebMath if their application calls for it.\n\nStill, we find that our dataset quality is often higher (by inspection) than that of other web datasets like C4 and RefinedWeb and we provide evidence in the paper that training on our dataset results in strong performance.\n\nThank you again for your review. We will update the pdf with the new paper version in the next few days. We hope that our clarifications are useful and that you will consider raising your score."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5983/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700365815348,
                "cdate": 1700365815348,
                "tmdate": 1700365815348,
                "mdate": 1700365815348,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kvatsPxS9T",
                "forum": "jKHmjlpViu",
                "replyto": "nXHR89utpl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5983/Reviewer_2Z7u"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5983/Reviewer_2Z7u"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your reply."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5983/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700706880698,
                "cdate": 1700706880698,
                "tmdate": 1700706880698,
                "mdate": 1700706880698,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BFOvOabszN",
            "forum": "jKHmjlpViu",
            "replyto": "jKHmjlpViu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5983/Reviewer_oFxt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5983/Reviewer_oFxt"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a large scale dataset of mathematical text (14.7B tokens, 6.3M documents) filtered from the Common Crawl dataset: OpenWebMath. The paper primarily describes the extensive pre-processing applied to obtain this dataset. To indicate the value of the dataset the paper trains a 1.4B Pythia model on the gathered data and reports perplexity on GSM8k and MATH datasets and task accuracy on MATH and LILA-multiarch. The results indicate that a model trained on OpenWebMath sees improved perplexity and better task accuracy."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper presents a well documented dataset.\n- The dataset seems to be useful for training LLMs of small-medium scale."
                },
                "weaknesses": {
                    "value": "- The paper presents no special insight on the dataset or the effect of the processing steps applied - it only describes the pre-processing pipeline. A key aspect that would strengthen this paper is a description of the overlap (computed in some apt way: eg overlap of urls, text overlap, others) between OpenWebMath and the benchmark datasets it evaluates on - computing overlap other popular reasoning benchmarks would also be a welcome addition."
                },
                "questions": {
                    "value": "- It seems like the MATH dataset was gathered from aops.com/community/c3158_usa_contests. Is this a part of Common Crawl? What is the overlap between OpenWebMath and MATH? This is important given concerns of dataset contamination with web scale datasets: https://arxiv.org/abs/2310.10628\n- How does OpenWebMath differ from ProofPile? Are there obvious reasons why using OpenWebMath results in significantly better performance than ProofPile?\n\t- The citation for ProofPile (\"Proofnet: Autoformalizing and formally proving undergraduate-level mathematics.\") seems incorrect. Please consider adding a note of what the dataset is and its source.\n- What exactly is LILA-multiarith? It seems the LILA benchmark contains multiple different datasets, why did the evaluation here only use this one dataset in the benchmark? \n\t- In similar vein to the above comments, does the data here overlap with OpenWebMath?\n\t- Please consider citing the original source of the multiarith dataset in addition to the benchmark, its not clear what the original data is. The citation chain to the original dataset seems to be: https://arxiv.org/pdf/2210.17517.pdf (LILA) -> https://arxiv.org/pdf/1608.01413.pdf (methodological paper using the data?) -> https://aclanthology.org/Q15-1001.pdf (original data) - please verify this.\n- Please consider describing the tasks of Table 2 in more detail.\n- Please place a table or figure closer to the texts discussing it."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5983/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5983/Reviewer_oFxt",
                        "ICLR.cc/2024/Conference/Submission5983/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5983/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698786496780,
            "cdate": 1698786496780,
            "tmdate": 1700690919218,
            "mdate": 1700690919218,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KtovO1tVka",
                "forum": "jKHmjlpViu",
                "replyto": "BFOvOabszN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5983/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5983/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer oFxt (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your review and for your comments. We are happy you find our dataset well-documented and useful!\n\n> The paper presents no special insight on the dataset or the effect of the processing steps applied - it only describes the pre-processing pipeline. A key aspect that would strengthen this paper is a description of the overlap (computed in some apt way: eg overlap of urls, text overlap, others) between OpenWebMath and the benchmark datasets it evaluates on - computing overlap other popular reasoning benchmarks would also be a welcome addition.\n> \n\nWe believe that releasing open, well documented datasets such as OpenWebMath are critical to enabling good science in the fields of LLMs. Certainly one of the core reasons for this is that it enables greater transparency into the relationship between data overlap and downstream performance. Per your request, we evaluated the n-gram overlap between two datasets and used this to measure the overlap between our benchmarks datasets and OpenWebMath. As shown in the table below, only 348 out of 5000 MATH problems, 2 out of 1319 GSM8k problems, and 2 of 1310 LILA problems tested overlap with OpenWebMath. Due to the potential for false positives that we discovered during our overlap analysis, we opt to leave decontamination to those training models on the dataset. We will update the paper to include this overlap analysis (and include the code and overlap locations in our release) and we are open to adding other reasoning benchmarks to this analysis if they are suggested.\n\n| Evaluation | Problems with 30-gram Overlap | Total Problems |\n| --- | --- | --- |\n| MATH | 348 | 5000 |\n| GSM8k | 2 | 1319 |\n| LILA (all tested) | 3 | 1310 |\n\n> The citation for ProofPile (\"Proofnet: Autoformalizing and formally proving undergraduate-level mathematics.\") seems incorrect. Please consider adding a note of what the dataset is and its source.\n> \n\nProof Pile is hosted here: https://huggingface.co/datasets/hoskinson-center/proof-pile. We have checked with the authors of Proof Pile and confirmed that the Proofnet citation is correct.\n\n> How does OpenWebMath differ from ProofPile? Are there obvious reasons why using OpenWebMath results in significantly better performance than ProofPile?\n> \n\nProof Pile is a hand-curated mathematics dataset that mostly ArXiv math papers. OpenWebMath is a web-scale dataset of 14.7B tokens which contains over 100k unique domains. The sources present in Proof Pile are distributed as follows:\n\n- ArXiv.math (13B tokens)\n- Open-source math textbooks and formal mathematics libraries (265M tokens)\n- Math Overflow and Math Stack Exchange (825M tokens)\n- Wiki-style sources (17M tokens)\n- MATH training set (2M tokens)\n\nUnlike OpenWebMath, Proof Pile itself is not a great dataset for math problem solving, likely since the vast majority of its data is arXiv papers which do not contain as many instances of step-by-step problem solving and usually skip basic concepts. OpenWebMath is complementary to Proof Pile and includes almost exclusively data that wasn\u2019t in that dataset. As shown in our experiments, training on a diverse mixture of documents from OpenWebMath and Proof Pile is essential to train the best model.\n\nWe will add a more thorough description of Proof Pile to the paper as well as the following table that illustrates how the two datasets are complementary.\n\n|  | Papers | Formal Math | Web Pages | Openly Available |\n| --- | --- | --- | --- | --- |\n| Minerva Web Math Pages | 21B | 0 | 17.5B | No |\n| ProofPile | 13B | 0.2B | 0.8B | Yes |\n| OpenWebMath | 0 | 0 | 14.7B | Yes |"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5983/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700365496702,
                "cdate": 1700365496702,
                "tmdate": 1700365496702,
                "mdate": 1700365496702,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OtikgDiC24",
                "forum": "jKHmjlpViu",
                "replyto": "BFOvOabszN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5983/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5983/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer oFxt (2/2)"
                    },
                    "comment": {
                        "value": "> What exactly is LILA-multiarith? It seems the LILA benchmark contains multiple different datasets, why did the evaluation here only use this one dataset in the benchmark?\n> \n\nLILA is a large collection of math benchmarks of varying quality and required skill levels, so we just picked one that we thought tested a unique aspect relative to our existing benchmarks. The LILA evaluations involve running Python code in order to produce an answer and LILA-multiarith is essentially a Python version of the multiarith benchmark. \n\nWe have now evaluated our models on all the LILA evaluations which have numerical answers and automatic evaluation below. Note the accuracies are quite low on some of the evaluations across the board due to their difficulty, which can result in a lot of variance in scores - we believe perplexity evaluations (included in the paper) are the most appropriate for measuring the strength of datasets like OpenWebMath at this type of scale since performance scales more smoothly with this metric [1].\n\n|  | The Pile | ProofPile | OpenWebMath | Mixture | Pythia 1.4b |\n| --- | --- | --- | --- | --- | --- |\n| MATH-Algebra-Easy | 2.81% | 2.81% | **5.62%** | 5.06% | 3.93% |\n| MATH-Algebra-Easy maj@16 | 3.93% | 3.93% | 9.55% | **10.11%** | 5.62% |\n| LILA-multiarith | 9.77% | 8.04% | **16.67%** | 13.22% | **21.80%** |\n| LILA-mathqa-probability | 0.00% | 0.00% | 0.00% | 0.00% | **4.17%** |\n| LILA-mathqa-general | 0.13% | 0.13% | **0.51%** | 0.39% | 0.00% |\n| LILA-mathqa-gain | **1.02%** | **1.02%** | 0.26% | 0.26% | 0.26% |\n| LILA-mathqa-geometry | 0.85% | 0.85% | **2.56%** | 0.85% | **2.56%** |\n| LILA-mathqa-other | 0.00% | 0.00% | **1.10%** | 0.00% | 0.00% |\n| LILA-mathqa-physics | 0.61% | 0.82% | **1.23%** | 1.02% | **1.43%** |\n| LILA-GSM8k-structured | 1.30% | 0.84% | **2.21%** | 1.68% | 0.92% |\n| LILA-add-sub | 33.94% | 26.61% | 44.95% | **46.79%** | **53.21%** |\n| LILA-asdiv | 5.50% | 4.37% | 16.50% | **17.80%** | **19.58%** |\n| LILA-svamp-structured | 5.35% | 7.69% | 10.70% | **14.72%** | **11.71%** |\n\n> In similar vein to the above comments, does the data here overlap with OpenWebMath?\n> \n\nPlease see our above response for the data overlap results for LILA.\n\n> Please consider citing the original source of the multiarith dataset in addition to the benchmark, its not clear what the original data is. The citation chain to the original dataset seems to be: https://arxiv.org/pdf/2210.17517.pdf (LILA) -> https://arxiv.org/pdf/1608.01413.pdf (methodological paper using the data?) -> https://aclanthology.org/Q15-1001.pdf (original data) - please verify this.\n> \n\nWe will clean up our citations in order to cite the original sources of the LILA benchmarks.\n\n> Please consider describing the tasks of Table 2 in more detail.\n> \n\nMATH Algebra-Easy is the Algebra subset of the MATH benchmark, filtered down to only questions with difficulty level 1. We evaluated our models on MATH Algebra-Easy with both greedy decoding and self-consistency. We also evaluated models on LILA tasks, which involve writing Python code to solve a math word problem and contains many subsets corresponding to different types of math problems. These problems test the ability to do mathematical problem solving while offloading arithmetic and computation to the Python interpreter. We tested on all the LILA subsets which have numerical answers, making it simple to automatically evaluate the performance of our models. We will add a paragraph with this information to the paper.\n\n> Please place a table or figure closer to the texts discussing it.\n> \n\nWe will rearrange the figures in the paper to be closer to the text discussing them.\n\nThank you again for your review. We will update the pdf with the new paper version in the next few days. We hope that our clarifications are useful and that you will consider raising your score.\n\n[1] Schaeffer et al. \"Are Emergent Abilities of Large Language Models a Mirage?\" arXiv 2023."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5983/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700365615843,
                "cdate": 1700365615843,
                "tmdate": 1700366387027,
                "mdate": 1700366387027,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QZoNZ2qVbr",
                "forum": "jKHmjlpViu",
                "replyto": "OtikgDiC24",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5983/Reviewer_oFxt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5983/Reviewer_oFxt"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the clarifications."
                    },
                    "comment": {
                        "value": "Thank you for the clarifications and the updates to the paper. Is there a reason 30-gram overlap was used to measure overlap with the test set? What is the level of overlap with smaller n-grams, eg, 5, 10, 20?"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5983/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700676365084,
                "cdate": 1700676365084,
                "tmdate": 1700676365084,
                "mdate": 1700676365084,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aES7JoBdhk",
                "forum": "jKHmjlpViu",
                "replyto": "KaszaHkuti",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5983/Reviewer_oFxt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5983/Reviewer_oFxt"
                ],
                "content": {
                    "title": {
                        "value": "Raised my score."
                    },
                    "comment": {
                        "value": "Thanks for the clarifications. I would encourage the inclusion of the above clarifications and examples in the paper's appendix to substantiate the choice of 30 grams. I have raised my score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5983/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690994290,
                "cdate": 1700690994290,
                "tmdate": 1700690994290,
                "mdate": 1700690994290,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]