[
    {
        "title": "CrossTVR: Multi-Grained Re-Ranker for Text Video Retrieval with Frozen Image Encoders"
    },
    {
        "review": {
            "id": "97pYyVZkMz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4495/Reviewer_qf6J"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4495/Reviewer_qf6J"
            ],
            "forum": "Q4FmJPQwuJ",
            "replyto": "Q4FmJPQwuJ",
            "content": {
                "summary": {
                    "value": "This paper aims at text-video retrieval in the domain of vision-and-language pretraining and understanding. Similar to general information retrieval or multimedia retrieval system, the authors proposed a two-stage or coarse-to-fine retrieval framework. In the first stage, the consine similarity network is trained with contrastive learning loss. Thus, a general text and video matching score could be obtained by such a model. The main contribution is the proposed re-ranker for fine-grained ranking in the second stage. Specifically, the authors proposed a novel cross attention module called multi-grained text-video cross attention in order to compute text-frame level attention and text-video level attention respectively. Finally, this prototype has been verified on several popular text-video retrieval benchmarks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The motivation of this work is clear and is easy to follow. \n\n2. I think the overall technology roadmap is in the right way which includes: (a) a course-to-fine retrieval framework for the sake of efficiacy and effiency trade-off; (b) freezing the image encoders to make the training process practical and affordable;\n\n3. The proposed  multi-grained text-video cross attention mechanism is brilliant and makes sense. Specifically, spatial text attention module is proposed at the frame level in order to discover small objects or entities. And temporal text attention is designed to capture subtle movement."
                },
                "weaknesses": {
                    "value": "1. It seems that the experimental results do not include retrieval performance (e.g. stage 1 / stage 2/ pre- and post-processing), which is important in terms of a cross-modal retrieval topic. \n\n2. Again, in terms of a retrieval task, it is important to verify the effectiveness under a huge database setting. However, the most large dataset only contains 118,081 videos which is much less than industiral scales such as YouTube. At least, more distractors should be included if scaling positive text-video pairs is a concern. \n\n3. I think the visualization is not enough. For example, the retrieval results of different work or the effect of proposed re-ranking could be included as supplementary materials if space limitation is a concern.  Besides, the visualization should verify the effectiveness of proposed saptial text attention and temporal text attention."
                },
                "questions": {
                    "value": "Besides the problems in the above weaknesses part, there are a few other questions listed as follows.\n\n[Q1] The token selector is employed as a trade-off between missing detailed information and bringing redundant information issues. The implementation of such a module is a MLP followed by a Softmax Layer, which predicts each token's importance score and selects the M most informative tokens as output. The question is what is the ground truth (GT) for such predictions? It seems that it is even impossible for human beings to label. Note that this is not a single label classfication problem. \n\n[Q2] From the ablation study Table 6, the hard negative mining module makes little contribution to the final results, which is below my expectation. Is there any possible explanations?\n\n[Q3] It is quite often to incorporate query expansion for the sake of increasing recall. Is it still effective after the 2nd stage re-ranking? It is not a necessary ablation study but could be considered as a option."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4495/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697452175919,
            "cdate": 1697452175919,
            "tmdate": 1699636425414,
            "mdate": 1699636425414,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "n3oGen5Uc5",
                "forum": "Q4FmJPQwuJ",
                "replyto": "97pYyVZkMz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4495/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4495/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### 1. Experimental results in stage1/stage2/post-processing.\n\nThe results of our searches are in Tables 1-5. The original cosine similarity method 'CLIP-VIP' is the stage1 result.  The cosine similarity method togher with our method  'CLIP-VIP + Ours' is the stage 2 result. The method denoted with (*) is the post-processing result. For better presentation, we restate  in the following table the results of the stage1/stage2/post-processing in the MSRVTT dataset.\n\n|                                 | R@1  | R@5  | MnR  |\n|---------------------------------|------|------|------|\n| CLIP-VIP (stage1)                   | 50.1 | 74.8 | -    |\n| CLIP-VIP+Ours  (stage2)         | 51.9 | 76.8 | 14.3 |\n| CLIP-VIP+Ours*(post-processing) | 56.9 | 80.0 | 11.5 |\n| TS2-Net (stage1)                      | 47.0 | 74.5 | 13.0  |\n| TS2-Net+Ours  (stage2)         | 50.0 | 75.7 | 12.0 |\n| TS2-Net+Ours*(post-processing) | 52.8 | 78.9 | 11.2 |\n\n### 2.Larger datasets and scaling video-text pairs. More distractors should be included if scaling positive text-video pairs is a concern.\n\nYes, we do not have such a industrial scale video retrieval dataset like YouTube.  We create a larger validation dataset by combining the vallidation set of MSRVTT, LSMDC, and Didemo dataset.  LSMDC datasets contains 118,081 videos  is currently the largest video-text retrieval public dataset for scholarly.  And MSVTT datasets contains 20 titles per video, with up to 200,000 video text pairs. Didemo contains more than 10,000 videos and 40,000 captions, \nWe use the positive pairs from MSRVTT as groundtruth and use validatiaon set from LSMDC and Didemo as distractors\u00a0. We use the model trained on MSRVTT dataset for evaluation. The following results show that our method is still effective in large scale retrieval dataset.\n\n|                                 | R@1  | R@5  | MnR  |\n|---------------------------------|------|------|------|\n| TS2-Net (stage1)               |  37.2   |63.5   | 44.1  |\n| TS2-Net+Ours  (stage2)         | 40.5 | 67.8   | 43.8 |\n| TS2-Net+Ours*(post-processing) | 42.7 | 68.0 | 47.0 |\n\n### 3.More visualization results about re-ranking and spatial text attention and temporal text  attention.\n\nWe add more text to video re-ranking visualization results in Figure.7 and video to text re-ranking results in Figure. 8 of the supplementary material, and visualizations of spatial attention and temporal attention are presented in Figure 5 of the paper. Spatial attention allows us to identify small objects at the frame level, while temporal attention helps us to discriminate continuous actions at the video level.\n\n### 4. The ground truth of Token selector predicted importance score.\n\nYes, the token selector aim to predict the most sailent video level features.  As we do not have the groundtruth of importance score, the prediction may be not accurate. However, in frame-level cross attention, we use all tokens to make sure all details information are not missed. This is the reason why combine video level cross attention and frame level cross attention can further improves the performance.\n\n### 5.Hard negative sampling has little contribution to the final result.Is there any possible explanations?\n\nThe effect of hard negative sampling method is sensitive to batch size. The hard negative sampling method usually performs better using large batch size. Prevous vision language pretraining method such as ALBEF and BLIP use a batch of 512 and 2880, while we set batch size to 128 for all of our experiments due to limited training resources. We also tried a smaller batch size of 64 on MSRVTT dataset and get a marginal performance contribution.\n\n### 6.Is query expansion effective in the 2nd stage re-ranking?\n\nWe have not yet experimented on query expansion for fair comparison since other methods do not use it . We followed previous work using DSL post-processing, which serves as a reviser to correct the similarity matrix and achieves the dual optimal match. We use (*) to denote methods use DSL in Table 1~5. It shows the DSL is equally effective in stage one and stage two."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4495/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662163435,
                "cdate": 1700662163435,
                "tmdate": 1700662163435,
                "mdate": 1700662163435,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nYCLXHUkAX",
                "forum": "Q4FmJPQwuJ",
                "replyto": "n3oGen5Uc5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4495/Reviewer_qf6J"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4495/Reviewer_qf6J"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your furhter response. When referring to retrival performance, I particularly mean the processing time in each stage (stage 1 / stage 2/ pre- and post-processing), which is vital for retrieval related tasks."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4495/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700720051583,
                "cdate": 1700720051583,
                "tmdate": 1700720051583,
                "mdate": 1700720051583,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "p6rd3ILAFQ",
                "forum": "Q4FmJPQwuJ",
                "replyto": "97pYyVZkMz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4495/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4495/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We measure the stage1 and stage2 time performance in Table 8 of the main paper. For better presentation, we restate in the following table the processing time  to index 1000 videos with one query text. In the following table, the stage1 time is the sum of time to encode 1 text, the time to encode1000 videos, and the time to compute cosine similarity. The stage1+stage2 time is the sum of the stage1 time and the time to compute the cross attention. We can see stage2 only increase 2\\% retreival time. Since we use dual softmax  as the post procssing step, which only involves  matrix calcuation, the post-procssing time is very small and can be ignored comparing to retrieval time.\n\n| Method                         | Times(s) |\n|--------------------------------|----------|\n| CLIP4CLIP (stage1)             | 7.06     |\n| CLIP4CLIP+ours (stage1+stage2) | 7.21     |\n| CLIP4CLIP+ours (stage1+stage2+post procssing) | 7.21     |\n| Xpool (stage1)                 | 12.17    |\n| Xpoll+ours (stage1+stage2)     | 12.38    |\n| Xpoll+ours (stage1+stage2+post procssing)     | 12.38    |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4495/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732819515,
                "cdate": 1700732819515,
                "tmdate": 1700737393954,
                "mdate": 1700737393954,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YdIPk4DaCE",
            "forum": "Q4FmJPQwuJ",
            "replyto": "Q4FmJPQwuJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4495/Reviewer_TCyW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4495/Reviewer_TCyW"
            ],
            "content": {
                "summary": {
                    "value": "This paper puts forward a new method for text-video retrieval. The core contribution of the study revolves around a multi-grained re-ranker that is designed to effectively retrieve relevant videos using text as a query. The paper also proposes a design strategy for the cross attention module by collaborating with different cosine similarity based methods, which further enhances the retrieval efficiency. And the experiments show good performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is articulated with clarity and precision.\n2. The multi-grained re-ranker is an innovative approach that can potentially enhance text-video retrieval's efficacy. Furthermore, the use of frozen image encoders in the context of text-video retrieval is a commendable attempt at exploring uncharted territories."
                },
                "weaknesses": {
                    "value": "1. Explain the specific implementation and mechanism of video-level and frame-level cross attention. And how do these components operate and interact.\n2. Although frozen vision encoder reduce computational costs, this approach may limit the model's flexibility and ability to adapt to different tasks.\n3. There are some formatting errors, i.e Figure 4."
                },
                "questions": {
                    "value": "1. Please elaborate on how to match and align between video and text.\n2. In addition to TS2Net, the results of other existing methods are given to better evaluate the performance improvement and advantages of this method"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4495/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698759947111,
            "cdate": 1698759947111,
            "tmdate": 1699636425342,
            "mdate": 1699636425342,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0Nx7nzFKfo",
                "forum": "Q4FmJPQwuJ",
                "replyto": "YdIPk4DaCE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4495/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4495/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### 1. Explain the specific implementation and mechanism of video-level and frame-level cross attention.\n\nPlease refer to the supplementary material Fig. 5, which shows the details of our video-level and frame-level cross attention.  Specifically, for frame text attention, learnable queries are first concatenated with text tokens. These queries interact through self attention layer and then interact with each frame features in cross attention layer. These queries feed into video text attention after averaging. For video text attention, these queries interact with the selected video token in cross attention.\n\n### 2. Although frozen vision encoder reduce computational costs, this approach may limit the model's flexibility and ability to adapt to different tasks.\n\nYes, we have not try other video understanding tasks like video QA, video caption so we do not know whether the frozen vison encoder strategy is also applicable on these tasks. However,  we are certain that finetuning the whole vision encoder will not perform worse than the frozen vision encoder. Therefore, when adapting our method to other tasks, we can finetune the vision encoder to increase the performance.\n\n### 3. There are some formatting errors, i.e Figure 4.\n\nWe have revised Figure 4 and update the manuscript. \n\n### 4. Please elaborate on how to match and align between video and text.\n\nWe use contrastive loss and video text matching loss to supervise the alignment between video and text in stage 1 and stage 2, respectively.  The contrastive loss compare each text to all videos in a batch. And the matching loss compute the text to the hardest video case in a batch with the help of hard negative mining strategy.  Fig. 5 in  supplementary material shows the details of the interaction between text and video in cross attention.\n\n### 5. In addition to TS2Net, the results of other existing methods are given to better evaluate the performance improvement and advantages of this method.\n\nWe report the results our method on top of four SOTA cosine similarity methods in Tables 1 and 8 of paper. Specifically on the t2v R@1 metric for the MSRVTT dataset, our method improves by 2.5, 1.2, 3.0, and 1.8 on the Clip4Clip, Xpool, TS2-Net, and clip-vip methods, respectively."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4495/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660545928,
                "cdate": 1700660545928,
                "tmdate": 1700660545928,
                "mdate": 1700660545928,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0zMleDKuBH",
            "forum": "Q4FmJPQwuJ",
            "replyto": "Q4FmJPQwuJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4495/Reviewer_dEsS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4495/Reviewer_dEsS"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a multi-grained re-ranker for text video retrieval with frozen image encoders, named CrossTVR. Although experimental results have demonstrated the effectiveness of this paper, the main contributions of this paper are not clear enough."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.The authors develop a novel method to capture comprehensive and fine-grained text-video interaction cues, which uses a multi-grained video text cross attention transformer module to enhance the retrieval process.\n2.The corresponding experiments validate the effectiveness of the proposed method on five public text-video retrieval benchmarks and show a significant improvement in solution acceptance rates."
                },
                "weaknesses": {
                    "value": "1.\tThe Abstract is not well written and it is not easy to understand the contributions. Recommended to rewrite and well present your contributions. \n2.\tFor the first stage, this paper uses an existing cosine similarity network, the novelty is limited for me.\n3.\tFor the technique part, the motivation for multi-grained video text cross-attention is unclear. How to enhance the fine-grained correspondences by the cross-attention module.\n4.\tFurthermore, what specific attention mechanism is being employed? Please explain this work's fundamental research insight. \n5.\tThe main contributions of this paper are not clear enough to show the advantages and the novelty is not well explained. Please summary the main contributions in the introduction section.\n6.\tThe organization of this manuscript is a mess, especially the order of the discussion of Tables 1 to 5. The related discussion should be even clearer.\n7.\tIn addition, the authors should revise the manuscript thoroughly. Here are some data errors and typos as listed below (including but not limited to). The authors need to make the necessary changes.\n1)\tIn Table 5, there are data errors that need to be corrected.\n2)\tIn the 4.2 section, \u2018moreover, our method, which leverages CLIP-VIP, achieves \u2026\u2019"
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4495/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698802431537,
            "cdate": 1698802431537,
            "tmdate": 1699636425243,
            "mdate": 1699636425243,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fIMWupvGGj",
                "forum": "Q4FmJPQwuJ",
                "replyto": "0zMleDKuBH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4495/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4495/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### 1. Rewrite Abstract. \n\nState-of-the-art text-video retrieval (TVR) methods typically utilize CLIP and cosine similarity for efficient retrieval.\nMeanwhile, cross attention methods, which employ a transformer decoder to compute attention between each text query and all frames in a video, offer a more comprehensive interaction between text and videos. \nHowever, these methods lack important fine-grained information as they directly compute attention between text and video-level tokens. \nTo address this issue, we propose a fine grained re-ranker named CrossTVR with a multi-grained video text cross attention module to capture fine-grained multimodal information from both frame and video level. The re-ranker only improves the top K similar results from the cosine similarity network, so the inference can be performed efficiently.\nTo reduce the additional training cost by the multi-grained cross attention module, we freeze the vision backbone and only train the video text cross attention matching header, enabling scalability to larger pre-trained vision models like ViT-G, resulting in improved retrieval performance. \nExperiments on text video retrieval datasets demonstrate the effectiveness and scalability of our proposed CrossTVR compared to state-of-the-art approaches.\n\n### 2. For the first stage, this paper uses an existing cosine similarity network, the novelty is limited\u00a0.\n\nYes, we clarify Stage 1 is not our contribution. CrossTVR is a  re-ranker which is compatible with existing retrieval methods.\n\n### 3. The motivation for multi-grained video text cross-attention is unclear. How to enhance the fine-grained correspondences by the cross-attention module.\n\nIn previous work on video text retrieval, one approach is to use the embeddings of video and text for cosine similarity, which is efficient but limited in accuracy. While another is to use all video tokens and text for cross attention, which costs expensive computation. We consider an method that minimizes computational effort while maximizing the retention of frame-level and video-level information to achieve full interaction between video and text in cross attention.  \nIn the first stage, existing cosine similarity network can retrives Top K most similar results with high recall. Then in the second stage, the goal of re-ranker is to find the minor differents between these K results, which we believe fine grained correspondences are critical. Therefore, we use frame text attention to achieve the interaction between frame and text to obtain fine-grained features. And we further visualize the fine-grained feature attention map to show our fine-grained feature extraction capability in supplymentary meterial Fig. 7.\n\n### 4. What specific attention mechanism is being employed? Please explain this work's fundamental research insight.\n\nOur attention mechanism is built on top of BERT, which is also widely used in previous multimodal work such as ALBEF, BLIP, X-VLM. We show the details of our cross attention in the supplementary material Fig. 5. The attention mechanism is not novel, the way to fuse frame level information and video level information is critical in our method. Specifically, We use a set of  fixed number learnable queries to sequentially  interact with text, frame level , and video level information, which extracts relavent information from each other.\n\n### 5. The main contributions of this paper are not clear enough to show the advantages and the novelty is not well explained. Please summary the main contributions in the introduction section.\n\nIn summary, The main contributions of this work are as follows:\n\n* We propose a multi-grained re-ranker called CrossTVR, which achieves comprehensive interaction between text and video at the frame level and video level.\n* As a re-ranker, our method can be widely applied to existing cosine similarity-based methods and effectively improve the SOTA retrieval performance with marginal additional computation cost.\n* Benefiting from our freezing visual coder training method, our approach can scale to larger pretrain visual models with small computational resources.\n\n### 6. There are a few minor issues in the writing of this paper.\n\nWe follow the advice and have fixed the writing issues in the uploaded revised manuscript."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4495/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660408877,
                "cdate": 1700660408877,
                "tmdate": 1700660408877,
                "mdate": 1700660408877,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]