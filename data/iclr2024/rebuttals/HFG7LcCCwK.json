[
    {
        "title": "Conditional Generative Models are Sufficient to Sample from Any Causal Effect Estimand"
    },
    {
        "review": {
            "id": "qHPWCDq2ga",
            "forum": "HFG7LcCCwK",
            "replyto": "HFG7LcCCwK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8014/Reviewer_ZLmN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8014/Reviewer_ZLmN"
            ],
            "content": {
                "summary": {
                    "value": "This addresses the challenge of estimating causal effects, which typically requires access to conditional likelihoods. In high-dimensional scenarios, this can be problematic. The paper introduces a novel approach using conditional generative models to compute identifiable causal effects in graphs with latent confounders. The authors also present a diffusion-based method for sampling from interventional distributions. Experimental results are demonstrated on synthetic and real-world datasets, showing the algorithm's utility."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The writing in this paper is quite clear, and the organization is well-structured. The theoretical foundation of the method is solid.\n- The problem studied in the paper is quite interesting."
                },
                "weaknesses": {
                    "value": "- While using a causal graph to model the relationships between statistical variables, deriving identifiability conditions is crucial. However, when extending this to high-dimensional data, the theory is elegant, but the practical applications are quite tricky. For instance, as seen in the experiments within the paper, they are entirely simulated cases. I find this to be not very practical.\n- If the authors could provide some simulation data in the experimental section, it would better support the conclusions in the paper. This is because, when relying solely on image data, evaluation metrics can sometimes be less accurate.\n- Could the authors provide some failure cases, where the situations that would occur if the assumptions in the paper are not met?"
                },
                "questions": {
                    "value": "I'm familiar with DAG learning and effect estimation, but I don't have a deep understanding of ADMG. Therefore, I find it challenging to provide an assessment of the method's innovation at this point. I will rely on the opinions of other reviewers to evaluate this aspect later on."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8014/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8014/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8014/Reviewer_ZLmN"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8014/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698311846282,
            "cdate": 1698311846282,
            "tmdate": 1700637480511,
            "mdate": 1700637480511,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "keKOceBfOb",
                "forum": "HFG7LcCCwK",
                "replyto": "qHPWCDq2ga",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8014/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8014/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ZLmN (1/2)"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for their comments and feedback and are happy to learn that the reviewer found our\npaper well-structured and our method theoretically solid.\n\n## Practical applications with high-dimensional data\n\n> When extending this to high-dimensional data, the theory is elegant, but the practical applications\n> are quite tricky. For instance, as seen in the experiments within the paper, they are entirely simulated cases. I find\n> this to be not very practical.\n\nWe thank the reviewer for their comment. We humbly point the reviewer to section 5.2 of our paper where we shared our\nalgorithm performance of\nan experiment on the real-world COVID\nX-RAY dataset (https://www.kaggle.com/datasets/andyczhao/covidx-cxr2/data). However, if\nthe reviewer could kindly suggest any challenging applications or datasets suitable for causal inference,\nwe would be very happy to evaluate our algorithm on those.\n\nFor the reviewer's convenience, here we share our real-world experiment in detail.\nThe COVID X-RAY contains 30k X-ray images and COVID and pneumonia labels.\nWith this dataset, we associated the front door causal graph:\n$$\\mathrm{Covid Symptoms} \\rightarrow \\mathrm{Xray Images} \\rightarrow \\mathrm{Pneumonia Detection};\n\\mathrm{Covid Symptoms} \\leftarrow U \\rightarrow \\mathrm{Pneumonia Detection}$$\nwhere $U$ is an unobserved variable (patient/hospital location).\nOur aim was to generate high-quality samples from $P(\\mathrm{Xray images}| do(\\mathrm{Covid Symptoms}))$ and estimate\n$P(\\mathrm{Pneumonia Detection}|do(\\mathrm{Covid Symptoms}))$, i.e., how likely a patient will be diagnosed with\nPneumonia if we intervene on the COVID-19 symptoms, assessing the causal effect.\n\nFor the reviewer's convenience, here we share part of the section 5.2 experiment results. Let N= Pneumonia detection and\nC = Covid Symptoms.\n\n### Result 1:\n\nWe evaluated the image quality of the diffusion model approximating $P(X|C)$. Below we provide the Frechet Inception\nDistance (FID)\nscores of generated image samples compared to the true image samples. Here, `Generated C=c`, means we sample from\nthe diffusion model conditioned on the value $C=c$. `Real (C=c)` refers to the corresponding image samples consistent\nwith $C=c$. Low values on the diagonal and high values on the off-diagonal imply we are sampling correctly from conditional\ndistributions.\n\nClass-conditional FID scores for generated Covid XRAY images (lower is better).\n\n| FID scores of <br/>Generated vs Real image | Real: C = 0 | Real: C = 1 |\n|--------------------------------------------|-------------|-------------|\n| Generated: C = 0                           | 15.77       | 61.29       |\n| Generated C = 1                            | 101.76      | 23.34       |\n\n### Result 2:\n\nWe evaluate the query of interest $P_C(N)$. Since we do not have access to the ground truth of an interventional\ndistribution\nfor real data, we consider i) our evaluated\n$P_C(N)$ versus ii) an ablated version where we replace the diffusion sampling mechanism with $\\hat P(X|C)$, where we\nrandomly select an X-ray image from the validation set. We also consider the query $P_C(N)$ if there were iii) no\nlatent confounders in the graph, in which case, the interventional query $P_C(N)$ is equal to $P(N|C)$.\n\n| $P_{C}(N=1)$     | c=0   | c=1   |\n|------------------|-------|-------|\n| i) Diffusion     | 0.622 | 0.834 |\n| ii) No Diffusion | 0.623 | 0.860 |\n| iii) No Latent   | 0.406 | 0.951 |\n\n## Simulated experiments\n\n> If the authors could provide some simulation data in the experimental section, it would better support the conclusions\n> in the paper.\n\nWe agree with the reviewer that it will be useful to include experiments with simulated data where variables are low\ndimensional discrete.\nThen we would be able to estimate $P_{X}(Y)$ and compare with the ground truth and evaluate them with different measures\nsuch as\ntotal variation distance (TVD) or KL divergence. Since our main target was to sample from the high-dimensional\ninterventional\ndistribution, we only considered image data during submission. Based on the reviewer's suggestion, we will add these new\nexperiments with discrete data in the camera-ready version of our paper. If the reviewer has\nany specific suggestions about simulated datasets, we would be happy to include them."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8014/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700347727574,
                "cdate": 1700347727574,
                "tmdate": 1700347727574,
                "mdate": 1700347727574,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QhJjAPiwGd",
                "forum": "HFG7LcCCwK",
                "replyto": "oaHskBWS8D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8014/Reviewer_ZLmN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8014/Reviewer_ZLmN"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for providing detailed explanations. Based on the addressing of all my concerns, I have revised my rating to 6. I will also discuss the paper's contribution with other reviewers in the next discussion phase."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8014/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637457615,
                "cdate": 1700637457615,
                "tmdate": 1700637457615,
                "mdate": 1700637457615,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gcB1gJYciw",
            "forum": "HFG7LcCCwK",
            "replyto": "HFG7LcCCwK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8014/Reviewer_2Jjc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8014/Reviewer_2Jjc"
            ],
            "content": {
                "summary": {
                    "value": "The authors present an algorithm that allows for sampling from interventional distributions (using observational data & generative models). The algorithm is based on Shpitser & Pearl's revised ID algorithm with the modification that training data and conditional generative models (to be trained on said data) are discussed. The intuition behind said modification is being discussed using three examples. At the end, an empirical part on MNIST and semi-synthetic COVID/X-ray data are presented."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "IMHO the paper's noteworthy strengths are limited to their idea rather than the execution, therefore, they should be considered (where applicable) as counterfactuals for the moment. Said \"potential\" strengths are considered one-by-one in the following list (the list is ordered in correspondence to the paper presentation):\n* Precise coverage of necessary ideas within Pearl's causality framework for understanding the contribution (a.k.a. paper is good on the causality side of things)\n* Use of examples with increasing difficulty to conceptualize the key idea from first principles. Usage of visual means (schematic illustrations).\n* Discussion of Algorithm 1's steps\n* Reasonable semi-synthetic extension for real-world data\n* An effort of a self-enclosed treatise i.e., theory + empirics"
                },
                "weaknesses": {
                    "value": "TL;DR: Respectfully, the authors should not feel attacked by any of the following, mostly, I feel like ignoring many of the \"weaknesses\" but at the core of this section really stands the heavy overreliance on the causal side of things, even though the project is intended as a work on the intersection to ML. The lack of discussions on the other end essentially invalidates the key contribution, the ID-DAG algorithm, as simply being a copy of Shpitser & Pearl's simplified version of Tian's original algorithm, just that we have to use actual samples and models instead of magically having the actual probability distributions at hand. This is precisely what ML is in essence, and discussing characteristics w.r.t. learning, and not the causal part that we already know of by Shpitser, Pearl and others, would have been exactly this paper's key contribution.\n\nThe paper suffers from several disadvantages, ranging in importance from minor to more fundamental (and the minor ones, especially w.r.t. presentation, can be improved quickly). Thereby, the following list - again one-by-one - aims to provide specific pointers with improvement suggestions if applicable (please note, the list is unordered):\n\n* While agreeing with the sentiment that estimating high dimensional, arbitrary conditional distributions to arbitrary precision is difficult, doing so in a more general sense is not and there exist different ways of handling this (just through Bayes' rule or through explicit modelling or through approximations). It comes to mind that in the introduction all Pearl related work (the causal side of things) are described precisely, whereas there is no reference whatsoever to works on the generative side of things apart from the handful of deep models. For example, following the difficulties encountered with Bayesian Networks's inference, advances were made on probabilistic circuits by Darwiche, Poon, Domingos, Perharz, Vergari, Van den Broeck and many others. In said realm, there are even first results on causality both w.r.t inference and sampling (as discussed in this work). To point the authors to concrete works, I link the following (sorted by date): NeurIPS 2021, \"Interventional Sum-Product Networks: Causal Inference with Tractable Probabilistic Models\" and AISTATS 2023 \"Compositional Probabilistic and Causal Inference using Tractable Circuit Models\". By now there are probably a lot more works out there, which is why I'd kindly encourage the authors to have an extensive look at all of these related works.\n* The background section clearly suffers from overloading. Many concepts are needed for the work, however, this work should not be considered as an introductory lecture and given the space limitations IMHO they are the things (as opposed to others listed below that) should arguably be placed in the appendix in a more appropriate manner, covering only the highly relevant prerequisites in the background section.\n* $X$ is not defined in Lemma 3.3.\n* $P_x(y)$ is not defined when first introduced in Lemma 3.3.\n* Please consider improving figure presentation alongside the following three dimensions: (i) proportions, especially Figures 2,3,4,5 suffer from for example small font sizes that make zooming a necessity, (ii) descriptions, legends for things like color codes within the figure & figure caption are missing but also generally the captions do not even work as capturing a \"take-away message\" let alone as being self-enclosed means of communicating the figure's ideas, and (iii) consistency, for example Figure 3 has \\sum written out (as a not compiled normal text).\n* Visual components, shaded quadrangle and circular node, are both not defined in Figure 3. Unlike the examples before, one cannot conclude anything (which is unfortunate since this is exactly the interesting part). Labels, as previously, for training and sampling phases are missing.\n* As a fun nod, when looking closely since it is quite ironic, but DAGs are actually never defined. The paper begins with ADMGs and then abandons them through replacement with DAGs without any notice. Said DAGs further give a false sense of generality of the result since ADMGs are a generalization of DAGs.\n* $\\pi$ in Algorithm 1 is not defined.\n* Key parts of the algorithm s.a. ConstructDAG or Step 7's modification are not being discussed in the main paper.\n* Most severe contribution issue: the \"key contribution\" as the author's put it, Algorithm 1, is identical to Algorithm in Figure 3 of Ilya Shpitser and Judea Pearl \"Identification of joint interventional distributions in recursive semi-Markovian causal models\" (AAAI 2006) up to the sampling network part, which does not become apparent whatsoever through the paper's discussions. Furthermore, the lack of characterization nullifies this contribution and quickly renders the situation a pure application of these prior results but without an actual emphasis on the application part.\n* Most severe writing issue: Theorem 4.1. is neither covered nor proven in the main paper. It is quite evident that the style of writing chosen by the authors (based on the insights from this whole section) is based on \"exploiting\" the appendix for more presentation space, this goes against the submission guidelines of ICLR (and while not as severe as something like non-anonymity, it could still warrant a discussion of desk rejection). I've thoroughly checked the appendix, however, it is important to note that this is not a requirement for the reviewer and specifically so because of the fact that otherwise the meaning of the 9-page main paper would be rendered meaningless. Personally, this does not bother me since I'm content-centric, however, this is a conflict with the conference's guidelines and needs to be covered.\n* IDTrain in Section 5 is not defined.\n* The experimental section evaluation is rather a protocol, than it is an interpretation/discussion of the observed results. The resolution on Fig.5 even on highest zoom does not allow a proper inspection. Still, it seems that \"regular\" diffusion-based issues arise as well (however, to be expected).\n\n\nThis final list is a list of suggestions with concrete ideas that can hopefully help the authors improve their work, as this review is intended as a means of constructive feedback i.e., to help the author's contribution be a great one for the community:\n\n* Please consider using \\citep{} for non-direct references like the ones in the introduction. Especially since the citations are not highlighted otherwise, rendering readability especially difficult.\n* Please consider another pass (possibly by a non-author reader) to avoid writing mistakes, which are not easily detectable by common software. The manuscript various such instances, for example \"international\" instead of \"interventional\" in the Related Work section. Or also cases of punctuation such as for example a missing dot between the two sentences at beginning of Section 4.\n* In the same way as for the regular text writing part, please careful check the mathematical notation. Similarly, there are also such instances where intentions don't comply with what is typeset, for example model $M$ instead of model $\\mathcal{M}$ in Lemma 3.3.\n* Avoid using notation in mathematical results if not needed, for example in Lemma 3.3. introducing $M$ is not necessary i.e., it is not being used by the result. For example rewrite as: \"Let $G$ denote the causal graph entailed by some SCM.\"\n* Please consider highlighting the two critical (and by the way rather strong) assumptions at the end Section 3.\n* Please consider doing another pass also with respect to captions and such. For example in your \"key contribution\" the algorithm in Alg.1. the label for \"causal graph\" is missing at the Input part.\n\n\nAs a final remark on the score: if the ICLR reviewing scale where a 1-10 scale, then I'd opt for a score of 3. However, I only get to choose between 1 and 3 this time around and since the identified issues with this work are severe based on this review's assessment, the conclusion is a 1. To not end this on a negative note, though, IMHO the potential for this work is good and would be of good value for the community."
                },
                "questions": {
                    "value": "TL;DR: No questions.\n\nEven though several quantities throughout have not been defined, I'm confident in being able to guess what the authors actually mean, therefore, no questions are derived on that end. Furthermore, the lack of discussions on both the theoretical and empirical end regarding the non-causal side of things, that is so-defined sampling networks/generative models, does not allow me to raise any further questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8014/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698793410349,
            "cdate": 1698793410349,
            "tmdate": 1699636988156,
            "mdate": 1699636988156,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "np61NaqNdb",
                "forum": "HFG7LcCCwK",
                "replyto": "gcB1gJYciw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8014/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8014/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2Jjc (1/3)"
                    },
                    "comment": {
                        "value": "We cordially thank the reviewer for their effort in writing a detailed review. Their feedback has greatly\nmotivated us to improve our paper and more definitely establish its novelty compared to the current literature.\n\nWe divide our responses into three main parts.\n\n## Writing issues:\n### Paper's presentation:\nWe greatly appreciate the reviewer's effort in thoroughly checking our manuscript and pointing out the minor writing\nissues, and we deeply apologize for those. Based on the reviewer's suggestion, we made the following updates to our manuscript\nand will upload the updated version soon:\n\n* Definitions: We redefined $X$ and $P_{X}(Y)$ in Lemma 3.3 and $\\pi$ in Algorithm 1. We would like to kindly point out\n  that our algorithm is for acyclic directed mixed graph (ADMG), we used the term DAG to refer to the set of trained\n  neural networks which have a different structure, edges, and even nodes compared to the original causal graph. To avoid\n  confusion, we replaced the \"DAG\"\n  term with a \"network\" of trained models.\n* Figures: We i) improved the objects in Figures 2,3,4 and 5 and mentioned the significance of those, and ii) increased the\n  readability of some texts. We iii) added more descriptive captions to assist the readers. We iv) fixed the uncompiled\n  equations in the figures.\n* Discussion: We added a new section (Appendix D) in our updated submission, where we described the ConstructDAG and the\n  step 7 of our algorithm in more detail.\n* Another pass: We made another pass through the paper and fixed the remaining minor issues mentioned by the reviewer. \n  Since ICLR policy allows to update the submitted paper with minor changes during rebuttal, we fixed all the writing\n  issues mentioned by the reviewer and will upload the new version as soon as possible. We will be very happy if the\n  reviewer does not finalize his decision about our paper based on these.\n\n\n### Exploiting the appendix:\n\n> \"Theorem 4.1 is neither covered nor proven in the main paper\"\n\nWe would like to humbly disagree with the reviewer's concern about us exploiting the appendix. The reviewer is right\nthat the proofs are provided in Appendix B and C. However, to our knowledge, it is quite common in the ML community to\npostpone theorem proofs to the appendix. We also believe that the statement of Theorem 4.1 is self-explanatory as it\nonly states soundness and completeness, again a well-understood notion in the community. As evidence that this practice\ndoes not violate ICLR guidelines, we refer to two ICLR-2023 accepted papers (top 5\\%:\nopenreview.net/pdf?id=7YfHla7IxBJ; openreview.net/pdf?id=l6CpxixmUg) where the authors not only put all their proofs in\nthe appendix but also explained their theorems very little, if any. Therefore, we would kindly request the reviewer to\nreconsider his claim that we have \"exploited\" the appendix. We believe this unfortunate choice of words was unjustified.\nTo consider the reviewer's feedback for improving our paper, we will add a proof sketch and elaborate on what soundness\nand completeness means in the main paper for Theorem 4.1.\n\n\n## The role of the ID algorithm in our work:\n\n> The \"key contribution\" as the authors put it, Algorithm 1, is identical to Algorithm\n> In Figure 3 of Shpitser et al \"Identification of joint interventional distributions in recursive\n> semi-Markovian causal models\" (AAAI 2006) up to the sampling network part.\n\nWe believe there is a severe misunderstanding, and we apologize if our writing was not clear enough.\nFirst, note that both the 2006 and 2008 papers below have the same algorithm (conference vs. journal versions):\n\n* Shpitser et al (2008) \"Complete identification methods for the causal hierarchy.\"\n* Shpitser et al (2006) \"Identification of joint interventional distributions in recursive semi-Markovian causal\n  models.\"\n\nBut more importantly, our algorithm is not identical to Ilya and Pearl's algorithm.\nID algorithm gives a way to step-by-step convert an interventional distribution to **some** function of the observational\ndistribution.\nWe ask ourselves, can we use this algorithm to convert any identifiable query to a sequence of forward-conditional\nnetworks.\nIt is then natural that the sampling version should follow the steps of this algorithm and identify the necessary\nmodifications to turn it into a sampling algorithm.\nTherefore, the ID algorithm is not a baseline but a crucial, necessary component of our solution. We cordially invite the\nreviewer to look at our contribution from this lens.\n\nFor the high-level differences note that the ID algorithm is only practical for estimating the causal effect when variables are discrete. Whereas, we\npropose a sampling algorithm and adapt it to deal with high-dimensional variables using the ID algorithm as a guide.\nThroughout the paper, we attempted to precisely describe the limitations each step of the ID algorithm will face while\ndealing with high dimensional variables and how our proposed new techniques can resolve that."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8014/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700334734317,
                "cdate": 1700334734317,
                "tmdate": 1700334734317,
                "mdate": 1700334734317,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yubdFqhC6M",
            "forum": "HFG7LcCCwK",
            "replyto": "HFG7LcCCwK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8014/Reviewer_Xy4K"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8014/Reviewer_Xy4K"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an approach that leverages conditional generative models to sample from identifiable interventional distributions. The method is validated using diffusion models on synthetic image data and a real-world COVID-19 chest X-ray dataset."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper claims that their approach can be applied to any identifiable interventional distribution. This suggests a wide range of potential applications in different domains where causal inference is critical.\n\n- The application of the method to a real-world COVID-19 chest X-ray dataset showcases its practical relevance in addressing a critical public health concern, highlighting its strength."
                },
                "weaknesses": {
                    "value": "- The idea of sampling according to the topological order is not new. Most of the causal effects estimators (explicitly or implicitly) utilise this technique to draw samples from the interventional distribution using conditional distributions. For example, [1][2][3]\n\n1/ Louizos, C., Shalit, U., Mooij, J. M., Sontag, D., Zemel, R., & Welling, M. (2017). Causal effect inference with deep latent-variable models. Advances in neural information processing systems, 30.\n\n2/ Zhang, W., Liu, L., & Li, J. (2021, May). Treatment effect estimation with disentangled latent factors. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 35, No. 12, pp. 10923-10930).\n\n3/ Vo, T. V., Bhattacharyya, A., Lee, Y., & Leong, T. Y. (2022). An Adaptive Kernel Approach to Federated Learning of Heterogeneous Causal Effects. Advances in Neural Information Processing Systems, 35, 24459-24473.\n\nCould the author please highlight the technical novelties of the proposed method?\n\n- The experiments lack comparisons with baseline methods."
                },
                "questions": {
                    "value": "1/ Is it possible to compare with some baselines?\n\n2/ What are technical novelties of the proposed method?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8014/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8014/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8014/Reviewer_Xy4K"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8014/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698806383599,
            "cdate": 1698806383599,
            "tmdate": 1699636988043,
            "mdate": 1699636988043,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "a1WachSM8L",
                "forum": "HFG7LcCCwK",
                "replyto": "yubdFqhC6M",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8014/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8014/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Xy4K (1/3)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback and questions and appreciate them acknowledging the utility of our results in\ndifferent domains and our practical relevance.\nBelow we address all of the reviewer's concerns.\n\n## Sampling according to the topological order is not sufficient:\n\n> The idea of sampling according to the topological order is not new. Most of the causal effects estimators (explicitly\n> or implicitly)\n> Utilize this technique to draw samples from the interventional distribution using conditional distributions.\n\nThe reviewer raised an interesting concern that the idea of sampling according to the topological order is not new and\nwe agree with them. However, this approach is **only feasible when there exists no unobserved confounders in the causal\ngraph**.\nFor graphs with confounders, sampling according to topological order has failure cases.\nConsider this causal graph: \n$$X \\rightarrow W_1 \\rightarrow W_2 \\rightarrow Y; X \\leftrightarrow W_2; W_1 \\leftrightarrow\nY.$$\nHere, we can not sample variables one-by-one following the order {$X,W_1,W_2,Y$} since {$X,W_2$} and {$W_1,\nY$} are confounded and we have to sample them together, i.e., sample {$X, W_2$} or sample {$W_1, Y$}. But which one\nto sample first? If we consider sampling {$X, W_2$} first, we need $W_1$ as input to the model to generate $W_2$ since\n$W_1$ is $W_2$'s parent. If we consider sampling {$W_1, Y$} first, we need both $X_1$ and $W_2$ as input to the model\nfor the same reason. This creates a cycle in the sampling network.\n\nOur algorithm takes several steps to avoid this issue for any causal graph and identifiable interventional query. For\nexample, we train models for {$X, W_2$} considering all possible values of $W_1$ as input. Similarly, we train models for the 2nd c-component considering\nall possible values of its input variables. Finally, we connect the trained models from two c-components to build a\nsingle sampling network.\nIn this sampling network, we do not have to sample any variables together with another one, and we can follow the topological order to\nsample all variables.\nTherefore, we would state that sampling according to the topological ordering is one single part of our whole solution.\nThere are more subtleties such as:\n*  Building a sampling network: we carefully follow the steps of the ID algorithm to recursively divide the problem into\nc-components, train corresponding models, and build a consistent sampling network with the trained models.\n*  New interventional data generation: We generate new interventional datasets by training conditional models on\nobservational data and utilize them in later steps.\n* Maintaining sampling consistency: We avoid the cyclic dependency issue mentioned above by strategically updating the\ncausal graph and the intervention set.\n\nWe describe all these steps in more detail in the next paragraph."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8014/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700343465685,
                "cdate": 1700343465685,
                "tmdate": 1700343465685,
                "mdate": 1700343465685,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ysCssS8Yvh",
            "forum": "HFG7LcCCwK",
            "replyto": "HFG7LcCCwK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8014/Reviewer_jtV4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8014/Reviewer_jtV4"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes ID-DAG as an algorithm to turn any causal estimand into a set of conditional generative models that can be used to estimate the causal query. The paper tests the algorithm on a synthetic MNIST task as well as a covid chest x-ray dataset and shows promising results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper tackles the important problem of causal estimation in the presence of high-dimensional variables. It proposes a sound algorithm to efficiently estimate the causal query by expanding upon the ID algorithm. The experiments tackle interesting causal settings that would be difficult to solve with current causal estimation methods and show reasonable results."
                },
                "weaknesses": {
                    "value": "The paper's presentation could be a little clearer with more precise definitions of the notation (e.g. $p(...\\mid do(x=..))$ vs $p_x(...)$). Even though the paper cites [1] it simply states that they do not handle high-dimensional variables. It would be interesting to see a comparison of both methods to get a better understanding of the performance of ID-DAG. Generally, the evaluation is fairly limited and could be expanded upon. It mentions that the groundtruth interventional distribution in the case of MNIST is not accessible even though the data is synthetically generated. I recommend looking at a evaluation with MorphoMNIST [2] (and cite the package).\nAdditionally, it would be nice to spell out the differences of this algorithm in case of no unobserved confounding.\n\n[1] Kevin Xia, Kai-Zhan Lee, Yoshua Bengio, and Elias Bareinboim. The causal-neural connection: Expressiveness, learnability, and inference. Advances in Neural Information Processing Systems, 34:10823\u201310836, 2021.\n[2] Castro, Daniel C., et al. \"Morpho-MNIST: quantitative assessment and diagnostics for representation learning.\" Journal of Machine Learning Research 20.178 (2019): 1-29."
                },
                "questions": {
                    "value": "- It seems that most equations are written assuming discrete variables. Why is that?\n- Fig. 2 uses $Y_2$ while the text mentions $Y$ - is this a typo?\n- What does it mean for a digit to be thin or thick?\n- For the covid graph, in the real world wouldn't we assume an edge from C->N as well?\n- The evaluation generally could be more thorough. Things that could support this would be observational / interventional likelihoods or the evaluation that e.g. p(thickness) shouldn't change with a colour intervention.\n- Please have a look at the usage of \\citep vs \\citet."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8014/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699351119797,
            "cdate": 1699351119797,
            "tmdate": 1699636987927,
            "mdate": 1699636987927,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2PzHQBtCp8",
                "forum": "HFG7LcCCwK",
                "replyto": "ysCssS8Yvh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8014/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8014/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer jtV4 (1/3)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback and are happy to hear that they found the problem we are solving interesting\nand the method we are proposing sound and efficient. Below we address all their concerns.\n\n## Notation\n\n> The paper's presentation could be a little clearer with more precise definitions of the notation (e.g. $P(...|do(\n> x=...))$ vs $P_x(...)$}:\n\nWe thank the reviewer for pointing it out. Both notations are meant to represent the same notion. We will make it more\nclear in our background section.\n\n## MorphoMNIST package:\n\n> I recommend looking at an evaluation with MorphoMNIST [2] (and cite the package)\n\nWe thank the reviewer for suggesting the MorphoMNIST [2] package\nand would like to happily share that we are already using this package from\nhttps://github.com/dccastro/Morpho-MNIST/ for data generation of our MNIST image experiment.\nIn the napkin graph, three out of four variables are images: $W_1, X$, and $Y$. To generate these images we picked a random digit from the original MNIST dataset and thickened or thinned it using the MorphoMNIST package.\n\nNonetheless, evaluating our algorithm performance with this package would be certainly an interesting step. \nFor example: after training our models on the colored MNIST dataset, it would be interesting to observe how\nour models perform if we intervene with a digit image perturbed with swelling or fracture. How does using \na set of diverse and unseen interventions affect the interventional samples generated from the algorithm? We would\ninclude these evaluations in the camera-ready version of the paper.\nWe mentioned the URL in our paper, we will cite it now.\n\n## Evaluation with the ground truth and\n\n> It mentions that the ground truth interventional distribution in the case of MNIST is not accessible even though the\n> Data is synthetically generated.\n\nThis ground truth image dataset $D$ we generated corresponds to the observational distribution $P(W_1, X, Y)$.\nThus, for specific image $X$ we know how image $Y\\sim P(Y|X)$ will look like since we\ncan check that in the observational dataset $D$. However, the ground truth dataset $D$ does not let us know,\nhow image $Y\\sim P(Y|do(X))$ would look like for a specific image $X$ since those are interventional samples.\nThis is what we meant when we said that we do not have access to the ground truth interventional image data.\nHowever, we still evaluate our interventional samples. Please check below.\n\nSince we generated the images from some discrete properties such as digit$\\sim[0,9]$, thickness$\\sim[0,1]$, and color$\\sim[0,5]$ (\nusing MorphMNIST, etc), we have access to these discrete values. Therefore, we can use the original ID algorithm\nto estimate true $P(Y.color|do(X.color))$.\n\nAfter generating interventional image samples $Y\\sim P(Y|do(X))$ with our models, we use a classifier to obtain the color of\n$X$ and the generated $Y$, i.e, $X.color$ and $Y.color$. Thus, we can estimate $P'(Y.color|do(X.color))$ from our\ngenerated interventional samples and compare against the true $P(Y.color|do(X.color))$. We showed this comparison in Table 1 in our paper.\n\n[2] Castro, Daniel C., et al. \"Morpho-MNIST: quantitative assessment and diagnostics for representation learning.\"\nJournal of Machine Learning Research 20.178 (2019): 1-29.\n\n## Our algorithm when no unobserved confounding\n\n> It would be nice to spell out the differences of this algorithm in case of no unobserved confounding.\n\nWhen there exists no unobserved confounder in the causal graph, each variable is an individual c-component of size 1.\nThus, our algorithm will first visit step 4 and then directly go\nto step 6: the base case. At step 6, due to no confounding, we simply train one conditional model for each variable\n$V_i$ to match $P(V_i|V_{\\pi}^{(i-1)}))$, i.e., the probability of $V_i$ given all variables appear before it in the topological\norder. We will make it more clear in the paper.\n\nWhen there exist confounders in the causal graph, we would have larger c-components, and we will recursively solve the\nproblem for each c-component.\n\n## Equations with discrete variables\n\n> It seems that most equations are written assuming discrete variables. Why is that?\n\nWe followed the notation of the ID algorithm (Shpitser et al 2008), which is practical when variables are discrete,\nwhich is a major drawback. We used those equations to indicate which steps of\nthe ID algorithm we are following and how we are replacing them. Our algorithm is not limited to discrete variables and can adapt to\ncontinuous high-dimensional probability distributions. We will clarify that the p(.) could refer to any probability measure, continuous or discrete.\n\n* Shpitser et al (2008) \"Complete identification methods for the causal hierarchy.\"\n\n## Fig. 2 typo:\n\n> Fig. 2 uses $Y_2$ while the text mentions $Y$ - is this a typo?\n\nWe thank the reviewer for pointing out the typo. We apologize for this and already fixed it."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8014/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700346928767,
                "cdate": 1700346928767,
                "tmdate": 1700346928767,
                "mdate": 1700346928767,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]