[
    {
        "title": "LATEC \u2014 A benchmark for large-scale attribution & attention evaluation in computer vision"
    },
    {
        "review": {
            "id": "GH8q5No9L2",
            "forum": "tQYsKBTTaV",
            "replyto": "tQYsKBTTaV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2401/Reviewer_aNQJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2401/Reviewer_aNQJ"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a framework for benchmarking attribution-based XAI methods, which combines many previously proposed metrics regarding faithfulness, robustness, and complexity of the attributions. Using this framework, the authors evaluate a wide range of different attribution methods on a variety of models and datasets, including CNNs and ViTs, as well as 2D, 3D, and point-cloud inputs. \n\nThe authors analyse and discuss their results and specifically aim to answer three questions they deem relevant in the context of attribution-based XAI methods, namely whether (1) attention-based attributions differ from 'classical' attribution methods, (2) whether the applicability of attribution methods depends on the modality, and (3) whether 'classical' methods still work for transformer models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The following aspects that strengthen this submission:\n\n**S1**: The experimental evaluation is very extensive, covering a wide range of attribution methods, models, and evaluation metrics on multiple datasets. To achieve this, the authors adapt existing approaches and models to point-cloud and 3D volume inputs when necessary.\n\n**S2**: By making not only the code but various intermediate results publicly available (model weights, attribution maps, metric scores), the framework proposed by the authors allows for easily integrating additional evaluations, methods, and models. This is an important step to increase comparability and allow researchers to fairly test new XAI methods against existing approaches.\n\n**S3**: The experimental details are clearly described and code for reproducing the results has been made available."
                },
                "weaknesses": {
                    "value": "While the publicly available benchmark can surely prove useful to fellow researchers, I am hesitant to recommend the submission for acceptance for the following reasons.\n\n**W1**: In order to deal with the large number of metrics across multiple datasets, modalities, and models, the authors heavily summarise the results into the broad categories of _faithfulness_, _robustness_, and _complexity_. While this seemingly allows to 'zoom out', I am concerned that this effectively hides much of the complexity of the design choices involved and makes it difficult to understand what one can really deduce from the results. E.g., as the authors note, the _complexity_, which is supposed to be a proxy for the subjective 'human interpretability' of the attributions does not seem to actually coincide with the perceived complexity. What is the value of this measure then? How do we know that the same is not true for faithfulness (see also W2)?\n\n**W2**: For every subcategory, the authors essentially treat all metrics as being equally relevant and adequate for measuring, e.g. _faithfulness_. However, it is unclear to me whether it is meaningful to summarise these metrics into a single score, as each of them measure a different aspect of the models and define 'faithfulness' differently. This can also be observed in Fig. 9 in the appendix, as the different metrics for faithfulness do not seem to exhibit a reliable and general trend (compare, e.g., monotonicity and deletion). Further, they are sometimes based on assumptions that might not hold for a given model. E.g., models might be differently robust to pixel insertion or deletion and it is thus unclear whether these metrics measure aspects of the _underlying model_ or the _XAI method_. How stable are the findings when computing the ranks on different subsets of the metrics? If they are highly dependent on the chosen set of metrics, what do we really learn from the summary?\n\n**W2b**: The authors missed highly relevant studies that also extensively study a wide range of attribution methods according to different criteria (e.g., Hesse et al., 2023, Rao et al. 2022). A comparative discussion to such works seems necessary, especially as those works explicitly try to avoid the pitfalls raised in W2 (i.e., disentangling model behaviour from the attribution methods).\n\n**W3**: The implications and relevance of questions Q1-Q3 as well as their answers are not fully clear to me and I would appreciate if the authors could elaborate. \n\n**W4 (minor)**: The manuscript is very dense and not easy to read and I encourage the authors to place a particular focus on making the writing more accessible. For example, using non-standard abbreviations for the various methods makes it tedious to read the main text, as one needs to constantly look up what a particular abbreviation stands for (e.g., LI for LIME or LA for 'LRP Attention', etc.). Further, it is difficult to understand a given table or figure from figure + caption alone, which makes the results much less accessible (this is heavily aggravated by the non-standard abbreviations, see e.g., Fig. 3c). \n\n**W5 (minor)**: The relevance and implications of the discussion of Fig. 3a are unclear to me and the 'distinct clusters' that form seem to be an exaggeration of what is visible in the plots. Without the distinct colourings and the boxes drawn around some of the points, these points seem fairly uniformly distributed to me."
                },
                "questions": {
                    "value": "Please see weaknesses. If the authors convince me that my concerns are unwarranted, I will consider raising my score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2401/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698163008408,
            "cdate": 1698163008408,
            "tmdate": 1699636175592,
            "mdate": 1699636175592,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aak7XooKfA",
                "forum": "tQYsKBTTaV",
                "replyto": "GH8q5No9L2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2401/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2401/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Answer to W1 & W2:**\\\nThank you for this important feedback. We now provide an extensive metric-analysis in the paper (Appendix G) and modified the main text to reflect the increased awareness. In short, our analysis shows that aggregated ranks do indeed convey meaningful signals and are not significantly biased and hard to cheat in practice.\n\nWe identified the main concern as the meaningfulness of our results given the potentially subjective selection and weighting in metric aggregation. Currently, studies use one or two metrics to reflect the criteria, leading to the displayed inconsistencies and even contradictions. Crucially, it is also common practice to aggregate over the (few) metrics (see e.g. Hesse et al. [1], brought up in your review, or Hedstr\u00f6m et al. [2]). In contrast, our approach gets more robust insights by including a large number of adequate metrics per criterion. Adequacy is ensured by only selecting standard metrics in the field that are e.g. also implemented in the widely used Quantus library [2]. While individual metric-results are presented in Appendix K for assessing specific designs, they are too complex for readers; thus, we follow common practice in reporting aggregated ranks in the main paper for clarity.\n\nConcerning the meaningfulness of aggregated ranks, we argue that instead of measuring \u201cdifferent aspects\u201d (like F1 and accuracy measure different aspects of classification performance), XAI metrics are rather nuanced proxy implementations aiming to measure the same specific aspect (e.g. faithfulness). Thus, aggregating over many of these proxies results in a robust rank invariant to individual implementations. Simultaneously, disagreement between several metrics provides essential insights for XAI evaluation, reflecting the diversity of implementations. Our new analysis shows that the aggregated ranks are indeed no meaningless mix, but in fact, convey meaningful signals based on the (dis)agreement of individual metrics. Further, we empirically see that variance in metric rankings is invariant to model architecture and dataset choice, and instead sensitive to XAI method selection as intended (see Appendix G for analysis using the Levene-Test).\n\nWe agree that in scenarios of high disagreement among metrics, potential biases could arise when considering only a limited subset of metrics, a concern we have addressed in contrast to other studies via our large-scale setting. Our analysis suggests, that while it is in theory possible to selectively pair similar-ranking metrics to deliberately skew results, the actual risk of undue influence from such subgroups in our study is minimal.\n\nOur critique of complexity metrics, which can be arguably subjective, was methodological in nature and not based on metric disagreement, focusing on the unsuitability of their mathematical formulations for high-dimensional image data. This concern does not extend to faithfulness and robustness metrics, as these criteria are well-defined and more straightforward to express through mathematical mechanisms.\n\n**Answer to W2b:**\\\nWe thank you for highlighting these relevant studies. We've included [1] in the main manuscript. Moreover, in the added metric-analysis, we compare ourselves to their approach for metric aggregation and evaluation of the metric-sensitivity to the model. We would like to highlight that, unlike our real-world study, both approaches include explanation \"ground truths\". This makes it for example impossible for us to compute a metric like Background Independence from [1].\n\n**Answer to W3:**\\\nQ1: Comprehending the differences between attention and attribution methods is crucial to determine whether new attention methods can be universally applied or if they present specific drawbacks or advantages over traditional methods.\\\nQ2: While almost all XAI methods are designed for structured or image data, we don\u2019t know if their applicability also transfers to other computer vision modalities, hindering their range of application.\\\nQ3: Attribution methods are widely employed; however, it remains unclear whether their application to popular Transformer models parallels their use with CNNs. Thus ensuring their applicability with forthcoming DL architectures is crucial.\n\nWe have updated the introduction of our work to express the significance of the three questions. \n\n**Answer to W4:**\\\nWe acknowledge that the abbreviations restrict readability. To address this issue, we have created a clean List of abbreviations on the first page of the appendix, which provides a quick and accessible reference for all abbreviations used in the manuscript.\n\n**Answer to W5:**\\\nWe agree that the term \"distinct clusters\" could be misleading and have toned down this statement to convey that similar operating methods tend to group together, which implies a non-random structure to the rankings.\n\n[2] Hedstr\u00f6m et al. (2023). An explainable ai toolkit for responsible evaluation of neural network explanations and beyond."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2401/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700218175556,
                "cdate": 1700218175556,
                "tmdate": 1700218175556,
                "mdate": 1700218175556,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "H4BfaOvffF",
                "forum": "tQYsKBTTaV",
                "replyto": "aak7XooKfA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2401/Reviewer_aNQJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2401/Reviewer_aNQJ"
                ],
                "content": {
                    "title": {
                        "value": "Answer to the authors and additional discussion"
                    },
                    "comment": {
                        "value": "**Re W2b:** I appreciate that this concern has been addressed by the authors. \n\n\n**Re W4:** I appreciate that the authors understand my concern and that a list of abbreviations has been included. I would still heavily recommend to use full names or common abbreviations where possible. E.g., why abbreviate LIME to LI in the first place? Why abbreviate GradCAM with GC and GradCAM++ with C+? There is very limited gain from the abbreviations introduced by the authors, but they heavily restrict readability. In this context, note also that DL is used to represent \"Deep Learning\" and \"DeepLIFT\". \n\n\n**Re W5:** I appreciate that this concern has been addressed by the authors. \n\n\n**W6 (reliability of the conclusions drawn for XAI methods):** The authors write on page 4: \"XAI methods like LRP, DS, DLS, and CAM require non-negative activation outputs\" without providing a reference for it. As far as I understand, this is an incorrect characterisation of the methods, and I believe all of them are able to handle negative activations; I would appreciate if the authors could explain why this should be the case and ideally provide references. Similarly, the claim that \"[Architectural elements of the transformer] break the conservation rule for relevance backpropagation methods, (see [1] for a detailed analysis)\" in [the response above](https://openreview.net/forum?id=tQYsKBTTaV&noteId=dXtg4kyfWv) is an inaccurate characterisation of the discussion in  [1] as well as of LRP as an XAI framework (see below).\n\nThis brings me to a more general concern that I have, which is conceptually similar to W1: to make reliable statements about any given XAI method, it should be ensured that methods are used correctly and compared fairly. Otherwise, a benchmark as the one presented here might in fact *hurt* the research community by cementing certain views about XAI methods based on an inaccurate representation of a given method. This becomes most clear with LRP: specifically, LRP is not a monolith, but rather a general framework that subsumes many other approaches (e.g., depending on the rules chosen, it can be equivalent to Input x Gradient or the method presented by Chefer et al [1]). For an extension of LRP to transformers, i.e., one that defines attention-specific rules, see also Ali et al. (ICML, 2022, \"XAI for Transformers\"). \n\nAs mentioned in W1, my concern is that the heavy summarization for the metrics as well as the XAI methods makes it difficult to understand what we really learn. E.g., the authors say hyperparameters for the individual XAI methods were chosen based on a \"qualitative evaluation\" (page 3), which is highly subjective to begin with, but further does also not seem to include the LRP rules, despite the fact that (for CNNs at least) specific recommendations on which rules to use for which layers exist (see Montavon et al., 2019).  Given that the choice of hyperparameters (and rules for LRP) can drastically change the performance of most XAI methods, I am not convinced that the described procedure does justice to the methods that were evaluated.\nThis also directly implicates the findings regarding Q2: could the variations across modalities be due to the authors' adaptations and specific hyperparameter choices or is there a principled reason that some XAI methods are simply not compatible with other modalities?\n\n**W7 (novelty):** I also agree with the novelty concern raised by RKNb [see here](https://openreview.net/forum?id=tQYsKBTTaV&noteId=MKv9yJBgug). Specifically, apart from uploading intermediate results, I am not fully convinced that the proposed approach is significantly novel and different in its utility than the Quantus library itself (on which the authors also rely) or provides sufficiently novel and convincing insights; in fact, note that with 6 distinct evaluation categories, Quantus arguably allows for a broader understanding of a methods' behaviour. This concern is aggravated by my concerns regarding the insights we truly gain by simply aggregating the results of all metrics (W1+2) for a fixed choice of hyperparameters for each method which was chosen based on an unspecified qualitative inspection (W6).\n\n**W8 (minor):** Not all changes are integrated well and I would recommend that the authors carefully revise for readability. E.g.: The change on page 9 results in a sentence that seems to not make sense. Similarly, the newly added sentence on page 4 as well as the first sentence on page 2 (These questions...) are hard to understand."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2401/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700583653873,
                "cdate": 1700583653873,
                "tmdate": 1700583653873,
                "mdate": 1700583653873,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qBsv8Oe7Az",
            "forum": "tQYsKBTTaV",
            "replyto": "tQYsKBTTaV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2401/Reviewer_dkvT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2401/Reviewer_dkvT"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes LATEC, a benchmark for large-scale attribution and attention evaluation in computer vision tasks. It evaluates 17 prominent XAI methods, with 20 different metrics. The dataset has been made publicly available."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* Overall I appreciate the effort of curating such a large-scale benchmark for XAI methods and metrics. I am not an expert in this particular field but I do believe it can be beneficial to the community. \n* The presentation and description of the benchmark as well as the dataset is quite clear. \n* The empirical study which aims to address the three pivotal questions makes a lot of sense. \n* The discussion on insights and main takeaways are very clear and easy to follow."
                },
                "weaknesses": {
                    "value": "* Particularly for faithfulness, from the dataset it is very much uniformly distributed. Does that mean this is not a meaningful metric to consider? \n* Is there any interesting interplay between faithfulness, robustness and complexity? \n* It is mentioned that theoretically biased XAI methods are less faithful on Transformers, but it is not clear why (from Sec. 3)? At least can you give some intuitions? \n* Texts in some images, e.g. Fig 2 are a bit small and difficult to read."
                },
                "questions": {
                    "value": "see above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2401/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698750450336,
            "cdate": 1698750450336,
            "tmdate": 1699636175490,
            "mdate": 1699636175490,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dXtg4kyfWv",
                "forum": "tQYsKBTTaV",
                "replyto": "qBsv8Oe7Az",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2401/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2401/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Answer to W1:**\\\nThe uniform distribution of faithfulness metrics across the highest level of aggregation indicates that this measure does not discriminate well between methods under these broad conditions. However, when we delve into specific groups (modalities, models, datasets, XAI methods), we observe variations that suggest faithfulness can indeed be a meaningful metric (see Table 2). Based on your appreciated feedback we also further explored and discussed this variation of faithfulness in specific conditions in the newly added Appendix section G, where we analyze metric behavior across different scenarios.\n\n**Answer to W2:**\\\nWe would argue that, theoretically, there can be a \"trade-off\" between faithfulness and complexity. A very faithful map can be very noisy, thus more complex, and very uncomplex maps such as CAM maps can be evaluated as quite unfaithful as they only highlight a general region of interest, not the exact pixels/features the model uses. Although a trade-off is noticeable for methods like LRP, GC, or EG, we advise against overinterpreting these findings in light of our reservations regarding the complexity metrics.\n\n**Answer to W3:**\\\nSpecific architectural choices in Transformer architectures, such as negative activation outputs, matrix multiplication in attention layers, or skip connections can, in theory, affect attribution methods. For example, this breaks the conservation rule for relevance backpropagation methods, (see [1] for a detailed analysis). In response to your valuable comment, we made the argument more prominent, especially in Section 3.\n\n**Answer to W4:**\\\nWe agree and increased the text size.\n\nWe appreciate that there were no further significant wishes for improvement to the manuscript and hope to have addressed all questions to your satisfaction. Please let us know if there are any remaining concerns that could prevent you from increasing your score.\n\n\n\n[1] Chefer, H., Gur, S., & Wolf, L. (2021). Transformer interpretability beyond attention visualization. In Proceedings of the IEEE/CVF (pp. 782-791)."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2401/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700217875119,
                "cdate": 1700217875119,
                "tmdate": 1700217875119,
                "mdate": 1700217875119,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MKv9yJBgug",
            "forum": "tQYsKBTTaV",
            "replyto": "tQYsKBTTaV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2401/Reviewer_FKNb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2401/Reviewer_FKNb"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a benchmark for XAI methods LATEC. \n\n- *Methods to benchmarks:* This benchmark includes 17 different methods, this include both traditional (i.e saliency/gradient-based) methods trained on CNNs and Transformers and attention-based methods trained on transformers. \n\n- *Datasets:* The benchmark considers 3 computer vision data modalities Images, volume(3D data) and point cloud. For each modality 3 different datasets were considered.\n\n- *Benchmarking Metrics:* The paper investigates 3 aspects of XAI methods faithfulness, robustness and complexity. While the paper does not introduce any metrics itself it investigates the performance of different XAI methods on 20 previously proposed metrics and groups them into the 3 aspects.\n\n Overall the LATEC consists of 7,560 different combinations that were tested, after running this benchmark the paper summarizes the takeaways as follows:\n- No XAI method ranks consistently high on all evaluation criteria. \n- The rankings of XAI methods generalize well over datasets from the same modality.\n- The complexity metrics proposed for CV tasks does not always have to match the perception of low complexity.\n\nThe paper then uses LATEC to answer 3 open XAI questions:\n\n - How does the performance of attention versus attribution methods differ in practice? They found a large difference in complexity (attention methods are more complex)  and a smaller difference in robustness  (attribution methods are more robust), while the difference in faithfulness is substantially insignificant. However, the paper does mention that the complexity methods are generally debatable since they favor methods that attribute to the smallest set of single pixels.\n\n- Does the efficacy of XAI methods vary across different computer vision modalities? The performance varies across different modalities. \n\n- With the ascendency of Transformer architectures, is there a potential misalignment with established attribution-based XAI methods?  The benchmark showed that the faithfulness of different attribution methods highly fluctuates for transformers and that attribution based methods are not necessarily a wrong choice when using a transformer architecture."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The main advantage of this benchmark is:\n- (a) It considers different neural architectures.\n- (b) It considers different data modalities.\n- (c) It considers both traditional XAI methods and attention-based methods for transformers.\n- (d) It considers different aspects in XAI that researchers or users might care about (i.e faithfulness, robustness and complexity).\n- (e) Most of the popular XAI metrics were included.\n- (f) It's very comprehensive overall, 7,560 different combinations were tested.\n-(g) The code is structured in a way that it is very easy to add to either a new method or a new metric, which can make it an excellent resource for open-source collaborations in the future.\n\n\nThe paper is well-written and easy to follow. The takeaways from the experiments are clearly stated."
                },
                "weaknesses": {
                    "value": "Novelty is limited: No new datasets or evaluation metrics were introduced in this benchmark.\n\nAlthough different data modalities were considered, this benchmark only applies to computer vision tasks, the performance of different   XAI methods on other data types like tabular, time series, and language was not investigated."
                },
                "questions": {
                    "value": "- Do you think the ranking of methods would have varied if synthetic baselines with ground-truth were included?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2401/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2401/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2401/Reviewer_FKNb"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2401/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698993849239,
            "cdate": 1698993849239,
            "tmdate": 1699636175404,
            "mdate": 1699636175404,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "K9lDuWF12M",
                "forum": "tQYsKBTTaV",
                "replyto": "MKv9yJBgug",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2401/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2401/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Answer to W1:**\\\nWe acknowledge the reviewer's perspective on the novelty of our benchmark; however, we posit that the proliferation of datasets, metrics, and methods in the field necessitates a comprehensive and foundational analysis before introducing additional complexity. Our benchmark demonstrates that rankings are robust across datasets, indicating that the diversity of datasets is not a constraint on the insights gained. Moreover, by making all intermediate steps of our evaluation process publicly available, we have laid the groundwork for the easy integration of new metrics and methods into our framework. This extensibility is a significant contribution to our work and one which we will emphasize further in the conclusion to underscore the benchmark's potential as a foundational platform for future explorations in the field.\n\n**Answer to W2:**\\\nWe agree with the significance of multi-modality in XAI research. However, we believe that expanding beyond the three modalities covered in this paper would have exceeded its intended scope. We fully support and anticipate future extensions of our benchmark to encompass additional modalities, and we have already earmarked this as a direction for our future research endeavors.\n\n**Answer to Q1:**\\\nIn our opinion, this would have heavily depended on the definition and implementation of ground truth and the synthetic baseline. Arras et al. [1] stated: \u201cIn many cases there probably exists no perfect Ground Truth\u201d and there is a trade-off between realism and evaluation precision for synthetic evaluation data. However, the inclusion of a ground truth on synthetic data allows for much more detailed metrics regarding faithfulness and robustness (see [2]) as well as simple accuracy metrics for localization (see [3]). While the implications of these metric-results for real-world contexts such as ours remain unclear, exploring the transferability of results from artificial to real-world settings presents an intriguing avenue for future research in metric and benchmark evaluation.\n\nWe appreciate your indication that there are no significant remaining requests for enhancements to our manuscript and thank you for the insightful question and the interest to already extending the benchmark. Please let us know if there are any remaining concerns that could prevent you from increasing your score.\n\n[1] Arras, L., Osman, A., & Samek, W. (2022). CLEVR-XAI: A benchmark dataset for the ground truth evaluation of neural network explanations. Information Fusion, 81, 14-40.\\\n[2] Hesse, R., Schaub-Meyer, S., & Roth, S. (2023). FunnyBirds: A Synthetic Vision Dataset for a Part-Based Analysis of Explainable AI Methods. In Proceedings of the IEEE/CVF (pp. 3981-3991).\\\n[3] Zhang, J., Bargal, S. A. et al. (2018). Top-down neural attention by excitation backprop. International Journal of Computer Vision, 126(10), 1084-1102."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2401/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700217771720,
                "cdate": 1700217771720,
                "tmdate": 1700217771720,
                "mdate": 1700217771720,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vIB4yr4pVh",
                "forum": "tQYsKBTTaV",
                "replyto": "K9lDuWF12M",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2401/Reviewer_FKNb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2401/Reviewer_FKNb"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for their response. \nMy score remains as is."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2401/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700498289668,
                "cdate": 1700498289668,
                "tmdate": 1700498289668,
                "mdate": 1700498289668,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jbbS67l6sq",
            "forum": "tQYsKBTTaV",
            "replyto": "tQYsKBTTaV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2401/Reviewer_Kney"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2401/Reviewer_Kney"
            ],
            "content": {
                "summary": {
                    "value": "Explainable AI (XAI) is an important and rapidly growing research area that aims to propose XAI methods for a better understanding of complex machine learning decisions. This paper performs a comprehensive evaluation of existing XAI methods to analyze their advantages and disadvantages in three aspects, including faithfulness, robustness, and complexity. Specifically, the proposed LATEC is a large-scale benchmark that evaluates 17 XAI methods using 20 distinct metrics. Furthermore, it extends the evaluation from 2D images to 3D point clouds. As a result, it incorporates vital elements like varied architectures and diverse input types, resulting in 7,560 examined combinations. The code, models, and data are publicly available."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed LATEC performs an extensive evaluation of 17 XAI methods using 20 distinct metrics, which incorporates 7,560 examined combinations. The evaluation results are solid.\n2. LATEC proposes solutions to extend the evaluation of XAI methods from 2D images to 3D point clouds, leading to more comprehensive benchmark results. \n3. The code, models, and data are available to evaluate customized XAI methods or metrics."
                },
                "weaknesses": {
                    "value": "1. The presentation is hard to read and understand. It is an experimental report rather than a well-organized research paper.\n2. As a benchmark and analysis work, The takeaways from this paper are not insightful but rather just some straightforward observations. After reading this paper, I am not able to gain good insights about the good way to evaluate XAI methods.\n\nOverall, this paper makes solid experiments. However, it fails to reveal convincing explanations and insightful comments. This paper is an experimental report rather than a well-prepared paper."
                },
                "questions": {
                    "value": "See weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2401/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699041847564,
            "cdate": 1699041847564,
            "tmdate": 1699636175328,
            "mdate": 1699636175328,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eqLrilvqGR",
                "forum": "tQYsKBTTaV",
                "replyto": "jbbS67l6sq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2401/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2401/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Answer to W1:**\\\nWe thank you for your critique regarding the presentation of our manuscript. We would like to point out that our submission aligns with the structure and style that is customary for the benchmark and dataset track of this conference, which should not be confused with the structure of a classical method proposal. Exemplary, we selected five previously accepted and outstanding ranked ICLR benchmark and dataset papers, that have similar structure: Jaeger et al. (Notable Top 5%) [1],  Galil et al. (Notable Top 25%) [2], Vaze et al. (Oral) [3], Zong et al. (Notable Top 25%) [4] and Galil et al. [5]. We followed these successful ICLR papers in their structure: \n- Formulating/Motivating Research Question\n- Benchmark Design\n- Experiment Evaluation\n- Insights and Take-aways.\n\nNotably all other reviewers specifically acknowledged that our paper is \u201cwell thought out presentation\u201d (\u201c6LcT\u201d), \u201cwell-written and easy to follow\u201d (\u201cFKNb\u201d), \u201cpresentation and description of the benchmark [...] is quite clear\u201d (\u201cdkvT\u201d) and \u201cexperimental details are clearly described\u201d (\u201caNQJ\u201d). We would greatly appreciate it if you could specify the sections that you found to be lacking in organization so that we may enhance those areas.\n\n**Answer to W2:**\\\nWe would like to clarify that, while our evaluative strategies may not differ conceptually from existing ones (and were not intended to), the scale of our analysis is unprecedented and the dimensionality is unique. The resulting insights regard the application of XAI methods, not their evaluation, and exemplary show how relevant knowledge can be derived from our benchmarking.\n\nSpecifically, our benchmark offers a practical tool for practitioners to test various XAI methods across different metrics suited to their specific problem configurations. For researchers developing new XAI methods or metrics, our recommendations for testing across diverse datasets, modalities, and model architectures ensure robust and meaningful insights. Only because we followed these recommendations, we resolved prior inconsistencies in method evaluations (e.g. ignoring attention methods and modalities besides images) and showcased how benchmarking according to these recommendations is highly relevant. We have amended the manuscript to better highlight these contributions and ensure the insights gained are clear and actionable.\n\nWe thank you for pointing out the lack of clarity in this context and believe that our respective changes will avoid future confusion. As we believe to have resolved your comments, please let us know if there are any remaining concerns that could prevent you from recommending acceptance.\n\n[1] Jaeger, P. F. et al. (2022). A call to reflect on evaluation practices for failure detection in image classification.\\\n[2] Galil, I.  et al. (2023). A framework for benchmarking class-out-of-distribution detection and its application to ImageNet.\\\n[3] Wiles, O. et al (2021). A fine-grained analysis on distribution shift.\\\n[4] Zong, Y. et al.  (2022). MEDFAIR: Benchmarking fairness for medical imaging.\\\n[5] Galil, I. et al. (2023). What can we learn from the selective prediction and uncertainty estimation performance of 523 imagenet classifiers."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2401/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700217655707,
                "cdate": 1700217655707,
                "tmdate": 1700217655707,
                "mdate": 1700217655707,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IMGiVB3Vpw",
            "forum": "tQYsKBTTaV",
            "replyto": "tQYsKBTTaV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2401/Reviewer_6LcT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2401/Reviewer_6LcT"
            ],
            "content": {
                "summary": {
                    "value": "The authors developed a large-scale benchmark that critically evaluates 17 XAI methods using 20 distinct metrics, using three guiding questions of current interest. The authors perform non-trivial statistical analyses to compare different methods along 3 evaluation criteria (faithfulness, robustness, and complexity), with special attention to the specifics of each method/dataset, as well as the interplay between attribution and attention (as in Transformer models). The authors go the extra step by making all their data available, e.g., model weights and saliency maps, to further facilitate future work."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- Well thought out presentation, detailing all steps taken and analyses applied, with high quality figures.\n- Excellent reproducibility.\n- Evaluates on point cloud and image volume inputs, beyond traditional evaluations on 2D images.\n- Clear summary of main findings and takeaways."
                },
                "weaknesses": {
                    "value": "Nothing stands out beyond the few points raised below."
                },
                "questions": {
                    "value": "**Presentation:**\n- Section 1:\n    - The citations on the first few sentences aren't directly tied to the citing text. Perhaps better references can be used, or the text can be modified slightly to better link to those cited article. Please clarify whether those articles provide supporting evidence, focused on subproblems or application domains, or whether their authors simply echoed similar opinions.\n    - Follow-up: it seems this discussion at the beginning is focused on saliency maps specifically. If so, please clarify the scope on the onset to be focused on XAI for computer vision models, with saliency methods as the primary approach being discussed. General statements about state-of-the-art in XAI would require an equally general selection of citations.\n    - Indeed, the authors seem to repeatedly use \"XAI research\" to solely mean works on computer vision.\n    - Please also clarify why saliency methods were chosen as the primary approach for this study, with a forward reference to a literature review highlighting other approaches.\n- Section 2:\n    - Please include a reference to Fig.1 in the main text.\n    - S2.1: it would help to include a brief description of each XAI method, e.g., in an appendix, together with references to recent surveys. (perhaps in a form similar to Appendix B.2)\n    - Figure 3/4: it seems the legend shows significance indicators that don't actually appear in the figures.\n\n**Nitpicking:**\n- Page 3: without adaptions -> adaptation? (I found 7 or so other occurrences)\n- S2.2: Due to the LATEC dataset -> Thanks to? Using?\n- S3.1: Transformer architecture inherit attention -> inherent?\n- Page 6: commendable more robustness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2401/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2401/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2401/Reviewer_6LcT"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2401/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699046898871,
            "cdate": 1699046898871,
            "tmdate": 1701029158975,
            "mdate": 1701029158975,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ljgSOigEoW",
                "forum": "tQYsKBTTaV",
                "replyto": "IMGiVB3Vpw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2401/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2401/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Answer to Q1 (Section 1):**\\\nThank you for your valuable feedback regarding the citations in Section 1 of our manuscript. Upon reviewing the section, we concur that the initial references were not optimally aligned with the corresponding text. To address this, we have carefully revised the text to more accurately reflect and connect with the cited articles. While most citations reflect the definition of a term or method (e.g. for faithfulness, robustness, or complexity), others give supporting evidence (e.g. (Adebayo et al., 2018) or (Wang et al. (2019); Rosenfeld (2021))), or are examples for applications (e.g. Doshi-Velez& Kim (2017); Ribeiro et al. (2016); Shrikumar et al. (2017)).\n\n**Answer to Q2 (Section 1):**\\\nWe agree with your observation that the term \"XAI methods\" was initially defined too broadly. Accordingly, we have refined the manuscript to specify our focus on saliency map methods right from the beginning.\n\n**Answer to Q3 (Section 1):**\\\nWe appreciate the opportunity to clarify the scope of our research. The shortcomings identified in our work, though applicable across various domains of XAI research, are indeed addressed within the confines of computer vision modalities in this paper. In light of this, we have revised the manuscript to clearly define our focus on computer vision within the broader context of XAI research. We have made the necessary adjustments in the manuscript to ensure that our narrative accurately represents the scope and boundaries of our work.\n\n**Answer to Q4 (Section 1):**\\\nIn our manuscript, we posit that saliency maps represent the most prevalent post-hoc XAI methods in computer vision. These maps, while primarily illuminating \"what\" aspects of the data are influential in a model's decision-making process rather than \"why,\" continue to form the foundational basis of most XAI analyses [1,2,3]. We underscore the criticality of trustworthy saliency maps at this stage, as mistakes can have critical consequences in subsequent analysis, leading to a misdirected emphasis on irrelevant input features. Moreover, we highlight the evolving sophistication of saliency maps, as advancements in attention and concept methods [4] show promise for the future of saliency maps. It is also noteworthy that post-hoc methods such as saliency maps offer ease of integration with existing model training and evaluation workflows, a practical advantage over ante-hoc XAI methods.\n\nIn response to the feedback, we have adapted the manuscript to provide a more comprehensive overview of the XAI field, emphasizing the significance of saliency methods within this domain. We have included references to two exemplary approaches, counterfactual examples, and concept testing, and cited an extensive literature review to offer readers a full perspective of the field.\n\n**Answer to Q5 (Section 2):**\\\nThank you for pointing out the oversight. We have now included a direct reference to Figure 1 in Section 2.\n\n**Answer to Q6 (Section 2):**\\\nWe agree that this would be beneficial for readers who are new to the field, or just miss the knowledge about a specific method. In accordance with your advice, we have included a new section in the appendix that offers concise summaries of the XAI methods discussed, akin to the structure presented in Appendix B.2. We trust that this addition will enrich the informational value of our work for all readers.\n\n**Answer to Q7 (Section 2):**\\\nThank you for bringing this to our attention. The significance indicators are indeed present above the figures but are rendered at a size that may compromise their visibility. We acknowledge this oversight and have since adjusted the size of the significance indicators to ensure they are clearly visible within the figures.\n\n**Answer to Q8 (Nitpicking):**\\\nWe thank you for the comments and revised the manuscript accordingly.\n\nWe appreciate the thoughtful questions you raised, as they have significantly contributed to the improvement in the clarity and explicitness of our manuscript. Please let us know if there are any remaining concerns that could prevent you from increasing your score.\n\n\n[1] Hauser, K. et al. (2022). Explainable artificial intelligence in skin cancer recognition: A systematic review. European Journal of Cancer, 167.\\\n[2] Gevaert, C. M. (2022). Explainable AI for earth observation: A review including societal and regulatory perspectives. International Journal of Applied Earth Observation and Geoinformation, 112.\\\n[3] Van der Velden, B. H. et al. (2022). Explainable artificial intelligence (XAI) in deep learning-based medical image analysis. Medical Image Analysis, 79.\\\n[4] Achtibat, R. et al. (2023). From attribution maps to human-understandable explanations through Concept Relevance Propagation. Nature Machine Intelligence, 5(9)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2401/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700217492703,
                "cdate": 1700217492703,
                "tmdate": 1700217492703,
                "mdate": 1700217492703,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OtfTS52xZp",
                "forum": "tQYsKBTTaV",
                "replyto": "ljgSOigEoW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2401/Reviewer_6LcT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2401/Reviewer_6LcT"
                ],
                "content": {
                    "title": {
                        "value": "Acknowledgement"
                    },
                    "comment": {
                        "value": "Thank you for addressing my comments.\n\nI will be revising my score based on the discussion with other reviewer."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2401/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587270946,
                "cdate": 1700587270946,
                "tmdate": 1700587270946,
                "mdate": 1700587270946,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]