[
    {
        "title": "ChEF: A Comprehensive Evaluation Framework for Standardized Assessment of Multimodal Large Language Models"
    },
    {
        "review": {
            "id": "audv7xD6cs",
            "forum": "ClqyY6Bvb7",
            "replyto": "ClqyY6Bvb7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission177/Reviewer_ZiWA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission177/Reviewer_ZiWA"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a comprehensibe evaluation framework ChEF for evaluating Multimodal Large Language Models.  ChEF consists of four modular components and allows for versatile evaluations in a standardized manner by designing new \"recipes\". The authors conduct evaluation of nine MLLMs  across various scenarios."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. ChEF is modularly designed with four components, Scenario, Instruction, Inferencer, and Metric, which facilitates versatile evaluations in a standardized framework and easy set up pf new evaluations.\n2. ChEF evaluates six capabilities that a competent MLLM model should possess, through constructing corresponding evaluation pipelines from a ChEF Recipe. These capabilities have not been systematically evaluated in exisiting MLLM Benchmarks.\n3. The authors evaluate the generalizability of nine MLLMs across various scenarios and their composite capability for multimodal\ninteractions, and summarize valuable observations."
                },
                "weaknesses": {
                    "value": "1. I am not certain if it is fair to incorporate current MLLM benchmarks into ChEF. These benchmarks have taken a significant amount of time to develop, so what is the core contribution of ChEF?\n2. Besides in-context learning, ChEF only evaluates single-image input. However, the comprehension of multi-image input is also an important assessment dimension for MLLMs."
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "In my opinion, no ethics review are needed."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission177/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission177/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission177/Reviewer_ZiWA"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission177/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698561975214,
            "cdate": 1698561975214,
            "tmdate": 1699635943290,
            "mdate": 1699635943290,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XmzBYNIf7l",
                "forum": "ClqyY6Bvb7",
                "replyto": "audv7xD6cs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission177/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission177/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ZiWA"
                    },
                    "comment": {
                        "value": "Before addressing your specific comments and questions, we would like to kindly inform you that we have provided an overall response to all reviewers. We believe that reading this response first will offer a comprehensive view of the revisions and clarifications we have made in light of the feedback received. We appreciate your time and effort in reviewing our paper. Following this note, I will proceed to address your specific concerns in detail.\n\n### Q1: What is the core contribution of ChEF?\n\nWe sum up the core contribution of our work in section 4 of our overall response. Various evaluation pipelines and criteria make fair comparison within the same evaluation pipeline a considerable challenge for MLLMs. ChEF effectively addresses these issues, enabling unified evaluation, and providing many interesting observations provided in our paper and supplementary materials. ChEF also propose evaluation on several capabilities that beyond visual performance, which have not been evaluated before. \nAdditionally, ChEF is scalable, enabling users to extend their recipes for each component and supporting various evaluation pipelines of different dimensions of capabilities. These contributions assist users in assessing MLLM performance and guiding the improvement of capabilities in MLLMs. \n\n\n\n### Q2: The comprehension of multi-image input is also an important assessment dimension for MLLMs.\n\nWe are very grateful for your suggestion of this important capability dimension. However, it's important to understand that currently, very few open-source models support multiple image inputs, especially for multi-image VQA tasks, which require the insertion of image tokens into text tokens. This feature is not supported by most open-source models. Nevertheless, in ChEF, we rapidly implemented a multi-image evaluation recipe and conducted evaluations on the Winoground[1] dataset for four models that support multi-image input. Winoground is a dataset that proposes a task of correctly matching two given images and captions, which can be used to evaluate MLLMs' understanding of text-to-image references. The results are shown below:\n\n|**Setting**| **Model** | **Text** | **Image** | **Group** |\n| :-------: | :-------: | :-------: | :-------: | :-------: |\n|MLLM multi-image |**mPLUG-owl**|\t36.25|\t38|\t31.25|\n|MLLM multi-image |**MiniGPT-4**|\t26|\t36\t|22.75|\n|MLLM multi-image |**Kosmos-2**|\t25.5|\t32.5|\t20.75|\n|MLLM multi-image |**Otter**|\t22.75|\t33.25|\t18.75|\n|MLLM single-image|**PALI**| \t46.5|\t38|\t28.75|\n|MLLM single-image|**Blip-2** |\t44|\t26|\t23.5|\n|CLIP-based       |\t**VQ2**|\t47|\t42.2|\t30.5|\n|\t              / |**MTurk Human**| \t89.5|\t88.5|\t85.5|\n|\t              / |**Random Chance**|\t25|\t25|\t16.67|\n\n\nWinoground-Text/Image/Group are text, image and group score metrics proposed in Winoground. We prompt the MLLM with \"Does image a match with the text b?\" and calculate the probability that MLLM output \"yes\". Different from previous works where similarity was computed for individual image-text pairs, such as VQ2[2], PALI[3], and Blip-2[4]. In our experiment, we take two images and two captions together with a question as input to the MLLM models. Therefore, comparing the results directly with previous works may not be entirely fair due to the differences in experimental setup.\n\nThe results indicates that the current MLLM models are not ideal in handling multiple images. It's noticeable that Otter, which was specially trained on ICL data, exhibits lower performance than the other MLLMs. This might be due to a significant gap between treating each image as an individual case and linking multiple images together in a task. There also exists a certain 'tug of war' in multi-image tasks. \n\nWe are very grateful for your suggestion, which brings the interesting findings. We will incorporate more essential evaluation pipelines in the future. We also hope the community will provide more recommendations. \n\n[1] Winoground: Probing vision and language models for visio-linguistic compositionality.\n\n[2] What you see is what you read? improving text-image alignment evaluation.\n\n[3] Pali: A jointly-scaled multilingual language-image model.\n\n[4] Bootstrapping language-image pretraining with frozen image encoders and large language models."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission177/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699985549555,
                "cdate": 1699985549555,
                "tmdate": 1700019644445,
                "mdate": 1700019644445,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QE8hP6BuTr",
                "forum": "ClqyY6Bvb7",
                "replyto": "XmzBYNIf7l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission177/Reviewer_ZiWA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission177/Reviewer_ZiWA"
                ],
                "content": {
                    "comment": {
                        "value": "I disagree with the claim that \"very few open-source models support multiple image inputs\" since one can simply flatten all image tokens of multiple images and append them before text tokens. Considerting that ChEF is a comprehensive evaluation benchmark, I tend to accept it and keep my rating."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission177/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700731605385,
                "cdate": 1700731605385,
                "tmdate": 1700731605385,
                "mdate": 1700731605385,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sWZjSlxAkT",
            "forum": "ClqyY6Bvb7",
            "replyto": "ClqyY6Bvb7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission177/Reviewer_n4Xu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission177/Reviewer_n4Xu"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a comprehensive assessment framework for large multimodal models using  four modular components and six \"recipes\" that stem from desiderata. They then apply the proposed framework to several state of the art large models and present many interesting insights on their performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The authors demonstrate a good understanding of the problem and lay out a comprehensive framework.\n2. The overall proposed framework is wide ranging and thus leads to interesting insights.\n3. The authors have been thorough in their implementation and experiments."
                },
                "weaknesses": {
                    "value": "1. The paper does not justify its choices in a principled manner. The overall framework has an ad-hoc feel to it. While the reference are comprehensive, there is not enough logic to back up why those six desiderata for example are chosen and why some others are not. The work comes across as an engineering requirements style work rather than a scientific paper. I am open to being convinced otherwise. The field is moving very fast so just seemingly brute force evaluation of a bunch of models is not going to be helpful.\n2. The writing needs to tone down the claims to being pioneering etc. Or at least back up such claims."
                },
                "questions": {
                    "value": "1. What are the insights that drive your work? Please see the comments above on weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission177/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698787847840,
            "cdate": 1698787847840,
            "tmdate": 1699635943193,
            "mdate": 1699635943193,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eCSZM0WtEK",
                "forum": "ClqyY6Bvb7",
                "replyto": "sWZjSlxAkT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission177/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission177/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer n4Xu"
                    },
                    "comment": {
                        "value": "Before addressing your specific comments and questions, we would like to kindly inform you that we have provided an overall response to all reviewers. We believe that reading this response first will offer a comprehensive view of the revisions and clarifications we have made in light of the feedback received. We appreciate your time and effort in reviewing our paper. Following this note, I will proceed to address your specific concerns in detail.\n\n\n### Q1: There is not enough logic to back up why those six desiderata for example are chosen and why some others are not.\n\nWe provide the reason for our choice of these desiderata in section 3 of our overall response. More details of these desiderata are supported in Supplementary Section C. **The principles of the six dimensions we selected are based on a survey and statistical analysis of the original LLM field, as well as the application of MLLM as an AI agent.**\n\n### Q2: The field is moving very fast so just seemingly brute force evaluation of a bunch of models is not going to be helpful\n\nWe provide the significance of ChEF and its contributions to the development of open-source MLLMs in section 2 and 4 of our overall response. We also agree that this research field is developing rapidly, especially with the strong capabilities demonstrated by API-only MLLMs like GPT-4V and Bard. We currently sampled some data and evalauted GPT-4V and Bard across multiple dimensions. Considering we couldn't access the probability outputs of these two models, and using GPT4 to evaluate GPT-4V's response for language performance evaluation seams unreasonable, we only evaluated them in terms of ICL, instruction following, robustness, and hallucination. We compared their performances with some open-source models on the same dataset, as shown in the table below:\n\n| **MLLM** | **ScienceQA** | **MMBench** | **ICL** | **Ins. Follow.** | **Robustness** | **Hallucination** |\n| :------: | :-----------: | :---------: | :-----: | :--------------: | :------------: | :---------------: |\n|**GPT-4V**|   **96.67**   | **93.80**   |  43.98* | **97.69**        | **82.16**      | **96.00**         |\n| **Bard** |  90.00        | 71.43       | 39.61*   |     71.41        | 71.05          |  88.88            |\n| **LLaVA**| 50.00         | 43.33       |**47.99**|       36.67      |  34.18         |  36.67            |\n| **Otter**|  63.33        | 50.00       | 47.91   |    44.44         |  37.35         | 80.00             |\n| **mPLUG-Owl**|  53.33    |  46.67      |   42.14 |     41.67        |  63.46         | 36.67             |\n\nAs can be seen, GPT-4V nearly reaches the upper bound on several recipes proposed by ChEF, far exceeding the current open-source MLLMs. It's important to note that ICL is calculated as a relative metric of few-shot results compared to zero-shot results. For API-only MLLMs, their absolute performance in the few-shot setting is far superior to that of open-source models. In terms of VQA accuracy, instruction following, robustness, and hallucination, **there's a significant gap between open-source MLLMs and the two API-only MLLMs.** \n\nChEF is mainly intended for the broad research community, aiming to inspire continuous improvement and advancement in open-source models. As an evaluation framework, **ChEF can guide open-source models in improving their performance in visual capabilities and trustworthiness, interactivity, and other competencies required by MLLMs.** Besides, ChEF supports a more interpretable implementation of MLLMs evaluation by modularizing each component. This is a contributor-friendly work with the goal of building a community where users can more easily build upon each other's contributions.\n\n### Q3: The writing needs to tone down the claims to being pioneering etc. Or at least back up such claims.\n\nWe claim that **ChEF is the first evaluation framework that covers existing benchmarks and provides expandable recipes for more traditional visual tasks, enabling unified evaluation and fair comparison of MLLMs across a variety of evaluation criteria.** Moreover, **we are the first to systematically evaluate MLLMs in dimensions beyond visual capabilities**, namely the six desiderata. However, we acknowledge that ChEF has limitations and is still in its preliminary stage. Despite its potential for expansion, the currently implementable recipes are limited and do not cover all possibilities. The six desiderata also do not encompass all other capability dimensions for evaluation, such as toxicity, privacy, societal, multilingualism, etc. Some methods of evaluation and datasets might not be the most appropriate. Nonetheless, we hope that our first attempt in building an evaluation framework and evaluation for the desiderata, will aid the development of the academic society. We also aim to expand our recipes in the future to include more evaluation pipelines, guiding the performance improvement of MLLMs."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission177/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699985425831,
                "cdate": 1699985425831,
                "tmdate": 1699985425831,
                "mdate": 1699985425831,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ydH54y7f4M",
            "forum": "ClqyY6Bvb7",
            "replyto": "ClqyY6Bvb7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission177/Reviewer_mfBa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission177/Reviewer_mfBa"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a comprehensive evaluation framework for Multimodal Large Language Models (MLLMs). The newly proposed framework has four modules, Scenario, Instruction, Inferencer, and Metric; and existing evaluation benchmarks can be summarized as recipes of the proposed framework. The authors conduct large-scale evaluations and presents valuable observations in the paper.\n\nAfter rebuttal: I have read the rebuttal and I'd like to keep my scores."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "This paper introduces a comprehensive evaluation framework for Multimodal Large Language Models (MLLMs). The newly proposed evaluation framework has a modular design, which allow it to recover various existing benchmarks with different recipes. Interesting observations are also presented in the paper."
                },
                "weaknesses": {
                    "value": "Since the main contribution of this paper is introducing this new evaluation framework. I suggest the authors to add a section describing the system design/implementation of this framework in detail. It seems that such information is missing in the current draft."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission177/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission177/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission177/Reviewer_mfBa"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission177/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699074123550,
            "cdate": 1699074123550,
            "tmdate": 1700777175141,
            "mdate": 1700777175141,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uNaEEwMKYJ",
                "forum": "ClqyY6Bvb7",
                "replyto": "ydH54y7f4M",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission177/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission177/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mfBa"
                    },
                    "comment": {
                        "value": "Thank you for your insightful comments on the ChEF framework. I appreciate your perspective and would like to address your concern about the system design/implementation of ChEF.\n\nThe criteria supported in each module, as well as their interrelationships, are shown in Figure 1(a) in our paper. We also provide the functions and details of each module's implementation in the Appendix Section B. To clearly present the entire evaluation pipeline, we provide details of the data flow, explaining how the data from scenario is transformed into input through instruction, and then how the inferencer enables the model to output answers for evaluation.\n\n```\n1. Load model and scenario. If ppl configured, each data sample in the scenario will include several predefined negative candidates. These negative candidates, together with the ground truth, form an `answer_pool`.\n2. Configure the instruction, inferencer, and metric.\n3. Iterate through each sample in the scenario. The instruction transforms each sample into an adaptive input for the model. When evaluating `sample_a`:\n    3.1 Get the input question from `sample_a`. For VQA tasks, a specific question is presented, while for other tasks, such as classification, there may be no question.\n    3.2 Concatenate the standard query or a specified query from the query pool with the question, along with the input image, to form `input_a`. If ICE is configured, retrieve a specified number of ICE from the scenario. Retrieval methods include random (random selection), fixed (selecting samples from the scenario via configured IDs), top-k images (based on similarity of images to the image of `sample_a`), and top-k text (based on similarity of text to the text of `sample_a`). The retrieval has following steps:\n      3.2.1 Retrieve a specified number of ICEs.\n      3.2.2 For each sample in ICE, concatenate the query with the question, to align with `input_a`.\n      3.2.3 Combine ICE and `input_a`, according to the MLLM's accepted format. For MLLMs that accept multiple images input, combine the entire ICE and `input_a`, while for MLLMs that accept only a single image, combine `input_a` with ICE without images, along with an additional system message.\n    3.3 If multi-turn inferencer is used, copy the input and replace the query in each with the corresponding multi-turn query.\n4. Feed the inputs into MLLM and obtain the response, by outputing the probability token by token. There are several types of inferencer:\n    4.1 Direct inferencer: Choose the word with the highest probability for each token as the output.\n    4.2 CoT inferencer: Save the query in input as `original_query`, and replace that with a special query 'Let's think step by step'. Execute step 4.1, and then combine the response with `original_query` and execute 4.1 again.\n    4.3 PPL inferencer: Replicate the input, and append each candidate from the `answer_pool` to the query of each input. Feed to MLLM and calculate perplexity, and select the candidate corresponding to the input with the lowest perplexity as the response.\n    4.4 Multi-turn inferencer: Iterate the turn and exceute the corresponding inferencer.\n5. Save the reponse results and execute the metric.\n```\nEach component can be implemented to meet specific needs by passing a simple configuration. To more intuitively explain the design and implementation of our framework, we provide the pseudo code aligned with the data flow claimed above. \n```\n# step 1\nmodel = get_model(model_name)\nscenario = build_scenario(scenario_name, ppl_cfg, **other_cfgs)\n# step 2\ninstruction = build_instruction(instruction_type, prompt_cfg, icl_cfg, **other_cfgs)\ninferencer = build_inferencer(inferencer_type, **other_cfgs)\nmetric = build_metric(scenario_name, **other_cfgs)\n\nanswers = inferencer.inference(scenario, model, instruction) # step 3 and 4\n\ndef inference(scenario, model, instruction):\n    answers = []\n    for sample in scenario:\n        input = instruction.generate(sample) # step 3\n        output = model.inference(input) # step 4\n        answers += output\n    return answers\n\ndef generate(sample):\n    question = sample.get('question','') # step 3.1\n    # step 3.2\n    prompt = get_prompt(prompt_cfg) \n    input = prompt.format(question)\n    # step 3.2.1 to 3.2.3\n    ice = retriever.retrieve(sample)\n    ice = [(image, prompt.format(text)) for (image, text) in ice]\n    # input = [prompt_t.format(question) for prompt_t in multi_turn_prompt] # step 3.3\n    input = (ice, sample['image'], input)\n    return input\n\ndef inference(input):\n    return model.direct_inference(input) # 4.1\n    # return model.CoT_inference(input) # 4.2\n    # return model.PPL_inference(input) # 4.3\n    # return model.multiturn_inference(input) # 4.4\n\nresults = metric.metric(answers) # step 5\n```\n\nWe sincerely hope these provide sufficient clarity. We will also provide a more detailed tutorial and toolkit in the code release version, and include these details in the final version of our paper after revision."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission177/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699985315554,
                "cdate": 1699985315554,
                "tmdate": 1699985315554,
                "mdate": 1699985315554,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yDgV2xVZr5",
            "forum": "ClqyY6Bvb7",
            "replyto": "ClqyY6Bvb7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission177/Reviewer_V16Y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission177/Reviewer_V16Y"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes ChEF, a framework for evaluating Multimodal Large Language Models (MLLMs). The main idea is to instantiate a \u201cRecipe\u201d, called \u201cdesiradata\u201d, consisting of Scenarios (datasets), Instruction (how to pose questions such as in-context learning (ICE)), Inferencer (how an MLLM answers questions including Perplexity (PPL), Chain of Thought (CoT), and Multi-Turn), and Metrics.\n\nThey evaluate 9 MLLMs using 6 desiderata (9 Scenarios) that target measuring Calibration, In-context Learning, Instruction Following, Language Performance, Hallucination, and Robustness. See page 3 and Section 2.3 for more details."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- S1: The proposed ChEF framework is sound.\n\n- S2: The experimental results are conducted on multiple models and settings and quite comprehensive."
                },
                "weaknesses": {
                    "value": "- W1: Significance, Related work, and Execution. While I generally like the work that attempts to connect the dots and organize previous work, this work falls short. I do not think that the ChEF framework itself is a significant contribution as the 4 components of the Recipes are normally what people usually think about when it comes to evaluation. Thus, IMO, the main contributions lie in the instantiations of these Recipes or desiderata and their experimental results. However, the significance of this part is unclear due to two reasons. \n\n  - W1.1: First, it is unclear both in the main text and in the supplement how this work is better than existing work in terms of \u201cscalability\u201d and \u201ccomprehensiveness\u201d (cf. the first paragraph of the intro). The paper has to put more emphasis on the discussion of related work in order for the reader to understand the significance.\n\n  - W1.2: Second, the desiderata in Section 2.3 themselves need more rationales/justification. Why do we care about these capabilities? Why do we instantiate them this way? For example, Hallucination consists of asking binary questions about the existence/absence of objects. Yet, this is not the only kind of hallucination. In general, it is unclear why the desiradata is what it is. \n\n- W2: Clarity: related to W1, the paper would benefit from better presentation of desiradata. Perhaps having a table that lists down the 4 components. Justify why this is \u201cversatile\u201d evaluation."
                },
                "questions": {
                    "value": "Please clarify as much as you can on my comments in Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission177/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699249010072,
            "cdate": 1699249010072,
            "tmdate": 1699635943036,
            "mdate": 1699635943036,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RBsZMcXI2y",
                "forum": "ClqyY6Bvb7",
                "replyto": "yDgV2xVZr5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission177/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission177/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer V16Y"
                    },
                    "comment": {
                        "value": "Before addressing your specific comments and questions, we would like to kindly inform you that we have provided an overall response to all reviewers. We believe that reading this response first will offer a comprehensive view of the revisions and clarifications we have made in light of the feedback received. We appreciate your time and effort in reviewing our paper. Following this note, I will proceed to address your specific concerns in detail.\n\n### Q1: The main contributions lie in the instantiations of these Recipes or desiderata and their experimental results.\n\nBesides the recipes, desiderata, and the experimental results, the ChEF evaluation framework itself is also a significant contribution. Creating a benchmarking framework is an immensely challenging endeavor. We have elucidated this in Section 2 of our overall response. Indeed, after extensive research on numerous MLLM benchmarks, as well as conducting in-depth studies on works that explore instruction tuning and training methods, it became quite natural for us to decompose the evaluation pipeline into the four modules that we proposed. However, **no existing work has introduced this concept before, and none have successfully interlinked these modules in a compatible and cohesive manner.** ChEF achieves this features, thereby encompassing a wide range of existing benchmarks and enabling a unified assessment of MLLMs. We claim that the development of ChEF as an evaluation framework is a main contribution of our work. We are glad that Reviewer mfBa and n4Xu shares this perspective, recognizing the significant strides we have made in advancing the field.\n\n### Q2: It is unclear both in the main text and in the supplement how this work is better than existing work in terms of scalability and comprehensiveness.\n\nWe introduce several related works in Section 1 of our overall response, and more related works in Appendix A. We also have outlined these comparisons with other works in Figure 1 (b) of our paper. Compared to the previous works, ChEF firstly decomposed the evaluation pipeline into four components. We also provide a toolkit for a quick extension of each component. \n\n- **More scalable**: We provided an example to demonstrate the scalability. Reviewer Ziwa mentioned the need for multi-image understanding task evaluations in the comments. We chose Winoground[1] dataset and spent only several hours to complete the construction of the recipe and the evaluation of multiple MLLMs. We simply build the recipe by adding the dataset to the scenario, defining prompts for this task, and then utilizing PPL along with a metric for calculating accuracy from ppl results. The modular design allows us to build the new recipe easily. However, this process of expanding the recipe is difficult to implement in any current benchmark that provides a single undecoupled evaluation pipeline.\n\n- **More comprehensive**: ChEF has evaluated many traditional tasks and has also attempted to use more suitable methods for MLLM to evaluate detection tasks and fine-grained classification tasks, which have not been evaluated in any existing benchmarks. Additionally, ChEF has specially designed some unique recipes for evaluating the desiderata of MLLM, which have not been evaluated before. Based on these recipes, ChEF provides a comprehensive assessment of MLLMs.\n\n### Q3: Why do we care about these capabilities? Why do we instantiate them this way?\n\nWe provide the rationales and justification of these desiderata in Section 3 of our overall response. More details of these desiderata and the rationale for their evaluation are supported in Appendix Section C. **The principle of selecting these desiderata are based on a survey and statistical analysis of the original LLM field, as well as the application of MLLM as an AI agent.** We have specified the recipes for these desiderata by conducting survey on MLLM and related works in NLP. For example, for hallucination, we use the method proposed in POPE[2]. Other methods also reference some approaches from NLP, adapted for multimodal tasks. Of course, these recipes are just attempts to evaluate these desiderata. We welcome the community to expand the evaluation of desiderata by constructing new and more reasonable recipes through the expansion of each module of ChEF.\n\n### Q4: Clarity\n\nThe desiderata are evlauated through specially designed recipes. We illustrate the recipes in Figure 3 in our paper, and show distinguished design of each desideratum in Figure 4 in the paper. For a clearer and more intuitive understanding of the recipes, we list a table to show the four components of the recipe for each desiderata in Appendix Section D.3, Table 8. We sincerely hope these provides sufficient clarity.\n\n[1] Winoground: Probing vision and language models for visio-linguistic compositionality.\n\n[2] Evaluating Object Hallucination in Large Vision-Language Models."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission177/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699984644737,
                "cdate": 1699984644737,
                "tmdate": 1699984644737,
                "mdate": 1699984644737,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]