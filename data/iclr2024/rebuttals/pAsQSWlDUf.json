[
    {
        "title": "Soft Contrastive Learning for Time Series"
    },
    {
        "review": {
            "id": "aAfq18jlxA",
            "forum": "pAsQSWlDUf",
            "replyto": "pAsQSWlDUf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4667/Reviewer_e9k3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4667/Reviewer_e9k3"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a new contrastive learning method for time series. Specifically, they propose to remove the hard positive/negative assignment from the original NCE by a soft reweighting incorporating prior information about the temporal closeness or similarity of inputs. The authors evaluate their method on various time series-related tasks showing strong improvement compared to \"hard\" CL methods. They also provide ablation experiments concerning their new objective hyperparameters."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper is of high quality with the following strengths: \n\n- Overall very well-written paper with an easy-to-follow structure, in particular:\n    - Related work is structured and extensive (with the exception mentioned below in weaknesses).\n    - Method is clear and figures relevant making the method easy to understand.\n    - Experiments are very extensive and well described both in the manuscript and the supplementary materials.\n\n- I appreciate that despite introducing numerous components and hyperparameters ( temperature, distance metric, weight function, etc..), authors provide ablations to each of these components. \n- The authors discussed the additional computational complexity of their method and in particular DTW known to have a squared complexity. \n- The amount of experiments carried out is very large and diverse"
                },
                "weaknesses": {
                    "value": "There are, however, some weaknesses, in particular in terms of related work, which I detail below:\n\n\n**Related work**\nAs mentioned the related work is extensive but misses some seminal works regarding contrastive learning methods for time series tackling the challenges of inter/intra samples dependencies:\n- First, \"Subject-aware contrastive learning for biosignals\" by Cheng et al (2020) proposes to only use negative representations from the same time series to \"promote subject-invariant\" representation, what the authors would refer to as \"temporal CL\".\n- Second, \"CLOCS: Contrastive Learning of Cardiac Signals Across Space, Time, and Patients\" by Kiyasseh et al. (2021) proposes to on the contrary use representations from the same time series as positives, what the authors would refer to as \"instance-wise CL\". This is similar to TNC but with a neighborhood being defined as being from the same time series. \n- Finally, and more importantly, \"Neighborhood Contrastive Learning Applied to Online Patient Monitoring\" by Yeche et al. (2021), introduces the trade-off used by the authors between instance-wise and temporal-wise CL controlled by $\\lambda$. In particular, the objective proposed by Yeche et al., namely NCL, is similar to taking a hard assignment for instance-wise CL $w_I(i,j) = \\mathbb{1}_{[i = j]}$ and a (discontinuous) uniform one over a window for temporal CL. \n\nThus, I think it's really important that this work refers to these three works and in particular NCL from which the SoftCLT is an extension to continuous neighborhood definitions. It would be nice to have a comparison to it as well. \n\n\n**Clarity**\n\n- The authors refer multiple times to instance-wise and temporal CL before defining it properly in the method section. I think pointing the reader to this section or defining the terms in the introduction could improve clarity.\n\n- referring to a \"temperature\" parameter $\\tau_t$ and $\\tau_i$ can be quite misleading in the context of contrastive learning, where this term was coined by Chen et al. (2020), in the simCLR paper. (See my comment below on the choice of assignment function) Given the assignment function is some form of Laplacian kernel, referring to $l =\\frac{1}{\\tau}$ as a lengthscale parameter would be more coherent with literature and avoid confusion with temperature parameters from previous works on CL. \n\n**Method**\n- The authors define their assignment function around a sigmoid function which is defined over $\\mathbb{R}$ whereas its input $D$ lies in $\\mathbb{R}^+$. It seems to overcome this, they tweak around their sigmoid function to obtain a symmetric function $w(D) = \\frac{2}{1+e^{Dt}}$. Why not rely on existing literature instead and typically use a Laplacian kernel $w(D) = e^{-\\frac{D}{l}}$? \n- Exploring further different kernel and their impact on performance would have been a nice addition. In particular, using a generalized Gaussian kernel and looking at the impact of the shape parameter $\\beta$ would be nice as $\\beta=1$ is SoftCLT and  $\\beta=\\infty$ is NCL temporal CL. \n- Exploring further the impact of the trade-off between local (temporal) and global (instance) features learning ruled by $\\alpha$ would be a nice addition to ablations. \n**Conclusion**\n\n\nClarity and Method weakness are easily addressable. Regarding related work, despite the similarities with NCL, I still think the contribution to be significant given the novelty around the neighborhood/assignment function and the extent of the experiments on various tasks, justifying my choice of recommending acceptance. However,  I firmly believe the three works I mentioned should be correctly cited in particular the link to Yeche et al. (2021) work."
                },
                "questions": {
                    "value": "I don't have any questions beyond the points raised in the above sections."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4667/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698411847176,
            "cdate": 1698411847176,
            "tmdate": 1699636447727,
            "mdate": 1699636447727,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vD2P7OUpkE",
                "forum": "pAsQSWlDUf",
                "replyto": "aAfq18jlxA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4667/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4667/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**W1. Related works from medical domain**\n\nIn our initial submission, we cited papers about general TS rather than specific domains like the medical domain. However, we agree that a discussion on related works from specific domains would still be interesting and helpful to audiences. We have added a brief discussion on the papers that e9k3 suggested in **Section 2**. Below is a longer discussion that e9k3 might be specifically interested in:\n\n- **Subject-aware contrastive learning for biosignals (Cheng et al, 2020)**:  Their proposed subject-aware CL contrasts multiple TS, which can be considered as **instance-wise CL** rather than temporal CL, as instance-wise CL contrasts the representations of **multiple TS**, while temporal CL contrasts the representations of multiple timestamps within a **single TS**, as illustrated in **Figure 1**. By architecture design, subject-aware CL cannot perform temporal CL, as they generate representations for the entire TS, such that the information along with timestamps are entangled.\n\n- **CLOCS: Contrastive Learning of Cardiac Signals Across Space, Time, and Patients (Kiyasseh et al, 2021)**: Their proposed CLOCS performs CL over temporal and spatial dimensions, where the concept of spatial dimension is close to the channel (or multivariate) dimension in general TS. Hence, this idea might have a limited applicability, e.g., for univariate TS.\n\n- **Neighborhood Contrastive Learning Applied to Online Patient Montoring (Yeche et al, 2021)**:  NCL performs CL with **multiple hard** assignments, where assignments are based on predefined attributes of instances. This assignment strategy of NCL is similar to NNCLR [A]. NCL jointly optimizes two conflicting losses with a trade-off: the neighbor alignment loss maximizing the similarity of neighbors as well as positive pairs, and the neighbor discriminative loss maximizing the similarity of positive pairs while minimizing the similarity of neighbors. However, different from two conflicting losses in NCL, our proposed two losses (soft instance-wise CL loss and soft temporal CL loss) do not contradict each other and operate on different dimensions: instance-wise and temporal, respectively.\n\nAlso, similar to subject-aware CL, **NCL does not perform temporal CL**; it contrasts representations of multiple TS, which corresponds to instance-wise CL. Note that, as illustrated in Figure 5 of the NCL paper, representations obtained from NCL disregard the temporal axis. Hence, by architecture design, subject-aware CL cannot perform temporal CL, as they generate representations for the entire TS, such that the information along with timestamps are entangled.\n\nAs we understand, NCL can be considered a variation of NNCLR [A], where it takes advantage of supervision from the medical domain to determine neighbors and jointly optimizes two conflicting losses. Hence, we categorize NCL as a soft contrastive learning and have added a discussion in **Section 2**.\n\n[A] Dwibedi, Debidatta, et al. \"With a little help from my friends: Nearest-neighbor contrastive learning of visual representations.\" ICCV (2021)\n\n\n&nbsp;\n\n**W2-a. Definition of instance-wise and temporal CL**\n\nThanks for the suggestion, we have added the definition of instance-wise and temporal CL in **Section 1** and **Figure 1**. We believe this will improve the clarity of the proposed method from the earlier part of our paper.\n\n\n&nbsp;\n\n**W2-b. Confusion with the term \"temperature\"**\n\nThanks for raising this concern. We agree that it can be confused with the temperature parameter used in CL literature. To avoid any confusion and maintain coherence with previous works, we replaced the term \"temperature\" with \"hyperparameter controlling the sharpness\" for $\\tau$ in our paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4667/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700434792796,
                "cdate": 1700434792796,
                "tmdate": 1700434792796,
                "mdate": 1700434792796,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "y0TM9bHSKr",
            "forum": "pAsQSWlDUf",
            "replyto": "pAsQSWlDUf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4667/Reviewer_mvHV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4667/Reviewer_mvHV"
            ],
            "content": {
                "summary": {
                    "value": "This study introduce a new method of performing constrastive learning, a soften version of normal positive-negative strategy. These soft assignments are determined by the distance between time series in the data space for instance-wise contrastive loss and the difference in timestamps for temporal contrastive loss."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Contribution:\n\n- The idea of soft constrastive learning is straight-forward and natural. The underlying functions are widely-adopted and straightforward to implement.\n- The experiments are extensive and cover many time-series tasks (classification, anomaly detection) as well as scenario (self/semi supervised and supervised learning). The comparison with soft-CL techniques from other domains and ablation study make the whole experimental section be quite well-rounded.\n\nRepresentation:\n\n- Intuitive and direct illustrations via Figures (e.g. Fig.1, 2)"
                },
                "weaknesses": {
                    "value": "- Contribution:\n    - For instance-wise CL:\n        - the use of DTW might be a potential bottleneck in case of dealing with lengthy time-series. While the authors suggest the use of FastDTW, the complexity regarding the memory might be increased, and also the potential reduce in approximation (in case the warping path between two time series instances is highly nonlinear). In other words, the choices of DTW or FastDTW are hurting the pipeline in some ways.\n        - the calculation of weight based on the distance in the data space. However, this make the weighting process be dependent on the scale of input data. Together with the wrapper of Sigmoid function, it might be saturated upon too large or too small input. This effect might make the weights not representative to use in instance-wise CL. While empirically, it illustrates the effective over in latent space, more effort need to be done to consider on which space one should rely on to calculate distance.\n    - For temporal-wise CL, the current weight assignment implicitly assume the data from neighbors\u2019 timesteps should be weighted heavier than the data from far timesteps. However, that behavior might not always hold true, as illustrated in work of Tonekaboni (2021)."
                },
                "questions": {
                    "value": "The authors please address or provide answers to any questions from the weaknesses listed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4667/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4667/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4667/Reviewer_mvHV"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4667/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698449360767,
            "cdate": 1698449360767,
            "tmdate": 1700604064233,
            "mdate": 1700604064233,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eSfq4eZDpO",
                "forum": "pAsQSWlDUf",
                "replyto": "y0TM9bHSKr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4667/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4667/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**W1-a. Complexity issue with DTW and FastDTW**\n\nWe first emphasize that DTW is just one option, and other distance metrics can be employed; we chose DTW because it performed best according to **Table 6(d)**. We note that other simpler metrics without dynamic programming like the cosine distance (COS) or Euclidean distance (EUC) also show decent performance, so they can be the option if the complexity matters.\n\nNonetheless, regarding the concern on the computational complexity, we note that distances between TS (for soft assignments for the instance-wise CL) are computed based on the original TS, such that they can be precomputed offline before training or cached for efficiency.\n\nRegarding FastDTW, we found that the approximation error is almost negligible for datasets we experimented with, resulting in identical performance throughout experiments.\n\nFor better clarification, we added more description in **Section 3.2**.\n\n&nbsp;\n\n**W1-b. Soft assignment's dependency on the scale of the input data**\n\nThank you for bringing this to our attention. In fact, we missed to talk about the detail that we **min-max normalized the pairwise distance matrix**, such that the minimum and maximum values become zero and one, respectively. We have updated the definition of D in **Section 3.2** accordingly.\n\nWe also note that the specific details that we could miss should be clear through the code we provided in the supplementary materials, and we will release the code upon acceptance for reproducibility.\n\n&nbsp;\n\n\n\n**W2. Implicit assumption on soft assignments for temporal CL**\n\n(Tonekaboni, 2021) considers CL for TS under non-stationarity. Indeed, non-stationarity can be addressed by other strategies like TNC [A] or CoST [B], and our method can be applied on top of them to improve the performance further. For example, CoST [B] is designed to learn disentangled seasonal-trend representations, and applying our SoftCLT on top of CoST improves the performance, which can be found in the new experiment in **Table H.3 in Appendix H**.\n\n[A] Tonekaboni, Sana, Danny Eytan, and Anna Goldenberg. \"Unsupervised representation learning for time series with temporal neighborhood coding.\" ICLR (2021)\n\n[B] Woo, Gerald, et al. \"CoST: Contrastive learning of disentangled seasonal-trend representations for time series forecasting.\" ICLR (2022)\n\n\nNonetheless, we provide more discussion on our method with non-stationarity below.\n\n\n\n&nbsp;\n\n\n**W2-a. TS with non-stationarity - (1) Seasonality**\n\nSoft temporal CL assigns more weight to closer timestamps, so one might be concerned if it fails when seasonality is present.\nWe argue that **instance-wise CL can indirectly capture the seasonality of TS**, as it contrasts by looking at the representations of the original TS, while **temporal CL** takes advantage of the non-seasonal portions. (In **Section 3.2**, we have clarified that the soft assignments for instance-wise CL are calculated using the original TS, not the augmented views.)\nFollowing the terms from the review of e9K3, soft temporal CL and soft instance-wise CL captures the local and global features, respectively.\n\n&nbsp;\n\nTo support our claim, we present both qualitative and quantitative analyses as follows.\n\n\n**Quantitative analysis (Table 8)**\n\n**Table 8** categorizes 128 UCR datasets by seasonality and shows that the performance gain by sot temporal CL is consistent regardless of the degree of seasonality.\n\n&nbsp;\n\n**Qualitative analysis (Section M - Figure M.1)**\n\nWe conducted additional analysis to show that our proposed **soft CL captures seasonality better than the hard CL**, as shown in **Figure M.1**. This figure depicts the t-SNE of representations for all time steps in a single TS **with seasonality**. The figure indicates that while hard CL fails to capture the seasonal patterns in the TS (as similar values, regardless of which phase they are in, are located closely in the embedding space), our proposed soft CL can grasp the global pattern, enabling it to capture the seasonal patterns (as similar values but with different seasonal phases are located differently in the embedding space).\n\n\n&nbsp;\n\n\n**W2-b. TS with non-stationarity: distribution shift**\n\nSoft instance-wise CL captures the global view of the TS, and we believe this allows us to detect the **distribution shifts** in the TS. To support this claim, we present additional analysis in **Figure M.2 in Appendix M**.\n\n**Figure M.2** shows the t-SNE of representations for all timestamps in a single TS **with distribution shifts**. The figure indicates that while hard CL fails to capture the sudden change in the TS (as all points are located gradually regardless of the sudden change in the TS), our proposed soft CL can detect such distribution shifts (as points before/after a certain change are clustered in groups)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4667/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700434696365,
                "cdate": 1700434696365,
                "tmdate": 1700436605481,
                "mdate": 1700436605481,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "X64cVqbfMR",
                "forum": "pAsQSWlDUf",
                "replyto": "eSfq4eZDpO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4667/Reviewer_mvHV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4667/Reviewer_mvHV"
                ],
                "content": {
                    "title": {
                        "value": "Additional Review of Reviewer mvHV"
                    },
                    "comment": {
                        "value": "Dear Authors, \n\nThanks for making clarifications and great attempt in modifying the manuscript. \nHowever,  I still have several concerns basing on top of your answers:\n- *we found that the approximation error is almost negligible for datasets we experimented with*\n\nI would expect a verification for this claim, I suggest the Authors providing an additional experiment to your Ablation study in Table 6(d). \n\n- *our method can be applied on top of them to improve the performance further*\n\nI suspect this claim in case of incorporating softCL with TNC [1]. \nTo clear out my initial comment, I think the current weighting strategy of temporal alignment contrasts with what proposed in [1]. I am not sure if the two method can yield better result if being fused together. \n\nIn addition, why it is more ideal if your framework is more sensitive toward seasonality? For example in Figure M.1, why phase (a) and (c) representations need to be distinguished, as illustrated by the figure, what if that two phase have the same label?\n\nTo sum up, with these concerns, for now, I still keep the scores as it is.\n\n\n\n\n[1] Tonekaboni, Sana, Danny Eytan, and Anna Goldenberg. \"Unsupervised representation learning for time series with temporal neighborhood coding.\" ICLR (2021)"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4667/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700525392998,
                "cdate": 1700525392998,
                "tmdate": 1700525392998,
                "mdate": 1700525392998,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DX877uAghU",
                "forum": "pAsQSWlDUf",
                "replyto": "y0TM9bHSKr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4667/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4667/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for engaging more in the discussion! \n\nBelow we provide our responses to each concern, and we updated our manuscript accordingly (Including the new **Appendix N** and minor changes):\n\n&nbsp;\n\n> (1) Your request to verify our statement that the FastDTW approximation error is negligible\n\nFirst of all, we would like to clarify that concerns you might have around DTW and FastDTW like *how much DTW is computationally expensive* and *how accurate the approximation by FastDTW is* are **out of scope** for this work, as **our main contribution does not lie in the particular design choices on a specific metric (e.g., DTW), but in the proposal of a simple yet effective soft contrastive learning framework that harnesses (self-)supervision from the data space.** If you really concern on the complexity of DTW or inaccuracy of FastDTW, then you can go for the simple Euclidean or cosine distance at the cost of small performance drops; according to **Table 2 and 6(d)**, the performance drop is indeed minor compared to the gain from the baseline: in classification, **TS2Vec: 83.0% vs. Ours-EUC: 84.8% vs. Ours-DTW: 85.0%**.\n\n&nbsp;\n\nNonetheless, we address this concern as follows. As we did not keep our early experiments with FastDTW, we are running more experiments now, and below we provide a part of results from the ongoing experiments with 60 out of 128 UCR datasets. Note that we used python libraries to compute distances: `tslearn` for DTW and `fastdtw` for FastDTW.\n\n&nbsp;\n\n**[Concern 1] Difference in soft assignments ($w_I$).**\nThe average L2 error between the soft assignments generated by DTW and FastDTW over 60 datasets is 0.0019. Note that the soft assignments lie in the range [0,1], so **0.0019 is relatively a small error**.\n\n&nbsp;\n\n**[Concern 2] Difference in accuracy.**\nThe average accuracy difference in SoftCLT when using DTW and FastDTW is **0.04%p, which is negligible** for our comparisons using only one decimal place. Notably, among the 60 datasets, 53 have exhibited exactly the same accuracy. Based on these results, we can say that using FastDTW as an approximation of the original DTW is acceptable.\n\n\n&nbsp;\n\n> (2) TNC + SoftCLT; Doesn't TNC contradict with the soft temporal CL?\n\nThanks for your suggestion, we have added the TNC + SoftCLT experiment in the new **Appendix N**.\n\nThe proposed soft assignment strategy can be applied to TNC. First note that TNC finds temporal neighbors and performs contrastive learning along with the temporal dimension. **Instead of hard assignments, SoftCLT applies soft assignments to those neighbors found by TNC based on the temporal distance.** Note that TNC performs only temporal contrasting, so we soften the temporal dimension only.\n\n\n&nbsp;\n\nTo observe the effectiveness of SoftCLT on top of TNC, we implemented our SoftCLT on the official TNC code and experimented on the Simulation and HAR datasets (Table 2 in TNC; the ECG dataset is too large to quickly run the experiments). As in **Table N.1** in the new revision or the table below, **applying SoftCLT to TNC results in consistent performance improvements across datasets.**\n\n\n\n| | Simulation | | HAR | |\n|-|-|-|-|-|\n| | Acc.(%) | AUPRC | Acc.(%) | AUPRC |\n|T-Loss | 76.66 | 0.78 | 63.60 | 0.71| \n|CPC| 70.26 | 0.69  | 86.43 | 0.93| \n|TNC| 97.52 | 0.996  | 88.21 | 0.940 | \n|TNC+Ours| 97.95 | 0.998 | 89.14 | 0.952 | \n\n&nbsp;\n\n\nFinally, note that TNC is a relatively early work compared to other methods, such that its performance is generally lower than others (e.g., in classification, TNC: 76.1% vs. TS2Vec: 83.0%), making it low-prioritized in the application of our method throughout experiments in our main paper.\n\n&nbsp;"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4667/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587434477,
                "cdate": 1700587434477,
                "tmdate": 1700588043868,
                "mdate": 1700588043868,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GkHx613qvl",
                "forum": "pAsQSWlDUf",
                "replyto": "i7AhhyCapd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4667/Reviewer_mvHV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4667/Reviewer_mvHV"
                ],
                "content": {
                    "title": {
                        "value": "Final Response From Reviewer mvHV"
                    },
                    "comment": {
                        "value": "Dear Authors,\n\nThank you for the further clarifications.\n\nYour answer this time seems fair enough for me. \n\nI have modified my final score for this work.\n\nThanks.\n\nBest,"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4667/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700604046257,
                "cdate": 1700604046257,
                "tmdate": 1700604046257,
                "mdate": 1700604046257,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MPT6GNPiay",
            "forum": "pAsQSWlDUf",
            "replyto": "pAsQSWlDUf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4667/Reviewer_EHZC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4667/Reviewer_EHZC"
            ],
            "content": {
                "summary": {
                    "value": "This study argues that when using time series data in contrastive learning, contrasting ( between positive and negative) instances or values located in proximity can lead to the neglect of their inherent correlation. Therefore, they introduce a continuous (referred as soft) weighting approach as an alternative to binary labeling, serving as a generalization of the standard contrastive loss, with the transformation occurring when replacing soft assignments with hard assignments of zero for negatives and one for positives. For soft assignment, the authors take into account two aspects: the similarity between two time series in data space and the proximity of two time series with respect to their timestamps."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The papers is well written and clear. The figures presented help to clarify the main idea and how it is implemented. The idea is novel for the simplified setup that is considered. The experimental results cover 3 downstream tasks and comprehensively evaluate the assumed setup."
                },
                "weaknesses": {
                    "value": "The paper addresses a simplified scenario in which issues related to noise, seasonality, and non-stationarity are not considered, as there is no apparent mechanism in the approach to address these prevalent challenges found in real-world time series data.\nRegarding robustness in the presence of noise and non-stationarity there  is no specific discussion or empirical evaluation.  Regarding seasonality, the authors mentioned \"Our conjecture is that TS in the real world usually do not exhibit the perfect seasonality, as indicated by the ADF test result, such that SoftCLT takes advantage of the non-seasonal portions.\" While perfect seasonality may be absent in some datasets and may vary in intensity across different datasets, I believe completely disregarding it is not a practical approach."
                },
                "questions": {
                    "value": "Can this case be elaborated a bit further:\u201d when \u03b1 = 1, we give the assignment of one to the pairs with the distance of zero as well as the pairs of the same TS\u201d What if in the same TS we are experiencing two different patterns, shifts or different distribution\n\nIn equation 3, augmentation for I and i+N, how it  is performed? What if there is only a shift in the pattern in the instances, otherwise there are very similar how you address this in your computation, It would be great to include an illustration for this case to show you approach is robust to shift (or some noise) which is very common in real world applications.\n\n\nHow do you manage non stationary in the time series, where the immediate next point might be the start of a different distribution? How your similarity comparison handles it when the the proximity is assumed to have high similarity which is not necessarily true."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4667/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698658100622,
            "cdate": 1698658100622,
            "tmdate": 1699636447564,
            "mdate": 1699636447564,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "85aMTpPNhr",
                "forum": "pAsQSWlDUf",
                "replyto": "MPT6GNPiay",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4667/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4667/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**W1. TS with noise, seasonality, and non-stationarity**\n\nWe found that concerns raised by EHZC are mostly about specific problems in TS, such as noise, seasonality, and non-stationarity. However, we note that **the primary focus on this work is on improving CL for TS** (which is SOTA in TS representation learning), **rather than addressing specific problems in TS**. Indeed, our proposed **SoftCLT is a simple** yet effective soft contrastive learning strategy for TS, so addressing such problems at the expense of simplicity might result in hampering our main contribution.\n\nRather, such specific problems can be addressed by other strategies, and our method can be applied on top of them to improve the performance further. For example, CoST [A] is designed to learn disentangled seasonal-trend representations, and applying our SoftCLT on top of CoST improves the performance, which can be found in the new experiment in **Table H.3 in Appendix H**.\n\nNonetheless, we discuss each problem below.\n\n[A] Woo, Gerald, et al. \"CoST: Contrastive learning of disentangled seasonal-trend representations for time series forecasting.\" ICLR (2022)\n\n&nbsp;\n\n\n\n**W1-a. TS with noise**\n\nWe agree that some significant noises could pose challenges when calculating distances between TS for soft instance-wise CL. However, we think addressing this issue falls beyond our current scope and is more aptly addressed by metrics designed for measuring distances between TS, as we are not confined to a specific metric. Data preprocessing techniques such as smoothing may alleviate this concern.\n\n&nbsp;\n\n\n\n**W1-b. TS with non-stationarity - (1) Seasonality**\n\nSoft temporal CL assigns more weight to closer timestamps, so one might be concerned if it fails when seasonality exists.\n\nWe argue that **instance-wise CL can indirectly capture the seasonality of TS**, as it contrasts instances by looking at the representations of their original TS, while **temporal CL** takes advantage of the non-seasonal portions. (In **Section 3.2**, we have clarified that the soft assignments for instance-wise CL are calculated using the original TS, not the augmented views.)\nIn other words, following the terms from the review of e9K3, soft temporal CL and soft instance-wise CL captures the local and global features, respectively.\n\n&nbsp;\n\nTo support our claim, we present both qualitative and quantitative analyses as follows.\n\n**Quantitative analysis (Table 8)**\n\n**Table 8** categorizes 128 UCR datasets by seasonality and shows that the performance gain by soft temporal CL is consistent regardless of the degree of seasonality.\n\n&nbsp;\n\n**Qualitative analysis (Section M - Figure M.1)**\n\nWe conducted additional analysis to show that our proposed **soft CL captures seasonality better than the hard CL**, as shown in **Figure M.1**. This figure depicts the t-SNE of representations for all time steps in a single TS **with seasonality**. The figure indicates that while hard CL fails to capture the seasonal patterns in the TS (as similar values, regardless of which phase they are in, are located closely in the embedding space), our proposed soft CL can grasp the global pattern, enabling it to capture the seasonal patterns (as similar values but with different seasonal phases are located differently in the embedding space).\n\n\n&nbsp;\n\n\n**W1-c, Q3. TS with non-stationarity: distribution shift**\n\nTo our understanding, your concern around \"non stationary in the time series, where the immediate next point might be the start of a different distribution\" is the problem of distribution shift. Soft instance-wise CL captures the global view of the TS, and we believe this allows us to detect the **distribution shifts** in the TS. To support this claim, we present additional analysis in **Figure M.2 in Appendix M**.\n\n**Figure M.2** shows the t-SNE of representations for all timestamps in a single TS **with distribution shifts**. The figure indicates that while hard CL fails to capture the sudden change in the TS (as all points are located gradually regardless of the sudden change in the TS), our proposed soft CL can detect such distribution shifts (as points before/after a certain change are clustered in groups)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4667/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700434475499,
                "cdate": 1700434475499,
                "tmdate": 1700434875640,
                "mdate": 1700434875640,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XTCGPXSCsT",
                "forum": "pAsQSWlDUf",
                "replyto": "frdra76tYd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4667/Reviewer_EHZC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4667/Reviewer_EHZC"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for addressing my questions and performing further experimentation. In my opinion, the novelty presented in this work is marginal and I will keep the score I have given before."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4667/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659482035,
                "cdate": 1700659482035,
                "tmdate": 1700659482035,
                "mdate": 1700659482035,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "eIozRiQg6V",
            "forum": "pAsQSWlDUf",
            "replyto": "pAsQSWlDUf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4667/Reviewer_bKG2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4667/Reviewer_bKG2"
            ],
            "content": {
                "summary": {
                    "value": "The authors address challenges in time series data annotation and the limitations of standard contrastive learning (CL) in representing TS data. Key contributions of the paper are:\n\n- Introduction of SoftCLT, a soft contrastive learning strategy tailored for time series data. Their framework can be adapted to other CL frameworks relatively easily.\n- The proposal of soft contrastive losses for both instance and temporal dimensions, addressing the shortcomings of existing CL methods for TS.\n- Comprehensive experimental evidence demonstrating that SoftCLT enhances state-of-the-art performance across multiple TS tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The submission has the following strenghts:\n\n- Ablation study is present and seems to demonstrate the usefulness of the proposed additions.\n- Compared to the selected baselines (emphasis on selected), the model performs well.\n- I appreciate that the authors have chosen to go for a more detailed analysis of the representation learning abilities of their model. By this I mean that rather than considering only task-performance, they also investigate aspects such as robustness to non-stationarity, and also semi-supervised learning. This is usually absent from related papers.\n- The ideas are well explained, the paper is clear."
                },
                "weaknesses": {
                    "value": "The submission has the following weaknesses:\n- Problem with the comparisons. Entirely absent from the main paper is any comparison with CoST [1], or any more recent contrastive approach. While this is a single issue it is one I am quite concerned about. Similarly, a comparison to recent approaches in the regression setting of TS2Vec (nothing prevents that comparison, the TS2Vec code works seamlessly for both approaches). My reasoning is that the proposed idea is interesting, but also relatively simple. This is fine in general: simple ideas bring value in research as well. However, coupled with a lack of comparison to recent approaches, it is very difficult to ascertain the value of the contribution. \n\nCurrently this is enough for me to not recommend acceptance, but as noted in the questions section, I am willing to update my score should the authors adress this.\n\nReferences\n[1] CoST: Contrastive Learning of Disentangled Seasonal-Trend Representations for Time Series Forecasting"
                },
                "questions": {
                    "value": "As noted in the Weaknesses section, I would like a detailed comparison with CoST and an evaluation in the regression setting. If the authors provide this, and the results warrant it, I will raise my score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4667/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4667/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4667/Reviewer_bKG2"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4667/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698790265767,
            "cdate": 1698790265767,
            "tmdate": 1700614294099,
            "mdate": 1700614294099,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jrmtXYHVDH",
                "forum": "pAsQSWlDUf",
                "replyto": "eIozRiQg6V",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4667/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4667/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**W1, Q1. Detailed comparison with CoST and an evaluation in the regression setting**\n\nThanks for bringing this to our attention. We applied our SoftCLT to CoST [A], and provide the results below and **Table H.3 in Appendix H** in the revision:\n\n| | Multivariate || forecasting | | Univariate | | forecasting | |\n| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | \n| | CoST | | CoST + Ours | | CoST | | CoST + Ours | |\n| | MSE | MAE | MSE | MAE | MSE | MAE | MSE | MAE |\n| ETTh1 | 0.650 | 0.585 | **0.619** | **0.570** | 0.094 | 0.230 | **0.091** | **0.227** |\n| ETTh2 | 1.322 | 0.876 | **1.315** | **0.872** | 0.161 | 0.307 | **0.156** | **0.304** |\n| ETTm1 | 0.409 | 0.439 | **0.407** | **0.436** | 0.054 | 0.164 | **0.051** | **0.159** |\n| Weather | 0.432 | 0.465 | **0.430** | **0.463** | 0.189 | 0.310 | **0.181** | **0.307** |\n\nWe observed consistent improvements in performance for CoST in both multivariate and univariate forecasting tasks under various datasets. The results are average MSE/MAE above 5 horizons, and averaged over four runs with different random seeds, using the official code of CoST.\n\n[A] Woo, Gerald, et al. \"CoST: Contrastive learning of disentangled seasonal-trend representations for time series forecasting.\" ICLR (2022)\n\n&nbsp;\n\n**W2. Comparison with recent contrastive approaches**\n\nWe agree that comparing with recent CL approaches and figuring out the applicability of our SoftCLT to them should be interesting and strengthen our contribution. We are supposed to apply our SoftCLT to the SOTA methods per downstream tasks, e.g., TS2Vec [A] for classification/anomaly detection and TS-TCC/CA-TCC for semi-supervised learning. Thanks to bKG2, we have added CoST [B] for regression following their suggestion.\n\nTo be more specific, we provide how we choose the baseline that we apply SoftCLT to:\n\n1. **Classification (TS2Vec [A])**: We chose the widely used UCR 128 and UEA 30 datasets for both univariate and multivariate TS classification, respectively, as these datasets cover TS from various domains and lengths, and TS2Vec has demonstrated SOTA performance for these tasks.\n\n2. **Semi-supervised Classification (TS-TCC [B], CA-TCC [C], TS2Vec [A])**: We chose CA-TCC, as it is the CL method specifically designed for semi-supervised settings and has demonstrated SOTA performance. Given that CA-TCC is an extension of TS-TCC, we also included TS-TCC in our evaluation. Additionally, TS2Vec, renowned for its SOTA performance in standard classification, is also included.\n\n3. **Transfer Learning (TS-TCC [B], CA-TCC [C])**: While there were other candidates (TS2Vec [A], TF-C [D], CoST [E], CLOCS [F]), we specifically chose TS-TCC and CA-TCC for the following reasons:\n\n- TS2Vec [A]: Its performance, as indicated by experiments conducted by CA-TCC, is mostly worse than both TS-TCC and CA-TCC.\n\n- TF-C [D]: Their result is not reproducible with their official code. This issue seems not unique to our experience by looking at several GitHub issues.\n\n- Other CL methods (CoST [E], CLOCS [F]): These methods are specifically designed for TS forecasting, hence not considered for transfer learning in classification. Their performance is inferior to TS-TCC, as shown in the updated **Table 4(a)**.\n\n4. **Anomaly Detection (TS2Vec [A])**: Given the scarcity of CL methods addressing anomaly detection, TS2Vec, a SOTA method in this domain, is chosen for our evaluation.\n\n\nWe have also added other recent methods (CoST [E], CLOCS [F], LaST [G]) to **Table 4** by comparing the transfer learning performance, as it would be beneficial to audiences.\n\n\n[A] Yue, Zhihan, et al. \"Ts2vec: Towards universal representation of time series.\" AAAI (2022)\n\n[B] Eldele, Emadeldeen, et al. \"Time-series representation learning via temporal and contextual contrasting.\" IJCAI (2021)\n\n[C] Eldele, Emadeldeen, et al. \"Self-supervised contrastive representation learning for semi-supervised time-series classification.\" TPAMI (2023)\n\n[D] Zhang, Xiang, et al. \"Self-supervised contrastive pre-training for time series via time-frequency consistency.\" NeurIPS (2022).\n\n[E] Woo, Gerald, et al. \"CoST: Contrastive learning of disentangled seasonal-trend representations for time series forecasting.\" ICLR (2022)\n\n[F] Kiyasseh, Dani, Tingting Zhu, and David A. Clifton. \"Clocs: Contrastive learning of cardiac signals across space, time, and patients.\" ICML (2021).\n\n[G] Wang, Zhiyuan, et al. \"Learning latent seasonal-trend representations for time series forecasting.\" NeurIPS (2022)"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4667/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700434354429,
                "cdate": 1700434354429,
                "tmdate": 1700434354429,
                "mdate": 1700434354429,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "doDqdrRyOq",
                "forum": "pAsQSWlDUf",
                "replyto": "jrmtXYHVDH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4667/Reviewer_bKG2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4667/Reviewer_bKG2"
                ],
                "content": {
                    "title": {
                        "value": "Response to the rebuttal"
                    },
                    "comment": {
                        "value": "I thank the reviewers for adressing my point about CoST. As this adresses the main conern I have, I have raised my score to a 6."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4667/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700614362175,
                "cdate": 1700614362175,
                "tmdate": 1700614362175,
                "mdate": 1700614362175,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]