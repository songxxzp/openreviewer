[
    {
        "title": "Boosting Semi-Supervised Learning via Variational Confidence Calibration and Unlabeled Sample Elimination"
    },
    {
        "review": {
            "id": "Q3jS0oZfFz",
            "forum": "2OwSqvxjP2",
            "replyto": "2OwSqvxjP2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1561/Reviewer_iR75"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1561/Reviewer_iR75"
            ],
            "content": {
                "summary": {
                    "value": "This study centers on pseudo labeling within the context of semi-supervised learning. To tackle the issue of inaccurate confidence scores and abundant unlabeled examples without data pruning, the author proposes two strategies of variational confidence calibration (VCC) and influence-function-based unlabeled sample elimination (INFUSE). Empirical assessments conducted on widely-adopted benchmark datasets demonstrate the efficacy of these proposed strategies. Notably, VCC yields a remarkable 3.16% reduction in error rates when compared to FixMatch."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed strategies bolster pseudo labeling by addressing two key facets: computing dependable confidence scores and judiciously selecting a subset of the unlabeled dataset. These innovations result in a significant enhancement of generalization performance while also substantially reducing computational overhead in practical applications.\n2. The writing is commendable, making the method easily comprehensible. The author offers ample experimental details, thereby facilitating the reproducibility of the study."
                },
                "weaknesses": {
                    "value": "1. The novelty of the method appears somewhat constrained. Several components of the approach, such as Monte-Carlo Dropout, temporal consistency, exponential moving average, variational auto-encoder, and influence functions, are established techniques in the field.\n2. The effectiveness of the Variational Autoencoder (VAE) implementation raises questions. VAE's main advantage lies in introducing randomness, and the efficacy of its calibration may require further substantiation. Additionally, the improvements achieved through VAE, as evidenced in Table 7, seem marginal at best."
                },
                "questions": {
                    "value": "1. The author's proposal to generate ground-truth labels using a mixup-based method raises a valid concern about the dataset's stability during training. It's essential to verify whether the constructed labeled dataset remains invariant throughout the training process.\n2. Table 9 in Appendix F.2 highlights the dominance of temporal scores in the experiments. It would be beneficial if the author could provide an explanation for this observation, shedding light on the reasons behind the temporal score's strong performance.\n3. Suggestions for improvement: 1) Placing the table title above the table itself would enhance the document's readability. 2) Updating the template, especially for the page header, would contribute to a better presentation of the work."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1561/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1561/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1561/Reviewer_iR75"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1561/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698414014369,
            "cdate": 1698414014369,
            "tmdate": 1699636084519,
            "mdate": 1699636084519,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FxoBMFAEmP",
                "forum": "2OwSqvxjP2",
                "replyto": "Q3jS0oZfFz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1561/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1561/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review and questions.  \n\n**W1:** The novelty of the method appears somewhat constrained.   \n**A1:** We admit that these techniques have been established in the literature. But the under-calibrated problem lacks enough attention in the SSL setting. We adopt three easy COSISTENCY measures to build calibration target under semi-supervised conditions. Furthermore, we make a step forward to leverage VAE to implicitly simulate the intractable factors. Although some components used in our method appear to be simple, our purpose is to make VCC a flexible and easy plug-in that can be combined with other SSL method. In addition, this work provides a new perspective by focusing on calibration in learning unreliable target. Finally, INFUSE leverages unreliable pseudo labels for selecting a small core set, which allows efficient training for time consuming SSL methods.\n\n**W2:** The effectiveness of the Variational Autoencoder (VAE) implementation raises questions.   \n**A2:** We argue that the improvement obtained by VAE is remarkable. The accuracy improvement on accuracy is 1.23/% on CIFAR100 dataset for comparing VCC-FlexMatch with FlexMatch. This improvement is almost double when compared to using MLP simply. The overall results degenerate much if directly approaching the calibration target without VAE reconstructing (more than 2/% in some cases).    \n\n**Q1:** The author's proposal to generate ground-truth labels using a mixup-based method raises a valid concern about the dataset's stability during training. It's essential to verify whether the constructed labeled dataset remains invariant throughout the training process.   \n**A3:** The constructed dataset is invariant at the early stage of training, thus producing a stable selection. But at the later stage, we frequently switch the subset for preventing overfitting on unlabeled data. We will try to clarify this stability in detail in next edition.    \n\n**Q2:** It would be beneficial if the author could provide an explanation for this observation, shedding light on the reasons behind the temporal score's strong performance.   \n**A4:** Thank you for your valuable suggestion. All the three scores are useful for constructing a good calibration target, while the temporal consistency (TC) score helps to select unlabeled samples with more consistent prediction over time. As it is analyzed in previous work [1], selecting unlabeled data with less fluctuation prevents a rapid surge on the labeled sample loss, which explains the reason why temporal score can offer great benefits. Although our idea appears like previous work [1], there are two differences. First, a fixed moving window is used in our method, instead of EMA, to allow certain degree of forgetting and to permit easier confidence calibration. Second, the scores are combined to formulate the final calibration confidence target,  instead of selecting samples directly as used in [1].   \n\n**Q3:** Suggestions for improvement.   \n**A5:** Thank you for your valuable suggestions, and we will make these improvements in the next edition.   \n\n[1] Tianyi Zhou, Shengjie Wang, and Jeff A. Bilmes. Time-Consistent Self-Supervision for SemiSupervised Learning. In International Conference on Machine Learning, Virtual, 2020."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1561/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700489709498,
                "cdate": 1700489709498,
                "tmdate": 1700535813197,
                "mdate": 1700535813197,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "02EcRdFAfC",
                "forum": "2OwSqvxjP2",
                "replyto": "FxoBMFAEmP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1561/Reviewer_iR75"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1561/Reviewer_iR75"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed response, which addresses most concerns. However, I believe the work does not meet the acceptance criteria of ICLR in terms of novelty and contribution. I have chosen to keep the current score."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1561/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700702831885,
                "cdate": 1700702831885,
                "tmdate": 1700702831885,
                "mdate": 1700702831885,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DqDcu78Gii",
            "forum": "2OwSqvxjP2",
            "replyto": "2OwSqvxjP2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1561/Reviewer_5hwM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1561/Reviewer_5hwM"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript proposes two methods, VCC and INFUSE, to improve semi-supervised learning by better utilizing unlabeled data. The effectiveness of these methods is demonstrated through experiments on multiple datasets. Overall, these methods offer promising solutions for improving the efficiency and accuracy of SSL."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality:\n- The manuscript proposes two novel methods, VCC and INFUSE, to improve semi-supervised learning by better utilizing unlabeled data. These methods are designed to address the challenges of leveraging large-scale unlabeled data in SSL, and they offer promising solutions for improving the efficiency and accuracy of SSL.\n\nQuality:\n- The manuscript provides a thorough and well-organized presentation of the proposed methods, including detailed descriptions of the models, algorithms, and experiments. The experiments are conducted on multiple datasets and in various settings, and the results demonstrate the effectiveness of the proposed methods.\n\nSignificance:\n- The proposed methods have the potential to significantly improve the efficiency and accuracy of SSL, which is an important and challenging problem in machine learning. The manuscript discusses the potential for extending the proposed methods to other SSL tasks, suggesting that they have broad applicability and potential impact in real-world scenarios."
                },
                "weaknesses": {
                    "value": "1. The manuscript has some issues with the expression of details, making it difficult to follow. For example, the article does not provide an introduction to the first two loss terms in Eq. (11).\n2. The latter part of the method involving INFUSE in the manuscript, and the earlier part on confidence calibration, seem to address two completely different problems, giving the paper a scattered feel and failing to highlight the main focus of the work. This leaves an impression of breadth over depth. \n3. The author mentions that 'INFUSE uses the influence function from Koh & Liang (2017) to compute the importance of each unlabeled example', which implies that the solution to the second issue addressed in the manuscript merely references someone else's strategy. Both the problem itself and the method of solving it lack novelty.\n4. The part on VIEW CONSISTENCY seems somewhat strained. Firstly, obtaining multiple views is difficult, and moreover, the EMA in the manuscript doesn't really have any connection with multiple views. EMA has already been showcased in the TEMPORAL CONSISTENCY section.\n5. There is an issue in the reconstruction loss, where $\\tilde{r}$ is treated as ground-truth; this itself is not accurate enough."
                },
                "questions": {
                    "value": "1. I don't quite understand \u201cwe argue that the optimizing function in RETRIEVE only considers the loss on the labeled training set, which may lead to a deviation from the desired results (i.e. minimizing the loss on the validation set)\u201d, could you please explain it in detail?\n2. The author mentioned that \" Although both consider the problem from the perspective of time, our temporal-consistency method is very dissimilar from the time-consistency method proposed by Zhou et al. (2020).\" in Sec 3.2. Please give a detailed explanation and analysis."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1561/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698567459726,
            "cdate": 1698567459726,
            "tmdate": 1699636084444,
            "mdate": 1699636084444,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AUDSR0G11Z",
                "forum": "2OwSqvxjP2",
                "replyto": "DqDcu78Gii",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1561/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1561/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review and questions.\n\n**W1:** The manuscript has some issues with the expression of details, making it difficult to follow.   \n**A1:** Thanks for your reminding, we are sorry for this and we promise to refine the details in the next version. As for the first two terms in Eq. 11, $\\mathcal{L_{lab}}$ stands for the loss of labeled examples, which is a common cross entropy loss, and the $\\mathcal{L_{unlab}}$ is the loss for unlabeled data, which is also a cross entropy loss, but implemented with the pseudo labels generated by SSL method.  \n\n**W2:** The latter part of the method involving INFUSE in the manuscript, and the earlier part on confidence calibration, seem to address two completely different problems, giving the paper a scattered feel and failing to highlight the main focus of the work. This leaves an impression of breadth over depth.   \n**A2:** The shared purpose of proposing VCC and INFUSE is to reduce the demanding need of accessing more data, but from two different perspectives. VCC can enhance the performance of SSL method when the number of labeled examples is small, such as only 4 labels per class for CIFAR10. This saves a lot of human labor for labeling. On the other hand, most pseudo-label-based SSL method is time consuming because of the multi-time inference and complex mechanism. INFUSE is proposed to largely reduce the time cost needed for training. Both of the two methods are trying to solving the dilemma that SSL method meets. Thank you for your reminding, and we will try to explain more about this motivation in next edition. \n\n**W3:** 'INFUSE uses the influence function from Koh & Liang (2017) to compute the importance of each unlabeled example'.  \n**A3:** The original influence function is proposed under the fully-supervised setting with plenty of accessible labels. However, most of labels are unknown in SSL, so the original influence function is no longer suitable. The proposed INFUSE method leverages unlabeled data without ground-truth, so that the influence function is quite different from the original one. Previous SSL coreset selection method of RETRIEVE also makes some improvements based on the influence function. But a risk occurs to indicate a bias toward minority labeled subset because of its unreliable validation mechanism. Our INFUSE proposes a mix-up method to reduce this negative bias.  \n\n**W4:** The part on VIEW CONSISTENCY seems somewhat strained.   \n**A4:** We use the EMA scheme to prevent training multiple models from scratch when small subsets are provided to include only a few labeled examples in each. Note that accessible labeled data is extremely limited. The EMA scheme can be regarded as an easy way to obtain a host of different models with temporal variations. However, the differences of these obtained EMA models may be small, so the cross-feature trick is adopted to produce better multi-view models with great diversity. In summary, the purpose of VIEW CONSISTENCY is totally different from the TEMPORAL CONSISTENCY. \n\n**W5:** The target in reconstruction loss is not accurate enough.   \n**A5:** We have already considered this inaccurate problem. Accordingly, we introduce VAE to employ hidden variables for enabling the reconstruction prediction, instead of directly approaching the reconstruction target. The experiment result justifies the effectiveness.    \n\n**Q1:** \u201cwe argue that the optimizing function in RETRIEVE only considers the loss on the labeled training set, which may lead to a deviation from the desired results (i.e. minimizing the loss on the validation set)\u201d?   \n**A6:** RETRIEVE tries to select a subset of unlabeled data that achieves the lowest loss only on the labeled data, but the number of labeled data might be very small (e.g. 4 labeled examples per class in CIFAR100). In other words, the subset selected by RETRIEVE is not validated on the huge amount of unlabeled data. Therefore, the coreset obtained by RETRIEVE introduces large deviation on the overall dataset consisting of labeled and unlabeled examples. We will clarify this point in the next edition.  \n\n**Q2:** \" Although both consider the problem from the perspective of time, our temporal-consistency method is very dissimilar from the time-consistency method proposed by Zhou et al. (2020).\"?  \n**A7:** Our temporal consistency (TC) score is different from Zhou's in several ways. First, our temporal consistency score employs a fixed moving window instead of EMA used in Zhou's method. The fixed moving window permits to forget the past to some extent, which is beneficial to easier confidence calibration as the fundamental goal of VCC. Second, we combine the scores to formulate the final calibration confidence target for the model. Instead, Zhou's TC method directly selects examples with high TC scores into the training procedure."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1561/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700489822035,
                "cdate": 1700489822035,
                "tmdate": 1700550832543,
                "mdate": 1700550832543,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QJsfvqAS97",
            "forum": "2OwSqvxjP2",
            "replyto": "2OwSqvxjP2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1561/Reviewer_o4va"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1561/Reviewer_o4va"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new semi-supervised learning technique which is based on \"variational confidence calibration\" (for calibrating the predictions on unlabeled examples) and \"unlabeled sample elimination\" (for pruning data with the goal to decrease the running time of the method). The main contributions of this paper are as follows:\n\n(i) The authors propose the Variational Confidence Calibration (VCC) method, which aims to obtain well-calibrated scores for pseudo-label selection. The method is based on computing three different scores (ensemble consistency, temporal consistency view consistency), appropriately combining them, and feeding them to a (trainable) variational auto encoder  to get the final calibrated score. The resulting score can be used in combination with other standard/SOTA semi-supervised learning techniques.\n\n(ii) The author propose the INFUSE method, which can dynamically prune unimportant unlabeled examples, in order to speed up  the convergence and reduce the computation costs in training.\n\n(iii) Extensive experimental evaluation showing the competitiveness of the proposed method with respect to other SOTA methods."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "\u2014\u00a0This is a well-written paper that proposes an interesting approach to semi-supervised learning. \n\n\u2014 The use of the VAE in the computation of the calibrated scores is a novel and intriguing idea.\n\n\u2014 Extensive experimental evaluation showing SOTA results in various datasets. In a few cases the performance gains (in terms of test-accuracy) are quite significant, e.g. in the CIFAR-100 dataset with 400 labeled examples VCC reduces the classification error rate of FixMatch from 46.47% to 43.31% (with improvement of 3.16%)."
                },
                "weaknesses": {
                    "value": "\u2014 The performance gains of using the method (in terms of test-accuracy) are typically somewhat mild, and often times less than 0.5%.\n\n\u2014\u00a0The method seems to be a bit involved, especially given the typical overall benefit."
                },
                "questions": {
                    "value": "Have the authors tried to apply their method in larger datasets like Imagenet? (I know that many of the SOTA semi-supervised learning approach suffer in the case of many classes/ large scale datasets this is why I am asking.)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1561/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698640112654,
            "cdate": 1698640112654,
            "tmdate": 1699636084365,
            "mdate": 1699636084365,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xZFCbk9VZB",
                "forum": "2OwSqvxjP2",
                "replyto": "QJsfvqAS97",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1561/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1561/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review. And hare are our responses.  \n\n**W1:** The performance gains of using the method (in terms of test-accuracy) are typically somewhat mild, and often times less than 0.5%.   \n**A1:** It's true that the gains are sometimes mild in terms of accuracy, but this phenomenon mostly happens in the cases when enough labels are provided. As we have mentioned in our paper, the notable advantage of our approach is that VCC can provide significant improvement compared with base algorithms when the number of available labels is small, such as the 3.16% improvement on FixMatch when only 4 labels of each class in CIFAR-100 is provided. The results in Table 2 can further justify this. Moreover, please see the new experimental results on Imagenet in response to your Question 1. Former methods like FlexMatch also suffers from decreasing improvement compared with base FixMatch when the labeled data increase, since the advantages of the 'novel design' decrease gradually. Besides, the test accuracy is not the only metric we considered in SSL. The calibration metrics like ECE are also introduced into this field, and the gain on these metrics should not be ignored when evaluating the effectiveness of VCC, since the under-calibrated problem is essential to SSL methods. \n\n**W2:** The method seems to be a bit involved, especially given the typical overall benefit.   \n**A2:** VCC is a plug-in and can be combined with any pseudo-label-based SSL methods. Once the super-parameters are validated, the training strategy can be easily transferred to different methods. Due to this benefit, VCC and INFUSE reduce the amount of data and labels for any SSL methods. We do believe that the overall framework is valuable to the SSL community.  \n\n**Q1:** Have the authors tried to apply their method in larger datasets like Imagenet?   \n**A3:** We are sorry about this, but we only have one 3090 GPU to carry out all our experiments. We try to make a compensation by carrying out trials on ImageNet-32 based on FixMatch and FlexMatch with 100 labels per class (less than 8%), and the results are shown at below. We promise to provide more completed results in the next version.  \n\n| method        | accuracy(%) |  \n| --------      | ---------   |  \n| FixMatch      | 32.06       |\n| VCC-FixMatch  | 36.53 (+4.47%)       |\n| FlexMatch     | 36.89       |\n| VCC-FlexMatch | 39.21 (+2.32%)       |"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1561/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700489561142,
                "cdate": 1700489561142,
                "tmdate": 1700492796757,
                "mdate": 1700492796757,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "G7G8gXtwgT",
            "forum": "2OwSqvxjP2",
            "replyto": "2OwSqvxjP2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1561/Reviewer_LFY9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1561/Reviewer_LFY9"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies semi-supervised learning (SSL). This paper points out two issues of existing SSL methods, including 1) the incorrect pseudo labels caused by calibration error, 2) the huge computation cost in training. To address the first issue, this paper proposes Variational Confidence Calibration (VCC), a variational method to obtain the calibrated confidence scores for pseudo-label selection. To address the second issue, this paper proposes the INfluence Function-based Unlabeled Sample Elimination (INFUSE) method, which uses the influence function to compute the importance of each unlabeled example. The two methods can be combined together to achieve high prediction accuracy with lower training costs. Experimental results demonstrate the effectiveness of the proposed methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The writing is very clear.\n- The proposed two methods are reasonable. There is an important advantage of VCC, i.e., it can be plugged into existing SSL methods to enhance their performance.\n- Experimental results and ablation studies support the proposed methods."
                },
                "weaknesses": {
                    "value": "- The proposed methods seem not novel enough, because they are only adapted from existing techniques, i.e., Variational Auto Encoder and Influence Function. It is intuitive that such a combination method can work well and thus I cannot see any important insights brought by the two methods.\n- I do not think it is a good strategy to address two independent problems of SSL together, which may not increase the contributions of this paper. A good paper is supported by an important finding/contribution. Two independent minor contributions to address different issues may not form a single significant contribution. So I would suggest that the authors should focus on a major problem and try to solve this paper from another novel perspective to dig more deeply.\n- In some tables, only a single result (without using mean$\\pm$std) is provided. I suggest further providing standard deviations."
                },
                "questions": {
                    "value": "Please check the above weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1561/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698681447463,
            "cdate": 1698681447463,
            "tmdate": 1699636084265,
            "mdate": 1699636084265,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GTy5Mw9K6r",
                "forum": "2OwSqvxjP2",
                "replyto": "G7G8gXtwgT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1561/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1561/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your careful review.   \n**W1:** The proposed methods seem not novel enough, because they are only adapted from existing techniques.   \n**A1:** We believe that the introducing of two methods is not a plain adaption of existing methods. First, the under-calibrated problem is the improtant problem of SSL methods. While most existed calibration methods are designed for fully-supervised problem, we propose a reasonable strategy to generate calibration target under Semi-Supervised background. However, directly approaching this target is not reliable enough since there are many \"nuisance\" factors as we have mentioned in paper, and this is why we introduce VAE as a calibration tool to carry out a soft approaching to the sub-optimal target. Second, most existing data selection method is designed under fully-supervised settings and can not be directly used for SSL. To address this issue, this paper proposes the INFUSE method for selecting the core data set when unlabeled data can only have access to pseudo labels in SSL setup. We believe none of these methods have been thoroughly studied before for SSL problems.\n\n**W2:** Two independent minor contributions to address different issues may not form a single significant contribution.   \n**A2:** Originally we started the study of two methods as two independent research lines at the very beginning, but later we find that there is a deep connection between them: they both further reduce the resources to train a good SSL method. To be more specific, the plug-in VCC can largely improve the accuracy of base algorithms with only a few labels, which saves a lot of effort for labeling. On the other hand, most existed SSL method requires multiple forward passes and complex mechanism to select data and to generate pseudo labels for training, which makes the training procedure remarkably time-consuming. The proposed INFUSE method can largely save the training time. We want to combine two resource-effective methods together to provide a unified framework. We will illustrate in details about the motivation above in our next edition. \n\n**W3:** In some tables, only a single result (without using mean std) is provided. I suggest further providing standard deviations.   \n**A3:** Thank you for your suggestion. We are sorry for this, but we have tried our best to provide as more results as we could with limited GPU resources. We will provide further results as you suggest in next version."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1561/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700489546119,
                "cdate": 1700489546119,
                "tmdate": 1700492740774,
                "mdate": 1700492740774,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "k8mSZYtBS6",
                "forum": "2OwSqvxjP2",
                "replyto": "GTy5Mw9K6r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1561/Reviewer_LFY9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1561/Reviewer_LFY9"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for providing the rebuttal. I feel that I am still not convinced. 1) It is indeed a combination of two existing techniques without new insights. 2) I do not agree with that \"there is a deep connection between them because they both further reduce the resources to train a good SSL method\". If this is true, many methods that have the same purpose can be said to be deeply connected. 3) I would expect the results, because standard deviations are really important.\n\nI would admit that the points I am concerned about are somewhat subjective. But personally, I feel that the novelty and contribution are not enough."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1561/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666662641,
                "cdate": 1700666662641,
                "tmdate": 1700666662641,
                "mdate": 1700666662641,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]