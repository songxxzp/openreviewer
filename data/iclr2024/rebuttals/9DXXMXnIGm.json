[
    {
        "title": "Elucidating the design space of classifier-guided diffusion generation"
    },
    {
        "review": {
            "id": "mvSDm2vrXN",
            "forum": "9DXXMXnIGm",
            "replyto": "9DXXMXnIGm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4483/Reviewer_VZ9L"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4483/Reviewer_VZ9L"
            ],
            "content": {
                "summary": {
                    "value": "The paper focuses on a very interesting question of guiding the diffusion modes by leveraging off-the-shelf classifiers in a training-free fashion. Specifically, the authors first provide a simple analysis to demonstrate that the off-the-shelf classification can achieve better accuracy than the fine-tuned classifier when the noise level is high, which may have been ignored in the previous works. Then, the authors turn to exploit the pre-trained classifier for guiding diffusion generation with comprehensive consideration of the detailed settings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tIn my opinion, the major contribution of this paper lies in Section 4.1, which provides an empirical analysis by evaluating the calibration of both fine-tuned and off-the-shelf classifiers.  The results reveal that fine-tuned classifiers are less calibrated than off-the-shelf ones when the noise level is high, which indicates that Off-the-shelf classifiers\u2019 potential is far from realized.\n2.\tTo optimize the design of the proposed methods, the authors have considered multiple aspects, including the classifier inputs, smoothing guidance, guidance direction, and guidance direction, and designed the corresponding strategies to improve the overall performance."
                },
                "weaknesses": {
                    "value": "1.\t The paper focuses on the classic class-conditional diffusion generation, which is a simple case in the text-to-image generation. How about the performance of the proposed methods for the general case with text prompts as conditions? In particular, in section 5.4, the authors also provide a simple analysis of this case with the CLIP model. However, the results are not good.\n2.\tHuman-level metrics should be involved for clearer comparisons. It is well-known that some quantitative metrics like FID, may be problematic in some cases. In particular, the FID metric, used in ablation analysis in section 4 and experiments in section 5, cannot measure conditional adherence which is important for conditional generation. CLIP score should also be considered to evaluate the performance.\n3.\tIt is weird that the final images are merely the same as each other in Figure 4 with different settings of logit temperature, while the FID score varies with different settings in Table 4."
                },
                "questions": {
                    "value": "My major commons lie in the experimental evaluation. Please provide more experimental analysis or discussions to validate the effectiveness and robustness of the proposed methods.\n\nHow about the performance in terms of CLIP score and Human-level metrics?\n\nHow about the performance for the conditional generation with the general text prompts?\n\nPlease provide more discussions about Table 4 and Figure 4."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4483/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698569260951,
            "cdate": 1698569260951,
            "tmdate": 1699636424402,
            "mdate": 1699636424402,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sWRhDCYOzT",
                "forum": "9DXXMXnIGm",
                "replyto": "mvSDm2vrXN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4483/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4483/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VZ9L"
                    },
                    "comment": {
                        "value": "Thanks for the helpful comments. Below, we address the raised concerns.\n\n\n## How about the performance of the proposed methods for the general case with text prompts as conditions? In particular, in section 5.4, the authors also provide a simple analysis of this case with the CLIP model. However, the results are not good.\n\nThanks for the question. We agree that the more general text prompt condition is very important. The class-conditional setting, although simple, does provide a solid test bed for different kinds of guidance methods, with well-accepted quantitative measurements. \nIf one method is not good at class-guided generation, we wouldn't expect it to excel at the more complicated text prompts. \nTherefore, we think our evaluations on the more typical class-guided generation case are valuable and the findings can potentially be generalized to more general settings. \n\n\nNevertheless, we have explored the CLIP-guided generation task. \nIn the added Figure 6 of our revised paper, we present a comparison of CLIP scores between the original guidance schedule and our proposed schedule. Notably, our updated schedule consistently achieves significantly higher CLIP scores throughout the sampling process. The objective of our CLIP-guided sampling approach is to showcase the capability of a traditional off-the-shelf CLIP model in guiding the ImageNet-trained diffusion model (ADM [Dhariwal et al. 2021]). By leveraging this guidance, our model generates images that surpass the limitations of the ImageNet dataset and align with the provided prompt.\n\nWe aim to investigate more complicated discriminative models such as BLIP-VQA, as well as discriminative models in other modalities as guidance in the future. \n\n\n## Human-level metrics should be involved for clearer comparisons... CLIP score should also be considered to evaluate the performance.\n\nGiven the subjectivity of human-level metrics, we primarily focus on utilizing quantitative metrics to evaluate the performance of the classifier-guided diffusion (including our extensive experiments in evaluating the classifier-guided diffusion with quantitative metrics across diverse diffusion architecture: DDPM, EDM, DiT). For a more comprehensive evaluation, we add more quantitative metrics to the experiments. There are some values missing because the authors did not provide them. \n\n\nTable 1: ImageNet 64x64 evaluation: 250 steps of DDPM sampling results.  \n|                    |   IS   |   FID  |   sFID  | Precision | Recall \n|---|---|---|---|---|---\n| ADM                |   -    |  2.61  | 3.77  | 0.73  |  0.63 \n| ADM-G              |   -    |  2.07  | 4.29  | 0.74  | 0.63  \n| Resnet101-guided(Ours)   |  70.52 |  1.72  | 3.36  | 0.76  | 0.60 \n\nTable 2: ImageNet 128x128 evaluation: 250 steps of DDPM sampling results.  \n|                 |   IS   |   FID  |  sFID | Precision| Recall \n|---|---|---|---|---|---\n| ADM             |    -   |  5.91  | 5.09  | 0.70  | 0.65 \n| ADM-G           | 182.69 |  2.98  | 5.09  | 0.78  | 0.59  \n| CFG             | 158.47 |  2.43  | -     |   -   |  -    \n| Resnet101-guided(Ours)| 187.83 | 2.19   | 4.53  | 0.79 | 0.58 \n\nTable 3: ImageNet 128x128 evaluation: 25 steps of DDIM sampling results.  \n|                 |   IS   |   FID  |  sFID | Precision| Recall \n|---|---|---|---|---|---\n| ADM-G           |   -    |  5.98  | 7.04  | 0.78  | 0.51 \n| Resnet101-guided(Ours)|  139.88|  5.72  |  5.56 | 0.73  |  0.55 \n\n\nIn the added Figure 6 of our revised paper, we present a comparison of CLIP scores between the original guidance schedule and our proposed schedule. Notably, our updated schedule consistently yields significantly higher CLIP scores throughout the sampling process.\n\n\n## It is weird that the final images are merely the same as each other in Figure 4 with different settings of logit temperature, while the FID score varies with different settings in Table 4.\n\nIn Figure 4 of our paper, the random seed was fixed to ensure a consistent and strict comparison, thereby minimizing dramatic changes. However, it is worth noting that some significant differences still exist between the two settings. For example, in the left-hand side figure representing the traditional guidance setting, the generated dog gradually loses the depiction of its feet from time 50 to 0. In contrast, in the right-hand side figure where the joint guidance was amplified, the dog's feet remain clear, and there is a noticeable enrichment of environmental details."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4483/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700403630782,
                "cdate": 1700403630782,
                "tmdate": 1700403630782,
                "mdate": 1700403630782,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vvJWRRZKgX",
                "forum": "9DXXMXnIGm",
                "replyto": "sWRhDCYOzT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4483/Reviewer_VZ9L"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4483/Reviewer_VZ9L"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your responses."
                    },
                    "comment": {
                        "value": "We sincerely appreciate the authors' helpful responses, which have effectively addressed most of my inquiries, particularly regarding the CLIP score metric. However, I prefer to hold my score at this time, due to the concerns on the generalizability of the proposed method. Additionally, I believe that the paper would greatly benefit from an extension of the current method to encompass the broader task of text-to-image synthesis."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4483/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700573106279,
                "cdate": 1700573106279,
                "tmdate": 1700573106279,
                "mdate": 1700573106279,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1YcysWy8tS",
            "forum": "9DXXMXnIGm",
            "replyto": "9DXXMXnIGm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4483/Reviewer_wqgP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4483/Reviewer_wqgP"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript analyzes the limitations of current classifier guidance in terms of flexibility, calibration error as well as smoothness. Based on the analysis, the authors propose to use an off-the-shelf classifier instead of a noise-aware classifier for guidance which shows some encouraging results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The problem is very interesting, the method is simple yet effective. The current approach requires finetuning classifiers as well as training diffusion models to be a joint model. This work removes this limitation and shows that the off-the-shelf classifier has the potential to outperform noise-aware models."
                },
                "weaknesses": {
                    "value": "Although the work is very interesting, there are still some questions:\n\n1. The comparison is not so comprehensive. Since the main baseline should be the noise-aware classifier, the work provides little comparison in Table 5. More metrics such as IS, sFID, Precision and Recall should be included. \n2. In Table 5, results for IMN64x64 as well as IMN256x256 for DDPM from Dhariwal and Nichol (2021) is missing. Whether or not the method can perform well on other resolutions.\n3. Does off-the-shelf classifier guidance provide conditional information for the unconditional diffusion model? In my belief, the main objective of the guidance should be providing conditional information for the unconditional diffusion model rather than just improving the generated image quality. In Table 5, the Diffusion model on ImageNet128x128 is conditionally trained. Thus, the conditional information from an off-the-shelf classifier is not important. Results for combining off-the-shelf classifiers with unconditional models should be included.\n4. How off-the-shelf models Resnet can be used with ImageNet64x64 or ImageNet128x128 although these models are trained on ImageNet224x224? Besides, the generated images are clip to be in range of [-1; 1], how does it fit with model trained with input images normalized by ImageNet datasets?\n5. There is a fatal gap in the modeling part in Algorithm 1, in the sampling equation $\\hat{x}_{t-1} \\sim \\mathcal{N}(\\mu + \\gamma _t g, \\sigma _t)$. In the original paper by Dhariwal and Nichol (2021), the sampling resulted from the $\\log(p _{ \\theta }(x_t|x _{t+1}) p _{\\phi}(y|x_t))$. However, given the structure of Algorithm1, this equation is no longer valid since the gradient is taken regarding $x _0 (t)$ instead of $x_t$. In order to apply the same sampling process, even when $x_0(t)$ is forward through the classifier, the gradient should be taken regarding $x_t$. \n6. I guess from the point (5), this is the main reason why the method can not be applied to DDIM?\n7. Ablation study is missing, it is quite vague to understand which proposed scheme is the main course for the improvement. From my understanding, there are three main differences from normal classifier guidance which are:\n* Gradients via $x_0(t)$ instead of $x_t$\n* Guidance schedule\n* $\\tau _2$ temperature\n\nHowever, discussion on the contribution of each of them is missing\n\n8. Given three contributions as in (7), does the performance of the noise-aware classifier guidance also get improvements?\n9. Besides Resnet, how are other architectures such as DenseNet, Transformers? Do they also work with this scheme?\n10. CLIP guidance should be compared against the noise-aware CLIP guidance\n11. The connection from 4.1, 4.2, 4.3 and 4.4, 4.5 as well as the design of the algorithm lacks some connections.\n\nIt seems that the paper is written in hurry so that the format of the paper is not really good as well as some errors in equations:\n1. Equation (2) should be $\\log exp(\\tau f _{y(x)})$? Check format of the equation (2) also.\n2. Equation (3) should be $\\log exp(\\tau _1 f _{y(x)})$?"
                },
                "questions": {
                    "value": "See the weaknesses. The work is interesting and potential, yet there are a number of concerns as well as writting. Will raise the score if all concerns are solved."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N.A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4483/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4483/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4483/Reviewer_wqgP"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4483/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698821190175,
            "cdate": 1698821190175,
            "tmdate": 1700974870821,
            "mdate": 1700974870821,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kXrWl8YgDX",
                "forum": "9DXXMXnIGm",
                "replyto": "1YcysWy8tS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4483/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4483/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wqgP (Part 1)"
                    },
                    "comment": {
                        "value": "Thanks for the helpful comments. Below, we address the raised concerns.\n\n\n## \"The comparison is not so comprehensive. Since the main baseline should be the noise-aware classifier, the work provides little comparison in Table 5. More metrics such as IS, sFID, Precision and Recall should be included.\"\n\nIn our research, we aim to demonstrate the potential of the more accessible traditional ResNet classifier and prove that even without specifically noise-finetuned, the off-the-shelf classifier can guide the diffusion to generate high-quality sampling: FID: 2.19 in ImageNet128x128) not only largely surpass the noise-aware fine-tuned classifier guided diffusion [Dhariwal et al. 2021] (FID: 2.97) but also the classifier-free trained diffusion (FID: 2.43) [Ho et al. 2022].\n\nDue to the computation resource and time limits, we prioritize testing the ImageNet64x64 setting and we plan to include the ImageNet 256x256 results in our future research to provide a comprehensive analysis across different image resolutions. There are some values missing because the authors did not provide them.\n\nTable 4: ImageNet 64x64 evaluation: 250 steps of DDPM sampling results.  \n|                    |   IS   |   FID  |   sFID  | Precision | Recall |\n|---|---|---|---|---|---\n| ADM                |   -    |  2.61  | 3.77  | 0.73  |  0.63\n| ADM-G              |   -    |  2.07  | 4.29  | 0.74  | 0.63  \n| Resnet101-guided (Ours)   |  70.52 |  1.72  | 3.36  | 0.76  | 0.60 \n\nTable 5: ImageNet 128x128 evaluation: 250 steps of DDPM sampling results.  \n|                 |   IS   |   FID  |  sFID | Precision| Recall \n|---|---|---|---|---|---\n| ADM             |    -   |  5.91  | 5.09  | 0.70  | 0.65 \n| ADM-G           | 182.69 |  2.97  | 5.09  | 0.78  | 0.59 \n| Resnet101-guided (Ours) | 187.83 | 2.19   | 4.53  | 0.79 | 0.58 \n\n\n## Resolution: \"In Table 5, results for IMN64x64 as well as IMN256x256 for DDPM from Dhariwal and Nichol (2021) is missing... How off-the-shelf models Resnet can be used with ImageNet64x64 or ImageNet128x128 although these models are trained on ImageNet224x224? Besides, the generated images are clip to be in range of [-1; 1], how does it fit with model trained with input images normalized by ImageNet datasets?\"\n\nInside the ResNet, there is an average layer: torch.nn.AdaptiveAvgPool2d((1,1)), which is capable of reshaping the different pixel input into the fixed size. \nDuring guided sampling, we primarily adhere to the guidance definition outlined in ADM [Dhariwal et al. 2021]. In this approach, a gradient is computed with respect to the generated samples since the guidance is incorporated into the generated images. Additionally, the guidance is computed as the gradient of the logarithm of softmax classifier logits, which is then normalized through an exponential operation across all classes.\n\n\n## \"Does off-the-shelf classifier guidance provide conditional information for the unconditional diffusion model? Classifier guidance vs. conditional diffusion. \"\n  \nAccording to the findings presented in Table 4 of ADM [Dhariwal et al. 2021], classifier-guided unconditional diffusion falls short in terms of performance (FID: 12.0) compared to classifier-guided conditional diffusion (FID: 4.59). Consequently, the subsequent experiments in Table 5 of ADM [Dhariwal et al. 2021] specifically focus on classifier-guided conditional diffusion. \nTo ensure a fair comparison with SOTA results, we have adopted the guided conditional diffusion setting in this study. \n\nAdditionally, DiT is trained in a classifier-free manner, which encompasses both class-conditional and unconditional conditions. Table 9 of our revised paper demonstrates that off-the-shelf classifier guidance can also be employed in classifier-free diffusion.\nHowever, we acknowledge that there are further settings and configurations to explore, which will be addressed in future research endeavors."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4483/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700399235523,
                "cdate": 1700399235523,
                "tmdate": 1700399235523,
                "mdate": 1700399235523,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "axERaVMgFw",
                "forum": "9DXXMXnIGm",
                "replyto": "1YcysWy8tS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4483/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4483/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Appreciation"
                    },
                    "comment": {
                        "value": "Dear Reviewer wqgP,\n\nThank you for your detailed review and valuable feedback. \nWe sincerely hope our previous response has addressed your questions and concerns.\nWe look forward to hearing back from you and addressing any further questions or concerns you may have. \nThank you for your time."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4483/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654133348,
                "cdate": 1700654133348,
                "tmdate": 1700654133348,
                "mdate": 1700654133348,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hh3JHqNI70",
            "forum": "9DXXMXnIGm",
            "replyto": "9DXXMXnIGm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4483/Reviewer_rssc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4483/Reviewer_rssc"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a novel method to improve guidance in conditional diffusion generation without additional training by utilizing off-the-shelf classifiers. The authors introduce pre-conditioning techniques that enhance the performance of existing state-of-the-art diffusion models by up to 20% on ImageNet. Their approach is efficient, scalable, and leverages the widespread availability of pretrained classifiers, promising advancements especially in text-to-image generation tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper adeptly integrates elements from both classifier-free and classifier-based diffusion approaches. Additionally, it employs off-the-shelf classifiers to enhance performance while maintaining efficiency. The experimental results presented confirm the effectiveness of these methods."
                },
                "weaknesses": {
                    "value": "1. Sec 4.2 \"PREDICTED DENOISED SAMPLES\" seems trivial. This is already explored by the CVPR paper \" Universal guidance for diffusion models\". The authors should not make this part a separate subsection if this is not the author's original work.\n\n2. Sec 4.3 \"SMOOTH CLASSIFIER\" seems trivial. I think there are already many works which use Softplus as activation function and explore its difference/advantage with ReLU. If I understand it correctly, the only contribution here is to replace ReLU with Softplus. The novelty point is not enough for an ICLR paper.\n\n3. Sec 4.4 \"JOINT VS CONDITIONAL DIRECTION\", the author mention \"we reduce the value of marginal temperature\", which seems kind of manual tuning parameters. Is there some validation metric the authors use to determine the optimal temperature?\n\n4. Sec 4.5 \"GUIDANCE SCHEDULE\" seems trivial. If I understand it correctly, the only contribution here is introducing a sin factor. This seems more like a trick instead of some research contribution. For it to be a research contribution, the author should first discuss what kind of guidance is good and why the author choose the sin factor here. \n\n5. Sec 5.3 \"OFF-THE-SHELF GUIDANCE FOR DIT\", the authors propose to incorporate classifier guidance g into classifier-free sampling. The idea is straightforward but the same question the authors introduce a parameter gamma_t here. How do the authors choose a proper gamma_t for a specific case?"
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4483/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4483/Reviewer_rssc",
                        "ICLR.cc/2024/Conference/Submission4483/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4483/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699017820646,
            "cdate": 1699017820646,
            "tmdate": 1700604531358,
            "mdate": 1700604531358,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8Twwcs5tVm",
                "forum": "9DXXMXnIGm",
                "replyto": "hh3JHqNI70",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4483/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4483/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer rssc"
                    },
                    "comment": {
                        "value": "Thanks for your time in reviewing our work. \nBelow, we address the raised concerns.\nWe hope you can give our work the benefit of the doubt and look at it as a whole, rather than singling out each of its elements. \n\n## Sec 4.2 \"PREDICTED DENOISED SAMPLES\" seems trivial.\n\nThe idea of using predicted denoised samples is not hard to think of and it has already been applied by other methods, e.g., universal guidance [Bansal et al. 2023]. However, why it is a better choice still lacks justification. \nWe do not claim the proposal of it as one of our novelties, but we did offer additional empirical results including integral calibration error $\\overline{\\text{ECE}}$ and FID as justification, as shown in Table 2 of our paper. Such a detailed comparison has not been made in previous work. \n\n## Sec 4.3 \"SMOOTH CLASSIFIER\" seems trivial. \n\nThe smoothness of the classifier is linked to its calibration. This may seem trivial but to the best of our knowledge, we are the first to point out calibration as one of the guidelines for classifier-guided diffusion generation. \nFurther, we take the diffusion process into consideration and propose to look at the expected calibration error along the diffusion process. \n\nIn Figure 1 of our paper, surprisingly, we found that off-the-shelf classifiers are more calibrated than the specifically fine-tuned ones when the noise levels are high. \nWe followed up with this observation with guided generation experiments. \nIn Appendix A.1, we found that the integral calibration error is strongly positively correlated with the performance \nThis has been one of our biggest motivations for improving the classifier calibration. \nWe have extended this part in Section 4.1 to make a more convincing case. \n\nOn the practical side, we propose to augment the activation function for calibration, from ReLU to softplus, and choose the hyperparameter $\\beta$ by examining the ECE error. \nThis type of modification has been explored in [Zhu et al. 2021] for improving adversarial transferability but not for guiding diffusion generation. \nWith this design choice, we are able to achieve better performance than well-established classifier guidance and classifier-free guidance on ImageNet benchmarks, which is not known to be possible before. \n\n## Sec 4.4 \"JOINT VS CONDITIONAL DIRECTION\", the author mention \"we reduce the value of marginal temperature\", which seems kind of manual tuning parameters. Is there some validation metric the authors use to determine the optimal temperature?\n\nEmpirically, we determine the hyperparameter for the marginal temperature based on a lightweight guided sampling of 1000 generated samples. However, we emphasize that the true value lies in the underlying insight. This is supported by the ablation study presented in Table 4 of our paper (we put it below), where all reduced values of the marginal temperature exhibit significant improvements over the baseline($\\tau_2=1$), differing only in magnitude.\n\n\nTable: Ablation study of marginal logit temperature $\\tau_2$ with respect to FID. $\\tau_1$ is fixed as 1. The evaluation process involves generating 10,000 samples from the 128x128 ImageNet dataset using off-the-shelf classifier-guided ADM[4], utilizing 250 DDPM steps. \n|      Marginal temperature           |   1.0 (base)   |   0.8  |  0.7 |  0.5  | 0.3 \n|---|---|---|---|---|---\n| FID             |    6.20   | 5.62  | 5.45  | 5.27 | 5.30 \n\n\n\n## Sec 4.5 \"GUIDANCE SCHEDULE\" seems trivial.\n\nThe inspiration for the proposed new guidance schedule comes from the observation of CLIP scores' evolution in the CLIP-guided experiment, as depicted in the added Figure 6 of our revised paper. We observed that the CLIP scores only increased at the beginning of the guided sampling process (from 1000 to 800 time steps), but plateaued afterward. Consequently, we introduced a sine function to modify the original guidance schedule. This choice was motivated by two reasons: firstly, the sine function is continuous and smooth, allowing for a seamless transition in guidance; secondly, it amplifies the guidance, particularly during the early to middle stages of the guided sampling process, shown in Figure D.1 of the appendix. \n\n## Sec 5.3 \"OFF-THE-SHELF GUIDANCE FOR DIT\", how do the authors choose a proper gamma_t for a specific case?\n\nWhen it comes to selecting the appropriate choices for $\\gamma_t$, we typically employ a lightweight guided sampling of 1000 generated samples for evaluation."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4483/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700398992534,
                "cdate": 1700398992534,
                "tmdate": 1700399016751,
                "mdate": 1700399016751,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fqY5wxs9yD",
                "forum": "9DXXMXnIGm",
                "replyto": "8Twwcs5tVm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4483/Reviewer_rssc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4483/Reviewer_rssc"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the feedback"
                    },
                    "comment": {
                        "value": "Thanks for the feedback. It did clarify some of my concerns. I think the overall novelty is not enough as it involves many manual design.\n\nI have increased my score to 5."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4483/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700604511401,
                "cdate": 1700604511401,
                "tmdate": 1700604511401,
                "mdate": 1700604511401,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OqzW268P83",
            "forum": "9DXXMXnIGm",
            "replyto": "9DXXMXnIGm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4483/Reviewer_r6Zr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4483/Reviewer_r6Zr"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors analyze off-the-shelf classifier guidance diffusion through multiple perspectives including calibration, smoothness, score decomposition and guidance scale scheduling. They use calibration error as a new metric to assess the performance of classifier guidance diffusion and propose several techniques to improve this task. They conduct experiments on multiple diffusion models with pre-trained ResNet classifiers and show consistent improvement with their proposed method compared to baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper provides several interesting techniques to improve off-the-shelf classifier guidance diffusion. These techniques are practical, training-free and fairly easy to be incorporated into the current diffusion frameworks. Their analysis also provides interesting insight into what happens in the process of classifier guidance diffusion from multiple different perspectives."
                },
                "weaknesses": {
                    "value": "1. I don\u2019t really see the direct connection between Proposition 4.1 and Eq. 1. There seems to be a gap between ECE and $\\|p_n-p\\|$. Also to make the paper self contained, \u201cbins\u201d, \u201cacc\u201d, \u201cconf\u201d should be defined before mentioning.\n\n2. It is unclear to me how Proposition 1 and ECE inform the design choices of the method. For Section 4.2, the same conclusion can be drawn with only FID. For Section 4.3, Proposition 1 only says \u201cwith smoothness $k>1$\u201d, but it doesn\u2019t claim better calibration/score prediction with higher smoothness. Section 4.4 and 4.5 seems to be completely irrelevant to ECE.\n\n3. Relating to Weakness 2, I think the story line of this paper is a little bit scattered. There are many components in the story and it is difficult to tell which one is the main contribution of this paper. The components can also be better connected.\n\n4. There are four components to the proposed method: (1) use $ \\nabla_{\\hat{x_0}(t)} \\log p(y|\\hat{x_0}(t))$ instead of $\\nabla_{\\hat{x_t}} \\log p(y|\\hat{x_t})$ (2) use Softplus activation to increase the smoothness (3) use a second temperature to adjust the \u201cratio\u201d of joint and marginal guidance (4) sin factor guidance schedule. Since there is no ablation study that involves all four components conducted, it is very difficult to tell which one is actually effective. It would be great if the authors can include experiments that gradually exclude these components one-by-one to see which one is the most effective.\n\nMinor suggestions:\n\n1. According to the official style files provided by ICLR call for papers, the appendix should be included in the same PDF file as the main text and the bibliography.\n\n2. Eq. 1 $k$ notation conflicts with the smoothness $k$ in Proposition 4.1."
                },
                "questions": {
                    "value": "1. The formula for $\\text{ECE}_t$ is with respect to $\\hat{x_t}$ but in Section 4.2 the authors talked about using $\\hat{x_0}(t)$ will provide better calibration, so which sample did the authors use when providing the ECE results for the rest of the experiments?\n\n2. It is unclear to me how did the authors incorporate the Softplus activation function into the pre-trained classifiers, did they just replace all the activation functions in the pre-trained models? Or is there anything else that they did?\n\n3. How did the authors calculate the marginal likelihood for CLIP guidance generation?\n\n4. What is the recurrent step in Table 1? Is it the same as the \u201cbackward guidance\u201d in \u201cUniversal Guidance for Diffusion Model\u201d paper? And why is the inference time not changed significantly with the calibrated method given there is extra marginalization required for all classes?\nWhat type of GPU did the authors use in their experiments?\n\nI am happy to raise my score if the authors address my concerns and questions in the rebuttal."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4483/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4483/Reviewer_r6Zr",
                        "ICLR.cc/2024/Conference/Submission4483/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4483/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699138863904,
            "cdate": 1699138863904,
            "tmdate": 1700624049710,
            "mdate": 1700624049710,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2XzanM35EJ",
                "forum": "9DXXMXnIGm",
                "replyto": "OqzW268P83",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4483/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4483/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer r6Zr (Part 1)"
                    },
                    "comment": {
                        "value": "Thanks for the helpful comments. Below, we address the raised concerns.\n\n## Connection between Proposition 4.1 and Eq. 1.... It is unclear to me how Proposition 1 and ECE inform the design choices of the method. \n\nWe apologize for the confusion. Please allow us to clarify. \n\nIn score-based generative modeling, accurate estimation of the score function is of critical importance. \nHowever, for off-the-shelf classifiers, the second-order score information is difficult to access. \nProposition 4.1 states that if the calibration error is controlled, so is the fisher divergence. \nSince the calibration error can be estimated by the so-called expected calibration error (ECE), we turn to it as our criterion for tuning the classifier. Equation 1 is the definition of the expected calibration error, which counts for a certain range of predicted confidence, how much does it agree with the actual accuracy. \n\nTaking the diffusion process into consideration, we propose to look at the integral calibration error and it has been one of our biggest motivations for the design choices. \n\n\n## I think the storyline of this paper is a little bit scattered. \n\nThe integral calibration error, denoted as $\\overline{\\text{ECE}}$, plays a crucial role in our research. It assesses the calibration of the off-the-shelf classifier throughout the diffusion process. Proposition 4.1 establishes a connection between classifier guidance and probability estimation, emphasizing the significance of achieving a lower calibration error. In Figure 1, we analyze $\\overline{\\text{ECE}}$ values and demonstrate that the off-the-shelf ResNet achieves lower $\\text{ECE}_t$ values than the fine-tuned classifier when the noises are large. This finding provides a strong basis for asserting that the off-the-shelf classifier has the potential to match the performance of fine-tuned classifier guidance.\n\nTo leverage this potential, we utilize the predicted denoised image $\\hat{x}_0(t)$ as the input to the classifier, as suggested in universal guidance [Bansal et al. 2023]. To achieve improved calibration, we propose three key design elements: smooth classifier guidance, consistent guidance direction, and an updated guidance schedule. These design choices aim to calibrate the off-the-shelf classifier with the diffusion models and enable the generation of high-quality guided samples. By incorporating these elements, we enhance the calibration of the off-the-shelf classifier and facilitate the production of superior samples.\n\nWe have refined the writing at the end of Section 4.1. \n \n## \"Ablation study is missing\"\n\nThanks for the question. From Section 4.2 to Section 4.5, we have provided ablation studies for each of the design choices. \nWe agree that it's important to look at all of them together in one task. Hence, we have provided additional ablation experiments. \n\nThe table below (Table 6 of our revised paper) showcases the results of our off-the-shelf classifier guided sampling in sequential ablation experiments, where we explore different design choices. The sequential parameter rule is employed to identify the selection that yields the best FID value. The percentage number within the brackets of each row indicates the improvement achieved by each design choice compared to the baseline setting, which is represented in the first column of each row. The evaluation process involves generating 10,000 samples from the 128x128 ImageNet dataset using ADM[4], utilizing 250 DDPM steps.\n\nTable: Sequential ablation: \n| Inputs |   $\\hat{x}_t$   | **$\\hat{x}_0(t)$** |  |  |   |\n|---|---|---|---|---|---\n| FID    |  8.61  | **7.17 (16\\%)**    |   |   |   \n| Softplus $\\beta$ | inf (ReLU) |  5.0 | 4.0 | **3.0** |  2.0 |\n| FID              | 7.17 |  6.89  |  6.73  |  **6.61 (8\\%)** | 7.02  | \n| marginal temperature $\\tau_2$| 1.0 | 0.8 | 0.7 | **0.5** | 0.3 | \n| FID                          | 6.61 |  5.62  | 5.45 | **5.27 (20\\%)** | 5.30 |\n| sin factor $\\gamma$ | 0.0 | 0.1 | 0.2 | **0.3** | 0.4 | \n| FID                 | 5.27 | 5.30  | 5.27 | **5.24 (1\\%)** | 5.38 |\n\nWe have added the ablation study in Section 4.6 of our revision."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4483/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700397524902,
                "cdate": 1700397524902,
                "tmdate": 1700397524902,
                "mdate": 1700397524902,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qdGqVJyp53",
                "forum": "9DXXMXnIGm",
                "replyto": "2XzanM35EJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4483/Reviewer_r6Zr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4483/Reviewer_r6Zr"
                ],
                "content": {
                    "title": {
                        "value": "Response to Rebuttal Part 1"
                    },
                    "comment": {
                        "value": "Thank you for your response. While some of my questions are addressed in authors' response, there are still further clarification I would like to request from them:\n\n1. **Connection between Proposition 4.1 and Eq. 1:** Thank you for your clarification. This answers my question.\n2. **How does ECE inform the design choices:** Although the authors clarified the relationship between Proposition 4.1 and their motivation, which has been very helpful, unfortunately they didn't answer my second concern directly. Can the authors provide more details about how ECE inform their design choices for increasing the smoothness and marginal temperature? (If you have mentioned it in your modified manuscript, please correct me and point me to the location where you mentioned it)\n3. **Ablation Study**: Thank you for the additional experiment. This has addressed my concerns. A minor suggestion is to include the type of evaluation (i.e. FID) in your manuscript because the current text doesn't mention that."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4483/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700599535937,
                "cdate": 1700599535937,
                "tmdate": 1700599535937,
                "mdate": 1700599535937,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FT5cZBCQyq",
                "forum": "9DXXMXnIGm",
                "replyto": "vwoVBD06wY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4483/Reviewer_r6Zr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4483/Reviewer_r6Zr"
                ],
                "content": {
                    "title": {
                        "value": "Response to Rebuttal Part 2"
                    },
                    "comment": {
                        "value": "1. **Which sample is used for calculating ECE:** Please allow me to rephrase my question: For reporting ECE values, is the sample $x_t$ or $\\hat{x_0}(t)$ used in the classifier?\n2. **How to use Softplus:** Thank you. This answers my question.\n3. **Marginal Likelihood for CLIP:** Thank you. This answers my question, and I would encourage the authors to include this detail in their manuscript if they haven't done that already.\n4. **Details about the experiments:** Thank you and this answers my question."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4483/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700599970841,
                "cdate": 1700599970841,
                "tmdate": 1700599970841,
                "mdate": 1700599970841,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kBeejTSnsK",
                "forum": "9DXXMXnIGm",
                "replyto": "OqzW268P83",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4483/Reviewer_r6Zr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4483/Reviewer_r6Zr"
                ],
                "content": {
                    "title": {
                        "value": "Overall response to the rebuttal"
                    },
                    "comment": {
                        "value": "Although the authors answered many of my questions, I would still like the authors to clarify how they choose their design choices based on ECE before I raise my score. But overall I am inclining to raise my score and also adjust my confidence to 3 in case there is any further misunderstanding from my side."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4483/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700600211074,
                "cdate": 1700600211074,
                "tmdate": 1700600211074,
                "mdate": 1700600211074,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QWvsalC6rO",
                "forum": "9DXXMXnIGm",
                "replyto": "OqzW268P83",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4483/Reviewer_r6Zr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4483/Reviewer_r6Zr"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response"
                    },
                    "comment": {
                        "value": "Thank you for your response. These answers clarify all my questions, and although I still think the story of the paper can be better structured since not all the tricks/methods developed are related to the main claimed contribution of the paper (i.e. using calibration as the general guideline to analyze the design space of classifier guidance diffusion), the analysis presented in this paper is significant enough to warrant an acceptance. Therefore, I am increasing my score to a 6."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4483/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700624017334,
                "cdate": 1700624017334,
                "tmdate": 1700624214857,
                "mdate": 1700624214857,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5UgaWXTT30",
            "forum": "9DXXMXnIGm",
            "replyto": "9DXXMXnIGm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4483/Reviewer_6jri"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4483/Reviewer_6jri"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates improving sample quality in conditional diffusion generation by leveraging off-the-shelf classifiers in a training-free context. The authors propose pre-conditioning techniques based on classifier calibration, significantly enhancing diffusion models' performance with minimal computational overhead, verified through experiments on ImageNet. They introduce a novel metric, integral calibration error (ECE), to evaluate the effectiveness of classifier guidance and find that off-the-shelf classifiers outperform trained classifiers under high noise. To combat the diminishing influence of classifier guidance in later diffusion stages, a new weighing strategy is suggested, yielding better sample quality. The paper highlights the potential of their methods in text-to-image generation and points out the limitations of current guidance enhancement methods in terms of sampling efficiency."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper convincingly establishes the research context. In particular, it is effective in demonstrating the robustness differences across time step intervals between off-the-shelf classifiers and fine-tuned classifiers using the ECE metric.\n- The division of the design space for diffusion guidance appears appropriate, and the empirical process of selecting among the various options seems justified.\n- The authors' exploration reveals that guidance using off-the-shelf models exhibits performance comparable to or surpassing previous methods, which required significant computational costs.\n- The experiments with guidance through CLIP demonstrate the potential for extending guidance models beyond classifiers, indicating scalability in the approach.\n- The theoretical explanations provided lend solid persuasive strength to the authors' design choices."
                },
                "weaknesses": {
                    "value": "- While I rate this study highly in general, it falls short in comparing with previous research utilizing off-the-shelf models.\n- Specifically, the omission of closely related prior works, [1, 2] , is significant. PPAP[2] explores guidance using a wide range of modalities of off-the-shelf models in an efficient tuning and plug-and-play manner without significant computational costs, which is directly relevant to this paper's discussion. Observations such as the varying contributions of tuning guidance models across different time steps could also reinforce the evidence between the two studies.\n- Overall, I highly appreciate the paper's contribution to the completeness of the discussion on diffusion models' guidance. However, to properly highlight this paper's direct contributions, a comparison with prior studies attempting off-the-shelf guidance is crucial. Although the authors conducted experiments comparing various design choices of off-the-shelf guidance, it is not clearly presented how these relate to previous research, and there is no direct comparison of the final FID scores with prior methodologies. The lack of such comparisons has inclined my evaluation towards rejection.\n- In the CLIP guidance experiments, it seems that the authors did not apply the same level of guidance as they did with off-the-shelf classifiers. The CLIP guidance experiment does not appear to reflect the authors' contributions with a new methodology for off-the-shelf guidance.\n\n[1]: Graikos, Alexandros, et al. \"Diffusion models as plug-and-play priors.\" Advances in Neural Information Processing Systems 35 (2022): 14715-14728.\n\n[2]: Go, Hyojun, et al. \"Towards practical plug-and-play diffusion models.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."
                },
                "questions": {
                    "value": "- Based on the various considerations and discoveries made by the authors, I am curious whether it is possible to extend off-the-shelf guidance beyond class conditions to various modalities of conditional generation. It seemed that the CLIP experiment was intended to demonstrate such a possibility; however, as mentioned in the weaknesses, it did not feel like an experiment based on a new methodology reflecting the authors' considerations and findings."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4483/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4483/Reviewer_6jri",
                        "ICLR.cc/2024/Conference/Submission4483/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4483/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699418808145,
            "cdate": 1699418808145,
            "tmdate": 1700546683859,
            "mdate": 1700546683859,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "g1nwIf4UK9",
                "forum": "9DXXMXnIGm",
                "replyto": "5UgaWXTT30",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4483/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4483/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6jri - Part 1"
                    },
                    "comment": {
                        "value": "Thanks for the helpful comments. Below, we address the raised concerns. \n\n## Related work: the omission of closely related prior works, [1, 2], is significant...  Although the authors conducted experiments comparing various design choices of off-the-shelf guidance, it is not clearly presented how these relate to previous research, and there is no direct comparison of the final FID scores with prior methodologies. \n\n\nThanks for pointing out the related works [1] and [2]. Both works investigated guided diffusion generation. However, their focus is different from ours, and hence their proposed methodologies are fundamentally different. Below we provide a detailed comparison to each of them.\n\n\nThe goal of [1] is to demonstrate the flexibility of conditional diffusion generation given specified constraints by introducing a regularization term $\\log c(x, y)$ in the denoising process. Specifically, it explores constraints to control the digit shape in MNIST generation, facial specified attributes, infer semantic segmentations of city images, and solve a continuous relaxation of the traveling salesman problem. \n\nThe take-home message of this work is that diffusion models, trained on domain-specific data and under carefully designed conditions, are **capable of generating samples that conform to specified constraints**. While this showcases the flexibility of diffusion models in producing constraint-compliant samples, it **doesn't test the limit of the performance in each of the categories**.\n \nIn comparison, we specifically investigate class-conditional image generation and demonstrated that with proper designs, off-the-shelf classifiers can beat existing state-of-the-art methods. \nThrough comprehensive experiments on different architectures of diffusion models, including DDPM, EDM, and DiT, our off-the-shelf classifier-guided sampling consistently surpasses the baseline performance across various diffusion model architectures. \n\nSpecifically, in the table below (Table 7 of our revised paper), our FID result of 2.19 on ImageNet128x128 not only surpasses the fine-tuned classifier-guided performance (FID: 2.97) [4], but also outperforms classifier-free trained diffusion models (FID: 2.43) [5]. This highlights that off-the-shelf guidance is not merely a substitute for fine-tuned guided diffusion to save training costs, but rather a better choice for generating high-quality samples.\n\nTable: ImageNet 128x128 generation results.  \n|                 |   IS   |   FID  |  sFID | Precision| Recall |\n|---|---|---|---|---|---|\n| ADM             |    -   |  5.91  | 5.09  | 0.70  | 0.65\n| ADM-G           | 182.69 |  2.98  | 5.09  | 0.78  | 0.59 \n| CFG             | 158.47 |  2.43  | -     |   -   |  -  \n| Resnet101-guided (Ours)| 187.83 |  2.19  | 4.53  | 0.79  | 0.58 \n\n\n[2] introduces a multi-expert strategy that utilizes multiple expert classifiers, each trained to handle a specific range of noise values. These \"experts\" guide the diffusion sampling process at their corresponding time steps. The authors propose a parameter-efficient knowledge transfer strategy to reduce the computational cost. Further, they proposed a teacher-student training approach to avoid the need for labeled data.\n\nHowever, the experiments conducted in [2] do not provide conclusive evidence that multi-expert-guided sampling can compete with the fine-tuned classifier-guided diffusion ADM-G [4]. \nMoreover, though they employed parameter-efficient fine-tuning strategies like LORA, **their computation cost is still non-negligible and cannot be used in a plug-and-play fashion**. \n\nIn contrast, our off-the-shelf guided sampling not only eliminates the need for further training but also delivers significantly higher sampling performance. As demonstrated in Table 7 of our revised paper, our FID result of 2.19 on ImageNet128x128 not only surpasses the performance of the fine-tuned classifier-guided approach (FID: 2.97) [4], but also outperforms diffusion models trained without the use of classifiers (FID: 2.43) [5]."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4483/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700396007065,
                "cdate": 1700396007065,
                "tmdate": 1700396007065,
                "mdate": 1700396007065,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "79ZxCDOz63",
                "forum": "9DXXMXnIGm",
                "replyto": "tFVtzzswVD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4483/Reviewer_6jri"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4483/Reviewer_6jri"
                ],
                "content": {
                    "comment": {
                        "value": "The authors have satisfactorily addressed all the concerns I had raised. I trust that the discussions, particularly regarding the previously omitted comparisons and citations of related work, will be diligently incorporated into the final revision. I extend my thanks to the authors and am inclined to adjust my final rating towards acceptance."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4483/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546511903,
                "cdate": 1700546511903,
                "tmdate": 1700546511903,
                "mdate": 1700546511903,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]