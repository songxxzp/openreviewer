[
    {
        "title": "Zero-Shot Continuous Prompt Transfer: Generalizing Task Semantics Across Language Models"
    },
    {
        "review": {
            "id": "NIZ1upePIc",
            "forum": "26XphugOcS",
            "replyto": "26XphugOcS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3795/Reviewer_AxcL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3795/Reviewer_AxcL"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a new method for zero-shot continuous prompt transfer from a pretrained language models to another. This method firstly finds the anchors between two language models (subsets of shared vocabulary) to construct a relative space. Then source prompts are encoded in the relative space and corresponding prompts in the target space are searched based on the source embeddings. Extensive experiments and analysis are carried out to support the effectiveness of this method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The motivation is strong and clear. Continuous prompt tuning is an important issue, and this method allows easier transfer between model-specific continuous prompts. This may contribute to further research of parameter efficient learning and continual learning, etc.\n\n2. The idea of using vocabulary as anchors and connectors between models is novel and interesting."
                },
                "weaknesses": {
                    "value": "1. Method limitation: It seems that this method highly relied on shared vocabularies. Quality of vocabulary and selection of shared tokens may be important but no analysis is given.\n\n2. Clarification: Some key details are not clarified (See questions)\n\n3. Minor: Inconsistent use of search loss and matching loss in Section 3.4."
                },
                "questions": {
                    "value": "1. When \u201cshared vocabulary\u201d is mentioned in the fourth paragraph of the Introduction Section, does it mean the vocabularies should be the same or just have some common parts?\n2. In Equations 5 and 7, what is the optimization method and how do you implement this?\n3. In experiments, BERT based models are deliberately chosen. Will this method be applicable to generative models such as GPT? This question is especially important when LLMs have recently been popular because one can use this method to find a set of optimal prompts on a small model and transfer them to a large one.\n4. How are the anchors chosen? Are they randomly sampled? Will the selection process affect model performance?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3795/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3795/Reviewer_AxcL",
                        "ICLR.cc/2024/Conference/Submission3795/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3795/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698500661527,
            "cdate": 1698500661527,
            "tmdate": 1700630164623,
            "mdate": 1700630164623,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "udeiM2wfOX",
                "forum": "26XphugOcS",
                "replyto": "NIZ1upePIc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3795/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3795/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer AxcL"
                    },
                    "comment": {
                        "value": "We thank the reviewer for recognizing the contribution and novelty of our work.\n\n>Weaknesses 1 & Question 4: \u201cQuality of vocabulary and selection of shared tokens may be important but no analysis is given\u201d. \u201c How are the anchors chosen? Are they randomly sampled? Will the selection process affect model performance?\u201d\n\nThanks for pointing out the importance of the shared vocabulary because it provides the ground for prompt transfer. However, we would like to mention that the performance of the transfer is not sensitive to the number of anchors. In Figure 4, we show that even if we use only 512 anchors, there is not much decrease in transfer performance. For the selection of anchors, Appendix A.1 shows that we may simply pick the top-k nearest neighbors based on the prompt embeddings, and we applied this to all the experiments for our method.\n\n>Weakness 2: \u201cInconsistent use of search loss and matching loss in Section 3.4.\u201d\n\nThanks for pointing out the inconsistency of terminologies. We fixed them in the paper. \n\n>Question 1: \u201cDoes it mean the vocabularies should be the same or just have some common parts?\u201d\n\nWe only require the vocabularies to share a common subset, which is simply the intersection of the BERT, RoBERTa, and ALBERT\u2019s vocabularies in our experiments. \n\n>Question 2: \u201cIn Equations 5 and 7, what is the optimization method and how do you implement this?\u201d\n\nWe used gradient descent. Clarified after Eqn 5. Thanks!\n\n>Question 3: \u201cWill this method be applicable to generative models such as GPT?\u201d\n\nThanks for pointing out this interesting further direction. We didn\u2019t include generative models because we adhered to the framework established in the OptiPrompt paper, utilizing identical models and the factual probing dataset. We\u2019ve now extended OptiPrompt and tried generative models, namely, gpt2. The results are as follows.\n\n| source \\ target | gpt2-small | gpt2-medium |\n|---|---|---|\n| direct tune   | 31.62      | 32.23      |\n| manual        | 4.62       | 8.98        |\n| gpt2-small    | -          | 13.72       |\n| gpt2-medium   | 5.02       | -           |\n| bert-base     | 10.46      | 11.52       |\n| roberta-base  | 14.06      | 13.70       |\n\nWe found that our method is indeed applicable to generative models, and transferring the induced prompts from masked language models to generative models is also possible."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3795/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700099271516,
                "cdate": 1700099271516,
                "tmdate": 1700099271516,
                "mdate": 1700099271516,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "70reaTyMmP",
                "forum": "26XphugOcS",
                "replyto": "udeiM2wfOX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3795/Reviewer_AxcL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3795/Reviewer_AxcL"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response. It seems that the performance of \"manual\" is extremely low. Do you have any comments? Thanks!"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3795/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700550297915,
                "cdate": 1700550297915,
                "tmdate": 1700550297915,
                "mdate": 1700550297915,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pLFPdo4dMt",
                "forum": "26XphugOcS",
                "replyto": "xwLIUK8Dw6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3795/Reviewer_AxcL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3795/Reviewer_AxcL"
                ],
                "content": {
                    "comment": {
                        "value": "I believe using prompts like \"[X] plays [mask] music\" directly is not suitable. I encourage you to rewrite them to fit generative models' character and rerun the \"manual\" experiments."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3795/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700615779918,
                "cdate": 1700615779918,
                "tmdate": 1700615779918,
                "mdate": 1700615779918,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2OP9aprFdE",
                "forum": "26XphugOcS",
                "replyto": "EvyKmNa8Rt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3795/Reviewer_AxcL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3795/Reviewer_AxcL"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. The idea is interesting, but given the overall performance, I have decided to maintain my current overall rating."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3795/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630359017,
                "cdate": 1700630359017,
                "tmdate": 1700630359017,
                "mdate": 1700630359017,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UM4MRXveQG",
            "forum": "26XphugOcS",
            "replyto": "26XphugOcS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3795/Reviewer_NAhq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3795/Reviewer_NAhq"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a zero-shot continuous prompt transfer approach that learns prompt representations for target models (large models) from the representations of source models (small models). The assumption is that target models and source models share some common words as anchors. The learning approach is then forcing the projection of the soft prompts on the source anchors and the projection of those on the target anchors to be similar enough. Evaluation results on factual probing verifies the effectiveness of the approach. \n\nThe paper can be improved if the authors could\n\n(1) Compare the proposed method with a straightforward baseline:  v^t_i=\\sum_{l=1}^k cos(v^s_i, a^s_l) a^s_l. This is a simplification of the proposed method with no need of learning.\n\n(2) Evaluate the proposed method on tasks other than factual probing. Though there are 14 types of relations in the task, the task itself lacks diversity. The results can be more convincing if the authors could report comparison results on tasks such as text classification, NLI, semantic matching, and QA."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "a new zero-shot prompt transfer approach\nempirical studies on the benchmark of factual probing"
                },
                "weaknesses": {
                    "value": "The empirical results are not convincing enough due to the selection of the evaluation task"
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3795/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3795/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3795/Reviewer_NAhq"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3795/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698676039880,
            "cdate": 1698676039880,
            "tmdate": 1700523169365,
            "mdate": 1700523169365,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "I6zp78qSGB",
                "forum": "26XphugOcS",
                "replyto": "UM4MRXveQG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3795/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3795/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NAhq"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the comments.\n\n>Weakness 1 \u201cCompare the proposed method with a straightforward baseline: v^t_i=\\sum_{l=1}^k cos(v^s_i, a^s_l) a^s_l\u201d:\n\nWe believe there is a minor typo in the formula. If we understand correctly, the suggested method is to set \u201cv^t_i=\\sum_{l=1}^k cos(v^s_i, a^s_l) a^t_l\u201d. Thanks for the suggested baseline (which we call weighted target anchors below). We conducted a preliminary study by transferring the induced 41 prompts from BERT-base to BERT-large. The result compared with other methods is as follows.\n\n| Method | Accuracy |\n|---|---|\n|Weighted target anchors |1.95 |\n|Discretization | 10.76 |\n|Neural projector | 12.49 |\n|Single source | 31.40 |\n\nWe see that the proposed baseline does not perform very well in our scenario, probably because it weighs target anchors based on the source cosine similarity in a heuristic manner. \n\n>Weakness 2: \u201cEvaluate the proposed method on tasks other than factual probing\u201d\n\nOur choice of factual probing was driven by its suitability for effectively testing prompt transfer across various models, considering the range of subtasks and the inherent difficulty of the task.\n\nWe did preliminary experiments on text classification using the SST-2 (2-way sentiment classification) and DBpedia (14-way topic classification) datasets. We keep our default settings and tune five prompt embeddings on two smaller source models (bert and roberta base models) and transfer them to the roberta-large model. The results are as follows.\n\nPerformance when transferring to roberta-large\n| source | SST- 2 (accuracy) | DBpedia (accuracy) |\n| --- | --- | --- |\n| manual | 69.95 | 72.28 |\n| bert-base | 82.45 | 77.05 |\n| roberta-base | 84.63 | 80.81 |\n\nWe found the transferred prompts' performance is better than manual prompting, which is consistent with the factual probing task.\n\nWe hope our response has addressed your concerns, and we believe that our work makes a valuable contribution showing that induced prompt embedding is transferable across different models. Thanks!"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3795/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700099085570,
                "cdate": 1700099085570,
                "tmdate": 1700099085570,
                "mdate": 1700099085570,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7cTISJLEmu",
                "forum": "26XphugOcS",
                "replyto": "I6zp78qSGB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3795/Reviewer_NAhq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3795/Reviewer_NAhq"
                ],
                "content": {
                    "title": {
                        "value": "Acknowledgement"
                    },
                    "comment": {
                        "value": "I read the response and find my concerns have been addressed. Therefore, I adjusted my ratings.\n\nThanks for the effort and pointing out my typos."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3795/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700523313733,
                "cdate": 1700523313733,
                "tmdate": 1700523313733,
                "mdate": 1700523313733,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "p8J4M51EVs",
            "forum": "26XphugOcS",
            "replyto": "26XphugOcS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3795/Reviewer_2xK8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3795/Reviewer_2xK8"
            ],
            "content": {
                "summary": {
                    "value": "The paper explores methods to train continuous/soft prompts in a source model and then transfer that prompt (and use it directly) for a target model. This can be useful to do relatively expensive soft prompt tuning on small source models and transfer the trained prompt to a bigger target model (compared to training the big model itself). \n\nThe transfer idea involves encoding soft prompts from the source in a \"relative space\" (encode cosine similarity scores between soft prompt tokens and selected source token embeddings - \"anchors\") and then trying to search for target prompts that achieve similar similarity scores against corresponding anchor embeddings of the tokens in the target model. This can be done without backpropagating through the target model. This method can be also extended for multi-source transfer setup."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The target of transferring soft-prompt task semantics is an underexplored area of study. The target is well motivated in the paper. \n\n2. The method is novel and elegant.\n\n3. The idea works better against some relevant baselines for transferring prompt task semantics from a source model to a target model."
                },
                "weaknesses": {
                    "value": "1. If I understand correctly, at this point the method does not seem practical. We seem to get better accuracy just by directly using the source model. While the current approach transfers better than naive baselines or baselines based on earlier ideas (discretization, projectors), the transfer itself appears like a lose-lose scenario -- because you have to do extra work for transfer, and then you are (generally) trying to run the target soft prompt in a bigger (or same size) model. This seems pointless if just running the base source model gives us better overall performance. \n\nI am willing to accept this despite this because this paper seems like an early foray into the transfer of soft prompts and can inspire future research while serving as a baseline. Please correct me, however, if I am mistaking something about the immediate practical value of transfer given the current method."
                },
                "questions": {
                    "value": "1. How is the optimum of eqn 7 or 5 searched for exactly?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3795/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3795/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3795/Reviewer_2xK8"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3795/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698738928998,
            "cdate": 1698738928998,
            "tmdate": 1700220472771,
            "mdate": 1700220472771,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wPqVFYtpN3",
                "forum": "26XphugOcS",
                "replyto": "p8J4M51EVs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3795/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3795/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2xK8"
                    },
                    "comment": {
                        "value": "We thank the reviewer for recognizing the novelty of our method by saying \u201cThe method is novel and elegant\u201d.\n\n>Weakness (performance lower than the source model): \n\nThanks for this insight! We acknowledge that currently our transfer performance is lower than the source model, which may hinder its immediate practice. We added a limitation paragraph in the conclusion section discussing this. Nevertheless, our work has certain practical values, as LLMs tend to unify and it\u2019ll be handy if we may just use one LLM to solve various tasks. \n\nAs also recognized by the reviewer, our paper addresses an important research direction, and with increasingly powerful target LLMs and improved transfer techniques, we can expect that our research serves as a stepping stone towards more effective transfer methods. \n\n>Question: \u201cHow is the optimum of eqn 7 or 5 searched for exactly?\u201d\n\nThe search was conducted by gradient descent. In particular, we compute the gradient of the loss (either eqn 5 or eqn 7) wrt to the embeddings of the prompt tokens and update them accordingly. We\u2019ve clarified it in the revision.  \n\nIt\u2019s also worth mentioning that our code is publicly available (Footnote 1) to support replication.\n\nWe thank the reviewer again for the insightful comments and we\u2019ve revised our paper accordingly."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3795/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700098931246,
                "cdate": 1700098931246,
                "tmdate": 1700098931246,
                "mdate": 1700098931246,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QA76WIkU31",
                "forum": "26XphugOcS",
                "replyto": "wPqVFYtpN3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3795/Reviewer_2xK8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3795/Reviewer_2xK8"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the feedback. Overall, upon further reflection, revisions, and the additional results I have decided to increase my score to 8 from 6. The idea of unification is interesting and shows the potential of the overall idea of transfer although for immediate practical application, it's still not as clear why one would not just use a good source model for everything given the loss from the transfer. Nevertheless, I am willing to mostly overlook the weakness given many research directions (like neural machine translation) started out without being as immediately practical in the beginning compared to prior methods."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3795/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700220923296,
                "cdate": 1700220923296,
                "tmdate": 1700220923296,
                "mdate": 1700220923296,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FJLz4mJVE6",
            "forum": "26XphugOcS",
            "replyto": "26XphugOcS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3795/Reviewer_6csB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3795/Reviewer_6csB"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new route to Prompt-Tuning problems. Besides human-writing prompts and doing gradient back-propagation to learn continuous prompt, it brings the possibility of transferring an existing corpus of prompts for different models, regardless of the size of their embedding space. \n\nThe idea of the paper involves first translate one model's continuous prompt into a shared high-dimensional space, and search in that space for the target continuous prompt. The experiment shows that prompts are more or less transferable between BERT, RoBERTA, ALBERT, especially if you use dual source prompt transfer."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "As said by the paper, novelty is a big strength. It's quite unthinkable to transfer learned continuous prompt of one model to another model with different embedding size. And this paper shows that it's possible. \n\nThe writing of the paper is clear. \n\nThe introduction of the method including translation to shared embedding space, and then search for target continuous prompt makes sense. \n\nI appreciate the experiment design in Table-2, which includes random and manual, as two baselines, along with the learned prompt baseline. The manual baseline(human) is important because it tells me that even though transferred prompt isn't as good as the learned prompt, but it is still competitive with manual prompts. \n\nOverall, I find the idea to be novel, results to be solid (did not beat the learned prompt baseline), but gives overall good performance, compared with human baseline."
                },
                "weaknesses": {
                    "value": "Why is generative models not included for experiments?\n\nI understand the explanation for choosing a factual dataset for evaluation. But the result and the claim by the paper will be much strong if there are more than 1 dataset to support its claim."
                },
                "questions": {
                    "value": "Is BERT embedding transferrable to a GPT2 model?\nOr is a GPT2 model embedding transferable to a pythia model?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3795/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3795/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3795/Reviewer_6csB"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3795/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698777702812,
            "cdate": 1698777702812,
            "tmdate": 1699636336353,
            "mdate": 1699636336353,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HTYrg5tnHH",
                "forum": "26XphugOcS",
                "replyto": "FJLz4mJVE6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3795/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3795/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6csB"
                    },
                    "comment": {
                        "value": "Thank you for your strong support, especially for recognizing the novelty of our work.\n\n>Weakness 1: \u201cWhy is generative models not included for experiments?\u201d & Question: \u201cIs BERT embedding transferable to a GPT2 model? Or is a GPT2 model embedding transferable to a pythia model?\u201d\n\nThank you for the suggestion. We are also interested in the prompt transferability across different models other than the masked language models. We didn\u2019t include generative models because we followed the OptiPrompt paper by using the same settings as they did (same models and the same LAMA dataset). \n\nWe\u2019ve now experimented with generative models, and obtained some preliminary results:.\n| source \\ target | gpt2-small | gpt2-medium |\n|---|---|---|\n| direct tune   | 31.62      | 32.23       |\n| manual        | 4.62       | 8.98        |\n| gpt2-small    | -          | 13.72       |\n| gpt2-medium   | 5.02       | -           |\n| bert-base     | 10.46      | 11.52       |\n| roberta-base  | 14.06      | 13.70       |\n\nIn the experiment, we considered the small and medium versions of the gpt2 model. We found that our method is applicable to generative models as well. Moreover, the transfer from masked language models to generative models is also possible.\n\n>Weakness2: \u201cthe result and the claim by the paper will be much strong if there are more than 1 dataset\u201d\n\nThanks for the suggestion. Again, we followed previous work and used LAMA as the dataset, which contains 41 sub-tasks and provides a comprehensive evaluation for prompting transferability. \n\nWe did a preliminary experiment on two classification datasets: SST-2, a 2-way sentiment classification, and DBpedia, a 14-way topic classification. We tuned five soft prompt tokens on BERT and RoBERTa base models, and transferred them to the RoBERTa large model. The results are as follows.\n\nPerformance when transferring to RoBERTa-Large\n| source | SST- 2 (accuracy) | DBpedia (accuracy) |\n| --- | --- | --- |\n| manual | 69.95 | 72.28 |\n| bert-base | 82.45 | 77.05 |\n| roberta-base | 84.63 | 80.81 |\n\nWe see that the transferred performance is much higher than manual prompting, which shows the clue that our method can be adapted to other datasets and tasks.\n\nWe thank the reviewer again for the strong support and valuable suggestions. As our paper on soft prompt transfer is the first of its kind, we\u2019re willing to further pursue this direction in our future work."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3795/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700098780651,
                "cdate": 1700098780651,
                "tmdate": 1700098780651,
                "mdate": 1700098780651,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "d161pJh2vc",
                "forum": "26XphugOcS",
                "replyto": "HTYrg5tnHH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3795/Reviewer_6csB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3795/Reviewer_6csB"
                ],
                "content": {
                    "title": {
                        "value": "Good experiments"
                    },
                    "comment": {
                        "value": "Thank you for uploading new results. They addresses my concerns for the applicability of the method across different architecture."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3795/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700610923947,
                "cdate": 1700610923947,
                "tmdate": 1700610923947,
                "mdate": 1700610923947,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]