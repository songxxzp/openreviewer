[
    {
        "title": "Vision Transformer with Irregular Attention"
    },
    {
        "review": {
            "id": "Nun8fARWL9",
            "forum": "xVoj8AtyQ1",
            "replyto": "xVoj8AtyQ1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9363/Reviewer_ycW6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9363/Reviewer_ycW6"
            ],
            "content": {
                "summary": {
                    "value": "This paper mainly focuses on the compression of Multi-Head Self-Attention (MHSA) mechanism. This paper introduces a novel compressed structure for MHSA named Irregular Attention (IrrAtt). This structure is built upon the BTD-(L,L,1) and aims to sparsify pre-trained Vision Transformers by pruning the query and key (QK) contraction dimension in the MHSA block. This paper also presents an algorithm for rank selection based on the structure of the fusion layer derived from the CP decomposition of original MHSA kernels. The main goal is to achieve better compression ratios without compromising the quality of the model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Irregular Attention (IrrAtt) provides a new perspective on compressing the Multi-Head Self-Attention mechanism, especially for computer vision tasks. It is applicable to pretrained Transformer models, and holds substantial research value in the field of Machine Learning system and model deployment.\n2. The Vision Transformer can be significantly sparsified by using BTD-(L,L,1) tensor decomposition for constructing IrrAtt, which results in a more compact model. It theoretically holds certain feasibility, and is thoroughly discussed.\n3. The proposed rank selection algorithm, derived from the fusion layer structure, enables each attention head to have its optimal rank for the Query-Key (QK) contraction. \n4. The experiments have validated that the proposed method can achieve a good balance between performance maintenance and model compression."
                },
                "weaknesses": {
                    "value": "1. The paper presents a number of conceptual and technical difficulties, and the ambiguous explanations make it challenging for readers to understand the paper. The introduction of the methods is not detailed enough, making it hard to grasp the true contributions of the author.\n2. It may be necessary to carefully adjust the rank for the QK contraction in the IrrAtt for each attention head in order to achieve optimal performance.\n3. The proposed rank selection algorithm, derived from the fusion layer structure, may result in extra computing overhead, particularly when the model is being trained.\n4. Although validation on a single dataset can provide valuable insights for research, it's beneficial for the model's robustness and generalization to be validated on multiple datasets.\n5. The paper did not conduct ablation experiments on various modules of the method, such as initialization, making it difficult for me to judge its true effectiveness.\n6. There are several mistakes in the article, such as the MHSA formula in Equation 2. Please clarify or provide references if my understanding is incorrect. English abbreviations appearing for the first time in the paper should be followed by their full names or explanations."
                },
                "questions": {
                    "value": "1. Are the comparison methods TruncAtt and CP SlimAtt first introduced in this paper? If not, please indicate the reference. If they are, there is a lack of comparison with existing methods, making it difficult to evaluate the level achieved by the proposed method.\n2. How is the compression ratio controlled to reach the target compression ratio in this method? Are the significant discrepancies in the FLOPs and params CR values in Table 1 due to different target compression ratios set by the comparison methods?\n3. The current version leaves me doubting its reproducibility."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9363/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698759456873,
            "cdate": 1698759456873,
            "tmdate": 1699637177746,
            "mdate": 1699637177746,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "EAL5xGQKZG",
            "forum": "xVoj8AtyQ1",
            "replyto": "xVoj8AtyQ1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9363/Reviewer_v2e6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9363/Reviewer_v2e6"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a mechanism called Irregular Attention to compress the retrained Vision Transformer, which is built based on BTD-(L,L,1) tensor decomposition. The proposed method automatically determines the own rank of attention weight under the constraint of the total ranks of heads."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper proposes a method which has the same computation complexity as TruncAtt and performance similar to CP SlimAtt. \n\nThe paper provides quantitative results to prove the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "The paper is not very well written. For example, the abbreviation \"CP\" has made multiple prior appearances without prior explanation, only being clarified in Section 4.2, which may confuse the reader.\n\nThe paper evaluates the effectiveness of the proposed method based on the experimental results on DeiT and ILSVRC-2012 dataset, which is not comprehensive. The paper should conduct experiments on more datasets to have a more solid conclusion."
                },
                "questions": {
                    "value": "Please refer to the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9363/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9363/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9363/Reviewer_v2e6"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9363/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698821570633,
            "cdate": 1698821570633,
            "tmdate": 1699637177602,
            "mdate": 1699637177602,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "jNgTqUATiu",
            "forum": "xVoj8AtyQ1",
            "replyto": "xVoj8AtyQ1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9363/Reviewer_QrK6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9363/Reviewer_QrK6"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to compress Transformers. It proposes the Irregular attention build on top of BTD-(L,L,1). It sparsifies pre-trained vision transformers by pruning the query and key contract dimensions in the MHSA. A fine-tuning scheme is also introduced to improve the performance. The proposed irregular attention is validated for DeiT on ILSVRC-2012 dataset. Experiments show the better results of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The studied problem of compressing vision transformers is quite important for the community.\n\nThe idea of diversing the importance of multi-attention heads in transformers is reasonable.\n\nUsing BTD-(L,L,1) in this problem is new."
                },
                "weaknesses": {
                    "value": "The paper only evalutes on one dataset and one vision transformer. It needs more validations to support the paper's arguments.\n\nThe organization of experiments can be improved. Instead of presenting the results, it is better also to analyze the results and give the readers insights about the improvement if possible.\n\nIt seems that the paper did not compare to other compression methods. The methods compared in Table 1 are the preliminaries for the proposed one, however, many related works described in Section 5 are not compared.\n\nThe contributions listed in the end of Section 1 are not significant enough for an ICLR paper.\n\nThe importance of the obtained results and the derived method need to be further strengthed."
                },
                "questions": {
                    "value": "Is there any evidence to support the assumption of (5)?\n\nThe way of presenting Algorithm 1-3 should be revised to improve its readability.\n\nIt lacks an overview of the proposed irregular attention, and how it can be used in existing vision transformer architectures."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9363/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698822123432,
            "cdate": 1698822123432,
            "tmdate": 1699637177475,
            "mdate": 1699637177475,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "0ezSYRnryO",
            "forum": "xVoj8AtyQ1",
            "replyto": "xVoj8AtyQ1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9363/Reviewer_NApC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9363/Reviewer_NApC"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Irregular Attention (IrrAtt) for the compression of multi-head self-attention (MHSA) in vision transformers. IrrAtt is built on top of BTD-(L,L,1) tensor decomposition and is aimed at sparsifying pre-trained vision transformers by pruning the query and key contraction dimension in the MHSA block. The proposed IrrAtt is validated for the DeiT architecture on the ILSVRC-2012 dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The compression of vision transformers is an important research problem. This paper is well-motivated. The writing is professional and convincing."
                },
                "weaknesses": {
                    "value": "The comparison with existing methods is very limited, i.e., only three methods are compared. Considering that there has been a very large literature on the compression and efficient design of vision transformers, such a limited comparison cannot demonstrate the effectiveness of the proposed method.\n\nThe proposed IrrAtt is only validated for the DeiT architecture. Considering that there have been many popular transformer architectures such as Swin Transformer, PVT, and MViT, the only validation for DeiT cannot demonstrate the effectiveness of the proposed method.\n\nThere is no ablation study in this paper. This paper has many designs and components (Eq. (1) \u2013 Eq. (7), Alg. 1 - Alg. 3), and it is important and necessary to evaluate each of these designs and components. Recently, ablation study is also a necessary part of computer vision papers, especially for deep learning papers.\n\nWill the code be released? This is not mentioned in the paper. This is important to ensure the reproducibility of the paper."
                },
                "questions": {
                    "value": "Please see the above weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9363/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698828324800,
            "cdate": 1698828324800,
            "tmdate": 1699637177369,
            "mdate": 1699637177369,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]