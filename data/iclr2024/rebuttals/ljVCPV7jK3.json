[
    {
        "title": "Fairness Under Demographic Scarce Regime"
    },
    {
        "review": {
            "id": "Mvfbh5tIUN",
            "forum": "ljVCPV7jK3",
            "replyto": "ljVCPV7jK3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6268/Reviewer_CnjD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6268/Reviewer_CnjD"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a new solution for improving fairness under demographic scarce regime. It utilizes a teacher-student model to train an attribute classifier with uncertainty estimation first. Then trains a label classifier with only the training data whose sensitive attributes can be estimated by the attribute classifier with high confidence. Results show that the proposed approach achieves better than state-of-the-art accuracy-fairness trade-offs with three different fairness metrics on five real data sets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed technique is sound.\n\n2. The presentation is clear.\n\n3. The experiments are conducted on five real data sets.\n\n4. The source code is provided."
                },
                "weaknesses": {
                    "value": "1. The biggest concern I have is that the uncertainty threshold greatly impacts the performance of the proposed approach. It is not clear that if the authors have a reliable way to determine this threshold in practice (other than selecting the best performing one on the test data). Does tuning on a validation set reveal the best threshold for the data? Does it generalize to the test set? This should be either clarified or added as new experiments.\n\n2. Despite the fact that the scenario discussed in this paper is not entirely new, I would still suggest the authors provide a real example scenario when D1 has no demographic information but has label information and D2 has demographic information but no label information."
                },
                "questions": {
                    "value": "Q1: Results of \"Ours\" in Figure 2 were tuned for uncertainty threshold over [0.1, 0.7]. When you tune the threshold, are you using a validation set from the 0.7 training data or are you just selecting the best performing threshold on the 0.3 test data?\n\nQ2: In Section 3 Problem formulation, you mentioned: \" However, to be able to estimate bias in label classifier f, we assume there exists a small set of samples drawn from the joint distribution X \u00d7 Y \u00d7 A, i.e., samples that jointly have label and demographic information.\" How was this reflected in the experiments? Does it refer to the 0.3 test data you used in the experiments? If so, I do not think this is a hard requirement in real world applications.\n\nQ3: Please clarify: in Section 4.2, \"the label classifier with fairness constraints is trained on a subset D\u20321 \u2282 D1\". Does this mean the second phase classifier does train on any information except for D'1? Is that possible to train that label classifier on the entire training set D1 with fairness constraints on D'1--- fairness loss will be 0 for data do not belonging to D'1? This could potentially increase the accuracy of the label classifier when uncertainty threshold is low."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6268/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6268/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6268/Reviewer_CnjD"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6268/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697471611667,
            "cdate": 1697471611667,
            "tmdate": 1699636685897,
            "mdate": 1699636685897,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qm5hWsCEDg",
                "forum": "ljVCPV7jK3",
                "replyto": "Mvfbh5tIUN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6268/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6268/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer CnjD"
                    },
                    "comment": {
                        "value": "We thank the reviewer for detailed comments and constructive feedback. \n \n>  Despite the fact that the scenario discussed in this paper is not entirely new, I would still suggest the authors provide a real example scenario when D1 has no demographic [...]\n\nWe have added in the problem formulation references to real-world examples where this setup applies. In particular, they can be found in healthcare [1, 2] or finance [3, 4] where patients/users self-report their sensitive information.  \n\n> Q1: Results of \"Ours\" in Figure 2 were tuned for uncertainty threshold over [0.1, 0.7]. When you tune the threshold, are you using a validation set from the 0.7 training data or are you just selecting the best performing threshold on the 0.3 test data?\n\nThank you for bringing this up. The uncertainty threshold is a hyperparameter that can be tuned on the validation set. We used 10% of the training data for validation. We highlighted in the paper how we tuned the threshold. Furthermore, we observe the higher variation of the uncertainty threshold occurs in datasets with already higher uncertainty (e.g., LSAC and New Adult), from which models without fairness constraints provide lower bias. There are similar variations in the Pareto fronts over the validation set and the best-performing finetuned threshold also transfers to the test set.  \n\n>  Q2: In Section 3 Problem formulation, you mentioned: \" However, to be able to estimate bias in label classifier f, we assume there exists a small set of samples drawn from the joint distribution X \u00d7 Y \u00d7 A, i.e., samples that jointly have label and demographic information.\" How was this reflected in the experiments? Does it refer to the 0.3 test data you used in the experiments? If so, I do not think this is a hard requirement in real world applications.\n\nThe joint distribution X \u00d7 Y \u00d7 A refers to test data used in the experiments. This assumption allows us to report the true fairness violation. Moreover, we could alleviate this assumption by using proxy-sensitive attributes for the evaluation, which might be a more realistic setup. But these proxies are likely noisy and there is a risk of overestimating or underestimating the true disparities in the model. We have added a limitation section where we discuss the limitation of such assumption in real-world applications and point out the possibility of using of bias estimation approaches that effectively capture the true disparities when the proxy demographic information is used [5] [6].\n\n> Q3: Please clarify: in Section 4.2, \"the label classifier with fairness constraints is trained on a subset D\u20321 \u2282 D1\". Does this mean the second phase classifier does train on any information except for D'1? Is that possible to train that label classifier on the entire training set D1 with fairness constraints on D'1--- fairness loss will be 0 for data do not belonging to D'1? This could potentially increase the accuracy of the label classifier when uncertainty threshold is low.\n\nGreat suggestion. The main objective of the paper is to provide evidence for the hypothesis we formulated. Modifying fairness-enhancing algorithms to account for the uncertainty is an interesting direction to explore. We have mentioned in the conclusion a similar perspective for future work where the fairness-enhancing model is trained on samples weighted by the uncertainty of predicting their sensitive attributes. \nPlease note that all the changes in the revised version of the paper have been highlighted in blue.\n\nWe hope we have addressed most of your concerns. Please consider increasing the score if you find our responses satisfactory. We would be happy to answer any further questions.\n\n[1] Brown, D. P., Knapp, C., Baker, K., & Kaufmann, M. (2016). Using Bayesian imputation to assess racial and ethnic disparities in pediatric performance measures. Health services research, 51(3), 1095-1108.\n\n[2] Fremont, A. M., Bierman, A., Wickstrom, S. L., Bird, C. E., Shah, M., Escarce, J. J., ... & Rector, T. (2005). Use of geocoding in managed care settings to identify quality disparities. Health Affairs, 24(2), 516-526\n\n[3] Zhang, Y. (2018). Assessing fair lending risks using race/ethnicity proxies. Management Science, 64(1), 178-197.\n\n[4] Silva, G. C., Trivedi, A. N., & Gutman, R. (2019). Developing and evaluating methods to impute race/ethnicity in an incomplete dataset. Health Services and Outcomes Research Methodology, 19(2-3), 175-195.\n\n[5] Chen, J., Kallus, N., Mao, X., Svacha, G., & Udell, M. (2019, January). Fairness under unawareness: Assessing disparity when protected class is unobserved. In Proceedings of the conference on fairness, accountability, and transparency\n\n[6]   Awasthi, P., Beutel, A., Kleindessner, M., Morgenstern, J., & Wang, X. (2021, March). Evaluating fairness of machine learning models under uncertain and incomplete information. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6268/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700164382723,
                "cdate": 1700164382723,
                "tmdate": 1700164382723,
                "mdate": 1700164382723,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BrwCwfRHnX",
            "forum": "ljVCPV7jK3",
            "replyto": "ljVCPV7jK3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6268/Reviewer_EiNj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6268/Reviewer_EiNj"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce a framework designed to facilitate the training of fairness-enhancing interventions when sensitive information is only partially observed. Their approach involves developing a classifier that seeks to predict the sensitive attributes of instances. Subsequently, they leverage instances with the least uncertain predictions, along with their predicted sensitive attributes, to train the fairness-enhancing intervention."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "S1 - The authors present a solution to a significant challenge that fairness-enhancing interventions may encounter when implemented in real-world applications.\n\nS2 - The experiments exhibit several strengths, including the diverse range of classification tasks involving different datasets, the validation of various aspects of the work. Particularly noteworthy are the investigations into the relationship between the threshold and the encoding of sensitive information by features, as well as the analysis of uncertainty in the sensitive attribute and the impact on the fairness of non-fairness-aware classifiers. Overall, the experiments provide supporting evidence for their central hypothesis.\n\nS3 - The paper's commitment to reproducibility is highly commendable. The detailed and transparent presentation of the experimental setup, data sources, and code availability significantly enhances the reliability and trustworthiness of the research findings. This transparency not only promotes the understanding of the study but also encourages further research in the field. \n\nS4 - The paper's writing is remarkably clear, making it easy for readers to grasp the content. Furthermore, the well-structured sections and the logical flow of information make it easy for readers to follow the research from start to finish.\n\nS5 - The paper effectively incorporates citations of pertinent related works, which helps contextualize their approach within the existing literature."
                },
                "weaknesses": {
                    "value": "W1 - I believe there's a significant ethical concern in constructing a classifier with the objective of predicting the sensitive attribute of instances. This practice may raise legal and ethical issues, especially when individuals choose not to disclose this information willingly. Instead, it would be preferable if this classifier incorporated desirable privacy properties, as outlined in Diana et al. (2022).\n\nW2 - I find the comparison with respect to the state of the art to be lacking. The attribute classifiers chosen in this work, such as Proxy-kNN and Proxy-DNN, are rather simplistic and not well-documented in existing literature. Moreover, there are established attribute classifiers like those introduced by Diana et al. (2022) and Awasthi et al. (2021) that are not considered in this comparison.\nFurthermore, the selected methods for the 'baselines' (Lahoti et al., 2020; Hashimoto et al., 2018; Levy et al., 2020; Yan et al., 2020; Zhao et al., 2022) assume that they lack access to the sensitive attribute, making the experimental setting fundamentally different. Therefore, comparing the proposed approach with these state-of-the-art methods that operate under distinct conditions may not provide a fair assessment of its improvements and contributions to the field.\nTo offer a more comprehensive evaluation of the proposed method and better understand its advancements over existing techniques, I suggest including experiments involving Diana et al. (2022) and Awasthi et al. (2021).\n\nW3 - The authors assert in the abstract that 'our framework outperforms models trained with constraints on the true sensitive attribute,' referring to the results from Figure 2. However, this result only considers a single fairness-enhancing intervention. Additionally, their framework does not consistently outperform models trained with ground truth sensitive attribute values in all cases, making the statement partially true. This discrepancy is even more apparent in the results from Figure 9, where a different fairness-enhancing intervention is employed. It's unclear to what extent this outcome is influenced by the chosen fairness-enhancing intervention. Both interventions analyzed in the study share similarities, and it would be beneficial to examine a more diverse set of fairness-enhancing interventions to better understand the impact of the chosen intervention on the results. Therefore, I recommend that the authors modify the statement to emphasize that the framework outperforms models trained with constraints on the true sensitive attribute in some cases, and I encourage them to delve into the conditions under which this outperformance occurs.\n\nW4 - I believe that the related works section should also encompass privacy-related research. There are privacy-focused approaches, such as cryptographic solutions, that deal with situations where sensitive features are available but can only be accessed through secure cryptographic methods. Veale and Binns [1] or Kilbertus et al. [2] discuss scenarios where individuals' sensitive information is held by a third party or the individuals themselves, respectively, and can only be accessed via secure multiparty computation. Additionally, Jagielski et al. [3] explores cases in which sensitive features can only be utilized in a differentially private manner. Considering the nature of inferring sensitive information, privacy considerations become crucial. Therefore, it would be valuable to include these privacy-focused works in the related literature to provide a more comprehensive perspective.\n\n[1] Veale, M., & Binns, R. (2017). Fairer machine learning in the real world: Mitigating discrimination without collecting sensitive data. Big Data & Society, 4(2), 2053951717743530.\n\n[2] Kilbertus, N., Gasc\u00f3n, A., Kusner, M., Veale, M., Gummadi, K., & Weller, A. (2018, July). Blind justice: Fairness with encrypted sensitive attributes. In International Conference on Machine Learning (pp. 2630-2639). PMLR.\n\n[3] Jagielski, M., Kearns, M., Mao, J., Oprea, A., Roth, A., Sharifi-Malvajerdi, S., & Ullman, J. (2019, May). Differentially private fair learning. In International Conference on Machine Learning (pp. 3000-3008). PMLR."
                },
                "questions": {
                    "value": "Q1 - The experiments provide support for the assertion that discriminating against samples with more uncertain sensitive information is a challenging task. Rather than attempting to predict the sensitive information of instances (an action that is illegal and morally questionable) and use those instances for which you know the sensitive information with high confidence, why not directly utilize those instances for which the uncertainty is highest with respect to the sensitive attribute and train a non fairness-aware classifier on top of those instances? In other words, perhaps utilizing your attribute classifier to identify the most 'fair' samples based on high uncertainty in sensitive information might yield more ethically favourable results.\n\nQ2 - For classifiers that propose fairness-enhancing interventions while lacking information on the specific sensitive attribute considered in the experimental section (Lahoti et al., 2020; Hashimoto et al., 2018; Levy et al., 2020; Yan et al., 2020; Zhao et al., 2022), it's essential to clarify the dataset utilized. Do you feed them with the complete D1, D1 + D2, or only D1'?\n\nQ3 - The experiments demonstrate significant variations in results depending on the chosen uncertainty threshold. If this model were to be applied in a real-world scenario, do you have a practical method for selecting the optimal uncertainty threshold, rather than relying on trial and error to determine the best-performing value?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6268/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6268/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6268/Reviewer_EiNj"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6268/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698665154102,
            "cdate": 1698665154102,
            "tmdate": 1699636685780,
            "mdate": 1699636685780,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lACHegjBkz",
                "forum": "ljVCPV7jK3",
                "replyto": "BrwCwfRHnX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6268/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6268/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer EiNj - Part 1"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comments and feedback. Below we provide clarifications for the concerns raised. \n\n> W1 I believe there's a significant ethical concern in constructing a classifier with the objective of predicting [\u2026] Instead, it would be preferable if this classifier incorporated desirable privacy properties, as outlined in Diana et al. (2022)\n\nWe acknowledge the ethical concern in inferring sensitive attributes. This represents a general concern for the group of methods relying on proxy-sensitive information. We believe the prediction of sensitive attributes with the objective of mitigating discrimination in high-stakes decision-making scenarios could be ethically acceptable. The inference of the sensitive attributes\u2014using our proposed method or others, should not be used for a purpose different from bias assessment and mitigation.  Our method provides insights into the benefit of uncertainty-awareness in predicting sensitive attributes for fairness of the downstream classifiers. Furthermore, the ethical concern in inferring sensitive attributes comes from the dilemma posed between privacy and fairness. In particular, laws or regulations enforce discrimination-free decision-making while they also prohibit the use or collection of demographic information, which is necessary for auditing and mitigating discrimination. While privacy guarantees for the sensitive attributes are out of the scope of the paper incorporating privacy aspects is a suggestion that we welcome. Moreover, methods designed under a privacy-preservation setup generally do not guarantee that an adversary cannot reconstruct the sensitive attributes, especially for methods relying on trusted third parties or secure multi-party computation.  For example,  [1] shows that one can exploit information about a fair model to reconstruct the sensitive attributes, even with black box access.  We have added a limitation section in the revised version (please see Appendix A) where we discuss ethical concerns in inferring sensitive information.\n\n> W2 I find the comparison with respect to the state of the art to be lacking. The attribute classifiers chosen in this work, such as Proxy-kNN and Proxy-DNN, are rather simplistic and not well-documented in existing literature [...]\n\nSome of the baselines considered also use proxy-sensitive attributes (Zhao et al., 2022; Liang et al., 2023; Yan et al., 2020;), and methods without relying on sensitive attributes such as ARL (Lahoti et al., 2020) can also improve group metrics such as equalized odds. Other baselines are considered for a comprehensive evaluation and we highlighted that the experimental setup might be different, in particular for methods targeting worst-case group improvement. \nWe couldn\u2019t perform comparisons with Diana et al, (2022) as the authors did not publish their code and we couldn\u2019t reproduce their results within the short rebuttal period. Moreover, Awasthi et al. (2021) focused on postprocessing techniques for equalized odds, which generally highly impact accuracy,  while we considered inprocessing techniques with controllable fairness-accuracy tradeoffs. Furthermore, the results reported using true sensitive attributes represent the optimal expected baseline for fairness, and comparison shows our method can achieve similar and in most cases better tradeoffs. It does not seem to be the case for Awasthi et al. (2021) and Diana et al, (2022) on the same datasets. \n\n> W3 The authors assert in the abstract that 'our framework outperforms models trained with constraints on the true sensitive attribute,' referring to the results from Figure 2. However, this result only considers a single [...]\n\nThank you for the suggestions. We have updated the statement accordingly and included privacy-related approaches in the related work section in the revised version.  We have also highlighted in the limitation section and emphasized the need for further studies to explore whether our method and hypothesis extend to other fairness-enhancing methods, e.g., preprocessing or post-processing techniques. \n\n[1] Ferry, J., A\u00efvodji, U., Gambs, S., Huguet, M. J., & Siala, M. (2023, February). Exploiting Fairness to Enhance Sensitive Attributes Reconstruction. In 2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6268/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700154061141,
                "cdate": 1700154061141,
                "tmdate": 1700154122336,
                "mdate": 1700154122336,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "toyOTR4R9l",
                "forum": "ljVCPV7jK3",
                "replyto": "BrwCwfRHnX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6268/Reviewer_EiNj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6268/Reviewer_EiNj"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "I appreciate your response addressing my concerns.\n\nLinked to W1, I acknowledge that legal regulations mandate discrimination-free decision-making while simultaneously prohibiting the use or collection of demographic information. However, I find this justification insufficient for supporting a practice that is both illegal and ethically dubious, involving the inference of sensitive information from individuals who have chosen not to disclose such details. Particularly considering the wealth of contributions in the field of differential privacy, it seems plausible to explore alternative approaches. I concur with reviewer PEk1 that the current approach appears somewhat basic, and the incorporation of privacy-preserving techniques could significantly enhance the paper's potential.\n\nConcerning W2, I believe the comparison of the proposed method against state-of-the-art methods should be prominently featured in the main text to underscore the advantages of using this approach over existing methods. This emphasis is especially crucial given the reported results indicating a significant outperformance of the proposed method compared to the considered state-of-the-art approaches. Consequently, the comparisons involving Proxy-kNN and Proxy-DNN could be relegated to the appendix.\n\nFurthermore, I'd like to take this opportunity to provide feedback on a couple of potential improvements in Tables from Appendix D and E. For instance, in Tables 3 and 4, Vanilla emerges as the most accurate method, but it is not highlighted in bold; instead, ARL and your method are highlighted. Additionally, it would be beneficial to maintain consistency in the number of decimals reported in the results. In Tables 5 and 6, most numbers have 3 decimals, but a few have 4 decimals."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6268/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700224820284,
                "cdate": 1700224820284,
                "tmdate": 1700225005958,
                "mdate": 1700225005958,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qFyr9TD9pX",
                "forum": "ljVCPV7jK3",
                "replyto": "ILdvCeIhgR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6268/Reviewer_EiNj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6268/Reviewer_EiNj"
                ],
                "content": {
                    "title": {
                        "value": "Reply to authors answer"
                    },
                    "comment": {
                        "value": "Thank you once again for addressing my concerns.\n\nI still have a couple of points to discuss:\n\n(a) While Figure 3 in the Appendix is informative, it would be more insightful if it included additional baseline methods mentioned in the experimental section (e.g., FairDA, DRO).\n\n(b) Based on your results, training the classifier on instances with high uncertainty in sensitive information yields excellent outcomes (Table 6). However, a more comprehensive comparison of both methods is lacking. For Adult, COMPAS, and LSAC, you apply the approach involving instances with certain sensitive information, while evaluating the CelebA classification tasks with the method that considers instances with uncertain sensitive information. It would be beneficial to assess both approaches for all four classification tasks in Tables 1, 2, 3, and 6 as OURS(certain) and OURS(uncertain). This way, if OURS(uncertain) performs comparably to OURS(certain) and surpasses all baselines, it could offer a fair method without the need to infer sensitive information, avoiding any illegal or morally questionable tasks. Additionally, exploring the conditions under which one method might outperform the other would provide valuable insights.\n\n(c) I concur with Reviewer PEk1 that it seems unusual for the proposed method to outperform all baselines by such a significant margin. Providing convincing reasons for this observed performance would be appreciated.\n\n(d) In Table 6, there is still a need to address the decimal problem in the row for 'ours' in the first column."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6268/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700554987930,
                "cdate": 1700554987930,
                "tmdate": 1700554987930,
                "mdate": 1700554987930,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "30ILud6qaP",
                "forum": "ljVCPV7jK3",
                "replyto": "BrwCwfRHnX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6268/Reviewer_EiNj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6268/Reviewer_EiNj"
                ],
                "content": {
                    "title": {
                        "value": "Additional question on results"
                    },
                    "comment": {
                        "value": "I appreciate your additional clarifications regarding Figure 3 and the superiority of your results, and thank you for rectifying my misunderstanding on Table 6. Furthermore, I find it noteworthy that Ours(uncertain) yields promising results.\n\nI have an additional question: You assert that the superior performance of your results might be attributed to the utilization of a highly potent fairness-enhancing intervention (exponentiated gradient). Nevertheless, the remaining baselines employ logistic regression as their foundational classifier, a relatively simplistic approach that may not be the most suitable choice for these classification tasks. I'm inclined to think that this setup doesn't facilitate a truly fair comparison. Have you explored the use of other base classifiers such as SVM or even some straightforward neural networks?"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6268/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639987791,
                "cdate": 1700639987791,
                "tmdate": 1700640567198,
                "mdate": 1700640567198,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RaJB48ZQcd",
                "forum": "ljVCPV7jK3",
                "replyto": "BrwCwfRHnX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6268/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6268/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Additional results"
                    },
                    "comment": {
                        "value": "Dear Reviewer EiNj,\n\nThank you for your time and constructive comments. We hope our last response provided a clarification to your question.\nAs per your suggestion, we explored the use of a different base classifier such as a feedforward network. We could not consider SVM as all of the methods considered for comparison do not support SVM.\nWe trained all the baselines using a multi-layer perception of one hidden layer with 32 units. We evaluated each method on the Adult dataset under the same setup to analyze its impact on the comparison. The results on other datasets are not provided due to the limited time and computational resources. The newly added Table 7 in the appendix shows that our method can still outperform other baselines under a more complex non-linear base classifier. We observed an increase in the accuracy of other baselines due to the increased capacity of the neural network, while our method still provides a significant gap in fairness.\n\nWe will be happy to address any remaining concerns within the remaining short time for discussion. We would like to ask you to kindly consider adjusting your score if you find that our discussion addressed your concern."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6268/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740801700,
                "cdate": 1700740801700,
                "tmdate": 1700740836928,
                "mdate": 1700740836928,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bIGooID5Gu",
            "forum": "ljVCPV7jK3",
            "replyto": "ljVCPV7jK3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6268/Reviewer_PEk1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6268/Reviewer_PEk1"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies how to achieve a better fairness-accuracy tradeoff when no access to full sensitive attributes in the dataset. The method has two steps: (1) training a proxy classifier to predict the missing sensitive attributes with a student-teacher distillation and (2) thresholding the confidence on predictions. The paper evaluates the method on common fairness benchmark datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper targets an important problem. Given the increasingly stringent privacy constraint, the problem of studying fairness without full access to sensitive attributes is an important problem."
                },
                "weaknesses": {
                    "value": "I have two major concerns.\n\n(1) If I am not mistaken, it seems the technical contribution of the paper is limited. The first step is not far from merely training a classifier to predict sensitive attributes, which is usually treated as a baseline in this area, with a little enhancement of student-teacher transfer learning. Overall, I do not see significant technical novelty. The second step is just to filter by thresholding prediction confidence. I have a hard time finding the technical contributions of the paper.\n\n(2) In experiments, the paper only compares to the basic bias mitigation algorithm, but there is literature of fairness with not full access to the sensitive attributes:\n\n[1] Diana, Emily, et al. \"Multiaccurate proxies for downstream fairness.\" Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency. 2022.\n\n[2] Chen, Jiahao, et al. \"Fairness under unawareness: Assessing disparity when protected class is unobserved.\" Proceedings of the conference on fairness, accountability, and transparency. 2019.\n\n[3] Prost, Flavien, et al. \"Measuring model fairness under noisy covariates: A theoretical perspective.\" Proceedings of the 2021 AAAI/ACM Conference on AI, Ethic\n\n[4] Fogliato, Riccardo, Alexandra Chouldechova, and Max G\u2019Sell. \"Fairness evaluation in presence of biased noisy labels.\" International conference on artificial intelligence and statistics. PMLR, 2020.\n\n[5] Zhu, Zhaowei, et al. \"Weak Proxies are Sufficient and Preferable for Fairness with Missing Sensitive Attributes.\" International Conference on Machine Learning, 2023.\n\n[6] Yan, Shen, Hsien-te Kao, and Emilio Ferrara. \"Fair class balancing: Enhancing model fairness without observing sensitive attributes.\" Proceedings of the 29th ACM International Conference on Information & Knowledge Management. 2020.\n\nI do not see why this paper should not be compared with any of those works."
                },
                "questions": {
                    "value": "1. Can authors clarify if there is anything I misunderstood about the technical contribution? Note that there is no point in merely repeating the details of the method. The constructive communication would be to point out if I am wrong when I say the method is just training a proxy classifier with transfer learning and thresholding predictions.\n\n2. Can authors explain the reason why no comparison to any of the methods in the literature of fairness without full access to sensitive attributes?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6268/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698713150322,
            "cdate": 1698713150322,
            "tmdate": 1699636685659,
            "mdate": 1699636685659,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b1GTVTsNXd",
                "forum": "ljVCPV7jK3",
                "replyto": "bIGooID5Gu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6268/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6268/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer PEk1"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the feedback on our work. Below we address the concerns. \n\n> Can authors clarify if there is anything I misunderstood about the technical contribution? Note that there is no point in merely repeating the details of the method. The constructive communication would be to point out if I am wrong when I say the method is just training a proxy classifier with transfer learning and thresholding predictions.\n\nWe apologize for the confusion. Our contribution is indeed more than just training a proxy classifier with transfer learning and thresholding. Our proxy classifier is trained with uncertainty awareness and not transfer learning. The threshold is later applied on the uncertainty of predictions and not on the predictions themselves. The teacher network has the same architecture as the student network and the teacher\u2019s weights are updated using the moving average of the student\u2019s weights. We use Monte-Carlo Dropout over the teacher for uncertainty estimation and the consistency loss to enforce the classifier to focus on samples with low uncertainty. \n\nTo the best of our knowledge, our proposed method is the first to draw connections between the uncertainty in sensitive attribute predictions and fairness. We have highlighted our contribution in the revised version of the draft.\n\n> Can authors explain the reason why no comparison to any of the methods in the literature of fairness without full access to sensitive attributes?.\n\nWe have indeed performed comparisons with six other baselines. In the experiment section, we mentioned the comparison with other baselines addressing fairness issues in a similar setup and we referred the reader to the appendix for results. We performed comparisons with the following baselines:\n\n[1] Yan, S., Kao, H. T., & Ferrara, E. (2020, October). Fair class balancing: Enhancing model fairness without observing sensitive attributes. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management\n\n[2] Zhao, T., Dai, E., Shu, K., & Wang, S. (2022, February). Towards fair classifiers without sensitive attributes: Exploring biases in related features. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining\n\n[3] Liang, Y., Chen, C., Tian, T., & Shu, K. (2023). Fair classification via domain adaptation: A dual adversarial learning approach. Frontiers in Big Data, 5, 129.\n\n[4] Lahoti, P., Beutel, A., Chen, J., Lee, K., Prost, F., Thain, N., ... & Chi, E. (2020). Fairness without demographics through adversarially reweighted learning. Advances in neural information processing systems\n\n[5] Hashimoto, T., Srivastava, M., Namkoong, H., & Liang, P. (2018, July). Fairness without demographics in repeated loss minimization. In International Conference on Machine Learning\n\n[6] Levy, D., Carmon, Y., Duchi, J. C., & Sidford, A. (2020). Large-scale methods for distributionally robust optimization. Advances in Neural Information Processing Systems\n\nThese comparisons are presented in the appendix due to the page limit and the main objective of the proposed method, which is to demonstrate the utility of uncertainty awareness in the proxy classifier in improving fairness-accuracy tradeoffs in the downstream classifiers. The results presented in the appendix (Tables 3-6) also show the effectiveness of the method in providing better fairness-accuracy tradeoffs compared to the considered state-of-the-art baselines. Moreover, our evaluations are performed over a testing set with the true sensitive attributes, i.e., we report the true fairness violation. We do not aim to improve bias estimation when proxy attributes are used which is why some of the baselines [2] [3] [4]  proposed by the reviewer were not considered for comparison. \n\nWe hope our response clarifies the reviewer\u2019s concerns and we will be happy to provide further clarifications where needed."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6268/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700066647176,
                "cdate": 1700066647176,
                "tmdate": 1700066647176,
                "mdate": 1700066647176,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WG7SytWB0M",
                "forum": "ljVCPV7jK3",
                "replyto": "b1GTVTsNXd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6268/Reviewer_PEk1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6268/Reviewer_PEk1"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Rebuttal"
                    },
                    "comment": {
                        "value": "I thank the authors for the rebuttal.\n\nRegarding technical novelty, unfortunately, the rebuttal does not seem convincing enough to me although the authors disagreed and mentioned it in the response to Reviewer EiNj. First, the authors stress that this is the first work that \"draw connections between the uncertainty in sensitive attribute predictions and fairness.\" I think it depends on what is meant by \"uncertainty.\" Technically speaking, when you do not have sensitive attributes and have to estimate them, any estimation would not be 100% certain and therefore one can argue any method that tries to predict the missing sensitive attribute has the flavor of uncertainty in it. If the word uncertain means in its literal sense, I am afraid this claim is too broad. Any method that tries to solve fairness problems with incomplete information and therefore has a probabilistic modeling of sensitive attributes can be considered as an uncertainty-based method, e.g. [1]. Hence it would be an overclaim. If the word uncertainty has a more technical meaning, I do not see any dedicated uncertainty method, e.g. conformal prediction, used in the paper.\n\nOverall, I still tend to think the method to be basic: training a better sensitive attribute proxy with some heuristic, and then threshold the confidence. The authors stressed in the rebuttal that thresholding is based on uncertainty estimation, but if I understand it correctly, what it does is nothing more than thresholding on the model's predicted confidence. Please correct me if I am wrong.\n\nRegarding the baseline comparison, I agree with Reviewer EiNj that the comparison to the baselines when the sensitive attribute is missing should be put into the main text because comparing to the methods that assume full sensitive attribute is not apple-to-apple and therefore meaningless. I also agree with Reviewer EiNj that \"the comparisons involving Proxy-kNN and Proxy-DNN could be relegated to the appendix.\"\n\nIn terms of Table 6 in the Appendix, it seems strange that the proposed method can beat all baselines with such a big margin: In DP, the best baseline is 0.02 while the proposed method is 0.003; in EOP numbers are 0.11 vs. 0.001; in EO the numbers are 0.129 vs. 0.007. It indicates the proposed method is over 20x better than the best baseline. I am suspicious that the method can be so effective. Please let me know if you have any convincing reasons.\n\n[1] Awasthi, Pranjal, et al. \"Evaluating fairness of machine learning models under uncertain and incomplete information.\" Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. 2021."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6268/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700523573861,
                "cdate": 1700523573861,
                "tmdate": 1700523573861,
                "mdate": 1700523573861,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "frjQzUgSLq",
                "forum": "ljVCPV7jK3",
                "replyto": "ukw1kyA0ow",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6268/Reviewer_PEk1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6268/Reviewer_PEk1"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the response"
                    },
                    "comment": {
                        "value": "I thank the authors for replying and reorganizing the experimental section. The discussion will be taken into consideration."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6268/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691444926,
                "cdate": 1700691444926,
                "tmdate": 1700691444926,
                "mdate": 1700691444926,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KsrAslkCaX",
                "forum": "ljVCPV7jK3",
                "replyto": "bIGooID5Gu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6268/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6268/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "Dear Reviewer PEk1,\n\nWe Thank you for your time and for the constructive discussion, which has helped us to improve the manuscript. We have integrated all the suggestions you requested throughout our discussion and we will be happy to provide further clarifications and improvement where needed within the remaining rebuttal time. In case we have addressed all the concerns, we would like to request if you can adjust the score accordingly."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6268/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700731167373,
                "cdate": 1700731167373,
                "tmdate": 1700731185157,
                "mdate": 1700731185157,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]