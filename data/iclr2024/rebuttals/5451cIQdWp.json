[
    {
        "title": "On Synthetic Data and Iterative Magnitude Pruning: a Linear Mode Connectivity Study"
    },
    {
        "review": {
            "id": "7MRnwbkSWV",
            "forum": "5451cIQdWp",
            "replyto": "5451cIQdWp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9121/Reviewer_6HLj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9121/Reviewer_6HLj"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the effects of pruning on distilled datasets (the synthetic data created by dataset distillation) and compares it with Iterative Magnitude Pruning (IMP). The authors find that pruning on distilled datasets has a higher efficiency, causes flatter landscapes, and has better linear mode connectivity. They also interpret the phenomena via the information bottleneck perspective."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The information bottleneck perspective on the distilled dataset is interesting and novel.\n* The findings that the pruned networks on the distilled dataset have flatter landscapes and better linear mode connectivity are new and insightful.\n* The experiments are somewhat solid."
                },
                "weaknesses": {
                    "value": "* **Presentation and structure.** I think this paper may have poor presentation and language. I recommend the authors to polish the paper and reorganize the structure. For example, the information bottleneck part seems strange to me at first glance. Maybe the authors can elaborate more on how information bottleneck is related to the main findings and claims and conduct a more fluent transition between (sub)sections.\n* **Novelty.** Using the distilled dataset for better efficiency in pruning has already been proposed in previous literature [1], which weakens the novelty and contribution of the proposed method. Therefore, the efficiency cannot be a main claim. And I think the whole novelty of this paper is weak.\n* **Lack of further evidence.** Information bottleneck is a good perspective to understand the distilled datasets, but I think the paper lacks further evidence on how information bottleneck is related to the findings. Concretely, how the information bottleneck is quantized? I think the loss landscape is not a direct aspect to show the point. More direct and intuitive evidence is needed.\n* **Lack of implications to practices.** Knowing the fact that pruning on distilled data is not new in this paper, the author should provide more insights on how the findings can guide applications and practices.\n\n------\n[1] McDermott L, Cummings D. Distilled Pruning: Using Synthetic Data to Win the Lottery[J]. arXiv preprint arXiv:2307.03364, 2023."
                },
                "questions": {
                    "value": "* The used models are only ResNets and ConvNets. I am interested in the results regarding more model architectures, specifically, Transformer is of particular interest, and other architectures, such as MLPs, VGGs, and MobileNets, are also needed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9121/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698245740174,
            "cdate": 1698245740174,
            "tmdate": 1699637147917,
            "mdate": 1699637147917,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KhxAjJqJEB",
                "forum": "5451cIQdWp",
                "replyto": "7MRnwbkSWV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9121/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9121/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 6HLj"
                    },
                    "comment": {
                        "value": "Weaknesses on Presentation / Structure: I agree. I have revised my language and removed all connection to the IB framework. I will push this idea to future work, as it deservers extensive experiments to back up this connection.\n\nWeakeness on Novelty: \n\nI made a mistake introducing the paper \u201cDistilled Pruning: Using Synthetic Data to Win the Lottery\u201d by McDermott & Cummings as this work is a nonarchival extended abstract, not a full paper. I now have removed all reference of this paper in my submission, and I hope this submission will be treated as if this algorithm, or even the idea to use distilled data with pruning, is introduced for the very first time. I request that you do not look into the authors identity for further reviewing. Hopefully, this will improve the novelty of this paper, as there are no \u201cpublished\u201d works that explore this idea.\n\nWeaknesses on further evidence:\nSee above, I agree and have removed this.\n\nWeaknesses on lack of implications:\n\nTo address the concerns for practical implications, I have added a new section that discusses what you can do with these synthetic subnetworks. I have shown with experiments that you can use synthetic subnetworks as a stable initialization for IMP, finding Lottery Tickets at higher sparsity. Previously, I only mentioned this in the body of the paper. I originally did not include this information in the paper as I aimed to focus more on the why the stability is happening, drawing parallels to the IB framework. It appears this paper has been reviewed less as a study and more as pruning algorithm work, so this new section seems more fitting.\n\nQuestions:\nI completely agree that we need to see this on transformer architectures; however, because dataset distillation is so new, they cannot yet extend to compute-intensive transformer models. For the others, yes, there is always some additional architectures / datasets to include.\n\nThank you for your feedback, I appreciate your concerns and have adjusted the paper accordingly. For novelty, I am not at liberty to go into the specifics; however, I please ask that you give an additional rating that assumes this work is novel and has never been published before. I realize after these reviews, I should not have included the original extended abstract on Distilled Pruning. I hope without the novelty concerns, your rating will significantly increase. I have also provided a separate section to address your points of lack of implications, see section 5."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9121/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684840424,
                "cdate": 1700684840424,
                "tmdate": 1700684840424,
                "mdate": 1700684840424,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jBb1nLLA54",
            "forum": "5451cIQdWp",
            "replyto": "5451cIQdWp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9121/Reviewer_rRcD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9121/Reviewer_rRcD"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates the effect of using distilled datasets for training deep neural networks in terms of the linear mode connectivity of pruned models (analogously to experiments by Frankle et al. with iterative pruning). Empirical investigation shows that sparsity masks found with distilled (synthetic) data are approximately as good as the ones found with standard IMP, but at the same moment they demonstrate more stability to SGD noise in terms of linear mode connectivity, i.e., the barrier between \"synthetic\" subnetworks is smaller than one between standard IMP subnetworks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The idea to understand the effect of the synthetic data on the training loss surface is interesting. It can shed light on the ways data affects the optimization landscape. It also has a benefit of a faster pruning process, since training on less data is (hopefully) faster."
                },
                "weaknesses": {
                    "value": "While the core idea of the investigation is interesting, the overall contribution of the paper is questionable. The largest part of the paper is dedicated to the description of the process of data distillation, while that is not the contribution of the paper (existing distilled datasets are used for the experiments). \n\nThe discussion about information bottleneck is largely misplaced in this work. The initial idea of IB is that a deep learning model implicitly tries to learn a representation that keeps maximal possible information about targets and minimal possible information about inputs, thus forming a bottleneck. It is still not proven that such compression is needed for generalization and that deep neural networks indeed perform it. Moreover, a natural conjecture about using a data that is already optimized in terms of information is that the model (neural network in this case) does not have to learn it anymore. Meaning, that from the initialization its task is simplified significantly, it basically does not have to form a bottleneck in itself. The empirical evidence in this paper is supporting this conjecture directly (especially with the observation that for more complex datasets there is no effect of smoothing barriers). I see the analysis of direct differences between sparsity masks induced by distilled data and natural data and analysis of them as an important experiment not performed in this paper. As well as the exhaustive comparison between the performance of synthetic masks should be one of the central and most discussed results, but it is reduced to one diagram in the paper. \n\nMinor:\n\n- please make use of \\citep and \\citet to distinguish citations that are not part of the sentence and part of it correspondingly\n\n- in section4 there is a mention of \"architectural relationship of the data\" - it is completely unclear what does this term mean\n\n- diagram in Fig.7 is very hard to understand. Why only 60 and 87 sparsity are chosen? Why the barrier is always same for IMP setup? What is performance ratio? Why IMP baseline is only one no matter that there are several setups?\n\n- I think the first sentence of Discussion does not belong to the text\n\n- the flatness investigation is left out to the appendix, nevertheless it is mentioned as one of the conclusions for the paper. There is no clear connection in general in the existing research between LMC and flatness, so the conclusions are inaccurate."
                },
                "questions": {
                    "value": "1 - What is the core goal of the research performed in the paper?\n\n2 - How easy it is to produce distilled datasets analogous to ones used for experiments? Why they are tightly bound to a particular architecture?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9121/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698404552066,
            "cdate": 1698404552066,
            "tmdate": 1699637147808,
            "mdate": 1699637147808,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mXnvB7BMOb",
                "forum": "5451cIQdWp",
                "replyto": "jBb1nLLA54",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9121/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9121/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Disclaimer: I made a mistake introducing the paper \u201cDistilled Pruning: Using Synthetic Data to Win the Lottery\u201d by McDermott & Cummings as this work is a nonarchival extended abstract, not a full paper. I now have removed all reference of this paper in my submission, and I hope this submission will be treated as if this algorithm, or even the idea to use distilled data with pruning, is introduced for the very first time. I request that you do not look into the authors identity for further reviewing. Hopefully, this will improve the novelty of this paper, as there are no \u201cpublished\u201d works that explore this idea.\n\nI have completely removed this section and any reference to the IB framework in this paper. I agree in retrospect, this seems out of place and would require a full paper with experimentations to draw this connection.\n\nI have attempted to include such an analysis on sparsity masks; however, I have not been able to find such pattern in weight connectivity / sparsity mask. Even considering hamming distance over time, the sparsity masks look indistinguishable. The only differences I am able to conclude is how these sparsity masks perform or train on the real data. I have now added this information in the discussion.\n\nI hoped for this work to be seen as a study of the linear mode connectivity in these models, not a paper pushing the performance of the algorithm. However, to acknowledge Reviewer 6HLj\u2019s concern for practical implications, I have added a new section that discusses what you can do with these synthetic subnetworks. I have shown that you can use synthetic subnetworks as a stable initialization for IMP, finding Lottery Tickets at higher sparsity. I originally did not include this information in the paper as I aimed to focus more on the why the stability is happening, drawing parallels to the IB framework. It appears this paper has been reviewed less as a study and more as pruning algorithm work, so this section seems more fitting. \nMinor:\n- I have instances between \\citep and \\citet\n- Changed this line. \n- 60% and 87% are two values chosen to mimic \u201cMedium Sparsity\u201d and \u201cHigh Sparsity\u201d. 60% sparsity refers to Pruning Iteration 4 in Figure 4. Which is where nearly all the \u201ceffects of distilled data\u201d on stability begin. 87%, or high sparsity, is chosen when both IMP and Distilled Pruning start to drop off in performance. \nThe barrier is always the same because this figure represents the performance $\\textbf{ratio}$ distilled pruning compared to IMP: $Acc_{\\text{Syn}} / Acc_{\\text{IMP}}$.  The performance ofIMP divided by itself is 1, regardless of setting. I have added a few things in the Overview of Synthetic Subnetworks section to make it simpler, but overall this information was included.\n- Removed IB framework entirely.\n- I have now included this in the main body of the paper. \n\nI disagree, in the extreme if you have a completely flat loss landscape, all points are linearly mode connected. Sharper landscapes, even with the same number of minima/maxima will have large barrier heights, leading to poor stability.\n\nQuestion #1: Traditional Iterative Magnitude Pruning can only find Lottery Tickets on a stable model. Currently, we can only find such stability for this by training for a few epochs, which is not at initialization. My primary research question was, \u201cIf a dense network is unstable at initialization, can we find a stable subnetwork here instead?\u201d\n\nQuestion #2: Very easy. You can download them yourself from the original dataset distillation method\u2019s repository  https://github.com/snu-mllab/Efficient-Dataset-Condensation. However, for datasets not commonly used in distilled data, you will have to rerun this your self. Distilled Data as a field is so new, so I released this paper to show what you can do with it, hopefully encouraging more research on it.\n\nThey are tightly bound to a specific architecture because they are biased towards whatever architecture type is used in the optimization procedure for generating distilled data. Because these methods are intensive with significant retraining and trajectory matching, they can currently only scale the architectures so much. Therefore, anything larger than ResNet-50 is not even considered by most modern dataset distillation work. Even then, we do not use ResNet-50 in this paper as these methods perform very poorly at that scale. I hope I made these challenges and issues with current distilled data methods apparent enough in the background section.\n\nI appreciate your feedback on the content of the paper. Your reviews have definitely strengthened my paper and pointed out flaws that I needed a second set of eyes for. For novelty, I am not at liberty to go into the specifics; however, I please ask that you give an additional rating that assumes this work is novel and has never been published before. I realize after these reviews, I should not have included the original extended abstract on Distilled Pruning."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9121/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684794931,
                "cdate": 1700684794931,
                "tmdate": 1700684794931,
                "mdate": 1700684794931,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "W4H4mCaIFL",
                "forum": "5451cIQdWp",
                "replyto": "mXnvB7BMOb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9121/Reviewer_rRcD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9121/Reviewer_rRcD"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the author(s) for the reply. I am a bit confused though, because I was not claiming that the paper is not novel, but I got a long explanation about connection to an extended abstract. Nevertheless, I appreciate the answers to _my_ questions as well.\n\nAbout flatness: when it is evaluated using Hessian characteristics, it reflects the property of the surface only in a very small surrounding of the selected point. This will not have an effect (at least obvious one) on the LMC between two random points.\n\nQ2. The fact that I can download them does not mean that it is _easy_. What is the complexity of the creation of such dataset? What is the connection with a particular architecture? How it possibly reduces the complexity of the training? All these are the questions that are interesting for such research, but stayed outside of the scope of the paper.\n\nWhile I still agree that the phenomenon itself (stability with distilled data) is interesting, I do not see this paper to be ready for publication and containing enough analysis. I encourage author(s) to continue work."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9121/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734265330,
                "cdate": 1700734265330,
                "tmdate": 1700734265330,
                "mdate": 1700734265330,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KuNExSH6ZC",
            "forum": "5451cIQdWp",
            "replyto": "5451cIQdWp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9121/Reviewer_mHnL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9121/Reviewer_mHnL"
            ],
            "content": {
                "summary": {
                    "value": "This paper follows the main idea of McDermott & Cummings (2023) that leveraging the synthetic dataset (specifically, dataset distillation) to find a sparsity mask with IMP. The difference between this paper and McDermott & Cummings (2023) is that this paper applies a better dataset distillation method. The strong part of this paper is that they found the subnetworks at initialization that are already stable to SGD noise, which is surprising."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Sec 5.'s experiments are interesting to me. The authors found that there exits subnetworks are already stable to SGD noise using distilled data."
                },
                "weaknesses": {
                    "value": "1. The contribution is limited. The main idea of this paper actually follows McDermott & Cummings (2023) but change the dataset distillation method. \n2. IB framework seems to be irrelevant to the main logic of this paper. A large part of this paper is trying to explain the connection between the IB framework and dataset distillation method, but I don't see a deep connection between this part and the main flow of this paper. Moreover, the IB framework lacks experiment or theoretical analysis which cannot convince me firmly. \n3. The Sec 5.1 seems to be redundant. The LMC experiments can already give a clear conclusion over the stability of subnetworks. I don't see a large advantage to include such fancy visualizations in the main text. \n4. Some figures seem to be non-informative (e.g., Fig 1 right part and Fig 2). There is no need to explain the dataset distillation and IMP with both texts and figures. This part can be put into a \"background\" section but no need to explain in such a detailed manner. The audience can be assumed to be people who are knowledgeable in these fields and the effect of including these figures is to lower the informativeness of this paper.\n5. The writing of this paper sometimes makes me lost. There are many \"therefore\" in this paper, but most times when \"therefore\" occurs there is no clear causal relationship between the sentences before and after. Some expressions are vague, e.g., the \"important\" in \"...What is deemed \"important\" for real data might not be important for distilled data; therefore, distilled pruning may attempt to remove these....\" (P. 5). Also, some expressions are actually wrong, e.g., \"Linear paths or Linear Mode Connectivity (LMC) is an uncommon phenomenon that only occurs in rare cases...\" and LMC is not a rare phenomenon but happens with both spawning case and permutation case [cite 1].\n[cite 1] Samuel Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. Git re-basin: Merging models modulo\npermutation symmetries.\n6. Most important references are missing. Only one paper is referenced in the LMC section."
                },
                "questions": {
                    "value": "No question."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9121/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9121/Reviewer_mHnL",
                        "ICLR.cc/2024/Conference/Submission9121/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9121/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698573406679,
            "cdate": 1698573406679,
            "tmdate": 1700733150102,
            "mdate": 1700733150102,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "E6ClRAeea2",
                "forum": "5451cIQdWp",
                "replyto": "KuNExSH6ZC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9121/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9121/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer  mHnL"
                    },
                    "comment": {
                        "value": "Weaknesses 1)\n\nI made a mistake introducing the paper \u201cDistilled Pruning: Using Synthetic Data to Win the Lottery\u201d by McDermott & Cummings as this work is a nonarchival extended abstract, not a full paper. I now have removed all reference of this paper in my submission, and I hope this submission will be treated as if this algorithm, or even the idea to use distilled data with pruning, is introduced for the very first time. I request that you do not look into the authors identity for further reviewing. \n\nI understand where your original ratings stand; however, would it be possible for you to provide a rating of the revised paper that assumes this idea is being published for the first time. You can make a heavy disclaimer with this.\n\nWeaknesses 2) \n\nI have completely removed this section and any reference to the IB framework in this paper. I agree in retrospect, this seems out of place and would require a full paper with experimentations to draw this connection.\n\nWeaknesses 3)\nSection 5.1 showcases similar results, but across an added dimension which shows new information not capture in the linear interpolation plots. Those plots show nothing about the landscape around the trained models, only giving information that the path between them. I disagree that LMC experiments are clear enough, for evidence see Reviewer qh7B\u2019s questions.\n\nWeaknesses 4)\n\nWhile I agree about comments in weakness 2, I disagree heavily here. Readers who have not read into dataset distillation can be confused on how we generate these representations in the first place. I have faced numerous questions and confusion in the past all about distilled data, even the point of this paper is only to use it. Figure 2 explains when we are adding distilled data, this is not traditional IMP. In the texts, I stress that we are not using distilled data as the final training stage for evaluation; however, I have received comments believing that we are, or even that the loss landscape figures are for distilled data. The audience is not entirely people who are knowledgable in this field because both fields \u2014 linear mode connectivity and dataset distillation \u2014 are small. One motivates of this paper is to invite researchers interested in pruning and neural architecture evaluation methods like NAS to look into synthetic data as an option, which is an entirely separate field. \n\nWeaknesses 5) I have fixed wording, I agree with the criticisms of language. \n\nFor your criticisms on the commonness of Linear Mode Connectivity, this is purely semantics. Git Re-basin shows that after $\\textit{permuting}$ a model, they are linear mode connected. LMC across permutation is entirely out of scope of this paper; however, I have now updated the intro/background to at least acknowledge its existence. In addition, I feel that my section title of \u201cLinear Mode Connectivity\u201d in the background was misleading. Changing this to discuss stability to SGD noise seems more appropriate. \n\nWeaknesses 6) Relating to weakness 5, we need LMC without permutations to actually find Lottery Tickets. I referenced works in the intro that motivate the study of linear mode connectivity such as (Nagarajan & Kolter, 2021), but also discuss the study of nonlinear mode connectivity with  (Freeman & Bruna, 2017), (Draxler et al., 2019), (Garipov et al., 2018). I now have added permutation LMC references to acknowledge this subfield; however, the reader should be able to fully understand the motivation for stability without these.\n\nIn addition, to replace the section on IB Framework & to acknowledge Reviewer 6HLj\u2019s concern for practical implications, I have added a new section that discusses what you can do with these synthetic subnetworks. I have shown that you can use synthetic subnetworks as a stable initialization for IMP, finding Lottery Tickets at higher sparsity. I originally did not include this information in the paper as I aimed to focus more on the why the stability is happening, drawing parallels to the IB framework. It appears this paper has been reviewed less as a study and more as pruning algorithm work, so this section seems more fitting.\n\nI appreciate your feedback on the content of the paper, and completely agree with most of the comments in hindsight after writing this paper. For novelty, I am not at liberty to go into the specifics; however, I please ask that you give an additional rating that assumes this work is novel and has never been published before. I realize after these reviews, I should not have included the original extended abstract on Distilled Pruning. I hope with these changes, the adjustment to novelty, and the additional section expanding the LTH, you can update your ratings accordingly."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9121/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684493749,
                "cdate": 1700684493749,
                "tmdate": 1700684493749,
                "mdate": 1700684493749,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Vf6Z2InWoG",
                "forum": "5451cIQdWp",
                "replyto": "E6ClRAeea2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9121/Reviewer_mHnL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9121/Reviewer_mHnL"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback, especially the clarification on the novelty part. The idea is interesting and still the experiments in sec 5 are interesting to me, and thus I will raise my rating correspondingly. However, I will still keep the rating negative, as this paper needs further polish and I recommend the author to consider the resubmission of this work."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9121/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733125711,
                "cdate": 1700733125711,
                "tmdate": 1700733125711,
                "mdate": 1700733125711,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2WmFFthfVq",
            "forum": "5451cIQdWp",
            "replyto": "5451cIQdWp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9121/Reviewer_qh7B"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9121/Reviewer_qh7B"
            ],
            "content": {
                "summary": {
                    "value": "This paper studied the stability of synthetic subnetworks (the lottery ticket at initialization obtained by IMP after training o distilled data) with linear mode connectivity. In contrast to the usual instability observed in both the dense network and the standard IMP subnetwork, the synthetic subnetwork proves to be remarkably stable to SGD noise at initialization. To gain a better understanding of this phenomenon, the paper proceeds to visualize the loss landscape and quantitatively assess its sharpness through hessian approximation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper offers a compelling insight into the impact of data distillation on the stability of lottery tickets obtained through the IMP method. This observed stability holds true across various datasets and models, strengthening the credibility of the findings.\n2. The paper conducts a comprehensive analysis of the performance and stability of synthetic subnetworks. Furthermore, its impressive visual representations provide valuable insights into the study of stability and the intricacies of the loss landscape."
                },
                "weaknesses": {
                    "value": "The analysis of the loss landscape and its sharpness for dense models, synthetic subnetworks, and IMP subnetworks is quite interesting. However, I have a couple of questions that I hope the author can clarify:\n\n1. The loss landscape of dense models and IMP subnetworks appears to be sharper than that of synthetic subnetworks. Can the sharpness of the loss landscape be used as a criterion for determining the quality of a subnetwork?\n2. In the last paragraph of Section 5.1 (on page 8), it is mentioned, 'We see the trained models fall into two separate minima in both the IMP and Dense cases, explaining the loss barrier in Figure 4.' This seems to connect stability with the loss landscape. If I understand correctly, the linear path for stability and loss landscape are different. Is this proper to explain the stability with this loss landscape? \n\nOther tiny issues:\n\n1. The axis legend in Figure 6 is almost unreadable."
                },
                "questions": {
                    "value": "1. How is the stability of the other distilled data other than the one evaluated in the paper? \n2. How does IPC impact the stability? Can more studies are provided for ResNet18 on CIFAR10 and CIFAR100?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9121/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9121/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9121/Reviewer_qh7B"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9121/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698832759054,
            "cdate": 1698832759054,
            "tmdate": 1699637147583,
            "mdate": 1699637147583,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VUxFqY1YpV",
                "forum": "5451cIQdWp",
                "replyto": "2WmFFthfVq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9121/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9121/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer qh7B"
                    },
                    "comment": {
                        "value": "In response to 1) in Weaknesses, smoothness of the loss landscape is not exactly a direct criteria for performance (that would be the exact loss value), but it does tell us about generalization and how the model responds to small perturbations. For example, consider the functions $x^2$ and $5x^2$, they have the same minimum 0 when $x=0$, meaning they will \u201cperform\u201d the same at inference on the set they are optimized for. However, $5x^2$ cannot handle small perturbations to x as well as $x^2$, as it has a larger gradient and thus worse loss for some $5(x+\\epsilon)^2$ for some small $\\epsilon$. While this is a convex example, we see more drastic issues when nonconvex landscapes are sharp. This matters in when you deploy your models as the distribution shifts away from your training data. Also during training time, smoother landscapes are much easier to optimize for.  In summary, they are not an exact 1-1 comparison for loss, but they are much more robust models. Robustness is extremely important for sparse models.\n\nIn response to 2) in Weaknesses, the linear path for stability is drawn by interpolating between two trained models. We visualize the loss landscape around converged models, showcasing that we exist in different minima. If you linearly interpolated between minima in the loss landscape, you would be drawing a line through that loss barrier. The linear interpolation barrier from the plots are directly seen in the loss landscape figure. The landscape is essentially giving us 1 more dimension worth of information.\n\nTiny Issue 1): Fixing this issue. The axes values are not entirely important to understand the smoothness, but I realize it is important to ensure the reader that these values are held constant.\n\nQuestion #1: Previous methods on Distilled Pruning used MTT (https://arxiv.org/abs/2203.11932) which is a lower performing method than the one we use. Despite lower performance, MTT also can be used to find stability on small cases of AlexNet / CIFAR-10 as shown in previous nonarchival work. Both of these methods aim to match training trajectories, which means the distilled loss landscape & real loss landscape are closely aligned. Other methods have no such requirement, which I hypothesize may not always find stability; however, I have not tried them.\n\nQuestion #2\n\nThis answer is dependent on a few things / settings. Generally, when the images per class decreases the stability increases. This is why we see stability at all with pruning on distilled data rather than real data. As shown in Figure 7, we see this trend with ConvNet-3 on CIFAR-10. I would have added more points to this plot to support this claim; however, it became too cluttered.\n\nFor CIFAR-100, I firmly believe dataset distillation is too young as a field to handle are large number of classes, so I would prefer to wait for the next SOTA distillation method to explore this. For CIFAR-10, we illustrate the linear interpolation plots in Figure 4. In addition, we also show the performance of the distilled pruning algorithm on ResNet-18/CIFAR-10 in Figure 3.  \n\nThank you for your responses and questions. If you have anymore, please feel free to comment."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9121/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700522659429,
                "cdate": 1700522659429,
                "tmdate": 1700522659429,
                "mdate": 1700522659429,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]