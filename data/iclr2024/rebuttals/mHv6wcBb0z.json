[
    {
        "title": "Preventing Model Collapse in Deep Canonical Correlation Analysis by Noise Regularization"
    },
    {
        "review": {
            "id": "hTY8aOImPq",
            "forum": "mHv6wcBb0z",
            "replyto": "mHv6wcBb0z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7558/Reviewer_hTsZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7558/Reviewer_hTsZ"
            ],
            "content": {
                "summary": {
                    "value": "The paper deal with Multi-View Representation Learning and solves a problem which affects Deep Canonical Correlation Analysis (DCCA) approaches, which is the model collapse. Model Collapse occurs when the performance of DCCA-based methods drops with the advancements of the training epochs, and is due to the model correlation among neural networks. \nThe authors make a comparison with simple CCA (not deep) approaches, and demonstrate that the main reason for CCA not having the model collapse issue is that the full-rank property holds in its transformation matrix, while the DNNs in DCCA do not possess such property. In particular, CCA searches for as a full-rank matrix, and it is robust to random noise, given the fact that the correlations before and after the (linear ) projection is kept\nTherefore, the authors propose a noise regularization to enforce the DNNs to be \u201cfull-rank\u201d, tailored for DCCA-based methods, dubbed NR-DCCA. NR-DCCA generates a set of i.i.d Gaussian white noise, with the same shape as the multi-view data Xk. Subsequently, DCCA is enforced to be full rank by adding a Noise regularized loss which requires that the correlation between the raw data and the noise is kept after having embedded the data into a latent space with the deep network, for each of the view.\nExperiments on synthetic and real world datasets, against a set of SOTA approaches, show the validity of the simple idea of the authors"
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The problem is important, since DCCA-based MVRL methods without model collapse are hard to achieve.\nA theoretical analysis of the model collapse issue in DCCA-based methods for MVRL is shown \nA novel noise regularization approach is proposed for DCCA\nThe full-rank property of the CCA method which prevents model collapse is demonstrated, this drives the noise regularization approach from a theoretical perspective.\nExperiments demonstrate a good performance. The comparative approaches are appropriate. The resulst tel a clear story, and there are no question marks that arise."
                },
                "weaknesses": {
                    "value": "I did not find any major lack. I was focusing on the theory, but it seems very clear (yet simple). I was looking for additional, more appealing comparative approaches doing DCCA, but I did not find any. One may argue why the approach has been casted solely for Multi-View Representation Learning, since it could have a broader scope, but this is not a minus. Just a curiosity. \nIn general I think that the paper can be squeezed a little, in order to host some of the experiments of the additional material. In particular, I found fascinating the experiment reported in Fig.12, about the the correlation between unrelated data. This is a further proof of the goodness of the idea. I would also report the tsne visualization in the main paper."
                },
                "questions": {
                    "value": "See my suggestions and curiosity above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "10: strong accept, should be highlighted at the conference"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7558/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698833110727,
            "cdate": 1698833110727,
            "tmdate": 1699636914739,
            "mdate": 1699636914739,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xyYR8rA6Di",
                "forum": "mHv6wcBb0z",
                "replyto": "hTY8aOImPq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7558/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7558/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to reviewer hTsZ"
                    },
                    "comment": {
                        "value": "We really appreciate your recognition and positive feedback of our work! The itemized responses to your questions are as follows:\n\n> One may argue why the approach has been casted solely for Multi-View Representation Learning, since it could have a broader scope, but this is not a minus.\n\nThanks for the thoughtful comments, and we believe this comment echoes Comment 3 by Reviewer 5U9E. The proposed Noise Regularization (NR) is indeed a way to regularize the neural network, and in general, it can be widely useful for different tasks. \n\nThe developed \u201cfull-rank\u201d property of the neural network stems from the full-rank of the transformation W_k, so this paper mainly focuses on associating DCCA and CCA. Current theoretical derivations only apply to CCA, and it is still challenging to provide a good explanation of why such NR would work in other tasks. This also echoes Comment 3 by Reviewer w9Bv and Comments 2 and 3 by Reviewer w9Bv. We do think this is an interesting area worth further exploration, and we will leave it for future study.\n\n> In general I think that the paper can be squeezed a little, in order to host some of the experiments of the additional material. In particular, I found fascinating the experiment reported in Fig.12, about the the correlation between unrelated data. This is a further proof of the goodness of the idea. I would also report the tsne visualization in the main paper.\n\nThanks for your kind suggestions. We have squeezed the paper and merged the duplicate original Figures 2 and  5. We have also shown and analyzed the correlation of unrelated data in the revised section 5.2. We have mentioned the T-SNE results could also be evidence of model collapse in revised section 3.4 and pointed the readers to Appendix A.9 for details. We will further re-organize the paper to highlight the points the reviewer mentioned if it is accepted.\n\nOverall, we sincerely appreciate your positive feedback and hope that our responses have sufficiently answered your questions. Should there be any queries or points of discussion, we stand ready to provide further clarifications."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7558/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700219067770,
                "cdate": 1700219067770,
                "tmdate": 1700219067770,
                "mdate": 1700219067770,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "91GLFxoSEr",
            "forum": "mHv6wcBb0z",
            "replyto": "mHv6wcBb0z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7558/Reviewer_5U9E"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7558/Reviewer_5U9E"
            ],
            "content": {
                "summary": {
                    "value": "The authors of this study empirically observe a critical issue of model collapse in DCCA-based methods. This phenomenon is characterized by a significant drop in performance as the training progresses. The model collapse issue poses a substantial challenge to the widespread adoption of DCCA-based methods, as it becomes difficult to determine the appropriate stopping point during training. To address this issue, the authors introduce NR-DCCA, which incorporates a novel noise regularization approach designed to prevent model collapse. Furthermore, they provide theoretical insights demonstrating that maintaining the full-rank property is essential for preventing model collapse, and the proposed noise regularization effectively enforces this property within the neural network. Additionally, they develop a framework for generating synthetic data containing various common and complementary information to facilitate a comprehensive comparison of Multiple View Representation Learning (MVRL) methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "NR-DCCA consistently outperforms the baseline methods in both synthetic and real-world datasets. Moreover, the proposed noise regularization approach can be applied to other DCCA-based methods, such as DGCCA. This paper is well-structured and easy to follow."
                },
                "weaknesses": {
                    "value": "Some key details are missing. See my comments below."
                },
                "questions": {
                    "value": "However, the reviewer has several questions:\n\nThe proposed method involves adding a regularization term to the loss function. The choice of the optimal value for the hyperparameter \u03b1 in Equation 6 is crucial. If \u03b1 is set too large, the model may tend to behave like an identity mapping function, potentially reducing its effectiveness. Conversely, if \u03b1 is too small, the model may not maintain a \"full-rank\" property. The authors should provide guidance or suggestions on how to select an appropriate \u03b1 value.\n\nThe reviewer is interested in whether there is any theoretical analysis of the generalization ability of the proposed method. An exploration of how NR-DCCA's performance might extend to new, unseen data or domains would add depth to the paper.\n\nThe paper showcases the performance improvement of the proposed method in downstream tasks using off-the-shelf methods like Support Vector Regression (SVR). However, the reviewer is curious about whether there are any significant performance differences when fine-tuning the system using downstream tasks. An analysis of how NR-DCCA performs in this scenario would be informative."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7558/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699000549003,
            "cdate": 1699000549003,
            "tmdate": 1699636914638,
            "mdate": 1699636914638,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zzLCu3oddD",
                "forum": "mHv6wcBb0z",
                "replyto": "91GLFxoSEr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7558/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7558/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response  (1/2)  to reviewer 5U9E"
                    },
                    "comment": {
                        "value": "We appreciate your comments and feedback. In addition to the general response, we address your itemized concerns here.\n\n> The proposed method involves adding a regularization term to the loss function. The choice of the optimal value for the hyperparameter \u03b1 in Equation 6 is crucial. If \u03b1 is set too large, the model may tend to behave like an identity mapping function, potentially reducing its effectiveness. Conversely, if \u03b1 is too small, the model may not maintain a \"full-rank\" property. The authors should provide guidance or suggestions on how to select an appropriate \u03b1 value.\n\nThanks for the thoughtful comment. Yes, the choice of the hyperparameter \u03b1 is essential in NR-DCCA. However, different from the conventional hyperparameter tuning, the determination of \u03b1 is actually simpler, as we just need to search for the smallest \u03b1 that can prevent the model collapse, and the model collapse can be directly observed on the validation data. \n\nIn  Appendix A.6.2, we present performance curves of NR-DCCA in CUB under different \u03b1. if \u03b1 is too large, the performance will converge slowly; if it is too small, the model collapse issue may remain. We increase the \u03b1 adaptively until the model collapse issue is tackled, i.e., the correlation with noise will not increase or the performance of DCCA will not drop when the training progresses. The optimal \u03b1 values of NR-DCCA for the CUB, PolyMnist, and Caltech datasets are found to be 1.5, 5, and 15, respectively.  Additionally, one can see the NR-DCCA outperforms DCCA robustly with a wide range of \u03b1.\n\n> The reviewer is interested in whether there is any theoretical analysis of the generalization ability of the proposed method. An exploration of how NR-DCCA's performance might extend to new, unseen data or domains would add depth to the paper.\n\nThanks for the comment. The proposed Noise Regularization (NR) is indeed a way to regularize the neural network, and in general, it is widely acknowledged that regularization on neural networks is helpful for generalization ability [1,2].\n\nThe common regularization methods include two categories: explicit methods (i.e. regularizing the parameters, hidden features, or gradients in the neural networks) [1]  and implicit methods (e.g., noise regularization) [2]. It is clear that the proposed NR falls in the second category, which lacks comprehensive studies in representation learning.\n\nActually, the implicit methods usually \u201cregularize the behavior\u201d of neural networks. We believe this is a new perspective and our numerical experiments also indicate its outperformance. However, it might be challenging to analyze what has been regularized in neural networks, and hence it would be difficult to provide theoretical analysis regarding the generalization ability, due to the intricate neural network structures. Here we actually observe a trade-off between interpretability and performance in regularization methods (explicit vs. implicit). Instead, what we did in the paper is to show the analogy with the classic CCA, and we let the neural network mimic the good behaviors of CCA, given the fact that CCA has good generalization ability.\n\nWe appreciate that the reviewer pointed out the right direction for potential future work. The primary objective of this paper lies in addressing the model collapse issue within DCCA, while we believe analyzing the detailed properties of such NR would be impactful.\n\n[1] Huang, Lei, et al. \"Normalization techniques in training dnns: Methodology, analysis and application.\" \n\n[2] Poole, Ben, Jascha Sohl-Dickstein, and Surya Ganguli. \"Analyzing noise in autoencoders and deep networks.\" arXiv preprint arXiv:1406.1831 (2014)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7558/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700219393555,
                "cdate": 1700219393555,
                "tmdate": 1700219393555,
                "mdate": 1700219393555,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "J1ZQOn1tsn",
                "forum": "mHv6wcBb0z",
                "replyto": "91GLFxoSEr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7558/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7558/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response (2/2) to reviewer 5U9E"
                    },
                    "comment": {
                        "value": "> The paper showcases the performance improvement of the proposed method in downstream tasks using off-the-shelf methods like Support Vector Regression (SVR). However, the reviewer is curious about whether there are any significant performance differences when fine-tuning the system using downstream tasks. An analysis of how NR-DCCA performs in this scenario would be informative.\n\nThanks for the thoughtful comment. It is possible to also fine-tune the representation learning during the training of the downstream tasks (like SFT in LLM), while this approach deviates from the standard validation procedure of multi-view representation learning (MVRL).\n\nIn MVRL, the predominant validation procedure is first to generate representations, and then use the representation directly. To ensure a fair comparison with existing methods, we still follow this procedure. \n\nIndeed, we believe that if we further fine-tune the representation for specific tasks, then the performance will definitely improve, but the representation will lose generality.\n\nThis actually resembles other supervised multi-view problems, such as multi-view classification [3]. Investigating how NR-DCCA performs under these problems might be interesting and it is left for future work.\n\n\nOverall, we sincerely appreciate your insightful feedback and hope that our responses have sufficiently addressed any concerns raised. Should there be any queries or points of discussion, we stand ready to provide further clarifications.\n\n[3] Han, Zongbo, et al. \"Trusted multi-view classification.\" arXiv preprint arXiv:2102.02051 (2021)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7558/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700219588274,
                "cdate": 1700219588274,
                "tmdate": 1700219588274,
                "mdate": 1700219588274,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lTboOdYuU3",
            "forum": "mHv6wcBb0z",
            "replyto": "mHv6wcBb0z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7558/Reviewer_w9Bv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7558/Reviewer_w9Bv"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an algorithm for regularizing the multi-view deep CCA model with noise. Specifically, the algorithm introduces additional loss terms that encourage the correlation between DNN output of signal and noise, to be consistent with linear mapping output of signal and noise."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I like that the authors somewhat carefully designed numerical simulations to test their method."
                },
                "weaknesses": {
                    "value": "I have several concerns.\n\n1. Important references and discussions are missing. The fact that powerful neural networks could lead to degenerate feature space is known. In the original deep CCA paper [Andrew et al, 2013] there was already ridge regularization of the auto-covariance matrices (see their eqn 5) to avoid degeneracy and improve numerical stability. And that regularization technique was already studied for linear CCA, see\n- De Bie and De Moor. On the regularization of canonical correlation analysis, 2003.\nand an probabilistic interpretation where a Gaussian observation model leads to regularized covariance matrices\n- Bach and Jordan. A Probabilistic Interpretation of Canonical Correlation Analysis. 2005.\nWhile the authors' proposal appears different, it is still important to discuss the connections to existing methods and compare with them, both theoretically and empirically.\n\n2. The intuition behind the proposal is not super clear to me. If the goal is to have full-rank f(X), so that the covariances are better conditioned, the abovementioned covariance regularization approach already achieves the same effect. If I look at Proposition 4, it essentially says CCA is invariant to linear transformations of input; but this is well-known and easy to see from the original formulation, even without complicated linear algebra. The more interesting analysis would be to explain why the correlation between signal and random noise is a good quantity for deep neural networks to mimic; there must be more structure than saying that fk is full-rank in my opinion.\n\n3. The paper is not purely about optimization and numerical stability. And I expect to see different inductive bias from the proposed regularization. There shall be investigation of the feature quality on real-world datasets, as shown by prior deep CCA-based papers."
                },
                "questions": {
                    "value": "- I hope to see non-trivial analysis regarding the effect of noise regularization.\n- I hope to see comparison of feature quality against alternatives."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7558/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699486837397,
            "cdate": 1699486837397,
            "tmdate": 1699636914456,
            "mdate": 1699636914456,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oqDR6Ensak",
                "forum": "mHv6wcBb0z",
                "replyto": "lTboOdYuU3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7558/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7558/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response (1/3) to reviewer w9Bv"
                    },
                    "comment": {
                        "value": "We appreciate your comments and feedback. In addition to the general response, we address your itemized concerns here.\n\n> Important references and discussions are missing. The fact that powerful neural networks could lead to degenerate feature space is known. In the original deep CCA paper [Andrew et al, 2013] there was already ridge regularization of the auto-covariance matrices (see their eqn 5) to avoid degeneracy and improve numerical stability. And that regularization technique was already studied for linear CCA...\n\nThanks for the thoughtful comment. The reviewer mentioned the ridge regularization, which is a widely used technique to regularize the features (W_kX_k in CCA and f_k(X_k) in DCCA using our notations), and it is a default setting across almost all (D)CCA methods. In our experiments, **we have actually included ridge regularization during hyper-parameter tuning for all the methods with and without noise regularization**. \n\nHowever, the ridge regularization **mainly regularizes the features (i.e., W_kX_k in CCA and f_k(X_k) in DCCA), rather than the transformations (i.e., W_k in CCA and f_k in DCCA)**. W_k in CCA does not need any extra regularization as it is automatically full-rank, while the f_k is still unconstrained and unregulated in DCCA. Therefore, our study mainly focuses on the regularization of the transformation in DCCA. That is why we did not compare with the ridge regularization in the paper.\n\nThe reviewer agreed that \u201cpowerful neural networks could lead to degenerate feature space\u201d. However, the degenerated features produced by neural networks in DCCA can still be full-rank, and simply enforcing the covariance matrix to be full-rank by adding ridge regularization is not even helping, because the collapse of neural networks is intricate. Therefore, we develop the concept of the \u201cfull-rank\u201d property of f_k (note this is an analogy of the full-rank property of W_k), and it is different from the full-rank of the features f_k(X_k). We apologize for the potential confusion raised by this definition. \n\nOverall, we believe that ridge regularization is mainly for combating noise and improving numerical stability, it cannot prevent the neural networks from degenerating (i.e., model collapse). In contrast, we should directly study the \u201cfull-rank\u201d property of the neural networks f_k and we believe the relevant studies are lacking. \n\nTo further support our arguments, we provide the experimental results with different ridge parameters on a real-world dataset CUB at 500-th epoch as follows:\n\n| Ridge parameter | Performance | Square sum of feature covariance |\n|-----------------|-------------|---------------------------------|\n| 0               | 0.805       | 0.019                           |\n| 1e-5            | 0.786       | 0.028                           |\n| 1e-4            | 0.776       | 0.210                           |\n| 1e-3            | 0.731       | 7.336                           |\n\nOne can see that the ridge regularization **even damages the performance** of DCCA. In our NR-DCCA, we actually set the ridge parameter to **zero**. We conjecture the reason is that the large ridge parameter could make the neural network even \u201clazier\u201d to actively project the data into a better feature space, as the full-rank property of features and covariance matrix are already guaranteed, and this is also evidenced by the \u201cSquare sum of feature covariance\u201d shown in the table.\n\nWe apologize that we should have included the above discussion in the paper, and we have now incorporated the above results in Appendix A.6.1 for clarification."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7558/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700220692812,
                "cdate": 1700220692812,
                "tmdate": 1700220692812,
                "mdate": 1700220692812,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hD8fD07ELF",
                "forum": "mHv6wcBb0z",
                "replyto": "lTboOdYuU3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7558/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7558/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response (2/3) to reviewer w9Bv"
                    },
                    "comment": {
                        "value": "> The intuition behind the proposal is not super clear to me. If the goal is to have full-rank f(X), so that the covariances are better conditioned, the abovementioned covariance regularization approach already achieves the same effect. If I look at Proposition 4, it essentially says CCA is invariant to linear transformations of input; but this is well-known and easy to see from the original formulation, even without complicated linear algebra. The more interesting analysis would be to explain why the correlation between signal and random noise is a good quantity for deep neural networks to mimic; there must be more structure than saying that fk is full-rank in my opinion.\n\nWe apologize for the potential misunderstanding caused, and we have updated the paper to make it clearer (in Section 3.4). Actually, **we define the \u201cfull-rank\u201d property of f_k and it is different from the full-rank property of features  f_k(X_k)**. In CCA, the two concepts are nearly the same, as features W_kX_k are forced to be full-rank by the loss function, and W_k is naturally full-rank (refer to Lemma 2 in our paper). However, it is difficult to define what is a \u201cfull-rank\u201d property of neural networks since it is not a matirx. Because the current DCCA has no regularization on the neural networks, we believe **such inconsistency is the potential reason for the model collapse in DCCA**.\n\nWe agree with the reviewer that the Proposition looks straightforward. To be precise, the correlation with noise is invariant to full-rank linear transformation. However, it is the first time that the connection between the correlation with noise and the full-rank property of W_k is built. In terms of DCCA, as it is difficult to directly define the \u201cfull-rank\u201d property of f_k, we first find the equivalent condition of the full-rank property of W_k in CCA, and then we can transplant this condition to DCCA. Specifically, because of Theorem 1 (Proposition 3 + Proposition 4), we can equivalently define the \u201cfull-rank\u201d property of  f_k based on the noise in Definition 2.\n\nWe do agree with that reviewer that more structure of the neural networks should be explored to further understand what has been regularized by our proposed Noise Regularization. However, we have to admit that interpreting the parameters in neural networks is challenging. Our way to tackle this problem is to look at the behavior of the neural networks. Through numerical experiments, we find that the CCA never collapses and the DCCA always collapses. Then we design the experiments more extremely to let both CCA and DCCA learn the correlation between two independent random noises. The results indicate that CCA can identify the invariant correlation between the noise, while DCCA cannot (we have moved related experiments into Section 5.2 ). We believe this is mainly because **the powerful neural networks have \u201ccreated\u201d correlation itself**. Therefore, we enforce the neural networks to preserve the correlation for random noise in DCCA, and this motivates our idea of noise regularization. \n\n**We call f_k \u201cfull-rank\u201d to help the reader associate with the full-rank property of W_k, as the full-rank transformation matrix would not \u201ccreate\u201d correlation**. But this name might cause the reviewer\u2019s misunderstanding. We apologize and we have incorporated more discussions to elaborate on the \u201cfull-rank\u201d property of neural networks in the updated paper to eliminate confusion (in Section 4.2)."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7558/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700220769026,
                "cdate": 1700220769026,
                "tmdate": 1700220769026,
                "mdate": 1700220769026,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eWYv78kuiJ",
                "forum": "mHv6wcBb0z",
                "replyto": "lTboOdYuU3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7558/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7558/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response (3/3) to reviewer w9Bv"
                    },
                    "comment": {
                        "value": "> The paper is not purely about optimization and numerical stability. And I expect to see different inductive bias from the proposed regularization. There shall be investigation of the feature quality on real-world datasets, as shown by prior deep CCA-based papers.\n\nThanks for the comment. Indeed, our work is not about optimization and numerical stability. We **focus on imposing the inductive bias on the neural networks for DCCA**. The correlation with random noise is invariant to full-rank linear transformation, and hence we hope the correlation is also invariant to \u201cfull-rank\u201d non-linear transformation. Hence we define such \u201cfull-rank\u201d property as the inductive bias of the neural networks.  \n\nIn terms of the investigation of the **feature quality** on real-world datasets, our evaluation of feature quality is mainly guided by **visualization techniques** (as demonstrated by TSNE in Appendix A.9 ). Additionally, the **performances** across a multitude of downstream tasks, encompassing both synthetic and real-world datasets, demonstrate the consistent outperformance of the proposed method. Moreover, guided by our theory, we reported the **correlation between the outputs of unrelated data**, which provides another way to observe feature quality and model collapse issues.\n\n\n> I hope to see non-trivial analysis regarding the effect of noise regularization.\n\nPlease check our response to your Comments 1 and 2.\n\n> I hope to see comparison of feature quality against alternatives.\n\nPlease check our response to Comment 3.\n\nOverall, we sincerely appreciate your insightful feedback and hope that our responses have sufficiently addressed any concerns raised. Should there be any queries or points of discussion, we stand ready to provide further clarifications. In light of the revisions and discussions, we kindly invite you to reconsider your score on this paper."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7558/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700221112479,
                "cdate": 1700221112479,
                "tmdate": 1700224839875,
                "mdate": 1700224839875,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fG8anvxd2R",
                "forum": "mHv6wcBb0z",
                "replyto": "lTboOdYuU3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7558/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7558/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking for Further Discussion"
                    },
                    "comment": {
                        "value": "Dear Reviewer w9Bv,\n\nWe are aware that after tomorrow, the opportunity for further discussion will no longer be available in this year's ICLR.  We wonder if we have adequately addressed the concerns and clarified the misunderstanding you previously raised?\n\nIt is our earnest hope to continue our discussion, should there be any lingering questions or points needing clarification."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7558/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547467231,
                "cdate": 1700547467231,
                "tmdate": 1700547467231,
                "mdate": 1700547467231,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mCLvYDLE7a",
                "forum": "mHv6wcBb0z",
                "replyto": "lTboOdYuU3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7558/Reviewer_w9Bv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7558/Reviewer_w9Bv"
                ],
                "content": {
                    "title": {
                        "value": "Reply"
                    },
                    "comment": {
                        "value": "I appreciate the authors providing details on the tuning of ridge parameters for deep CCA. I am OK on that aspect.\n\nI understand at the high level that the intuition is to not create non-existent or spurious correlation between the views, when we use powerful neural networks for parameterizing feature mappings. But still\n- I feel the desired property is more than the \"full-rank\" property proposed in this paper, and this property has to be characterized more concretely (for example, similarly to how adding noise to data is shown to be equivalent to L2 regularization under assumptions). This is important since the authors claim their method to be advantageous over ridge regularized DCCA, which adds isotropic noise to data to ensure covariance is well-conditioned. \n- It is not very clear how the additional loss on matching correlation obtain with DNNs with linear mappings achieves the desired property. In fact, because noise is independently generated from data, the true correlation between noise and data should be zero (with infinite data). Why not instead minimize correlation between data and noise for regularizerization?"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7558/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700692276887,
                "cdate": 1700692276887,
                "tmdate": 1700692329112,
                "mdate": 1700692329112,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iysT0mBgKZ",
                "forum": "mHv6wcBb0z",
                "replyto": "lTboOdYuU3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7558/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7558/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further Response (1/2) to Reviewer w9Bv"
                    },
                    "comment": {
                        "value": ">I appreciate the authors providing details on the tuning of ridge parameters for deep CCA. I am OK on that aspect.\nI understand at the high level that the intuition is to not create non-existent or spurious correlation between the views, when we use powerful neural networks for parameterizing feature mappings. \n\nWe appreciate your feedback on our response and thoughtful comments. Great to know that the reviewer understands the key idea in this paper, and we also understand that the reviewer request a deeper theoretical understanding of the proposed noise regularization (NR) approach. We actually thought about this, but indeed the further exploration of such a \u201cfull-rank\u201d property was non-trivial, because our NR approach is not simply adding noise to the input,  and f_k can be any form of neural networks. However, we still believe our NR approach is quite new, and it achieves state-of-the-art performance consistently, so we would like to share this approach with the community.\n\n>I feel the desired property is more than the \"full-rank\" property proposed in this paper, and this property has to be characterized more concretely (for example, similarly to how adding noise to data is shown to be equivalent to L2 regularization under assumptions). This is important since the authors claim their method to be advantageous over ridge regularized DCCA, which adds isotropic noise to data to ensure covariance is well-conditioned.\n\nThanks a lot for the thoughtful comments! As the reviewer concluded, our desired property of neural networks is to \u201cavoid creating non-existent correlation between views\u201d, and this property is actually intricate because it is challenging to regularize the weights in neural networks to obtain this property. In contrast, this paper enforces the property directly, while it is indeed non-trivial to connect this property to the weights of neural networks. \n\nOur NR approach differs from the standard NR approach, as we did not add noise to the data directly and the standard NR approach requires strong assumptions (e.g., autoencoder structures) for analysis, so it is challenging to obtain similar properties like L2 regularization in our approach. As we mentioned earlier, we believe our NR approach mainly regularizes the behavior of the neural network, and how the parameters are regularized actually depends on the structure of the neural network, which we do not restrict directly.\n\nIt might be possible to derive some analytical properties of our NR approach if we assume specific structures of the neural networks. For example, [1] assumes an autoencoder structure with a linear decoder, and [2] relies on the linear transformation in CCA. However, this paper aims to develop a generalized NR approach for deep CCA, so we did not touch on this aspect. We do think this will be a great direction for further exploration.\n\nOverall, we still believe our NR approach is quite new, as it provides a new aspect to regularize the neural network, and its performance is consistently state-of-the-art, so we would like to share this finding with the community. Thanks again for your thoughtful comments and for pointing out the potential directions for further investigation!\n\n\n[1] Poole, Ben, Jascha Sohl-Dickstein, and Surya Ganguli. \"Analyzing noise in autoencoders and deep networks.\" arXiv preprint arXiv:1406.1831 (2014).\n\n[2] De Bie, Tijl, and Bart De Moor. \"On the regularization of canonical correlation analysis.\" Int. Sympos. ICA and BSS (2003): 785-790."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7558/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700721234398,
                "cdate": 1700721234398,
                "tmdate": 1700721444515,
                "mdate": 1700721444515,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]