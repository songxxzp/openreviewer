[
    {
        "title": "Federated Learning, Lessons from Generalization Study: Communicate Less, Learn More"
    },
    {
        "review": {
            "id": "prVLCFiJSk",
            "forum": "kWsJkH1tNi",
            "replyto": "kWsJkH1tNi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1745/Reviewer_yDVy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1745/Reviewer_yDVy"
            ],
            "content": {
                "summary": {
                    "value": "The paper examines generalization error in Federated Learning (FL) and focuses on the impact of communication rounds (R) on this error. It introduces PAC-Bayes and rate-distortion bounds for FL and applies them to Federated Support Vector Machines (FSVM). The study finds that as R increases, the generalization error of FSVM worsens, suggesting that more frequent communication with the parameter server diminishes its generalization power. The paper also shows that, for any R, FL outperforms centralized learning by a factor proportional to O(log(K)/K). Experiments with neural networks support these findings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The strengths of this paper are as follows:\n\n1. **Novel Bounds**: The paper introduces novel PAC-Bayes and rate-distortion theoretic bounds that explicitly consider the influence of the number of communication rounds (R), the number of participating devices (K), and dataset size (n) on generalization error. These bounds are the first of their kind for the problem addressed in the study.\n2. **Modeling Contributions**: The research provides insight into the structure of distributed interactive learning algorithms, showing how each client's contribution at each round affects the generalization error of the final model. \n3. **Applicability to FSVM**: The paper applies these theoretical bounds to Federated Support Vector Machines (FSVM) and derives explicit bounds for generalization error. Notably, it reveals that more frequent communication with the parameter server reduces the generalization power of FSVM algorithms. This suggests that the parameter R can be optimized to minimize the population risk of FSVM.\n4. **Generalization of Findings**: The research demonstrates that the generalization error of the FSVM setting decreases faster than that of centralized learning by a factor proportional to O(log(K)/K) for any value of R. This generalizes recent findings for \"one-shot\" Federated Learning to any arbitrary number of rounds."
                },
                "weaknesses": {
                    "value": "1. **Problem Setting**: The paper assumes a partitioning of data (n samples) into R disjoint subsets for each client, with each subset used in one communication round. This assumption may be considered strong and unrealistic, as many Federated Learning (FL) approaches typically use the entire batch for training in each round. Furthermore, in real-world scenarios, a mix of old and new data is often present in an online approach. \n\n2. **Counter-Intuitive Observation**: Moreover, if we consider a fixed n, each client ends up with n/R data for each communication round. To illustrate this with a special case, let's take R=1, which essentially reduces the scenario to centralized learning. Surprisingly, based on the results presented in the paper, which indicate that the generalization error increases with R, it might suggest that R=1 should be chosen. This finding appears counter-intuitive and raises questions about the practicality and relevance of the assumed setting for real-world federated learning applications."
                },
                "questions": {
                    "value": "I suggest that the author undertake a more extensive exploration of the practical implications stemming from the theoretical findings. It is imperative to conduct an exhaustive examination of the interdependencies between key parameters across various scenarios. Such an in-depth analysis holds the potential to offer invaluable insights into the development of real-world federated learning algorithms. Personally, I hold the theoretical contribution of this paper in high regard, despite my reservations about the underlying settings. A more comprehensive analysis would significantly enhance the overall quality of this paper and can be translated into guidance for the refinement and enhancement of federated learning systems."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1745/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697990981211,
            "cdate": 1697990981211,
            "tmdate": 1699636103409,
            "mdate": 1699636103409,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7incKFEl5P",
                "forum": "kWsJkH1tNi",
                "replyto": "prVLCFiJSk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1745/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1745/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer yDVy"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the time spent on our paper and their interest in our results.\n\n1. Please refer to our response to all reviewers.\n\n2. Note that the case $R=1$ is not the centralized learning algorithm, but the \"one-shot\" FL, where only one aggregation is performed. Indeed, in this case we obtain the best generalization performance, intuitively, since \"the variance of the model\" becomes small after one aggregation of the independent models. In contrast, it is hardly possible to minimize the empirical risk using only a single aggregation step. Therefore, $R=1$ is rarely the optimal value of $R$ when considering the population risk. In principle, there is a trade-off between the generalization error and the empirical error, as can be explicitly observed in Figure 3.\n\n- Regarding further experiments: We would like first to reiterate that the presented experiments on FSVM mainly aim at validating the behavior suggested by our Theorem 4, \\ie that the generalization error of FSVM might increase with $R$. The additional experiments on neural networks are meant to suggest that this behavior may hold in other setups. It should be remarked that the experiment on ResNet-56, presented in the Section 5 of the paper, was reproduced for different sets of hyperparameters, giving similar results. As a complement, we added the reproduced experiments for two different values of the learning rate in Appendix E.6. Finally, we agree that extensive exploration of the influence of optimization hyperparameters on the generalization error as a function of $R$ is indeed an interesting question. We believe however, it is beyond the scope and motivation of this work and therefore left for future work."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1745/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700068882906,
                "cdate": 1700068882906,
                "tmdate": 1700068882906,
                "mdate": 1700068882906,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xGP5rFsjTc",
            "forum": "kWsJkH1tNi",
            "replyto": "kWsJkH1tNi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1745/Reviewer_9uEx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1745/Reviewer_9uEx"
            ],
            "content": {
                "summary": {
                    "value": "This work provides novel PAC-Bayesian and rate-distortion typed generalisation bounds tailored for federated learning. Contrary to classical bounds designed for batch learning, authors take into account the evolution of the learning phase through successive rounds and now consider the number of rounds as an hyperparameter to optimise to ensure a good tradeoff between empirical performances and generalisation ability. They particularise their results to the case of federated SVMs and provide associated experiments."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Having such theoretical bounds tailored for federated learning is novel and provide exciting new leads to understand the efficiency of FL."
                },
                "weaknesses": {
                    "value": "I have concerns about correctness of Theorem 1, some presentation issues and the conclusion of the experiments, see the 'Questions' section."
                },
                "questions": {
                    "value": "-  The definition of $P_{\\mathbf{W},\\mathbf{S}}$ is unclear. Is it a distribution over $\\mathcal{W}^{(K+1)R}$ and then, is the use of the product $\\Pi$ equivalent to $\\otimes$ ? Otherwise, if it is a distribution over $\\mathcal{W}$ can you make this explicit? \n- Given the way $S_k$ is partitioned for any $k$, users are not allowed to use their whole dataset at each round but only a smaller fraction $n_R$. Is it a realistic in practice? For instance take the instance of federated learning between hospital to better detect a rare disease (i.e. each user has few data), is it reasonable in this case to force the users not to use their whole dataset each round? Furthermore, this would imply that if $n$ is small, then one would not be allowed to perform many training rounds.  What can the authors say about this?  \n- In section 2, you said that 'the aggregation function at the PS is set to be deterministic and arbitrary' while in Theorem 1,  $\\bar{W}^{[r-1]}$ is drawn according to a probability distribution, what did I miss? Furthermore, how costful is this additional expectation in terms of computational time as it does not appear in many classical PAC-Bayes bounds?\n- I found Theorem 1's proof poorly organised, for instance,  $\\mathbf{\\nu}_{S}$ is defined in Lemma 1's proof, which has been put on another appendix. Similarly, I did not find a definition of $\\mathcal{G}_S^{\\delta}$ although somewhat inferable from context.\n- I don't understand the use of the subgausiannity assumption at the end of page 2022. Let rename $X= gen(s_k^{(r)}, \\bar{W}^{(R)})$, then you affirm that because $X$ is $\\sigma_{k,r}:= \\sqrt{\\frac{R}{4n}}$ subgaussian we have $\\mathbb{E}[e^{\\lambda X^2}]\\leq \\frac{2n}{R}$.  This is highly non-standard, how do you prove it?  To me, subgaussianity would only imply $\\mathbb{E}[e^{\\lambda X}]\\leq \\exp(\\frac{\\lambda^2 \\sigma_{k,r}^2}{2})$.    \n- About the experiments, I am not convinced that communicating less implies a better learning phase, at least for FVSM. Indeed, this conclusion appears to be true for $K=10,20$ (not the case $K=50$ as the short decrease in the end of the curve exhibits more a stabilisation than a deterioration) and holds when considering the generalisation gap, instead of observing directly the population risk . Thus, if we focus on the notion of population risk, Figure 4 shows that increasing the number of rounds only leads to positive outcomes as the empirical risk continues to decrease while the population risk either decreases or stabilises. A similar conclusion can be derived from Figure 6. I acknowledge that you affirm in section D.2 that 'fewer rounds may be needed, if one can effectively take the \u201cestimated\u201d generalisation error into account', but a practitioner does not have access to this information and see only benefits to continue its training as the empirical risk decreases, and the test error does not vary: there is no stopping criterion in practice. \nTo me, the interest of a tradeoff in $R$, only appears in Figure 3, which is not covered by Theorem 4.\n\n\nIn conclusion, although my enthusiasm about having PAC-Bayesian guarantees tailored to FL, I believe this paper needs to be rewritten before its acceptance, given my current concerns about the correctness of Theorem 1 and the shift between the message conveyed by the paper (starting from its title) and the proposed experiments."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1745/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1745/Reviewer_9uEx",
                        "ICLR.cc/2024/Conference/Submission1745/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1745/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698417262438,
            "cdate": 1698417262438,
            "tmdate": 1700214959043,
            "mdate": 1700214959043,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GcCADVzgpb",
                "forum": "kWsJkH1tNi",
                "replyto": "xGP5rFsjTc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1745/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1745/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9uEx"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the time spent on our paper and their interest in the results.\n\n - **Regarding $P_{\\mathbf{S},\\mathbf{W}}$:** The reviewer's understanding is correct. It is a probability distribution over $\\mathcal{W}^{(K+1)R}$. We agree with the reviewer that $\\bigotimes$ is a more accurate notation, that is used in the new version.\n\n- **Regarding the setup:** Please refer to the response to all reviewers.\n\n- **Regarding the aggregation:** First, we mention that there exists a recurrent typo in some of the bounds: $\\overline{W}^{[r-1]}$ should be read as $\\overline{W}^{(r-1)}$, which is corrected in the new version. We are deeply sorry for this. Now, concerning the question of the reviewer (by considering $\\overline{W}^{(r-1)}$): The aggregation function is deterministic. This means that once $W_{[K]}^{(r-1)}$ is known, then $\\overline{W}^{(r-1)}$ is deterministic. However, in general, $W_{[K]}^{(r-1)}$ are random due to the randomness in the way the mini-batches $\\beta_{k,r,t}$ are picked from $S_k^{(r)}$, for all clients/rounds/iterations and also due to initialization randomness. Hence, $\\overline{W}^{(r-1)}$ is also a random variable. Now, regarding the cost of expectation, we agree that if expectation were need to be taken with respect to $\\overline{W}^{[r-1]}$, it would be costly. However, we only need to consider the expectation with respect to $\\overline{W}^{(r-1)}$. If we now consider, for example, the approach of Wang et al. (\"PAC-Bayes Information Bottleneck\", ICLR 2022) on estimating the mutual information or KL divergence terms, then this means that we need an extra bootstrapping phase for $\\overline{W}^{(r-1)}$. An initial idea is to aggregate a subset of the clients' models $\\{W_k^{(r-1)}\\}_{k \\in [K]}$, drawn uniformly at random, and repeat the process a number of times. This is in the same spirit as the bootstrapping (for other variables) in Wang et al. This further investigation, which is indeed interesting, is left for future work.\n\n- **Regarding $\\mathcal{G}_{\\textbf{S}}^{\\delta}$:** We apologize for omitting the definition of  $\\mathcal{G}_{\\textbf{S}}^{\\delta}$ (that contains $\\nu_{\\mathbf{S}}$) in the proof of Theorem 1, which happened after reorganization of that proof. We have added this in the new version.\n\n- **Regarding the subgausiannity:** This is a classical property of subgaussian random variables. Borrowing the reviewer's notation, if $X$ is $\\sigma_{k,r}$-subgaussian, where $\\sigma_{k,r}=\\sqrt{\\frac{R}{4n}}$, then by Theorem 2.6.IV. of Wainwright, 2019, (available at the link provided below), for any $\\lambda_1 \\in [0,1)$, $\\mathbb{E}\\left[e^{\\frac{\\lambda_1 X^2}{2\\sigma_{k,r}^2}}\\right]\\leq \\frac{1}{\\sqrt{1-\\lambda_1}}$. Now, letting $\\lambda_1 =\\frac{2n/R-1}{2n/R}=\\frac{\\lambda}{2n/R}$, we drive that\n$$\\mathbb{E}[e^{\\lambda X^2}]=\\mathbb{E}\\left[e^{\\frac{\\lambda_1 X^2}{2\\sigma_{k,r}^2}}\\right]\\leq \\frac{1}{\\sqrt{1-\\lambda_1}}=\\sqrt{2n/R}\\leq 2n/R.$$\nWe added the reference in the proof for clarity. Moreover, previously for ease of expositions, we had reported the bound with the term $\\log(2n/(R\\delta))$ (and also since this term is rarely a dominant term). In the new version, we present them with the slightly tighter bound $\\log(\\sqrt{2n}/(\\sqrt{R}\\delta))$, for the clarity of the derivation.\n\nThe link: https://www.cambridge.org/core/books/highdimensional-statistics/basic-tail-and-concentration-bounds/30AF7B572184787F4C99715838549721\n\n- **Regarding the experiments:** Please refer to our response to all reviewers."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1745/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700068751008,
                "cdate": 1700068751008,
                "tmdate": 1700068929255,
                "mdate": 1700068929255,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LvZ1BGwWe3",
                "forum": "kWsJkH1tNi",
                "replyto": "GcCADVzgpb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1745/Reviewer_9uEx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1745/Reviewer_9uEx"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response."
                    },
                    "comment": {
                        "value": "I thank the authors for their response, I increased my score to 5.\n\nI believe however that presenting your experimental results with respect to the generalisation gap and not the generalisation risk is misleading as the way curves are presented suggest simply to take $R=0$ to ensure a minimal generalisation gap. However, this is obviously undesirable as a random predictor, can both have a bad generalisation risk and empirical risk. The conveyed message is that we may need to communicate less often, however the ways results are presented suggest not to communicate at all, I believe a clear appearance of the tradeoff in $R$ in the curves is essential."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1745/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700214931698,
                "cdate": 1700214931698,
                "tmdate": 1700214931698,
                "mdate": 1700214931698,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bYigVSAfDv",
                "forum": "kWsJkH1tNi",
                "replyto": "xGP5rFsjTc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1745/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1745/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you much for the feedback and new comments, which are relevant.\n\nThe reviewer is right indeed on that, if one\u2019s focus is only the generalization error then the message would be \u201cjust communicate once\u201d (note the minimum value is $R=1$, not $R=0$), which of course would not be reasonable as small values of $R$ may not be enough to drive the empirical risk into a sufficiently small value. We are aware of this; and did exercise caution on this in the exposition at places of the paper. Also, it is precisely for this reason that all the experimental results that we reported we also provide curves on the evolution of the population risk (which is the true right measure of performance). For example, please see Fig. 3(b) where it is clear that the minimum value of population risk is obtained with $R = 100$ (which is way smaller than the optimum choice of $R$ from an empirical risk perspective (which is $R=3600$), and way bigger than the optimum choice of $R$ if we were to base only on generalization error which is $R=1$). Please note that for FSVM too, we did provide curves of empirical and population risks versus $R$; but for lack of space they were deferred to the appendices (see Fig. 4). Observe that as is clear from those curves, $R=1$ is not enough to get the empirical risk small; and, in fact, in that setup the ERM is minimum for $R=15$. The figure also shows that the population risk decreases way less fast than the EMR; and it barely decreases after $R \\approx 5$.  Other figures in the appendix also convey the same message such as Fig. 5-8. If the reviewer agrees, we can move those to the main body by merging Fig 2 and Fig 4.\n\nWe hope this is now clearer. Nonetheless, based on the reviewer suggestion, we will revisit the exposition to make sure that no single sentence in the material may be misleading on this point."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1745/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700217893914,
                "cdate": 1700217893914,
                "tmdate": 1700408092281,
                "mdate": 1700408092281,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0iy2Sf9FZo",
                "forum": "kWsJkH1tNi",
                "replyto": "bYigVSAfDv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1745/Reviewer_9uEx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1745/Reviewer_9uEx"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your reply. \n\nIn my humble opinion, the clearest way to present your result would be to plot two curves: first, the population risk which satisfies empirically a tradeoff in $R$ as in Fig 3b) and also a new curve empirical risk + bound to see a similar tradeoff appearing in the upper bound. This would emphasise properly the role of $R$."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1745/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700487580841,
                "cdate": 1700487580841,
                "tmdate": 1700487580841,
                "mdate": 1700487580841,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "R0psXFBWOS",
                "forum": "kWsJkH1tNi",
                "replyto": "0JhTHXRD5v",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1745/Reviewer_9uEx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1745/Reviewer_9uEx"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their reply. \n\nI agree that predicting such a value for $R$ is beyond the current scope. However, I believe the paper would be strengthened by showing such a tradeoff in $R$ appears in the theoretical bound (without trying to predict such a value). Indeed, 'showing the increasing behaviour of the generalisation bound' would be more impacting (to me) only if it reflects some similar behaviour for the population risk. Here, you show that your bound follows the same increasing behaviour wrt to $R$ as the generalisation gap, but this increase is not surprising as we start from a predictor uniformly performing poorly on both training and test sets. This generalisation gap does not translate properly the generalisation ability of either FSVM or neural nets as $R=1$ is the optimal value. The neural nets experiments are going in this direction but are not supported by numerical implementations of theoretical results.\n\nI am not saying that controlling the generalisation gap is not interesting, but I am not convinced by the experiments section in its current shape and believe it has to be reworked, therefore I am maintaining my current score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1745/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700650050335,
                "cdate": 1700650050335,
                "tmdate": 1700650050335,
                "mdate": 1700650050335,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QaBXqdIAp1",
            "forum": "kWsJkH1tNi",
            "replyto": "kWsJkH1tNi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1745/Reviewer_jTnd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1745/Reviewer_jTnd"
            ],
            "content": {
                "summary": {
                    "value": "This paper examines the impact of the number of rounds on generalization errors in a federated learning setting. It presents generalization errors in the form of PAC-Bayes bounds and rate-distortion theoretic bounds and applies these results to federated learning support vector machines. The authors argue that the generalization errors increase with more rounds of communication (R)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) It's a really interesting problem. There are many papers studying the convergence properties of FL algorithms, however, little work is done in terms of the generalization of these algorithms.\n\n2) This paper proves three new bounds that explicitly have the number of rounds (R) in their bounds which allows the study of its effect on generalization.\n\n3) They apply the bounds to FL-SVM to get an explicit result and have experiments validating their bound."
                },
                "weaknesses": {
                    "value": "1) The assumption that R < N and each data point is only visited in one round is not realistic and doesn't capture what happens for FL in practice. In most settings in FL, the number of samples per client (N) is small, and the number of rounds is much higher. Additionally, I don't see if the current analysis is possible to extend to these cases.\n\n2) Generally in FL papers with a focus on optimization, they report the curves about the loss and accuracy of the test set during the training, and it's common that the performance on the test set improves over more rounds, so the results can't extend to the more general cases, beyond FL-SVM.\n\n3) The quantity of interest in ML is the true risk (population risk) and not the generalization. Even with the current assumptions, If the speed of decrease of training loss is more than the increase of generalization, it's not possible to argue the smaller number of rounds is better. In Fig 4. and Fig 8. of the appendix we can't see the increase in the population risk with the number of rounds for FL-SVM.\n\n4) As mentioned in the paper, another approach would be to just apply basic PAC-Bayes bounds such as McAllester's without explicitly considering the dynamics of FL. Some numerical comparison of these bounds is needed. It's not clear to me that the bound in theorem 1 gives a better guarantee. McAllester's bound doesn't have the structure of the FL explicitly, however, it would be applied to the output of the algorithm and that might be enough. Also, it doesn't require the assumptions mentioned in the weakness 1.\n\n5) Based on my understanding of the analysis, I assume that the same approach can be applied to the number of local steps per round if each data point would participate in just one local round. As a result, the number of local steps would also appear in the bound. It would be interesting to see its effect, as in the experiments of the paper, the total number of SGD steps is fixed, and with the increase of R, the number of local steps decreases."
                },
                "questions": {
                    "value": "Please discuss the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1745/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1745/Reviewer_jTnd",
                        "ICLR.cc/2024/Conference/Submission1745/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1745/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698800516477,
            "cdate": 1698800516477,
            "tmdate": 1700737299264,
            "mdate": 1700737299264,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Pb3JkMQ58y",
                "forum": "kWsJkH1tNi",
                "replyto": "QaBXqdIAp1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1745/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1745/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer jTnd"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the time spent on our paper and their interest in the problem studied in our paper.\n\nFor the responses regarding the raised concerns 1-3, please refer to our response to all reviewers.\n\n4. Using the classic McAllester's PAC Bayes bound has two major shortcomings. First, considering the lossy version of it (which is more general) for FSVM leads to bounds of order $\\mathcal{O}\\left(\\sqrt{\\frac{B^2 \\log(nK)}{nK\\theta^2}}\\right)$. This bound does not reflect the effect of the number of clients (since it depends only on $nK$, i.e., the total number of samples available, in contrast to our bound that depends on $nK^2$), nor does it include the effect of the number of rounds. More importantly, considering the entire learning algorithm as a single centralized algorithm and applying McAllester's bound results in a bound that can only be computed when all datasets are available on a server, and also only when the final model $\\overline{W}^{(R)}$ is obtained. In contrast, our bound requires only the \"local\" availability of each client's data in each round, and requires only that client's \"local\" model at the end of the corresponding round. This is indeed a realistic assumption, and also helpful for \"estimating\" the total cumulative contributions to the generalization error at each round.\n\n5. We could not unfortunately understand what the reviewer is suggesting. If our interpretation is correct, we recall that each data point is already used in only one local round in our considered setup, since a local dataset is split into $R$ subsets. We would like to kindly ask the reviewer to give us more details about their suggestion."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1745/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700068529882,
                "cdate": 1700068529882,
                "tmdate": 1700068529882,
                "mdate": 1700068529882,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UbXTx2ZnKl",
                "forum": "kWsJkH1tNi",
                "replyto": "Pb3JkMQ58y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1745/Reviewer_jTnd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1745/Reviewer_jTnd"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their response.\n\nI'm not convinced by the explanations regarding the studied setting. Unlike online learning, in Federated Learning (FL), we don't have streams of data, and clients use their samples multiple times. Usually, after many rounds of communication, both the training and test errors continue to decrease, especially when each sample can be used multiple times.\n\nAs a consequence of this setup, an increase in $R$ means a decrease in samples per round to $\\frac{n}{R}$. I think the effect of the smaller number of samples per round is more significant for these results than the effect of the number of rounds.\n\nRegarding 5: Consider the scenario where, in each global round, every client performs $e$ local steps, and each sample appears in only one local step (thus, each client has $Re$ disjoint subsets of $\\frac{n}{Re}$ samples). If we apply the same analysis here, which factors in an explicit effect of $e$, can we expect a similar conclusion? (That the increase in $Re$ worsens the generalization.)"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1745/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700499226087,
                "cdate": 1700499226087,
                "tmdate": 1700499226087,
                "mdate": 1700499226087,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EIJfRzbMkg",
                "forum": "kWsJkH1tNi",
                "replyto": "NHdG5tkIDa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1745/Reviewer_jTnd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1745/Reviewer_jTnd"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their reply. I increase my score to 5.\n\nI still think more study about the effect of the assumption is needed. For example, some experiments where each sample is used in a fixed number of rounds, to study the effect of the number of samples vs the effect of number of rounds. Also, a clear study and clarification of the trade-off, the conclusion and the results in Fig.4 and Fig.8 is needed."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1745/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737286538,
                "cdate": 1700737286538,
                "tmdate": 1700737286538,
                "mdate": 1700737286538,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]