[
    {
        "title": "Adversarial Machine Learning in Latent Representations of Neural Networks"
    },
    {
        "review": {
            "id": "gx6mkSLz4P",
            "forum": "hgrZluxFC7",
            "replyto": "hgrZluxFC7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5558/Reviewer_4MyR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5558/Reviewer_4MyR"
            ],
            "content": {
                "summary": {
                    "value": "This paper uses information bottleneck (IB) analysis to study adversarial robustness in the split learning setting, consisting of a data encoder that gives the latent representations, followed by a local deep neural network (DNN) that takes the latent presentation for subsequent inference. The results show that the compressed latent representations can reduce the success rate of adversarial attacks, as also indicated by the theory."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Use IB to study adversarial robustness is a good angle.\n2. The paper is well-written and easy to read"
                },
                "weaknesses": {
                    "value": "I have several major concerns about the technical novelty and empirical evaluation.\n\n1. On the claim that \"assuming the same level of information distortion, latent features are always more robust than input representations\", does this hold even if the latent embedding dimension is larger than the input dimension?  If yes, why it can be more robust? Moreover, if the latent embedding dimension is lower than the input dimension, then it has been proven in the ICML 2019 paper \"First-order Adversarial Vulnerability of Neural Networks and Input Dimension\" that the minimal adversarial perturbation scales inversely with the data dimension. So if we treat the latent representations as the new \"data\" and they have lower dimensions than the raw inputs, the new insights are not clear.\n\n2. The assumption on the same level of information distortion seems very strong and lacks justification. It's also not clear what is the threat model (what the attacker can do) that leads to a latent representation $T_{adv}$.\n\n3. The evaluated attacks are naive input perturbation attacks, and no adaptive attacks that take into account modifying the latent representations were studied. It should be easy to add a regularizer to attack objectives to encourage finding adversarial examples that share very similar (or very different) latent representations as the original data, and therefore the claim on improved robustness may not hold."
                },
                "questions": {
                    "value": "1. W.r.t. to W1, is there any implicit assumption that the latent embedding dimension is smaller than the input dimension? If so, the improved robustness is a direct consequence of the ICML 2019 result.\n\n2. How to justify the assumption of \"the same level of information distortion\"? Does $T_{adv}$ hold for any arbitrary threat model? \n\n3. Does the result of improved robustness still hold against adaptive attacks, where the attacker can have access to the latent representations?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5558/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698368366567,
            "cdate": 1698368366567,
            "tmdate": 1699636571726,
            "mdate": 1699636571726,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HsSx69WosA",
                "forum": "hgrZluxFC7",
                "replyto": "gx6mkSLz4P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5558/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5558/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## **Regarding Weakness 1 & Question 1** \n\n> On the claim that \"assuming the same level of information distortion, latent features are always more robust than input representations\", does this hold even if the latent embedding dimension is larger than the input dimension?\n\nWe thank the reviewer for this comment. To this end, we remark that Theoretical Finding #2 indeed holds when the latent embedding dimension is larger than the input dimension. Our experiments shown in Figure 4 include the original VGG and ResNet, where the space is respectively $\\times 5.33$, $\\times 2.66$ larger.\n\nTo better prove our point, we are actively working on a new experiment with respect to the depth of latent space in the revision. We present some preliminary results in Table 1 and Table 2.\n\n---\n\n**Table 1: Adversarial robustness of ResNet152 as a function of depth with perturbation budget $\\epsilon = 0.003$**\n\n| Attack | Input | FeatureMap0 | FeatureMap1 | FeatureMap2 | FeatureMap3 | FeatureMap4 |\n| :-------: | :-------: | :------: | :-------: | :-------: | :-------: | :--------: |\n| dimension | x1 | x5.33 | x5.33 | x2.66 | x1.33 | x0.66 |\n| FGSM | 71.9% | 53.6% | 53.1% | 35.8% | 34.9% | 1.6% |\n| PGD | 88.1% | 65.5% | 66.3% | 43.1% | 43.2% | 1.6% |\n\n---\n\n**Table 2: Adversarial robustness of VGG16 as a function of depth with perturbation budget $\\epsilon = 0.003$**\n\n| | Input | FeatureMap0 | FeatureMap1 | FeatureMap2 | FeatureMap3 | FeatureMap4 |\n| :-------: | :-------: | :------: | :-------: | :-------: | :-------: | :-------: |\n| dimension | x1 | x5.33 | x2.66 | x1.33 | x0.66 | x0.33 |\n| FGSM | 94.5% | 41.2% | 28.7% | 20.5% | 14.0% | 6.4% |\n| PGD | 98.8% | 50.3% | 33.2% | 22.3% | 14.5% | 6.5% |\n\n---\n\nThe experiment is based on a smaller model VGG16 and a deeper model ResNet152. Same level of $l_\\infty$ norm distortions are introduced to all feature maps before max pooling layers. As shown in Table 1 and Table 2, all the latent feature maps are more robust than the input regardless of dimensionality and depth. Furthermore, ASR reduces with respect to the depth of latent space, which is in line with our Theoretical Finding #2.\n\n\n>  If yes, why it can be more robust?\n\nThis finding holds because it is derived directly from the chain rule of mutual information which does not depend on cardinality $|\\mathcal{T}|$. \n\n> it has been proven in the ICML 2019 paper \"First-order Adversarial Vulnerability of Neural Networks and Input Dimension\" that the minimal adversarial perturbation scales inversely with the data dimension.\n\nWe thank the reviewer for bringing [1] to our attention. We want to address the key difference between the work [1] and our Theoretical Finding #1. In our Theoretical Finding #1, the robustness is jointly determined by its upper bound $I^*(Y;T)$ and $\\mathcal{O}( |\\mathcal{T}||\\mathcal{Y}| / \\sqrt{n})$. Although $\\mathcal{O}( |\\mathcal{T}||\\mathcal{Y}| / \\sqrt{n})$ indicates that a larger cardinality will result in less robustness, which is in line with [1], $T$ is a DNN-specific space depending on the DNN  architecture design. In stark opposition, Theoretical Finding #1 addresses the variance-bias tradeoff in the distributed DNN design, which is also supported by our experiments as shown in Figure 6. Moreover, it also explains why latent space is more robust even if it has larger cardinality, since the latent robustness depends both on the cardinality and $I^*(Y;T)$.\n\n## **Regarding Weakness 2 & Question 2** \n\n> The assumption on the same level of information distortion seems very strong and lacks justification\n\nWe thank the reviewer for this insightful comment. We remark that such constraints on information distortion are necessary to formally analyze robustness of distributed DNNs and provide a fair comparison with attacks in the input space. Moreover, although there is much work focused on unconstrained attacks \u2013  such as [2, 3, 4] \u2014,  the constrained distortions are still a popular way to evaluate robustness [5, 6]. Since this is the first work to investigate this topic, we aim at formally analyzing the robustness with information constraints. However, we also point out that in real-world scenarios the attacks in the latent space can have a different level of information than the input. We will clarify this crucial aspect in the final version of the paper.\n \n> It's also not clear what is the threat model (what the attacker can do) that leads to a latent representation $T_{adv}$\n\nIn a distributed DNN scenario, latent representations are sent to the local DNN that can be exposed to attackers. We refer the reviewer to Figure 1 for a visual depiction of the attack. We assume that attackers can directly add perturbations to latent representations in the same manner as attacks performed in the input space. Thus, $T_{adv}$ denotes the adversarial latent representation generated by attackers. For a detailed attack model formulation, we kindly refer the reviewer to Section 4.1."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5558/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700276399718,
                "cdate": 1700276399718,
                "tmdate": 1700317744534,
                "mdate": 1700317744534,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cOeQ5P1kST",
                "forum": "hgrZluxFC7",
                "replyto": "gx6mkSLz4P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5558/Reviewer_4MyR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5558/Reviewer_4MyR"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the reviewer for providing the responses. Although some of my concerns have been partially addressed, some major concerns remain:\n1. Lack of robustness evaluation on adaptive attacks - without this experiment, it's hard to argue, even numerically, that the analysis and the assumptions hold in practice. As I pointed out, many earlier defenses based on latent representation regularization were later broken by adaptive attacks. This setting should apply to the considered distributed DNN setting as well.\n2. I appreciate the new experiments from authors. However, it is implemented with a very small $\\epsilon$ budget and on simple attacks like FGSM and PGD. It would be more convincing to try on larger $\\epsilon$ values with AutoAttack."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5558/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700597500631,
                "cdate": 1700597500631,
                "tmdate": 1700597500631,
                "mdate": 1700597500631,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "05NCWzcSJc",
                "forum": "hgrZluxFC7",
                "replyto": "5uJqWC672G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5558/Reviewer_4MyR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5558/Reviewer_4MyR"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors again for the clarification. I agree that the proposed setting is not a defense, but a claim that the considered distributed DNN setup will enhance robustness.\n\nHowever, the reason I brought this up is that in this setting, knowing partial information about the embedding is similar to the construction of adaptive attacks. One cannot guarantee that having less information about the entire network will automatically yield improved robustness, especially since the theory relies on the strong assumption of information bottleneck. Adaptive attacks are by far the easiest way I can think of to support the theoretical claim. For example, one can generate adversarial examples that make the local embeddings as close as possible to the original samples, etc.\n\nOn attack evaluation, following the norm of adaptive attacks, I also suggest the authors show a complete robustness accuracy versus attack strength ($\\epsilon$) plot. This is also strong empirical evidence for inspecting the robustness claims. See https://arxiv.org/abs/1902.06705 for more details."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5558/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621384413,
                "cdate": 1700621384413,
                "tmdate": 1700621384413,
                "mdate": 1700621384413,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EfCVGsgeqf",
            "forum": "hgrZluxFC7",
            "replyto": "hgrZluxFC7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5558/Reviewer_nH51"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5558/Reviewer_nH51"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the adversarial robustness of deep neural networks (DNNs) in latent space and compares it with the more common adversarial attacks on DNN inputs. The target models are distributed DNNs where the network is splitted in two parts, one is running on a mobile device and sends the output features to other part of the network which is running in cloud. The authors build on top of Information Bottleneck (IB) theory and show that 1) latent features of DNNs are more robust to adversarial attacks than inputs and 2) the smaller the latent dimension, the more difficult it is for an adversary to attack the network successfully. Their results on a wide variety of attacks and networks support the theoretical hypothesis."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is easy to follow, has good background sections and exploits the work of Shamir et al. 2010 (equation 3) in a natural way. The experiments are performed on a wide range of adversarial attacks, such as gradient-based, score-based and decision based, as well as white-box and black-box attacks. The results are clear and in line with the theoretical hypothesis."
                },
                "weaknesses": {
                    "value": "There are a few weaknesses that I would like to point out:\n1. the paper does not take into consideration attacking the latent representations of a DNN that was adversarially trained. To the best of my knowledge, the adversarial training [1] has an impact over the latent representations of a neural network and under certain settings (a perturbation budget $\\epsilon$ not too large) the network is robust to adversarial input perturbations. I would be interested in the robustness of adversarially trained networks, would it be possible to perform such an experiment? I believe the paper is not complete without this experiment and I would really appreciate if you could include this. \n\nReferences:\n\n[1] **TOWARDS DEEP LEARNING MODELS RESISTANT TO ADVERSARIAL ATTACKS**, available at **https://openreview.net/forum?id=rJzIBfZAb**"
                },
                "questions": {
                    "value": "1. Related to the weakness point: how do you think the adversarial training would change the success rate of these attacks?\n2. recently there was another adversarial attack introduced for slightly more particular DNNs architectures with multi-exits (or early-exits), called DeepSloth [2], which aims to make the early-exits ineffective. They show that this attack changes the latent representation of DNNs (for example, Figure 3 in the paper) to actually create delay.\n3. Did think about analyzing the latent features robustness in LLMs? What do you think the challenges would be?\n4. In Section 6 you mention about defense mechanisms for adversarial attack on latent features. What would be the key element in designing defenses for this attack?\n\nReferences:\n\n[2] **A Panda? No, It's a Sloth: Slowdown Attacks on Adaptive Multi-Exit Neural Network Inference**, available at **https://openreview.net/forum?id=9xC2tWEwBD**"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5558/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5558/Reviewer_nH51",
                        "ICLR.cc/2024/Conference/Submission5558/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5558/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698602388842,
            "cdate": 1698602388842,
            "tmdate": 1700518558538,
            "mdate": 1700518558538,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7BR13fbynb",
                "forum": "hgrZluxFC7",
                "replyto": "EfCVGsgeqf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5558/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5558/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## **Regarding Weakness 1 & Question 1** \n\n> The paper does not take into consideration attacking the latent representations of a DNN that was adversarially trained.\n\nWe thank the reviewer for pointing this out. We have developed a new experiment to evaluate the input and latent robustness of two adversarial training approaches presented in recent work [1] and [2] in Table 1. \n\n---\n\n**Table 1: Robustness of ResNet50, DAT [1] and FastAT [2] with perturbation budget $\\epsilon = 0.003$**\n\n| | PGD | | N-Attack | | Triangle | | \n| :------- | :-------: | :------: | :-------: | :-------: | :-------: | :--------: |\n| Model | Input | Latent |  Input | Latent | Input | Latent |\n|ResNet50 | 92.9% | 62.9% | 61.1% | 45.5% | 85.8% | 13.0% |\n| DAT | 14.6% | 1.0% | 54.5% | 0.0% | 53.0% | 1.0% |\n| FastAT | 7.9% | 3.1% | 43.3% | 1.0% | 29.6% | 1.1% |\n\n---\n\nIn Table 1, DAT and FastAT are pretrained ResNet50 with different adversarial training approaches. We evaluate the robustness in both input and latent space as we did with the original ResNet50. The results show that latent representations of adversarially trained DNNs are also more robust than input, which is in line with our theoretical findings.\n\n\n> how do you think the adversarial training would change the success rate of these attacks?\n\nWe have evaluated this aspect from an experimental standpoint in the table above. From a theoretical perspective, while the adversarial training should be able to improve the robustness in both input and latent space to some extent, it will not affect our Theoretical Finding #2. Indeed, in our Theoretical Finding #1, the upper bound is defined by a term $\\mathcal{O}( |\\mathcal{X}||\\mathcal{Y}| / \\sqrt{n})$, where n is the number of samples. Adversarial training actually introduces more samples, resulting in a smaller $\\mathcal{O}( |\\mathcal{X}||\\mathcal{Y}| / \\sqrt{n})$ as well as a larger upper bound in robustness. We also point out that our Theoretical Finding #2 holds irrespective of the sample size n, therefore adversarial training will not impact this finding. We are going to include these explanations in the final version of the manuscript. \n\n## **Regarding Question 2** \n\n> Recently there was another adversarial attack introduced for slightly more particular DNNs architectures with multi-exits (or early-exits)\n\nWe thank the reviewer for bringing [3] to our attention. We point out that [3] can be explained with information bottleneck theory which is related to our Theoretical Finding #2. We explain it as follow:\n\nIt is pointed out in [3] that conventional adversarial attacks cannot slow down the early-exit networks.  Since we proposed to measure distortion with residual information $I(X;Y|T)$, the residual information of adversarial samples $I(X_{adv};Y|T\u2019)$ will be larger than the residual information of benign samples $I(X;Y|T)$, i.e.,\n\n$I(X_{adv};Y|T') \\geq I(X;Y|T)$\n\nFrom Equation 8 in our paper,  it follows that\n\n$I(X;Y) - I(X_{adv};Y) \\leq I(T;Y) - I(T\u2019;Y).$\n\nIn other words, the information loss between latent $T\u2019$ and ground-truth $Y$ is larger than the information loss between input $X_{adv}$ and $Y$. Thus, a small information loss gets amplified in hidden layers, which is consistent with Figure 3 in [3]. \n\nIt follows that in early-exit models, the adversarial samples and benign samples will have less distortion in early layers, and thus early exits can be activated.\n\nWe thank the reviewer for pointing out [3] which could help us enhance our theoretical aspect. We are working on a revision to include [3] and the discussion."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5558/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700275679637,
                "cdate": 1700275679637,
                "tmdate": 1700275679637,
                "mdate": 1700275679637,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XDC3a8sfAr",
                "forum": "hgrZluxFC7",
                "replyto": "EfCVGsgeqf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5558/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5558/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## **Regarding Question 3** \n\n> Did you think about analyzing the latent features robustness in LLMs? \n\nWe believe the adversarial robustness of latent features in LLMs is an intriguing and extremely timely topic, and we are planning to have a separate investigation into this matter. \n\n> What do you think the challenges would be?\n\nWe believe one big challenge could be the intrinsic difference between textual and image data. For example, textual data are non-differentiable due to their semantic nature, while image data are differentiable RGB values. As a result, the problem formulation cannot be directly applied to LLMs.\n\n\n## **Regarding Question 4**\n\n> What would be the key element in designing defenses for this attack?\n\nWe believe the latent representations intrinsically have more semantic information compared to input. And there is evidence that adversarial input and benign input have different distributions in hidden layers [4]. We believe this can also be generalized to latent representations. In order to develop defense specifically for latent representations, we believe one key element is to understand the semantic meaning of the distribution change caused by adversarial perturbations. \n\n---\n\n### **Reference**:\n\n[1] Zhang, Gaoyuan, et al. \"Distributed adversarial training to robustify deep neural networks at scale.\" Uncertainty in Artificial Intelligence. PMLR, 2022.\n\n[2] Wong, Eric, Leslie Rice, and J. Zico Kolter. \"Fast is better than free: Revisiting adversarial training.\" arXiv preprint arXiv:2001.03994 (2020).\n\n[3] Hong, Sanghyun, et al. \"A panda? no, it's a sloth: Slowdown attacks on adaptive multi-exit neural network inference.\" arXiv preprint arXiv:2010.02432 (2020).\n\n[4] Galloway, Angus, et al. \"Batch normalization is a cause of adversarial vulnerability.\" arXiv preprint arXiv:1905.02161 (2019)."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5558/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700275729568,
                "cdate": 1700275729568,
                "tmdate": 1700276326052,
                "mdate": 1700276326052,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Gh9cc0RwSu",
                "forum": "hgrZluxFC7",
                "replyto": "EfCVGsgeqf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5558/Reviewer_nH51"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5558/Reviewer_nH51"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank authors for addressing my comments.\n\n**About Table 1.** I think these results should be added to the paper because I believe they will add value to the paper for the simple fact that attacks on latent features are even less successful than attacks on inputs for adversarially trained networks.\n\nI agree with the authors' explanations to my questions, they are intuitive and I really appreciate the effort you put into this. As a result, I will increase my score to 8."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5558/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700518602752,
                "cdate": 1700518602752,
                "tmdate": 1700518633227,
                "mdate": 1700518633227,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DyrUDxGQ15",
            "forum": "hgrZluxFC7",
            "replyto": "hgrZluxFC7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5558/Reviewer_z2Hg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5558/Reviewer_z2Hg"
            ],
            "content": {
                "summary": {
                    "value": "A distributed DNN can be regarded as a combination of two parts, namely, a mobile DNN and a local DNN, respectively. The mobile DNN is trained to learn the latent representations, which can reduce the amount of data that will be transmitted but suffers the risk of being attacked. Along this side, this paper investigates the robustness of the latent representations. Based on information theory, this paper claims that: 1) latent features are always more robust than input representations, and 2) the feature dimensions and the generalization capability of the DNN determine the adversarial robustness. Extensive experiments on ImageNet-1K are conducted to support the claims, considering 6 different DNN architectures, 6 different ways for distributed DNN, and 10 different adversarial attacks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper is well-written, with clear explanations and illustrations. Section 2 is comprehensive, and someone interested in those related topics can learn from the article.\n2. Based on Information theory, Section 3 provides a thorough theoretical analysis. Besides, the theoretical conclusions are supported by the experimental results in Section 4 with detailed experimental settings."
                },
                "weaknesses": {
                    "value": "In the Conclusion section, this article claims that ``This paper has investigated adversarial attacks to latent representations of DNNs for the first time``. Through the lens of distributed DNNs, this work may be the first one, as it claims. However, I am concerned about whether the distributed DNNs are a necessary background as the motivation to investigate the problem. Since I think the local DNN and mobile DNN are very like the architecture of an autoencoder, and there are many works about the adversarial robustness of autoencoders."
                },
                "questions": {
                    "value": "As mentioned in the Weaknesses, my questions/concerns are mainly about the differences between the distributed DNNs and the autoencoders.\n1. If we compare the architecture between a distributed DNN and an autoencoder, I think the local DNN is very much like the encoder part, and the mobile DNN is very much like the decoder part. Can I compare them like this?\n2. If yes, I think some works have studied the adversarial robustness of the latent features, e.g., [1]. \n3. Therefore, I am a bit curious about whether distributed DNNs are a necessary background as the motivation to investigate the adversarial robustness of the latent features.\n\n---\n[1] Espinoza-Cuadros, F. M., Perero-Codosero, J. M., Ant\u00f3n-Mart\u00edn, J., & Hern\u00e1ndez-G\u00f3mez, L. A. (2020). Speaker de-identification system using autoencoders and adversarial training. arXiv preprint arXiv:2011.04696."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5558/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698770191987,
            "cdate": 1698770191987,
            "tmdate": 1699636571496,
            "mdate": 1699636571496,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NjtWXGVFiL",
                "forum": "hgrZluxFC7",
                "replyto": "DyrUDxGQ15",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5558/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5558/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## **Regarding Weakness & Questions**\n> However, I am concerned about whether the distributed DNNs are a necessary background as the motivation to investigate the problem. Since I think the local DNN and mobile DNN are very like the architecture of an autoencoder, and there are many works about the adversarial robustness of autoencoders.\n\nWe agree with the reviewer that distributed DNNs have an encoder-decoder structure similar to autoencoders, and we appreciate the reviewer pointing out adversarial autoencoders. However, there is a stark distinction between robust distributed DNNs and robust autoencoders. In robust autoencoder, adversarial training is used to enforce the encoder to learn adversarially robust latent representations \u2013 for example, for speaker de-identification [1] or outlier detection [2]. Such latent representations are not considered to be exposed to attackers thus the \u201crobustness\u201d in the robust autoencoder context is actually \u201csemantically meaningful representations\u201d. \n\nIn our case, we consider attacks adding perturbations directly to the latent space. The scenario considered is extremely relevant to mobile scenarios since distributed DNNs have been proven to be extremely effective in reducing network load while preserving accuracy [3, 4, 5]. In this case, latent representations are exposed to the attacker, which can lead to performance degradation. To this end, the \u201crobustness\u201d in latent representation means \u201cadversarially robust to attacks\u201d, which is a remarkably different concept than robust autoencoders. \n\nTo the best of the authors\u2019 knowledge, there is no such prior work to investigate adversarial perturbation in latent representation. Thus, we would like to highlight the significance and timeliness of our work in the context of distributed DNN.\n\nWe appreciate the constructive feedback to help us clarify our novelty and contribution. We are actively working on a revision that includes the suggested work [1] and clarification.\n\n---------------------------------\n\nReference:\n\n[1] Espinoza-Cuadros, Fernando M., et al. \"Speaker de-identification system using autoencoders and adversarial training.\" arXiv preprint arXiv:2011.04696 (2020).\n\n[2] Salehi, Mohammadreza, et al. \"Arae: Adversarially robust training of autoencoders improves novelty detection.\" Neural Networks 144 (2021): 726-736.\n\n[3] Matsubara, Yoshitomo, et al. \"Bottlefit: Learning compressed representations in deep neural networks for effective and efficient split computing.\" 2022 IEEE 23rd International Symposium on a World of Wireless, Mobile and Multimedia Networks (WoWMoM). IEEE, 2022.\n\n[4] Matsubara, Yoshitomo, et al. \"Supervised compression for resource-constrained edge computing systems.\" Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2022.\n\n[5] Shao, Jiawei, and Jun Zhang. \"Bottlenet++: An end-to-end approach for feature compression in device-edge co-inference systems.\" 2020 IEEE International Conference on Communications Workshops (ICC Workshops). IEEE, 2020."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5558/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700275407773,
                "cdate": 1700275407773,
                "tmdate": 1700275407773,
                "mdate": 1700275407773,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UlJt42mVZQ",
                "forum": "hgrZluxFC7",
                "replyto": "NjtWXGVFiL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5558/Reviewer_z2Hg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5558/Reviewer_z2Hg"
                ],
                "content": {
                    "title": {
                        "value": "Thanks to the Authors for the Further Explanation"
                    },
                    "comment": {
                        "value": "Thanks to the authors clarifying the differences between the distributed DNNs and autoencoders. I would like to keep my original score of 6."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5558/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635998480,
                "cdate": 1700635998480,
                "tmdate": 1700635998480,
                "mdate": 1700635998480,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7gfuPBGCXx",
            "forum": "hgrZluxFC7",
            "replyto": "hgrZluxFC7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5558/Reviewer_an5M"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5558/Reviewer_an5M"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the robustness of distributed DNN against adversarial attack theoretically and experimentally. The authors analyze the robustness of latent features using information bottleneck theory and prove that latent space perturbations are always less effective than input space perturbations. Empirically, the authors conduct extensive experiments to verify their theoretic findings from multiple perspectives."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well written in general. The theoretical analysis from information bottleneck perspective is interesting and solid. Most of the claims are supported by ample experimental analysis."
                },
                "weaknesses": {
                    "value": "1. In this paper, the attacker only has access to the latent representation provided by the mobile DNN from a mobile phone. While the authors successfully demonstrate that attacks on these latent representations exhibit a lower Attack Success Rate (ASR) than those on raw images\u2014given the same level of information distortion\u2014the appropriateness of imposing an identical distortion level in this context needs more clarification. In discussions around the adversarial robustness of image input, constraints are placed on distortion levels to ensure modifications are imperceptible to the human eye, yet potent enough to deceive the classifier [1]. However, when considering distortions applied to latent features, the rationale for enforcing the same constraint level is less clear. Given that the transmission in this scenario occurs between a mobile device and a cloud computer, with no human observer in the loop, the attacker might as well nullify all latent features, potentially achieving an ASR close to 100%. Comparing the ASR between attacks on raw input images (which are observable by humans) and attacks on latent features (processed by a cloud computer) under an equivalent distortion level seems illogical.\n\n2. Although the authors posit that this study on the robustness of distributed DNNs in the face of adversarial actions is novel, I find the concept markedly similar to existing works on attacks targeting intermediate layers or latent features [2]. I would appreciate it if the authors could highlight the distinctions between their work and prior research, and attempt to apply the attack methods delineated in [2] where feasible.\n\n3. I am confused about two terminologies in the paper: feature compression and bottleneck. Their relationship is not clear to me. In Table 1, it seems that only the first two feature compression methods contain a bottleneck layer. However, in Section 4.2 - DNN architectures, the authors write, '...the feature compression layer (i.e., the 'bottleneck').' Additionally, the authors use the same bottleneck design as Matsubara et al. (2022b) and denote the new architectures with feature compression as Resnet50-fc, etc. However, the specific design mentioned in Matsubara et al. (2022b) does not seem to belong to any of the six feature compression approaches in Table 1. Table A-3 is also confusing, as I cannot understand what the authors mean by 'JC and QT are DNNs without a bottleneck,' while JC and QT are feature compression approaches, and the authors claim that feature compression is the same as bottleneck.\n\n4. The experimental results in Section 5.3 need further explanation. ResNet152-fc with 12 channels achieves a validation accuracy of 77.47%, while ResNet152-fc with 3 channels achieves 70.01% accuracy. On the middle of page 9, the statement 'decreases to 7.47%' should be corrected to 'decreases by 7.46%.' However, the fact that I\u2217(Y ; T) decreases by 7.46% cannot explain why the ASR increases by a much larger percentage than 7.46% in Table A-4. For example, when \\epsilon=0.003, the ASR of PGD_2 increases by 19.9% when transitioning from 12 channels to 3 channels. According to the inequality in Key Theoretical Finding #1, since O(|T||Y|/\u221an) is smaller when transitioning from 12 channels to 3 channels, the ASR difference should be less than the difference in I\u2217(Y ; T), which is 7.46%. Please provide a detailed explanation.\n\n[1] Kurakin, Alexey, Ian J. Goodfellow, and Samy Bengio. \"Adversarial examples in the physical world.\" Artificial intelligence safety and security. Chapman and Hall/CRC, 2018. 99-112.\n[2] Yu, Yunrui, Xitong Gao, and Cheng-Zhong Xu. \"Lafeat: Piercing through adversarial defenses with latent features.\" CVPR. 2019."
                },
                "questions": {
                    "value": "My primary concern is related to the validity of the problem setting presented in this paper (See weakness 1). Although the theoretical findings are intriguing and the experimental data is comprehensive, there is still uncertainty regarding the significance of defining the robustness of distributed Deep Neural Networks (DNNs) in the proposed manner."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5558/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5558/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5558/Reviewer_an5M"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5558/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698843063111,
            "cdate": 1698843063111,
            "tmdate": 1700526334358,
            "mdate": 1700526334358,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9fFVBwstIA",
                "forum": "hgrZluxFC7",
                "replyto": "7gfuPBGCXx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5558/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5558/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## **Regarding Weakness 1**\n\n> the appropriateness of imposing an identical distortion level in this context needs more clarification\n\nWe thank the reviewer for this insightful comment. We point out that estimating the distortion is necessary to formally compare the robustness of the DNN against adversarial attacks in the latent space with respect to attacks in the input space. Specifically, to compare input space and latent space attacks, we assume an identical adversarial effort in both input and latent space through the $l_p$ norm constraint. We point out that the $l_p$ norm is routinely used as a metric to characterize such effort, as it is used in existing work, for example [1, 2, 3]. Moreover, we also point out that in real-world scenarios the attacks in the latent space can (and most likely, will) have a different level of information than attacks in the input space. We will clarify this crucial aspect in the final version of the paper.\n\n> with no human observer in the loop, the attacker might as well nullify all latent features, potentially achieving an ASR close to 100%\n\nWe strongly agree with the reviewer that without a human observer in the loop, unconstrained perturbations in latent space can lead to ~100% ASR. However, we also believe latent representations intrinsically contain more semantic information than the input, which may result in easier outlier detection. For example, image data usually has redundant information such as background. Therefore, changing one pixel in the background might not be perceptible by humans. Conversely, since semantic representations capture only the sufficient statistics of the input, changing one element of them can result in invalid interpretations (take one-hot encoding as an extreme case example). As such, an outlier detection system which can estimate the perturbation magnitude in latent space can play a similar role as the human observer in input space. We will definitely delve deeper into this intriguing aspect in our future work as it deserves a separate and exhaustive investigation. \n\n## **Regarding Weakness 2** \n\n> I would appreciate it if the authors could highlight the distinctions between their work and prior research\n\nWe appreciate the reviewer bringing [4] to our attention. We point out that paper [4] uses the latent representation as a help to craft the adversarial input. In stark opposition, our work focuses on the perturbation of the latent space itself. We are working on a revision to address the distinction between our work and suggested work [4].\n\n> and attempt to apply the attack methods delineated in [4] where feasible.\n\nWe thank the reviewer for suggesting to compare our paper to [4], which we definitely plan to do in future work.\n\n## **Regarding Weakness 3**\n\n> I am confused about two terminologies in the paper: feature compression and bottleneck.\n\nWe are sorry for the confusion. In the literature, we refer to \u201cbottleneck\u201d as a neural network architecture that can be trained in different ways to reduce the dimensionality of feature representations. For this reason, we present different training strategies for bottlenecks \u2013 Supervised Compression (SC), Knowledge Distillation (KD), Entropic Student (ES) and BottleFit (BF).\n\nConversely, other feature compression approaches such as JEPG Compression (JC) and Quantization (QT) compress features by digitizing the feature map in its frequency domain and original domain respectively. These approaches compress the data size but are not able to reduce the dimensionality of the latent space.\n\nIn short, feature compression can be done in different ways, compressing the dimensionality (bottleneck) and compressing the data size (JC and QT).\n\n> Additionally, the authors use the same bottleneck design as Matsubara et al. (2022b) and denote the new architectures with feature compression as Resnet50-fc, etc. However, the specific design mentioned in Matsubara et al. (2022b) does not seem to belong to any of the six feature compression approaches in Table 1.\n\nThank you for your remark. Since there can be different bottleneck architectures, we choose the same architecture that is used in [5] which is a benchmark paper. Notice that similar to our work, [5] did not present the architecture but different training strategies as different approaches. However, in their codebase, they use a consistent bottleneck design.\n\n> Table A-3 is also confusing, as I cannot understand what the authors mean by 'JC and QT are DNNs without a bottleneck,' \n\nWe are sorry for the confusion. Since JC and QT are not related to the DNN architecture, such approaches can be add-ons for any DNN with or without bottleneck to reduce the communication overhead. However, as we want to verify the point that dimensionality helps to enhance latent robustness, we choose JC and QT without bottlenecks in our experiment for comparison."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5558/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700275246992,
                "cdate": 1700275246992,
                "tmdate": 1700275246992,
                "mdate": 1700275246992,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7Q1l1obs1I",
                "forum": "hgrZluxFC7",
                "replyto": "7gfuPBGCXx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5558/Reviewer_an5M"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5558/Reviewer_an5M"
                ],
                "content": {
                    "title": {
                        "value": "Increase my score 6"
                    },
                    "comment": {
                        "value": "I appreciate the detailed explanations and great efforts made by the authors. Thus, I increased my score to 6. However, as mentioned in the reply, the explanations need numerical experiments to support the claims, which can give strong motivation of this work."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5558/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700526529581,
                "cdate": 1700526529581,
                "tmdate": 1700526529581,
                "mdate": 1700526529581,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GuzsN4s6ek",
                "forum": "hgrZluxFC7",
                "replyto": "40HmhLd2oI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5558/Reviewer_an5M"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5558/Reviewer_an5M"
                ],
                "content": {
                    "title": {
                        "value": "Keep my score"
                    },
                    "comment": {
                        "value": "Thanks for providing the preliminary experiments. As more comprehensive experiments are needed, I keep my current score."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5558/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700611802676,
                "cdate": 1700611802676,
                "tmdate": 1700611802676,
                "mdate": 1700611802676,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gj4mn00z9J",
            "forum": "hgrZluxFC7",
            "replyto": "hgrZluxFC7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5558/Reviewer_CQZD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5558/Reviewer_CQZD"
            ],
            "content": {
                "summary": {
                    "value": "This work aims to evaluate the adversarial robustness of distributed DNNs. In this setting, latent representations of the DNNs are communicated among devices, and thus, an adversary could perturb the latent representations instead of the model's input. However, the work claims that attacks on latent representations are less effective than perturbing the input and presents theoretical information bounds supporting this claim. Standard adversarial attacks are then used to test this bound empirically over several distributed DNN architectures, and the results show that the attacks are less successful when employed on latent spaces than on the model's input."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The suggested information bound entails that standard adversarial attacks targeting only the latent representation of DNNs would be less effective than those targeting the input. This is relevant not only in distributed DNN but for side-channel attacks as well. Moreover, it greatly aids in evaluating the robustness of distributed DNN, as such attacks arise naturally in this setting."
                },
                "weaknesses": {
                    "value": "1. The experimental setup is lacking and insufficient to support the authors' claims. Only attacks on several distributed DNN architectures were reported. Not only is this insufficient to support the claim that attacks on latent spaces are generally less effective, but it does not explain the phenomenon or the behavior of the suggested information bound.\n2. No novel attacks targeting latent spaces were suggested, or even settings in which both the input and latent space are attacked. Without testing such attacks and settings, the robustness of distributed DNN cannot be correctly evaluated.\n3. The second key finding of \"DNN robustness is intrinsically related to the cardinality of the latent space\" is not a phenomenon exclusive to latent spaces. There are several examples of attacks working better on larger input samples such as Imagnet, compared to CIFAR10/100. In addition, the effect of the l_inf norm bound is highly dependent on the input size."
                },
                "questions": {
                    "value": "1. For a correct evaluation of the suggested bound on a given DNN architecture, the experimental settings should present attacks on all the latent spaces in the architecture and not only those available in specific distributed DNN settings. The results should be compared for the depth of the latent spaces in the network and their cardinality. Such experiments consider side-channel attacks on DNN and not only the distributed DNN setting.\n2. Attacks targeting explicitly latent spaces should be considered; such attacks should be aware of the specifics of the latent spaces (e.g., depth and cardinality) and make use of them to improve the efficiency of the attack.\n3. Adversarial attacks targeting input and latent representations should be considered to evaluate if such a setting presents a greater risk to distributed DNN. \n4. As the effectiveness of perturbations depends on the input size, the results should be normalized accordingly."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5558/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699195219615,
            "cdate": 1699195219615,
            "tmdate": 1699636571298,
            "mdate": 1699636571298,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gn96hHwC4j",
                "forum": "hgrZluxFC7",
                "replyto": "gj4mn00z9J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5558/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5558/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## **Regarding Weakness 1**\n\n> The experimental setup is lacking and insufficient to support the authors' claims. Only attacks on several distributed DNN architectures were reported.\n\nWe appreciate the reviewer\u2019s concern. On the other hand, we point out that it is hardly feasible to investigate each and every distributed DNN architecture since each of them exhibits its own customized design for feature compression and model partitioning. \n\nMoreover, it is well known that neural networks share similar robust and non-robust features in the same dataset [1]. Thus, it has been shown that adversarial samples generated by one model can transfer to another [2, 3, 4].  For this reason, the target of this paper is to provide theoretical proof of the robustness of distributed DNNs and evaluate the soundness of the proof on the 6 most widely-used approaches in recent years derived from the VGG and ResNet family. This was the same procedure followed by other benchmarking papers in the distributed DNN community [5] and adversarial machine learning community [6]. \n\nThus, we think that the combination of theoretical proof and experimental evaluation put forth by the paper provides a strong contribution to the field. We believe that the extensive evaluation of 6 distributed DNN approaches, 6 model architectures as well as 10 attack algorithms is considered a sufficient experimental setup.\n\n## **Regarding Weakness 2**\n\n> No novel attacks targeting latent spaces were suggested, or even settings in which both the input and latent space are attacked\n\nThank you for your comment. We point out that proposing new adversarial attacks is out of the scope of the paper, which is instead evaluating the robustness of distributed DNNs to existing attacks, while achieving a fundamental understanding of the robustness of latent spaces.\n\nMoreover, for a fair comparison, we needed to guarantee that each attack we performed could be applied to both input and latent space. Proposing new and advanced attacks specifically tailored to distributed DNNs \u2013 for example, the suggested joint input/latent space attacks \u2013 is an intriguing direction and deserves a separate and dedicated investigation.\n\n## **Regarding Weakness 3**\n\n> The second key finding of \"DNN robustness is intrinsically related to the cardinality of the latent space\" is not a phenomenon exclusive to latent spaces.\n\nWe agree that there is other work with similar findings in input space [7]. However, our Theoretical Finding #1 is different from [7] because in our analysis the upper bound is jointly determined by $I^*(Y;T)$ and $\\mathcal{O}( |\\mathcal{T}||\\mathcal{Y}| / \\sqrt{n})$. Theoretical Finding #1 focuses on the variance-bias tradeoff in distributed DNN designs, while previous work only focuses on the input dimension. We prove this novel finding with experiments shown in Figure 6.\n\nOn the other hand, if we limit our scope to data space $X$, the robustness provided by dataset $X$ can be described by its upper bound $I^*(Y;X)$ and $\\mathcal{O}( |\\mathcal{X}||\\mathcal{Y}| / \\sqrt{n})$. Since $X$ represents the dataset, $I^*(Y;X)$ is constant. The adversarial robustness for a given dataset is directly described by the cardinality, which is in line with the findings reported in [7]. For this reason, our analysis from the information perspective can also generalize to input space.\n\n---------------------------------\n### **Reference**:\n\n[1] Ilyas, Andrew, et al. \"Adversarial examples are not bugs, they are features.\" Advances in neural information processing systems 32 (2019).\n\n[2] Liu, Yanpei, et al. \"Delving into transferable adversarial examples and black-box attacks.\" arXiv preprint arXiv:1611.02770 (2016).\n\n[3] Dong, Yinpeng, et al. \"Evading defenses to transferable adversarial examples by translation-invariant attacks.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.\n\n[4] Xie, Cihang, et al. \"Improving transferability of adversarial examples with input diversity.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019.\n\n[5] Matsubara, Yoshitomo, et al. \"SC2 Benchmark: Supervised Compression for Split Computing.\" Transactions on Machine Learning Research (2023).\n\n[6] Dong, Yinpeng, et al. \"Benchmarking adversarial robustness on image classification.\" proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020.\n\n[7] Simon-Gabriel, Carl-Johann, et al. \"First-order adversarial vulnerability of neural networks and input dimension.\" International conference on machine learning. PMLR, 2019"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5558/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700274463398,
                "cdate": 1700274463398,
                "tmdate": 1700274463398,
                "mdate": 1700274463398,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dfIpFYIVlC",
                "forum": "hgrZluxFC7",
                "replyto": "gj4mn00z9J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5558/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5558/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## **Regarding Questions**\n\n> For a correct evaluation of the suggested bound on a given DNN architecture, the experimental settings should present attacks on all the latent spaces in the architecture and not only those available in specific distributed DNN settings. \n\nWe strongly agree with the reviewer that to support our general theoretical findings, the attacks should be performed on all the latent spaces in different depths, and we are currently working to include this experiment. We present some preliminary results in Table 1 and Table 2. We are going to add more results in our final revision. As far as the latent cardinality is concerned, we do have an experiment in the current paper which is shown in Figure 6. \n\n---\n\n**Table 1: Adversarial robustness of ResNet152 as a function of depth with perturbation budget $\\epsilon = 0.003$**\n\n|  | Input | FeatureMap0 | FeatureMap1 | FeatureMap2 | FeatureMap3 | FeatureMap4 |\n| :-------: | :-------: | :------: | :-------: | :-------: | :-------: | :--------: |\n| dimension | x1 | x5.33 | x5.33 | x2.66 | x1.33 | x0.66 |\n| FGSM | 71.9% | 53.6% | 53.1% | 35.8% | 34.9% | 1.6% |\n| PGD | 88.1% | 65.5% | 66.3% | 43.1% | 43.2% | 1.6% |\n\n---\n\n**Table 2: Adversarial robustness of VGG16 as a function of depth with perturbation budget $\\epsilon = 0.003$**\n\n| | Input | FeatureMap0 | FeatureMap1 | FeatureMap2 | FeatureMap3 | FeatureMap4 |\n| :-------: | :-------: | :------: | :-------: | :-------: | :-------: | :-------: |\n| dimension | x1 | x5.33 | x2.66 | x1.33 | x0.66 | x0.33 |\n| FGSM | 94.5% | 41.2% | 28.7% | 20.5% | 14.0% | 6.4% |\n| PGD | 98.8% | 50.3% | 33.2% | 22.3% | 14.5% | 6.5% |\n\n---\n\nWe choose one simple model VGG16 and one relatively deeper model ResNet152 for the experiment. In VGG16 and ResNet152, multiple feature extraction blocks which consist of several identical convolutional layers are used to extract features. After each block a max pooling layer is used to reduce the feature dimensions at different depths. We report the ASR in all feature maps generated by feature extraction blocks. As shown in Table 1 and Table 2, irrespective of the depth and dimensionality, latent representations are always more robust than input, which can support our theoretical analysis.\n\n> As the effectiveness of perturbations depends on the input size, the results should be normalized accordingly.\n\nWe completely agree that the effectiveness of perturbations depends on the input size. For this reason, in the first version of the paper, we have rescaled the perturbation with respect to cardinality, similar to what is done in Equation 3 in [7]. Please refer to Section 4.1 for more details. Furthermore, we are following the literature where it is not common to normalize the Attack Success Rate."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5558/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700274642824,
                "cdate": 1700274642824,
                "tmdate": 1700317721384,
                "mdate": 1700317721384,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]