[
    {
        "title": "Solving Diffusion ODEs with Optimal Boundary Conditions for Better Image Super-Resolution"
    },
    {
        "review": {
            "id": "QIsnQomkd0",
            "forum": "BtT6o5tfHu",
            "replyto": "BtT6o5tfHu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4723/Reviewer_34bX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4723/Reviewer_34bX"
            ],
            "content": {
                "summary": {
                    "value": "This paper propose to steadily sample high-quality SR images from pre-trained diffusion-based SR models by solving diffusion ordinary differential equations with optimal boundary conditions."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper seems well-written"
                },
                "weaknesses": {
                    "value": "1 The methods compared in Table 2 are all outdated. It is necessary to compare them with some state-of-the-art real-world super-resolution tasks [1,2].\n\n2 You need to compare with the state-of-the-art diffusion SR method [3], which also appears to have performed bicubic-SR and real SR tasks.\n\n3 From the figures in the appendix, it seems that the visual improvement is not significant.\n\n4 Please provide a comparison of the computational complexity and runtime for all the methods mentioned in the paper to show your effectiveness.\n\n5 Tables 1 and 2 show that the PSNR is not particularly high, indicating that the network's fidelity is not good. Super-resolution tasks not only seek visual improvement but also place great importance on fidelity. Compared methods have better fidelity. Therefore, I suggest the authors work on improving both PSNR (fidelity) and LPIPS, as this would provide stronger evidence of the effectiveness of your method.\n\n\n[1] Real-esrgan: Training real-world blind super-resolution with pure synthetic data\n\n[1] Knowledge Distillation based Degradation Estimation for Blind Super-Resolution\n\n[2] Diffir: Efficient diffusion model for image restoration"
                },
                "questions": {
                    "value": "see weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4723/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4723/Reviewer_34bX",
                        "ICLR.cc/2024/Conference/Submission4723/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4723/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697520661665,
            "cdate": 1697520661665,
            "tmdate": 1700152266059,
            "mdate": 1700152266059,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "t3JheSkC09",
                "forum": "BtT6o5tfHu",
                "replyto": "QIsnQomkd0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4723/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4723/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' responses to Reviewer 34bX (1/2)"
                    },
                    "comment": {
                        "value": "Thank you so much for your time of giving thoughtful review to our paper. About your concerns, we give the responses below. We hope that these responses can solve your questions about our work.\n\n**W1. The methods compared in Table 2 are all outdated. It is necessary to compare them with some state-of-the-art real-world super-resolution tasks [1,2].**\n\nThanks for the reminder. We have included the comparisons in our rebuttal with more clarifications.\n\nFirst, the proposed method is a novel sampling method that aims at getting better SR results from existing diffusion-based SR models. Thus, the effectiveness of our method is more revealed by the performance gap of the model with/without our sampling technique, instead of the overall performance comparison, which is demonstrated in existing comparisons (i.e., the bottom 3 rows in Tab. 2).\n\nSecond, our comparisons to DASR (published in ECCV 2022) have shown the effectiveness of our method in both LPIPS and NIQE.\n\nThird, we give the results of Real-ESRGAN [1] and KDSR-GAN [2] in the following table.\n\n||DIV2k-test|||RealSR|||\n|-|-|-|-|-|-|-|\n||NIQE$\\downarrow$|LPIPS$\\downarrow$|PSNR$\\uparrow$|NIQE$\\downarrow$|LPIPS$\\downarrow$|PSNR$\\uparrow$|\n|Real-ESRGAN|4.805|0.3109|22.36|6.182|0.2511|25.12|\n|KDSR-GAN|5.253|0.2840|22.92|6.673|0.2425|26.09|\n|StableSR + DDPM-200 (official)|4.741|0.3189|19.42|6.185|0.3065|21.37|\n|StableSR + DDIM-50 + $\\tilde{\\mathbf{x}}_T$ (Ours)|4.309|0.3169|19.55|5.252|0.2999|22.13|\n\nAlthough our method does not outperform them in LPIPS due to the performance gaps between different baselines, it can still generate SR results with significantly better NIQE.\n\n**W2. You need to compare with the state-of-the-art diffusion SR method [3], which also appears to have performed bicubic-SR and real SR tasks.**\n\nThanks for the reminder. We will make efforts to add the related results in our updated version.\n\nFirst, the proposed method is a novel sampling method that aims at getting better SR results from existing diffusion-based SR models, instead of a new diffusion-based SR model. Thus, the vital comparison is between our sampling method and previous sampling methods on the same diffusion-based SR model (i.e., the bottom 3 rows in Tab. 2), as we mentioned in the answer to your question 1. \n\nSecond, we leveraged StableSR (https://arxiv.org/abs/2305.07015) as our baseline, which was pre-printed in May 2023. DiffIR [3] (https://arxiv.org/abs/2303.09472) was pre-printed in March 2023. Thus, StableSR is also up-to-date and competitive compared with DiffIR.\n\nLast, we are glad to include the related comparisons. However, in the original paper of DiffIR [3], the real-SR task presented in our paper is not involved (only including image inpainting, bicubic-SR, and motion deblurring). We need more time to train the DiffIR on the task of real-SR to adapt it to our setting and include the related results in the future.\n\n**W3. From the figures in the appendix, it seems that the visual improvement is not significant.**\n\nThe visual improvements of SR results are relatively subjective. However, it is still easy to recognize obvious improvements in our visual results in the appendix. \n\nIn Fig. 5, our method generates more accurate and clearer crossties of the railway and windows of the building.  In Fig. 6, our method reconstructs more sharp-edged floor tiles and wall bricks with fewer artifacts. In Fig. 7, our method generates more distinct handrails and columns.\n\nIn order to make the visual effects more distinctive, we will give residual images to reveal the differences in the camera-ready version. We will also consider conducting a user study to give a quantitative comparison of the results from different methods."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4723/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700040297570,
                "cdate": 1700040297570,
                "tmdate": 1700040912211,
                "mdate": 1700040912211,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rc6x1HuRKL",
                "forum": "BtT6o5tfHu",
                "replyto": "QIsnQomkd0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4723/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4723/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' responses to Reviewer 34bX (2/2)"
                    },
                    "comment": {
                        "value": "**W4. Please provide a comparison of the computational complexity and runtime for all the methods mentioned in the paper to show your effectiveness.**\n\nThe comparisons of running time are given in the following tables. All the times are tested on one single RTX 2080Ti GPU.\n\nFor bicubic-SR, our baseline is SR3 and resolution of SR images is 256*256. The comparison is:\n\n||GAN||Diffusion|Diffusion (SR3)|||\n|-|-|-|-|-|-|-|\n|method|ESRGAN|RankSRGAN|SRDiff|DDPM-100 (official)|DDIM-50|DDIM-50 + $\\tilde{\\mathbf{x}}_T$ (Ours)|\n|Time (Sec/Img)|0.0989|0.2407|5.4523|5.6283|2.9973|2.9968|\n\nFor real-SR, our baseline is StableSR and resolution of SR images is 512*512. The comparison is:\n\n||GAN|||Diffusion (StableSR)|||\n|-|-|-|-|-|-|-|\n|method|DASR|RealSR|BSRGAN|DDPM-200 (official)|DDIM-50|DDIM-50 + $\\tilde{\\mathbf{x}}_T$ (Ours)|\n|Time (Sec/Img)|0.1058|0.1970|0.1438|22.1815|5.9140|5.9143|\n\nOur method could achieve better performances with fewer sampling steps, which means better efficiency compared with the official sampling methods of existing diffusion-based SR models. The comparison of running time is in proportion to the comparison of sampling steps, which has already been given in the paper. However, because of the characteristics of diffusion models, it is still extremely difficult to achieve shorter running time with diffusion models compared with GAN-based methods which only need one forward process.\n\n**W5. Tables 1 and 2 show that the PSNR is not particularly high, indicating that the network's fidelity is not good. Super-resolution tasks not only seek visual improvement but also place great importance on fidelity. Compared methods have better fidelity. Therefore, I suggest the authors work on improving both PSNR (fidelity) and LPIPS, as this would provide stronger evidence of the effectiveness of your method.**\n\nThe authors understand that PSNR is a well-recognized measure for SR. However, in recent years, its drawbacks have been revealed due to its failure to consider sampling randomness in measurement and not robust to simple geometric transformation. Therefore, recent generative model-based SR methods (including GAN-based, flow-based, diffusion-based, and so on) [4, 5, 6, 7] focus on the perceptual quality of SR images, mainly measured by LPIPS. This protocol is also adopted in a recently published paper [8], confirming that worse PSNR but better LPIPS leads to better perceptual quality.\n\nFurthermore, thank you for your valuable suggestion of improving both distortion and perception simultaneously. However, it is extremely challenging to achieve such a goal due to the widely known \"Perception-Distortion Tradeoff [9]\" which has been studied in depth. We will take your advice into consideration seriously in our future work.\n\nWe really appreciate your excellent review of our paper. If our responses do not address all of your concerns, you are welcome to ask us to give more detailed discussions. We are glad to answer your further questions.\n\n[1] Wang, Xintao, et al. \"Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data\", ICCVW, 2021.\n\n[2] Xia, Bin, et al. \"Knowledge Distillation based Degradation Estimation for Blind Super-Resolution\", ICLR, 2022.\n\n[3] Xia, Bin, et al. \"DiffIR: Efficient Diffusion Model for Image Restoration\", ICCV, 2023.\n\n[4] Ledig, C., et al. \"Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\", CVPR, 2017.\n\n[5] Wang, Xintao, et al. \"ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks\", ECCVW, 2018.\n\n[6] Lugmayr, Andreas, et al. \"SRFlow: Learning the Super-Resolution Space with Normalizing Flow\", ECCV, 2020.\n\n[7] Saharia, Chitwan, et al. \"Image Super-Resolution via Iterative Refinement\", IEEE TPAMI, 2022.\n\n[8] Gao, Sicheng, et al. \"Implicit Diffusion Models for Continuous Super-Resolution\", CVPR, 2023.\n\n[9] Blau, Yochai, and Tomer Michaeli. \"The Perception-Distortion Tradeoff\", CVPR, 2018."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4723/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700040751198,
                "cdate": 1700040751198,
                "tmdate": 1700049745857,
                "mdate": 1700049745857,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XvtbzrPVrf",
                "forum": "BtT6o5tfHu",
                "replyto": "QIsnQomkd0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4723/Reviewer_34bX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4723/Reviewer_34bX"
                ],
                "content": {
                    "title": {
                        "value": "Response to author"
                    },
                    "comment": {
                        "value": "Thanks to author's response\n\nHowever, I need to point out several issues:\n\n1 The referential significance of NIQE is not substantial. Increasing the proportion of GAN loss easily diminishes it, but it leads to severe artifacts. I am more inclined towards the author's use of FID as an alternative.\n\n2 DiffIR has reported comparisons on real-world super-resolution in the supplementary material and has open-sourced relevant code and models. You can directly compare them. Please provide information on parameter count, runtime, and performance comparsion.\n\n3 From a time comparison perspective, it seems that your method takes 30 times longer than GAN methods, but the performance improvement is limited. This is difficult to accept in practical applications.\n\n4 Both PSNR and LPIPS are crucial. A high LPIPS and low PSNR indicate significant distortion in images, perhaps making them appear better. However, the authenticity of super-resolution results is crucial in many scenarios.\n\nI hope the author can demonstrate the significance of this article to me. Is it effective, fast, or does it achieve high metrics? I am  willing to improve my rating based on such aspects."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4723/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700042183325,
                "cdate": 1700042183325,
                "tmdate": 1700042237146,
                "mdate": 1700042237146,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5bYTRqfvNi",
                "forum": "BtT6o5tfHu",
                "replyto": "XxtvTCQzHN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4723/Reviewer_34bX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4723/Reviewer_34bX"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thanks for your response. \n\n1. The author should revise their paper by presenting results on the best-performing DiffIR and comparing them with SOTA RealSR methods, such as real-esrgan and KDSR in order to demonstrate their real contribution to pushing the upper bounds in the field. \n\n2. I want to know whether the author will release their codes and models for public application."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4723/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700150048487,
                "cdate": 1700150048487,
                "tmdate": 1700150048487,
                "mdate": 1700150048487,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EZ4pEVGd4d",
                "forum": "BtT6o5tfHu",
                "replyto": "QIsnQomkd0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4723/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4723/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further responses to Reviewer 34bX"
                    },
                    "comment": {
                        "value": "Thank you so much for your reply. We respond to your concerns below.\n\n**Q1. The author should revise their paper by presenting results on the best-performing DiffIR and comparing them with SOTA RealSR methods, such as real-esrgan and KDSR in order to demonstrate their real contribution to pushing the upper bounds in the field.**\n\nThank you so much for your suggestion. We will add the results on DiffIR in the next version of our paper to demonstrate the superiority of the proposed method.\n\n**Q2. I want to know whether the author will release their codes and models for public application.**\n\nOur method employed several codebases because of the different tasks.\n\nFor bicubic-SR, we will release the code (including the diffusion model itself and the code of extracting the $\\tilde{\\mathbf{x}}_T$), the re-implemented SR3 model, and the $\\tilde{\\mathbf{x}}_T$ used in the corresponding experiments if our paper is accpted.\n\nFor StableSR and DiffIR, we directly leveraged their open-source codes (StableSR: https://github.com/IceClear/StableSR, DiffIR: https://github.com/Zj-BinXia/DiffIR) and the corresponding models. We really appreciate their contributions to the community. We will release the $\\tilde{\\mathbf{x}}_T$ used in the corresponding experiments and the codes of extracting the $\\tilde{\\mathbf{x}}_T$ if our paper is accpted.\n\nThank you again for your time in giving reviews and suggestions for our paper. All of your comments really assist us in demonstrating the superiority of our paper and make our paper more robust."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4723/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700151694595,
                "cdate": 1700151694595,
                "tmdate": 1700153093346,
                "mdate": 1700153093346,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZNz8RHCoVX",
                "forum": "BtT6o5tfHu",
                "replyto": "EZ4pEVGd4d",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4723/Reviewer_34bX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4723/Reviewer_34bX"
                ],
                "content": {
                    "title": {
                        "value": "Response to author"
                    },
                    "comment": {
                        "value": "The author addressed some of my concerns. I decide to improve my rating"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4723/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700152255801,
                "cdate": 1700152255801,
                "tmdate": 1700152255801,
                "mdate": 1700152255801,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Kzi9c0ptKp",
            "forum": "BtT6o5tfHu",
            "replyto": "BtT6o5tfHu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4723/Reviewer_8EEt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4723/Reviewer_8EEt"
            ],
            "content": {
                "summary": {
                    "value": "The paper analyzes the characters of boundary conditions (BCs) in the diffusion-ODE sampling process of diffusion-based Super-Resolution (SR) models and finds out that the optimal BC is shared by different LR images approximately. Based on the analysis, the paper further proposes a method of obtaining an approximately optimal BC based on a reference LR-HR set. The derivation of the paper is mathematically complete. Experiments on the tasks of both bicubic-SR and real-SR demonstrate the superiority of the proposed approximately optimal BC."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. The motivation of analyzing the BCs of diffusion ODEs is intuitive. Different BCs would apparently lead to different SR results. It is vital to find the rule of how BCs affect the results and to propose a method to get a better BC.\n2. The analysis of the paper is mathematically complete. The paper proposes the concept of optimal BC and proves that such an optimal BC is common to different LR images. The conclusion is seemingly solid.\n3. The experiments are sufficient enough to support the theory. The paper claims that the method of obtaining the approximately optimal BS is not related to the degradation model. Experiments on the tasks of bicubic-SR and real-SR demonstrate the assertion. Further ablation studies show the influence of reference set and the set of BCs."
                },
                "weaknesses": {
                    "value": "1. The proposed method leverages LPIPS as the implementation of distance measurement function M(\u00b7,\u00b7). Can we leverage pixel-level metrics like negative PSNR as M? The authors should give more discussions.\n2. The paper assumes that the model is well-trained. However, even a \u201cwell-trained\u201d model cannot fit the real data distribution precisely. Thus, does such an assumption cause potential inaccuracy of the conclusion?\n3. The paper claims that p_\u03b8 (y|h_\u03b8 (x_T,\u03d5)) is approximately uniform. However, what ensures that the model does not have biases when leveraging the \u201cblank token\u201d, which is essentially a placeholder token (that doesn't actually exist)?\n4. It seems that the process of calculating the approximately optimal BC is time-consuming. How long does it take?"
                },
                "questions": {
                    "value": "1. The paper only discusses the context of SR (and other low-level tasks in the Sec. 5). But it seems that the theory does not limit the relationship between guidance and generated results. Do the authors think that the method can be leveraged in more general generation tasks such like text-to-image generation?\n2. It seems that the improvement on StableSR (in the task of real-SR) is less than the improvement on SR3 (in the task of bicubic-SR). Could the authors discuss the reasons for this phenomenon?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4723/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4723/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4723/Reviewer_8EEt"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4723/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698645913553,
            "cdate": 1698645913553,
            "tmdate": 1699636454004,
            "mdate": 1699636454004,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "B0txFhvTRw",
                "forum": "BtT6o5tfHu",
                "replyto": "Kzi9c0ptKp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4723/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4723/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' responses to Reviewer 8EEt (1/2)"
                    },
                    "comment": {
                        "value": "Thank you so much for your thoughtful review which helps us to improve the quality of our paper. For your questions, we have the following responses. We hope that they can help to address your concerns. Sincerely looking forward to your reply.\n\n**W1. The proposed method leverages LPIPS as the implementation of distance measurement function $M(\u00b7,\u00b7)$. Can we leverage pixel-level metrics like negative PSNR as $M$? The authors should give more discussions.**\n\nWe can also use MSE (which is equivalent to negative PSNR) as the implementation and give results on bicubic-SR in the following table. \n||DIV2k-test||Urban100||BSD100||\n|-|-|-|-|-|-|-|\n||LPIPS|PSNR|LPIPS|PSNR|LPIPS|PSNR|\n|DDIM-50 + LPIPS-oriented $\\tilde{\\mathbf{x}}_T$|0.1053|28.65|0.1164|24.26|0.1552|23.99|\n|DDIM-50 + MSE-oriented $\\tilde{\\mathbf{x}}_T$|0.1060|28.56|0.1164|24.24|0.1557|24.01|\n\nThe results are slightly worse than the LPIPS-oriented $\\tilde{\\mathbf{x}}_T$ which is used in our paper. This is because MSE is not a less optimal metric to evaluate such distance. Lower MSE may not mean better image quality, which is a consensus in the domain of image SR [1, 2].\n\n**W2. The paper assumes that the model is well-trained. However, even a \u201cwell-trained\u201d model cannot fit the real data distribution precisely. Thus, does such an assumption cause potential inaccuracy of the conclusion?**\n\nIndeed, the gap between the parameterized distribution of diffusion models and real data distribution will affect the accuracy of our analysis, and this is the reason that our $\\tilde{\\mathbf{x}}_T$ is an \"approximately\" optimal boundary condition. We have pointed it out in our paper (in the text description before the Eqn. 16). In order to mitigate the inaccuracy, we leverage a reference set of LR-HR image pairs to find the $\\tilde{\\mathbf{x}}_T$, instead of one single LR-HR pair.\n\n**W3. The paper claims that $p_\\theta (\\mathbf{y}|h_\\theta (\\mathbf{x}_T, \\phi))$ is approximately uniform. However, what ensures that the model does not have biases when leveraging the \u201cblank token\u201d, which is essentially a placeholder token (that doesn't actually exist)?**\n\nAs you mentioned, the blank token $\\phi$ is only symbolic guidance, which means not give any guidance to the model when generating. When generating images from $\\phi$, we are actually unconditionally generating images (such unconditional generating can be achieved by randomly dropping the LR images during training [3]). For $p(\\mathbf{x}_0|\\phi)$, a \"well-trained\" model means it should be close to the training data distribution $q(\\mathbf{x}_0|\\phi)$, which is uniform.\n\nFurthermore, we give more discussion on our claim, \"for a well-trained model, such probability (the probability of unconditionally generating an image which is the corresponding SR image of the LR image y) is also approximately uniform\". For a \"well-trained\" model, such probability is close to the probability of \"randomly sampling an image which is the corresponding SR image of a certain LR image $\\mathbf{y}$ from the HR image dataset\". The distribution of the latter one is consistent with sampling training data during the training process, which is uniform for different $\\mathbf{y}$. Thus, such probability will also be uniform.\n\n**W4. It seems that the process of calculating the approximately optimal BC is time-consuming. How long does it take?**\n\nThe time of calculating the approximately optimal BC is about 20 hours on a V100 GPU. As we have mentioned in our paper, we take $R=300$ and $K=1000$ when calculating the approximately optimal BC. In practice, we did not simply sample $R \\cdot K=300,000$ SR images. We first test all the $K=1000$ $\\mathbf{x}_T$ on a small reference set containing 10 LR-HR pairs and only retain the top 300 $\\mathbf{x}_T$ as a pre-screening process. Then we leverage a larger reference set and repeat the filtering process. At last, the final reference set contains all the 300 reference pairs. \n\nSuch a time cost is a one-off. We just need to calculate the approximately optimal BC once for one diffusion-based SR model. This time cost is much lower than the time cost of training the model itself. Thus, such a process is not time-consuming."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4723/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700036826538,
                "cdate": 1700036826538,
                "tmdate": 1700052038094,
                "mdate": 1700052038094,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iESJ6MxSsT",
                "forum": "BtT6o5tfHu",
                "replyto": "Kzi9c0ptKp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4723/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4723/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' responses to Reviewer 8EEt (2/2)"
                    },
                    "comment": {
                        "value": "**Q1. The paper only discusses the context of SR (and other low-level tasks in the Sec. 5). But it seems that the theory does not limit the relationship between guidance and generated results. Do the authors think that the method can be leveraged in more general generation tasks such like text-to-image generation?**\n\nYour suggestion of leveraging the method in other tasks is so valuable. As we have mentioned in our paper, the proposed method can also be used in other low-level vision tasks. However, maybe it is not suitable enough for high-level generation tasks. Because these tasks do not have unique ground truths, it is difficult to define the concept of \"optimal sample\" which was defined by the ground truth (to be more specific, HR images in our paper). For example, in the task of text-to-image generation, as you mentioned, there can be several images that are semantically aligned to the input text. In other words, there is no exact \"ground truth\" to an input text.\n\nThank you so much for your suggestion. We will consider your advice seriously in our future works of leveraging our method in more low-level vision tasks.\n\n**Q2. It seems that the improvement on StableSR (in the task of real-SR) is less than the improvement on SR3 (in the task of bicubic-SR). Could the authors discuss the reasons for this phenomenon?**\n\nWe think that the reason to the performance gap on StableSR and SR3 is that StableSR builds diffusion model in the latent space, while SR3 builds diffusion model in the image space. However, we actually propose the concept of \"optimal sample\" in the image space (Eqn. 9). Thus, the diffusion process in the latent space affects the accuracy of the approximation of our method (Eqn. 15, 16, 18) slightly, causing less improvement of StableSR. However, our method still helps StableSR to sample better results than its official sampling method (DDPM-100) in shorter time.\n\nThank you again for your approval of our paper and your valuable review. You are welcome to ask us to give further responses to address your concerns.\n\n[1] Ledig, C., et al. \"Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\", CVPR, 2017.\n\n[2] Wang, X., et al. \"ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks\", ECCVW, 2018.\n\n[3] Ho, J., and Salimans, T.. \"Classifier-Free Diffusion Guidance\", NIPSW, 2021."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4723/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700037751306,
                "cdate": 1700037751306,
                "tmdate": 1700037946439,
                "mdate": 1700037946439,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UwxeRQboaW",
            "forum": "BtT6o5tfHu",
            "replyto": "BtT6o5tfHu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4723/Reviewer_4gay"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4723/Reviewer_4gay"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the randomness in the inverse process of the diffusion model applied to super-resolution tasks, which makes it difficult to ensure the quality of SR results. By solving diffusion ordinary differential equations with optimal boundary conditions, the authors propose an efficient plug-and-play method that enables diffusion models to stably sample high-quality SR images with fewer sampling steps.\n\nPost rebuttal:\nI have read the rebuttal and would like to raise my score a little bit."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe proposed method achieves good visualization results with fewer steps in SR tasks. \n2.\tThe proposed method has good flexibility and can be applied to multiple diffusion models"
                },
                "weaknesses": {
                    "value": "1.\tThe experiments are insufficient. Although the proposed method has achieved good results on LPIPS, the PSNR values on multiple test sets are very low. The author did not discuss it in depth and did not show the results of SSIM.\n2.\tThe paper stated that the proposed method has fewer parameters and is more efficient than the GAN method, but did not show the corresponding comparison results. Such as comparison of specific parameters or running time."
                },
                "questions": {
                    "value": "Further supplement and improve the experiment, please refer to the Weaknesses for details."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "na"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4723/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4723/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4723/Reviewer_4gay"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4723/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698766074091,
            "cdate": 1698766074091,
            "tmdate": 1700697013704,
            "mdate": 1700697013704,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2tzRW5gmZ4",
                "forum": "BtT6o5tfHu",
                "replyto": "UwxeRQboaW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4723/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4723/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' responses to Reviewer 4gay"
                    },
                    "comment": {
                        "value": "Thank you so much for your reviews and comments of our paper. Your summary of our paper is precise and concise. About your concerns, we give responses below. We hope they can address your concerns.\n\n**W1. The experiments are insufficient. Although the proposed method has achieved good results on LPIPS, the PSNR values on multiple test sets are very low. The author did not discuss it in depth and did not show the results of SSIM.**\n\nFirst, the authors understand that PSNR and SSIM are well-recognized measures for SR. However, in recent years, their drawbacks have been revealed due to their failure to consider sampling randomness in measurement and not robust to simple geometric transformation. Therefore, recent generative model-based SR methods (including GAN-based, flow-based, diffusion-based, and so on) [1, 2, 3, 4] focus on the perceptual quality of SR images, mainly measured by LPIPS. This protocol is also adopted in a recently published paper [5], confirming that worse PSNR but better LPIPS can lead to better perceptual quality.\n\nSecond, we appreciate the reviewer's comments. The evaluations of SR are still open questions. We will thoroughly discuss and address these issues in our revised paper.\n\n**W2. The paper stated that the proposed method has fewer parameters and is more efficient than the GAN method, but did not show the corresponding comparison results. Such as comparison of specific parameters or running time.**\n\nSorry for the confusion. We make the further clarification as follows.\n\nFirst, our method is a sampling method of current diffusion-based SR models, instead of a new SR model. Thus, our method could be leveraged to sample better SR results from existing methods without any additional parameters. \n\nSecond, our method could achieve better performances with fewer sampling steps, which means better efficiency compared with the baseline diffusion-based SR model. The comparison of running time is in proportion to the comparison of sampling steps, which is already given in the paper. However, because of the characteristics of diffusion models, it is still extremely difficult to achieve faster running time compared with GAN-based methods which only need one forward process. The comparisons of running time among different sampling methods from one diffusion-based SR model are given in the following tables. All the times are tested on one single RTX 2080Ti GPU.\n\nFor bicubic-SR, the comparison on baseline SR3 is (resolution 256*256):\n||DDPM-1000|DDPM-250|DDPM-100 (SR3 official)|DDIM-50|DDIM-50 + $\\tilde{\\mathbf{x}}_T$ (Ours)|\n|-|-|-|-|-|-|\n|Time (Sec/Img)|54.3940|13.6769|5.6283|2.9973|2.9968|\n\nFor real-SR, the comparison on baseline StableSR is (resolution 512*512):\n\n||DDPM-200 (StableSR official)|DDIM-50|DDIM-50 + $\\tilde{\\mathbf{x}}_T$ (Ours)|\n|-|-|-|-|\n|Time (Sec/Img)|22.1815|5.9140|5.9143|\n\nOur statement refers to that our method is able to sample better SR results compared with GAN-based methods from current diffusion-based SR models, achieving better efficiency and effectiveness. Such diffusion-based SR models cannot outperform GAN-based methods with previous sampling methods. We apologize if our statement leads to ambiguity. We will refine the corresponding statement in the next edition of our paper to make it more precise.\n\nThank you again for your review of our paper. You are welcome to further ask us about your concerns which our responses do not fully address and ask us to give more detailed responses. Really looking forward to your reply.\n\n[1] Ledig, C., et al. \"Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\", CVPR, 2017.\n\n[2] Wang, Xintao, et al. \"ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks\", ECCVW, 2018.\n\n[3] Lugmayr, Andreas, et al. \"SRFlow: Learning the Super-Resolution Space with Normalizing Flow\", ECCV, 2020.\n\n[4] Saharia, Chitwan, et al. \"Image Super-Resolution via Iterative Refinement\", IEEE TPAMI, 2022.\n\n[5] Gao, Sicheng, et al. \"Implicit Diffusion Models for Continuous Super-Resolution\", CVPR, 2023."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4723/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700036234955,
                "cdate": 1700036234955,
                "tmdate": 1700038174848,
                "mdate": 1700038174848,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]