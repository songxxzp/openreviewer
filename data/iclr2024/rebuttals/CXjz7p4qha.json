[
    {
        "title": "Rotation Invariant Quantization for Model Compression"
    },
    {
        "review": {
            "id": "fhz0No7vIA",
            "forum": "CXjz7p4qha",
            "replyto": "CXjz7p4qha",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5056/Reviewer_qJtZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5056/Reviewer_qJtZ"
            ],
            "content": {
                "summary": {
                    "value": "The paper examines post-training quantization of neural networks featuring linear layers, taking into account cosine distortion. The authors first demonstrate the rate-distortion trade-off between the original and quantized weights to establish the step size, $\\Delta_{\\ell}$, for each layer. Notably, all $\\Delta_{\\ell}$ values are governed by a singular parameter, $k$. Additionally, the paper investigates the rate-deviation analysis, wherein the deviation assesses the disparity in output. The authors introduce surrogate models in which the quantized weight is uniformly distributed across a space subject to random rotation, characterized by the angle $\\theta_{\\ell}$. Subsequently, it is proven that the mutual information is minimized when using the product distribution."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-written and straightforward to understand.\n2. The concept of rotational invariance in neural network quantization is intriguing."
                },
                "weaknesses": {
                    "value": "1. The central assumption, $\\||w\\|| = \\||\\hat{w}\\|| + o(\\||w\\||)$, appears to be contentious. For instance, with fixed-bit quantization, the discrepancy between $\\||w\\||$ and $\\||\\hat{w}\\||$ is proportional to $\\||w\\||$, or in other words, $O(\\||w\\||)$.\n\n2. Lemma 1 could benefit from a more rigorous presentation, especially concerning the $o(\\cdot)$. Additionally, both Lemma 1 and Corollary 1 do not appear to offer significant novel contributions.\n\n3. The surrogate model feels somewhat contrived. Moreover, the method for deriving $\\tilde{w}_{\\ell}$ is not clear. For instance, are the angles $\\theta_{\\ell}$ specified? The notion of being \"uniformly distributed on a cone\" is ambiguous due to its unbounded norm. As a result, Theorem 1 requires a more comprehensive problem definition.\n\n4. There seem to be some technical inaccuracies in the proof of Theorem 1. It is also essential to ensure that each mutual information, \n$I(w_{\\ell}, \\tilde{w}_{\\ell})$, adheres to the distortion criteria.\n\n5. The link between distortion (related to weights) and deviation (pertaining to output) is unclear."
                },
                "questions": {
                    "value": "Please check Weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5056/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5056/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5056/Reviewer_qJtZ"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5056/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698650404209,
            "cdate": 1698650404209,
            "tmdate": 1699636495604,
            "mdate": 1699636495604,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "A22jNMQDQa",
                "forum": "CXjz7p4qha",
                "replyto": "fhz0No7vIA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5056/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5056/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "*We would like to thank the reviewer for the time and effort of the thorough review and helpful comments!*\n\n- The central assumption, $\\Vert w \\Vert = \\Vert \\hat{w}\\Vert + o(\\Vert w \\Vert )$, appears to be contentious. For instance, with fixed-bit quantization, the discrepancy between and is proportional to $O(\\Vert w \\Vert)$\n\n[A]  The order of discrepancy depends on the rate regime that is considered. This paper considers the high-rate regime as mentioned explicitly throughout the paper.\n\nAs the reviewer pointed out, the quantization error in each layer is bounded by $\\Vert \\epsilon \\Vert \\leq \\sqrt{n}\\frac{\\Delta}{2}$. Assuming that the range of $w$ is symmetric around zero for simplicity, then, $\\Delta \\approx \\frac{2 max(w)}{2^R}$, where $R$ is the rate of quantization. Hence, the error $\\Vert \\epsilon\\Vert \\leq \\sqrt{n}\\frac{max(w)}{2^R} \\approx \\frac{\\Vert w \\Vert}{2^R}$, and thus, the $o(\\cdot)$ is contentious. \n\nYet, $O(\\Vert w\\Vert)$ does not reflect the high rate regime that is considered in the paper. In particular, when letting $R = O(\\log^c(\\Vert w \\Vert))$ for $c>1$, then, we obtain the desired $o(\\Vert w \\Vert)$. \nMoreover, even when $R = O(\\log(\\Vert w \\Vert))$, which results in a relaxed error of $O(1)$, still provides meaningful results. In this case, the resulting approximation error would be $O(1/\\Vert w \\Vert)$. \n\nWe thank the reviewer for pointing this out and agree that the discussion above would improve the presentation and clarity of the lemma. In the revised version, we explicitly restrict the lemma to the high-rate regime, and we provide the above discussion in the proof of Lemma 1. \n\n- Lemma 1 could benefit from a more rigorous presentation, especially concerning the $o(\\cdot)$. Additionally, both Lemma 1 and Corollary 1 do not appear to offer significant novel contributions.\n\n[A] As mentioned, in the revised version of Lemma 1 proof, we provide the above discussion about the high rate regime, which we believe should address the concern about $o(\\cdot)$. Please note that the goal of Lemma 1 is to provide the asymptotic connection between the bin width and the resulting cosine distance in the high-rate regime. As far as we know, this connection is missing in the literature. \n\nCorollary 1 connects the overall cosine distance to the layers' cosine distance, and it is used in the proof of Theorem 1, where we used this convex combination representation to minimize the mutual information. Furthermore, this corollary connects the distortion of the layers to the distortion when considering the whole parameters.\n\n- The surrogate model feels somewhat contrived.  Moreover, the method for deriving $\\tilde w_\\ell$ is not clear. For instance, are the angles $\\theta_\\ell$ specified? The notion of being \"uniformly distributed on a cone\" is ambiguous due to its unbounded norm. As a result, Theorem 1 requires a more comprehensive problem definition.\n\n[A]  Note that $\\hat w_\\ell$, and hence, its norm and  $\\theta_\\ell$ are specified by the quantization. The $\\tilde w_\\ell$ models the quantized vector $\\hat w_\\ell$. In particular, the surrogate model considers all the vectors that are $\\theta_\\ell$ away by using a random rotation matrix $U(\\theta_\\ell)$ (see Figure 3 for illustration). That is, $\\tilde w_\\ell$ is a realization of this random rotation. Note that the norm of $\\tilde w_\\ell$ is the same as $\\hat w_\\ell$ (and hence, bounded). Thus, any realization of  $\\tilde w_\\ell$ is uniformly distributed on the cone's ring. \n\nIn the revised version we clarify this point after introducing the surrogate model. \n\n- There seem to be some technical inaccuracies in the proof of Theorem 1. It is also essential to ensure that each mutual information $I(w_\\ell,  \\tilde w_\\ell)$ adheres to the distortion criteria.\n\n[A] As mentioned in the problem statement, our goal is to minimize the cosine distance between the models' output. In the rate-distortion analysis, we focus on solutions in which the cosine distance between the outputs is small, and not in the cosine distance per each layer. In other words, we are interested in the total deviation. A good analogy would be vector quantization vs. scalar quantization, where restricting the quantization error in each element would lose optimality (See e.g., [Ch. 24.2](https://people.lids.mit.edu/yp/homepage/data/itbook-export.pdf)).  \n\n- The link between distortion (related to weights) and deviation (pertaining to output) is unclear.\n\nThe link between the distortion and the deviation is given in Proposition 1 proof. Specifically, we prove that both the distortion and the deviation scale with the quantization rate as $O(1/k^2)$. To clarify this link, Proposition 1 now explicitly states this connection in the revised version."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5056/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699966270127,
                "cdate": 1699966270127,
                "tmdate": 1700567572875,
                "mdate": 1700567572875,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EGpyguTTRk",
            "forum": "CXjz7p4qha",
            "replyto": "CXjz7p4qha",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5056/Reviewer_4eGc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5056/Reviewer_4eGc"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a post-training quantization algorithm named Rotation-Invariant Quantization (RIQ) to quantify the NN to mixed-precision, and the main approach is picking the quantization bin width to be proportional to the layers\u2019 norm. Based on the rate-distortion theory, the proposed method searching for the optimal solution over the family of spherical distributions. Empirical results show the competitive performance of RIQ on several benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.The authors provide a detailed analysis of the rate-distortion theory, which clarifies their research motivation well.\n\n2.This paper is well-written and organized, and the supplementary material is sufficiently detailed."
                },
                "weaknesses": {
                    "value": "1. Mixed precision is difficult to apply in the industrial scenarios. It usually requires the design of specialized chips to achieve a slight increase in inference speed, so I have doubts about the impact of the proposed method. \n\n2. The experimental result lacks a comparison of inference speed of compressed model, between the proposed method and existing works.\n\n3. The experimental result lacks a comparison of lightweight structure including separable convolution (like mobilenetv2) with existing methods like AdaRound or BRECQ."
                },
                "questions": {
                    "value": "1. I suggest the author provide more descriptions about the scenarios where the mixed-precision model can be applied. The advantages of mixed-precision models can be manifested in NLP-type structures."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5056/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698668589193,
            "cdate": 1698668589193,
            "tmdate": 1699636495522,
            "mdate": 1699636495522,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4cyQYLQtrb",
                "forum": "CXjz7p4qha",
                "replyto": "EGpyguTTRk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5056/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5056/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "*We thank the reviewer for the time and effort and useful feedback!* \n\n- Mixed precision is difficult to apply in the industrial scenarios. It usually requires the design of specialized chips to achieve a slight increase in inference speed, so I have doubts about the impact of the proposed method\n\n[A] This study does not target any specific hardware and allows running on standard CPU/GPU with 8 bits after de-quantizing the weights. By doing this, we do not benefit from acceleration from the mixed precision (only from the 8-bit execution), and the main benefit is a reduced model size. \n\nOne may consider a specialized hardware solution, but in practice, if all symbols are encoded with less than 8 bits, it allows running the model with 8-bit computation, rather than using a mixed-bit on dedicated hardware and achieving a slight increase in inference speed, as the reviewer mentioned. Accordingly, it is not necessary to restore the weights to full precision. Our compression scheme facilitates such quantization, as we point out in the Remark below Proposition 2.\n\nStill, we would like to emphasize that our goal is to reach the smallest model size rather than accelerate the running time, and it should be the main takeaway from this study. \n\n- The experimental result lacks a comparison of inference speed of the compressed model, between the proposed method and existing works.\n\n[A] As mentioned, the objective of this study is to minimize the model size under the cosine distance constraint. In Section A.8.2, we did perform full quantization (i.e., quantizing weights by RIQ and the activations by [Wu 20'](https://arxiv.org/pdf/2004.09602.pdf)), where the de-quantized weights were packed in 8-bits, allowing to further accelerate the inference. Nevertheless, the same speedup was obtained also by a simple uniform 8-bit quantization. So, the main benefit of RIQ is the reduced model size and not its acceleration.  \n\n- The experimental result lacks a comparison of lightweight structure including separable convolution (like mobilenetv2) with existing methods like AdaRound or BRECQ.\n\n[A] Please note that Table 1 provides comparisons with the AdaRound and BERCQ on the Resnet50 model. \nRIQ is a powerful method for models with high redundancy and attains good compression results without further tuning. MobileNet_v2 is a relatively compact and efficient model (with a size of only 13MB), and hence, it cannot benefit significantly from RIQ compared to the BRECQ method, which applies further optimization for tuning the weights and requires a few hours to tune on GPU. \n\nAs per the reviewer's request, we evaluated RIQ on mobilenet_v2 on ImageNet1k, and the results are given below:\n\n| Method   | compression   | Accuracy(%) | Reference(%) | Drop (%) |\n|----------|---------------|-------------|--------------|----------|\n| BRECQ    | $\\times 8$    | $71.66$     | $72.49$      | $0.83$   |\n| AdaRound | $\\times 8$    | $69.78$     | $72.49$      | $2.71$   |\n| RIQ      | $\\times 5.33$ | $71.19$     | $71.87$      | $0.68$   |\n\n\n- I suggest the author provide more descriptions about the scenarios where the mixed-precision model can be applied. The advantages of mixed-precision models can be manifested in NLP-type structures.\n\n[A] We thank the reviewer for this suggestion. Indeed, NLP-type structures and Large-Language Models (LLMs) in particular, have high redundancies that RIQ can exploit. Indeed, the benefit of a reduced model size is twofold. First, the low memory that is required to store the model. Second, the lower loading and extracting time of the model weights may be significant in LLMs. For example, Llama2-70B requires 140GB to load on the device. Whereas, assuming $\\times 4$ compression with RIQ (as demonstrated in Table 4 in the appendix for Llama-7B) would require only 35GB, which can fit a single GPU with 40GB memory (e.g., Nvidia A100).  \n\nWe added this motivation in the introduction of the revised version."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5056/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699977394584,
                "cdate": 1699977394584,
                "tmdate": 1700566923294,
                "mdate": 1700566923294,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iNm1HvfT08",
            "forum": "CXjz7p4qha",
            "replyto": "CXjz7p4qha",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5056/Reviewer_9KPJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5056/Reviewer_9KPJ"
            ],
            "content": {
                "summary": {
                    "value": "The authors proposed a post-training mixed-precision weight quantization technique for neural networks by optimization based on an information-theoretic paradigm.  They choose to minimize layer-wise bitwidth constrained by cosine distance.  They experimented with example models in comparison with other methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paradigm of optimization for mixed-precision quantization is novel.  \n- Theoretical results on optimization bounds are useful."
                },
                "weaknesses": {
                    "value": "- How do layer-wise quantization errors accumilate?  The proposed algorithm does not seem to address this.   \n- In order for post-training quantization to be practical, activations are quantized too.  How activation quantization can be jointly done is not addressed.  \n- Experimental results did not show a definitive advantage over competing methods."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5056/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698791238554,
            "cdate": 1698791238554,
            "tmdate": 1699636495421,
            "mdate": 1699636495421,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5f8S8CwXHj",
                "forum": "CXjz7p4qha",
                "replyto": "iNm1HvfT08",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5056/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5056/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "*We thank the reviewer for the time and effort and useful comments!*\n\n- How do layer-wise quantization errors accumulate? The proposed algorithm does not seem to address this.\n\n[A] The deviation between the outputs dictates the actual performance, and hence, this is the focus of our study. \n\nIn Section 3.1, we do mention that \"each quantized layer produces a rotation distortion in its output, and this distortion keeps propagating and accumulating through the layers until reaching the model\u2019s output\". In other words, like wave propagation, due to the random nature of errors, the rotations (errors) may accumulate constructively or destructively, and the accumulated errors determine the deviation, and hence, the accuracy.  \nNevertheless, this study does provide a link between the layer-wise errors and the final deviation in Proposition 1. Specifically, this proposition shows that both the final deviation and the layer-wise errors decrease together with the rate as $O(1/k^2)$. \n\nIn the revised version, we stress this point out before Proposition 1.\n\n- In order for post-training quantization to be practical, activations are quantized too. How activation quantization can be jointly done is not addressed.\n\n[A] We refer the reviewer to Section A.8.2, where we perform full quantization (i.e., quantizing weights by RIQ and the activations by [Wu 20'](https://arxiv.org/pdf/2004.09602.pdf)). To gain speed on GPU, the de-quantized weights were packed in 8 bits, allowing acceleration of the inference. This way, one can attain a small memory footprint of RIQ and still benefit from full quantization speedup.\n\n- Experimental results did not show a definitive advantage over competing methods.\n\n[A] Table 1 shows that RIQ outperforms many of the methods, achieving a small accuracy drop. Note that there are other methods that also attain small degradation from the baseline, thus, it is hard to show a definitive advantage in this setting. \n\nHowever, as mentioned in Section A.8.1, RIQ is an efficient method for reducing the model size, and it optimizes models quickly. For example, compressing the ResNet-50 takes less than a minute on a CPU, while most of the methods are more complex and require a longer time to optimize."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5056/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699978265351,
                "cdate": 1699978265351,
                "tmdate": 1700566151641,
                "mdate": 1700566151641,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qGnYsR2SdN",
            "forum": "CXjz7p4qha",
            "replyto": "CXjz7p4qha",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5056/Reviewer_N6Nd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5056/Reviewer_N6Nd"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new post-training quantization algorithm that given a neural network and some calibration images, produces mixed-precision quantized network. The key contribution of the paper is a new analysis technique motivated by the cosine-similarity based distortion measure between outputs quantized and unquantized network. The authors provide rate distortion analysis under the proposed measure and find the relation between the quantization bin width ($delta$) and layer's distortion (lemma 1) and the whole model's distortion (corollary 1). The authors then re-parametrize the search over $delta$ as search over parameter $k$ defined wrt distortion, and provide an efficient search algorithm (alg.1). Authors bound the possible values for optimal $k$ and make a heuristic search. The effectiveness of search algorithm, and of the proposed analysis is demonstrated on multiple networks and datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The technique is well motivated and executed; even without the application to compression, the results and its analysis have their merits on its own.\n- The heuristic search over $k$ (alg 1), has good initial parameters and does not seem to require heavy tuning.\n- Very good presentation and flow."
                },
                "weaknesses": {
                    "value": "- I found it a bit hard to understand how $k$ come into picture during the first readthrough of the paper. I feel like a little more work needed to introduce it. \n- There are a few arguable points that I count as a weakness, but they are easily addressable:\n  - I would like to understand how good is the search parameters wrt a synthetically created problem where (say weights sampled from Gaussian/Laplacian), single, layer and etc. How much the heuristics during the search may (or may not) miss the optimal bin width?\n  - Also, while I agree that in general setting search of $k$ is unbounded (as you write in the paper), practically speaking it is not the case: the weights are finite, and thus there are only certain number of  $delta$-s to check. This, has in fact been done in the work called \"Optimal quantization using scaled codebook\" (btw, you cite this paper but attribute it as QAT, which is not correct)\n- Results:\n  - I believe all compression results are given after ANS encoding; providing the compression ratio before ANS would be of great value (most papers report results before any additional encodings)"
                },
                "questions": {
                    "value": "Please see weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5056/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698824491408,
            "cdate": 1698824491408,
            "tmdate": 1699636495320,
            "mdate": 1699636495320,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "x0DodoymFt",
                "forum": "CXjz7p4qha",
                "replyto": "qGnYsR2SdN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5056/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5056/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "*Thank you for the supportive comments and insightful feedback!* \n\n- I found it a bit hard to understand how $k$ came into the picture during the first readthrough of the paper. I feel like a little more work is needed to introduce it.\n\n[A] We agree with the reviewer and the following text was added to introduce $k$:\n\n> The connection of $\\Delta_\\ell$ to $\\Vert w_\\ell \\Vert$ in Lemma1 hints at the rotation-invariant nature of the optimization. To focus on rotation-invariant solutions, RIQ introduces a search parameter $k$ that maintains proportion with $\\Vert w_\\ell \\Vert$, allowing efficient search over these solutions.  Specifically, when $\\Delta_\\ell(k) = \\Vert w_\\ell \\Vert/k$  where $k$ to be optimized, the bin-width...\n\n- There are a few arguable points that I count as a weakness, but they are easily addressable:\n    - I would like to understand how good is the search parameters wrt a synthetically created problem where (say weights sampled from Gaussian/Laplacian), single layer, etc. How much the heuristics during the search may (or may not) miss the optimal bin width?\n    \n    - Also, while I agree that in the general setting search of $k$ is unbounded (as you write in the paper), practically speaking it is not the case: the weights are finite, and thus there are only a certain number of $\\Delta$-s to check. This has in fact been done in the work called \"Optimal quantization using scaled codebook\" (btw, you cite this paper but attribute it as QAT, which is not correct)\n    \n[A1] RIQ performs constrained optimization of parameter $k$, where the constraint is the deviation between outputs. The heuristic allows tolerance from the optimal deviation which is specified by the smallest step.  \n\nAs suggested by the reviewer, we tested a synthetic problem, where RIQ quantizes a single convolution layer with a Gaussian input tensor to examine the searching points and the possible miss (See results in [this Figure](https://anonymous.4open.science/r/riq-3CE1/conv2d_deviation.pdf)). In this example, the deviation constraint between the outputs is 0.005 and RIQ reached very near with a deviation value of 0.00499 (i.e., missed by 1e-5).    \n    \n[A2]  Indeed, practically there's a bounded search space in which the optimization can be done.  In the revised text we rephrase the text below eq. (7):\n\n> Even though the search of $k$ is unbounded in general, practically it is sufficient to search in bounded space since the weights' norm is finite (Idelbayev et al. 2021).  \n\nThank you for pointing out the incorrect citation of this work. We fixed it in the revised version. \n\n- I believe all compression results are given after ANS encoding; providing the compression ratio before ANS would be of great value (most papers report results before any additional encodings)\n\n[A] Indeed, the results are after applying ANS encoding, and we agree that there is great value in seeing the contribution of ANS to RIQ alone, and hence, we provide the rate before and after ANS for the ResNet50 model in Figure 4(a), from which we can infer the contributions of ANS to Table 1. \n\nFigure 4(a) shows that the entropy limit is 2-3 bits/symbol away from RIQ only (depends on the deviation requirement). This difference stems from the different representations' efficiency. In other words, the RIQ representation minimizes the entropy without attaining it, whereas the ANS encoding attains the entropy limit.\n\nPlease note that while other baselines did not apply further encoding, most of the baselines have used per-channel quantization, for which encoding may be less beneficial due to the encoding tables overhead."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5056/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700132908365,
                "cdate": 1700132908365,
                "tmdate": 1700565968512,
                "mdate": 1700565968512,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]