[
    {
        "title": "Meta-Learning Strategies through Value Maximization in Neural Networks"
    },
    {
        "review": {
            "id": "qiY1LXgLNr",
            "forum": "Zz61cEY84L",
            "replyto": "Zz61cEY84L",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6160/Reviewer_QJtu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6160/Reviewer_QJtu"
            ],
            "content": {
                "summary": {
                    "value": "The paper explores various meta-algorithmic choices (\"control signals\") that influence the learning dynamics of neural networks. The authors define optimal control signals by the condition that they maximize the cumulative learning performance (minus control costs) over the learning process. They study the setting of deep linear networks which allows analytic solutions of the learning dynamics equations, and, consequently, enables gradient-based optimization of the control signal. In the experiments, the authors consider control signals describing parameter initialization, learning-rate choice, learning curriculum, and gain modulation. The authors discuss the relevance of their results in the context of cognitive control in neuroscience."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The writing style of the paper is excellent and the authors supplement their exposition with helpful explanations and figures (e.g., Fig. 1). The proposed framework appears as a nice approach to reason about meta-algorithmic choices in the cognitive sciences and machine learning. The authors derive a general framework and apply it to a range of experiments which seem to be well fleshed out. I appreciate that the authors study an analytically tractable variant (linear networks) and derive results that seem to be consistent with the cognitive control literature. While I am uncertain of the practical relevance of the experimental results (cf. weaknesses), I consider theoretical studies of the proposed kind interesting and relevant in their own right. Thus, I recommend acceptance (with low confidence)."
                },
                "weaknesses": {
                    "value": "While the authors argue that their results are mostly consistent with the cognitive control literature (on which I cannot comment because I'm not an expert in this field), I doubt that the results have any practical relevance for the design of modern machine learning/meta-learning algorithms. I would be interested in whether the authors think that any practical advice for the machine learning practitioner can be derived  from their experiments."
                },
                "questions": {
                    "value": "cf. weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6160/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698326692153,
            "cdate": 1698326692153,
            "tmdate": 1699636668885,
            "mdate": 1699636668885,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Bhv4rF17lv",
                "forum": "Zz61cEY84L",
                "replyto": "qiY1LXgLNr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6160/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6160/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review. We are glad you found our writing style excellent, and our experiments well explained. We note that our work offers a path to get intuition on meta-learning strategies, and hopefully motivate further theoretical work on this tractable setting. Please, see below our response to your comments.\n\n> Relevance for modern machine learning models and practical advice\n\nTo show relevance in more complex models, we added a new experiment using a non-linear neural network trained on the semantic task, to then optimize the category engagement control signal to maximize value using our framework. The results of this new experiment show that the learning dynamics of linear networks resemble the ones from a non-linear network, including the optimal meta-learning signal found in this non-linear network (See qEL8 for more details).\n\nRegarding practical advice to train modern architectures, we can suggest a few things from the conclusions of our work. For example, we recover the general idea of learning rate annealing which is generally used in modern architectures (See Figure 8 in Appendix F.2). This was not forced into the meta-learning system in any way, and just comes out from finding the optimal learning rate signal to maximize value through the learning trajectory. Another example is the idea of a curriculum from easy to hard examples. Besides the links with cognitive neuroscience mentioned in App. B, there is literature on machine learning developing tools around this concept (Bengio, et al. (2009). \u201cCurriculum learning\u201d). In Sinha et al. 2021 \u201cCurriculum by smoothing\u201d, the authors blur the feature maps of a convolutional neural network (large and complex architecture) to expose the network to lower frequency components on images at the early stages of the training, while exposing the network to higher frequency components in latest stages of learning. This is a way of curriculum where lower frequency information is arguably easier, and higher frequencies are harder. Exposing the network to easy components in early stages improves learning as we also concluded using our framework. We hope more of these types of interventions can be discovered, or perhaps, fully understood in simplified settings or through mathematical analysis. We will add these examples in the extended discussion. Thank you for this question."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6160/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700278097677,
                "cdate": 1700278097677,
                "tmdate": 1700278097677,
                "mdate": 1700278097677,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0XKdCC8rpm",
                "forum": "Zz61cEY84L",
                "replyto": "Bhv4rF17lv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6160/Reviewer_QJtu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6160/Reviewer_QJtu"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the comment"
                    },
                    "comment": {
                        "value": "Thanks for your answer. Although the other reviewers tend to disagree with my assessment, I still consider the paper an interesting contribution that is presented well and provides a nice approach to reason about meta-algorithmic choices. Personally, I think such papers can be valuable for the community, even if their practical applicability might be limited. Therefore, I keep my score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6160/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672930299,
                "cdate": 1700672930299,
                "tmdate": 1700672930299,
                "mdate": 1700672930299,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "V68kyy4EaL",
            "forum": "Zz61cEY84L",
            "replyto": "Zz61cEY84L",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6160/Reviewer_LGyh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6160/Reviewer_LGyh"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors present a meta-learning framework based on control signals via discounted cumulative performance.\nIn this framework, the authors describe a model, learning dynamics and a discounted reward based on a measure of the performance.\nThe authors then present a simplifying example that helps the reader better understand the approach and then continues with a two layer linear network model, evaluating on different perspectives of a classification task.\nThe authors show how this approach generalizes other existing meta-learning methods and present different instances of meta-learning tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper is technically sound and presents a great exposition that helps the reader better understand the ideas in the paper. \nThe experiments are backed by an extensive appendix that clarifies details."
                },
                "weaknesses": {
                    "value": "The main limitation of the paper is as the authors mention, based on the linear models they use. This limits applicability.\nAnother limitation is the lack of comparison against other meta-learning instances, where the evaluation could compare computational time. However, the linear limitation probably makes this a non-important issue, and lifting it might introduce tractability problems."
                },
                "questions": {
                    "value": "I found it interesting how the control adapts as expected. However, I'm wondering if for the given linear models that you present, if we were to collapse the two layers into a single one, your meta-learning turns into a form of regularization or instance weighting?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6160/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6160/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6160/Reviewer_LGyh"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6160/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698815473862,
            "cdate": 1698815473862,
            "tmdate": 1700700741472,
            "mdate": 1700700741472,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "72V782f9cc",
                "forum": "Zz61cEY84L",
                "replyto": "V68kyy4EaL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6160/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6160/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your review. We appreciate you found our paper well explained and with an extensive description of the framework and experiments. The goal of our simplified setting is to give a better intuition on how the framework is implemented, we are glad you found it useful. Please, find our answers to each of your comments below:\n\n> Non-linear networks\n\nWe further justify our use of linear networks by providing an extra experiment on a real non-approximated non-linear network, showing we obtain similar results as when using a linear network (see answer to reviewer qEL8).\n\n> Relation to other meta-learning algorithms\n\nWe find the connection between our framework and standard meta-learning algorithms very important. This is why we show that our framework can instantiate MAML, and bilevel programming (App F1 and F2). This is not a comparison but a way to show that they are mathematically equivalent under certain parameter configurations. Describing this connection is a way to motivate the use of our framework to understand meta-learning solutions to other problems as well.\n\nWe discussed the applicability of scalable meta-learning algorithms to our setting in Appendix F, second paragraph. Scalable meta-learning algorithms that can be applied to state-of-the-art architectures are either simplifying the meta objective (compared to the value function computed in our framework), restricting the optimized meta-variables, or approximating some of the relevant quantities (like meta-gradients). As we mentioned in this discussion, \u201ceach step in our outer loop considers the entire inner loop learning trajectory and computes the gradient of the meta-loss at all time steps to capture the effect of the meta-parameters across the entire learning trajectory, not just the last step as in the referenced work. In addition, our meta-parameters can be as complex as the (size of the network) times (inner loop training iterations), in other words, our meta-variables also depend on time, increasing the complexity of the optimization problem. We are solving the full complex meta-learning problem (which is the desired target in both references), by considering a simpler model, instead of approximating our computation.\u201d This can be seen explicitly in App. C where we show the form of the meta-gradient to update the control signal. Further discussion of other meta-learning algorithms we are not covering with our framework can be found in Appendix F4. In addition, we included a new experiment in Appendix F3 where we meta-learn the learning rule, further showing the capabilities of our framework to relate to other meta-learning problems.\n\n> Collapsing the two layers into one:\n\nThis is an interesting question, and the answer reflects the complexity that linear network dynamics is bringing to our model.\n\nFirst, you can collapse the forward path to a single matrix, as\n$$ \\hat{Y} = W_{2}W_{1}X = WX$$\nbecause the network is linear. The best solution we can find using this network is linear regression, but the path it takes from the initial weights to the solution is given by the non-linear dynamics of $W_1$ and $W_2$, so it does not follow the same trajectory as a linear regression trained using gradient descent, due to the weight coupling between $W_{1}$ and $W_{2}$. The next step we could do is to compute gradient descent updating $W$ directly. We included the derivation of the learning dynamics of $W$ under a control signal which can be written in closed form (see Appendix G2). Each $W(t)$ depends on every previous value of the control signal g, giving a non-trivial relation that does not look like a common regularization (it might regularize in some way that is not explicit, such as finding sparser solutions as shown in the results of Section 6). The reason for this is that $W(t)$ reduces the loss instantly, while g reduces the cumulated loss. This can be also seen in the computation of the gradient of the control signal in App. C. Since we are optimizing the control signal in the gradient flow regime, there is no such thing as \u201csingle samples\u201d from the dataset in the dynamics, but rather the summarized statistics of the data as $\\Sigma_{x}$, $\\Sigma_{xy}$ and $\\Sigma_{y}$, hence instance weighting does not apply."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6160/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700271897832,
                "cdate": 1700271897832,
                "tmdate": 1700271897832,
                "mdate": 1700271897832,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xpjli0i4Lf",
                "forum": "Zz61cEY84L",
                "replyto": "72V782f9cc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6160/Reviewer_LGyh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6160/Reviewer_LGyh"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the clarifications and I will raise my score"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6160/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700700716804,
                "cdate": 1700700716804,
                "tmdate": 1700700716804,
                "mdate": 1700700716804,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "crQBdB7F4C",
            "forum": "Zz61cEY84L",
            "replyto": "Zz61cEY84L",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6160/Reviewer_eAk8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6160/Reviewer_eAk8"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a framework for meta-learning where the learning dynamic can be analytically solved. The framework optimizes the control signals through gradient descent by considering  the total discounted learning performance minus control costs as an objective. Analytical tractability is reached by using a simple two-layer linear neural network that simplifies the dynamic. The authors show that they can model MAML, Bilevel Programming and other meta-learning methods with the introduced framework."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) By suitable choice of the control signal, one can model MAML, Bilevel Programming, task switch, and other techniques. \n2) The experimental section is vast, well-described, and explained."
                },
                "weaknesses": {
                    "value": "The authors propose the framework as a test bed for meta-learning. However, I have several concerns regarding its practical applicability:\n\n1) A simple two-layer linear neural network is used, so it may not account for all the effects during training, and the results may not translate to more complex NNs. \n2) It's likely one will need to find a solution for every novel considered intervention.\n3) I don\u2019t think all interventions can be modeled within this framework, even when restricted to the context of two-layer linear neural networks. (please see the Question section)\n\nSo I believe the framework may have limited usage as a test bed."
                },
                "questions": {
                    "value": "1) Will the framework be able to find tractable solutions if we consider learning rules as a control signal (e.g. [1])? \n2) To what extent can we generalize control signals in a two-layer linear neural network while still maintaining solution tractability?\n3) Do you think it is possible to develop task switching in your framework when one doesn't know what task will be next in advance?\n\nSmall incorrectness. \nImages (b) and (c) should be swapped In Fig. 2. \n\n[1] Andrychowicz, Marcin, et al. \"Learning to learn by gradient descent by gradient descent.\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6160/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698873826228,
            "cdate": 1698873826228,
            "tmdate": 1699636668650,
            "mdate": 1699636668650,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Kfs1eTvymA",
                "forum": "Zz61cEY84L",
                "replyto": "crQBdB7F4C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6160/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6160/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer Summary"
                    },
                    "comment": {
                        "value": "Thanks for your review. We are glad you found the experimental section vast and well explained. As we mentioned in other responses, our framework can accommodate a large set of interventions, mostly in the context of cognitive neuroscience, but we believe it might help the study of meta-learning strategies in machine-learning models as well. In this work, we have shown at least 10 (+ 2 more in the revised paper) different combinations of learning settings and interventions, describing them thoroughly. Our goal is to show that many more interventions can be studied using this framework, for example, the one you suggest on learning the learning rules [1]. We have added an experiment in this direction. We describe more details below."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6160/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700271067583,
                "cdate": 1700271067583,
                "tmdate": 1700271067583,
                "mdate": 1700271067583,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OMQJO5ThUT",
                "forum": "Zz61cEY84L",
                "replyto": "crQBdB7F4C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6160/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6160/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Detailed Answer"
                    },
                    "comment": {
                        "value": "> W1: Use of linear networks and comparison to non-linear networks.\n\nAs mentioned in the answer to reviewer qEL8, the dynamics of linear networks are non-linear with a non-convex landscape, resembling the dynamics of more complex non-linear neural networks. The part that is linear is the forward path, but the control signal is dealing with essentially a non-linear dynamical system due to the nonlinear loss function and weight coupling between the first and second layers as described in Sec 3. We run an additional experiment to show this, please see the answer to reviewer qEL8.\n\n> W2: It's likely one will need to find a solution for every novel considered intervention.\n\nWe agree with your statement. However, as shown in the appendix for each of the models, the math necessary to write the differential equations describing learning as a function of the control signal, in most cases, is standard gradient computation. Your intuition is right, changing the intervention of the control signal or the learning system, will change these equations. This is part of the challenge when extending to non-linear networks. Techniques to describe learning dynamics in non-linear networks exist (see paragraph 3, appendix A), but they have their own drawbacks in terms of mathematical and computational tractability, or required limits in input or hidden dimensionality to obtain closed-form differential equations describing the dynamics. This is a direction we are currently exploring and we can definitely see a path forward, but this is beyond the scope of the paper, since the general framework (Sec 2) presented here is the same.\n\n> W3, Q1 and Q2: Other interventions and learning rules as a control signal\n\nAs you mentioned in your review, the experiments presented in this work are vast, and due to the flexibility of our framework, we do not believe it is possible to cover all possible interventions in a single manuscript. The tractability of each control intervention will largely depend on the learning system, data distribution, and degrees of freedom of the control signal. This is all a design choice, for example, in App. H.1.1 we discussed the possibility of using a low-rank basis for the control signal, which might simplify computation and search space. We acknowledge that not all meta-learning phenomena can be instantiated with our framework. There is a literature in meta-learning that we are not covering with our framework, where there is no explicit variable that is meta-optimized. We discussed this in App. F4.\n\nThank you for the suggestion, using learning rules as control signals is possible, and we now include an experiment demonstrating one approach. In particular, we have considered the setting of \u201cCao, Summerfield, and Saxe et al. 2020, NeurIPS\u201d, which describes a space of possible learning rules using two parameters, spanning gradient descent, Hebbian contrastive learning, quasi-predictive coding, Hebbian and anti-Hebbian rules. These rules instantiate several popular ideas in theoretical neuroscience. Concretely, for the deep linear network setting, the weight updates are\n\n$$ \\Delta W_{1} = \\Delta W_{1}^{C}(\\gamma) + \\Delta W_{1}^{H}(\\eta), \\hspace{0.2cm} \\Delta W_{2} = \\Delta W_{2}^{C}(\\gamma),$$\n\nwith $\\Delta W_{i}^{C}$ being a contrastive learning rule, and $\\Delta W_{i}^{H}$ a Hebbian learning rule, which are controlled by the parameters $\\gamma$ and $\\eta$ respectively.\nUsing our framework, we can find the optimal $\\gamma(t)$ and $\\eta(t)$ that maximizes value as described in Sec 2. By inferring these parameters, the system is deciding which learning rule to employ within this space over learning. We found that the optimal learning rule is consistent with the original Cao et al. 2020 paper. We provide a new experiment in App F3 which will be referenced in the main text. These methods could offer a route to examining how a learning agent might adapt its learning algorithm to better learn different tasks, for instance. We believe this additional experiment further shows the flexibility of our framework to accommodate different meta-learning settings.\n\n> W3: Switching without knowing the next task in advance\n\nThis is an important future direction. We note that in many cognitive science experiments, task switches happen at predictable times (after every 50 trials, say), and our analysis is directly applicable to this setting. To consider the case where task switches are unpredictable would require extensions: fundamentally, one could compute the expected loss over possible next tasks, either analytically in certain cases or through sampling. In essence, while we roll out a single learning trajectory corresponding to a predictable sequence of tasks, a sampling approach would roll out many trajectories with possible next tasks, and construct the loss function as the sum of these tasks. The control signal would then be optimized to perform well for the distribution of switches."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6160/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700271398367,
                "cdate": 1700271398367,
                "tmdate": 1700271434566,
                "mdate": 1700271434566,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3gwOctRBXU",
            "forum": "Zz61cEY84L",
            "replyto": "Zz61cEY84L",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6160/Reviewer_qEL8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6160/Reviewer_qEL8"
            ],
            "content": {
                "summary": {
                    "value": "The works introduces a learning effort framework which is capable of optimizing control signals on objectives with discounted cumulative performance throughout learning. Frameworks and settings being analyzed in the work are meta-learning, curriculum learning, and continual learning and a number of results are provided, showing how and when the control effort might be helpful under linear settings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The authors present a learning effort framework over a number of problem settings to analyze optimal strategies for learning. Having an understanding and intuition about this can help our current deep learning design problems learn better.\n2. The document motivates the problem well, emphasizing the importance of the work.\n3. The arguments made by the work are linked to cognitive science and neuroscience, which can be used to get inspiration from when designing our current models.\n4. The work conducts experiments over multiple paradigms, including meta-learning & continual learning."
                },
                "weaknesses": {
                    "value": "Some of the areas that the work could be improved upon:\n1. As pointed out by the authors themselves, a limitation is the assumption of linear models. Since the motivation behind the current work is to provide ways in improving the current neural networks, more analysis on non-linear systems is needed. Although very large neural networks are hard to analyze, simpler variations could be considered for non-linear settings to make this direction even more interesting.\n2. Optimization is a big challenge in neural networks. An analysis focused on the effect of different optimization techniques might be helpful when extending the work on non-linear settings, which is currently being analyzed in only simpler artificial settings.\n3. A number of other biases exist in the cognitive literature, eg having modular systems in which different modules do different tasks, or learn them over multiple tasks. Analyzing this explicitly can be an interesting addition to the work.\n4. Figure 1 could be made a bit larger for better readability."
                },
                "questions": {
                    "value": "1. Have the authors come across any unexpected results while analyzing the settings? For example, for curriculum learning where defining the curriculum itself might effect the learning in unexpected ways.\n2. Is it possible to include some experiments from non-linear models (without the approximation) to address the concerns from the \"weaknesses\" section?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6160/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6160/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6160/Reviewer_qEL8"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6160/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698888402073,
            "cdate": 1698888402073,
            "tmdate": 1700687507074,
            "mdate": 1700687507074,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sTOgpFWVAk",
                "forum": "Zz61cEY84L",
                "replyto": "3gwOctRBXU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6160/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6160/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer Summary"
                    },
                    "comment": {
                        "value": "Thanks for your thoughtful review of our paper. We are pleased that you found our paper is well-motivated, offers useful intuition, contains many experiments in diverse settings, and usefully highlights links to cognitive neuroscience, which is a main purpose of our paper. Our goal is to show the flexibility of the framework to account for other learning settings that go beyond the experiments we presented here. Regarding your comments, while we agree that the use of linear networks is a limitation of the current paper, it is not an in-principle limitation of the framework itself. As in other parts of deep learning theory, we start with linearity as a useful prerequisite before tackling nonlinear extensions, and emphasize that deep linear networks still yield a nonconvex loss landscape and nonlinear dynamics that bears similarity to nonlinear networks. We further support the relevance of our deep linear network analysis by running our framework on a non-linear network, showing that similar qualitative results hold for non-linear networks. We also indicate the path forward to extensions of this framework to include non-linear network dynamics."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6160/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700270893811,
                "cdate": 1700270893811,
                "tmdate": 1700270893811,
                "mdate": 1700270893811,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9mjGlDFj9C",
                "forum": "Zz61cEY84L",
                "replyto": "3gwOctRBXU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6160/Reviewer_qEL8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6160/Reviewer_qEL8"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your responses for the rebuttal."
                    },
                    "comment": {
                        "value": "Thank you for clarifying the concerns raised, including concerns from other reviewers. \nI am updating my scores and will continue updating them after proof-reading the document again to see if all comments, including from other reviewers, have been addressed."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6160/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687638980,
                "cdate": 1700687638980,
                "tmdate": 1700687638980,
                "mdate": 1700687638980,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]