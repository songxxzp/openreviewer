[
    {
        "title": "Diffusion Sampling with Momentum for Mitigating Divergence Artifacts"
    },
    {
        "review": {
            "id": "iomCTP2IVu",
            "forum": "HXc5aXeoc8",
            "replyto": "HXc5aXeoc8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4631/Reviewer_uXih"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4631/Reviewer_uXih"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces the Heavy Ball (HB) momentum into diffusion numerical methods to expand the stability regions. Meanwhile, the authors propose the high-order method, Generalized Heavy Ball (GHVB), to select the suitable method. Experiments show that the proposed HB and GHVB improves existing on both pixel-based and latent-based diffusion model in reducing artifacts and improving image quality."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The authors introduce the Heavy Ball (HB) momentum into existing diffusion methods to expand the stability regions. And they propose a high-order method, Generalized Heavy Ball (GHVB), to trade off between accuracy and artifact suppression.\n2. The analyses are adequate. Through visualization and theoretical analysis, it is discovered that the small stability regions lead to model artifacts.\n3. The experiments are extensive. The authors apply HB and GHVB on pixel-based and latent-based diffusion models to prove the effectiveness of the proposed method.\n4. The authors also provide the code, which shows the solidness of the work."
                },
                "weaknesses": {
                    "value": "The paper primarily experiments with 10 or more generation steps. But, it lacks analyses of extreme cases, such as one or two steps. It is suggested to evaluate the effectiveness of the proposed methods in these scenarios, e.g., one or two steps. For instance, the consistency model [1] performs well in one- and few-step generation. How effective is the method proposed in this paper compared with CM?\n\n[1] Consistency Models, Song, Yang and Dhariwal, Prafulla and Chen, Mark and Sutskever, Ilya"
                },
                "questions": {
                    "value": "1. How effective are the proposed methods in extremely small generation steps, such as one or two?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4631/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4631/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4631/Reviewer_uXih"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4631/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697549780426,
            "cdate": 1697549780426,
            "tmdate": 1699636442584,
            "mdate": 1699636442584,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aNrNXYzNpX",
                "forum": "HXc5aXeoc8",
                "replyto": "iomCTP2IVu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4631/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4631/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal for Reviewer uXih"
                    },
                    "comment": {
                        "value": "> The paper primarily experiments with 10 or more generation steps. But, it lacks analyses of extreme cases, such as one or two steps. It is suggested to evaluate the effectiveness of the proposed methods in these scenarios, e.g., one or two steps. For instance, the consistency model [1] performs well in one- and few-step generation. How effective is the method proposed in this paper compared with CM?\n> How effective are the proposed methods in extremely small generation steps, such as one or two?\n- We appreciate the reviewer's suggestion to evaluate in extreme cases, particularly with one or two generation steps. Currently, every state-of-the-art high-order method relies on previous evaluations to better estimate the next evaluation point, and without a certain amount of previous evaluations, high-order methods will perform the same as low-order methods. In other words, all methods tend to yield similar results (see result in this [link](https://pic.in.th/image/TVMHCp)). Consequently, the performance of sampling becomes less dependent on the sampling method and more reliant on the model's effectiveness, especially when the number of steps is extremely low, such as one or two steps. So, when comparing with CM, any training-free methods will yield very similar, if not the same, result which can be seen in previous works [1,2].\n- It is true that methods like the consistency model can yield the extreme efficiency of one- or two-step generation. However, they require creating new models by either distilling old ones or training from scratch. This is contrary to our approach, which focuses on designing a training-free pipeline that can be applied to any existing models.\n- [2] Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4631/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700116434558,
                "cdate": 1700116434558,
                "tmdate": 1700116455104,
                "mdate": 1700116455104,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RHZpGRrkc6",
                "forum": "HXc5aXeoc8",
                "replyto": "aNrNXYzNpX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4631/Reviewer_uXih"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4631/Reviewer_uXih"
                ],
                "content": {
                    "title": {
                        "value": "After rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your rebuttal. The authors' analysis of extreme cases is reasonable, and therefore, a comparison with CM is not necessary. I maintain my original rating."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4631/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656759218,
                "cdate": 1700656759218,
                "tmdate": 1700656759218,
                "mdate": 1700656759218,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tYrh1K9WGE",
            "forum": "HXc5aXeoc8",
            "replyto": "HXc5aXeoc8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4631/Reviewer_Z3BT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4631/Reviewer_Z3BT"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on accelerating general diffusion sampling, where both unconditional and guided sampling are considered. Motivated by the observation that recent higher-order numerical methods would lead to diverging artifacts at lower sampling steps, the authors propose to incorporate heavy ball (HB) momentum into existing diffusion ODE solvers such as DPM++ and PLMS to mitigating their artifacts. In addition, an improved high-order version, namely generalized heavy Ball (GHVB) is also presented in this paper.  Experimental results have shown the effectiveness of this proposal."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1), Both pixel-based and latent diffusion models are considered in this paper.\n\n2), The presentation is overall easy to follow.\n\n3), Good practical extension to DPM++ and PLMS.\n\n4), The literature over existing high order ODE solvers seems up to date."
                },
                "weaknesses": {
                    "value": "1), The technical novelty behind this work seems to be not significant. The main techniques used in this paper are directly borrowed from Polyak\u2019s heavy ball (HB) momentum method, a conventional optimization algorithm. Besides, the main improvements of this work are built based on DPM++ and PLMS.\n\n2), While two methods are proposed in the same paper, it is unclear which one should be used under what circumstances. The paper only gives some vague statements without comprehensive comparison. \n\n3), While guided diffusion sampling is considered, the effectiveness of the HB/GHVB under different scaling factor \u201cs\u201d is not well discussed."
                },
                "questions": {
                    "value": "1), While the authors mentioned that the problem setup is more challenging in this paper than previous works, it is unclear what the challenges are. More discussions about why PLMS and DPM-Solver ++ perform worth than their original claims would strengthen this proposal.\n\n2), Given that the 1000-Step DDIM\u2019s sample is considered the benchmark, it would be reasonable to include evaluation metrics such as L2, LIPIS, and FID comparing HB/GHVB to DDIM, as depicted in Figure 11.\n\n3), In Figure. 12, the authors attribute the inconsistency of GHVB 2.5 and 3.5 to estimated error or other sources of error without further justifications. It would be helpful to discuss this more for better understanding. \n\n4), Seems the comparisons and discussions between HB and GHVB are not sufficient in the paper\u2019s current state. There is no clear cut which method is better for both conditional and unconditional diffusion sampling."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4631/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4631/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4631/Reviewer_Z3BT"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4631/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698785027598,
            "cdate": 1698785027598,
            "tmdate": 1699636442502,
            "mdate": 1699636442502,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qFZO85cb8x",
                "forum": "HXc5aXeoc8",
                "replyto": "tYrh1K9WGE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4631/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4631/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal for Reviewer Z3BT (part 1/2)"
                    },
                    "comment": {
                        "value": "> The technical novelty behind this work seems to be not significant. The main techniques used in this paper are directly borrowed from Polyak\u2019s heavy ball (HB) momentum method, a conventional optimization algorithm. Besides, the main improvements of this work are built based on DPM++ and PLMS.\n\n- Our GHVB, a high-order generalization of Polyak's heavy ball momentum method, is an original approach not borrowed from elsewhere. Though, we acknowledge that the HB (not GHVB) algorithm may be perceived as an application of existing concepts like momentum. Nonetheless, our distinct contribution is being the first to employ and analyze HB as an ODE solver within the context of diffusion sampling. Our study also provides novel findings resulting from extensive experiments and well-tested hypotheses, which serve as a basis for practical adoption\n- Additionally, our HB method may seem to build upon previous methods like DPM++ and PLMS because it is designed as an add-on specifically aimed at addressing their divergence artifact problem.\n\n> While two methods are proposed in the same paper, it is unclear which one should be used under what circumstances. The paper only gives some vague statements without comprehensive comparison.\n> Seems the comparisons and discussions between HB and GHVB are not sufficient in the paper\u2019s current state. There is no clear cut which method is better for both conditional and unconditional diffusion sampling.\n- A direct comparison is presented in Figure 5, demonstrating that both HB and GHVB effectively reduce artifacts. However, HB exhibits a faster accuracy drop, leading to blurry images.  A discussion between these methods can be found in Section 7, emphasizing HB's role as an add-on for artifact reduction, while GHVB offers flexibility in method selection. Therefore, we advocate for choosing the high-order generalization method (GHVB) over first-order methods like HB. Ideally, the selection should prioritize the highest-order method whose stability region still covers all eigenvalues, as detailed in Appendix S, Topic Q2 (moved to Appendix T in the revision).\n\n> While guided diffusion sampling is considered, the effectiveness of the HB/GHVB under different scaling factor \u201cs\u201d is not well discussed. \n- Please see Figures 24-32 in Appendix N (moved to Fig. 25-33 in Appendix O), where we demonstrate the effectiveness of our HB and GHVB under various guidance scales for guided diffusion sampling.\n\n> While the authors mentioned that the problem setup is more challenging in this paper than previous works, it is unclear what the challenges are. More discussions about why PLMS and DPM-Solver ++ perform worth than their original claims would strengthen this proposal.\n- A short answer: The challenges here refer to the ability to speed up and reduce the number of sampling steps while avoiding divergence artifacts. All these algorithms generate artifacts, but only when the sampling steps drop below a certain threshold, which might not always be tested or demonstrated in their papers. In theory, eigenvalues and step size are primary factors leading to divergence in ODE solving. Reducing the step size is the most obvious way to make the problem more challenging, as highlighted in Section 1's second paragraph and depicted in Figure 1. Another approach involves altering sampling problems' eigenvalues, such as those using guidance sampling, higher guidance scales, LoRA fine-tuning, ControlNet, or the \"Reference-Only\" pipelines detailed in Appendices M and N (moved to N and O). These sampling problems tend to produce more artifacts at the same number of sampling steps compared to standard diffusion sampling, making them more challenging in our context."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4631/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700498257291,
                "cdate": 1700498257291,
                "tmdate": 1700628394066,
                "mdate": 1700628394066,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "R1Nxnhfz8o",
                "forum": "HXc5aXeoc8",
                "replyto": "tYrh1K9WGE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4631/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4631/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal for Reviewer Z3BT (part 2/2)"
                    },
                    "comment": {
                        "value": "> Given that the 1000-Step DDIM\u2019s sample is considered the benchmark, it would be reasonable to include evaluation metrics such as L2, LIPIS, and FID comparing HB/GHVB to DDIM, as depicted in Figure 11.\n- We agree with and appreciate your suggestion and will update the evaluation to use the 1000-step DDIM including Figure 11. According to the convergence theorem, numerical methods converge to the same exact solution with a high number of steps. Hence, this modification should not impact the final outcome.\n\n> In Figure. 12, the authors attribute the inconsistency of GHVB 2.5 and 3.5 to estimated error or other sources of error without further justifications. It would be helpful to discuss this more for better understanding.\n- The discrepancies observed in the plots depicted in Figure 12, which appear less consistent with the theoretical predictions, may come from various sources and reasons. One possible explanation is the limited precision of neural networks and the already very low errors for 640-step sampling of Figure 11 ($\\sim 10^{-3}$). This scenario amplifies the sensitivity of the $e_{new} / e_{old}$ ratio to computational inaccuracies, even as small as $10^{-4}$.To illustrate this potential behavior, we demonstrate that errors from both the 999-step and 1,000-step DDIM can vary significantly by up to 0.0026. Theoretically, this difference should align much closer to zero. Additional sources of error include the time discretization nature of the diffusion network, which might not consistently across all number of steps (e.g., 320 vs 640). These errors are negligible when the total error is\u00a0 large but can significantly compound and affect our *empirical approximation* of convergence order when dealing with very small errors.\n- [link to the experiment](https://colab.research.google.com/drive/11d5eXZHaG2qCJY_TgB8-zq1fagSmA7b8?usp=sharing)"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4631/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700498385905,
                "cdate": 1700498385905,
                "tmdate": 1700498385905,
                "mdate": 1700498385905,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "r1ZYJrb5kX",
            "forum": "HXc5aXeoc8",
            "replyto": "HXc5aXeoc8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4631/Reviewer_DpnC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4631/Reviewer_DpnC"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers the artifacts problem of ODE/SDE-based diffusion sampling. Authors thought that the divergence artifacts are caused by the stability regions of high-order numerical methods for solving ODEs and proposed two solutions for expanding the stability regions of current diffusion numerical methods, called Heavy Ball (HB) momentum and Generalized Heavy Ball. And in the case of low-step sampling, the proposed methods are effective in reducing artifacts. But the actual improvement on diffusion sampling acceleration is unlear.\n\n-------------\nPost-rebuttal: I read the rebuttal and thanks for the authors' efforts. I would like to keep my score."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThe divergence artifacts problem is theoretically linked with the stability region of high-order numerical solvers for ODEs. The insight is very helpful for the design of diffusion sampling methods. \n2.\tTo enlarge the stability region, authors proposed Heavy Ball (HB) and generalized Heavy Ball (GHVB) as two solution without any training. Experiments show that the divergence artifacts are great mitigated in a low-step sampling case.\n3.\tThis paper is well organized and solid in theory."
                },
                "weaknesses": {
                    "value": "1.\tThe proposed method should be compared with the state-of-art methods in reducing divergence artifacts if it is a big challenge in diffusion models.\n2.\tThe stated motivation is diffusion model acceleration. Experiments are limited in comparing the results of few-step sampling, lacking clear numerical experiments in model acceleration.\n3.\tIt seems that the proposed methods show superior performance only in extremely low sampling steps. In the case of decent image quality, the improvement on sampling step is unclear."
                },
                "questions": {
                    "value": "1.\tThe main difference between HB and GHVB is that HB calculates the moving average after summing high-order coefficients, whereas GHVB calculates it before the summation. Why does such a difference lead a larger stability region? \n2.\tCan the divergence artifacts be solved or mitigated by improving the dynamic range of pixel?\n3.\tWith additional training, what is the proposed methods\u2019 complexity or cost?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4631/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4631/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4631/Reviewer_DpnC"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4631/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698803536063,
            "cdate": 1698803536063,
            "tmdate": 1700697233224,
            "mdate": 1700697233224,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iZGPsOu8fa",
                "forum": "HXc5aXeoc8",
                "replyto": "r1ZYJrb5kX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4631/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4631/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal for Reviewer DpnC (part 1/2)"
                    },
                    "comment": {
                        "value": "> The proposed method should be compared with the state-of-art methods in reducing divergence artifacts if it is a big challenge in diffusion models.\n- As far as we understand, no other researchers have identified divergence artifacts as a problem in diffusion sampling. Therefore, we solely compare our approach with the current state-of-the-art diffusion sampling techniques outlined in both our main paper and appendix.\n\n\n> But the actual improvement on diffusion sampling acceleration is unclear\n> It seems that the proposed methods show superior performance only in extremely low sampling steps. In the case of decent image quality, the improvement on sampling step is unclear.\n> The stated motivation is diffusion model acceleration. Experiments are limited in comparing the results of few-step sampling, lacking clear numerical experiments in model acceleration.\n- We would like to first clarify the term 'acceleration\u2019 in the context of our work. Here, acceleration refers to our ability to decrease the number of sampling steps while still producing good results without degeneration or divergence artifacts. For example, a sampling technique that produces artifacts using 10 sampling steps is considered slower than another that still remains artifact-free at step 10, when applied to the same diffusion model. In this context, the number of steps is a reasonable representation of speed, as the extra computation time for each step\u2014due to computations related to momentum or other high-order aggregation\u2014is negligible compared to the network evaluation time. A method that accelerates sampling by 25%, for example, means that the number of sampling steps can be reduced by 25% without the onset of artifacts.\"\n- Regarding your concern, please see Appendix M (moved to N in the revision) where we have provided comprehensive analyses of speed improvement, some of which is reproduced here. In Appendix M.1 (N.1), our experiments demonstrate that HB and GHVB reduce sampling time by 16.53% and 25.38%, respectively, compared to PLMS4 without momentum. Moreover, DPM-Solver++ with HB momentum achieves an 18.22% reduction in sampling time compared to the same algorithm without momentum. In Appendix M.2 (N.2), our experiments indicate significant reductions in sampling time from PLMS4, amounting to 32.40% and 42.92% with HB and GHVB, respectively. We believe these quantitative results show that our algorithms yield clear sampling speedups.\n- Additionally, our experiments encompass a wide range of numbers of sampling steps, not just extremely low ones. For instance, in Section 5.1, we experimented with step numbers ranging from 10 to 640. In Appendix M (moved to N), they range from 10 to 300 steps. In certain experiments requiring FID score computation, we limited the range due to computational resources. For examples, we use (1) the 10 to 30 range in Section 5.2 and Appendix I (moved to J) and (2) the 6 to 25 step range in Section 5.3 and Appendix J (moved to K). We don't consider these ranges to be extremely low.\n\n> The main difference between HB and GHVB is that HB calculates the moving average after summing high-order coefficients, whereas GHVB calculates it before the summation. Why does such a difference lead a larger stability region?\n- In the concluding paragraph of Section 2.2, we highlighted that higher-order methods tend to have smaller stability regions. Given HB's first-order convergence compared to GHVB's designed high-order convergence, HB naturally exhibits a larger stability region. To understand how formulation differences impact convergence orders, please refer to Section 4.2 and Appendix F (moved to G). Higher convergence orders indicate that more terms are matched in the Taylor expansion, resulting in a more accurate approximation. Notably, GHVB matches more terms than HB"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4631/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700116201756,
                "cdate": 1700116201756,
                "tmdate": 1700628075934,
                "mdate": 1700628075934,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MFcmisTCrq",
                "forum": "HXc5aXeoc8",
                "replyto": "r1ZYJrb5kX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4631/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4631/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal for Reviewer DpnC (part 2/2)"
                    },
                    "comment": {
                        "value": "> Can the divergence artifacts be solved or mitigated by improving the dynamic range of pixel?\n\n- The short answer is unlikely. We haven't found any evidence suggesting that improving the range of pixels can mitigate the divergence artifacts. Techniques like dynamic thresholding [1], which clamp the $x_0$ prediction within some thresholds, do not effectively prevent these artifacts (see below). Moreover, in Appendix I (moved to J), we've utilized a static thresholding technique [1], capping the $x_0$ value to the maximum and minimum pixel values at each iteration, but this method also doesn't alleviate the issue. As depicted in Figure 16, the artifacts persist even when their values are capped.\n\n- Here, we conducted a test involving dynamic thresholding specifically on Stable Diffusion. We utilized huggingface\u2019s diffusers implemented on DPM-Solver++ from the provided [link](https://github.com/huggingface/diffusers/blob/v0.23.0/src/diffusers/schedulers/scheduling_dpmsolver_multistep.py#L74). The resulting image, displayed in ([Here](https://pic.in.th/image/T0jFOy)), did not demonstrate a reduction in divergence artifacts.\n\n[1] : Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding (2022)\n\n> With additional training, what is the proposed methods\u2019 complexity or cost?\n- Our method is training-free and applicable to any diffusion sampling. We believe there might be some typos in the question. In case the reviewer meant the complexity and/or cost induced by our sampling algorithms, the inclusion of one line of code for computing the moving average has a very small impact on sampling time, as detailed in Table 12 of Appendix R (moved to S). Both of our methods show no significant differences in sampling times compared to other methods. We hope this clarifies these aspects."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4631/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700116303268,
                "cdate": 1700116303268,
                "tmdate": 1700628230814,
                "mdate": 1700628230814,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ki346JFC86",
            "forum": "HXc5aXeoc8",
            "replyto": "HXc5aXeoc8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4631/Reviewer_jPhx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4631/Reviewer_jPhx"
            ],
            "content": {
                "summary": {
                    "value": "This submission suggests to use higher order numerical scheme (heavy ball momentum coupled with higher order multi-step methods in numerical ODE) to compute the diffusion process in computer vision."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Authors' effort in experiments seem to be solid and thorough. \nAuthors have also been patient to review basics of stability concept in numerical ODEs."
                },
                "weaknesses": {
                    "value": "I recommend that authors add a paragraph explaining what \"sampling\" means in the context of diffusion in the appendix, so that the content can be more self-contained. From what I understand about the main text, authors mean generating/inferring an image with trained diffusion models. This is not equivalent to the meaning of illustrating the distribution of all potentially generated images given underlying diffusion models.\n\n\nI also suggest that authors make a table to list all used numerical formats, explicitly, either in the main text or in appendix, to generate images. In this way, readers can associate the listed methods in each table/figure with specific algorithms. \nThe current presentation stops at a conceptual derivation of discrete update format instead of concrete update formula. In a similar spirit, it will be also helpful for authors to detail the setup of the training paradigm (specifically, what the loss function is for training)."
                },
                "questions": {
                    "value": "- Are metrics \"high-frequency error norm (HFEN)\" [MR image reconstruction from highly undersampled k-space data by dictionary learning, Ravishankar and Bresler, 2011] and Structural Similarity Index (SSIM) potentially relevant to measure the divergence artifacts (section 5.1)? If yes, then reporting evaluation results in these two metrics can be helpful.\n\n- Conceptually, I would like to understand better what authors mean by \"classifier-guided diffusion sampling\". What is the difference (conceptually and when it comes to implementation) between classifier-guidance and text-prompt based generation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4631/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698864959678,
            "cdate": 1698864959678,
            "tmdate": 1699636442246,
            "mdate": 1699636442246,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qn1iEj5auG",
                "forum": "HXc5aXeoc8",
                "replyto": "Ki346JFC86",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4631/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4631/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal for Reviewer jPhx"
                    },
                    "comment": {
                        "value": "> add a paragraph explaining what \"sampling\" means\n> add table list all used numerical formats, explicitly,\n> Detail about setup of the training paradigm\n- Thank you for the valuable feedback. We appreciate the suggestions. We are happy to add a section in the appendix to explain more about diffusion sampling, create a table summarizing numerical methods for diffusion solver, and include a section with detailed information on diffusion training. (added to Appendix A in the revision)\n\n> What is classifier-guided diffusion sampling? What is the difference (conceptually and when it comes to implementation) between classifier-guidance and text-prompt based generation?\n- We acknowledge the confusion and will address it by adding a section in the appendix to provide a clearer explanation about guided diffusion sampling.\nIn short, classifier-guided diffusion sampling [1] utilizes another model, typically a classifier model, to guide the sampling process towards a desired direction. This is in contrast to classifier-free sampling [2], where the diffusion model itself directs the outcome. Text-to-Image generation can be achieved through both classifier-guided sampling, as seen in CLIP-guided sampling, or classifier-free sampling. However, in approaches like Stable Diffusion, the sampling is predominantly considered classifier-free.\n\n- [1] : Diffusion Models Beat GANs on Image Classification\n- [2] : Classifier-Free Diffusion Guidance\n\n> Are metrics HFEN, SSIM potentially relevant to measure the divergence artifacts?\n- In contrast to the magnitude score, the HFEN and SSIM metrics do not directly quantify the extent of divergence artifacts in the results. However, they can detect divergence by comparing the differences between individual samples and their convergence results, akin to the use of l2 and LPIPS metrics, which we have employed throughout our paper. We appreciate this suggestion and will include these measurements in our paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4631/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700498015639,
                "cdate": 1700498015639,
                "tmdate": 1700627754138,
                "mdate": 1700627754138,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]