[
    {
        "title": "Formal Verification for Neural Networks with General Nonlinearities via Branch-and-Bound"
    },
    {
        "review": {
            "id": "mBVpOBg0n5",
            "forum": "ivokwVKY4o",
            "replyto": "ivokwVKY4o",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6011/Reviewer_abyN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6011/Reviewer_abyN"
            ],
            "content": {
                "summary": {
                    "value": "This work extends the linear-bound-propagation and BaB based neural network verification framework $\\alpha\\beta$-CROWN to support branching on any node of the computational graph and thus on inputs to arbitrary non-linearities. Technically, these additional constraints are enforced using Lagrange multipliers. Additionally, a novel branching heuristic, BBPS, is introduced that leverages precomputed linear bounds as \"shortcuts\" to compute better approximations of the branching effect. The effectiveness of the method is demonstrated on a wide variety of activation functions."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The tackled issue of (certified) adversarial robustness is of high importance.\n* To the best of my knowledge, this paper is the first to describe the application of the popular BaB paradigm to general non-linearities.\n* The paper combines well established techniques (general cutting planes and optimisable relaxation slopes) to enable branching for general non-linearities.\n* The novel branching heuristic is elegant and effective (for non-ReLU activations).\n* The extensive empirical evaluation is convincing and clearly shows improved performance over a wide range of baselines."
                },
                "weaknesses": {
                    "value": "* Prior work and novel contributions are not always distinguished clearly, i.e., it is not immediately clear that Section 3.1 describes the prior work $\\alpha\\beta$-CROWN. Similarly, it is not clear that the general branching constraints described in Section 3.3 constitute special cases of the General Cutting Planes described by Zhang et al. (2022).\n* The technical contribution beyond BBPS, seem limited as both enforcing arbitrary constraints on intermediate activations (Zhang et al. 2022) and optimising relaxation parametrisation was already possible in the  $\\alpha\\beta$-CROWN version this work builds on.\n\n\n**References**  \nZhang, Huan, et al. \"General cutting planes for bound-propagation-based neural network verification.\"\u00a0 NeurIPS 2022"
                },
                "questions": {
                    "value": "### Questions\n1) Can you give an intuition on why BBPS does not seem to yield any improvement on ReLU networks while being crucial for other non-linearities?\n2) At the bottom of page 5, you state that branching a neuron in node i only affects the linear relaxations of nonlinear nodes immediately after node i. Can tighter bounds there not lead to tighter bounds in later nodes and thus changing relaxations?\n3) What is the impact of the branching factor $K$ on the resulting precision? Can you ablate its effect on one of the CIFAR10 networks, where branching was particularly effective? Can the same neuron be split multiple times in your framework?\n\n### Conclusion \nThe authors successfully establish the effectiveness of BaB for general non-linearities and the advantages of their novel branching heuristic on a wide range of benchmarks and compared to a diverse set of baselines. While the technical novelty seems limited, I believe that demonstrating the applicability and effectiveness of established techniques in this setting is a valuable contribution in itself and am thus leaning to accept the paper. However, I believe the authors should make sure that novel contributions (Section 3.2 and 3.4) are clearly distinguished from prior work (Section 3.1 and 3.3), and have thus reduced my score. I am happy to raise it once this concern is addressed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6011/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6011/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6011/Reviewer_abyN"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6011/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698333577271,
            "cdate": 1698333577271,
            "tmdate": 1699636644850,
            "mdate": 1699636644850,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Wq5I9GGHZC",
                "forum": "ivokwVKY4o",
                "replyto": "mBVpOBg0n5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6011/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6011/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer abyN"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the review and identifying the merits of our paper. Following the insightful feedback by the reviewer, we have revised the paper to more clearly distinguish our contributions from prior works. We also respond to the questions.\n\n## Distinguishing contributions \n\n>Prior work and novel contributions are not always distinguished clearly, i.e., it is not immediately clear that Section 3.1 describes the prior work \u03b1,\u03b2-CROWN. Similarly, it is not clear that the general branching constraints described in Section 3.3 constitute special cases of the General Cutting Planes described by Zhang et al. (2022).\n\nWe have added a paragraph in the beginning of Section 3.1 to clearly mention that the overall framework follows \u03b1,\u03b2-CROWN. Compared to \u03b1,\u03b2-CROWN, our methodological  contributions are on a general branching framework for general nonlinearities and also a new branching heuristic, which are discussed in the remaining subsections of Section 3.\n\nWe have also revised Section 3.3 to clarify the relation and difference between our contribution and Zhang et al., 2022. Our general branching constraints are indeed a particular type of cutting plane constraints that can be utilized by Zhang et al., 2022 to tighten bounds in linear bound propagation. However, our contribution is on formulating general branching constraints, while Zhang et al., 2022 is mainly on utilizing general  constraints once the constraints are already formulated. \n\n## Improvement on ReLU networks\n\n>Can you give an intuition on why BBPS does not seem to yield any improvement on ReLU networks while being crucial for other non-linearities?\n\nWe study this problem in Appendix C.2. Basically, BaBSR in De Palma et al. (2021) has a backup score which dominates the performance on the ReLU networks we experimented, when the backup score is combined with BaBSR or BBPS. When the backup score is not used, BBPS still improves over BaBSR (Table 7). The backup score is specifically designed for ReLU networks, and we leave it for future work to study the possibility of designing backup scores for general nonlinearities. More details are in Appendix C.2.\n\n## Intermediate bounds after a branching\n\n>At the bottom of page 5, you state that branching a neuron in node i only affects the linear relaxations of nonlinear nodes immediately after node i. Can tighter bounds there not lead to tighter bounds in later nodes and thus changing relaxations?\n\nWe fix the intermediate bounds except for neurons that are branched. Indeed branching can potentially also tighten the intermediate bounds of later nodes. However, re-computing intermediate bounds can be costly as the number of subdomains after branching can be quite large. We only re-compute the final output bounds following \u03b1,\u03b2-CROWN (see Section B.1 of Wang et al., 2021).\n\n## Number of branches\n\n>What is the impact of the branching factor K on the resulting precision? Can you ablate its effect on one of the CIFAR10 networks, where branching was particularly effective?\n\nWe mainly used $K=3$ for the number of branches in our main experiments, to demonstrate the ability of our framework for handling a general number of branches. We have added new results in Appendix C.3 to compare the performance of $K=2$ and $K=3$. \n\n| |  Sigmoid 4x100 | Sigmoid 4x500 | Sigmoid 6x100 | Sigmoid 6x200 |\n| --- | --- | --- | --- | --- | \n| $K=2$ | 54 | 20 | 62 | 49 |\n| $K=3$ | 53 | 21 | 61 | 49 |\n\n| | Sin 4x100 | Sin 4x200 | Sin 4x500|\n| --- | --- | --- | --- |\n| $K=2$ | 54 | 34 | 20 |\n| $K=3$ | 63 | 39 | 25 |\n\nOn Sigmoid networks, we find that $K=2$ and $K=3$ yield comparable results. However, on networks with the $\\sin$ activation which is more nonlinear compared to Sigmoid, using $K=3$ significantly outperforms $K=2$. The results demonstrate the effectiveness and potential of our framework supporting a general number of branches, on NNs involving functions that are relatively more nonlinear. \n\n>Can the same neuron be split multiple times in your framework?\n\nYes, the same neuron may be split multiple times if it is selected by the branching heuristic multiple times."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558117685,
                "cdate": 1700558117685,
                "tmdate": 1700558117685,
                "mdate": 1700558117685,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "28Uc8tkwJ2",
                "forum": "ivokwVKY4o",
                "replyto": "Wq5I9GGHZC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6011/Reviewer_abyN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6011/Reviewer_abyN"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "I want to thank the authors for answering my questions. \n\nHowever, my concerns regarding the novelty of the proposed method remain. In particular, the fact that prior work already applied BaB techniques to S-shaped activations (Henriksen and Lomuscio, 2020) should have been made much clearer. Generalizing from S-shaped to general activation functions seems to mostly be an implementation rather than a methodological challenge. \n\nGiven that branching constraints for more than two branches at a time are a special case of the general cutting planes discussed by Zhang et al. (2022), it seems like the only novelty of this work is the improved branching heuristic. While it is empirically effective, I believe this contribution falls short of the standard for a publication at ICLR and can thus not recommend acceptance."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672841728,
                "cdate": 1700672841728,
                "tmdate": 1700672841728,
                "mdate": 1700672841728,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "d8xbYthrwK",
                "forum": "ivokwVKY4o",
                "replyto": "mBVpOBg0n5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6011/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6011/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your prompt response and I hope the reviewer can reconsider our contribution"
                    },
                    "comment": {
                        "value": "We greatly appreciate your prompt response!\n\nWe acknowledge that (Henriksen and Lomuscio, 2020) conducted an initial study with a similar idea in a restricted setting. However, our approach is general, thorough, principled, and systematic, and we also conducted comprehensive experiments to demonstrate the power of our approach. We achieved state-of-the-art results on many new settings (including Transformer) and also enabled new applications of NN verification in ACOPF, which could never have happened without our efforts.\n\nSimilarly, (Zhang et al. 2022) also did not consider branch and bound at all - although we share some techniques (e.g., Lagrangian duality), (Zhang et al. 2022) never demonstrated the effectiveness of branch-and-bound on general nonlinear functions, and in fact their paper used purely ReLU networks. We believe that **using well-established mathematical techniques in a new setting and demonstrating impressive empirical results** are sufficient contributions for a conference paper.\n\n**We believe the contributions of a paper are not restricted to narrowly defined \u201ccompletely new ideas\u201d**, and we hope the reviewer can consider our contribution to the field of NN verification in the long term. Many existing works are restrictive to ReLU or simple networks (like feedforward S-shape activation) and are roadblocks to making any practical impact. Our systematic extension of powerful BaB-based verification techniques will enable many novel applications for the verification of general computation graph, and has enabled completely new settings like ACOPF. None of the existing work like (Henriksen and Lomuscio, 2020) and (Zhang et al. 2022) can enable these.\n\nThank you again for your constructive feedback, and we sincerely hope you can reevaluate our contribution."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700718195523,
                "cdate": 1700718195523,
                "tmdate": 1700718222220,
                "mdate": 1700718222220,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Wijhsf9pF8",
            "forum": "ivokwVKY4o",
            "replyto": "ivokwVKY4o",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6011/Reviewer_NV4g"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6011/Reviewer_NV4g"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a generalisation of a very popular network verifier, $\\alpha$-$\\beta$-CROWN, to new nonlinearities. In particular, branching support for sigmod, tanh, sine, GeLU and multiplication is added, along with bounding support (in terms of optimizable linear bounds) for the last three. Results show that the proposed techniques are more effective than relevant baselines on the considered settings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The work extends the support of a state-of-the-art network verifier to nonlinearities beyond piece-wise linear. \nAs one would expect, the resulting framework remains effective on the considered non-linearities. In particular, the authors show that the presented framework works reasonably well on LSTM (on vision data) and ViTs."
                },
                "weaknesses": {
                    "value": "While the work is definitely of interest to the practitioners in the area, the vast majority of the presented material is a fairly straightforward extension of very well known concepts in the literature. It would not appear to me that the extensions presented great technical challenges that needed to be overcome. Indeed, while branch-and-bound is mostly employed on piece-wise nonlinearities in the context of neural network verification, it is a fairly general concept which the authors simply instantiated on more variants of the neural network verification problem.\n\nMore in detail, the support of more nonlinearities in the bounding phase is a trivial applications of concepts presented in (Zhang et al., 2018; Xu et al., 2020). And, in practice, the $\\alpha$-$\\beta$-CROWN already extended the concept of optimizable linear bound propagation beyond ReLU (through support for sigmoid and tanh). As a result, extending these ideas to more nonlinearities is quite incremental. Similarly, previous work has already extended activation splitting to non-ReLU activations (Henriksen and Lomuscio, 2020). I believe this incrementality should be acknowledged more in the motivational sections of the paper.\nWhile the branching part could have more room for improvement, the authors focus on extending a relatively old branching heuristic (BaBSR) that is typically preferred by more effective strategies in state-of-the-art works (FSB in $\\beta$-CROWN, and the custom strategy introduced in MN-BaB). Furthermore, the authors do not justify the use of ternary branching (k=3) with uniform spacing between the branching points.\n\nThe experimental results are also on mostly toy problems and with fairly small perturbation sizes (1/255 as opposed to 2/255 and 8/255 typically employed in the literature on CIFAR-10). The authors sometimes select the properties to verify by excluding the properties that would be verified by CROWN (for instance, table 3). Taking the above into account, I am not sure how significant some of the improvements with respect to pre-existing work are (for instance, $\\alpha$-$\\beta$-CROWN without branching on Figure 2 and table 3).\n\nIn conclusion, I believe that most of the merit of the work pertains to the implementation. I am not sure this meets the bar for an ICLR publication."
                },
                "questions": {
                    "value": "- Could the authors justify their choice for ternary (and uniform) branching? For instance, an ablation study on the number of branching points would be useful.\n\n- I found the explanation of the branching strategy to be quite confusing. For instance, the presentation of BaBSR heavily differs from the one from the original authors, which is based on computing coefficients that estimate the impact of splitting on the last layer bounds from the (Wong and Kolter 2018) paper. Could the authors explain why the two presentations are equivalent?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6011/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698517288027,
            "cdate": 1698517288027,
            "tmdate": 1699636644749,
            "mdate": 1699636644749,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dLqXP7KWhy",
                "forum": "ivokwVKY4o",
                "replyto": "Wijhsf9pF8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6011/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6011/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NV4g (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the review. We have revised the paper to acknowledge previous works more clearly, and we have also added results on the impact of different numbers of branches. We also respond to the concerns on the applicability of other branching heuristics, the problem complexity in the experiments, and the evaluation. Finally, we also explain the formulation for BaBSR.\n\n## Acknowledging previous works more \n\n>And, in practice, the \u03b1,\u03b2-CROWN already extended the concept of optimizable linear bound propagation beyond ReLU (through support for sigmoid and tanh). As a result, extending these ideas to more nonlinearities is quite incremental. Similarly, previous work has already extended activation splitting to non-ReLU activations (Henriksen and Lomuscio, 2020). I believe this incrementality should be acknowledged more in the motivational sections of the paper. \n\nWe have revised Section 1 to acknowledge the previous works more. We have mentioned that \u03b1,\u03b2-CROWN does support non-ReLU activations but their BaB is still restricted to ReLU. We have also extended the discussion on the previous works about BaB for S-shaped activations including Henriksen and Lomuscio, 2020.\n\n## Branching heuristic\n\n>While the branching part could have more room for improvement, the authors focus on extending a relatively old branching heuristic (BaBSR) that is typically preferred by more effective strategies in state-of-the-art works (FSB in \u03b1,\u03b2-CROWN, and the custom strategy introduced in MN-BaB). \n\nOur contribution on the branching heuristic is focused on providing a more precise estimation on the bound improvement for neural networks with general nonlinearities. FSB and the heuristic in MN-BaB still need to first estimate the bound improvement for all the neurons.\n\nFSB is based on BaBSR, and it further has a filtering mechanism, which is independent from our contribution on BBPS, as filtering can be combined with generic branching heuristics. We experiment with \"BaBSR-like + filtering\" and \"BBPS+filtering\":\n\n| |  Sigmoid 4x100 | Sigmoid 4x500 | Sigmoid 6x100 | Sigmoid 6x200 | Tanh 4x100 | Tanh 6x100 |\n| --- | --- | --- | --- | --- | --- | --- |\n| BaBSR-like | 34 | 17 | 44 | 41 | 35 | 8 |\n| BaBSR-like + filtering | 33 | 17 | 44 | 41 | 34 | 8 |\n| BBPS | 53 | 21 | 61 | 49 | 41 | 9 |\n| BBPS + filtering | 50 | 21 | 56 | 48 | 41 | 9 |\n\nWe find that the filtering does not improve the performance on these models, possibly because their improved bound estimation is not sufficiently strong compared to the additional cost. Adding filtering sometimes hurts the performance more for BBPS compared to BaBSR-like, as BBPS tends to verify more hard instances, where the additional cost tends to cause timeout more easily when the runtime is originally long for the hard instances. We leave it for future work to study improving the filtering mechanism for NNs with general nonlinearities. We have revised the paper, and we have added the results and discussions Appendix C.4. \n\nMN-BaB proposed Active Constraint Score Branching (ACS). ACS specifically considers multi-neuron relaxation for providing a better estimation, when multi-neuron relaxation is used for ReLU activations. This is not applicable in our case because we do not have multi-neuron relaxation. We have not added multi-neuron relaxation, because in Table 2, we find that \u03b1,\u03b2-CROWN without BaB (which does not have multi-neuron relaxation) can already outperform PRIMA which has multi-neuron relaxation, on the Sigmoid/Tanh networks. However, it will be interesting for future works to further consider multi-neuron relaxation and make it more effective on general nonlinearities.\n\nIn MN-BaB's branching strategy, there is also Cost Adjusted Branching which considers the cost of re-computing the intermediate bounds, which is also not applicable here. We fix the intermediate bounds except for neurons that are branched, to save the cost of re-computing the intermediate bounds. This follows \u03b1,\u03b2-CROWN (mentioned in Appendix B.1 of Wang et al., 2021)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700557959143,
                "cdate": 1700557959143,
                "tmdate": 1700557959143,
                "mdate": 1700557959143,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZTUm2fRWo2",
                "forum": "ivokwVKY4o",
                "replyto": "Wijhsf9pF8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6011/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6011/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NV4g (2/2)"
                    },
                    "comment": {
                        "value": "## Number of branches\n\n>Could the authors justify their choice for ternary (and uniform) branching? For instance, an ablation study on the number of branching points would be useful.\n\nWe mainly used $K=3$ for the number of branches in our main experiments, to demonstrate the ability of our framework for handling a general number of branches. We have added new results in Appendix C.3 to compare the performance of $K=2$ and $K=3$. \n\n| |  Sigmoid 4x100 | Sigmoid 4x500 | Sigmoid 6x100 | Sigmoid 6x200 |\n| --- | --- | --- | --- | --- | \n| $K=2$ | 54 | 20 | 62 | 49 |\n| $K=3$ | 53 | 21 | 61 | 49 |\n\n| | Sin 4x100 | Sin 4x200 | Sin 4x500|\n| --- | --- | --- | --- |\n| $K=2$ | 54 | 34 | 20 |\n| $K=3$ | 63 | 39 | 25 |\n\nOn Sigmoid networks, we find that $K=2$ and $K=3$ yield comparable results. However, on networks with the $\\sin$ activation which is more nonlinear compared to Sigmoid, using $K=3$ significantly outperforms $K=2$. The results demonstrate the effectiveness and potential of our framework supporting a general number of branches, on NNs involving functions that are relatively more nonlinear. So far, we have only used uniform branching points, for its simplicity, but it will be interesting for future works to study the possibility of optimizing branching points for stronger performance. \n\n## Problem complexity\n\n>The experimental results are also on mostly toy problems and with fairly small perturbation sizes (1/255 as opposed to 2/255 and 8/255 typically employed in the literature on CIFAR-10). \n\nVerifying networks with general nonlinearities is inherently more challenging than verifying networks with ReLU activation only. On non-ReLU networks, previous works on NN verification also used relatively small perturbations. For example, on CIFAR-10, Henriksen and Lomuscio, 2020 used $\\epsilon\\in\\{ 0.05/255,0.1/255,0.2/255,0.5/255,1/255 \\}$ (Table 2 in Henriksen and Lomuscio, 2020). On MNIST, PRIMA (M\u00fcller et al., 2022b) used $\\epsilon$ between 0.002 and 0.015 (Table 5 in M\u00fcller et al., 2022b), which is also much smaller than 0.3 or 0.4 often used in adversarial robustness literature on MNIST. Moreover, we not only have \"toy problems\", but also have models for the practical \u200bMachine Learning for AC Optimal Power Flow (ML4ACOPF) problem. \n\t\t\t\t\n## Filtering instances to verify\n\n>The authors sometimes select the properties to verify by excluding the properties that would be verified by CROWN (for instance, table 3).  Taking the above into account, I am not sure how significant some of the improvements with respect to pre-existing work are (for instance, \u03b1,\u03b2-CROWN without branching on Figure 2 and table 3).\n\nWe focus on solving hard instances (instances that cannot be solved by vanilla CROWN in Zhang et al., 2018) in the verification problem. Our empirical results reflect how many of the hard instances each method can solve and demonstrate the significant effectiveness of our framework on solving the hard instances (e.g., in Table 3, our BaB is able to solve 53% of the hard instances on the 4x100 Sigmoid network, while \u03b1,\u03b2-CROWN without branching is only able to solve 28% of the hard instances). Easy instances can be verified by most of the recent methods anyway, but the percentage of hard instances can vary for different settings, depending on the difficulty of the verification problem. Thus, we believe it makes more sense to only evaluate on the hard instances, to benchmark the effectiveness of the recent verification methods proposed after the vanilla CROWN.\n\n## Explanation on the branching strategy\n\n>I found the explanation of the branching strategy to be quite confusing. For instance, the presentation of BaBSR heavily differs from the one from the original authors, which is based on computing coefficients that estimate the impact of splitting on the last layer bounds from the (Wong and Kolter 2018) paper. Could the authors explain why the two presentations are equivalent?\n\nWhen the linear relaxation of the activation functions is the same, Wong and Kolter, 2018 and CROWN (Zhang et al., 2018) are equivalent (Salman et al., 2019), and they are just derived from two different views (primal view in Zhang et al., 2018 v.s. dual view in Wong and Kolter, 2018) with different linear relaxation in their implementation. Given this equivalence, the coefficients from Wong and Kolter, 2018 utilized by the original BaBSR formulation correspond to the coefficients in CROWN's linear bounds ($A$ in Eq. (1)) which we use in this paper. The reviewer may refer to Salman et al., 2019 for more explanation on the equivalence and unification of the convex relaxation-based NN verification methods.\n\nSalman, H., Yang, G., Zhang, H., Hsieh, C. J., & Zhang, P. (2019). A convex relaxation barrier to tight robustness verification of neural networks. Advances in Neural Information Processing Systems, 32."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558055350,
                "cdate": 1700558055350,
                "tmdate": 1700558135892,
                "mdate": 1700558135892,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hv2Z86gxNZ",
            "forum": "ivokwVKY4o",
            "replyto": "ivokwVKY4o",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6011/Reviewer_ygGn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6011/Reviewer_ygGn"
            ],
            "content": {
                "summary": {
                    "value": "The paper extends the \u03b1,\u03b2-Crown verification framework to support the\nverification of neural networks with general activation functions. In\nparticular it introduces a novel branching mechanism that allows for splitting\na neuron in more than two branches. It additionally presents a variant of the\nBaBSR branching heuristic for selecting the neuron to split at each step. The\nexperimental results reported show improvements over the state-of-the-art\nverifiers on some common and on some more complex benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Novel extension of the \u03b1,\u03b2-Crown framework to tackle general activation\nfunctions. Good experimental evaluation showing the efficacy of the resulting\nmethod."
                },
                "weaknesses": {
                    "value": "- Highly incremental to \u03b1,\u03b2-Crown - the resulting method is essentially\n  \u03b1,\u03b2-Crown with support for more than two branches per split.\n\n- The BBPS branching heuristic a trivial variant of BaBSR, it gives marginal\n  gains, and it is not compared with other preciser variants of BaBSR from  De\n  Palma et al., 2021."
                },
                "questions": {
                    "value": "Please see comments above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6011/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6011/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6011/Reviewer_ygGn"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6011/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698783700638,
            "cdate": 1698783700638,
            "tmdate": 1700780278269,
            "mdate": 1700780278269,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "79bFbkXxte",
                "forum": "ivokwVKY4o",
                "replyto": "hv2Z86gxNZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6011/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6011/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ygGn"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the review. We respond to the concerns on the comparison to \u03b1,\u03b2-CROWN and the existing branching heuristics below:\n\n## Comparison to \u03b1,\u03b2-CROWN\n\nWhile this work is based on \u03b1,\u03b2-CROWN, we make important contributions on supporting general nonlinearities, as it will enable researchers from various domains (such as power flow as we demonstrate in the paper) to verify their non-ReLU networks. We generalize the verification framework to support BaB for general nonlinearities, which is not only about supporting more than two branches, but also others including general and non-zero branching points. We also propose a new branching heuristic crucial for general nonlinearities. These are all new methodological contributions compared to \u03b1,\u03b2-CROWN.\n\n## BBPS\n\n>The BBPS branching heuristic a trivial variant of BaBSR, it gives marginal gains\n\nWe believe our new BBPS heuristic is not a \"trivial variant of BaBSR\". First, BaBSR was originally formulated for ReLU only, while BBPS is generally formulated for general nonlinearities, allowing for BaB on networks with general nonlinearities. We also propose to utilize previously saved linear bounds (Eq. (13) in the revised paper) to make more precise estimation, which is also new compared to BBPS which simply ignored the term we estimated. Empirically, our BBPS archives significant gains over BaBSR in most cases. For example., in Table 3, BBPS verifies 53 instances while BaBSR verifies 34 instances for the Sigmoid 4x100 network, out of 100 instances, which is a +55.9% relative gain. Thus, we think the gains from BBPS is not marginal.\n\n>it is not compared with other preciser variants of BaBSR from De Palma et al., 2021\n\nThe FSB heuristic in De Palma et al., 2021 has a filtering mechanism which runs a full bound propagation for shortlisted candidates. Filtering is independent from our contribution on BBPS for computing initial estimations, as filtering can be combined with generic branching heuristics. We experiment with \"BaBSR-like + filtering\" and \"BBPS+filtering\":\n\n| |  Sigmoid 4x100 | Sigmoid 4x500 | Sigmoid 6x100 | Sigmoid 6x200 | Tanh 4x100 | Tanh 6x100 |\n| --- | --- | --- | --- | --- | --- | --- |\n| BaBSR-like | 34 | 17 | 44 | 41 | 35 | 8 |\n| BaBSR-like + filtering | 33 | 17 | 44 | 41 | 34 | 8 |\n| BBPS | 53 | 21 | 61 | 49 | 41 | 9 |\n| BBPS + filtering | 50 | 21 | 56 | 48 | 41 | 9 |\n\nWe find that the filtering does not improve the performance on these models, possibly because their improved bound estimation is not sufficiently strong compared to the additional cost. Adding filtering sometimes hurts the performance more for BBPS compared to BaBSR-like, as BBPS tends to verify more hard instances, where the additional cost tends to cause timeout more easily when the runtime is originally long for the hard instances. We leave it for future work to study improving the filtering mechanism for NNs with general nonlinearities. We have revised the paper, and we have added the results and discussions in Appendix C.4."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700557840148,
                "cdate": 1700557840148,
                "tmdate": 1700557840148,
                "mdate": 1700557840148,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "c1qmlkMqnq",
            "forum": "ivokwVKY4o",
            "replyto": "ivokwVKY4o",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6011/Reviewer_Cfzz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6011/Reviewer_Cfzz"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a verification framework with BaB for neural networks that 1) encompasses general branching points and an arbitrary number of branches, which could generalize NN verification to variety of networks with various activation functions; 2) develops a novel branching heuristic named BBPS with a more accurate estimation; 3) enables verification on models for the ACOPF application."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. BBPS constantly outperforming existing SOTA neural network verification on several benchmark datasets.\n2. The author conducts experiments on different network architectures, it's interesting to see the study about effectiveness of neural network verification on modern architectures like ViT.\n3. The paper is well written and easy to follow."
                },
                "weaknesses": {
                    "value": "1. It seems that all the classifiers are trained on PGD, it would be better if authors could report a more comprehensive evaluation on other robust training algorithms.\n2. It would be better if the author could provide a comparison including quantitative results of average running time for clarity."
                },
                "questions": {
                    "value": "Please refer to the questions in Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6011/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698823258505,
            "cdate": 1698823258505,
            "tmdate": 1699636644543,
            "mdate": 1699636644543,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "W0t95hSuHg",
                "forum": "ivokwVKY4o",
                "replyto": "c1qmlkMqnq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6011/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6011/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Cfzz"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the review. We have revised the paper to address the weakness points mentioned by the reviewer. We also respond to the questions below:\n\n## Other robust training algorithms\n\nWe thank the reviewer for the suggestion on including models with robust training techniques other than PGD. We added an experiment on models trained by Small Adversarial Bounding Regions (SABR) (Mueller et al., 2022) which is a method for certified training. We trained 4 Sigmoid networks with SABR and evaluated the performance of verification. We show the results in the table below. Our BaB with the BBPS heuristic verifies 67 more instances in total, compared to the method without BaB, and it also verifies 17 more instances compared to BaB with the BaBSR-like heuristic. The results demonstrate that our BaB with the BBPS heuristic effectively verifies more hard instances. We have also added the results and more details in Appendix C.5. \n\n|  | Sigmoid 4x100 | Sigmoid 4x500 | Sigmoid 6x100 | Sigmoid 6x200 |\n| --- | --- | --- | --- | --- |\n| $\\alpha$ only w/o BaB | 59 | 51 | 60 | 65 |\n| BaB (BaBSR-like) | 73 | 65 | 71 | 76 |\n| Our BaB (BBPS) | 74 | 75 | 76 | 77 |\n\nMueller, M. N., Eckert, F., Fischer, M., & Vechev, M. (2022, September). Certified Training: Small Boxes are All You Need. In The Eleventh International Conference on Learning Representations.\n\n## Running time\n\nWe have added results on the average running time in Appendix C.6, for the setting in Figure 2. \n\nAverage running time computed on verified instances:\n| | Sigmoid Networks | Tanh Networks |\n| --- | --- | --- | \n| Vanilla CROWN | 0.91 | 1.34 | \n| DeepPoly | 0.66 | 0.46 |\n| PRIMA | 139.55 | 107.84 |\n| VeriNet | 0.68 | 1.81 |\n| \u03b1,\u03b2-CROWN (\u03b1 only w/o BaB) | 1.78 | 2.87 |\n| Our BaB (BBPS) | 5.48 | 4.47 |\n\nWe first compute the average running time only on instances verified by each method. The average running time of our BaB is slightly higher than most baselines and is much lower than PRIMA. This evaluation tends to bias towards methods that solve much fewer hard instances, as hard instances tend to require larger running time. \n\nAverage running time computed on all instances:\n| | Sigmoid Networks | Tanh Networks |\n| --- | --- | --- | \n| Vanilla CROWN | 135.50 | 214.39 | \n| DeepPoly | 189.24 | 205.15 |\n| PRIMA | 202.12 | 184.06 |\n| VeriNet | 98.45 | 223.46 |\n| \u03b1,\u03b2-Crown (\u03b1 only w/o BaB) | 96.22 | 106.87 |\n| Our BaB (BBPS) | 87.94 | 101.01 |\n\nTherefore, we also compute the average running time on all instances, where we use the timeout (300 seconds in our experiments) as the running time for the instances that are not verified. Under this evaluation, the average running time of our BaB is lower than all the other methods. However, this evaluation is still not perfect as it can be affected by the timeout value.\n\nCompared to the average running time, we think that the plots in Figure 2, which show the numbers of instances against various running time thresholds, can more comprehensively reflect the time cost to verify different numbers of instances by each method."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700557687578,
                "cdate": 1700557687578,
                "tmdate": 1700557687578,
                "mdate": 1700557687578,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]