[
    {
        "title": "Strategic Recommendations for Improved Outcomes in Congestion Games"
    },
    {
        "review": {
            "id": "eWnKekaIug",
            "forum": "YJxhZnGU1q",
            "replyto": "YJxhZnGU1q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8311/Reviewer_rUVY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8311/Reviewer_rUVY"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the problem of performing recommendations for agents who repeatedly play congestion games to improve the social welfare. More specifically, this paper considers a situation where every agent is a Q-learner with states and he/she switches the state by the recommendation (Learning Dynamic Manipulation Problem, LDMP, in this paper). First, this paper defines the manipulative potential of a recommender and proves that it increases with the number of states, i.e., the possible kind of recommendations. Next, this paper proposes an algorithm for recommendation and empirically shows the effectiveness of this recommender by well-known Braess's paradox instance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper investigates both theoretical and algorithmic aspects of LDMP. On the theoretical aspect, it proves that if more recommendation space is allowed, we can induce more combinations of actions that agents take. On the algorithmic aspect, it proposes a heuristic algorithm for recommenders that can improve the social welfare. Since this algorithm is not only intended to improve the social welfare, it can also be used for improving the other objective depending on the preference of recommender; which is of independent interest."
                },
                "weaknesses": {
                    "value": "The main weakness of this paper is that the proposed heuristics is empirically evaluated on only a toy example of Braess's paradox that have only 3 actions. Although I admitted that toy examples like Braess's paradox is useful for investigating the behavior of the proposed and existing frameworks, the effectiveness of the proposed approach should be verified for more general and large datasets.\nIt is known that there are many instances that phenomenon like Braess's paradox occurs [1][2]. Therefore, the applicability for more larger congestion game should be empirically examined.\n\n[1] Greg Valiant and Tim Roughgarden. Braess's paradox in large random graphs. In Proceedings of the 7th ACM conference on Electronic commerce, pp. 296--305, 2006. https://doi.org/10.1145/1134707.1134740\n\n[2] Fan Chung and Stephen J. Young. Braess's paradox in large sparse graphs. In proceedings of the 6th International Workshop on Internet and Network Economics, pp. 194--208, 2010. https://doi.org/10.1007/978-3-642-17572-5_16"
                },
                "questions": {
                    "value": "In many congestion games including Braess's paradox, it is natural that the action of every agent is given as a combination of shared resources. In such a case, explicitly maintaining all the actions is prohibitive since the number of actions is generally exponentially large with respect to the number of shared resources. I think for some sets of actions (like path-sets of selfish routing setting), it may be possible to alleviate this issue because some computational components fall into easy problem like shortest-path problem; this is true for the equilibrium computation of congestion games. Do you have any idea for improving the time complexity of the proposed algorithm under such a situation where the set of actions is paths?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8311/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8311/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8311/Reviewer_rUVY"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8311/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698753485140,
            "cdate": 1698753485140,
            "tmdate": 1699637033795,
            "mdate": 1699637033795,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XYXSJ8ErcT",
                "forum": "YJxhZnGU1q",
                "replyto": "eWnKekaIug",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8311/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8311/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your review. With this first rebuttal we wish to answer your immediate questions. We are working on a reviewed manuscript that addresses your points that we expect to have ready in the next days.\n\nWe wish to address the main weakness you point out in our paper which is that our heuristic model is only tested on the Braess Pardaox. We completely agree that this is not ideal, however we also believe that algorithms that work for other networks can be constructed to achieve similar results. What we believe our contribution to be is a demonstration of the **controllability** of the learning dynamics of q-learners which we them empirically verify in a congestion game. \n\nOur theorems were supposed to make this case, however we understand from the other reviewers that they need more work. We have found some relevant tools from the field of control theory that will help us make our claims stronger and mathematical, and will include these ASAP in our revised manuscript. We also wish to point out that our results contribute to a debate in game theory and congestion games on the effectiveness of recommendations. As mentioned in our paper, correlated equilibria which are the closest analogues of recommendations for rational players have been shown to be identical to nash equilibria in congestion games. This means that for rational players it is impossible to provide recommendations that would avoid an inefficient nash equilibrium. Our results therefore show that this is no longer the case for Q-learners, which is very interesting to consider given that Q-learning is a more realistic model of learning and decision making than the rational players assumed traditionally in game theory.\n\nWe will be providing one more example of another network in the appendix which you may review. In this example our Q-learners face the initial network of the Braess Paradox where the Nash equilibrium is the same as the socially optimal solution. In this case, we show that with a cleverly designed recommendation \"switch\" it is possible to coordinate the Q-learners to rapidly converge on the Nash equilibrium.\n\nFinally, we wish to address your questions about scaling up the algorithm to large networks. This is clearly an interesting pursuit, and a very challenging problem. We believe that biggest hurdle in our assumptions is that we assume that the recommender knows all the q-tables of agents. Relaxing this assumption is necessary before we can scale up, but we believe that it may be possible. For example, the recommender may not know what the q-values of agents are exactly but instead can only infer it from the behaviour of the agents. Over time the recommender should be able to learn the q-values of the user. An even further relaxation may be to assume no knowledge of the q-values. We believe that it may even be possible to create a recommendation scheme that is able to achieve desirable results with no knowledge, but it will clearly have to be very different to a completely random scheme which have shown to be worse than no recommendations. One thing that is clear is that there must be correlations higher than random between the recommendations that different agents receive. If this last proposal were possible, then it would not even matter what the actions/paths available to the users are. The recommender system would operate with complete lack of this knowledge, and basically \"bombard\" all the users with correlated state information that influences their learning."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699993443081,
                "cdate": 1699993443081,
                "tmdate": 1699993443081,
                "mdate": 1699993443081,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zUOjwFenIh",
                "forum": "YJxhZnGU1q",
                "replyto": "XYXSJ8ErcT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8311/Reviewer_rUVY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8311/Reviewer_rUVY"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for detailed reply. I also partially read the updated manuscript, although the theoretical details were not carefully checked. I am still interested in scaling up the proposed framework to larger networks (with possibly exponentially many ), but I understand this is truly a future work.\nI have also read the example of another network (you mean Appendix E.4?), but I think this example is still a toy example. I understand that these examples are indispensable to investigate the behavior of the proposed framework. However, I think a concrete examples of, for example, ten (or dozens) of actions, will be needed to verify the practicality of the proposed framework."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699973755,
                "cdate": 1700699973755,
                "tmdate": 1700699973755,
                "mdate": 1700699973755,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aOVHecHfjd",
            "forum": "YJxhZnGU1q",
            "replyto": "YJxhZnGU1q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8311/Reviewer_B8k7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8311/Reviewer_B8k7"
            ],
            "content": {
                "summary": {
                    "value": "The authors consider the atomic, unweighted, symmetric, linear cost congestion games and introduces a framework where an agent, playing the role of a recommender system (RS), learns to give recommendations to Q-learning agents (playing the role of players in the congestion games) in order to optimize social welfare. The authors then derive some results regarding the manipulative potential of the recommender. The authors then propose a heuristic algorithm of RS and show, by experimental means, that it can drive the system toward social optimum."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The questions posed by the paper is quite interesting.\n- The effort of the authors made in conducting a quite extensive experiment on the Braess 's paradox is applaudable. \n- The presentation of the proposed framework is quite clear."
                },
                "weaknesses": {
                    "value": "- In general, the writing is quite confusing. This includes a lack of robust mathematical presentations of results (Theorem 1 and 2) and several typos/grammar issues. \n- The practicality aspect of this work is also questionable. As pointed out by the authors, the omniscience requirement for RS is a major limitation. Moreover, it is quite far-fetched to impose the Q-learning framework on all players in a congestion network. \n- The main results (Theorem 1 and 2) do not give any guarantee on the convergence toward a social optimum (or at least, it lacks an explanation/justification). These theorems are not presented in a mathematically manner which hinders the readers to understand their contributions. As an heuristic scheme, the proposed algorithm is not supported by any performance analyses. It is quite disappointing as it can be seen from the experiments that as the number of states (recommendation options) increases, the proposed algorithm does drive the system closer to the social optimum.\n\n- Some possible typos/ unclear sentences:  \n   + p.6, first paragraph, Regarding Theorem 1?\n   + p.6, greater detail --> greater details\n   + p.6 \"....reflects the action that most need agents to be assigned to them\"  ?\n   + notation r is used both for welfare function W (page 2) and reward function (page 5)\n   + In Equation (1), should z_t be a subscript instead of being a superscript?\n   + In Section 2.1., there is a sentence saying that \" the goal of RS is to find P*(a)\" and another sentence saying that the goal of RS is to find the policy Pi^*_RS."
                },
                "questions": {
                    "value": "- How the optimistic estimate $\\bar{r}$ is computed in the heuristic algorithm ?\n- What d*_a means exactly in the paragraph after Equation (3) ?\n- Given a congestion game model, how an RS can design a recommendation set that guarantees Assumption 1? \n- What is the relation between maximizing the manipulative potential and converging toward a social optimum?\n- How precisely does the proposed algorithm confuses the Q-leaner (as remarked in Conclusion)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethical issues with this paper to signify."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8311/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8311/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8311/Reviewer_B8k7"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8311/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698784444788,
            "cdate": 1698784444788,
            "tmdate": 1699637033681,
            "mdate": 1699637033681,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Fok9erOIMs",
                "forum": "YJxhZnGU1q",
                "replyto": "aOVHecHfjd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8311/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8311/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Preliminary rebuttal"
                    },
                    "comment": {
                        "value": "Thank you very much for your review. With this first rebuttal we wish to answer your immediate questions. We are working on a reviewed manuscript that addresses your points that we expect to have ready in the next days.\n\nYou and the other reviewers have pointed out that the theorems are to be significantly improved, and that it is not clear how the RS works from reading section 2. This clearly is a big shortcoming which we must address. We will get to that as soon as possible with the revised manuscript.\n\n**weaknesses**\n\n1. We completely agree with you and the other reviewers on this point, and is a major part of our revision delivered to you ASAP.\n2. Omniscience is a restrictive assumption which absolutely should be relaxed for future work. In the appendix we address this to an extent by showing that when the epsilon greedy users are kept at a constant epsilon (rather than decaying it) the RS is able to achieve an improvement for small values of epsilon but fails for larger values. This can be interpreted as the failure of the proposed heuristic algorithm to account for the actual exploration rate of the users.\n3. Theorems 1 and 2 guarantee that given assumption 1 the system can be manipulated to be in any target state. That said, we completely agree with you and the other reviewrs that our formulation of the theorems is vague and must be reworked. As it stands we have found that there are two concepts of accessibility and controllability from control theory that should fit to phrase these theorems with mathematical rigor.\n4. Thank you for pointing out all these typos. We will be sure to include them in our revision ASAP.\n\n\n\n**questions**\n\n1. The optimistic estimate is computed by first categorizing all the agents according to the actions which could be induced by recommendations: the argmax actions of their q-tables. There will be some agents which have the same argmax for each recommendation, and others that have many different argmax values. We categorize them on the basis of their argmax values to determine the degree of flexibility that the system exhibits in that state. Then on this basis we create a new congestion game such that the agents that will only do one thing are treated as part of the congestion network. We then determine the socially best assignment of states and take the congestion in this scenario as the optimisic estimate.\n   We agree that this step is not well explained, so expect to include a pseudocode explanation in the appendix for the specific case of the braess paradox.\n2. $d^*_a$  is the number of agents that take action $a$ at the social optimum.\n3. Assumption 1 can not be guaranteed by designing the recommendation set, but is instead an assumption about the distribution of q-values, which the recommender system may not be in control of. We realized that assumption 1 must be reformulated and will do so ASAP in our review.\n4. Maximizing the manipulative potential guarantees that you can induce any action distribution, including the socially optimal action distribution. We have realized that this is very similar to definitions of controllability from control theory, so we will be reformulating this theorem in these terms.\n5. The proposed algorithm selects states for q-learners such that the actions selected by q-learners lead to desired outcomes. After each action the q-learners update their q-values for that recommendation. If there were no recommendations, the dynamics would settle on the behaviour shown in figure 2. We call this \"confusion\" without recommendations the Q-learners would act differently, and because the recommender is effectively disrupting the learning process of the Q-learners by switching around the recommendations."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699992207759,
                "cdate": 1699992207759,
                "tmdate": 1699992527733,
                "mdate": 1699992527733,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BnTLdmcsxa",
                "forum": "YJxhZnGU1q",
                "replyto": "Fok9erOIMs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8311/Reviewer_B8k7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8311/Reviewer_B8k7"
                ],
                "content": {
                    "title": {
                        "value": "Responses to authors' feedback"
                    },
                    "comment": {
                        "value": "Thanks for the responses. They address some, but not all, of my raised concerns. I would like to keep the original score. \n(note that I cannot find, and hence, do not read, the \"revised\" paper mentioned by the authors)"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685960067,
                "cdate": 1700685960067,
                "tmdate": 1700685960067,
                "mdate": 1700685960067,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "f3OyqQv2YL",
            "forum": "YJxhZnGU1q",
            "replyto": "YJxhZnGU1q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8311/Reviewer_fzBq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8311/Reviewer_fzBq"
            ],
            "content": {
                "summary": {
                    "value": "This work considers the problem of designing recommendation systems to induce Q-learning agents in traffic networks to achieve near socially optimal outcomes; this problem is termed the Learning Dynamic Manipulation Problem (LDMP). In this model, agents are assumed to use greedy Q-learning to choose routes, and a central recommendation system can provide recommendations to each agent so as to influence the learning towards specific outcomes that minimize total congestion. This work proposes a heuristic algorithm for recommendation systems and shows that on the Braess paradox network, the heuristic algorithm can steer Q-learners towards more socially optimal outcomes, and this effect increases with the size of recommendation space."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The problem of inducing learning agents towards particular solutions is a very interesting direction on both the theoretical and applied side that is worth further explanation."
                },
                "weaknesses": {
                    "value": "Certain parts of the modeling are not entirely clear and could benefit from more motivation or discussion (see below). The theoretical results (Theorem 1 and 2) appear somewhat unclear; they could benefit from more details or precision. On the experimental side, while the improvements for the improved recommendation system look quite interesting, they appear to be localized to a single network structure and it is not clear how general these results are."
                },
                "questions": {
                    "value": "Recommendation: In full disclosure, my own interests are more on the theoretical side. Nonetheless, I think that this is a very interesting problem, but my current feeling is that this paper would greatly benefit from a clearer discussion of the modeling assumptions and choices. Since this is primarily an empirical paper, testing the heuristic algorithms on more general networks would also provide much better evidence for the utility of this approach. As a result, my initial belief would be to reject; that said, I could be convinced by the authors or the other reviewers to raise my score.\n\nComments/Questions\n\n---Was $\\varepsilon$-greedy Q-learning ever defined? My understanding was that this means that each agent uniformly explores with probability $\\varepsilon$; if so, how does this connect with Assumption 1?\n\n---What is the intuition behind the objective function of the RS in equation (2) in terms of KL divergence? Wouldn't a more natural objective be to minimize the weighted congestion of the actions of the Q-learners?\n\n---Regarding Assumption 1: should there be a $t$ dependence somewhere here? If so, is that meant to hold for all $t$?\n\n---It might be useful to provide pseudocode for the subroutines in Algorithm 1, at least in the Appendix.\n\n---I had significant difficulty parsing the statement and content of Theorems 1 and 2. In Theorem 1, what does it mean that \"the manipulative potential for every timestep $t$ is maximized\"? The proof itself seems to use ``infinity'' in somewhat unclear ways that made it hard to understand. I also had issues determining a precise way to formalize the statement of Theorem 2. For both results, it might be very useful to give a more formal statement and proof to make clear what is meant.\n\n---What is the (empirical or theoretical) computational cost of the heuristic algorithm, especially on generalizations to larger network topologies?\n\n---My impression is that the empirical performance of the heuristic algorithm is quite interesting. But as mentioned above, my main concern is that these experiments seem to have only been done in the Braess paradox network and not on more general topologies."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8311/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8311/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8311/Reviewer_fzBq"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8311/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698807541571,
            "cdate": 1698807541571,
            "tmdate": 1699637033550,
            "mdate": 1699637033550,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QJoF6txtdb",
                "forum": "YJxhZnGU1q",
                "replyto": "f3OyqQv2YL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8311/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8311/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Preliminary Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you very much for your review. With this first rebuttal we wish to answer your immediate questions. We are working on a reviewed manuscript that addresses your points that we expect to have ready in the next days. We are also interested in the theoretical side and hope that we can convince you that our results are an important initial step in understanding what may be provable.\n\nYou and the other reviewers have pointed out that the theorems are to be significantly improved, and that it is not clear how the RS works from reading section 2. This clearly is a big shortcoming which we must address. We will get to that as soon as possible with the revised manuscript.\n\nTo answer your direct **questions**:\n\n1. epsilon greedy means, as suggested, that agents uniformly explore with probability epsilon. Your comment makes clear that we should define that explicitly. This is not connected with assumption 1 which is an assumption on the initial distribution of q-values. We realize however that our assumption needs to be rephrased to: assume that for all agents and for all actions, the q-values are drawn from a distribution such that each argmax has a non-zero probability of occurring. We will formalize this and get back to you ASAP.\n\n2. If we understand correctly, you suggest that rather than looking at the KL divergence between action distributions and a target we minimize the congestion function directly. This point is well taken, and we believe that it will improve our presentation of the reward. In fact, what we care about is not the action distribution with a domain over $A^n$ but rather the action distribution with a domain over $A$: what is the proportion of players that selects each action? Thank you very much for pointing this out, we will modify it in the paper.\n\n3. Assumption 1 should not have a t dependence. We agree that we must improve this theoretical section. We will do so and get back to you with well revised material ASAP for your review.\n\n4. We fully agree that it would be useful to add pseudocode for the subroutines in the appendix, and see that it is not easy or intuitive to understand how the heuristic algorithm works from the way it is presented. We are adding these to the appendix.\n\n5. We are going to fully re-work the theorems to be mathematical and get back to you ASAP.\n\n6. The computational cost seems to be polynomial in the various sizes: the number of agents, the number of actions and the number of possible recommendations. We will try to give this full characterization to make it clearer.\n\n7. Thank you for this point. We will do our best to provide some intuition in the revision of how the heuristic algorithm works. In the paper we provide a generalized presentation of the algorithm. Our results use a specific instantiation of if. Hopefully when we get back with our theorems it will be clear that it should be possible to design algorithms like our heuristic one for different kinds of networks. We have another example of a network which we will include which is the initial network of the Braess Paradox. In this network the Nash Equilibrium is the socially Optimal Solution, and we show that we can rapidly increase the speed of convergence to the equilibrium by cleverly picking some recommendations. We will include this in the revision ASAP."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699990848746,
                "cdate": 1699990848746,
                "tmdate": 1700161764714,
                "mdate": 1700161764714,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EWuyg5ZL2l",
            "forum": "YJxhZnGU1q",
            "replyto": "YJxhZnGU1q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8311/Reviewer_rQFP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8311/Reviewer_rQFP"
            ],
            "content": {
                "summary": {
                    "value": "The authors proposed a novel model of recommendation in which a set of Q-learners receive recommendations as their states and play a congestion game repeatedly. An heuristic algorithm was proposed to drive the actions of Q-learners toward a target action distribution for optimizing social welfare. Experiments show that the proposed algorithm can drive the system to convergence at the social optimum of a specific congestion game."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed problem setting is timely and interesting and the high-level idea of recommending states to influence agents' beliefs sounds reasonable."
                },
                "weaknesses": {
                    "value": "The writing of section 2 needs to be significantly improved. I'm not able to understand how the RS works from reading section 2.1. For example, what is the meaning of the space Z? And intuitively why we should believe the heuristic algorithm can achieve what we want? The description of Algorithm 1 is directly followed by the experiment results, and useful discussions are missing. Theorem 1 and 2 are not stated in rigorous ways. Could you quantify what does it mean by 'slower' and 'smaller'?\n\nThe assumption that the RS has access to the q-values of the Q-learners might be overly strong. In reality, it is unreasonable to believe that the platform has full understanding about each agent's belief state. It would be more interesting to relax this assumption.\n\nThe experiment is only performed on Braess Paradox instances, which seems limited and less convincing to me. It would be nice to demonstrate a comprehensive result on various congestion game instances."
                },
                "questions": {
                    "value": "1. For the motivation, could you justify why it is reasonable to study the Q-learning agents in routing games, given that it is known that any better-response dynamics converges to the NE of congestion games. \n\n2. the description in section 1.2 is confusing to me: it is said that the congestion game can be framed as a stateless MDP, but the formulation of Q-learners involves the state S. What is the state S here? And is a player allowed to observe other players' actions or rewards? It is beneficial to add a detailed description of players' Q-learning policy.\n\n3. could you give an intuitive explanation why Q-learning agents can escape from the low-welfare NE of the Braess Paradox game even without the help of recommendations? I wish I could get some takeaway insight in this section but unfortunately the discussion is too little. \n\n4. In terms of the better social outcome result, it is known that the for smooth games (including the class of congestion games), the PoA under NE is essentially the same as the PoA under CE/CCE [1]. So I suppose in general, the social welfare improvement from steering the NE to CE in congestion games can be very little. Do you agree with this argument? \n\n5. there are some related works, e.g., [2,3] regarding mechanism designs for content recommendation platform, which also involves dynamical optimization over a congestion game structure. It would be nice to add some discussion in the intro or related work. \n\n\n[1] Roughgarden, T. (2015). Intrinsic robustness of the price of anarchy. Journal of the ACM (JACM), 62(5), 1-42.\n\n[2] Yao, F. et al. (2023). Rethinking Incentives in Recommender Systems: Are Monotone Rewards Always Beneficial?. Advances in Neural Information Processing Systems, 37.\n\n[3] Ben-Porat, O., & Tennenholtz, M. (2018). A game-theoretic approach to recommendation systems with strategic content providers. Advances in Neural Information Processing Systems, 31."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8311/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8311/Reviewer_rQFP",
                        "ICLR.cc/2024/Conference/Submission8311/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8311/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698892521202,
            "cdate": 1698892521202,
            "tmdate": 1700711305725,
            "mdate": 1700711305725,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1ej2OQmeFa",
                "forum": "YJxhZnGU1q",
                "replyto": "EWuyg5ZL2l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8311/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8311/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Preliminary Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you very much for your review. With this first rebuttal we wish to answer your immediate questions. We are working on a reviewed manuscript that addresses your points that we expect to have ready in the next days. \n\nYou and the other reviewers have pointed out that the theorems are to be significantly improved, and that it is not clear how the RS works from reading section 2. This clearly is a big shortcoming which we must address. We will get to that as soon as possible with the revised manuscript.\n\nTo answer your direct **questions**:\n\n1. Better-response dynamics converge to Nash, yes, but these dynamics are non-constructive. What do we mean. We mean that they do not provide the best response, they just assume that the best-response is played and thus Nash equilibrium is an almost trivial equilibrium point of these dynamics. Q-learning on the other hand is a constructive algorithm which can provably converge in MDPs. Its widespread application justifies its study in this model. Furthermore, as is shown in section1, Q-learning does not converge to the Nash equilibrium, which further justifies why we can not rely on best response analysis to know what to expect from practical RL algorithms when deployed IRL.\n2. Section 1.2 was introducing repeated q learning in congestion games without recommendations, and stateless. Thank you for your c0mment, we agree that it is confusing to present it with the state space S and propose that we remove it in the revised version. Section 2 introduces the recommendations, so the q learners aquire a state as a recommendation.\n3. The intuition is that the Nash equilibrium is unstable, since any multi-player deviation from the Nash will result in better outcomes for the deviating players. Q learners are able to implicitly coordinate, as is shown in other literature on prisoners dilemma, pricing games and the Braess Paradox. These are very interesting papers which show that Q-learning in some cases can lead to implicit coordination.\n   1. Calvano, Emilio, et al. \"Artificial intelligence, algorithmic pricing, and collusion.\" *American Economic Review* 110.10 (2020): 3267-3297.\n   2. Dolgopolov, Arthur. \"Reinforcement Learning in a Prisoner's Dilemma.\" *Available at SSRN 4240842* (2022).\n   3. Carissimo, Cesare. \"The Social Benefit of Exploration in Braess\u2019 S Paradox.\" *Available at SSRN 4348118* (2023).\n4. This is a very interesting point. Indeed the PoA under CE and CCE are the same. This is because there are no CE/CCE which are different from Nash Equilibrium, also one of the results from Koessler et. al 2023 we cite in our paper. Thank you for the additional reference for such result. What this means for rational players is that there are no recommendations that can reduce the Price of Anarchy. This is quite the strong result. Our paper, shows that there are recommendations that can reduce the Price of Anarchy when the players are Q-learners. Given that Q-learning is a more realistic and constructive algorithm, we try to conclude that understanding whether or not recommendations can influence the outcome IRL can not be determined from rational player assumptions. Indeed the rational player assumptions would say that recommendations can not help avoid inefficient Nash equilibria.\n5. Thank you for the added references that we were not aware of. We will look at them closer and see how they may be added to the discussion. [2] very interesting, as they think about designing rewards such that content creators are effectively playing a potential game. [3] takes again the perspective of content providers (or creators) which may strategically create content that will be recommended by a mediator (recommender system)\n\nThank you for all comments. These are our introductory responses. In the next days we will be working on including your feedback to full extent in the paper itself. Please expect further improvements in the coming days. If there are any additional comments we are very interested to hear them."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699991334674,
                "cdate": 1699991334674,
                "tmdate": 1700058405687,
                "mdate": 1700058405687,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XzKiAhxq5o",
                "forum": "YJxhZnGU1q",
                "replyto": "EWuyg5ZL2l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8311/Reviewer_rQFP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8311/Reviewer_rQFP"
                ],
                "content": {
                    "title": {
                        "value": "Responses to authors' feedback"
                    },
                    "comment": {
                        "value": "Thanks for the response. I appreciate the effort and the intuitive explanation makes sense to me. However, I do not completely buy your justification regarding the best-response dynamics, as what I was asking is better-response dynamics, which is a much broader class of dynamics compared to best-response. And also I do not completely agree with response 4, as the argument \"This is because there are no CE/CCE which are different from Nash Equilibrium\" does not make sense to me: in congestion games there is no evidence that CE/CCE coincide with NE. In addition, I do not believe the experiment section is comprehensive enough to support the theoretical claim. I would like to raise my score to 5 as I see the potential of this idea, however, I believe the overall quality of the current version is not good enough for acceptance."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713317752,
                "cdate": 1700713317752,
                "tmdate": 1700713382854,
                "mdate": 1700713382854,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]