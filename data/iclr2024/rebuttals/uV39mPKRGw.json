[
    {
        "title": "Concept Matching: Clustering-based Federated Continual Learning"
    },
    {
        "review": {
            "id": "0XuIoIGOZ1",
            "forum": "uV39mPKRGw",
            "replyto": "uV39mPKRGw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3925/Reviewer_anyU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3925/Reviewer_anyU"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on federated continual learning (FCL) and proposes Concept Matching (CM), a clustering-based FCL framework. CM learns different global models for different concepts and designs server/client matching to identify target concept models for local optimization and global aggregation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "S1. The idea of learning a model set for clients to select in FL setting is novel.\n\nS2. The introduction of two main challenges in FCL is clear."
                },
                "weaknesses": {
                    "value": "W1. There are some more related works [1, 2] about clustered federated learning not included in this paper.\n\nW2. The authors cite several related works about FCL but compare only one of them in experiments.\n\nW3. This paper claims that CM can handle both class-incremental and task-incremental FCL but the experiments only include class-incremental setting.\n\n[1] Felix Sattler, Klaus-Robert M\u00fcller, and Wojciech Samek. 2021. Clustered Federated Learning: Model-Agnostic Distributed Multitask Optimization under Privacy Constraints. IEEE Trans. Neural Netw. Learn. Syst. 32, 8 (2021), 3710\u201337\n\n[2] Yu Zhang, Moming Duan, Duo Liu, Li Li, Ao Ren, Xianzhang Chen, Yujuan Tan, and Chengliang Wang. 2021. CSAFL: A Clustered Semi-Asynchronous Federated Learning Framework. In IJCNN. IEEE, Shenzhen, China, 1\u201310."
                },
                "questions": {
                    "value": "Q1. Are there experimental results on the datasets of FedWeIT under its setting?\n\nQ2. How to select the correct concept model during inference/test as there are no data labels on the testset for Client Concept Matching?\n\nQ3. Can we regard it as privacy risk to update and upload only selected concept models after local training?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3925/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3925/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3925/Reviewer_anyU"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3925/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698457727952,
            "cdate": 1698457727952,
            "tmdate": 1699636353071,
            "mdate": 1699636353071,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aqnJ43nC6B",
                "forum": "uV39mPKRGw",
                "replyto": "0XuIoIGOZ1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3925/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3925/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for appreciating our work on **the novel idea of learning a model set for clients to select in FL, the introduction of the two main challenges in FCL**. We address your comments as follows.\n\n**W1**. There are some more related works [1, 2] about clustered federated learning not included in this paper.\n\n**Response**:\nWe added them in the updated version. Let us note that these two works are related only in the sense that they apply clustering in federated learning. They do not tackle any FCL challenges as our proposed approach. \n\n\n**W2**. The authors cite several related works about FCL but compare only one of them in experiments.\n\n**Response**:\nIn addition to vanilla FL, our evaluation adapts two baselines. One baseline (FedWeIT) is the most cited FCL work published recently in a top machine learning conference. The other baseline is to apply a classic CL method (EWC) in FL. We believe they are representative of the state-of-the art. \n\n\n**W3**. This paper claims that CM can handle both class-incremental and task-incremental FCL but the experiments only include class-incremental settings.\n\n**Response**: \nIn 5.1 experimental setup of the original submission, we mention that in task-incremental scenarios, the difference is that the clients do not have to perform concept matching every round, because they know the task IDs and the concept drift due to the transition of different tasks. For the same experimental setting, task-incremental training would achieve the same model performance with lower computation overhead at the clients.\n\n**Q1**. Are there experimental results on the datasets of FedWeIT under its setting?\n\n**Response**: \nWe evaluate the proposed approach on the same data settings of FedWeIT. However, as we mention in 5.2 results  of the original submission, the learning settings of FedWeIT are on a very small scale and unrealistic (i.e. 5 clients per round, 5 classes to train per client, and non-overlapping classes over clients). In addition, the server in FedWeIT knows the class subset labels from the clients. Since our goal is to evaluate under realistic assumptions and on a larger scale, these learning settings are different from the original FedWeIT paper.\n\n**Q2**. How to select the correct concept model during inference/test as there are no data labels on the testset for Client Concept Matching?\n\n**Response**: \nIn the updated paper, we added a clarification in 4.3 design discussion. For task-incremental training, during inference, the clients can pick the corresponding global concept model to perform prediction with their input data. For the class-incremental scenario, at inference, the clients can apply an ensemble model method to produce an output from all the global concept models. \n\n\n**Q3**. Can we regard it as privacy risk to update and upload only selected concept models after local training?\n\n**Response**: \nIn the original submission, we explain in 4.3 design discussion that in terms of privacy, the CM framework is the same as vanilla FL (i.e., the clients send only their model weights to the server). In the updated paper, we further added the explanation that an adversarial server might seek to extract information from the client model using the same techniques from vanilla FL, which is beyond the scope of this paper."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700352404253,
                "cdate": 1700352404253,
                "tmdate": 1700352404253,
                "mdate": 1700352404253,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "htQekd9Q0N",
                "forum": "uV39mPKRGw",
                "replyto": "aqnJ43nC6B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3925/Reviewer_anyU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3925/Reviewer_anyU"
                ],
                "content": {
                    "comment": {
                        "value": "Response to W2:\nFedWeIT adopts a dynamic model expansion scheme similar to CM in this paper. EWC adopts the regularization-based scheme. Could you add the replay-based method FedCIL as a comparison?\n\nResponse to W3:\nI agree with the author that task-incremental can be considered a slightly simpler task than class-incremental because of the task-ID signal. However, since there is no experimental verification, we cannot conclude that CM can still maintain superiority over the comparison methods in the task-incremental setting.\n\nResponse to Q2 (task-incremental):\nSince the experimental evaluation only relates to the class-incremental setting, can we conclude that the method described in Section 4.3 about the task-incremental setting is not supported by the experiments? Since concept model-ID and task-ID are bound by mapping in the task-incremental scenario, it seems unnecessary to perform concept matching during training. So there is no need to discuss the problem of task-incremental in this paper.\n\nResponse to Q2 (class-incremental):\nFor the class-incremental setting, concept models are selected with concept matching for training. However, the ensemble scheme used for inference is not discussed in detail in the updated version. If it's just a naive ensemble mechanism that doesn't match the training phase, will the unrelated models interfere with the prediction?\n\nResponse to Q3: I agree that the CM framework proposed in this paper employs a similar model uploading and aggregation scheme to vanilla FL. However, due to the maintenance of multiple global models, each client chooses different concept models to train and upload, which may lead to higher risks than vanilla FL, though it is not the main focus of this paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700536127483,
                "cdate": 1700536127483,
                "tmdate": 1700536127483,
                "mdate": 1700536127483,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RSLzXmP3FQ",
            "forum": "uV39mPKRGw",
            "replyto": "uV39mPKRGw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3925/Reviewer_Ustz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3925/Reviewer_Ustz"
            ],
            "content": {
                "summary": {
                    "value": "The authors investigate a continual learning setting in a federated learning framework. They propose concept matching as a method to avoid the catastrophic forgetting problem in continual learning under the assumptions of a federated framework, i.e., that only model weights can be shared between clients and server. The concept matching algorithm:\n-starts by sending a set of K concept-specific models to each client\n-the client evaluates each model and fine-tunes the locally best-performing model\n-the locally fine-tuned models are sent back to the server\n-the server clusters the models into J clusters within which models are aggregated \n-the server matches the J aggregated models with the initial K models to only update the matched ones\n-the next round commences"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The continual learning setting is relevant for many real-world applications. Investigating the interaction with restrictions from federated frameworks is valuable. \n\nThe authors provide some limited theoretical intuition for their algorithm and empirical evidence that it works."
                },
                "weaknesses": {
                    "value": "The algorithm requires each client to evaluate each model at every round. How does this impact run time? \n\nThe experiments are based on a single \"super\" dataset which is of a relatively modest input scale. \n\nThe experiments do not seem to have been run multiple times with different random seeds."
                },
                "questions": {
                    "value": "What happens if some concept models never get updated? \n\nIt would be helpful to have explicit definitions of concept and concept model in the introduction."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3925/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3925/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3925/Reviewer_Ustz"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3925/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698787763360,
            "cdate": 1698787763360,
            "tmdate": 1699636352999,
            "mdate": 1699636352999,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "75GtlMpzEM",
                "forum": "uV39mPKRGw",
                "replyto": "RSLzXmP3FQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3925/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3925/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for appreciating our work on **investigating CL in federated framework for real-world applications,  demonstrating the effectiveness of proposed approach through theoretical intuition and empirical evidence**. We address your comments as follows.\n\n\n**Weaknesses**:\n\n**1**. The algorithm requires each client to evaluate each model at every round. How does this impact run time?\n\n**Response**: We evaluate the client run time on a real IoT device in the original submission.  Due to the page limit, the details are in Appendix C.4, and the results are presented in Appendix C.6.  The total client end-to-end operation time in one round is 97.81 seconds, which is feasible for a real-world deployment. The average time for a client to evaluate models is 10% over the client training time,  and we believe the improvement in performance achieved by CM is worth this overhead cost. \n\n**2**. The experiments are based on a single \"super\" dataset which is of a relatively modest input scale.\n\n**Response**: Using the \u201csuper\u201d dataset follows the most cited FCL work [1] published recently in a top machine learning conference. The dataset covers 183 image classes with 292250 images. We believe this scale is sufficient to evaluate the proposed approach. \n[1] Federated continual learning with weighted inter-client transfer, International Conference on Machine Learning, 12073--12086, 2021.\n\n**3**.The experiments do not seem to have been run multiple times with different random seeds.\n\n**Response**: Due to page limit, we mention in the appendix C.3 of the original submission that we test CM with different hyper-parameters, and only present the results with the hyper-parameters that lead to the best results. The hyper-parameters here include different random seeds. We moved this statement to the main paper in the updated version. \n\n**Questions**:\n\n**1**. What happens if some concept models never get updated?\n\n**Response**: Due to the effectiveness of the proposed approach, concept matching achieves up to 100% accuracy, and we do not observe the situation that some concept models never get updated. A possible situation that some concept models never get updated is when we intentionally start the system with more concept models than the number of concepts. Figure 6 in Appendix of the original submission illustrates this situation, the testing accuracy on each concept data is simply not affected by the extra models in the system.  \n\n**2**. It would be helpful to have explicit definitions of concept and concept model in the introduction.\n\n**Response**:  In the updated submission, we moved the explicit definitions of concept and concept model from section 2 overview to section 1 introduction."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700347961721,
                "cdate": 1700347961721,
                "tmdate": 1700347961721,
                "mdate": 1700347961721,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "I3poBJXGjY",
            "forum": "uV39mPKRGw",
            "replyto": "uV39mPKRGw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3925/Reviewer_HPc6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3925/Reviewer_HPc6"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a Concept Matching (CM) method for Federated Continual Learning (FCL). The main innovation of this article lies in maintaining global concept models for matching on the server . At training step, each client selectively updates the weights of its corresponding model. These updated weights are then clustered on the server side to differentiate between different tasks and reassign them to their respective matches. From a technical perspective, the approach presented in this paper is straightforward. Experimental results indicate that the proposed method outperforms the benchmark."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. Federated Continual Learning is a genuine real-world problem that exists in practical scenarios.\n2. The experimental results clearly demonstrate the effectiveness of the proposed method within the experimental setting provided by the authors.\n3. The algorithm framework proposed in the paper is presented in a concise and easily understandable manner."
                },
                "weaknesses": {
                    "value": "1.The problem background is unreasonable and does not align with practical needs. The requirement for each client to train with different datasets each time is not realistic in real-world scenarios.\n2.The method lacks innovation and can be seen as a mere patchwork of existing client aggregation approaches.\n3.The experimental comparisons lack persuasiveness as the baselines should be diverse in their configurations."
                },
                "questions": {
                    "value": "1.In the paper, there is a lack of explanation regarding the calculation of distlist[k] in line 9 of Algorithm 1. The author should provide further clarification on this point to enhance the understanding of the algorithm.\n\n2.The model's implicit assumption that the number of concepts is smaller than the number of global concept models introduces a limitation. When there is a large number of concepts, it becomes evident that the algorithm's maintained models may encounter difficulties in effectively handling this situation. This limitation is a result of the algorithm's design.\n\n3.The overall design of the model lacks novelty, as there are many similar methods available. The author simply transfers existing methods from other domains to the problem of federated continual learning without introducing any substantial innovation.\n\n4.The experiments should be more comprehensive and diversified. It is recommended to include baselines that cover scenarios where all the data is combined or where all the data is separated. This will ensure a fair comparison and prevent experiments from being solely designed to favor the proposed method."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3925/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3925/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3925/Reviewer_HPc6"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3925/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699066158595,
            "cdate": 1699066158595,
            "tmdate": 1699636352927,
            "mdate": 1699636352927,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ufTlyMrVLe",
                "forum": "uV39mPKRGw",
                "replyto": "I3poBJXGjY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3925/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3925/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for appreciating our work on **addressing a genuine real-world problem, demonstrating the effectiveness of the proposed method with the experiment, and presenting the framework in a concise and easily understandable manner**. We address your comments as follows.\n\n**W1**.The problem background is unreasonable and does not align with practical needs. The requirement for each client to train with different datasets each time is not realistic in real-world scenarios. \n\n**Response**: In the introduction and overview, we state the motivation background for our problem as FCL for IoT and mobile devices, where it is difficult to train with the entire dataset on-device at every round due to resource constraints, and the data not only accumulate over time, but also change their distributions.  We provide human activity recognition (HAR) as an example, and its distribution change can be due to the subsets of activities, the locations of the activities, or the health status of the user. \nIt seems the reviewer only focused on the scenario in Figure 1 where each client trains with different image classes each time. Nevertheless, we added another example of such a real-world scenario in the updated paper. For the photos a client takes in daily life, the concepts can be different types of the place of interest visited (e.g., museum vs. national park), or different types of meals (i.e. breakfast, lunch, dinner). \n\n\n**W2**.The method lacks innovation and can be seen as a mere patchwork of existing client aggregation approaches. \n\n**Q3**. The overall design of the model lacks novelty, as there are many similar methods available. The author simply transfers existing methods from other domains to the problem of federated continual learning without introducing any substantial innovation.\n\n**Response**: The novelty was further clarified in the updated paper. There are two main novelties in the design. The first novelty is the clustering-based framework to alleviate catastrophic forgetting and interference among clients, and consequently achieve good model performance in FCL. The second novelty is our effective and theoretically grounded concept matching algorithms. Because the server does not know which cluster model is responsible for which concept, clustering alone will not solve FCL. \n\n\n**W3**.The experimental comparisons lack persuasiveness as the baselines should be diverse in their configurations.\n\n**Q4**.The experiments should be more comprehensive and diversified. It is recommended to include baselines that cover scenarios where all the data is combined or where all the data is separated. This will ensure a fair comparison and prevent experiments from being solely designed to favor the proposed method.\n\n**Response**:  \n- In the updated submission, we added the comparison results between vanilla FL and CM where all the data is separated (i.e., treating each dataset as a concept without mixing or splitting the datasets). Similar to the orignal settings, Figure 3 in Appendix C.5 illustrates the smooth learning curves for CM over vanilla FL for all concepts.Compared with vanilla FL, the weighted average accuracy over the number of samples for all the concepts is improved from 85.5% to 88.0% as well.\n- We do not test the scenario with  all the data combined, because it is not a CL setting. \n- In the original submission, we explain in Appendix C.1 that we split FaceScrub into two concepts to verify whether CM can differentiate them successfully. We mix the three MNIST datasets together to make it more difficult, because MNIST datasets are easy to learn. The purpose is to stress-test CM rather than favoring the proposed method.\n\n**Q1**.In the paper, there is a lack of explanation regarding the calculation of distlist[k] in line 9 of Algorithm 1. The author should provide further clarification on this point to enhance the understanding of the algorithm.\n\n**Response**: We fixed the typo, by replacing distlist[k] in line 9 as distRecord[k] (defined in line 3). \n\n**Q2**.The model's implicit assumption that the number of concepts is smaller than the number of global concept models introduces a limitation. When there is a large number of concepts, it becomes evident that the algorithm's maintained models may encounter difficulties in effectively handling this situation. This limitation is a result of the algorithm's design.\n\n**Response**: The paper does not assume the number of concepts is smaller than the number of global concept models. On the contrary, our evaluation shows CM has good resilience when configured with numbers of concept models that are different (either smaller or larger) from the number of concepts. The results are in the Appendix C.6 due to the page limit.  Even when the number of concepts is larger than the number of global concept models,  the average model accuracy achieves 89.5% and 88.9% respectively, and beats vanilla FL (86.7%)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700441469323,
                "cdate": 1700441469323,
                "tmdate": 1700441469323,
                "mdate": 1700441469323,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HCWPLlK7b5",
            "forum": "uV39mPKRGw",
            "replyto": "uV39mPKRGw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3925/Reviewer_2nMo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3925/Reviewer_2nMo"
            ],
            "content": {
                "summary": {
                    "value": "In order to overcome the two issues of catastrophic forgetting and the potential interference among clients in Federated Continual Learning (FCL), the authors proposed a clustering-based framework, called Concept Matching. At each time step, the CM framework first assigns a concept model for each client as initialization in the fine-tuning process. Then the client models trained with local data, are clustered into some groups, which are used to update those concept models via the designed server concept matching approach in Algorithm 1. Finally, the updated concept models are for the next time step."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed concept matching framework has used different clustering, aggregation, and concept matching algorithms, which can effectively improve performance compared with the state-of-the-art systems in Federated Continual Learning."
                },
                "weaknesses": {
                    "value": "1. In terms of equations, many equations are not written clearly and normatively. Specifically, Eq.(2) is wrong where k* has missed n in Eq.(2), and the argmin operation of the loss function is not presented. Aggregate function in Eq.(4) is not expressed. \\Theta and \\omega are not unified in \\Theta^t={\\omega_1,\\omega_2,...,\\omega_J}.\n2. In terms of the writing, this submission has not been written well. What is the meaning of the concept in the image experiment of this paper? This abstract vocabulary has not been explained clearly. More reasons for experiments should be analyzed. Many equations are not clear. In Algorithm 1, the \\omega_j and \\omega_k are repetitive in the inner and outer loop. \n3. In terms of experiments, the experiments are also insufficient. It is very important to discuss the number of concept models and clustered groups, as well as their relationship from the perspective of theory or experiment, which can help readers understand the importance of clustering for FCL. The authors are suggested to provide more analysis and reasons about all experiments, such as performance differences based on various distance functions, clustering methods, etc. \n4. The authors claim that two issues in Federated Continual Learning (FCL), catastrophic forgetting and interference among clients, can be greatly diminished. It will be better to prove that claim through conducting some experiments. \n5. The novelty is limited, as there are some Federated Learning (FL) works adopting the clustering method. however, the authors apply the clustering method to FCL at a single time step, which does not reflect the unique design of CL."
                },
                "questions": {
                    "value": "Shown in the above Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3925/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3925/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3925/Reviewer_2nMo"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3925/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699335629501,
            "cdate": 1699335629501,
            "tmdate": 1699636352864,
            "mdate": 1699636352864,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IMlSOXwYEA",
                "forum": "uV39mPKRGw",
                "replyto": "HCWPLlK7b5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3925/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3925/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for appreciating our work on **the effective performance improvement with different clustering and aggregation algorithms**. We address your comments as follows.\n\n**Weaknesses**\n\n**1**.In terms of equations,many equations are not written clearly and normatively.Specifically,Eq.(2) is wrong where k* has missed n in Eq.(2),and the argmin operation of the loss function is not presented.Aggregate function in Eq.(4) is not expressed.\\Theta and \\omega are not unified in \\Theta^t={\\omega_1,\\omega_2,...,\\omega_J}.\n\n**Response**\n- Eq.(2) is a client operation, k* is a client local value unrevealed to the server. Thus, n is omitted in the original submission.  Due to the confusion, we added n in the updated paper. \n- We do not find any equation with loss function in the original submission where the argmin operation is absent. They are all presented when expressing the optimization of the loss function, including eq(1) and eq(6). It will be helpful if the reviewer could indicate the specific equation in the paper where the argmin operation of the loss function is not presented. \n- From the abstract and throughout the paper, we state the CM framework is flexible to use any aggregation function. Thus, we did not specifically express any aggregation function. \n-  \\Theta (aggregated cluster model set) is used to differentiate the global model set \\Omega, even though its elements are also models. Thus, we used a different symbol. \n\n\n**2**. In terms of the writing,this submission has not been written well.What is the meaning of the concept in the image experiment of this paper?This abstract vocabulary has not been explained clearly.More reasons for experiments should be analyzed. Many equations are not clear.In Algorithm 1,the \\omega_j and \\omega_k are repetitive in the inner and outer loop.\n\n**Response**\n- In the original submission, we define the image concepts as different sets of classes from the first time we mention image in section2 overview.\n- We updated the abstract. We would greatly appreciate it if the reviewer could provide more specific details regarding the aspects in which the abstract is unclear.\n- The first paragraph in evaluation of the original submission specifies the main goals of the experiments as well as minor goals for Appendix. Our experiments are designed to target these goals. \n- \\omega_j and \\omega_k are elements in different sets. We fixed the minor issue due to the confusion. \n\n**3**. In terms of experiments,the experiments are also insufficient.It is very important to discuss the number of concept models and clustered groups,as well as their relationship from the perspective of theory or experiment,which can help readers understand the importance of clustering for FCL.The authors are suggested to provide more analysis and reasons about all experiments,such as performance differences based on various distance functions,clustering methods,etc.\n\n**Response** \n\nAll these experiments the reviewer considered insufficient are already part of the original submission. \n- The number of concept models, clustered groups and their relationship are evaluated and discussed as **Performance of clustering algorithms** and **Resilience to number of concepts different from the ground truth** in the paper.\n- We illustrate CM performance based on various distance functions, clustering methods in Table 2 and Table 3. The difference is negligible in terms of model accuracy, and we conclude in the paper that CM provides the flexibility to use different clustering algorithms and distance metrics, as it performs well with all of them. As neural networks grow in size and complexity, the curse of dimensionality may manifest itself and requires advanced clustering algorithms and distance metrics.\n\n\n**4**.The authors claim that two issues in FCL,catastrophic forgetting and interference among clients,can be greatly diminished.It will be better to prove that claim through conducting some experiments.\n\n**Response**\n\nFigure 2 in the original submission illustrates the negative effect of catastrophic forgetting and interference among clients on the learning curve and the model performance in vanilla FL, and CM can effectively smoothen the learning curve and improve the model performance. We will investigate how to further quantify catastrophic forgetting and interference among clients in the next version. \n\n\n**5**.The novelty is limited,as there are some FL works adopting the clustering method.however,the authors apply the clustering method to FCL at a single time step,which does not reflect the unique design of CL.\n\n**Response**\n\nThe reviewer misunderstood the proposed approach. Throughout the paper, we indicate clustering is applied every FCL training round. Clustering alone will not solve FCL, because the server does not know which cluster model is responsible for which concept. Our additional novelty lies in our effective and theoretically grounded concept matching algorithms."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700360215597,
                "cdate": 1700360215597,
                "tmdate": 1700360215597,
                "mdate": 1700360215597,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]