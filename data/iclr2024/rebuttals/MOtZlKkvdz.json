[
    {
        "title": "Are Large Language Models Post Hoc Explainers?"
    },
    {
        "review": {
            "id": "jAUAdb2MBS",
            "forum": "MOtZlKkvdz",
            "replyto": "MOtZlKkvdz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4341/Reviewer_nbwF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4341/Reviewer_nbwF"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to explain the black-box models' output by in-context learning on large language models (LLMs). To achieve this, the authors transformed the input into sentences and proposed four prompting strategies to generate different instructions for LLMs, using LLMs to extract top-k important features to explain the black-box model. The authors compare their faithfulness of explanation with other baselines, showing competitive results."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper is well-written and easy to understand.\n2. The proposed method is easy to reproduce.\n3. The authors provide enough experimental data to support their claim."
                },
                "weaknesses": {
                    "value": "The soundness of this paper is poor. The authors treat LLMs as a principal component analysis model, use them to fit the data distribution and find the top-k most important features with different prompts as the explanations for black-box models. This is not a guaranteed process because it is unclear how LLMs fit the data distribution inside the prompt, not to mention how LLMs \"understand\" the data distribution and further provide a faithful explanation from the perspective of data. In fact, the \"logical thinking skill\"[1], the ability to process math problems[1], and the instruction-following ability of LLMs[2] are poor or remain unclear; even the order of the input will affect the output of a LLM[3].\n\nTo maximize the power of LLM, a better way is to post-hoc explain the model's output from the perspective of \"natural language\", like [4], which is easy to understand and easy to evaluate. Using language as output, the faithfulness of explanation can be easily evaluated by human annotators intuitively. Another way is to let LLMs use tools (e.g., use Python to code) to enhance the extra ability of LLMs and further obtain a guaranteed faithful explanation for a black-box model.\n\n[1] Song et al. NLPBench: Evaluating Large Language Models on Solving NLP Problems. Arxiv 2023.\n\n[2] Zeng et al. Evaluating Large Language Models at Evaluating Instruction Following. Arxiv 2023.\n\n[3] Pouya et al. Large language models sensitivity to the order of options in multiple-choice questions. Arxiv 2023.\n\n[4] Menon et al. Visual Classification via Description from Large Language Models. ICLR 2023"
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4341/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4341/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4341/Reviewer_nbwF"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4341/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697469609463,
            "cdate": 1697469609463,
            "tmdate": 1699636404361,
            "mdate": 1699636404361,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "u7Dl8hKRKT",
                "forum": "MOtZlKkvdz",
                "replyto": "jAUAdb2MBS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4341/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4341/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal for Reviewer nbwF"
                    },
                    "comment": {
                        "value": "We thank the reviewer for acknowledging the reproducibility and experiments of our work. We greatly appreciate your feedback on solidifying the contributions of our work and address your concerns below.\n\n**Motivation for using LLMs as a post hoc explainer**\n\nWe understand your concern about the use of LLMs as a post hoc explainer. Our choice to use Large Language Models (LLMs) for this task was **driven by our desire to explore their potential beyond conventional language processing applications.** The **novelty** of our approach lies in its ability **to integrate contextual understanding and in-context learning (ICL) capabilities of LLMs** to provide richer, more nuanced explanations than what might be achievable through simpler methods. LLMs are useful for the following reasons:\n\n\nExisting methods often **lack the ability to offer detailed, human-like explanations.** LLMs can bridge this gap by generating explanations that are more understandable to humans, which is particularly valuable in fields where interpretability is crucial, such as healthcare or finance.\n\n\nGenerating explanations using LLMs is not just about identifying important features but also about understanding the rationale behind these selections in a manner that simpler methods may not provide. \n\n\nThe ability of LLMs to process and explain complex patterns in data can be particularly beneficial when dealing with intricate, high-dimensional datasets where traditional feature selection methods might struggle. \n\n\nWe would like to note that the potential benefits in terms of the depth and quality of explanations justify the exploration of our study, where we intended to be a stepping stone in the direction of using LLMs as possible post hoc explainers.\n\n\n\n**To maximize the power of LLM, a better way is to post-hoc explain the model's output from the perspective of \"natural language\" \u2026\u2026 Another way is to let LLMs use tools (e.g., use Python to code) to enhance the extra ability of LLMs and further obtain a guaranteed faithful explanation for a black-box model.**\n\nThank you for your valuable feedback on our work. We appreciate your insights on using the LLMs in other explainability frameworks and would like to note that there are  inherent challenges in using Large Language Models (LLMs) as explainers. However, we would like to clarify that **our work aims to explore the potential of LLMs in a new XAI domain** \u2014 introducing the first framework to study the effectiveness of LLMs in explaining other predictive models trained on tabular datasets. Regarding the application of LLMs to numerical datasets, we believe this represents an innovative step in understanding the capabilities of LLMs beyond their traditional scope. While LLMs are primarily designed for language tasks, **their ability to abstract and generalize can potentially be leveraged in a variety of contexts, including numerical data interpretation.** Our paper extends the TabLLM work [1] that studies the application of LLMs to zero-shot and few-shot classification of tabular data.\n\n**In fact, the \u201clogical thinking skill\u201d, the ability to process math problems[1], and the instruction-following ability of LLMs are poor or remain unclear; even the order of the input will affect the output of a LLM**\n\nWe agree with the reviewer that the current set of LLMs suffers from a range of problems, such as processing math problems, lack of instruction-following ability, and inability to process longer inputs. However, we would like to clarify that our exploration of LLMs does not expect them to solve extensive math problems to generate explanations. For instance, we specifically instruct the LLM to explain its reasoning (**see the \u201cInstructions\u201d section of the prompt templates on pages 4 and 5**) before reaching a decision on which features are most important to the model, and the LLMs follow the rule explicitly stated in the instruction (see Section 3.3). Moreover, recent works like Lightman et al. [2] show that LLMs show enhanced performance on mathematical reasoning tasks, which will improve with the progress of LLM research. Again, our **goal is to explore the potential of LLMs as a post hoc explainer**, and that like other explanation methods, is not without its limitations.\n\nWe are very grateful to the reviewer for all their questions/concerns, as they have helped us improve our paper significantly. We tried to address all the reviewer suggestions and hope the reviewer considers increasing their score.\n\n\n**References**\n\n[1] Hegselmann, S., Buendia, A., Lang, H., Agrawal, M., Jiang, X., & Sontag, D. Tabllm: Few-shot classification of tabular data with large language models. In AISTATS, 2023.\n\n[2] Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., ... & Cobbe, K. Let's Verify Step by Step. arXiv, 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4341/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700353381997,
                "cdate": 1700353381997,
                "tmdate": 1700353381997,
                "mdate": 1700353381997,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9swOLA3Jfp",
            "forum": "MOtZlKkvdz",
            "replyto": "MOtZlKkvdz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4341/Reviewer_e81j"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4341/Reviewer_e81j"
            ],
            "content": {
                "summary": {
                    "value": "The authors investigate the potential of using LLMs as explainers of the external models\u2019 behavior. To this end, the authors explore 4 strategies to quantify feature importance. Authors compare existing feature attribution algorithms to LLM-based explainers, and demonstrate comparable performance to existing algorithms."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The idea of using LLMs as general-purpose explainers is an interesting one that could be practical and useful if it works well. This paper does a good job at taking an initial stab to demonstrate the potential of such approaches.\n\n2. Authors show that LLMs can employ existing feature attribution paradigms, such as perturbation-based feature attributions to replace existing algorithms such as LIME or Shap. If the LLMs can perform better than or for cheaper than the existing algorithms, these approaches could be effective in practice."
                },
                "weaknesses": {
                    "value": "1. (Pareto Curve) Even if the LLMs are decisively better/worse than the existing explainability methods, the ultimate decision of the practitioner would also be based on how cheap/expensive it is to obtain the explanations. I would be interested to see an approximate cost-benefit tradeoff to have a better sense of whether the LLM-in-the-loop explainers are preferable in practice. The cost here could be with respect to $ compute and with respect to time. \n\n2. Authors focus on relatively simple tabular datasets, which I personally think limits the impact of the methodology and results. While I understand that it\u2019s not yet possible to focus on many different domains (e.g. vision seems not possible yet); I believe it would have been possible to use existing text classifiers, as perturbations could still be communicated to existing models via prompts. This would increase the practicality and the value of the evaluation, in my view.\n\n3. (Lack of sufficient experimental details) I find that the presentation of the experiments could be significantly improved. As it stands, there are a lot of missing details in the experiments section or the Appendix, which makes me feel less confident about the reliability of the results. I detail several points below. I believe most of these points could easily be addressed in the rebuttal phase, and I will be happy to revise my assessment during the rebuttal.\n\n- 3.1 Importantly, I believe the authors should define their evaluation metrics; e.g., for metrics like PGI, PGU, RA, FA the authors refer to earlier papers without explicitly defining what they are. The reader should ideally be able to see the metrics without navigating to different papers.\n\n- 3.2 I cannot find the hyperparameters of the explainers or the rationale for picking them. In particular, it\u2019s unclear if the performance may or may not be explained by a poor choice of hyperparameters, as even the rationale of the hyperparameter choices for existing algorithms (LIME, SHAP etc.) are not presented in the paper. Since most of the results are meaningful in a relative sense (compared to the baselines), this is an important point to clarify.\n\n- 3.3 Similarly, for reproducibility purposes, the details around the models used should be better provided. E.g. it\u2019s unclear what optimizer is used to train the models, with which learning rate, whether early stopping is applied, and so on. This would surely raise reproducibility issues for follow-up work unless addressed.\n\n- 3.4 The authors describe the process of parsing the response as `We first save each LLM query\u2019s reply to a text file and use a script to extract the features.` I believe further details are needed to better understand this process. Specifically, what is the existing parsing strategy? Are the responses always parseable? What fraction of the time they are not parseable? \n\n- 3.5 The authors present ` LLMs accurately identify the most important feature` as a significant result (e.g. abstract `identify the most important feature with 72.19% accuracy,`), however for this specific task I do not see baselines. Why do the authors have baselines for faithfulness, but not for this specific task (apologies if I\u2019m missing this and the result exists)? Specifically \u2013 how good are existing algorithms at identifying the most important feature?\n\n- 3.6 There are claims I find unjustified. For instance, `The second approach significantly aids the LLM in discerning the most important features` \u2013 how can we claim this without any results in Page 6 under implementation details? If there is an experimental finding that supports this, please refer to the result. \n\n4. I\u2019m slightly confused about the insights we can draw from the experiments. Specifically, the authors propose 3 different explanation strategies that seem to perform reasonably similarly. I understand the overall message that LLMs have the potential to be used as explainers. However, the confusing part to me is there are 3 algorithms presented, and it\u2019s hard to understand which one is better or when. I\u2019d appreciate it if the authors could provide a concise discussion around this."
                },
                "questions": {
                    "value": "1. How do the authors pick the hyperparameters for the baseline explainers?\n\n2. Could the authors please explicitly define the metric they are using?\n\n3. Could the authors please clarify the claim on Page 6 `The second approach significantly aids the LLM in discerning the most important features`?\n\n4. Do the authors have any insights about the costs of the explanation methods, to inform practitioners about whether it is worth using LLMs in practice?\n\n5. How well do the baselines perform in the most important feature identification tasks?\n\n6. Is it possible to verify the effectiveness of these methods in text classification tasks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4341/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4341/Reviewer_e81j",
                        "ICLR.cc/2024/Conference/Submission4341/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4341/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698005590963,
            "cdate": 1698005590963,
            "tmdate": 1700589238165,
            "mdate": 1700589238165,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "D3xOgLFDaJ",
                "forum": "MOtZlKkvdz",
                "replyto": "9swOLA3Jfp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4341/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4341/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal for Reviewer e81j (Part 1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for acknowledging the motivation and presentation of our work. We appreciate your helpful feedback and address all the mentioned questions/concerns below.\n\n\n**Compute and runtime for generating explanations using LLMs**\n\nGreat point! We would like to clarify that, since submission, the **cost of GPT-4 has already reduced 10-fold** and inference times have gone down too. In response to the reviewers\u2019 concern, we provide a detailed table below that compares the runtime for generating explanations using LLMs and existing post hoc explanation methods.\n\n| **Method** |   LR   |   LR  |   LR  |   LR   |   ANN  |   ANN  |   ANN  |   ANN  | **Mean runtime (in secs)** |\n|:----------:|:------:|:-----:|:-----:|:------:|:------:|:------:|:------:|:------:|:--------------------------:|\n|            | COMPAS | Blood | Adult | Credit | COMPAS | Blood  | Adult  | Credit |                            |\n| Grad        |  0.183 | 0.001 | 0.002 |  0.001 |  0.003 |  0.001 |  0.002 |  0.002 |            0.024           |\n| SG         |  0.174 | 0.121 | 0.124 |  0.123 |  0.134 |  0.127 |  0.131 |  0.128 |            0.133           |\n| IG         |  0.044 | 0.043 | 0.043 |  0.043 |  0.047 |  0.045 |  0.047 |  0.046 |            0.045           |\n| ITG        |  0.001 | 0.001 | 0.001 |  0.001 |  0.001 |  0.001 |  0.002 |  0.001 |            0.001           |\n| SHAP       |  8.93  | 9.064 | 9.151 |  8.996 |  11.21 | 11.143 | 11.165 | 11.077 |           10.092           |\n| LIME       |  2.922 | 1.482 | 0.407 |  0.398 |  3.051 |  1.574 |  0.476 |  0.456 |            1.346           |\n| LLM       | 1732 | 1668 | 1418 | 1624 | 1578 | 1313 | 1349 | 1723 | 1550 |\n\n\nFrom the above table, we show that the time taken by LLMs to generate explanations is greater than the other explanation methods. However, we would like to highlight that the runtime for generating explanations using LLMs by query OpenAI APIs depends on a range of factors, including time of the day, server/requests overload, rate limit imposed by OpenAI, etc. However, the above runtime is expected to go down as the query time for OpenAI APIs improves.\n\n\n**Lack of experimental details**\n\nWe apologize for the lack of clarity and provide additional experimental details below.\n*PGI/PGU and FA/RA metrics*\n\nWe have added a detailed definition of the four metrics in Section 6.1 of our revised manuscript.\n\n\n*Hyperparameters for explanation methods*\n\nWe followed OpenXAI and used the standard hyperparameters for these explanation methods. In response to the reviewers\u2019 feedback, we detail them below for reference and have added them to the revised manuscript.\n\n**LIME**\n\nkernel_width           = 0.75\n\nstd_LIME               = 0.1\n\nmode                   = 'tabular'\n\nsample_around_instance = True\n\nn_samples_LIME         = 1000 or 16\n\ndiscretize_continuous  = False\n\n**grad**\n\nabsolute_value = True\n\n**Smooth grad**\n\nn_samples_SG = 100\n\nstd_SG       = 0.005\n\n**Integrated gradients**\n\nmethod             = 'gausslegendre'\n\nmultiply_by_inputs = False\n\nn_steps            = 50\n\n**SHAP**\n\nn_samples = 500\n\n*Reproducibility - what learning rate used to train models etc*\n\nAll our models are trained in PyTorch using cross entropy and a class weighted term to encourage underrepresented classes to contribute more to the loss than the most popular class.\n\nOptimizer: Adam\n\nLR: 0.001\n\nEpochs: 100\n\nData normalization: minmax\n\nBatch size: 64\n\nWe have shared all these details in the Jupyter notebook shared in the supplementary material (Notebooks/TrainModels.ipynb)\n\n*What is our parsing strategy and how many bad replies?*\n\nThe GPT-4 and GPT-3.5 responses are parseable an average of **96.4%** and **85%** of the time, respectively. We improve parsing ability by encouraging the LLM\u2019s replies to be nicely formatted. This is done by requesting the ranked features to be on the last line in descending order of importance with no other information present. The exact parsing details can be found in our Supplementary Materials code folder **llms > response.py**. \n\nIn general, we use regex and string manipulation to identify the last line of the reply. We then extract key parts of the last line like **\u201cis\u201d**, **\u201c:\u201d**, and **\u201c=\u201d** which indicate that the ranked features will begin on the right side of the matched string. An example reply is as follows: \n\n*\u201cTherefore, the top five most important features, ranked from most important to least important, are: G, A, F, H, I.*\n\n*Answer: G, A, F, H, I\u201d*\n\nThis will be parsed by splitting the last line by **:** into two parts, namely \u201cAnswer\u201d and \u201cG, A, F, H, I\u201d. The latter is split by a comma and stripped of any white space to be finally placed into an array for further processing and faithfulness evaluation.\n\n**Top k = 1 for Post Hoc Explainers**\n\nIn response to the reviewers\u2019 feedback, we conducted a new experiment and added Figure 13 to show the top-1 performance of existing XAI methods."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4341/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700335696637,
                "cdate": 1700335696637,
                "tmdate": 1700345970804,
                "mdate": 1700345970804,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HuKVD7hjqj",
                "forum": "MOtZlKkvdz",
                "replyto": "9swOLA3Jfp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4341/Reviewer_e81j"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4341/Reviewer_e81j"
                ],
                "content": {
                    "title": {
                        "value": "Response to Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your rebuttal and clarifications.\n\n> **Compute and Runtime**\n\nThank you for this clarification in terms of the runtime, this is informative. Do we have a cost comparison too? Please correct me if I'm wrong, but the numbers suggest (to me) that it is i) Slower (albeit expected to get faster) ii) A lot more expensive (due to the API request costs). I think it would be good to acknowledge these limitations and be upfront in the paper (this does not diminish its value, but rather increases it with better transparency).\n\nAnother question: Why are these details not in the paper?\n\n> **Experimental Details**\n\nThank you for these clarifications. I'm not familiar with how OpenXAI picked these specific values -- is there a reason why the authors did not perform a hyperparameter sweep? From the compute time table above, it should not be too costly to quickly run a hyperparameter study, in my opinion.\n\nOverall, I'm still unclear why the authors do not share these experimental details in the main text / at least the appendix. Is there a specific reason? As a reviewer/reader of the paper, I would personally appreciate having these details at least in the appendix (and not in a notebook in the zip file). I don't think these important experimental details are to be found by readers in the lines of a jupyter notebook, neither the rebuttal page is the only place to make these clarifications.\n\n> **Experimental insights**\n\n- Thank you for your clarifications here. I do not see a Figure 13 in the paper, is it perhaps not updated?\n- I similarly do not see the discussion in the main text. Could you refer me to concretely where it is?\n\nI will revisit my evaluation after these clarifications."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4341/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700357185076,
                "cdate": 1700357185076,
                "tmdate": 1700357194871,
                "mdate": 1700357194871,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AmTpSBw3vF",
                "forum": "MOtZlKkvdz",
                "replyto": "Y0HIvjARid",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4341/Reviewer_e81j"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4341/Reviewer_e81j"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thank you for the additional clarifications.\n\n> Choice of hyperparameters\n\nFirst, I did not review or at any point use OpenXAI and am not familiar with the rationale for picking 1 fixed set of hyperparameters, but I do not think we should evaluate the current paper based on choices from other papers. However, from practical experience, I know that the results could significantly change based on the choice of hyperparameters of explainers, across different use cases and datasets. e.g. LIME does k-lasso, and in higher dimensional cases one needs to have the k larger; how can we pick a fixed k across all experiments? \n\n> Top-k = 1 experiments (Tables 4 and 5)\n\nThanks for pointing me to these. Am I interpreting the results correctly that there are methods better than LLMs? If yes, the message in the abstract `we observe that LLMs identify the most important feature with 72.19% accuracy, opening up new frontiers in explainable artificial intelligence (XAI) to explore LLM-based explanation frameworks` feels a bit misleading to me as I'm not sure why this particular result opens up new frontiers.\n\nOverall, I thank the authors for their rebuttal. While I still have some of the concerns outlined above, the authors also resolved some other concerns, such as reporting the compute/time/performance tradeoffs and expanding on the experimental details. I will be revising my score considering the above discussion."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4341/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700589223728,
                "cdate": 1700589223728,
                "tmdate": 1700589223728,
                "mdate": 1700589223728,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XE6clNUYcj",
                "forum": "MOtZlKkvdz",
                "replyto": "9swOLA3Jfp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4341/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4341/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer e81j"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for their engagement and for reconsidering their initial score. These discussions have been helpful for us whilst revising our paper.\n\n**Choice of hyperparameters**\n\nThis is a fair point regarding the rigidity we have assumed of the post hoc explainer hyperparameters. While we do believe it best to adhere to OpenXAI for means of comparison and benchmarking, this is an interesting discussion to have. It might well be the case that for even higher dimensional datasets than those used, picking one set of hyperparameters for LIME would not yield optimal results across all settings. We would like to make some quick follow-up points to address this in the context of our work:\n\n- We elected to use LIME with 1000 perturbations to mitigate any inconsistency or sub-optimality in its results.\n- More importantly, we also observe in the logistic regression setting that LIME achieves near-perfect FA/RA scores across datasets, as well as, in the neural network setting, performance on par with the other state-of-the-art methods that do not require hyperparameter tuning, i.e. gradients.\n- Lastly, the set-up we use evaluates explanations based on the *order* of top-k features (according to absolute value), and does not penalize based on *exact* feature importance values. We echo the reviewer's points that hyperparameter choices will affect the exact feature importance values returned by methods such as LIME. However, the overall *ranking* of the features would be affected to a lesser degree (evidenced by the previous point also) when averaged across the full dataset.\n\nWe thank the reviewer again for making this point, as it is an important consideration for follow-up work where explanation metrics could be based on values rather than rankings.\n\n**Top-k = 1 experiments (Tables 4 and 5)**\n\nThank you for reporting the misleading statement. We found that GPT-4 exhibits non-trivial performance for most important feature identification, and performs on par with state-of-the-art methods in certain settings like the Credit dataset (Tables 4 and 5). Comparing GPT-3.5 and GPT-4 (Figure 6) additionally demonstrates that LLMs are closing the performance gap to post hoc explainers. We deemed this a new frontier as high performance is achieved through prompting alone, a novel approach in our field.\n\nIn light of this, we have updated the sentence in the abstract to be clearer, reading: `we observe that LLMs identify the most important feature with 72.19% accuracy, indicating promising avenues for further research into LLM-based explanation frameworks within explainable artificial intelligence (XAI).`\n\nWe hope these points can help to address the reviewer's remaining concerns, and are happy to provide further clarifications if desired."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4341/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700598881836,
                "cdate": 1700598881836,
                "tmdate": 1700598961348,
                "mdate": 1700598961348,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zMdhN7aZYF",
            "forum": "MOtZlKkvdz",
            "replyto": "MOtZlKkvdz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4341/Reviewer_WeJ2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4341/Reviewer_WeJ2"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes using language models to provide post-hoc explanations for other model decisions in four ways."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "This paper addresses an important problem."
                },
                "weaknesses": {
                    "value": "The critical weakness, in my view, is that the interpretability method itself is uninterpretable. When the task involves understanding language, this can be somewhat understood, like Bills et al.'s 2023 \"Language models can explain neurons in language models.\" But this paper applies language models to ask them directly, \"What are the most important features in determining the model's prediction?\" And it does so on purely numerical datasets. It oversimplifies the task of interpretability, arbitrarily modeling logistic regression coefficients as fixed ground truths and using prediction gap -- but there are a million dramatically cheaper ways that they could have selected the most important features according to the same criteria.\n\nAlso, a notable limitation (if the more fundamental questions did not overshadow it) is that the models interpreted are all quite simple, the datasets are themselves simple, and more standard models for tabular data are not considered (but again, this is not the paper's primary limitation). Lastly, it seems presumptive to suggest that this approach is better than SHAP, at least without a deeper investigation into where this method outperforms it (and some discussion on why it ostensibly performs almost as poorly as randomly selecting features). These points are less important to me, but they are still worth raising."
                },
                "questions": {
                    "value": "1) What do you mean when you say the logistic regression model has \"one layer of size 16\"?\n2) Can you elaborate on the motivation for this work - why did you feel that a language model would be an appropriate tool here?\n3) When would you use this approach instead of LIME, which consistently performed better?\n4) Can you give some examples where SHAP performed worst according to your metrics? What were the values produced? What were the correct metrics?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4341/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698819329642,
            "cdate": 1698819329642,
            "tmdate": 1699636404180,
            "mdate": 1699636404180,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2MUJ9OKrw1",
                "forum": "MOtZlKkvdz",
                "replyto": "zMdhN7aZYF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4341/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4341/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal for Reviewer WeJ2 (Part 1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your insightful questions. We are glad that you recognize the importance of this problem and we appreciate your feedback as they have helped us improve our paper significantly.  Below, we address all your questions/concerns.\n\n**Our method is uninterpretable**\n\nWhile we agree with the reviewer that the inner workings of LLMs are inherently complex and (currently) uninterpretable by nature, it is not the primary focus of our work and is an important point worth exploring in future work. Further, we would like to clarify that the explanations generated by the LLM using our prompting strategy are not *uninterpretable* or *fundamentally flawed*. For example, we specifically instruct the LLM to explain its reasoning (**see the \u201cInstructions\u201d section of the prompt templates on pages 4 and 5**) before reaching a decision on which features are most important to the model. \n\nWe would like to note that identifying key features using LLMs is an interesting future direction as our current exploration shows that they achieve non-trivial performance when compared to some state-of-the-art explanation methods. Further, LLMs provide natural language explanations and a level of textual reasoning that is more accessible/interpretable to end users when explaining a model\u2019s feature space. For instance, LIME performs a specific job that does not adapt to any specific patterns it observes in the perturbations, whereas LLMs are not limited in this respect, and have the capacity to adapt their explanation based on the patterns they see in the local neighborhood, which offers an explanation for why GPT-4 is more sample efficient than LIME with 16 input perturbations.\n\nWe thank the reviewer for raising these points as we had not clearly articulated them in the original manuscript.\n\n**Comparison to Bills et al. 2023**\n\nThank you for your valuable feedback on our paper. We appreciate your insights and the opportunity to address the concerns you raised regarding the interpretability of our proposed method.\n\nFirstly, we acknowledge your point about the interpretability challenge inherent in using Large Language Models (LLMs) as explainers. The complexity and opaqueness of these models do indeed raise valid concerns about the 'black-box' nature of the explanations they generate. However, **our work aims to explore the potential of LLMs in a new domain**, specifically as tools for post hoc explanation in XAI (Explainable Artificial Intelligence). Regarding the application of LLMs to numerical datasets, we believe this represents an innovative step in understanding the capabilities of LLMs beyond their traditional scope. While LLMs are primarily designed for language tasks, **their ability to abstract and generalize can potentially be leveraged in a variety of contexts, including numerical data interpretation.** \n\nWith respect to the oversimplification of experimental setup in our approach to interpreting logistic regression coefficients. It was a deliberate design choice for initial experiments as we aimed to establish a baseline understanding of LLMs before moving on to more complex scenarios.\n\n**Motivation for using LLM instead of other cheaper alternatives**\n\nThank you for your insightful critique regarding the cost-effectiveness of our approach in selecting the most important features. We understand your concern about there being more straightforward and less resource-intensive methods available for feature selection. Our choice to use Large Language Models (LLMs) for this task was **driven by our desire to explore their potential beyond conventional language processing applications.** The **novelty** of our approach lies in its ability **to integrate contextual understanding and in-context learning (ICL) capabilities of LLMs** to provide richer, more nuanced explanations than what might be achievable through simpler methods. \n\nWhile we agree that there are numerous simpler and more cost-effective ways to identify important features, LLMs are useful for the following reasons:\n\n1. Existing methods often **lack the ability to offer detailed, human-like explanations.** LLMs can bridge this gap by generating explanations that are more understandable to humans, which is particularly valuable in fields where interpretability is crucial, such as healthcare or finance. \n\n2. Generating explanations using LLMs is not just about identifying important features but also about understanding the rationale behind these selections in a manner that simpler methods may not provide. \n\n3. The ability of LLMs to process and explain complex patterns in data can be particularly beneficial when dealing with intricate, high-dimensional datasets where traditional feature selection methods might struggle."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4341/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700335332721,
                "cdate": 1700335332721,
                "tmdate": 1700335332721,
                "mdate": 1700335332721,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YKE4VoWrL5",
                "forum": "MOtZlKkvdz",
                "replyto": "zMdhN7aZYF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4341/Reviewer_WeJ2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4341/Reviewer_WeJ2"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response. I'd like to follow up on a few points.\n\n> Further, we would like to clarify that the explanations generated by the LLM using our prompting strategy are not uninterpretable or fundamentally flawed. For example, we specifically instruct the LLM to explain its reasoning (see the \u201cInstructions\u201d section of the prompt templates on pages 4 and 5) before reaching a decision on which features are most important to the model.\n\nThis response seems to conflate interpretability and explainability -- I stand by my original point that LLM outputs are not interpretable. To the claim that they are explainable because the model outputs its reasoning, I again do not believe this is true because of the challenge of faithfulness. Namely, you do not know that the generated reasoning is reflective of the models' internal processing. Notably, faithfulness can be used in the context of reasoning and explanations, e.g. [1,2], and in the context of interpretability, e.g. [3], with slightly different meanings. Notably, just because your language model's generated explanations are faithful with respect to the explained model, that does *not* mean that they are faithful with respect to the language model. More concretely, if you had solved the faithfulness challenge in language models, that would be more than worthy of a paper of its own.\n\n> Our choice to use Large Language Models (LLMs) for this task was driven by our desire to explore their potential beyond conventional language processing applications... The ability of LLMs to process and explain complex patterns in data can be particularly beneficial when dealing with intricate, high-dimensional datasets where traditional feature selection methods might struggle.\n\nOne question that still has not been answered is, why? What about language models makes them particularly well suited to this task? No results in this paper suggest that language models are better at dealing with high-dimensional data than other machine-learning approaches. You described my comment as \"Motivation for using LLM instead of other cheaper alternatives,\" but I think \"cheaper\" misplaces my main concern: we are talking about a type of model that is fundamentally a black box and is well-known to hallucinate - there's no reason to trust that its explanations are correct, faithful, or generalize.\n\n> LLM was able to perfectly match the explanations of the LR model SHAP only matched \u2018age\u2019 (in the wrong order) and length of stay in the correct position.\n\nSHAP and LIME impose different priors, but I believe few people in the field would describe LIME as generally better than SHAP. More consistently matching LIME heuristically may suggest that the model's predictions are more local but not necessarily better. I think this additional context is useful, but as I noted in the original review, was not my primary concern.\n\nTaking all of this into account, I stand by my comments and my score.\n\n[1] \"Faithful Reasoning Using Large Language Models\" Creswell and Shanahan 2022  \n[2] \"Faithfulness Tests for Natural Language Explanations\" Atanasova et al. 2023  \n[3] \"A Comparative Study of Faithfulness Metrics for Model Interpretability Methods\" Chan et al. 2022"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4341/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700592368760,
                "cdate": 1700592368760,
                "tmdate": 1700592368760,
                "mdate": 1700592368760,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]