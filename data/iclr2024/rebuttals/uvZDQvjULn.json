[
    {
        "title": "A bi-objective perspective on controllable language models: reward dropout improves off-policy control performance"
    },
    {
        "review": {
            "id": "CMDroaXjpg",
            "forum": "uvZDQvjULn",
            "replyto": "uvZDQvjULn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7118/Reviewer_dnJ6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7118/Reviewer_dnJ6"
            ],
            "content": {
                "summary": {
                    "value": "This work studies a trade-off between reward and likelihood, which is an important but unexplored problem in the pertaining or finetuning of LMs. And the authors proposed a simple solution to this problem, dubbed Reward Dropout."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The problem studied in this work is interesting and important, and the authors provide a thorough theoretical analysis from the perspective of Pareto optimization/bi-objective optimization."
                },
                "weaknesses": {
                    "value": "This work only focuses on a single \"balanced\" Pareto solution to the proposed bi-objective optimization, which weakens the motivation for using bi-objective formulation. According to the experimental results such as Fig. 3, if you only consider the reward metric and would like to relatively neglect the likelihood objective, why not consider the trade-off problem from the perspective of constrained optimization with the likelihood objective as the constraint?\n\nOne possible solution is that the authors can provide evidence reflecting some other trade-off solutions on the Pareto front are also important (e.g., plotting an approximate Pareto front and showing different behaviors of Pareto solutions).\n\nMoreover, the experiment only uses a relatively small language model (i.e., GPT-2). LLMs can weaken the influence of the reward-likelihood trade-off due to their larger model capacity."
                },
                "questions": {
                    "value": "Some comments:\n\n1. for eq. (5), if the behavior policy has already maximized the reward objective, will the bi-objective optimization reduce to a single objective optimization?\n\n2. for fig. 2, please provide additional results under a non-normal distribution behavior policy.\n\n3. What is the core idea of reward dropout? I think the core idea is to relax the distribution of rewards in order to achieve an easier Pareto improvement. From this perspective, I wonder why the final performance is improved by the quantile dropout that sharpens the distribution.\n\n---post-rebuttal comment---\n\nAfter reading the authors' responses and their revised version, I decided to raise my score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7118/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7118/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7118/Reviewer_dnJ6"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7118/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698716071829,
            "cdate": 1698716071829,
            "tmdate": 1700642461774,
            "mdate": 1700642461774,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "V2B3odfNwo",
                "forum": "uvZDQvjULn",
                "replyto": "CMDroaXjpg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Weaknesses\n> 1. *\u201cThis work only focuses on a single \"balanced\" Pareto solution to the proposed bi-objective optimization, which weakens the motivation for using bi-objective formulation. According to the experimental results such as Fig. 3, if you only consider the reward metric and would like to relatively neglect the likelihood objective, why not consider the trade-off problem from the perspective of constrained optimization with the likelihood objective as the constraint? One possible solution is that the authors can provide evidence reflecting some other trade-off solutions on the Pareto front are also important (e.g., plotting an approximate Pareto front and showing different behaviors of Pareto solutions).\u201d*\n\n---\n        \n- Dear Reviewer #3, thank you for your thoughtful feedback. Personally, this comment really struck a chord with us - your question is the one that has the most implications for our research, which explores the \"perspective\" of training RLMs, as opposed to the majority of recent work that focuses on developing model, designing neural network structure, or implementing an algorithm. We are truly touched by your feedback.\n- As this was a very important question with implications for our study, we wrote an answer for this as *Meta Response 3 (MR3) to be shared with all reviewers and ACs.* Please refer to our response in *MR3*. We have also included additional experiments related to your question in Appendix I. Please see the revised submission.\n    - In the additional experiment, you can see that if the RLM is trained without *the transfer learning*, then the reward and likelihood rise together (i.e., maximizing both objectives simultaneously, which implies a bi-objective problem) until the reward touches RUBO, and then as training continues, the reward objective is sacrificed to maximize the likelihood objective (This is the theoretical Issue mentioned in MR3).\n    - The trend of the reward and likelihood objective increasing together until the reward touches the RUBO shows that RLM training is a bi-objective problem, not a constrained single-objective optimization problem.\n            \n- Once again, thank you so much for your thoughtful feedback.\n            \n---\n            \n> 2. *\u201cMoreover, the experiment only uses a relatively small language model (i.e., GPT-2). LLMs can weaken the influence of the reward-likelihood trade-off due to their larger model capacity.\u201d*\n        \n---\n        \n- Dear Reviewer #3, thank you for your thoughtful feedback. If you are asking *\"Does using LLMs with greater capacity mitigate the reward-likelihood trade-off?\"*, we would say *\"yes\"*.\n\n- In fact, we have already discussed this in Fig 1(b) and Section 5. Looking at the first column (left-most column) of Fig 1(b), we can see that the target policy $\\pi$ (green bars) is skewed to the right, where the higher reward exists from Case 0 to Case 2.\n\n- Here, the green bar represents the position to which $\\pi$ converges, and it implies the reward upper bound (RUBO) by which $\\pi$ can maximize the reward subject to the behavior policy $\\beta$ (blue bars) given for each case.\n\n- To summarize, Fig 1(b) shows that the wider the coverage of $\\beta$, i.e., the larger the capacity of the LLM, the higher the RUBO, and thus the position of the green bar shifts to the right. In other words, a larger capacity of LLMs mitigates the reward-likelihood trade-off and increases controllability.\n\n- Meanwhile, thanks to your feedback, we realized that it would be very instructive to experiment with Reward Dropout for different LLMs and compare their effects directly. Therefore, we have conducted additional experiments. Please refer to Appendix J. This experiment compared reward dropout effects across GPT2 (117M), OPT (350M), XGLM (564M), and GPT2-Large (774M). It was your thoughtful feedback that inspired us to conduct this experiment. We would like to express our sincere gratitude."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7118/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700506708004,
                "cdate": 1700506708004,
                "tmdate": 1700654195645,
                "mdate": 1700654195645,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fzUZIofzzE",
                "forum": "uvZDQvjULn",
                "replyto": "CMDroaXjpg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Questions\n> 1. *\u201cfor eq. (5), if the behavior policy has already maximized the reward objective, will the bi-objective optimization reduce to a single objective optimization?\u201d* \n        \n---\n        \n- Dear Reviewer #3, thank you for your thoughtful feedback. our answer to your question is *\"yes\"*. It is correct that the bi-objective problem reduces to single-objective optimization. We think it can be easily explained with a 10-turn positioning game.\n- Let us think of Fig 1 or Fig 2. If the behavior policy $\\beta$ is already maximizing $R(\\tau)$, then the distribution of $\\beta(\\tau)$ (the blue bars) will be skewed towards the highest rewarding position. This is because if the behavior trajectories $\\tau \\sim \\beta$ are the optimal trajectories that maximize $R(\\tau)$, there are no positions from which $\\tau$ is sampled except for the highest reward positions.\n- This means that the target policy $\\pi$ will be trained with only trajectories of the highest reward position, where reward maximization is already guaranteed by $\\beta(\\tau)$. Accordingly, all that $\\pi$ needs to optimize is the likelihood objective, i.e., $\\pi$ only needs to converge to the behavior policy $\\beta$.\n        \n---\n        \n> 2. *\u201cfor fig. 2, please provide additional results under a non-normal distribution behavior policy.\u201d*\n- Dear Reviewer #3, thank you for your thoughtful feedback. We sincerely appreciate your request for additional experiments on the main content of our study. Due to a lack of space in the main text, the requested experiment has been included in Appendix C.2. Please refer to it.\n- In addition to the results presented in Appendix C.2, we conducted more experiments with different behavior policies and reward distributions. However the message from the 10-turn positioning game is consistent: *\u201cTo maximize rewards, the coverage of a behavior policy should be broad enough to explore high rewarded areas, i.e., the use of LLMs is highly encouraged.\u201d* Hence, we have included only a few of the experiments in Appendix C.2 as examples."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7118/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700506988365,
                "cdate": 1700506988365,
                "tmdate": 1700654216679,
                "mdate": 1700654216679,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "M4JW0FamsU",
                "forum": "uvZDQvjULn",
                "replyto": "CMDroaXjpg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Questions (cont.)\n> 3. *\u201cWhat is the core idea of reward dropout? I think the core idea is to relax the distribution of rewards in order to achieve an easier Pareto improvement. From this perspective, I wonder why the final performance is improved by the quantile dropout that sharpens the distribution.\u201d*\n\n- Dear Reviewer #3, thank you for your thoughtful feedback. You are correct that the core idea of Reward Dropout is to relax the reward distribution in order to achieve Pareto improvement. To be more precise, Reward Dropout is a technique to *\"manipulate the reward distribution to force the update of $\\pi$ toward a direction where Pareto improvement is guaranteed\"*.\n- Here we need to decide *\"which way to manipulate\"* the reward. You may be wondering at this point why it has to be a \"quantile\" dropout. To explain this, we first need to understand Theorems 4.1, 4.2, and 4.3, and how they lead to the Reward Dropout:\n    - Based on Theorems 4.1 and 4.2, we proved that Eq (4) contains an inherent tradeoff between the likelihood objective $\\beta(\\tau)$ and the reward objective $R(\\tau)$. Specifically, the RUBO in Theorem 4.1 and the log-negative relation in Theorem 4.2 clearly demonstrate that there exists a trade-off between $\\beta(\\tau)$ and $R(\\tau)$ in Eq (4). Therefore, we can assure that the problem of minimizing Eq (4) is equivalent to a Pareto optimization problem that optimizes two conflicting objectives.\n        - $\\text{Eq (4)} \\qquad \\text{KL}\\left[ \\pi(\\tau) || \\beta(\\tau) e^{R(\\tau)} \\right] = \\sum_{\\tau} \\pi(\\tau) \\ln\\frac{\\pi(\\tau)}{\\beta(\\tau)e^{R(\\tau)}}$\n        - $\\text{(Theorem 4.1)} \\qquad \\text{KL}\\left[ \\pi(\\tau) || \\beta(\\tau) e^{R(\\tau)} \\right] \\Longrightarrow \\mathbb{E}\\_{\\tau \\sim \\pi} \\left[ R(\\tau) \\right] \\leq \\text{KL} \\left[ \\pi(\\tau) || \\beta(\\tau) \\right] = \\text{RUBO}$\n        - $\\text{(Theorem 4.2)} \\qquad \\mathbb{E}\\_{\\tau \\sim \\pi} \\left[ R(\\tau) \\right] \\leq \\text{KL} \\left[ \\pi(\\tau) || \\beta(\\tau) \\right] \\ \\overset{\\pi \\rightarrow \\pi^{\\*}}{\\Longrightarrow} \\ \\mathbb{E}\\_{\\tau^{\\*} \\sim \\pi^{\\*}} [ R(\\tau) ] = - \\mathbb{E}\\_{\\tau^{\\*} \\sim \\pi^{\\*}} [ \\ln \\beta(\\tau)] \\ \\Longleftrightarrow \\ R(\\tau)=-\\ln\\beta(\\tau) , \\ \\ \\forall{\\tau^{\\*}} \\sim \\pi^{\\*}$\n        - In Meta Response 2 (MR2), we have explained more about how RUBO implies a trade-off. Please refer to MR2.\n                \n    - **Both Eq (6) and Theorem 4.2 describe the optimal state of Eq (4)**, which implies a Pareto optimal solution $\\tau^{\\*} \\sim \\pi^{\\*}$. This means that **Eq (6) and the result of Theorem 4.2 is just describing the same phenomenon,** i.e., the Pareto optimal state, **from the different perspective**, **one focusing on bi-objectiveness** (Eq (6)) **and the other on the trade-off** **between the two objectives** (Theorem 4.2).\n        - $\\text{Eq (6)} \\qquad \\pi^{\\*}(\\tau)=\\frac{\\beta(\\tau)e^{R(\\tau)}}{e^{1-\\lambda}} = \\beta(\\tau)e^{R(\\tau)} \\quad \\left( \\because \\lambda = 1 \\right)$\n        - As for why it should be $\\lambda=1$, please refer to our response to Weakness 5 from reviewer #2."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7118/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507380790,
                "cdate": 1700507380790,
                "tmdate": 1700654228132,
                "mdate": 1700654228132,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "U6EpW9vu2v",
                "forum": "uvZDQvjULn",
                "replyto": "CMDroaXjpg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Question (cont.)\n> 3. *\u201cWhat is the core idea of reward dropout? I think the core idea is to relax the distribution of rewards in order to achieve an easier Pareto improvement. From this perspective, I wonder why the final performance is improved by the quantile dropout that sharpens the distribution.\u201d*\n\n  - **In a Pareto optimization problem,** it is obvious that **any policy that does not achieve Pareto optimality**, i.e., $\\pi(\\tau)\\neq \\beta(\\tau)e^{R(\\tau)}$, **always has room for Pareto improvement**. And we know that Pareto optimality presented in Theorem 4.2 is derived from RUBO presented in Theorem 4.1.\n    - Here, through *reductio ad\u00a0absurdum* (i.e., through the proof by contradiction), **Theorem 4.3 shows that** the contraposition of Theorem 4.1, i.e., $\\mathbb{E}\\_{\\tau \\sim \\pi}[R(\\tau)] > \\text{KL}[\\pi(\\tau)||\\beta(\\tau)]$, leads to$\\pi(\\tau)\\neq \\beta(\\tau)e^{R(\\tau)}$, or **the negation of Theorem 4.1 leads to the negation of Eq (6), and Pareto optimality does not hold.** In other words, **Theorem 4.3 provides the condition for the Pareto improvement.**\n        - $\\text{(Theorem 4.3)} \\qquad \\mathbb{E}\\_{\\tau \\sim \\pi}[R(\\tau)] > \\text{KL}[\\pi(\\tau)||\\beta(\\tau)]  \\ \\overset{\\text{Eq (14)}}{\\Longrightarrow} \\ \\mathbb{E}\\_{\\tau \\sim \\pi}[R(\\tau)] + \\mathbb{E}\\_{\\tau \\sim \\pi}[\\ln \\beta(\\tau)] > 0 \\ \\Longrightarrow \\ \\pi(\\tau)\\neq \\beta(\\tau)e^{R(\\tau)}$\n                \n    - Therefore, **by forcing the Pareto improvement condition given by Theorem 4.3 to hold**, i.e.,  $\\mathbb{E}\\_{\\tau \\sim \\pi}[R(\\tau)] > \\text{KL}[\\pi(\\tau)||\\beta(\\tau)] \\ \\Longrightarrow \\ \\mathbb{E}\\_{\\tau \\sim \\pi}[R(\\tau)] + \\mathbb{E}\\_{\\tau \\sim \\pi}[\\ln \\beta(\\tau)] > 0$, **we can ensure that the target policy $\\pi$ or the target LM $\\pi\\_{\\theta}$ is updated only in the direction of Pareto improvement** that increases both the likelihood and reward objectives without unnecessary exploration, and as a result reach the Pareto optimal solution faster.\n    - The message behind Theorem 4.3 is simple: *\u201cthe Pareto improvement condition is satisfied when $\\mathbb{E}\\_{\\tau \\sim \\pi}[R(\\tau)] + \\mathbb{E}\\_{\\tau \\sim \\pi}[\\ln \\beta(\\tau)] >0$, so manipulate either or both $R(\\cdot)$ and $\\beta(\\cdot)$ to hold it.\u201d* In other words, **having Eq (7) hold will always update the parameter $\\theta$ of the target policy (target LM) $\\pi\\_{\\theta}$ in the direction of Pareto Improvement.**\n        - $\\text{Eq (7)} \\qquad \\mathbb{E}\\_{\\tau \\sim \\pi}[R(\\tau)] + \\mathbb{E}\\_{\\tau \\sim \\pi}[\\ln \\beta(\\tau)] > 0$\n---\n  - Given that the behavior policy is either a pre-defined distribution, i.e., $\\beta(\\cdot)$, or a pre-trained LM, i.e., $\\beta_{\\bar{\\theta}}(\\cdot)$, whose parameter is fixed as $\\bar{\\theta}$, **it is only $R(\\cdot)$ that can be manipulated. Accordingly, we only need to focus on $R(\\tau)$ to hold Eq (7).**\n    - Specifically, we need to **manipulate $R(\\tau)$ such that only a few high rewards are considered** because $\\mathbb{E}\\_{\\tau \\sim \\pi} \\left[ R(\\tau) \\right]$ refers to the average reward $\\tilde{r}$ and the average is sensitive to bias caused by outliers. Extremely saying, **a single high reward is more effective at satisfying the Pareto improvement condition, i.e., $\\mathbb{E}\\_{\\tau \\sim \\pi}[R(\\tau)] + \\mathbb{E}\\_{\\tau \\sim \\pi}[\\ln \\beta(\\tau)] > 0$, than many average rewards.**\n        - In fact, we have already written about this in the Reward Manipulation paragraph in Section 5. Please refer to the orange text in section 5 of the revised submission.\n                \n    - This is where we can explain why quantile dropout is effective. **Quantile dropout is literally a way to discard all samples with rewards below a predefined quantile and consider only a few samples with higher rewards**. In other words, **it's a technique that reinforces the bias toward high-reward samples by focusing on highly rewarding outliers to achieve Pareto Improvement.** This is why Quantile Dropout cannot help but drive Pareto Improvement.\n---\n- Please refer to Meta Response 2 (MR2) and Appendix B for more details related to Theorems 4.1, 4.2, and 4.3."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7118/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507628818,
                "cdate": 1700507628818,
                "tmdate": 1700654236980,
                "mdate": 1700654236980,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "s79kxdxVFi",
                "forum": "uvZDQvjULn",
                "replyto": "CMDroaXjpg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for raising your score."
                    },
                    "comment": {
                        "value": "Thank you so much, you helped us clarify some things and improve our contribution. \n\nWe are very grateful to you for asking questions that are particularly relevant to the essence of our research, e.g., *\"Why not consider constrained optimization?\"*, *\"What is the core idea of reward dropout\"*, as well as pointing out trivial but easily missed experiments, e.g., *\"LLMs can weaken the influence of the reward-likelihood trade-off due to their larger model capacity\"*, which contributed greatly to our research. \n\nYour dedicated feedback and scores have proven to us that our research is valuable. Thank you from the bottom of our hearts. If you have any additional comments, please let us know."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7118/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700657405001,
                "cdate": 1700657405001,
                "tmdate": 1700657405001,
                "mdate": 1700657405001,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EY3WJv6UAq",
            "forum": "uvZDQvjULn",
            "replyto": "uvZDQvjULn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7118/Reviewer_EE6f"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7118/Reviewer_EE6f"
            ],
            "content": {
                "summary": {
                    "value": "The paper provides some theoretical aspects of the RLMs by casting it as an offline RL problem. With the offline RL objective, the paper provides some properties of the optimal policies or pareto optimal policies. Based on these observations, the paper provides the reward dropout method to improve policies and tested on several benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The reward dropout methods seem simple and widely applicable. The experiment results indicate the method is actually improving performance in practice. However, I do not conduct research on LLM at all so I am unable to judge the significance of the results from the experiment."
                },
                "weaknesses": {
                    "value": "The technical section of the paper is poorly written and there are many questionable claims. For example,\n\n1. The CML and RLM are still different problems. The paper should not claim that they analysis CLM by RLM because they are \"intrinsically analogous\".\n\n2. Footnote 1 is confusing: I do not understand why the paper mentions model-based RL, and I do not understand what \"the dynamics is usually assumed\" means.\n\n3. Eq. 3 is a constrained optimization problem, and Eq. 4 is a KL divergence, I do not see why the paper claims that \"Eq. 3 can be expressed as Eq. 4\".\n\n4. $\\pi$ is never defined. Also, $\\Pi$ function class is never defined. \n\n5. In Eq. 6, why would taking $\\lambda=1$ results in the optimal policy? It seems arbitrary to me unless I missed some important derivation proving that $\\lambda=1$ is the optimal Lagrange multiplier. \n\nAlso, the theory results in section 4 seem rather trivial, instead, the paper may be improved by showing why these observations are significant."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7118/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7118/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7118/Reviewer_EE6f"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7118/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698804008548,
            "cdate": 1698804008548,
            "tmdate": 1700754857081,
            "mdate": 1700754857081,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rG2UcDyw0X",
                "forum": "uvZDQvjULn",
                "replyto": "EY3WJv6UAq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Weakness\n> 1. *The CML and RLM are still different problems. The paper should not claim that they analysis CLM by RLM because they are \"intrinsically analogous\".*\n\n---\n        \n- Dear Reviewer #2, thank you for your thoughtful feedback. As we mention in *\u201cMR1) of the Meta responses to All Reviewers,\u201d* we initially defined the scope of our work as research on controllable language models (CLMs) because we believed that our work offered an original perspective on CLMs, i.e., controlling LMs can be viewed from a bi-objective perspective, and that this perspective contributes to the field of language model control research in general. However, your comment made us realize that this generalization could be misleading. We have therefore removed the sentence that includes the expression \u201c*intrinsically analogous*\u201c. We thank you for your rigorous comments.\n        \n---\n        \n> 2. *Footnote 1 is confusing: I do not understand why the paper mentions model-based RL, and I do not understand what \"the dynamics is usually assumed\" means.*\n        \n---\n        \n- Dear Reviewer #2, thank you for your thoughtful feedback.\n- The reinforcement learning (RL) framework consists of three elements: 1) an agent that learns a policy for making decisions, 2) a reward that is given as a result of the agent's decisions, and 3) an environment model that introduces uncertainty in the environment from which the agent learns.\n- The presence or absence of an environmental model divides the RL category into model-based RL and model-free RL. we assumed that when thinking of reinforcement learning, researchers typically consider the existence of an environment, i.e., model-based RL.\n- Accordingly, given that the primary audience for this paper would be in the NLP Society, we thought it would be more helpful to explicitly mention model-based RL and model-free RL, and then make it clear that RLM belongs to the latter category.\n- However, your feedback has made us realize that mentioning this could have been more confusing. Accordingly, we decided to remove the footnote 1 based on your feedback. Thank you again.\n        \n---\n        \n> 3. *Eq. 3 is a constrained optimization problem, and Eq. 4 is a KL divergence, I do not see why the paper claims that \"Eq. 3 can be expressed as Eq. 4\".*\n        \n---\n        \n- Dear Reviewer #2, thank you for your thoughtful feedback.\n- If you look carefully, you'll notice that Eq (3) does not represent a constrained optimization problem; it's just a mathematical representation of RLM as one of many ways to model $\\ln p\\_{\\text{clm}}(\\tau|c)$ (just like Eq (1) and Eq (2)).\n- In other words, it's just a mathematical way of saying \"RLM is a fine-tuning approach that updates the parameters $\\theta\\_{\\text{lm}}$ so that the pre-trained language model $p\\_{\\text{lm}}$ is maximized for a reward function $R(\\tau)$\".\n- Perhaps the notation $s.t.$ is causing some misunderstanding, so I'll change it to notation $\\text{where}$.\n    - $\\text{Eq (3)} \\qquad \\ln p\\_{\\text{clm}}(\\tau|c) \\approx \\ln p\\_{\\text{lm}}(\\tau|\\theta\\_{\\text{clm}}) \\qquad          \\text{where} \\qquad \\theta\\_{\\text{clm}} = \\argmax\\_{\\theta\\_{\\text{lm}}} \\mathbb{E}\\_{\\tau \\sim p\\_{\\text{lm}}(\\hat{\\tau}|\\theta\\_{\\text{lm}})} \\left[ R(\\tau) \\right] \\ .$\n- Meanwhile, Eq (4) only defines the representative objective function of RLM. Therefore, the claim *\u201cEq. (3) can be expressed as Eq. (4)\u201d* is simply a statement to explain that Eq (4) is a more refined mathematical fomulation of Eq (3).\n    - $\\text{Eq (4)} \\qquad \\text{KL}\\left[ \\pi(\\tau) \\big|\\big| \\beta(\\tau)e^{ R(\\tau) } \\right] = \\sum\\_{\\tau} \\pi(\\tau) \\ln \\frac{ \\pi(\\tau) }{ \\beta(\\tau) e^{ R(\\tau) } } = - \\ \\mathbb{E}\\_{\\tau \\sim \\pi}[R(\\tau)] + \\text{KL}[\\pi(\\tau)||\\beta(\\tau)]$\n    - Representative objective function of RLM : $\\mathbb{E}\\_{\\tau \\sim \\pi}[R(\\tau)] - \\text{KL}[\\pi(\\tau)||\\beta(\\tau)]$\n        \n- For evidence of the use of Eq (4) as a representative objective function in RLM, please refer to the following references:\n    - [Stiennon et al. (2020)](https://proceedings.neurips.cc/paper/2020/file/1f89885d556929e98d3ef9b86448f951-Paper.pdf), page 6, Section 3.4, Human Feedback Policies paragraph\n    - [Korbak et al. (2022)](https://aclanthology.org/2022.findings-emnlp.77.pdf), Section 3, Equation (4)\n    - [Perez et al. (2022)](https://arxiv.org/pdf/2202.03286.pdf), Section 2.2, Reinforcement Learning paragraph\n    - [Korbak et al. (2022)](https://proceedings.neurips.cc/paper_files/paper/2022/file/67496dfa96afddab795530cc7c69b57a-Paper-Conference.pdf), page 3, Equation (2)\n    - [Ouyang et al. (2022)](https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf)\u2019s [Supplementary Material](https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Supplemental-Conference.pdf), page 31, Section D.4, Equation (2)\n    - [Bai et al. (2022)](https://arxiv.org/pdf/2204.05862.pdf), page 17, Equation (4.1)"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7118/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700505797452,
                "cdate": 1700505797452,
                "tmdate": 1700654163696,
                "mdate": 1700654163696,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EXNiXMeJdv",
                "forum": "uvZDQvjULn",
                "replyto": "EY3WJv6UAq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> 4. $\\pi$\u00a0is never defined. Also,\u00a0$\\Pi$\u00a0function class is never defined.*\n\n - Dear Reviewer #2, thank you for your thoughtful feedback. But I'm sorry that I don't understand in what context you pointed out that $\\pi$ and $\\Pi$ are not defined. Is Section 3.2, i.e., the Terms and Notation section, insufficient to provide enough information about the definition of $\\pi$? If so, I would appreciate some clarity on this feedback.\n        \n---\n        \n> 5. *In Eq. 6, why would taking\u00a0$\\lambda=1$ results in the optimal policy? It seems arbitrary to me unless I missed some important derivation proving that\u00a0$\\lambda=1$\u00a0is the optimal Lagrange multiplier.*\n\n- Dear Reviewer #2, thank you for your thoughtful feedback. I understand what your point is. However, my explicit choice of $\\lambda = 1$ in Eq (6) (a seemingly arbitrary choice) was intentional, and not arbitrary.\n\n    - $\\text{Eq (6)} \\qquad \\pi^{\\*}(\\tau) = \\frac{\\beta(\\tau)e^{R(\\tau)}}{e^{1-\\lambda}} = \\beta(\\tau)e^{R(\\tau)} \\quad \\left( \\lambda=1 \\right)$\n            \n- To understand that $\\lambda=1$ is an optimal choice, not an arbitrary choice, we first need to know that from a probabilistic perspective, the optimal policy $\\pi^{\\*}$ is always defined as the Boltzmann distribution $\\frac{\\beta(\\tau)e^{R(\\tau)}}{\\sum\\_{\\tau}\\beta(\\tau)e^{R(\\tau)}}$ with regardless of $\\lambda$.\n- Specifically, given that Eq (6) is the solution of Eq (5) and that solving Eq (5) is equivalent to minimizing Eq (4), we can derive some properties underlying Eq (6) from the perspective of minimizing Eq (4).\n    - $\\text{Eq (4)} \\qquad \\text{KL}\\left[ \\pi(\\tau) || \\beta(\\tau) e^{R(\\tau)} \\right] = \\sum\\_{\\tau} \\pi(\\tau) \\ln\\frac{\\pi(\\tau)}{\\beta(\\tau)e^{R(\\tau)}}$\n    - $\\text{Eq (5)} \\quad \\argmax\\_{\\pi} \\mathbb{E}\\_{\\tau\\ \\sim \\pi} \\left[ R(\\tau) + \\ln \\beta(\\tau) \\right] + \\mathcal{H}[\\pi] \\quad s.t. \\quad \\sum\\_{\\tau}\\pi(\\tau)=1$\n            \n- We know that minimizing Eq (4) always results in the same optimal policy $\\pi^{\\*}$ regardless of $\\lambda$, because the denominator becomes a normalization constant due to the probability condition $\\sum\\_{\\tau}\\pi(\\tau)=1$ (see Eq (10) in Appendix A.1):\n    - Eq (10)    $\\pi^{\\*}(\\tau)=\\frac{\\beta(\\tau)e^{R(\\tau)}}{e^{1-\\lambda}} =\n            \\frac{\\beta(\\tau)e^{R(\\tau)}}{\\sum\\_{\\tau}\\beta(\\tau)e^{R(\\tau)}}  \\qquad \\left(\\ \\because \\sum\\_{\\tau}\\pi(\\tau)=1 \\right)$\n            \n- We also know that Eq (4) is the KL divergence, a metric that quantifies the distance between two probability distributions, where the minimum value is always equal to 0.\n\n- Accordingly, the condition $\\sum\\_{\\tau} \\beta(\\tau)e^{R(\\tau)}=1$ is necessarily required for Eq (4) to be 0.\n    - $\\text{Eq (4)} \\qquad \\text{KL}[\\pi(\\tau)||\\beta(\\tau)e^{R(\\tau)}] = -\\sum\\_{\\tau} \\pi(\\tau)\\ln \\frac{\\beta(\\tau)e^{R(\\tau)}}{\\pi(\\tau)} \\geq -\\ln \\sum\\_{\\tau} \\pi(\\tau) \\frac{\\beta(\\tau)e^{R(\\tau)}}{\\pi(\\tau)} = - \\ln \\sum\\_{\\tau} \\beta(\\tau)e^{R(\\tau)}=0 \\quad \\bigg( \\ \\text{if} \\ \\sum\\_{\\tau} \\beta(\\tau)e^{R(\\tau)}=1 \\bigg)$\n            \n- That is,  $\\sum\\_{\\tau} \\beta(\\tau)e^{R(\\tau)}=1$ is the necessary condition for minimizing Eq (4), i.e., solving Eq (5), and therefore this condition must be considered in Eq (10).\n\n- Let us plug $\\sum\\_{\\tau} \\beta(\\tau)e^{R(\\tau)}=1$ into Eq (10). As a result, we can obtain $\\pi^{\\*}(\\tau) = \\frac{\\beta(\\tau)e^{R(\\tau)}}{e^{1-\\lambda}} = \\frac{\\beta(\\tau)e^{R(\\tau)}}{1}$, which implies $e^{1-\\lambda}=1 \\Leftrightarrow \\lambda =1$  must hold.\n\n- In conclusion, given the condition $\\sum\\_{\\tau} \\beta(\\tau)e^{R(\\tau)}=1$ that guarantees the minimum value of Eq (4), it is obvious $\\lambda=1$ is the optimal Lagrange multiplier and thus $\\pi^{\\*} = \\beta(\\tau)e^{R(\\tau)}$ will always hold."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7118/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700506117847,
                "cdate": 1700506117847,
                "tmdate": 1700654178809,
                "mdate": 1700654178809,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZCnmeGByNb",
                "forum": "uvZDQvjULn",
                "replyto": "EY3WJv6UAq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer EE6f, \n\nThank you for your detailed review. \n\nYour comment *\"The CML and RLM are still different problems\"* helped us clarify the scope of our work, and your comment *\"Footnote 1 is confusing\"* made us realize that presenting too much information can be confusing from a reader's perspective. Also, your comment *\"Why would taking $\\lambda=1$ result in the optimal policy?\"* reminded us to be more mathematically rigorous about things that we take for granted.\n\nWe hope our responses above, including Meta Responses (MR1, MR2, and MR3), answered all of your questions.\n\nIf you have any further questions about our answers or the study itself, please let us know."
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7118/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700658743975,
                "cdate": 1700658743975,
                "tmdate": 1700658873023,
                "mdate": 1700658873023,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1TeM0logAA",
            "forum": "uvZDQvjULn",
            "replyto": "uvZDQvjULn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7118/Reviewer_pDqw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7118/Reviewer_pDqw"
            ],
            "content": {
                "summary": {
                    "value": "GOAL: Study Controlled Language Generation Models (CLM) through the lens of RL\n\nOVERALL/The Main Point:\n\nThe main point of this work is to point out that CLM is basically a biobjective RL problem. They then show that this insight helps by using an RL trick in the CLM context. The trick does improve results (compared to treating it as a simple RL method). It is unclear whether reframing as an RL problem helps compared to these two baselines:\n\n 1) other methods finetune the LLM per control code but do not frame the problem in an RL manner.\n\n 2) The same RL trick used in this paper used to update the policy, vs only using the trick at controlled decoding time (and no finetuning involved). This baseline is needed since the RL trick used in this paper actually also exists in in other controlled decoding papers: https://aclanthology.org/2022.naacl-main.57.pdf)\n\n\n\n\nOTHER CONTRIBUTIONS:\n1. Frame 3 variants on CLM problems as off policy RL that has to max both likelihood of behavior policy and reward score of reward model. This means we can minimize the KL divergence of the target policy and the behavior policy + exponentiated reward. (I\u2019m not clear to me why the reward is exponentiated, other than for math convenience in Eqs 5 and 6)\n2. Frame off policy RL as a bi-objective function with a necessary Pareto frontier. \n    - Discuss tT=heoretical outcomes of 2 and empirical justification of some properties in a simple 10-turn position game.\n3. Reward Dropout is an RL trick that keeps only the top %ile of rewards to guarantee improvement based on Pareto improvement condition. They show that using this RL trick helps controlled generation as motivation for why an RL framing of the CLM problem is useful."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proofs and experimental results in Sections 4 and 5 are well written and easy to read within each section. The connection between them is a weakness (see below)\n\n2. Reward dropout is a very simple and effective strategy used to enhance decode-time fine-tuning.  \n\n3. CLMs are a common biobjective function, and making the connection to RL is neat."
                },
                "weaknesses": {
                    "value": "1. The experimental results in Section 5 does not provide surprising or insightful results. Even the translation to LLM concepts doesn\u2019t provide insight. The takeaways are that we do better when the two rewards have more overlap (so have a wide span of vocabulary and use an LLM over a smaller model) and when the two rewards have similar maxes (so line up your two rewards by training on data that fulfills both objectives). The visualization is cute, but the takeaways are already standard knowledge (bigger models do better, OOD tasks are harder). If the intent is to empirically test your proofs, then can you write this section in terms of when a policy  becomes a uniform policy (4.2), or show the pareto frontier across policies with the same reward etc. etc. The connection between 4 and 5 is weak.\n\n2. [PRESENTATION]  I\u2019d put a minimal version of Appendix D.2 in the main text -- enough to understand the set-up without reading the full Appendix.\n\n3. Also this set-up described in Appendix D.2 doesn\u2019t really allow you to modify the first part of \u03b2(\u03c4 ), and I feel that to keep with the spirit of this being a bi-objective RL problem, it should be possible to get different rewards from that first part of \u03b2(\u03c4 ).\n\n4. [PRESENTATION]  The decoding method is quite expensive, as it involves updating the params of the entire LLM for each target objective (finetuning to be polite is one entire fine-tuning, finetuning to be a negative sentiment is one entire fine-tuning). A lot of CLM methods have the same model be able to output language under different control codes. Can you call this out more explicitly in the limitations section (because it doesn't come across until you read the Appendix)\n\n5. [SOUNDNESS] For the NLP experiment, I\u2019d like to see a non RL-motivated baseline method. The random dropout isn\u2019t really a baseline. It\u2019s nice that this trick from RL translates nicely here, but does the RL framing allow you do better than other CLM methods trained on those datasets? (ex: Perhaps FUDGE as another classifier guided CLM method, or better yet, Diffusion LM adapted to this task perhaps.).\n\n6. [SOUNDNESS/NOVELTY] The last section (the NLP experiment) is strikingly similar to: https://aclanthology.org/2022.naacl-main.57.pdf I'm actually not sure there are any differences except that they don't then re-train the policy (please highlight the other differences for me if they exist). Do you do better than this paper (them only using the method as a controlled decoding solution, and your version fine-tuning based on the controlled decodes)? (A positive answer to this question would also help motivate your RL framing where the policy is typically updated).\n\n7. [NOVELTY] Framing Controlled Decoding as an RL problems has been implicitly done before: https://arxiv.org/abs/1909.09492"
                },
                "questions": {
                    "value": "1. In Eq 4, why is the reward function exponentiated? Why does this make sense in context of the problem (and not only because it leads to nice cancellations in successive equations). \n\n2. How is R computed for each sentence?\n\n3. Are there other RL tricks you believe would be useful? Adding those here (and beating CLM baselines AND comparing the effect of updating policy with the trick vs only controlled decoding with the trick) would better help make the case for the utility of the RL-perspective empirically. As is, I'm not convinced this framing is marginally more empirically useful.\n\n4. My own main weakness as a reviewer is that I may be undervaluing the theoretical contributions here. Other works have already referred to controlled decoding as an RL problem, but they do not have your proofs. Either discuss why the theory alone is a solid contribution  (Why is treating CLM as an biobjective RL problem -- as past papers do --  is an unsafe assumption without these proofs established) OR make the case that the RL perspective is useful empirically (See Question 3)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "It's a standard controlled generation use case, that suffers from no ethics concerns that don't apply to all generative text models."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7118/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7118/Reviewer_pDqw",
                        "ICLR.cc/2024/Conference/Submission7118/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7118/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698996522251,
            "cdate": 1698996522251,
            "tmdate": 1700781227577,
            "mdate": 1700781227577,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VNdxNPHx11",
                "forum": "uvZDQvjULn",
                "replyto": "1TeM0logAA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Weakness\n>1. *The experimental results in Section 5 does not provide surprising or insightful results. Even the translation to LLM concepts doesn\u2019t provide insight. The takeaways are that we do better when the two rewards have more overlap (so have a wide span of vocabulary and use an LLM over a smaller model) and when the two rewards have similar maxes (so line up your two rewards by training on data that fulfills both objectives).\u00a0The visualization is cute, but the takeaways are already standard knowledge (bigger models do better, OOD tasks are harder). If the intent is to empirically test your proofs, then can you write this section in terms of when a policy becomes a uniform policy (4.2), or show the pareto frontier across policies with the same reward etc. etc. The connection between 4 and 5 is weak.*\n\n- Dear Reviewer #1, thank you for your thoughtful feedback. Your feedback, especially *\"Section 5 does not provide surprising or insightful results\"* and *\"the takeaways are already standard knowledge\"*, was very thought-provoking for us. We have had a lot of fun preparing this response and would like to express our sincere gratitude for giving us the opportunity to respond.\n- We believe that the goal of Section 5 is best captured by the last sentence of the first paragraph in Section 5: *\"The goal of this experiment is to confirm the theoretical results and analyze them in the CLM context.\"* That is, we wrote Section 5 to support the validity of the theoretical outcomes presented in Section 4, including theorems and propositions.\n- The most important contents of our work are Theorems 4.1, 4.2, and 4.3 and the simple yet powerful technique called Reward Dropout that derives from them. **Therefore, I sincerely request that you read *\u201cMR2) of the Meta Responses\u201d* first.**\n---\n- As explained in *\u201cMR2) of the Meta Responses\u201d*, **Reward Dropout is built on Theorems 4.1, 4.2, and 4.3**, and thus we need to verify how reliable these theorems are before examining the effectiveness of Reward Dropout. More precisely, we need to verify whether our theorems can derive *\u201cobvious facts\u201d*, *\"trivial properties\"* or, to use your words, *\"unsurprising takeaways.\"* **If our theorems fit well with obvious facts** (known as *\"standard knowledge\"*) **without contradiction,** then we can say that the theorems are reliable, and by extension, **we can say that the theoretical background for Reward Dropout is reliable as well.**\n- **Accordingly, we have chosen two specific questions** that can be applied to these theorems. **Our aim is to find answers to these questions and ensure that these answers are aligned with *\u201cstandard knowledge\u201d***. The two questions we have selected are as follows:\n    - *\u201cCan behavioral policy improve the Pareto optimum of target policy (auxiliary condition; Proposition 4.1)?\u201d*\n    - *\u201cWhat is the Pareto optimum of the target policy if the behavioral policy does not have a feasible region (violation condition; Proposition 4.2)?\u201d*\n    - The reasoning behind the above questions is straightforward. We consider RLM as a bi-objective optimization problem, and it is important for optimization research to understand the conditions under which a better local optimum can be obtained (auxiliary conditions) and the conditions under which a solution cannot exist (violation conditions).\n            \n- **Proposition 4.1 follows from Theorem 4.2 and states that the expected reward of the optimal target policy $\\mathbb{E}\\_{\\tau^{} \\sim \\pi^{}}[R(\\tau)]$\u00a0can be improved**. This provides the answer to *\u201cCan behavioral policies improve the Pareto optimum of a target policy?\u201d,* which implies that *\u201cthe larger the capability of the behavioral policy, the better the expected reward of the optimal target policy.\u201d* **This aligns well with our standard knowledge *\"bigger models do better\"*** without contradiction.\n    - $\\text{(Theorem 4.2)} \\qquad  \\mathbb{E}\\_{\\tau^{*} \\sim \\pi^{\\*}} [ R(\\tau) ] = - \\mathbb{E}\\_{\\tau^{\\*} \\sim \\pi^{\\*}} [ \\ln \\beta(\\tau)]$\n    - $\\text{(Proposition 4.1)} \\qquad \\mathbb{E}\\_{\\tau^{*} \\sim \\pi^{\\*}} [ R(\\tau) ] = - \\mathbb{E}\\_{\\tau^{\\*} \\sim \\pi^{\\*}} [ \\ln \\beta(\\tau)] \\quad \\Longrightarrow \\quad \\mathbb{E}\\_{\\tau^{\\*} \\sim \\pi^{\\*}}[R(\\tau)] \\propto \\mathcal{H}[\\beta(\\tau)]$"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7118/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700503944163,
                "cdate": 1700503944163,
                "tmdate": 1700654117597,
                "mdate": 1700654117597,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TQkHpDTl71",
                "forum": "uvZDQvjULn",
                "replyto": "1TeM0logAA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### (cont.)\n>1. *The experimental results in Section 5 does not provide surprising or insightful results. Even the translation to LLM concepts doesn\u2019t provide insight. The takeaways are that we do better when the two rewards have more overlap (so have a wide span of vocabulary and use an LLM over a smaller model) and when the two rewards have similar maxes (so line up your two rewards by training on data that fulfills both objectives).\u00a0The visualization is cute, but the takeaways are already standard knowledge (bigger models do better, OOD tasks are harder). If the intent is to empirically test your proofs, then can you write this section in terms of when a policy becomes a uniform policy (4.2), or show the pareto frontier across policies with the same reward etc. etc. The connection between 4 and 5 is weak.*\n\n- **Proposition 4.2 follows from Theorem 4.1 and states that if the behavioral policy is ill-defined for all target trajectories $\\forall{\\tau} \\sim \\pi, \\ \\ \\beta(\\tau)=0$,** (i.e., no solution exists because the behavioral policy cannot access a feasible region), then the RUBO disappears, and the bounded RL problem represented by Eq (4) or Eq (5) is transformed into an unbounded entropic RL problem where the original reward plus entropic reward are maximized together. This provides the answer for *\u201cWhat is the Pareto optimum of the target policy if the behavioral policy does not have a feasible region.\u201d* As a result, the target policy will converge to a uniform policy with the highest entropy, i.e., a random policy. **Therefore, we should collect as much data as possible so that the pre-training dataset of the behavioral policy contains feasible regions, i.e., the pre-training dataset of the behavior policy is well-defined, for the control objective (reward objective).** This aligns well with our standard knowledge **\"*more data do better*\"** without contradiction.\n    - $\\text{(Theorem 4.1)} \\qquad \\mathbb{E}\\_{\\tau \\sim \\pi} \\left[ R(\\tau) \\right] \\leq \\text{KL} \\left[ \\pi(\\tau) || \\beta(\\tau) \\right] = \\text{RUBO}$\n    - $\\text{(Proposition 4.2)} \\qquad \\mathbb{E}\\_{\\tau \\sim \\pi} \\left[ R(\\tau) \\right] \\leq \\text{KL} \\left[ \\pi(\\tau) || \\beta(\\tau) \\right] \\ \\overset{\\forall{\\tau} \\sim \\pi, \\ \\ \\beta(\\tau)=0}{{\\Longrightarrow}} \\ \\ \\mathbb{E}\\_{\\tau \\sim \\pi}[R(\\tau)] + \\mathcal{H}[\\pi] \\leq +\\infty$\n    - To emphasize the implications of Proposition 4.2, we have provided the following example in the text: *\u201cFor example, let us say we need to control the target LM so that sentences are generated with negative sentiment. If the behavior LM is pre-trained on a dataset consisting only of positive sentences (i.e., the behavior policy is ill-defined), then the behavior LM cannot provide negative candidate sentences.\u201c* Please refer to the text highlighted in orange in section 5 of the revised submission.\n---\n- **In Section 5, we confirmed that Theorems align well with standard knowledge** (e.g., *\"bigger models do better\"*, \"*more data do better\"*). Fig 2(b) illustrates this well. The first column confirms the statement of Proposition 4.1 and the second column confirms the statement of Proposition 4.2. With these results, we can consider the theorems, and the theoretical background of Reward Dropout built on top of them, to be sufficiently reliable. With this reliability, the effectiveness of reward dropout was further validated by Fig 2.\n---     \n- **We believe that the content of Section 5 will be of great interest to the NLP Society, and in particular to groups working on RLM**, because Eq (4) is the typical RLM objective function, and our theorems and propositions all stem from Eq (4). **Section 5 will convey messages that are essential to modern RLM researchers**, such as the existence of a tradeoff between $\\beta(\\tau)$ and $R(\\tau)$, the possibility of Pareto improvement through reward manipulation, and the importance of large models and large amounts of data.\n    - $\\text{(Eq 4)} \\qquad \\text{KL}\\left[ \\pi(\\tau) || \\beta(\\tau) e^{R(\\tau)} \\right] = \\sum_{\\tau} \\pi(\\tau) \\ln\\frac{\\pi(\\tau)}{\\beta(\\tau)e^{R(\\tau)}}$\n    - $\\text{(Objective Function of RLM)} \\qquad \\mathbb{E}\\_{\\tau \\sim \\pi_{\\phi}^{\\text{RL}}} [R(\\tau)] - \\alpha\\text{KL}[\\pi_{\\phi}^{\\text{RL}}(\\tau)||\\pi^{\\text{SFT}}(\\tau)] \\quad \\overset{\\times -1}{\\underset{\\alpha=1}{\\Longrightarrow}} \\quad \\text{KL}[\\pi_{\\phi}^{\\text{RL}}(\\tau)||\\pi^{\\text{SFT}}(\\tau)] - \\mathbb{E}\\_{\\tau \\sim \\pi_{\\phi}^{\\text{RL}}}[R(\\tau)] = \\sum_{\\tau} \\pi_{\\phi}^{\\text{RL}}(\\tau) \\ln \\frac{\\pi_{\\phi}^{\\text{RL}}(\\tau)}{\\pi^{\\text{SFT}}(\\tau)e^{R(\\tau)}} \\ \\cdots \\ \\text{Eq (4)}$"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7118/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700504253283,
                "cdate": 1700504253283,
                "tmdate": 1700654655220,
                "mdate": 1700654655220,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "g7yKd2cDs0",
                "forum": "uvZDQvjULn",
                "replyto": "1TeM0logAA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> 2. *[PRESENTATION] I\u2019d put a minimal version of Appendix D.2 in the main text -- enough to understand the set-up without reading the full Appendix.*\n\n- Dear Reviewer #1, thank you for your thoughtful feedback. Initially, Appendix D.2 was involved in the body, but we moved it to the Appendix due to the page limit. I agree with your opinion. I had put a *minimal version of Appendix D.2* in the main text (highlighted the text in orange color). Please refer to the Implementation Details paragraph in Section 6."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7118/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700504391556,
                "cdate": 1700504391556,
                "tmdate": 1700504391556,
                "mdate": 1700504391556,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Vn7jNR4Rsd",
                "forum": "uvZDQvjULn",
                "replyto": "1TeM0logAA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> 3. *Also this set-up described in Appendix D.2 doesn\u2019t really allow you to modify the first part of \u03b2(\u03c4 ), and I feel that to keep with the spirit of this being a bi-objective RL problem, it should be possible to get different rewards from that first part of \u03b2(\u03c4 ).*\n\n- Dear Reviewer #1, thank you for your thoughtful feedback. We understand your concerns.\n- However, as the mathematical notation throughout the paper and explicit references in Section 2.2 and Appendix D.1 indicate, it is the trajectory or sentence $\\tau$ that we consider as a variable in this study. The reason we followed the mathematical framework of probabilistic inference described in Section 2.2 rather than the traditional RL framework is that we wanted to treat *\u201cthe entire trajectory as a random variable,\u201d* not just the actions, states, or parts of the trajectory.\n- In this sense, fixing the initial part of $\\tau$ is not a problem. Of course, we agree that the fact that the initial part of $\\tau$, defined as a random variable, depends on a fixed prefix, i.e., *\u201ca constant variable,\u201d* may fade the meaning of *\"random variable\"*. However, what is important to us is the complete sentence (complete trajectory) sampled starting from the prefix, and since there are theoretically an infinite number of possible complete sentences, I don't see a big problem with interpreting $\\tau$ as a random variable.\n- To summarize,\n    1. Eq (6), which formalizes the bi-objective spirit, originates from the \"probabilistic inference\" problem of minimizing Eq (4).\n    2. And the random variable defined in this paper is the complete trajectory $\\tau$, not a part of it. \n    3. Therefore, if there is no problem with interpreting $\\tau$ as a random variable as a complete sentence, then fixing a part of $\\tau$ does not go against the spirit of the bi-objective"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7118/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700504485738,
                "cdate": 1700504485738,
                "tmdate": 1700504485738,
                "mdate": 1700504485738,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WAckDp3X2R",
                "forum": "uvZDQvjULn",
                "replyto": "1TeM0logAA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> 4. *[PRESENTATION] The decoding method is quite expensive, as it involves updating the params of the entire LLM for each target objective (finetuning to be polite is one entire fine-tuning, finetuning to be a negative sentiment is one entire fine-tuning). A lot of CLM methods have the same model be able to output language under different control codes. Can you call this out more explicitly in the limitations section (because it doesn't come across until you read the Appendix)*\n\n- Dear Reviewer #1, thank you for your thoughtful feedback. Your feedback is certainly valid. Unlike other approaches such as CCLM and BCLM that allow *\"a single model to generate text from different control codes,\"* what you point out is a common problem with RLM-based approaches. We will discuss this in the limitations section as page limits allow."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7118/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700504633438,
                "cdate": 1700504633438,
                "tmdate": 1700504633438,
                "mdate": 1700504633438,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lGlM8bgkQV",
                "forum": "uvZDQvjULn",
                "replyto": "1TeM0logAA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> 5. *[SOUNDNESS] For the NLP experiment, I\u2019d like to see a non RL-motivated baseline method. The random dropout isn\u2019t really a baseline. It\u2019s nice that this trick from RL translates nicely here, but does the RL framing allow you do better than other CLM methods trained on those datasets? (ex: Perhaps FUDGE as another classifier guided CLM method, or better yet, Diffusion LM adapted to this task perhaps.).*       \n        \n---\n        \n- Dear Reviewer #1, thank you for your thoughtful feedback.\n- As we mention in *\u201cMR1) of the Meta responses to All Reviewers,\u201d* we initially defined the scope of our work as research on controllable language models (CLMs) because we believed that our work offered an original perspective on CLMs, i.e., controlling LMs can be viewed from a bi-objective perspective, and that this perspective could contribute to the field of language model control research in general. However, we believe that this generalization may be misleading. Therefore, **we clarify the scope of our research as *\"Reinforced Language Models (RLMs) that control the generation of language models (LMs) through reinforcement learning (RL).\"***\n- We reiterate that the methodological contribution of this study is *\"the finding that language models based on a reinforcement learning framework can learn better through a technique called reward dropout, in which a portion of the observed reward is intentionally dropped when utilizing RLM as a control approach.\u201d*\n- Meanwhile, even if we limit the scope of this study to RLMs, we agree that meaningful insights can be obtained by comparing the performance of RLMs with reward dropouts to non-RL-based CLMs belonging to BCLM and CCLM. Therefore, we will conduct a comparative evaluation using FUDGE and CTRL as the baseline, which belong to BCLM and CCLM, respectively. The comparative evaluation is currently delayed due to computing resource issues in the lab. We will let you know the results of the evaluation as soon as the problem is resolved."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7118/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700504748501,
                "cdate": 1700504748501,
                "tmdate": 1700504748501,
                "mdate": 1700504748501,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "h9qF4XnQSe",
                "forum": "uvZDQvjULn",
                "replyto": "1TeM0logAA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> 6. *[SOUNDNESS/NOVELTY] The last section (the NLP experiment) is strikingly similar to:\u00a0[https://aclanthology.org/2022.naacl-main.57.pdf](https://aclanthology.org/2022.naacl-main.57.pdf)\u00a0I'm actually not sure there are any differences except that they don't then re-train the policy (please highlight the other differences for me if they exist). Do you do better than this paper (them only using the method as a controlled decoding solution, and your version fine-tuning based on the controlled decodes)? (A positive answer to this question would also help motivate your RL framing where the policy is typically updated).*\n        \n---\n        \n- Dear Reviewer #1, thank you for your thoughtful feedback. we have skimmed the paper you referenced. Unfortunately, we are not sure that your reference and our work are appropriate to compare, for the following reasons:\n    - Our work belongs to the class of RLMs.\n    - The reference work, NeuroLogic A*esque paper, seems to belong to the class of BCLMs.\n            \n- Nevertheless, we believe that there are some similarities between the two papers in terms of finding paths, or trajectories, that maximize future (expected) value. Therefore, we will contrast the differences between the two papers to the best of our knowledge, to make the contribution of our work more explicit.\n    - **Our work is a train-time approach**, considering the language model as a policy and optimizing the policy for a control objective.\n        - On the other hand, **the reference work is an inference-time approach,** predicting tokens\u2019 future value subject to the control objective and steering the decoding direction accordingly.\n    - **Our work proposes an RL technique.**\n        - On the other hand, **the reference work proposes a decoding algorithm.**\n    - **The Reward Dropout** proposed in our paper intervenes in the **\"training process\"** and improves **\"training performance\"** based on **\"theoretical grounds.\"**\n        - On the other hand, **the NeuroLogic Aesque Decoding** proposed in the reference paper intervenes in the **\"inference process\"** and improves **\"decoding efficiency\"** based on **\"heuristic grounds.\".**"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7118/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700505089137,
                "cdate": 1700505089137,
                "tmdate": 1700505089137,
                "mdate": 1700505089137,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8MxOPqG1d5",
                "forum": "uvZDQvjULn",
                "replyto": "1TeM0logAA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> 7. *[NOVELTY] Framing Controlled Decoding as an RL problems has been implicitly done before:\u00a0[https://arxiv.org/abs/1909.09492](https://arxiv.org/abs/1909.09492)*\n        \n---\n        \n- Dear Reviewer #1, thank you for your thoughtful feedback. Yes, apart from the paper you just recommended, there have been various studies on controllable generation (including controlled decoding) in the context of RL problems, which we believe demonstrates that this is a very active and vibrant research area.\n- Meanwhile, the novelty of our work is that we do not view controllable generation simply as an RL problem, but rather as a bi-objective problem, and even a Pareto optimization problem. In particular, instead of \"assuming\" that the likelihood objective and the reward objective are in conflict, we mathematically \"prove\" that they are in fact in trade-offs. Based on this, we proposed a simple yet powerful technique called Reward Dropout, which we believe is the biggest novelty and contribution of our work.\n- Thank you for recommending a good paper."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7118/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700505171975,
                "cdate": 1700505171975,
                "tmdate": 1700520738226,
                "mdate": 1700520738226,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "khpSBxCMO1",
                "forum": "uvZDQvjULn",
                "replyto": "1TeM0logAA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Questions\n> 1. *In Eq 4, why is the reward function exponentiated? Why does this make sense in context of the problem (and not only because it leads to nice cancellations in successive equations).*\n\n- Dear Reviewer #1, thank you for your thoughtful feedback. You might have asked this question lightly, but for us, your question had a very big implication. Eq (4) is taken for granted in RLM research as the objective function of RLM, and it was very refreshing for me to see that Eq (4) itself could be questioned. It was also a learning experience for me. We have answered this question in *\u201cMR3) of the Meta responses to All Reviewers.\u201d* Please refer to MR3.\n        \n---\n        \n> 2. *How is R computed for each sentence?*\n\n- Dear Reviewer #1, thank you for your thoughtful feedback.  As is usual in RLM research, we pretrained the reward model to compute the rewards. The pre-trained reward model predicts the probability that the generated sentence has the intended attribute. Please refer to the Pseudo algorithm table in Appendix D.4.\n        \n---\n        \n> 3. *Are there other RL tricks you believe would be useful? Adding those here (and beating CLM baselines AND comparing the effect of updating policy with the trick vs only controlled decoding with the trick) would better help make the case for the utility of the RL perspective empirically. As is, I'm not convinced this framing is marginally more empirically useful.*\n\n- Dear Reviewer #1, thank you for your feedback. Your question about other RL 'tricks' that we believe useful sounded like a 'tricky' question, and to be honest, it is because we didn't understand what you meant by the term 'RL trick'.\n\n- Of course, there are many \u201ctricky techniques \u201c that help improve RL performance. For example, adding a baseline to the reward is one of the simplest techniques you can use. This technique fixes the reward near the baseline, making the objective function that optimizes the reward an unbiased estimator, thereby reducing the variance of trajectory sampling and improving the stability of model training.\n\n- Another example of a simple and tricky technique is reward clipping, which is a common technique used in DQNs. Reward clipping was designed to improve the robustness of a model by reducing the impact of extreme values, and is widely used as an alternative to gradient clipping; in fact, Reward Dropout is the philosophical opposite of reward clipping in that it focuses on amplifying the impact of extreme values.\n\n- If the term RL trick you mentioned means the \u201creward-driven\u201d RL techniques as simple and tricky as the examples above, as far as we know, there are not many such simple & tricky techniques. In particular, most of them focus on training stability, like reward baseline and reward clipping introduced above, and we have not yet seen any techniques that focus on improving control performance, like Reward Dropout. In this sense, Reward Dropout can be said to be very original and novel since it aims to improve control performance while being simple and tricky.\n\n- **we are strongly confident that Reward Dropout will become a very influential technique in the future because it not only definitely improves the performance of the control, but also can be applied to any method, algorithm, or task** as long as it deals with RL problems. Considering the high applicability of the RL framework despite its high computational complexity, **I believe that more and more research will be based on RL in the future, and the importance of Reward Dropout will increase accordingly.**\n        \n---\n\n> 4. *My own main weakness as a reviewer is that I may be undervaluing the theoretical contributions here. Other works have already referred to controlled decoding as an RL problem, but they do not have your proofs. Either discuss why the theory alone is a solid contribution (Why is treating CLM as an biobjective RL problem -- as past papers do -- is an unsafe assumption without these proofs established) OR make the case that the RL perspective is useful empirically (See Question 3)*\n\n- Dear Reviewer #1, thank you for your thoughtful feedback. We think our response to *\u201cweakness #1\u201d*, and *\u201cMR2) and MR3) of the Meta Responses\u201d* might be the answer to this question."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7118/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700505477647,
                "cdate": 1700505477647,
                "tmdate": 1700654146408,
                "mdate": 1700654146408,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FQFTRAXH03",
                "forum": "uvZDQvjULn",
                "replyto": "1TeM0logAA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer pDqw, \n\nFrom the volume of your feedback, your sincerity regarding our research came through to us. Your comments have provided us with a great deal of intellectual stimulation. \n\nFor example, your comment *\"The experimental results in Section 5 does not provide surprising or insightful results.\"* was very thought-provoking. It is because we thought showing our theorems are linked to *obvious facts* was not only what we intended but also most importantly to support the reliability of our theorems. When we first saw the above comment, we were puzzled, but in preparing the response, we found it very intellectually stimulating. Similarly, the question *\"In Eq 4, why is the reward function exponentiated?\"* was mind-blowing to us because we never questioned it. It was a refreshing experience for us. \n\nAlso, your question *\"Are there other RL tricks you believe would be useful?\"* made us realize that mentioning some well-known reward-driven tricks and comparing them to the proposed technique, Reward Dropout, would be a very intuitive guide for the readers. We think this comment will help improve the quality of our paper for sure. Your comments related to Appendix D.2, e.g., *\" I\u2019d put a minimal version of Appendix D.2 in the main text\"* and *\"Also this set-up described in Appendix D.2 doesn\u2019t really allow you to modify the first part of $\\beta(\\tau)$\"*,  were also very helpful in improving the sophistication of our work.\n\nOnce again, thank you for your detailed and constructive feedback.\n\nIn response to your feedback, and in terms of reconsidering the value of our study, we would like to ask you to kindly read our MR1, MR2, and MR3.\n\nIf you have any additional comments, please do not hesitate to let us know."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7118/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700657981456,
                "cdate": 1700657981456,
                "tmdate": 1700660591816,
                "mdate": 1700660591816,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MROOuBJz66",
                "forum": "uvZDQvjULn",
                "replyto": "1TeM0logAA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7118/Reviewer_pDqw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7118/Reviewer_pDqw"
                ],
                "content": {
                    "comment": {
                        "value": "Your contribution is to show: language models based on a reinforcement learning framework can learn better through a technique called reward dropout...\n\nMy point here is that the use of your specific reinforcement learning framework for such models also needs some motivation. Restated, something must be gained by using this new perspective: better results or some brand new method.\n\nIf there is a simpler method and framing that comes up with an analogous method (I think it is a strict advantage to not have to do a train time approach which is usually far more costly) and achieves better results, then I don't feel the case for this new framing is made quite as strongly. \n\nIs your claim here that the fact that this paper's method is based on theory, what makes it better than past framings with very similar methods? (That's fine by me. I just want to be sure I'm grasping the claim)."
                    }
                },
                "number": 28,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7118/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661332021,
                "cdate": 1700661332021,
                "tmdate": 1700661708742,
                "mdate": 1700661708742,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "J5wjg8kaif",
                "forum": "uvZDQvjULn",
                "replyto": "1TeM0logAA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "- Dear Reviewer pDqw, thank you for clarifying your point.\n- First of all, to the best of my knowledge, the advantages and disadvantages of CCLM, BCLM, and RLM can be summarized as follows:\n    - CCLMs (Class Conditional Language Models)\n        - Pros\n            - **Easy to Implement** : All we need is to prepare an LM and prepend a sentence with a prefix for control.\n            - **Low Training Cost** : The training cost is extremely low because CCLMs only need to prepend the sentence with one control token corresponding to the control class and simply train the LM in a supervised learning manner.\n            - **Low Inference Cost** : The inference cost is low because CCLMs only need to prepend one control token corresponding to the control class before the prefix/prompt (e.g., <bos> token), and then generate sentences.\n            - **Easy to Deploy** : Once training-time control is completed, CCLMs are easy to deploy in real-world services\n            - **High Scalability** : CCLM is easy to scale to model with multiple control objectives because, once a multi-class control token vector, rather than a single control token, is prepended before the sentence, then at inference time, we can feed any class of control tokens.\n        - Cons\n            - **Inflexible Control** : Because a CCLM injects discrete signals based on class tokens as inputs to the LM, there is less flexibility to adjust the strength of the control. For example, for sentiment control (negative vs. positive), CCLMs can only control with a polarity that falls into either of two classes: positive or negative, and CCLMs cannot generate sentences that fall on an arbitrary continuum between the two (e.g., a review that is 75% positive + 25% negative)."
                    }
                },
                "number": 29,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7118/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700730315935,
                "cdate": 1700730315935,
                "tmdate": 1700731178335,
                "mdate": 1700731178335,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NDWhBv6NCo",
                "forum": "uvZDQvjULn",
                "replyto": "1TeM0logAA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "- Dear Reviewer pDqw, thank you for clarifying your point.\n- First of all, to the best of my knowledge, the advantages and disadvantages of CCLM, BCLM, and RLM can be summarized as follows (cont.):\n    - BCLMs (Bayesian Controllable Language Models)\n        - Pros\n            - **Easy to Implement** : All we need is to prepare an LM, pre-train the classifier to the control objective, compute the probability of the on-the-fly tokens generated by LM, and sum it with the probability that the $t$-th token is classified into a specific control class.\n            - **Flexible Control** : Because BCLM utilizes a continuous-valued signal from the classifier (i.e., the probability that a sentence belongs to a particular control class), the control strength can be adjusted flexibly. For example, for sentiment control (negative vs. positive), every $t$-th token sampled from LM can be fed into a pre-trained classifier to force it to only sample tokens with a predicted probability of 75% positive + 25% negative.\n            - **Low Training Cost** : The training cost is low because BCLMs only need to pre-train a classifier (that predicts the probability that a sentence belongs to a particular control class).\n            - **High Scalability** : BCLM is easy to scale to model with multiple control objectives because once a multi-class classifier is trained, then we can compute the probabilities of tokens belonging to any classes.\n        - Cons\n            - **High Inference Cost** : The inference cost is extremely high because BCLMs require a Monte Carlo sampling. This Monte Carlo approach is expensive a lot in that sampling each token up to a complete sentence is required for each mini-batch sample. Furthermore, BCLMs are the inference-time approach that computes the probabilities (e.g., how likely the $t$-th token (1) to be sampled and (2) to be classified to a specific control class) of on-the-fly tokens generated by LM, and sum the probabilities to adjust sampling distribution to be biased toward control objective. Therefore, the computation complexity at inference time is extremely high.\n            - **Hard to Deploy** : Since inference-time control is required, BCLMs are difficult to deploy in real-world services.\n    - RLMs (Reinforced Language Models)\n        - Pros\n            - **Easy to Implement** : All we need is to prepare an LM, pre-train the reward model to the control objective, and fine-tune LM with the reward model.\n            - **Flexible Control** : Because RLMs utilize a continuous-valued signal from the reward model (i.e., the probability that a sentence belongs to a particular control class), the control strength can be adjusted flexibly. Furthermore, instead of having to compute the reward for each $t$-th token, RLMs can utilize the reward of an entire sentence (i.e., the entire trajectory). For example, for sentiment control (negative vs. positive), the entire sentence sampled from the LM can be fed into a pre-trained reward model and perform reward maximization only for the sentences with a predicted probability of 75% positive + 25% negative.\n            - **Low Inference Cost** : The inference cost is extremely low because, once trained, RLMs only need to generate sentences from the prefix/prompt (e.g., <bos> token).\n            - **Easy to Deploy** : Once training-time control is completed, RLMs are easy to deploy in real-world services.\n        - Cons\n            - **High Training Cost** : The training cost is extremely high because RLMs need to pre-train both LM and reward model, and then fine-tune (align) LM with the reward model. For fine-turning, RLMs require a Monte Carlo sampling. This Monte Carlo approach is expensive in that sampling each token up to a complete sentence is required for each mini-batch sample.\n            - **Low Scalability** : RLM is difficult to scale to model with multiple control objectives because separate models must be trained for each control class."
                    }
                },
                "number": 30,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7118/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700730427270,
                "cdate": 1700730427270,
                "tmdate": 1700731926626,
                "mdate": 1700731926626,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YykUw3IGTV",
                "forum": "uvZDQvjULn",
                "replyto": "1TeM0logAA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7118/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "- As described above, **there are clear advantages and disadvantages to each approach**, and we don't believe that **one approach is necessarily superior to the other.**\n- **Nevertheless, here's why we chose to go with RL framing:**\n    - **First, it's simple to implement**. Of course, there may be differences in details such as the design of the algorithm, model, network structure, and training framework, but in essence, RLM is all about preparing separate LM and Reward models and then making them interact with each other under a reinforcement learning framework.\n    - **Second, RLM has a significant advantage at inference time-complexity,** which is probably the most important aspect of deployment performance. RLM requires high time complexity in the training stage, but the time complexity required in the inference stage is no different from the usual autoregressive LM model.\n    - **Third, \"controllability\" is theoretically guaranteed.** From the \"Policy Improvement Theorem\", a well-known theorem in reinforcement learning, **it is guaranteed that training under the RL framework always converges to a local optimum**. We don't know if such a theoretical guarantee of controllability exists in the field of CCLM and BCLM, but **at least if we train with RLM, the control performance at the local optimum is guaranteed.**\n        - We think that the theoretical guarantee of controllability is a tremendous advantage if we extend the scope of RLM to the problem of controlling sequences (e.g., driving sequences, stock price sequences, compound sequences, DNA sequences) not just controlling natural language generation.\n        - In a model where controllability is not theoretically guaranteed, there is no certainty that control will succeed, or even that it has succeeded. When applied to technologies such as autonomous driving, such models can lead to major accidents.\n\n---\n\n- In conclusion\n    - **Reward Dropout is a technique that always guarantees to improve the control performance** of an arbitrary model under the RL framework. This means that **the Improvement achieved by Reward Dropout addresses the training time complexity that we would otherwise have to sacrifice to guarantee controllability**.\n        - To guarantee controllability, we can decide to use RL frameworks in exchange for the training-time complexity.\n        - In Sections 4 and 5, you can see that we start with much higher performance when applying reward dropouts. This means that less time is needed to train the model.\n\n    - As such, the implication of **using Reward Dropout is that controllability and training efficiency** (i.e., improved optimization) **can be achieved together.**\n    - As a result, we believe that **utilizing Reward Dropout can alleviate the complexity of training time in using RLMs, which is a relative disadvantage compared to other approaches** such as CCLM or BCLM.\n\n---\n- We have incorporated the aforementioned points into the revised Section 7, \"Limitations & Concluding Remarks.\"\n- We hope our responses and revisions help address your questions and concerns. Thank you once again for your thoughtful and constructive feedback!"
                    }
                },
                "number": 31,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7118/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700730461526,
                "cdate": 1700730461526,
                "tmdate": 1700735444321,
                "mdate": 1700735444321,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]