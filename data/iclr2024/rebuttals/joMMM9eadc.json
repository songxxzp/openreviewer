[
    {
        "title": "Effective Generation of Feasible Solutions for Integer Programming via Guided Diffusion"
    },
    {
        "review": {
            "id": "eMhEyW4eQq",
            "forum": "joMMM9eadc",
            "replyto": "joMMM9eadc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3028/Reviewer_MZmK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3028/Reviewer_MZmK"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of finding feasible solutions to integer programming problems. The authors propose a novel framework that generates complete feasible solutions end-to-end. Their framework learns the embeddings for IP instances and their solutions and then uses diffusion models to learn the distributions. Finally, they perform sampling with trained models.\n\nKey results: From their experimental results, it appears that their sampling methods provide solutions with a higher proportion of which are feasible solutions and have smaller objectives than our approaches.\nTrained on small-size datasets, their models are able to scale to large-scale instances."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. From their experimental results, it appears that their sampling methods provide solutions with a higher proportion of which are feasible solutions and have smaller objectives than our approaches.\n2. Trained on small-size datasets, their models are able to scale to large-scale instances."
                },
                "weaknesses": {
                    "value": "Major comments:\n\n1.\t\u201cFor SCIP, we adopt the first solution obtained through non-trivial heuristic algorithms during the solving phase.\u201d I don\u2019t know whether this comparison is fair. Did you try, for example, using the solutions they get within a fixed window of time?\n\n2.\tWhy do you compare your algorithm mostly with SCIP instead of Gurobi which is possibly a much better solver.\n\n3.\tHow does the objective value that you sampled compare to the optimal solution? How close are they? If they are far from each other, having a high feasible ratio does not mean anything. The feasible region increases exponentially, so there could be a large number of feasible solutions that are far from the optimal solution.\n\nMinor comments:\n\n1.\tIn \u201cRelated work\u201d, you mentioned \u201cour method aims to learn the latent structure \u2026, without any reliance on the IP solver.\u201d, but you still need to complete partial solutions use Completesol heuristic from SCIP.\n\n2.\tIn page 8, the first paragraph, you mentioned \u201cthe coverage is set to 0.1 and 0.2 due to the difficulty in finding feasible partial solutions when C > 0.2.\u201d. What do you mean by difficulty? Does it mean that you cannot find any feasible partial solutions within 30 generated solutions?\n\n3.\tIs it possible to generate repeated solutions so that the performance is not improving?\nPossible typoes: Page 3 last paragraph: DDIM then"
                },
                "questions": {
                    "value": "Combined in the \"Weaknesses\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3028/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3028/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3028/Reviewer_MZmK"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3028/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698474754689,
            "cdate": 1698474754689,
            "tmdate": 1700492780858,
            "mdate": 1700492780858,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mqqP7qbKN6",
                "forum": "joMMM9eadc",
                "replyto": "eMhEyW4eQq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3028/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3028/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer MZmK"
                    },
                    "comment": {
                        "value": "Thank you for reviewing our work! We try to address your major concerns as follows\n\n**Fair comparison and Gurobi:** The main goal of this paper is to generate good and complete initial feasible solutions end-to-end using neural network approaches, which is different from both SCIP and Gurobi solves: they aim for producing optimal solutions using initial feasible solutions as a starting point by using the branch and bound algorithm within a fixed time window. Hence, we compare the heuristic solutions obtained from SCIP and solutions after completing from Neural Diving. To further ensure the comparison comprehensive, in the paper, we have added additional results of the best heuristic solutions from Gurobi, as well as the best heuristic solutions from the PS+Gurobi algorithm, as suggested by Reviewer wgcj. The results show that the complete solutions generated by our methods have comparable quality to the best heuristic solutions from Gurobi in the SC dataset and better objectives in the CF and IS datasets. The partial solutions produced from our methods, combined with the CompleteSol heuristic, further improve the quality of solutions beyond all baseline methods. For more details, please refer to table 1 and table 2 in Section 5 in the updated manuscript (or check Table 1 and Table 2 in our global responses).\n\n**Small $C$ and Qualitative analysis:**\nWe have to point out some misunderstanding in the reviewer's comments. Our algorithms do not require small $C$ to ensure feasibility because they have a high probability of generating complete solutions. Please refer to the result of IP Guided DDIM in table 1 in section 5, which shows that the complete solutions generated by our method alone have a feasibility ratio of at least 90%. Besides, in section 5.1, we provide an illustrative example to demonstrate the distinction between our methods and Neural Diving. The example highlights that IP Guided DDIM is capable of obtaining the optimal solution during sampling without the reliance of Solvers, whereas Neural Diving requires the Solver to complete the partial solution. In addition, we have discovered that randomly sampling a certain portion of variables from the generated solutions, with completion from the CompleteSol heuristic, further improves solution quality. \n\nMoreover, we have included qualitative analysis of the generatived solutions: we sampled 1000 solutions from a single instance of the SC and IS datasets and plotted the distribution of corresponding objectives, see Section 5.3 in the updated manuscript (or see [here](https://anonymous.4open.science/r/Guided_diffusion_for_IP-B4C0/SC_instance.pdf) for the distribution of SC instance and [here](https://anonymous.4open.science/r/Guided_diffusion_for_IP-B4C0/IS_instance.pdf) for the IS instance ). The majority of solutions (over 95%) from our methods have better quality than the Gurobi heuristic. Furthermore, when combining our methods with the CompleteSol heuristic,the distributions of solutions are even closer to the optimal values.\n\nWe try to address your minor concerns as follows:\n    \n* [Partial solution] No, our approach does not need CompleteSol to generate complete feasible solutions. \n \n* [Coverage ratio] We empirically observed that for the CF dataset, when the value of $C$ is set to above 0.2, the feasibility ratio of partial solutions obtained from Neural Diving is consistently low. On average, less than 40% of the 3000 samples (30*100 instances) were found to be feasible. When $C$ is set to 0.3, none of the generated solutions were found to be feasible.\n\n* [Repeated solutions] See our qualitative analysis."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3028/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700484360693,
                "cdate": 1700484360693,
                "tmdate": 1700484360693,
                "mdate": 1700484360693,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OYtGWbzLmz",
                "forum": "joMMM9eadc",
                "replyto": "mqqP7qbKN6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3028/Reviewer_MZmK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3028/Reviewer_MZmK"
                ],
                "content": {
                    "comment": {
                        "value": "I am grateful to the authors for their careful rebuttal. My comments are addressed. In view of that, I changed my rating upward by one point (from 5 to 6)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3028/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700492815174,
                "cdate": 1700492815174,
                "tmdate": 1700492815174,
                "mdate": 1700492815174,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wOL3Yt4tEV",
            "forum": "joMMM9eadc",
            "replyto": "joMMM9eadc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3028/Reviewer_wgcj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3028/Reviewer_wgcj"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel framework that generates complete feasible solutions end-to-end (i.e., assigning all variables using neural networks) for Integer Programming (IP) problems, in contrast to most prior works that generate partial solutions (i.e., only assigning a subset of variables using neural networks).\nSpecifically, it proposes a contrastive learning approach to capture the relationship between the IP instances and the solutions, a diffusion model to generate solution embeddings, and a guided sampling strategy to enhance the feasibility and quality of solutions.\nExperiments on four datasets show that the proposed method outperforms previous state-of-the-art methods in terms of feasible ratio and objective value."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThis work is well motivated and the paper is easy to follow.\n2.\tWhile most previous methods can only generate partial solutions, this work represents a valuable attempt to an end-to-end framework to generate complete feasible solution.\n3.\tExperiments demonstrate the effectiveness of the proposed method.\n\n\ta)\tExperiments on four datasets demonstrate the effectiveness of the proposed methods compared with Neural Diving and SCIP in terms of feasible ratio and objective value.\n\n\tb)\tThe scalability test demonstrates that the proposed method can generalize to large instances.\n\n\tc)\tThe ablation study demonstrates the effectiveness of the IP guidance.\n\n\td)\tThe authors also conduct hyperparameter tuning experiments to investigate the effect of the gradient scale $s$ and the leverage factor $\\gamma$."
                },
                "weaknesses": {
                    "value": "1.\tThe authors may want to add [1] as a baseline.\n2.\tThe prediction loss defined in Eq. (3) empirically performs better than that from general diffusion models. It would be better to provide some intuitive interpretation. Moreover, the authors may want to provide the inference algorithm of the modified diffusion model.\n3.\tAs diffusion generative models may suffer from inefficiency in both training and inference, the authors may want to report the training and inference time.\n\n[1] Qingyu Han, Linxin Yang, Qian Chen, Xiang Zhou, Dong Zhang, Akang Wang, Ruoyu Sun, and Xiaodong Luo. A gnn-guided predict-and-search framework for mixed-integer linear programming. In The Eleventh International Conference on Learning Representations, 2023."
                },
                "questions": {
                    "value": "1.\tIs this work the first one to generate complete solutions? \n2.\tSee Weakness 2. The training loss defined in Eq. (3) is different from general diffusion models. Does it cause a different inference algorithm?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3028/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698662095072,
            "cdate": 1698662095072,
            "tmdate": 1699636247914,
            "mdate": 1699636247914,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5J9ytym6IC",
                "forum": "joMMM9eadc",
                "replyto": "wOL3Yt4tEV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3028/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3028/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wgcj"
                    },
                    "comment": {
                        "value": "Thank you for reviewing our work!\n\n**More experiments:** We added more experimental results comparing our method with Gurobi and the baseline suggested by the reviewer. Please check the results in the Section 5 of the manuscript (or check Table 1 and Table 2 in our global responses). Results show that our methods consistently produce higher quality solutions than these two baselines. \n\nWe also report toal training time and total sampling time for 3000 solutions on a workstation equipped with two Intel(R) Xeon(R) Platinum 8163 CPUs @ 2.50GHz, 176GB RAM, and two Nvidia V100 GPUs. During the inference phase, IP Guided DDIM exhibits faster performance than IP Guided DDPM, with average time of 0.46s-1.68s for sampling each solution (see Appendix A.8 or Table 3 in global responses for more detail).\n\n\n**Q1**:\nYes, to the best of our knowledge, our approach is the first that generates complete and feasible solutions using pure neural techniques, without relying on any solvers. \n\n**Q2: loss and inference algorithm**\n\nThe loss in Eq.(3) utilizes noisy embeddings to reconstruct the original embeddings as the objective. Intuitively, this helps enhance denoising capability in the neural networks, and facilitates the simultaneous training of the solution decoder via estimating $\\mathbf{z} _{\\mathbf{x}}$. Regarding the inference algorithm, as mentioned in Section 2, in the reverse process, the mean of $\\mathbf{z} _{\\mathbf{x}}^{(t-1)}$ can be approximated by adding $\\mathbf{z} _{\\mathbf{x}}^{(0)}$ as a condition\n$$\n  \\mathbf{\\mu} _{\\theta}(\\mathbf{z} _{\\mathbf{x}}^{(t)},t) = \\frac{\\sqrt{\\alpha _t}(1-\\bar{\\alpha} _{t-1})}{1-\\bar{\\alpha} _t}\\mathbf{z} _{\\mathbf{x}}^{(t)}+\\frac{\\sqrt{\\bar{\\alpha} _{t-1}}\\beta _t}{1-\\bar{\\alpha} _t}\\mathbf{z} _{\\mathbf{x}}^{(0)} \\tag{1}\n$$\nIn paper [1], they use the following formula (2) from the forward process \n$$\n    \\mathbf{z} _{\\mathbf{x}}^{(t)} = \\sqrt{\\bar{\\alpha} _t} \\mathbf{z} _{\\mathbf{x}}^{(0)}+\\sqrt{1-\\bar{\\alpha} _t} \\mathbf{\\epsilon} \\tag{2}\n$$\nto replace $\\mathbf{z} _{\\mathbf{x}}^{(0)}$.  It implies that\n$$ \n            \\mathbf{\\mu} _{\\theta}(\\mathbf{z} _{\\mathbf{x}}^{(t)},t) = \\frac{\\sqrt{\\alpha _t}(1-\\bar{\\alpha} _{t-1})}{1-\\bar{\\alpha} _t}\\mathbf{z} _{\\mathbf{x}}^{(t)}+\\frac{\\sqrt{\\bar{\\alpha} _{t-1}}\\beta_t}{1-\\bar{\\alpha} _t}\\mathbf{z} _{\\mathbf{x}}^{(0)} \n            = \\frac{\\sqrt{\\alpha _t}(1-\\bar{\\alpha} _{t-1})}{1-\\bar{\\alpha} _t}\\mathbf{z} _{\\mathbf{x}}^{(t)}+\\frac{\\sqrt{\\bar{\\alpha} _{t-1}}\\beta _t}{(1-\\bar{\\alpha} _t)\\sqrt{\\bar{\\alpha} _t}}(\\mathbf{z} _{\\mathbf{x}}^{(t)}- \\sqrt{1-\\bar{\\alpha} _t} \\mathbf{\\epsilon}) \n            = \\frac{1}{\\sqrt{\\alpha _t}} (\\mathbf{z} _{\\mathbf{x}}^{(t)}- \\frac{1-\\alpha _t}{\\sqrt{1-\\bar{\\alpha} _t}} \\mathbf{\\epsilon)}. \n$$\nThe last equation holds because $\\alpha _t:=1-\\beta _t$ and $\\bar{\\alpha} := \\prod _{s=1}^t \\alpha _s$.  Therefore, we obtain the original sampling method of DDPM (Ho et al., 2020).  In our work, since we use the neural network to predict $\\mathbf{z} _{\\mathbf{x}}^{(0)}$,  we can directly use Eq.(1) to estimate the mean of $\\mathbf{z} _{\\mathbf{x}}^{(t-1)}$ in sampling phase as shown in Appendix A.4 in the updated manuscript. In the experiments, we use the same variance estimation as DDPM (see Eq.(7) in Ho et al. (2020)). Based on Eq.(2), we also produce the estimation of $\\mathbf{\\epsilon}$ by using $\\mathbf{z} _{\\mathbf{x}}^{(t)}$ and predicted $\\mathbf{z} _{\\mathbf{x}}^{(0)}$ and use it in the inference phase for DDIM.\n\n[1]. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840-6851, 2020."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3028/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700483865261,
                "cdate": 1700483865261,
                "tmdate": 1700483865261,
                "mdate": 1700483865261,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NFzTfngsJt",
                "forum": "joMMM9eadc",
                "replyto": "5J9ytym6IC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3028/Reviewer_wgcj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3028/Reviewer_wgcj"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors' responses, which have addressed most of my concerns. However, I think the observed performance improvement, while notable, is not yet substantial enough for a higher score. Overall, I would like to keep my score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3028/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665102021,
                "cdate": 1700665102021,
                "tmdate": 1700665102021,
                "mdate": 1700665102021,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fTI8bdnGu7",
            "forum": "joMMM9eadc",
            "replyto": "joMMM9eadc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3028/Reviewer_jBX3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3028/Reviewer_jBX3"
            ],
            "content": {
                "summary": {
                    "value": "A solution generation method is adopted to estimate binary solutions of integer programming. The method includes a contrastive learning  gaining initial representations of solutions and instances, and a conditioned generative model estimating binary solutions. Guided sampling is adapted from present diffusion models to increase the feasibility ratio."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Applying cutting-edge deep learning to solve integer programming problems is encouraging. This research focuses on generating feasible solutions by generative model and borrows the powerful representation learning capability of neural networks. The method is technically sound by simply applying contrastive learning and diffusion model for solution estimation."
                },
                "weaknesses": {
                    "value": "My first concern is the insufficient comparison in experiments. As described in related work, considerable literature attempted to improve the diving method in solvers. Except Neural Diving (Nair et al., 2020), many follow-up works continue similar research topics. More recent methods should be compared. Even by only comparing Neural Diving, the results are not enough. The training time and resource usage are not clear, which is important to show practicality and efficiency of applying multiple deep neural networks in the proposed method. Moreover, the functions of contrastive model and generative model are not showcased by ablation study.\n\nMany works apply deep learning methods to solve integer programming problems with totally feasible solutions. To name a few, \"A general large neighborhood search framework for solving integer linear programs\", \"Learning large neighborhood search policy for integer programming\", \"Mip-gnn: A data-driven framework for guiding combinatorial solvers\". The advantage of this research over this line of works is not clear. The use case of the given method is not given. Many descriptions are not well explained (see questions).\n\n-----------------After rebuttal-------------------------\n\nI appreciate authors' detailed rebuttal. I still think the novelty is not high. The results and literature added in rebuttal are important and should have been in the original version. I raised my score a bit to 5."
                },
                "questions": {
                    "value": "1. Why SCIP is chosen in experiments but not Gurobi, given the fact that Gurobi often performs better than SCIP. \n2. GCN is described by \"It does not explicitly incorporate objective and constraint information during sampling, often resulting in infeasible complete solutions.\" In Gasse et al. (2019), GCN always gains feasible solutions.\n3. Any integer programming problem can be converted into a 0-1 programming. But the conversion increases the number of constraints a lot. How large integer programming can the method solve?\n4. What is the advantage of contrastive learning compared to supervised learning? Additional experiment should be provided to see the effect of contrastive learning without labeled solutions"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3028/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3028/Reviewer_jBX3",
                        "ICLR.cc/2024/Conference/Submission3028/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3028/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698771078329,
            "cdate": 1698771078329,
            "tmdate": 1700864934204,
            "mdate": 1700864934204,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hYmOhiZVM5",
                "forum": "joMMM9eadc",
                "replyto": "fTI8bdnGu7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3028/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3028/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer jBX3 part 1"
                    },
                    "comment": {
                        "value": "Thank you for reviewing our paper! We try to address your concerns and answer the raised questions below. \n\n**First concern and Q1: More empirical comparisons.** We added more experimental results comparing our method with Gurobi and a more recent baseline method (Han et al., 2023) suggested by Reviewer wgcj, which shows that our methods consistently produce higher quality solutions than these two baselines. Please check Table 1 and Table 2 in Section 5 for more detail (or check Table 1 and Table 2 in our global response).\n\n**Other concerns and Q4: ablations and training time.** Further, we ablated on the contrastive learning, i.e., with and without contrastive learning for the embeddings. Specifically, we include an ablation experiment in which we train IP and solution embeddings directly via the training produce of diffusion and decoder (Algorithm 2 in appendix A.3 in the updated manuscript) without using CISP, the results were updated in the Appendix A.9 of the updated manuscript (or check Table 4 in the global responses ).\n\nThe results show that CISP plays a crucial role in ensuring that the solutions produced by our methods are more feasible. We found that the advantage of contrastive learning is its ability to extract meaningful representations for IP instances and solutions, which is achieved by the assumption that instances should stay close to their feasible solutions and away from their infeasible ones. This can further integrate features from different forms, as the instance is represented using a bipartite graph and the solution is represented in a vector space. In contrast, this cannot be simply achieved by supervised learning.\n\nAs for the training time and computation usage, all our evaluations were conducted on a workstation equipped with two Intel(R) Xeon(R) Platinum 8163 CPUs @ 2.50GHz, 176GB RAM, and two Nvidia V100 GPUs. We have provided the total training time and the total inference time for generating 3000 solutions below (also see appendix A.8 in the updated manuscript). During the inference phase, IP Guided DDIM exhibits faster performance than IP Guided DDPM, with average time of 0.46s-1.68s for sampling each solution (see Appendix A.8 or Table 3 in global responses for more detail).\n\n\n**Q2.**\nWe think there is misunderstanding of some parts of our work possibly due to some misleading discussions in our paper. We agree with the reviewer that Gasse et al. (2019) proposed using a bipartite graph structure to model an IP instance and applied GCN to extract variable representations. But their primary focus was on learning the branching policy, i.e., selecting the variable for partitioning the node's search space. Follow-up studies, such as Nair et al. (2020), Yoon Taehyun (2022) and Han et al. (2023) confirm that directly using the bipartite graph and GCN to learn solutions for an IP instance may result in infeasible solutions, and thus extended this method by using partial assignments and CompleteSol heuristic to obtain complete feasible solutions. We clarified this in the revised manuscript in the paragraph 2 of Section 1.\n\n**Q3.**\nWe agree with the review that the conversion increases the number of constraints. But the point we tried to make is that our approach is generic enough to solve more general integer programming problems. In fact, with the aforementioned computation space, our approach can solve sizable IP problems: the largest instance (CF dataset) solved by our approach has about 5000 variables and 5000 constraints. These sizes can be further improved with more computation resources and more pre-training on typical small-size IP instances.\n\n**References for part 1:**\n\n[1] Qingyu Han, Linxin Yang, Qian Chen, Xiang Zhou, Dong Zhang, Akang Wang, Ruoyu Sun, and Xiaodong Luo. A GNN-guided predict-and-search framework for mixed-integer linear programming. In International Conference on Learning Representations, 2023.\n    \n[2] Maxime Gasse, Didier Chetelat, Nicola Ferroni, Laurent Charlin, and Andrea Lodi. Exact combinatorial optimization with graph convolutional neural networks. Advances in Neural Information Processing Systems, 32, 2019.\n    \n[3] Vinod Nair, Sergey Bartunov, Felix Gimeno, Ingrid Von Glehn, Pawel Lichocki, Ivan Lobov, Brendan O\u2019Donoghue, Nicolas Sonnerat, Christian Tjandraatmadja, Pengming Wang, et al. Solving mixed integer programs using neural networks. arXiv preprint arXiv:2012.13349, 2020.\n\n[4] Taehyun Yoon. Confidence threshold neural diving. CoRR, abs/2202.07506, 2022. URL https://arxiv.org/abs/2202.07506."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3028/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700483362764,
                "cdate": 1700483362764,
                "tmdate": 1700483362764,
                "mdate": 1700483362764,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "T4hIRBE5hZ",
                "forum": "joMMM9eadc",
                "replyto": "fTI8bdnGu7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3028/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3028/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer jBX3 part 2"
                    },
                    "comment": {
                        "value": "**Other related works.**\nWe thank the reviewer for sharing many papers. However, it appears that there may have been some misunderstanding regarding the relationship between our work and other papers. Therefore, we would like to provide further context in order to clarify our position and help the reviewer better understand our contribution.\n\nIt is true that numerous works have been undertaken to apply deep learning methods to solve Integer Programming problems. But the underlying ideas are quite different to each other. Particularly, Bengio et al. (2021) categorize existing methods into three main groups (see Appendix A.12 in our update manuscript for a detailed discussion):\n\n* Group 1 End-to-end learning, which refers to the training of a machine learning model to directly generate solutions based on input instances. In the context of solving integer programming (IP) problems, this involves learning to construct solutions, as demonstrated by methods like Neural Diving (Nair et al.,2020; Taehyun Yoon, 2022) and Predict-Search framework (Han et al., 2023). However, **these methods still need to rely on a solver to generate complete solution**. Another line in end-to-end learning focuses on learning to improve solutions, i.e., neighborhood search techniques (Sonnerat et al., 2021; Wu et al., 2021). **It is noteworthy that these methods typically require a solver to acquire an initial solution**. \n\n* Group 2 Complex optimization algorithms usually have a set of hyper-parameters left constant during optimization. This area use machine learning to select the values of hyper-parameters.\n\n* Group 3 Learning alongside optimization:  This field focuses on developing existed CO algorithms, typically the branch-and-bound framework, that continuously utilize a machine learning model throughout their execution, including techniques such as learning to branch (Gasse et al., 2019) and learning to node selection (Khalil et al., 2022). These works aim to generate high-quality solutions by combining ML method with the branch-and-bound framework in solvers. \n\nOur approach falls into learning to construct solutions of Group 1. Importantly, to the best of our knowledge, our approach is the first that generates complete and feasible solutions using pure neural techniques, without relying on any solvers. \n\nThe papers mentioned by the review fall into other groups and study different research questions. Specifically, the papers (Sonnerat et al., 2021; Wu et al., 2021) falls into learning to improve solutions of Group 1 since they focus on learning methods for enhancing solution quality using neighborhood search techniques, not directly related to solution generation. The papers Khalil et al., (2022) falls more into Group 3 and primarily focuses on node selection and also proposes the policy of producing a partial solution through a prescribed rounding threshold, which is then completed using SCIP, similar to Nair et al. (2020) and Taehyun Yoon (2022).\n\n\n\n**References for part 2:**\n\n[1] Qingyu Han, Linxin Yang, Qian Chen, Xiang Zhou, Dong Zhang, Akang Wang, Ruoyu Sun, and Xiaodong Luo. A GNN-guided predict-and-search framework for mixed-integer linear programming. In International Conference on Learning Representations, 2023.\n    \n[2] Yoshua Bengio, Andrea Lodi, and Antoine Prouvost. Machine learning for combinatorial optimization: a methodological tour d\u2019horizon. European Journal of Operational Research, 290(2):405\u2013421, 2021.\n    \n[3] Vinod Nair, Sergey Bartunov, Felix Gimeno, Ingrid Von Glehn, Pawel Lichocki, Ivan Lobov, Brendan O\u2019Donoghue, Nicolas Sonnerat, Christian Tjandraatmadja, Pengming Wang, et al. Solving mixed integer programs using neural networks. arXiv preprint arXiv:2012.13349, 2020.\n\n[4] Taehyun Yoon. Confidence threshold neural diving. CoRR, abs/2202.07506, 2022. URL https://arxiv.org/abs/2202.07506.\n\n[5] Nicolas Sonnerat, Pengming Wang, Ira Ktena, Sergey Bartunov, and Vinod Nair. Learning a large neighborhood search algorithm for mixed integer programs. arXiv preprint arXiv:2107.10201,2021.   \n\n[6] Yaoxin Wu, Wen Song, Zhiguang Cao, and Jie Zhang. Learning large neighborhood search policy for integer programming. Advances in Neural Information Processing Systems, 34:30075\u201330087, 2021.\n\n[7] Maxime Gasse, Didier Chetelat, Nicola Ferroni, Laurent Charlin, and Andrea Lodi. Exact combinatorial optimization with graph convolutional neural networks. Advances in Neural Information Processing Systems, 32, 2019.\n\n[8] Elias B Khalil, Christopher Morris, and Andrea Lodi. Mip-gnn: A data driven framework for guiding combinatorial solvers. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 10219\u201310227, 2022."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3028/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700483674923,
                "cdate": 1700483674923,
                "tmdate": 1700483674923,
                "mdate": 1700483674923,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Oma3iyXNiE",
                "forum": "joMMM9eadc",
                "replyto": "fTI8bdnGu7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3028/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3028/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer jBX3, thank you again for your time and efforts. We appreciate your constructive suggestions to our paper. We hope our response can address your concerns and would like to hear your feedback again. Please forgive our eagerness and impatience, we are very keen to improve our paper and really appreciate the response from an expert like you!"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3028/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700564383214,
                "cdate": 1700564383214,
                "tmdate": 1700564383214,
                "mdate": 1700564383214,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JWU1GH6XcM",
            "forum": "joMMM9eadc",
            "replyto": "joMMM9eadc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3028/Reviewer_CS3k"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3028/Reviewer_CS3k"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed a learning based IP solver. For problem and solution embedding, the solver took the GCN framework, combined with contrastive learning inspired by CLIP. In addition the authors adapted DDPM/DDIM by introducing IP specific guidance into the sampling procedure. Experiments on several IP problems showed superior performance to both Neural Diving and SCIP."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Several key components were designed to make the solver specifically effective for IP. Experiments are solid."
                },
                "weaknesses": {
                    "value": "To better validate that the quality of the proposed solver, comparison between the found optimal objective value and the ground-truth (global optimum) would be more convincing, the paper only provided relative comparison between the proposed solver and two baseline approaches."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3028/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699610481568,
            "cdate": 1699610481568,
            "tmdate": 1699636247726,
            "mdate": 1699636247726,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tpG7FrRbrN",
                "forum": "joMMM9eadc",
                "replyto": "JWU1GH6XcM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3028/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3028/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer CS3k"
                    },
                    "comment": {
                        "value": "Thank you for your suggestion! We have employed Gurobi to solve each dataset instance for 100 seconds, aiming to obtain the best possible solutions as optimal values. In the updated manuscript, we have included the average optimal values for each dataset in Table 1 and Table 2 in Section 5 (we also reported these results in Table 1 and Table 2 in the global responses). The experimental results demonstrate that our feasible solutions achieve a gap of 7% to 34% compared to the optimal values. This performance is superior to the heuristic methods from Gurobi and SCIP."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3028/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700483029757,
                "cdate": 1700483029757,
                "tmdate": 1700484384074,
                "mdate": 1700484384074,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]