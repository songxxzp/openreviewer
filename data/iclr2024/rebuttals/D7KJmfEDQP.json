[
    {
        "title": "Model Merging by Uncertainty-Based Gradient Matching"
    },
    {
        "review": {
            "id": "xOBAXbtSXV",
            "forum": "D7KJmfEDQP",
            "replyto": "D7KJmfEDQP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3827/Reviewer_1bWs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3827/Reviewer_1bWs"
            ],
            "content": {
                "summary": {
                    "value": "The paper tries to theoretically understand the impact of gradient mismatch between tasks when merging these models together. The first shows how model merging and gradient mismatches are related to each other and shows the errors that are induced due to that, based on these insights they propose a new method to reduce gradient mismatch. Next, they demonstrate how many past model merging methods are the special case of their new method and finally establish the relationship with Bayesian inference. They conclude with a small set of experiments to demonstrate the usefulness of their method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(S1) The originality of the work lies in providing the theoretical connection between model merging and the gradient mismatch problem (the identification of gradient mismatch as a problem of model merging cannot be attributed to this work, see weaknesses). Moreover, the connection with the Bayesian inference is also interesting. Additionally, the insight that the work extends RegMean (Jin et. al;2023) to non-linear parts of transformers is useful.\n\n(S2) The paper is well-written for the most part and easy to follow along. However, there are things that are not clear and are listed as Questions below.\n\n(S3) The work improves the understanding of model merging and might be useful to the people working on the model merging."
                },
                "weaknesses": {
                    "value": "(W1) The idea of identifying gradient mismatch as a problem has been claimed as one of the main contributions throughout the paper (abstract, intro, and other sections). However past works like TIES-Merging [1] have identified this problem and proposed detailed empirical studies to quantify the degree of this problem and then propose some fixes that lead to significantly improved performance. \n\n(W2) Moreover, in the current version of the paper, TIES-Merging is discussed in passing but the differences compared to that works are not properly highlighted.\n\n(W3) The experimental section is pretty thin and the results presented are weak. See more details in the Questions below."
                },
                "questions": {
                    "value": "**Need to address these questions for me to retain my score:**\n\nQ1: The papers need to be positioned better, to adjust the main contribution and highlight the similarities/differences compared to TIES-Merging [1] which claims to identify and ameliorate interference when merging models.\n\n\n\n**Need to address these as well for me to consider increasing my score:**\n\nQ2: Ideally all the experiments should also compare with TIES-Merging as that method is the closest to the final method proposed in this paper. And addresses the exact same problem that this paper tries to get at. Hence, not including that as a baseline leaves lots of open questions about the utility of the proposed method. If these comparisons are added then I will update my score as I feel this work makes a good theoretical contribution but the experimental section leaves a lot of ambiguity about the utility of the final method.\n\n\nQ3: The experimental results are very weak and seem insignificant. For example in Table (nlp), the experimental setting seems to be not well designed due to multiple reasons (i) the performance of all the methods lies between 96.1-96.8 which is quite a narrow range to make any claim about any of the methods performing better or worse than the others. for example, the difference between your method and TA is 0.3% which is not significant from my experience. Hence, either the experimental setting is too simple to highlight the differences between these methods or the methods all perform the same. An experimental setting from past papers like Task Arithmetic, TIES-Merging can be adopted for such experiments. \n\nQ4: It is not clear how the approximations made in the paper about using fisher instead of hessian after the gradient mismatch as the model becomes bigger, something on this would be useful. Moreover, could these be the reasons why the method does not lead to significant improvements in both vision and NLP settings?\n\n\nQ5: Figure-1 (right), it is not clear to me how this gradient mismatch is computed. Seems like you are adding 5 tasks on Roberta (IMDB) but then what circles represent the gradient mismatch between which models on which data? Is it a pairwise comparison of gradient mismatch or are the models being added? Please clarify exactly what this figure means. \n\n\nQ6: Moreover, the Figure-1 (right) it is shown that as the gradient mismatch decreases the test error also decreases significantly (by ~2). however, this finding seems to be inconsistent with the results in table-3 where the performance difference between TA and your method is very minimal (0.3). What is the reason behind this? In general, what is the reason behind not leading to enough improvements over TA in the nlp setting even when there is a significant gradient mismatch for TA?\n\n\n**Other questions on clarifications and details:**\n\nQ7: How is the alpha selected in your proposed method? It is mentioned in many places that alpha is not tuned.\n\nQ8: Please specify the number of samples you use to compute the fisher. \n\nQ9: For Figure-2 (left) the best performance for both TA and your method seems to be comparable to each other. I agree that your method might not need to tune for alpha but in most practical cases obtaining a small validation set and tuning alpha is not that hard. Moreover, the proposed method need to compute the fisher (requires backward pass on a subset of training data) whereas TA need validation set to tune alpha (inference on a small number of val example), so overall the peak memory usage of the proposed method would be higher while TA requires additional data. This trade-off should be highlighted in the paper.\n\nOverall, I feel that the theoretical contributions of this work are nice and would be useful to the community, I expect the author to at least position the contributions of the work better in light of past works. Moreover, strengthening the experiments section would highly increase the quality of this works\n\n[1] Resolving interference when merging models, Yadav et. al."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3827/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698452632788,
            "cdate": 1698452632788,
            "tmdate": 1699636340617,
            "mdate": 1699636340617,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "r1K3I5H5cI",
                "forum": "D7KJmfEDQP",
                "replyto": "xOBAXbtSXV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3827/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3827/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review! (part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their in-depth review and appreciation for our method. The main criticism is lack of comparison to a recent method TIES-merging (accepted at NeurIPS 2023) and experimental results being weak. We disagree with both but, to address the reviewers\u2019 concerns, we have added a discussion to TIES-merging and an additional comparison in Table 3.\n\n\n> Q1: The papers need to be positioned better, to adjust the main contribution and highlight the similarities/differences compared to TIES-Merging [1] which claims to identify and ameliorate interference when merging models.\n\nA1: Thanks for the suggestion. We have now added a few sentences to highlight the differences to TIES-Merging (see the last line in Page 2 and also Section 3.2.1 in the revised version). In summary, the method can be related to ours by using a binary mask (similarly to Ansell et al. 2022 in Table 1) but the paper [1] does not mention gradient matching and it remains unclear how the operations used in TIES directly lead to reductions in gradient mismatch.\nWhile gradient mismatch is calculated between each $\\theta_t$ and the target model $\\theta_{1:T}$, their concept of interference is outlined between the $\\theta_t$.\n\n\n>Q2: Ideally all the experiments should also compare with TIES-Merging as that method is the closest to the final method proposed in this paper. And addresses the exact same problem that this paper tries to get at. Hence, not including that as a baseline leaves lots of open questions about the utility of the proposed method. If these comparisons are added then I will update my score as I feel this work makes a good theoretical contribution but the experimental section leaves a lot of ambiguity about the utility of the final method.\n\nA2: We request the reviewer to reconsider the need to compare *all* the experiments. The ICLR policy clearly advises to not have to compare with papers posted on arXiv after May 28, 2023. We do not agree that the absence of such comparisons \u201cleaves a lot of ambiguity about the utility\u201d of our method (see A3.1 below). We also disagree that TIES-Merging is \u201cclosest\u201d to our method: our proposal is to reduce gradient mismatch which is different from the trimming used in TIES; gradient matching is never even mentioned in the TIES paper. We respectfully request the reviewer to reconsider their opinion.\n\n**To address the reviewers\u2019 concerns, we have added a comparison in Table 3 which is feasible to do in this short time. We kindly ask the reviewer to reevaluate their rating and increase their score based on the new comparison.**\n\n>Q3.1: The experimental results are very weak and seem insignificant. For example in Table (nlp), the experimental setting seems to be not well designed due to multiple reasons (i) the performance of all the methods lies between 96.1-96.8 which is quite a narrow range to make any claim about any of the methods performing better or worse than the others. for example, the difference between your method and TA is 0.3% which is not significant from my experience. Hence, either the experimental setting is too simple to highlight the differences between these methods or the methods all perform the same.\n \nA3.1: We respectfully disagree: improvements (like 0.3%) are compared to a \u201cwell-tuned TA\u201d method in Table 3, but without such tuning the baselines are much worse (also see this in Fig. 2). Larger gains with respect to other measures are also obtained in Table 2 and 4 (e.g., a gain of 3.4 for fluency PPL). A 0.3% increase is also not insignificant for a large test set (in this case containing close to 500,000 test examples). We understand that, when only focusing on a specific measure (test accuracy) with respect to one method (well-tuned TA), one may call the improvements marginal, but this does not mean that the presented methods are ineffective (and insignificant) in all respects. We request the reviewer to reconsider their opinion.\n\n>Q3.2: An experimental setting from past papers like Task Arithmetic, TIES-Merging can be adopted for such experiments.\n\nA3.2: Our experiments are already adopted from the task arithmetic papers (for example, Figure 2 (left) and Table 4 (left)) and other recent works (Table 4 (right)). TIES-Merging is not used because it is too recent but we will consider adding further experiments for the final version.\n\n>Q4: It is not clear how the approximations made in the paper about using fisher instead of hessian after the gradient mismatch as the model becomes bigger, something on this would be useful. Moreover, could these be the reasons why the method does not lead to significant improvements in both vision and NLP settings?\n\nA4: We expect better approximations to give better results but it is not correct to directly attribute bad performance to the approximation. We use the GGN approximation which is a popular choice and is effective even at large scale [2,3,4,5]. We plan to study the effect of this approximation in the future. \n(pt.2 below)"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3827/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700242321754,
                "cdate": 1700242321754,
                "tmdate": 1700242321754,
                "mdate": 1700242321754,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gZ0WENuIel",
                "forum": "D7KJmfEDQP",
                "replyto": "OcsE8ZrZEG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3827/Reviewer_1bWs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3827/Reviewer_1bWs"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for providing a detailed response. I will keep my original score because I still feel that the main experimental setting for the main merging experiments is too trivial, most methods perform similarly to each other, hence the setting is not convincing enough for me. However, I really love the theoretical insights and hence still believe that this paper adds a lot of value to the community. \n\nA6 & A7: From Figure2 it seems like the value of alpha is being tuned. Second, if you are using alpha=1 then the performance would be worse than TA in the vision setting, I am still not sure about alpha selection and at all places in the paper, this should be clarified, for example for figure 2.\n\n\"We also disagree that TIES-Merging is \u201cclosest\u201d to our method: our proposal is to reduce gradient mismatch which is different from the trimming used in TIES; gradient matching is never even mentioned in the TIES paper. We respectfully request the reviewer to reconsider their opinion.\" -> They show interference for task vectors ($\\tau = \\theta_{ft} - \\theta_{init}$), which is the accumulated gradient from all the training steps, hence the interference they show is between accumulated gradient. However, I understand the work might be too recent which is precisely why my initial rating accounted for this and was positive. My score was provided independent of this comparison. Regardless of that, I agree that this is the first work that tries to theoretically understand the interference/gradient mismatch problem and highlight very neat connections, however not the first to identify it the problem.  \n\nReply to A8: Ideally it would be good to see if fisher can be approximated using a few samples say less than 1000. I think that might work. This is a suggestion and results on this are not needed right now."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3827/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700598320194,
                "cdate": 1700598320194,
                "tmdate": 1700598320194,
                "mdate": 1700598320194,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3RgO1FtPCN",
            "forum": "D7KJmfEDQP",
            "replyto": "D7KJmfEDQP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3827/Reviewer_V4z1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3827/Reviewer_V4z1"
            ],
            "content": {
                "summary": {
                    "value": "The describes a structured way of understanding linear combination of parameters. The concept of ``target model`` is introduced as a way of measuring fitness of merges. Subsequently, modification of the ``Task arithmetic`` loss is introduced such that the gradient mismatch between the target model and the averaged models is minimized. Experimental results show comparable results."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The introduction of the paper is very well written and framed the problem in clearly.\n- The idea of defining a ``target model`` is very useful concept in this space."
                },
                "weaknesses": {
                    "value": "- Section 3, overall, was difficult to follow with seemingly several notational errors and cluttered paragraphs, see questions and suggestions section."
                },
                "questions": {
                    "value": "**Questions**\n\n- Eq(3)  should be \n$$\\alpha_{1}\\bar{\\ell}_{1}(\\theta) + \\alpha_{2}\\bar{\\ell}_{2}(\\theta)$$\n right?\nIn other words, the optimization is looking for $\\theta$ that optimizes both losses which is $\\theta_{1+2}$. If this is not a mistake then Eq(5) is wrong. \n- what does $t$ stand for in Eq(8)\n- the error between $\\bar{\\theta}_{TA}$ and $\\theta_{1:T} = \\theta_{1:T}$? please clarify/correct?\n\n**Suggestions**\n- The discussion in the last paragraph in page 3 is best to be had in the experimental section with some data.\n   or under its own section with further details.\n- Section 3.1 is difficult to follow/understand, mainly because of several math annotation issue, and not well\n   organized paragraphs. For instance, the first two paragraphs can simply be phrased as `target model` definition\n   rather than using unnecessary details and confusing notations.\n- I generally, like the framing of the problem and the idea of ``target model``. I think the paper has good potential. I suggest re-writing of section 3, highlighting the problem and the solution (perhaps computational aspect and other details) and differing questions of generality and applications to later section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3827/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698539249761,
            "cdate": 1698539249761,
            "tmdate": 1699636340544,
            "mdate": 1699636340544,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "owoPkqinQP",
                "forum": "D7KJmfEDQP",
                "replyto": "3RgO1FtPCN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3827/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3827/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review!"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their review, appreciation of the usefulness of our method, and for pointing out various typos. Their suggestions have helped us greatly in improving the writing of Section 3. We hope that the reviewer would consider increasing their score if this answers their concerns. Below we address all questions:\n\n\n> R3\u2019s suggestions on improving the presentation in Section 3.\n\nA: Thank you for the constructive comments! We have now rewritten the initial paragraphs of Section 3.1 to clarify the model definitions and split Section 3.2 into further subsections to ease readability. We also have fixed the notation mistakes, thanks again for pointing these out. For the final version, we will make sure to take your suggestions into account! \n\n\n> The discussion in the last paragraph in page 3 is best to be had in the experimental section with some data. or under its own section with further details.\n\nA: Thank you for the suggestion. For now we have kept this paragraph at the end of Section 3 but we will add an additional experiment for the final version and add further discussion. Our experiments already outline the claim at the end of the paragraph: we can use gradient mismatch to improve model merging by reducing it, for example, in Figure 1 (right) or Table 2.\n\n\n> Q1: Eq(3) should be $$\\alpha_{1}\\bar{\\ell_{1}}(\\theta) + \\alpha_{2}\\bar{\\ell_{2}}(\\theta)$$ right? In other words, the optimization is looking for $\\theta$ that optimizes both losses which is $\\theta_{1+2}$. If this is not a mistake then Eq(5) is wrong.\n\nA: Yes, thank you for the careful reading. We have corrected it! \n\n>Q2: what does $t$ stand for in Eq (8)?\n\nA: The $t$ is arbitrary but fixed and stands for a specific task or dataset $\\mathcal{D}_t$ that the model $\\theta_t$ is trained on. During merging we combine all these $\\theta_t$, of which there are $T$ in total. We have made this clearer in Section 3.1.\n\n\n> Q3: What is the error between $\\theta_{TA} \\text{ and } \\theta_{1:T} = \\theta_{1:T}$? Please clarify/correct?\n\nA: The error is calculated between $\\theta_{1:T}$ and $\\bar{\\theta_{TA}}$. \nThe former is the target model, and the latter its approximation through Task Arithmetic. \nWe can rewrite Eq. (10) to make this more explicit by subtracting the left summand on the RHS (which is $\\bar{\\theta_{TA}}$).\nThen, we obtain the following equation: \n$\\theta_{1:T} - \\bar{\\theta_{TA}} = -\\sum_{t=1}^T \\alpha_t H_0^{-1}[ \\nabla \\bar{l_t}(\\theta_{1:T}) - \\nabla \\bar{l_t}(\\theta_t)]$.\nOn the right we therefore again find the gradient mismatch as the error. We have made this clearer in the paper and rewritten the equation, thank you for the comment!\n\n> R3: Experimental results show comparable results.\n\nA: We would like to emphasize that we get consistent improvements both in terms of performance over a well-tuned task arithmetic baseline, with scaling factors determined on the test data, and better robustness to choice of scale. This was the goal of reducing gradient mismatch. Furthermore, we would like to point out the improvements for data removal shown in Table 4, which are consistent across different tasks and models."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3827/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241573461,
                "cdate": 1700241573461,
                "tmdate": 1700241573461,
                "mdate": 1700241573461,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AkaNlyxeqZ",
            "forum": "D7KJmfEDQP",
            "replyto": "D7KJmfEDQP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3827/Reviewer_vWCx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3827/Reviewer_vWCx"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses an interesting problem in the domain of model merging and offers a novel perspective by connecting gradient mismatches to the inaccuracy of weighted-averaging methods. The paper also proposes a new uncertainty-based scheme to improve model merging, which is a valuable contribution."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "+ The authors connect the inaccuracy of weighted-averaging to mismatches in the gradients and propose a new uncertainty-based scheme to improve performance by reducing the mismatch.\n+ The authors propose a unified explanation on previous model merging technique. \n+ The new method shows consistent improvements for large language models and vision transformers in terms of performance and robustness to hyperparameters."
                },
                "weaknesses": {
                    "value": "+ My major concern lies in the problem setup. I admit that model merging is a well-defined problem with much previous literature, as is discussed in the submission. But I still wonder why we need this technology. If we could obtain the data for each task, why don't we simply perform multi-task learning on these data? If we couldn't, how could we obtain the fisher information matrix on each task, which is required to approach Eq.12? It seems like a contradiction and I think more clarification on the application scenario of the model merging technique is needed, in spite of the abundance of previous literature.\n\n+ The second concern is an important missing baseline. The derivation in section 3 is similar in some degree to Regmean[1] though the latter takes linear regression as an example and then extrapolates to neural networks.  Therefore,  I would list Regmean as one of the must-to-compare baseline methods."
                },
                "questions": {
                    "value": "+ In my opinion, the model merging technique takes two or more models as input and outputs a merged model. Therefore, the performance of a merged model on down-stream tasks (compared to the unmerged model) is only a single datapoint in the experiment. In other words, what if we use different hyper-parameters to train the base model on each task? Will your method outperform others under other hyper-parameters?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3827/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3827/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3827/Reviewer_vWCx"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3827/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698589229142,
            "cdate": 1698589229142,
            "tmdate": 1699636340464,
            "mdate": 1699636340464,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Yc72sHzRPg",
                "forum": "D7KJmfEDQP",
                "replyto": "AkaNlyxeqZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3827/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3827/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review!"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their review and their appreciation for the value of our contribution.\nBelow we answer all questions:\n\n\n>Q1: My major concern lies in the problem setup. I admit that model merging is a well-defined problem with much previous literature, as is discussed in the submission. But I still wonder why we need this technology. If we could obtain the data for each task, why don't we simply perform multi-task learning on these data? \n\nA1: In summary, finetuning can be highly resource-intensive, especially for large models, while model merging allows us to reuse already trained models. Finetuning on a new task can also lead to forgetting older tasks and retraining can be very costly at times, especially for removing data from a large pretrained corpus. To clarify, we have added a note on this in the section on data removal (Section 3.2.2); also see our response to Q1 of Reviewer idAX.\n\n\n>Q2:  If we couldn't, how could we obtain the fisher information matrix on each task, which is required to approach Eq.12? It seems like a contradiction [...]\n\nA2: Many optimizers, such as **Adam, already estimate the diagonal Fisher** which could be made available with the model, see [1; Section 11.2] for a discussion on how Adam approximates diagonal Fisher. Then, there is no overhead and if this estimate is shared, data can remain private. Thank you for pointing this out, we have emphasized this more in our discussion at the end of Section 3.3.\n\n\n>Q3: The second concern is an important missing baseline. The derivation in section 3 is similar in some degree to Regmean[1] though the latter takes linear regression as an example and then extrapolates to neural networks. Therefore, I would list Regmean as one of the must-to-compare baseline methods.\n\nA3: Thank you for the constructive suggestion, we have added RegMean to Table 3! In short, RegMean shows competitive performance to other well-tuned baselines and our method.\n\n\n> Q4: In my opinion, the model merging technique takes two or more models as input and outputs a merged model. Therefore, the performance of a merged model on down-stream tasks (compared to the unmerged model) is only a single datapoint in the experiment. In other words, what if we use different hyper-parameters to train the base model on each task? Will your method outperform others under other hyper-parameters?\n\nA4: For different hyperparameters, the performance will likely further improve. For our method, **we do not tune the hyperparameters** and use the same learning rate used in [2] for RoBERTa and the same default hyperparameters for AdamW taken from the transformers library [3]. \n\n**References**\n\n[1] J. Martens. New insights and perspectives on the natural gradient method. The Journal of Machine Learning Research 21.1 (2020): 5776-5851.\n\n[2] Y. Liu, et al. Roberta: A robustly optimized bert pretraining approach. arXiv 1907.11692, 2019.\n\n[3] T. Wolf, et al. Transformers: State-of-the-art natural language processing. Proceedings of the conference on empirical methods in natural language processing: system demonstrations, 2020."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3827/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700240079747,
                "cdate": 1700240079747,
                "tmdate": 1700242522990,
                "mdate": 1700242522990,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BOLkl6IWKu",
            "forum": "D7KJmfEDQP",
            "replyto": "D7KJmfEDQP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3827/Reviewer_idAX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3827/Reviewer_idAX"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a powerful method to average two models. Specifically, the proposed method averages the model by minimizing the gradient mismatch of different models. The paper provides a deep analysis of why their method makes more sense than others.\n Also, the paper validates their method in multiple datasets from both the NLP domain and the image domain."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "From my viewpoint, the weight of LLM is knowledge abstracted from data, which stresses the importance of quickly merging knowledge learned from the dataset. I believe the topics of the paper fit into this conference and have a certain inspiration for future works in this domain. \n\nThe motivation of this paper is extremely clear by analyzing the gradient of merged models. When I read the paper, I enjoyed the motivation despite the heavy math."
                },
                "weaknesses": {
                    "value": "I have some minor concerns about this paper. Before I lay out the weaknesses list, I would like to mention that I\u2019m not an expert on NLP and my comments are probably incorrect.\n\nFinetuning vs data-driven model averaging. Maybe I don\u2019t have the background.  I\u2019m curious about the advantage of the proposed model merging over simply fine-tuning the model. In my understanding, for the proposed method to work, we would need data to calculate the gradient matrix -- that\u2019s why I call the proposed method as a data-driven model averaging. In this case, why don\u2019t we just simply fine-tune the averaged model using the LORA on the data in hand? And fine-tuning sounds more straightforward. Thus, I would recommend having a discussion/quick comparison between those two.\n\nAgain, I\u2019m a bit concerned about the time efficiency since the proposed method requires the second-order Hessian matrix, especially when compared with the simple strategy. Although it doesn\u2019t matter for the inference, it might be still worth knowing if this Hessian calculation is practical or not. So I suggest to make it clear. \n\nIn short, I have some concerns about the comparison with simple fine-tuning and time efficiency. So I currently vote for the weak accept. Again, it might be because I don\u2019t have too much domain knowledge. So I would be happy to hear back from the authors during the rebuttal in case I misunderstand anything."
                },
                "questions": {
                    "value": "Please address the question above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3827/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698774433509,
            "cdate": 1698774433509,
            "tmdate": 1699636340281,
            "mdate": 1699636340281,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "g3VaDFUhs8",
                "forum": "D7KJmfEDQP",
                "replyto": "BOLkl6IWKu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3827/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3827/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review!"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their review and helpful suggestions. Below we address the questions they asked.\n\n> Q1: Finetuning vs data-driven model averaging. Maybe I don\u2019t have the background. I\u2019m curious about the advantage of the proposed model merging over simply fine-tuning the model. In my understanding, for the proposed method to work, we would need data to calculate the gradient matrix -- that\u2019s why I call the proposed method as a data-driven model averaging. In this case, why don\u2019t we just simply fine-tune the averaged model using the LORA on the data in hand? And fine-tuning sounds more straightforward. Thus, I would recommend having a discussion/quick comparison between those two.\n \nA1: Thank you for your suggestion! We already have a comparison to finetuning in Table 3. There, the \u201call-data\u201d baseline is finetuned on all datasets at once. \nIn general, finetuning can be resource-intensive, while model merging allows one to reuse already trained models. Finetuning on a new task can also lead to forgetting older tasks. Retraining is often costly, especially for removing small subsets of a large pretraining corpus from a model. Then, merging is much **more efficient than finetuning**. We have added further discussion of this in Section 3.2.2. We also stress that estimating Hessians does not require data, because we can do this in an online fashion during training (see our response to the next question Q2).\n\n\n\n> Q2: Again, I\u2019m a bit concerned about the time efficiency since the proposed method requires the second-order Hessian matrix, especially when compared with the simple strategy. Although it doesn\u2019t matter for the inference, it might be still worth knowing if this Hessian calculation is practical or not. So I suggest to make it clear.\n\nA2: **The Hessian calculation is practical**. Our squared gradient approximation to the Hessian only incurs a small overhead (a pass over a subset of the data is often enough) but it can also be obtained in an online fashion. For example, when running second-order optimizers (or approximations thereof such as Adam, see [1; Section 11.2]), an **approximation of the Hessian is calculated during training as a \u201cby-product\u201d**. See also our response Q9 to Reviewer 1bWs. We have emphasized this further in our paper (end of Section 3.3) and hope that this makes it clearer. \n\n\n\n**References**\n\n[1] J. Martens. New insights and perspectives on the natural gradient method. The Journal of Machine Learning Research 21.1 (2020): 5776-5851."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3827/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700239424821,
                "cdate": 1700239424821,
                "tmdate": 1700242565642,
                "mdate": 1700242565642,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QKfTRKriPU",
                "forum": "D7KJmfEDQP",
                "replyto": "AJYAH22Lne",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3827/Reviewer_idAX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3827/Reviewer_idAX"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the feedbacks!"
                    },
                    "comment": {
                        "value": "Thanks for authors' feedback. And sorry for the late. I think I have no further questions -- most of my questions have been addressed and I'll reconsider my rating along with my justification later. Thanks!"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3827/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700683746236,
                "cdate": 1700683746236,
                "tmdate": 1700683746236,
                "mdate": 1700683746236,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]