[
    {
        "title": "Constructing Sparse Neural Architecture with Deterministic Ramanujan Graphs"
    },
    {
        "review": {
            "id": "2umHlCtg4a",
            "forum": "x6gnuUXpxM",
            "replyto": "x6gnuUXpxM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7198/Reviewer_t3Ja"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7198/Reviewer_t3Ja"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a deterministic method for constructing sparse neural network structures which upon weight initialization can be trained to a high accuracy. The method is based on a Ramanujan graph construction technique. Experimental results show that their methods is able to achieve high sparsity without performances losses."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed methods achieve higher sparsity compared to SNIP and GraSP.\n\n2. The method works in pre-training phase and is weight free, which are two main advatanges."
                },
                "weaknesses": {
                    "value": "1. The experiment is kind of weak. Apart from SNIP and GraSP., there are other pre-training pruning methods as well. Besides, there is also a series of works focused on during-training pruning, such as SET/RigL. I recommend the author give a comprehensive comparison as well.\n2. The performance improved compared to existing methods is limited, which means the proposed method only makes a limited contribution.\n3. A critical problem of the methods remains that unstructured sparsity is hard for physical acceleration."
                },
                "questions": {
                    "value": "1. What is the computational complexity of the proposed sparsification method to compute the sparse mask?\n2. How the sparse training is implemented in the experiments? Can the authors give a detailed explanation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7198/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698761460751,
            "cdate": 1698761460751,
            "tmdate": 1699636854568,
            "mdate": 1699636854568,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b8wa0HXtD4",
                "forum": "x6gnuUXpxM",
                "replyto": "2umHlCtg4a",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7198/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7198/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Comments."
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful comments on our article, and we appreciate the opportunity to address the concerns you've raised.\n\nWeakness 1: The experiment is kind of weak. Apart from SNIP and GraSP., there are other pre-training pruning methods as well. Besides, there is also a series of works focused on during-training pruning, such as SET/RigL. I recommend the author give a comprehensive comparison as well.\n\nResponse: We chose SNIP and GraSP as they are widely used as baselines, even though more competitive techniques exist. We did not compare with during-training pruning methods as they have a higher computational complexity and are not directly comparable. \n\nWeakness 2: The performance improved compared to existing methods is limited, which means the proposed method only makes a limited contribution.\n\nResponse: As seen from Table 9, the accuracy drop for our method is close to that of the existing methods (about 1-3%). However, we could achieve this at a significantly lower remaining weight ratio of 1.7% as compared to 5.2% for the other methods reported in Table 9. Thus, our method could obtain a sparse network with a similar accuracy drop.\n\nWeakness 3: A critical problem of the methods remains that unstructured sparsity is hard for physical acceleration.\n\nResponse: The proposed deterministic construction technique using Cayley graphs and bi-regular Ramanujan graphs do achieve structured sparsity due to the underlying symmetry and structure of the generated adjacency matrices. See discussion in pages 3 and 5. Even though we have not conducted studies in compiler and hardware optimizations, we believe that structured sparsity is an inherent advantage of the proposed adjacency matrix construction technique.\n\nQuestion 1: What is the computational complexity of the proposed sparsification method to compute the sparse mask?\n\nResponse: Computational Complexity:\nA $m$ by $n$ bipartite graph connects two network layers with $m$ and $n$ nodes respectively.\n1. The technique for Convolution layers to generate bi-regular bipartite graphs (of order $m$ by $n$) has complexity $O(mn)$. This complexity is due to the creation of the pruning mask which is of size $m \\times n$. \n2. The LPS technique to generate $p+1$ regular Ramanujan graphs  has complexity$ O(q^5 + p^4) = O(q^5) = O(m^{(5/3)})$. Here $m = n = O(q^3)$. This complexity is due to the creation of the PGL_2 group in which first we need to create the generator matrix and then find the equivalence classes which takes time $O(q^4 * q)$. The solution to the four square problem has complexity $O(p^4)$. Since the number of nodes are much less compared to the total parameters, the complexity is not very high.\n\nQuestion 2: How the sparse training is implemented in the experiments? Can the authors give a detailed explanation?\n\nResponse: We generate an initial sparse structure for the network by the Ramanujan graph construction process. Our goal was to demonstrate the effectiveness of this sparse structure. Hence, no specialized sparse training method is used thereafter. The training parameters used are listed in Table 4. One may further exploit the Ramanujan graph structure by optimizations during sparse training. However, that has not been the goal of the current study. We would be very excited to explore these optimizations in future.\n\nWe are committed to addressing your concerns and ensuring that the significance of our contributions is clearly conveyed. If you have further suggestions or specific areas you would like us to elaborate on, please feel free to let us know."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7198/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699849372417,
                "cdate": 1699849372417,
                "tmdate": 1699849372417,
                "mdate": 1699849372417,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pIBhUDqGVW",
            "forum": "x6gnuUXpxM",
            "replyto": "x6gnuUXpxM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7198/Reviewer_qgd3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7198/Reviewer_qgd3"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to prune / sparsify fully connected and convolutional layers in deep nets using deterministic Ramanujan graphs,"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Interesting and theoretically well motivated approach."
                },
                "weaknesses": {
                    "value": "While the conceptual idea on its own is interesting and promising, this paper is lacking deeper theoretical insights (actual novel theoretical contributions) and/or more importantly solid experiments that properly compare the proposed methods with recent baselines.\n\nThe paper tries to achieve the latter in Table 9, where the achieved pruning and accuracy is compared with three recent baselines. However, the baselines are evaluated on different variants of the initial neural networks (e.g. different number of parameters, etc.) and hence are not really comparable. Besides, the baselines achieve typically much higher (absolute) accuracy, but as as just said they are not really comparable.\n\nOverall, the fact that Ramanujan graphs were already used for pruning (Hoang et al ICLR 2023) makes this contribution rather iterative (going from random Ramanujan graphs to deterministic ones). I am willing to change my score if authors / other reviewers address my concerns and convince me of the significance of the contribution.\n\nMinor comments:\n* Wrong use of \\citep vs \\citet (e.g.\"[..] data independent contexts Cheng et al. (2023)\" must be (Cheng et al., 2023) and many more such examples)\n* Perhaps typo in title \"Neural Architecture**s**\"?"
                },
                "questions": {
                    "value": "/"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7198/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698793152532,
            "cdate": 1698793152532,
            "tmdate": 1699636854455,
            "mdate": 1699636854455,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dCx4d86VMb",
                "forum": "x6gnuUXpxM",
                "replyto": "pIBhUDqGVW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7198/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7198/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to comments."
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful comments on our article, and we appreciate the opportunity to address the concerns you've raised.\n\nNovelty of our work:\n\nOne of the key innovations in our article is the consideration of actual Ramanujan graphs as a PaI technique. Unlike previous works, including those by [4] and [1], our approach introduces deterministic (bi-)regular Ramanujan graphs enhancing the robustness and effectiveness of the initialization process. [4] considered checking for the spectral bound of $\\lambda_2 \\leqslant  \\sqrt{(d1_{avg}-1)} +  \\sqrt{(d2_{avg}-1)}$ as a means to ensure Ramanujan property taking inspiration from the fact that for bi-regular graphs this is indeed the case. Hoang et al [1] made it more robust by treating the problem of irregularity by introducing a parameter called $\\Delta_{r-imdb}$ which is defined on regular subgraphs within the irregular expanders and ensures that it doesn't descend into complete randomness (see for instance, pg 4 - after Definition 4 and pg - 5 IMDB). They then proceeded to do a comparison test on several architectures including VGG, ResNet etc for Cifar 10 (see Fig - 1 of [1]).\n\nIn our case, we mitigate the problem of irregularity by implementing deterministic regular Ramanujan graphs, thus avoiding the issue raised by Hoang et al. [1]. Our Table 4-5 gives the accuracy of our pruned network architecture on Cifar-10 dataset for various base architectures including ResNet. Besides accuracy, we could also achieve a better IMDB value [1] for our network. See Table 6 of revised manuscript. \n\nExperimental comparison:\n\nIn our presentation, we intentionally chose to showcase the effectiveness of our method on the VGG architecture by using the VGG-base with 135M parameters. This choice was made to emphasize our ability to implement the method without additional information on dropout and pretraining, which the smaller VGG (15M parameters) architecture utilizes.\n\nOur key observation is that, even with the larger VGG-base, we achieved similar accuracy to the unpruned VGG-base while retaining only 1.7% network density. This highlights the robustness and efficiency of our approach in achieving significant pruning without compromising performance.\n\nFor ResNet34, we used the same architecture as Hoang's to facilitate a direct comparison. Notably, after just 30 epochs (compared to 250 in Hoang's article), our model reaches a comparable accuracy to the unpruned network. This result underscores the efficiency of our method in achieving competitive performance with a reduced training time.\nWe acknowledge your concern about the comparability of baselines and will ensure that this aspect is appropriately emphasized in the revised version of the paper. \n\nKey Contributions:\n\nThe transition from random Ramanujan graphs to deterministic ones is a crucial advancement with significant implications for the properties of network structures. The previous constructions of random Ramanujan networks, while exhibiting certain properties, may lack essential characteristics such as path connectivity and are irregular, even descending into complete randomness, as observed in Hoang et al., necessitating their introduction of the Iterative Mean Difference of Bound (IMDB).  In fact, there is no proper definition of irregular Ramanujan graphs when the minimal degree is small. Both [4] and [1] mention Hoory's work on the bound of the spectral radius of the universal cover $\\tilde{G}$ of the irregular graph $G$, but it needs to be added that the theorem only works where the minimal degree is at least 2, see Hoory [3], corollary 5. This is not ensured in the previous works. \n\nIn contrast, our proposal of deterministic Ramanujan graph architecture ensures path connectivity, eliminates irregularity concerns, and maintains a high degree of symmetry. This deterministic approach addresses limitations present in random Ramanujan networks, providing a more controlled and predictable foundation for network initialization.\n\nFurther, the implementation is the first mathematically rigorous implementation of Ramanujan graph network as the previous approaches couldn't ensure the following at the same time:\n1. Minimal degree of the created graph networks is at least $2$ and \n2. The universal cover graphs have spectral radius satisfying Hoory's bound. \nWe are committed to addressing your concerns and ensuring that the significance of our contributions is clearly conveyed. If you have further suggestions or specific areas you would like us to elaborate on, please feel free to let us know.\n\n[1] Hoang, Duc NM, et al. \"Revisiting pruning at initialization through the lens of Ramanujan graph.\" ICLR 2023.\n\n[3] Hoory. \"A lower bound on the spectral radius of the universal cover of a graph.\" J. Combinatorial Theory, 2005.\n\n[4] Pal, et al. \"A Study on the Ramanujan Graph Property of Winning Lottery Tickets.\" ICML 2022."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7198/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699849321762,
                "cdate": 1699849321762,
                "tmdate": 1700623392882,
                "mdate": 1700623392882,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hlzHuGwnTf",
                "forum": "x6gnuUXpxM",
                "replyto": "dCx4d86VMb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7198/Reviewer_qgd3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7198/Reviewer_qgd3"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the reply. Note that this is not my main research field. Having read the many concerns raised by the other reviewers, I trust their judgement. Hence, I keep my score."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7198/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700492264333,
                "cdate": 1700492264333,
                "tmdate": 1700492264333,
                "mdate": 1700492264333,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WM5pHe0eXl",
            "forum": "x6gnuUXpxM",
            "replyto": "x6gnuUXpxM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7198/Reviewer_FU2a"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7198/Reviewer_FU2a"
            ],
            "content": {
                "summary": {
                    "value": "The work empirically studies a deterministic method for constructing sparse networks. The method is based on some known regular and bi-regular Ramanujan graph construction techniques.  Experiments on Cifar10/Cifar100 show the effectiveness of the proposed method compared to baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The Ramanujan graph construction techniques are based on recent improvements in graph theory on Ramanujan graphs.\n+ Empirical results demonstrate the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "- The introduction of the Ramanujan graph construction techniques is too dense and without a good preliminary, which is hard for ML communities to appreciate in general. Actually, the paper, even for its main text has one paper left to fill in. The paper was written in rush. More details on the adopted techniques should be introduced. \n\n- It confuses people when constructing fully connected layers, one adopts Cayley graphs, while to construct convolution layers, one adopts the approach in Sec 4.3. \n\n- The construction seems not to be able to construct layers with arbitrary sizes.\n\n- Empirical evaluation is not based on larger datasets, such as ImageNet. As I am not an expert who works on network pruning, I am not sure if a larger dataset should be used.\n\n- In section 4.4, q should be larger than l. However, in table 9, q is smaller than l."
                },
                "questions": {
                    "value": "1. Can the authors explain why \"In section 4.4, q should be larger than l. However, in table 9, q is smaller than l\" ?\n\n2. Can the authors explain why \"when constructing fully connected layers, one adopts Cayley graphs, while to construct convolution layers, one adopts the approach in Sec 4.3\"?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7198/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698809343990,
            "cdate": 1698809343990,
            "tmdate": 1699636854348,
            "mdate": 1699636854348,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SI0knYp5A5",
                "forum": "x6gnuUXpxM",
                "replyto": "WM5pHe0eXl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7198/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7198/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the questions."
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback. We appreciate your comments and would like to clarify the rationale behind our choices.\n\nWeakness 1: The introduction of the Ramanujan graph construction techniques is too dense and without a good preliminary. More details on the adopted techniques should be introduced. \n\nResponse: We will add the preliminaries in the revised version.\n\nWeakness 2: It confuses people when constructing fully connected layers, one adopts Cayley graphs, while to construct convolution layers, one adopts the approach in section 4.3. \n\nResponse: In our article, we employ Cayley graphs specifically for balanced bipartite graphs when constructing fully connected layers. The reason behind this is that Cayley graphs are regular (and not bi-regular) which is suitable for modelising balanced bipartite graphs.\nHowever, when dealing with unbalanced bipartite graphs, we shift our approach to the methodology outlined in section 4.3. For unbalanced cases this decision is driven by the need to modelize these graphs using bi-regular structures. \n\nWeakness 3: The construction seems not to be able to construct layers with arbitrary sizes.\n\nResponse: Thank you for highlighting this concern. We want to clarify that the ability to construct layers with arbitrary sizes is addressed in our article, specifically in section 4.5. In this section, we point to a polynomial time algorithm that enables the construction of layers with arbitrary sizes and degrees, including bi-regular degrees.\n\nOne of the novelties of the article lies in demonstrating how deterministic Ramanujan graphs can be used as PaI techniques without prior knowledge of weights and or dropouts. For instance, a lot of PaI techniques  actually have prior knowledge of dropouts of the base model and start from smaller architectures before employing the respective pruning algorithms. In this case, we don't need it. Of course, if we have smaller architectures, then also our method works.\n\nWeakness 4: Empirical evaluation is not based on larger datasets, such as ImageNet. As I am not an expert who works on network pruning, I am not sure if a larger dataset should be used.\n\nResponse: Thank you for bringing up this point. In the context of network pruning, it's common practice not to use larger datasets like ImageNet for empirical evaluation. In general pruning techniques typically focus on optimizing model efficiency and reducing redundancy in parameters, rather than aiming for high accuracy on large-scale datasets. The choice of dataset for evaluation is often influenced by the specific goals of the pruning algorithm. Smaller datasets are frequently used to assess the effectiveness of pruning in terms of parameter reduction and computational efficiency. Our decision to use datasets that align with these objectives is in line with standard practices in the field.\n\nWeakness 5: In section 4.4, q should be larger than l. However, in table 9, q is smaller than l.\n\nResponse: Thank you for pointing out this discrepancy. It's important to note that in section 4.4, the condition $q \\geq l$ is not a necessary requirement for implementing the technique outlined in section 4.3. The reason behind this flexibility lies in the specific properties of the unbalanced bipartite graph $B$, which has dimensions $q^2$ by $lq$. The critical insight is that if the unbalanced bipartite graph $B$ is Ramanujan, then its transpose, denoted as $B^T$ (with dimensions $lq$ by $q^2$), is also Ramanujan. Therefore, the conditions in Table 9 do not strictly adhere to $q$ being larger than $l$, as the Ramanujan property holds even when $q$ is smaller than  $l$.  We will ensure that this clarification is appropriately reflected in the revised version of the article.\n\nWe are committed to addressing your concerns and ensuring that the significance of our contributions is clearly conveyed. If you have further suggestions or specific areas you would like us to elaborate on, please feel free to let us know."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7198/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699849039873,
                "cdate": 1699849039873,
                "tmdate": 1699849039873,
                "mdate": 1699849039873,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5gRNSOPG5u",
            "forum": "x6gnuUXpxM",
            "replyto": "x6gnuUXpxM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7198/Reviewer_Qqf9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7198/Reviewer_Qqf9"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to use deterministic Ramanujan graphs as a PaI technique for neural architecture sparsification, which can be applied to fully connected and convolutional layers. The proposed method is evaluated on classical architectures for vision tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposed technique is theoretically supported by spectral graph theory.\n- The adopted deterministic Ramanujan diagram has desirable properties for sparsification."
                },
                "weaknesses": {
                    "value": "- Ramanujan graphs have been well-studied for pruning at initialization [1] and sparse architectures [2]. \n- The experiments are not well designed, especially for the comparison with other PaI methods and the introduction of metric $\\delta_{acc}$.\n\n[1] Hoang, Duc NM, et al. \"Revisiting pruning at initialization through the lens of Ramanujan graph.\" ICLR 2023.  \n[2] Vooturi, Dharma Teja, Girish Varma, and Kishore Kothapalli. \"Ramanujan bipartite graph products for efficient block sparse neural networks.\" Concurrency and Computation: Practice and Experience 35.14 (2023): e6363."
                },
                "questions": {
                    "value": "- It is very interesting to see that the conclusion here is different from [1], as it claims \u201cnot only the Ramanujan property for sparse networks shows no significant relationship to PaI\u2019s relative performance, but maximizing it can also lead to the formation of pseudo-random graphs with no structural meanings\u201d. Can the author elaborate how the property of \u201cdeterministic\u201d may mitigate this?\n- It seems that Table 9 is not a fair comparison. PaI baselines are applied to CIFAR-VGG, while the proposed method is applied to VGG-base instead, which is 9 times larger than CIFAR-VGG. Meanwhile, the setting adopted for them is also different. The introduced metric $\\delta_{acc}$ is inappropriate to evaluate them as the setting and difficulty of pruning are all different."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7198/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7198/Reviewer_Qqf9",
                        "ICLR.cc/2024/Conference/Submission7198/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7198/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699643126414,
            "cdate": 1699643126414,
            "tmdate": 1700723472781,
            "mdate": 1700723472781,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nHI09bpUji",
                "forum": "x6gnuUXpxM",
                "replyto": "5gRNSOPG5u",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7198/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7198/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We highlight the significance of our contribution."
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful comments on our article, and we appreciate the opportunity to address the concerns you've raised. We want to provide a detailed explanation of our approach and its distinctions from previous methods using Ramanujan graphs for PaI.\n\nPrior approaches to using Ramanujan expander graphs for PaI have predominantly relied on constructions based on random or iterated magnitude pruning techniques. Unfortunately, this approach has led to the formation of irregular graph networks that do not strictly adhere to the rigorous definition of Ramanujan graphs.\n\nIt is to be emphasized that a $b$-regular (respectively ($b$,$l$) bi-regular)  bipartite graph is considered Ramanujan if the second largest eigenvalue of its adjacency matrix is less than or equal to $2\\sqrt{(b-1)}$ (respectively  $\\sqrt{(b-1)} +  \\sqrt{(l-1)}$) where $b,l \\geq 2$. Irregular Ramanujan graphs lack a singular definition, but a widely acknowledged one, as introduced by Hoory [3], relates the second largest eigenvalue of an irregular bipartite graph to the spectral radius of its universal cover graph. This method relies on the condition that the minimal degrees of the obtained graphs need to be larger than 2. See Corollary 5. This has been used in both [4] and [1]. It can only be applied when the minimal degrees of the obtained graphs are larger than 2. \n\nHoang et al [1] mitigates the issue of irregularity by considering regular subgraphs (of regularity $\\geq 3$) of the irregular structures so formed and then using their IMDB which is essentially computing the sum $\\delta_r$ for these regular subgraphs. We take a different, approach by implementing a deterministic regular Ramanujan graph architecture for PaI. This is not contradictory to [1]; rather, it complements it. In our case, our initial graphs are inherently and rigorously Ramanujan, avoiding the complications associated with irregularity.\n\nFurther, it should be emphasized that the existence of these Ramanujan graphs are not purely random. Random regular graphs are not known to be Ramanujan (they are weakly Ramanujan viz. Freidman's proof of Alon's conjecture [5]), and their construction poses challenges. Our method provides an explicit implementation of networks modeled by deterministic Ramanujan graphs, addressing the difficulties associated with randomness in previous approaches.\n\nResponse to Weaknesses:\nTable 9 reports the accuracy of the dense as well as sparse networks. The metric $\\delta_{acc}$ is additionally presented to represent the drop in classification accuracy. We have compared with the well known PaI methods namely, random, ERK, SNIP, and GraSP.\n\nResponse to Questions:\nOur paper complements the observations of [1] rather than contradicting it. The conclusion of [1] is not that Ramanujan property has no correlation with performance. Refer to sec 4.1, Figure 1 and Figure 3 of [1] for the relevant results in [1]. It is pointed out that characterization of Ramanujan property of irregular graphs in terms of the bound of the largest non-trivial eigenvalue $\\Delta_r$ produces pseudo-randomness in the generated networks. The measures IMDB and NaRC, proposed in [1] are found to have a better correlation with network performance. This motivates us to avoid such pseudo-random generations and use deterministic regular Ramanujan graph based constructions. See Table 6.\n\nWe appreciate your scrutiny of Table 9 and understand your concern about the comparability of the baselines. We intentionally chose to showcase the effectiveness of our method on the VGG architecture by using the VGG-base with 135M parameters. This choice was made to emphasize our ability to implement the method without additional information on dropout and pretraining, which the smaller VGG (15M parameters) utilizes.\n\nOur key observation is that, even with the larger VGG-base, we achieved similar accuracy to the unpruned VGG-base while retaining only 1.7% network density. For ResNet34, we used the same architecture as [1] to facilitate a direct comparison. Notably, after just 30 epochs (compared to 250 in [1]), our model reaches a comparable accuracy to the unpruned network. This result underscores the efficiency of our method in achieving competitive performance with a reduced training time. We acknowledge your concern about the comparability of baselines and will ensure that this aspect is appropriately emphasized in the revised version of the paper. \n\nWe are committed to addressing your concerns and ensuring that the significance of our contributions is clearly conveyed. If you have further suggestions or specific areas you would like us to elaborate on, please feel free to let us know.\n\n[3] Hoory. \"A lower bound on the spectral radius of the universal cover of a graph.\" J. Combinatorial Theory, 2005.\n\n[4] Pal, et al. \"A Study on the Ramanujan Graph Property of Winning Lottery Tickets.\" ICML 2022.\n\n[5] Friedman. \u201cRelative expanders or weakly relatively Ramanujan graphs.\u201d Duke Math J. 2003."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7198/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699849027556,
                "cdate": 1699849027556,
                "tmdate": 1700623144611,
                "mdate": 1700623144611,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pom0kaCJrW",
                "forum": "x6gnuUXpxM",
                "replyto": "nHI09bpUji",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7198/Reviewer_Qqf9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7198/Reviewer_Qqf9"
                ],
                "content": {
                    "title": {
                        "value": "Followup on Author Response"
                    },
                    "comment": {
                        "value": "Thank the authors for providing clarification and addressing my concerns. I believe this work should be recognized for employing the deterministic regular Ramanujan graph for PaI, which aligns with the findings in [1]. My only remaining concern is with the experiment design, where the authors may need to put more effort into showing the benefits of the proposed methods in fair comparisons and in different settings/tasks. I raised my rating to 5 accordingly."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7198/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700723444875,
                "cdate": 1700723444875,
                "tmdate": 1700723444875,
                "mdate": 1700723444875,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]