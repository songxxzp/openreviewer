[
    {
        "title": "Babel-ImageNet: Massively Multilingual Evaluation of Vision-and-Language Representations"
    },
    {
        "review": {
            "id": "tz7HIXzzdC",
            "forum": "uLOFyiruin",
            "replyto": "uLOFyiruin",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5317/Reviewer_3yPQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5317/Reviewer_3yPQ"
            ],
            "content": {
                "summary": {
                    "value": "The purpose of this work is to introduce Babel-ImageNet, a multilingual benchmark for vision-and-language (VL) models that is designed to evaluate their performance in zero-shot image classification and image-text retrieval across several languages. Babel-ImageNet provides translations of (up to) 1000 ImageNet labels in 92 languages without relying on machine translation or manual annotation, just on a multilingual knowledge base. The study evaluates several multilingual CLIP models on the proposed benchmark and shows significant performance disparities, with low-resource languages showing (as expected) the greatest performance gap. Additionally, the paper presents an approach for enhancing the performance of multilingual CLIP models in those low-resource languages."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The study goes beyond traditional monolingual evaluation, offering a comprehensive analysis of 8 multilingual CLIP models across 92 languages. \n- The paper is well-motivated and, in general, clear enough to follow through;\n- It provides a practical and parameter-efficient approach that significantly improves model performance, making multilingual models more relevant and accessible for underrepresented linguistic communities;\n- The dataset/benchmark contribution targets a relevant issue (the overall imbalance between high and low-resourced languages);\n- The authors already provide the code for reproducibility purposes;"
                },
                "weaknesses": {
                    "value": "- BabelNet reliance. This work relies entirely on BabelNet and assumes that the mapping between WordNet and other resources is high quality. However, BabelNet is automated and has a known percentage of error, potentially affecting the label mapping [1];\n- Using WordNet synsets for translations may introduce limitations, as not all concepts or words have direct equivalents in WordNet or BabelNet, potentially impacting the completeness of translations for some languages.\n- While the paper emphasizes the creation of the benchmark and model evaluation, it could benefit from a deeper analysis of why certain languages perform poorly according to the chosen metrics and explore potential solutions to address these disparities;\n- Considering that the paper belongs to the \"datasets and benchmarks\" area, the methodology employed (mapping from ImageNet to WordNet and then to BabelNet) is expectedly straightforward.  However, I think there's also some weakness in the data cleaning and validation since the obtained multilingual data is used to evaluate models, but those same benchmarks cannot be used to assess the quality of the data itself;\n- The paper's process of removing words with identical English counterparts in the class label translation and cleaning may not be fully justified, as there can be legitimate shared words between the English and language-specific vocabulary;\n\n[1] Ten Years of BabelNet: A Survey. Roberto Navigli, Michele Bevilacqua, Simone Conia, Dario Montagnini, Francesco Cecconi. IJCAI 2021"
                },
                "questions": {
                    "value": "- There's a missing reference to another text-image dataset produced by *manual* annotation over BabelNet synsets [1]. In general, I'm curious about the possible usage of this dataset as a proxy to evaluate at least the Babel-ImageNet methodology part that performs prompt translation. I'd like to kindly ask the authors what they think about it or if they have alternative ideas toward strengthening data validation, which I think is the strongest weakness in this current version of the manuscript;\n- Not a direct weakness, so I'm listing it here: multiple usages of the verb \"demonstrate\" (e.g., \"we demonstrate that OpenCLIP performance strongly correlates with the distribution of languages in LAION5B\" ). Personally, given the empirical nature of this work, I wouldn't suggest using such a theoretical-related term;\n\nGiven the doubts about the data validation, I'm starting the discussion period leaning toward rejection. However, I'm open to changing my assessment in light of the rebuttal discussions and the reviews of my colleagues.\n\n[1] Fatality Killed the Cat or: BabelPic, a Multimodal Dataset for Non-Concrete Concepts. Agostina Calabrese, Michele Bevilacqua, Roberto Navigli. ACL 2020"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5317/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5317/Reviewer_3yPQ",
                        "ICLR.cc/2024/Conference/Submission5317/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5317/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698770590461,
            "cdate": 1698770590461,
            "tmdate": 1700654905231,
            "mdate": 1700654905231,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "H6WwQOTsKv",
                "forum": "uLOFyiruin",
                "replyto": "tz7HIXzzdC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5317/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5317/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 1/2"
                    },
                    "comment": {
                        "value": "Thank you for your comprehensive review!\n\n> BabelNet reliance. This work relies entirely on BabelNet and assumes that the mapping between WordNet and other resources is high quality. However, BabelNet is automated and has a known percentage of error, potentially affecting the label mapping [1];\n\n\nYes, BabelNet is created automatically and thus not not error-free. But quoting [1]  \u201cFor example, in BabelNet 5.0 more than 90% of the mapping between Wikipedia pages and WordNet synsets has been manually validated by experts, resulting in an overall mapping precision above 99.5%.\u201d & \u201cWiktionary entries are now integrated automatically using a BERT-based neural model, finetuned to associate a word sense definition to its correct synset, attaining an F1 score of 92% on a manually annotated test set.\u201d . \n\nThis suggests an error rate that is, to us, in an acceptable range (and substantially lower than any machine translation of concepts in isolation would yield). There is a clear trade-off between the range of \u201ctranslation\u201d errors and the number of supported languages: manual annotation is (virtually) error free, but does not scale to many languages; MT scales to hundreds of languages, but is very error-prone for concept translation; by \u201ctrusting\u201d BabelNet, we can greatly increase the number of languages, if we accept the small error rate present in the resource \u2013 getting this way close to a \u201csweet spot\u201d of this trade-off between the error rate and number of supported languages. In sum, Babel-ImageNet offers a fairly reliable evaluation for a large number of languages, without expensive human annotation, and helps in particular with low(er)-resource languages.\n\nThe correlation with retrieval datasets (Figure 3) and the near-perfect correlation w.r.t. the four existing translated ImageNet variants (Arabic, Chinese, Japanese, and Italian) in Figure 7 (Appendix) further empirically validate our BabelNet-derived translations.\n\n\n> Using WordNet synsets for translations may introduce limitations, as not all concepts or words have direct equivalents in WordNet or BabelNet, potentially impacting the completeness of translations for some languages.\n\n\nCan you clarify this point? ImageNet itself is constructed from WordNet synsets so all classes map to a WordNet synset and BabelNet is constructed also from WordNet so all WordNet synsets also appear in BabelNet. The availability of translations in any particular language, however, is affected by other factors, e.g., is there a WordNet equivalent in a language, how big is the Wikipedia/Wikidata of that language, etc.\n\n> While the paper emphasizes the creation of the benchmark and model evaluation, it could benefit from a deeper analysis of why certain languages perform poorly according to the chosen metrics and explore potential solutions to address these disparities; \n\nWe are somewhat confused by the second point \u2013 exploring solutions to address these disparities -- because our Section 6 is wholly dedicated to this (i.e., improving the multilingual CLIP models\u2019 representational quality for low-resource languages). You have highlighted that as one of the strengths of our work (third bullet point in \u201cStrengths\u201d).\n\nRegarding the analysis point, we perform additional analysis in the Appendix to try and understand why models (do not) perform well for different languages: In D.3., we consider the effect of distillation and in D.4., we look into the effect of the training data language distribution for OpenCLIP models. \n\nUnfortunately, the prohibitive computational expense of training models from scratch ourselves limits us to observational analysis of existing models, so we cannot analyze how the choice of (pre)training data, objective, or initial multilingual text encoder, would affect various languages in controlled experiments.\n\n\n\n\n>Considering that the paper belongs to the \"datasets and benchmarks\" area, the methodology employed (mapping from ImageNet to WordNet and then to BabelNet) is expectedly straightforward. \n\n\nSee our general response to all reviewers. \n\n\n> However, I think there's also some weakness in the data cleaning and validation since the obtained multilingual data is used to evaluate models, but those same benchmarks cannot be used to assess the quality of the data itself;\n\n\nWe have commented on the reliability of BabelNet in the first point above. Also, we have validated (Section 5) the meaningfulness of Image-BabelNet via the (small) set of languages that also exist in retrieval benchmarks, showing substantial correlation as well as for the four languages for which translations of ImageNet exist (Appendix, Figure 7). See also our reply in relation to BabelPic below."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5317/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700052690636,
                "cdate": 1700052690636,
                "tmdate": 1700052690636,
                "mdate": 1700052690636,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oIdxdHNttP",
                "forum": "uLOFyiruin",
                "replyto": "8KegnuKT5r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5317/Reviewer_3yPQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5317/Reviewer_3yPQ"
                ],
                "content": {
                    "comment": {
                        "value": "First, I'd like to thank the authors for their thorough reply.\n\nThey have addressed all my concerns and those raised by other reviewers. Given their answers and the other reviews, I'm increasing my score to 6.\n\nA few comments regarding their answer:\n\n- BabelNet reliance. While that statement holds for English mapping, the quality of the mapping from other languages to English is not of high level at all, especially considering under-resourced languages;\n\n- WordNet limitation. My perspective on this point is that WordNet is orders of magnitude smaller than BabelNet (in number of synsets: ~100k vs ~20M). While I agree that the mapping to BabelNet from WordNet is straightforward, it doesn't imply that it gains access to the whole BabelNet senses. Plus, the intersection between WordNet synsets and Wikipedia pages is only ~40k, so the obtained annotations/samples are limited by this."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5317/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654888485,
                "cdate": 1700654888485,
                "tmdate": 1700654888485,
                "mdate": 1700654888485,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CgAA4ceifX",
            "forum": "uLOFyiruin",
            "replyto": "uLOFyiruin",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5317/Reviewer_rcTe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5317/Reviewer_rcTe"
            ],
            "content": {
                "summary": {
                    "value": "Zero-shot image classification and image-text retrieval evaluation primarily focusses on English only. Curation of high quality evaluation datasets in other languages is expensive and time consuming. This paper proposes e Babel-ImageNet, a massively multilingual\nbenchmark that offers partial translations of 1000 ImageNet labels to 92 languages, built without resorting to machine translation or requiring manual annotation. It leverages the connection between ImageNet classes, which are derived from WordNet synsets, and BabelNet, a massively multilingual lexico-semantic network, also (in part) derived from WordNet. \n\nBabel-ImageNet thus allows us to evaluate models in languages not covered by other evaluation datasets and it additionally expands the retrieval-focused evaluation with the zero-shot image classification task in languages included in the established datasets.\n\nThe paper proposes a computationally efficient approach for improving multilingual CLIP models for low-resource languages. This modular language specialization approach yields large performance gains (>20% for some of the low-resource languages)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "This paper introduces an extensive image-text evaluation benchmark on a large set of languages which motivates research in the largely unexplored multilingual VL representation learning space. Also, the technique is free from any machine translation or similar techniques that can introduce errors in the evaluation data. This makes it more robust and suitable for adoption. This evaluation corpus should be extremely helpful for furthering research in this area."
                },
                "weaknesses": {
                    "value": "None."
                },
                "questions": {
                    "value": "None."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5317/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5317/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5317/Reviewer_rcTe"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5317/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698788164141,
            "cdate": 1698788164141,
            "tmdate": 1699636533333,
            "mdate": 1699636533333,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xXEGbLMwNk",
                "forum": "uLOFyiruin",
                "replyto": "CgAA4ceifX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5317/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5317/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for your review and are happy to read that we convinced you of the merits of our work."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5317/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700052629924,
                "cdate": 1700052629924,
                "tmdate": 1700052629924,
                "mdate": 1700052629924,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "90Fx0BorTV",
            "forum": "uLOFyiruin",
            "replyto": "uLOFyiruin",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5317/Reviewer_3yZg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5317/Reviewer_3yZg"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a robust and machine-translation-free method to create non-English labels for the ImageNet-1k dataset in 92 different languages. When used to evaluate VL models, the new Babel-ImageNet dataset showed score correlated with retrieval performance on multilingual image-text datasets. Finally the paper used the dataset to evaluate models with parameter efficient tuning toward multilingual capability."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed translation method is robust and the claimed error rate from manual inspection is low.\n- The translation covers 92 languages, including many medium and low resource languages.\n- When evaluating multilingual models, the performance on Babel-ImageNet correlates well with the text to image retrieval performance on multilingual image-text datasets, suggesting the usefulness of this dataset as an alternative evaluation method for multilingual models"
                },
                "weaknesses": {
                    "value": "From a significance and usefulness perspective, the unique advantage of this dataset over the multilingual image-text datasets for model evaluation is unclear. It is not surprising that the performance of models on multilingual ImageNet classification is correlated with multilingual text to image retrieval. My concern is that Babel-ImageNet might not be as good as the multilingual image-text datasets as the former contains much less detailed description for the image, and that other image-text datasets support image-to-text retrieval as well for which Babel-ImageNet could not cover.\n\nThe section 6 discussion might be a good opportunity to set up such a comparison if the models there could be evaluated on the multilingual image-text datasets as well. If the authors can show that Babel-ImageNet better reflects the model quality improvement, that would make a strong argument."
                },
                "questions": {
                    "value": "Can you show some cases where Babel-ImageNet has wrong non-English labels? Are there any systematic errors?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5317/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5317/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5317/Reviewer_3yZg"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5317/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698825098925,
            "cdate": 1698825098925,
            "tmdate": 1699636533226,
            "mdate": 1699636533226,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FsnJwngS3a",
                "forum": "uLOFyiruin",
                "replyto": "90Fx0BorTV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5317/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5317/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review!\n> From a significance and usefulness perspective, the unique advantage of this dataset over the multilingual image-text datasets for model evaluation is unclear. It is not surprising that the performance of models on multilingual ImageNet classification is correlated with multilingual text to image retrieval. My concern is that Babel-ImageNet might not be as good as the multilingual image-text datasets as the former contains much less detailed description for the image, and that other image-text datasets support image-to-text retrieval as well for which Babel-ImageNet could not cover.\n\nThe goal of Babel-ImageNet is not to replace existing datasets but rather to complement them and extend evaluation possibilities primarily along the language axis.\n\nLanguages: Coverage of established multilingual image-text retrieval datasets is limited with 7 to 36 languages. Babel-ImageNet allows us to evaluate on substantially more (96 in total) languages and the correlation with retrieval on languages where we have both (retrieval and Babel-ImageNet) suggests that this is a sensible approach.\n\nComplementarity: We agree that image captions are more descriptive, but on the other hand, they are also generally less specific: Any breed of dog is in most captions just denoted \u201cdog\u201d, any type of tree is just a \u201ctree\u201d. ImageNet requires a more fine-grained understanding of objects to differentiate different dog breeds, tree species, etc. As a result, image-text retrieval and classification together make for a more comprehensive evaluation of the representations than either alone. Testing on a range of complementary retrieval and classification datasets/tasks is a standard procedure for English models: with Babel-ImageNet we take a step towards the same for multilingual models.\n\n> The section 6 discussion might be a good opportunity to set up such a comparison if the models there could be evaluated on the multilingual image-text datasets as well. If the authors can show that Babel-ImageNet better reflects the model quality improvement, that would make a strong argument.\n\n\nAs mentioned above, we are not claiming that Babel-ImageNet is better for evaluating models than retrieval datasets for (the limited set of) languages covered by those datasets. But your suggestion illustrates the problem we are aiming to solve: only 6/16 languages used in \u00a76 are included in XM3600\u2019s 36 languages and none of those 6 is one of the low-resource languages where we see improvements. Yes, we could have limited ourselves to the languages also covered by retrieval datasets but with Babel-ImageNet\u2019s language coverage, we had more options for the analysis to cover diverse language families, scripts, levels of pre-training data and distilled/not-distilled.\n\n\n> Can you show some cases where Babel-ImageNet has wrong non-English labels? Are there any systematic errors?\n\n\nWe have not noticed any systematic errors but, while not exactly wrong, sometimes the label for a plant or animal is its scientific name instead of a more common name (e.g., pumpkin -> cucurbita pepo pepo). Filtering out English options that included scientific names removed most such cases, but some instances remained in other languages.\n\nFiltering options can also result in incorrect choices, although rarely. For example, the concept \u201cGolf ball\u201d in BabelNet is erroneously mapped to German to \u201cPhysics of the golf ball\u201d (sourced from Wikipedia page redirects) because the more sensible choice \u201cGolfball\u201d was removed because it matched an English name of the concept (and those we removed not to overestimate the performance of models in other languages on the account of their English competencies, as explained in the last paragraph on page 4)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5317/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700052611867,
                "cdate": 1700052611867,
                "tmdate": 1700052611867,
                "mdate": 1700052611867,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cQqPKvNbKj",
            "forum": "uLOFyiruin",
            "replyto": "uLOFyiruin",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5317/Reviewer_SS5H"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5317/Reviewer_SS5H"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces Babel-ImageNet, a benchmark that translates English ImageNet labels into 92 languages using BabelNet. Furthermore, the paper evaluates eight different publicly available multilingual CLIP models on this benchmark. Experimental results indicate that there is a high correlation between the zero-shot performance of image classification and their performance in image-text retrieval, thereby validating the high quality of Babel-ImageNet."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) The multilingual ImageNet benchmark, which supports 92 languages, serves as an excellent platform for evaluating multilingual CLIP models, particularly for those languages that are under-resourced.\n2) The assessment of eight different multilingual CLIP models also provides valuable insights."
                },
                "weaknesses": {
                    "value": "My concern is about the simplicity of the method, which merely translates English ImageNet labels using BabelNet. While the resulting benchmark proves useful, the method's contribution appears to be limited."
                },
                "questions": {
                    "value": "Have you considered using GPT-4/ChatGPT to prompt the model to translate English ImageNet labels? Perhaps combining GPT-4/ChatGPT with WordNet could yield better results."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5317/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698980331989,
            "cdate": 1698980331989,
            "tmdate": 1699636533123,
            "mdate": 1699636533123,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nPJUfe3m83",
                "forum": "uLOFyiruin",
                "replyto": "cQqPKvNbKj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5317/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5317/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review!\n> My concern is about the simplicity of the method, which merely translates English ImageNet labels using BabelNet. While the resulting benchmark proves useful, the method's contribution appears to be limited\n\nSee our general response to all reviewers. We do not disagree that our creation method is straightforward and simple, but we see do not perceive this as a weakness.\n\n> Have you considered using GPT-4/ChatGPT to prompt the model to translate English ImageNet labels? Perhaps combining GPT-4/ChatGPT with WordNet could yield better results.\n\nWe have not but it is an interesting idea to explore: LLMs are surprisingly good at translations and if we provide them additional context (e.g. via WordNet synonyms or definitions), we could likely reduce the problem of polysemy that designated NMT models struggle with. However, the main problem here is the same as in traditional MT models: performance degrades massively for low-resource languages, which makes human verification necessary again, which is exactly the problem we want to avoid by exploiting BabelNet which is massively multilingual and readily available."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5317/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700052526317,
                "cdate": 1700052526317,
                "tmdate": 1700052526317,
                "mdate": 1700052526317,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]