[
    {
        "title": "Object2Scene: Putting Objects in Context for Open-Vocabulary 3D Detection"
    },
    {
        "review": {
            "id": "XA9sYTixL8",
            "forum": "gXfKPj4s7C",
            "replyto": "gXfKPj4s7C",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3112/Reviewer_ciKG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3112/Reviewer_ciKG"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies open-vocabulary 3D object detection. They propose Object2Scene, the first approach that leverages large-scale large-vocabulary 3D object datasets to augment existing 3D scene datasets for open-vocabulary 3D object detection. Object2Scene inserts objects from different sources into 3D scenes to enrich the vocabulary of 3D scene datasets and generates text descriptions for the newly inserted objects. They further introduce a framework that unifies 3D detection and visual grounding, named L3Det, and propose a cross-domain category-level contrastive learning approach to mitigate the domain gap between 3D objects from different datasets. Extensive experiments on existing open-vocabulary 3D object detection benchmarks show that Object2Scene obtains superior performance over existing methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper is the first paper that leverages large-vocabulary 3D object datasets to augment the existing 3D scene datasets.\n\n- They propose a contrastive learning technique to mitigate the cross-dataset domain gaps.\n\n- They propose several new open vocabulary 3D detection benchmarks.\n\n- Their method outperforms the existing baseline approaches on the benchmarks, which demonstrates the effectiveness of their proposed method.\n\n- The paper writing is clear and easy to follow."
                },
                "weaknesses": {
                    "value": "- The main idea of this paper is essentially an object cut-paste augmentation, which has been exploited as a common practice in most 3D object detectors. For example, starting from Point R-CNN, those 3D object detectors leverage object cut-paste to augment the 3D driving scenes. The proposed object anchor selection, normalization, and placement share very similar spirits with Point R-CNN [1] and Back-to-Reality [2].\n\n- The contrastive learning strategy has also been studied by many relevant papers. For example, CLIP^2 [3] leverages contrastive learning to align the text and point cloud representations for open-vocabulary 3D object detection.\n\n- Text prompts are generated by pre-defined rules and lack generalization ability.\n\n- The proposed detection framework didn't change much compared to the existing detectors. The detector is trained with supervised learning and doesn't rely on open-vocabulary foundation models such as CLIP. Hence it is hard to say whether the proposed method has the open-vocabulary detection ability, as the detection vocabulary is actually constrained by the 3D object datasets. \n\n- Evaluation is also questionable: The authors split the datasets into seen and unseen categories, but those unseen categories are not really ``unseen\", since the 3D object datasets they used can have those categories. Hence it is an unfair comparison with the baseline methods. \n\n- Essentially this paper is not an open-vocabulary 3D detection paper. It is 3D detection with categories augmentation by inserting new objects from 3D object datasets. The detected categories are bounded by the used object datasets.\n\n[1] Shi et al. PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud. CVPR 2019.\n\n[2] Xu et al.  Back to reality: Weakly supervised 3D object detection with shape-guided label enhancement. CVPR 2022.\n\n[3] Zeng et al. CLIP$^2$: Contrastive Language-Image-Point Pretraining from Real-World Point Cloud Data. CVPR 2023."
                },
                "questions": {
                    "value": "Please refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3112/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3112/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3112/Reviewer_ciKG"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3112/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698221585080,
            "cdate": 1698221585080,
            "tmdate": 1699636257912,
            "mdate": 1699636257912,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "daP05gruKH",
                "forum": "gXfKPj4s7C",
                "replyto": "XA9sYTixL8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3112/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3112/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**1. The main idea of this paper is essentially an object cut-paste augmentation, which has been exploited as a common practice in most 3D object detectors. For example, starting from Point R-CNN, those 3D object detectors leverage object cut-paste to augment the 3D driving scenes. The proposed object anchor selection, normalization, and placement share very similar spirits with Point R-CNN [1] and Back-to-Reality [2].**\n\nThanks for your reminder, we clarify the differences with the existing data augmentation methods in the overall rebuttal section and have updated the related discussion in the revised paper.\n\n**2. The contrastive learning strategy has also been studied by many relevant papers. For example, CLIP^2 [3] leverages contrastive learning to align the text and point cloud representations for open-vocabulary 3D object detection.**\n\nContrastive learning itself is a very versatile learning strategy, widely used in a variety of tasks. However, how to design suitable contrastive learning objectives and construct appropriate positive and negative learning samples for different scenarios is an important issue. In our paper, we address the domain gap problem introduced by multi-object dataset training through a carefully designed category-level contrastive learning loss. However, in CLIP^2, contrastive learning is employed to align text and point cloud representations.\n\n**3. Text prompts are generated by pre-defined rules and lack generalization ability.**\n\nOur generated prompts are indeed rule-based and to some extent lack generative ability. However, the purpose of generating prompts is to align the point clouds and texts of objects in scenes, thereby achieving open-vocabulary. Experimental results validate the effectiveness of our prompt generation. Of course, we believe that more complex and natural language descriptions will have better generalization capabilities and bring improved point cloud-text alignment results. We will consider introducing LLM or some other tools in the future to generate more natural prompts for experiments.\n\n**4. The proposed detection framework didn't change much compared to the existing detectors. The detector is trained with supervised learning and doesn't rely on open-vocabulary foundation models such as CLIP. Hence it is hard to say whether the proposed method has the open-vocabulary detection ability, as the detection vocabulary is actually constrained by the 3D object datasets.**\n\nWe offer the related statement in the overall rebuttal section. Thanks for your question.\n\n**5. Evaluation is also questionable: The authors split the datasets into seen and unseen categories, but those unseen categories are not really ``unseen\", since the 3D object datasets they used can have those categories. Hence it is an unfair comparison with the baseline methods.**\n\nWe offer the related statement in the overall rebuttal section.\n\n**6. Essentially this paper is not an open-vocabulary 3D detection paper. It is 3D detection with categories augmentation by inserting new objects from 3D object datasets. The detected categories are bounded by the used object datasets.**\n\nWe restate the definition of open-vocabulary 3D detection in the overall rebuttal section."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3112/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700474503217,
                "cdate": 1700474503217,
                "tmdate": 1700474503217,
                "mdate": 1700474503217,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "eA9WgE4mN6",
            "forum": "gXfKPj4s7C",
            "replyto": "gXfKPj4s7C",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3112/Reviewer_UJPz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3112/Reviewer_UJPz"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies open-vocabulary 3D object detection and proposes object2scene. They propose to use large-scale 3D object datasets and insert objects from different sources into 3D scenes. They also generate text descriptions for the newly inserted objects to distinguish from unseen category objects from the original scene. In obtaining the final open-vocabulary 3D model, they unify 3D detection and visual grounding and propose a cross-domain object-level contrastive learning approach to mitigate the domain gap between 3D objects from different datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The method proposed is also clearly described and easy to understand.\n\n2. The authors try to use existing 3D object datasets and combine them with the scene dataset. Based on this, they unify the 3D object detection task and the visual grounding task to realize the direct implementation of open-vocabulary detection models in 3D space.\n\n3. The experiments are comprehensive, they outperform other open-vocabulary detectors."
                },
                "weaknesses": {
                    "value": "I have reviewed this paper before. Compared to the last version, some presentation problems have been well-addressed in the version and it is now smoother in logic and easier to follow. \n\nPreviously, it was more like a dataset paper, but this time, it looks much better. \n\nMay I ask several questions: \n1. Compared to the last version, the results in Table 1 are slightly dropped, why? \n2. May I know the scale of inserted 3D objects and generated prompts? For example, the number of them? Since contrastive training like GLIP usually requires large-scale objects and prompts.\n3. As mentioned in the last rebuttal, \"we will add and discuss the cut and mix augmentation methods, such PointCutMix, in our final version.\" I could not find the discussion.\n4. Similarly, it is mentioned that \"we will consider using 'spatial proximity' instead of 'affordance' and update in our final paper.\" Do you have any comment about this?"
                },
                "questions": {
                    "value": "No further questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3112/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698317083846,
            "cdate": 1698317083846,
            "tmdate": 1699636257809,
            "mdate": 1699636257809,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0sBgShnU1c",
                "forum": "gXfKPj4s7C",
                "replyto": "eA9WgE4mN6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3112/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3112/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**1. Compared to the last version, the results in Table 1 are slightly dropped, why?**\n\nSince in the former version, there exist the parf of dataset overlap between the test set ScanNet and the training dataset ScanObjectN, so in the last version we change the ScanObjectNN to OmniObject3D, a new large-scale real-scanned 3D object dataetst, which has no overlap with ScanNet.\n\n**2. May I know the scale of inserted 3D objects and generated prompts? For example, the number of them? Since contrastive training like GLIP usually requires large-scale objects and prompts.**\n\nFor OV-ScanNet20 and OV-SUN RGBD20 benchmark , the inserted object datasets cover 6w 3D objects with 190 categories. As for the generated prompts, since they are generated randomly and dynamically, based on our statistics, the actual number of prompts generated during the training process is approximately 500,000.\n\n**3. As mentioned in the last rebuttal, \"we will add and discuss the cut and mix augmentation methods, such PointCutMix, in our final version.\" I could not find the discussion.**\n\nThanks for your reminder, we have added PointCutMix in the related work section in our updated version.\n\n**4. Similarly, it is mentioned that \"we will consider using 'spatial proximity' instead of 'affordance' and update in our final paper.\" Do you have any comment about this?**\n\nActually we have updated most of our descriptions in this ICLR version, thanks for your suggestions in last review. In this version, after double check, the statement about 'affordance' only lies in the \" Specifically, we choose seen objects in the scene as reference objects to guide the physically reasonable 3D object insertion according to their physical affordance. \", and we have revised it in our updated version. Besides, in this version, we clarify the generated prompts into three categories: Vertical Proximity, Horizontal Proximity and Allocentric in Section 3.2, and more details can be found in our supplementary."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3112/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700474195601,
                "cdate": 1700474195601,
                "tmdate": 1700474214014,
                "mdate": 1700474214014,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AxYspVACZa",
                "forum": "gXfKPj4s7C",
                "replyto": "0sBgShnU1c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3112/Reviewer_UJPz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3112/Reviewer_UJPz"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response. I think the authors have addressed my concerns. After going through the responses to other reviewers, I would like to keep my score."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3112/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700475502081,
                "cdate": 1700475502081,
                "tmdate": 1700475502081,
                "mdate": 1700475502081,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6mCp58TvBW",
            "forum": "gXfKPj4s7C",
            "replyto": "gXfKPj4s7C",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3112/Reviewer_sdvC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3112/Reviewer_sdvC"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a technique to improve 3D object detection, by creating additional synthetic data in the form of randomized rooms with language grounding. The paper also contributes a new architecture for the task, which takes text and a pointcloud as input and produces 3D bounding boxes as output. The method also uses a contrastive loss to help reduce inter-class dissimilarity and increase intra-class dissimilarity, which is applied on the features within the decoder of the model."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper makes a convincing case for creating additional diverse data with language grounding labels to train open-vocabulary 3d detectors, and the method presented here to create such data seems like a reasonable one."
                },
                "weaknesses": {
                    "value": "This paper never brings up the highly overlapping work \"RandomRooms: Unsupervised Pre-training from Synthetic Shapes and Randomized Layouts for 3D Object Detection\" from ICCV 2021. That paper covers the exact same idea of placing random objects into random locations to create diverse training rooms. To me it seems critical to discuss this work, and differentiate from it in some concrete ways.\n\nThe paper says \"With the proposed cross-domain category-level contrastive loss, the model is forced to learn generalizable feature representations based on their class labels irrespective of the source dataset of the objects.\" It is unclear to me why the contrastive loss will force generalizable features. I think the high-level idea here is that the contrastive loss will make the features more similar within a class, but this effect is already achieved by the classification loss. Although empirically the contrastive loss may help further still, I don't think the given explanation really works. \n\nSection 3.2 is very difficult to follow. I am lost from the first sentence: \"The 3D object insertion approach composes new 3D scenes with original scenes annotated by seen categories and new objects from large vocabulary categories covering seen and unseen categories.\" I think a comma will be good somewhere, or maybe this sentence can be broken into multiple sentences. The notation <target - class><spatial - relationship><anchor - class> is confusing for me. If these are 2-tuples, what is a \"spatial\" and what is a \"relationship\"? Finally this section states that the \"Absolute Location Prompt\" is proposed as a mechanism to validate the \"Relative Location Prompt\", but it is unclear how this mechanism works. The end of that paragraph is confusing as well, as it seems to say (via double negatives) that the prompts might be less unclear\u00a0than the relative ones (\"such prompts are limited by fewer ways of expression and unclear descriptions which may not be discriminative\")."
                },
                "questions": {
                    "value": "direclty -> directly"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3112/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698818518258,
            "cdate": 1698818518258,
            "tmdate": 1699636257720,
            "mdate": 1699636257720,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jcESXq5BiV",
                "forum": "gXfKPj4s7C",
                "replyto": "6mCp58TvBW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3112/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3112/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**1.This paper never brings up the highly overlapping work \"RandomRooms: Unsupervised Pre-training from Synthetic Shapes and Randomized Layouts for 3D Object Detection\" from ICCV 2021. That paper covers the exact same idea of placing random objects into random locations to create diverse training rooms. To me it seems critical to discuss this work, and differentiate from it in some concrete ways.**\n\nThanks for your reminder! We have added RandomRooms to our related work in our updated version for discussion. Here we want to clarify that the only common point between Object2Scene and RandomRooms may be that they both use 3D object datasets to enhance 3D scene understanding. However, they have significant differences:\n1. Completely different implementation methods: RandomRooms proposes to generate random layouts of a scene and construct the virtual scene using objects from synthetic CAD datasets, learning the 3D scene representation. On the other hand, Object2Scene inserts 3D objects into real-scanned 3D scenes to enhance diversity in existing scenes and further guides detectors with open-vocabulary detection capability through appropriate prompts.\n2. Different problems addressed: RandomRooms aims to pretrain models using generated virtual scenes, which can serve as better initialization for later fine-tuning on 3D object detection tasks. Object2Scene aims to train a model that can be directly used in real-scanned scenes and conduct 3D understanding.\n3. In initial experiments, we attempted training by directly composing virtual scenes using RandomRooms approach. However, due to the significant domain gap between constructed virtual scenes and real-scanned scenes, the detector trained on virtual scenes only achieved poor performance in open-vocabulary detection in real-scanned scenarios. By adopting our proposed Object2Scene, which is a less invasive method, we can avoid the scene-level domain gap and make it more practical.\n\n**2.The paper says \"With the proposed cross-domain category-level contrastive loss, the model is forced to learn generalizable feature representations based on their class labels irrespective of the source dataset of the objects.\" It is unclear to me why the contrastive loss will force generalizable features. I think the high-level idea here is that the contrastive loss will make the features more similar within a class, but this effect is already achieved by the classification loss. Although empirically the contrastive loss may help further still, I don't think the given explanation really works.\"**\n\nThanks for your question, here we show more experiment results to support our claim. Table 4 in our paper demonstrates the effectiveness of our proposed cross-domain category-level contrastive loss. It is shown that cross-domain object-level contrastive learning boosts the mAP25 of open-vocabulary 3D detection from 7.34% to 11.21%.  Besides, we also offer TSNE visualization to support our interpretation. The figure can be found in our updated version.\n\n**3. Section 3.2 is very difficult to follow. I am lost from the first sentence: \"The 3D object insertion approach composes new 3D scenes with original scenes annotated by seen categories and new objects from large vocabulary categories covering seen and unseen categories.\" I think a comma will be good somewhere, or maybe this sentence can be broken into multiple sentences. The notation <target - class><spatial - relationship><anchor - class> is confusing for me. If these are 2-tuples, what is a \"spatial\" and what is a \"relationship\"? Finally this section states that the \"Absolute Location Prompt\" is proposed as a mechanism to validate the \"Relative Location Prompt\", but it is unclear how this mechanism works. The end of that paragraph is confusing as well, as it seems to say (via double negatives) that the prompts might be less unclear than the relative ones (\"such prompts are limited by fewer ways of expression and unclear descriptions which may not be discriminative\").**\n\n1. The first sentence in Setection 3.2 has been updated to \"The 3D object insertion approach creates new 3D scenes by combining original scenes that have been annotated with seen categories, along with new objects from large vocabulary categories that include seen and unseen ones.\" in our uptaded version.\n2. The notation has been updated to \" <target> <spatial relationship> <anchor>\" in our updated version.\n3. As for section states about \"Absolute Location Prompt\", this paragraph has been rewritten to be more clear:\n [Besides, we also propose the Absolute Location Prompt. The construction of this type of prompt relies solely on the object's position in the scene, and not on other objects in the scene. For example: \"a table that is closer to the center/corner/wall of the room\". Compared to the Relative Location Prompt, the prompt is simpler, but it offers a cruder positioning of the object.]"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3112/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700474031074,
                "cdate": 1700474031074,
                "tmdate": 1700474062037,
                "mdate": 1700474062037,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xgNPd4HCAf",
            "forum": "gXfKPj4s7C",
            "replyto": "gXfKPj4s7C",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3112/Reviewer_XyDn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3112/Reviewer_XyDn"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces Object2Secene, a method that samples objects from large-scale 3D object datasets and integrates them into 3D scenes. This process is used to train open-vocabulary 3D object detection models. The authors generate language grounding prompts to facilitate the learning of inserted objects. They also implement a cross-domain category-level contrastive learning to mitigate the domain gap between inserted 3D objects and 3D scenes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Object2scene enhances the vocabulary diversity of objects in original 3D scene data by inserting extra 3D objects, which is straightforward and does not rely on large-scale image-text datasets or pre-trained 2D open-vocabulary detection models. \n2. Their proposed method significantly outperforms previous methods."
                },
                "weaknesses": {
                    "value": "1.\tDo the inserted object classes fully contain the unseen classes in the test set? If the unseen classes are fully covered during training, does this still counts as open vocabulary 3D object detection. Moreover, if these unseen classes are not included during object insertion, how well would the trained model perform on the unseen classes?\n2.\tWhile using existing 3D object datasets can provide annotated objects across more categories for model training, the training process may still be limited by the vocabulary of these datasets. Learn semantics beyond these datasets\u2019 object categories pose a challenge for the model.\n3.\tThe \u201cObject Placement\u201d process is central to Object2Scene but needs further clarification. How are injection regions determined? How is physical reasonability checked during placement? A pseudo code on this process could enhance understanding. \n4.\tLack of comparison on the larger OV-ScanNet200 benchmark."
                },
                "questions": {
                    "value": "Please see the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3112/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3112/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3112/Reviewer_XyDn"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3112/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698829023585,
            "cdate": 1698829023585,
            "tmdate": 1699636257613,
            "mdate": 1699636257613,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7KuaUcwKug",
                "forum": "gXfKPj4s7C",
                "replyto": "xgNPd4HCAf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3112/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3112/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**1. Do the inserted object classes fully contain the unseen classes in the test set? If the unseen classes are fully covered during training, does this still counts as open vocabulary 3D object detection. Moreover, if these unseen classes are not included during object insertion, how well would the trained model perform on the unseen classes?**\n\nIn our OV-ScanNet20 and OV-RGB-D20 benchmarks, inserted object classes fully contain the unseen classes in the test set, but not in OV-ScanNet200 benchmark.  If the unseen classes are fully covered during training, this still counts as open vocabulary 3D object detection according to the definition of open-vocabulary object detection stated in overall rebuttal statement, and here we offer the part of classes which are not included in training datasets: \n|  | copier | Computer tower | ottoman | soap  |\n| --- | --- | --- | --- | --- |\n| L3Det | 0.342 | 0.12 | 1.23 | 0.00 |\n\n**2.While using existing 3D object datasets can provide annotated objects across more categories for model training, the training process may still be limited by the vocabulary of these datasets. Learn semantics beyond these datasets\u2019 object categories pose a challenge for the model.**\n\nYes, it's true that learning semantics beyond these datasets\u2019 object categories poses a challenge for the model during the training. However, once the number of object categories included in the training data is large enough, like Detic using ImageNet21K datasets to broaden the object concept, then the model's open-vocabulary capability can cover most objects in daily scenes.\n\n**3.The \u201cObject Placement\u201d process is central to Object2Scene but needs further clarification. How are injection regions determined? How is physical reasonability checked during placement? A pseudo code on this process could enhance understanding.**\n\nThanks for your advice, we have added this pseudo code in the appendix of our updated paper for reference.\n\n**4.Lack of comparison on the larger OV-ScanNet200 benchmark.**\n\n|  | head |  common | tail |\n| --- | --- | --- | --- |\n| OV-PointCLIP | 2.76 | 0.56 | 0.0 |\n| Detic-ModelNet | 1.56 | 0.12 | 0.0 |\n| L3Det | 13.1 | 10.1 | 3.4 |"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3112/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700473863137,
                "cdate": 1700473863137,
                "tmdate": 1700473863137,
                "mdate": 1700473863137,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bAYYoTEKvL",
                "forum": "gXfKPj4s7C",
                "replyto": "7KuaUcwKug",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3112/Reviewer_XyDn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3112/Reviewer_XyDn"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for addressing my questions. While it can be expected that augmenting objects in 3D scenes could improve detection performance for related categories, this improvement may not be significant enough to warrant substantial progress in open-vocabulary object detection specifically. Hence, I prefer to keep my initial score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3112/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700649711595,
                "cdate": 1700649711595,
                "tmdate": 1700649711595,
                "mdate": 1700649711595,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]