[
    {
        "title": "Traveling Words: A Geometric Interpretation of Transformers"
    },
    {
        "review": {
            "id": "ceFIb9KHpa",
            "forum": "cSSHiLnjsJ",
            "replyto": "cSSHiLnjsJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission986/Reviewer_zeL2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission986/Reviewer_zeL2"
            ],
            "content": {
                "summary": {
                    "value": "This is a typical outlier paper. The submission firstly proposes an interpretation that the layer normalzation operation projects an embedding to a subspace perpendicular to [1,1,...,1] then normalizes it to a hypersphere in the subspace. Then the weight matrices in a transformer shift the projected hyperspere's position and shapes. Then the authors probe the embeddings of common words using cosine similarity. The conclusions are: (1) early layers slightly shift the embeddings to their typical contexts; (2) late layers are not understandable; (3) one case shows that the shifted embeddings are closer to the last word in a sentence."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ This is a creative and interesting *OUTLIER* paper. Some of these papers can be quite influential.\n+ I am a computer vision person and as far as I know the geometric interpretation of LayerNorm is new."
                },
                "weaknesses": {
                    "value": "- The experiments are only case studies in a small scale. And I cannot say the conclusions are meaningful or not. What's worse, some analysis does not lead to conclusions, e.g., later layer embeddings are not understandable. \n- To be honest, I fail to get why the word travelling analysis is related to the geometric interpretation of LayerNorm. We can still do these analyses without the d-1 projection interpretation right? Correct me if I am wrong."
                },
                "questions": {
                    "value": "See the last box."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission986/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698777116797,
            "cdate": 1698777116797,
            "tmdate": 1699636024657,
            "mdate": 1699636024657,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "neS7TL13gB",
            "forum": "cSSHiLnjsJ",
            "replyto": "cSSHiLnjsJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission986/Reviewer_qGqz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission986/Reviewer_qGqz"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores geometric views into some of the primitive operations in Transformer architectures. The starting point is layer normalization which projects and normalizes a vector so that it will lie on a hypersphere. Then the query-key matrix implements an affine transformation bringing closer on the hypersphere terms that are related. The value-output matrix can be seen as an additional key-value store in the attention layer, while the computation of output probabilities reflects the similarity of the final layer represention as projected on the (shared) hypersphere with the embedding vectors in the vocabulary. \n\n(Probing) experiments quantify these observations using a pre-trained GPT-2 model. The impact of layer normalization on word embeddings is verified, query-key transformations seem to exhibit interpretable patterns at the first layer and some key-value heads work together at the last layer to preserve input key meanings. Similarly some patterns could be captured in singular vector pairs of the key-value and query-key matrices (the latter at layer 0). This work concludes with the representation of the second to last token in a sentence which can be seen that gets closer and closer to the last token representation (in a projected view) as the layers of the network model are traversed."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This is an easy to follow paper, with interesting and intuitive geometric arguments, supported by simple matrix formulas.\n\n- Some of the examples/demonstrations reveal patterns which tend to happen either on early or deep layers and could loosely fit into the high-level geometric insights developed."
                },
                "weaknesses": {
                    "value": "- The novelty of this work is limited since the key observations have already been mentioned in other works (that are adequately cited): [Brody et al.] for layer normalization, query-key matrix; [Millidge & Black] for value-output matrix.\n\n- The \"journey\" of the representation of one word towards the representation of the next one in a sentence is interesting but is could well be an artifact of the reduction in dimensionality in the projection as also noted. Regarding the examples that are expected to frame the geometric arguments presented, they are either very slim in volume to be conclusive (when some signal/pattern is observed) or there is simply no interesting pattern that could be easily extracted."
                },
                "questions": {
                    "value": "- Regarding the traveling words interpretation: It would be nice to test it with more sentences and alternatively work with the original encoding vectors through the layers (no reduction: do the distances to the last token representation decrease? what about the respective distances for successive tokens not  at the end of a sentence? is there a intuitive way to argue for a possible pattern in this?)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission986/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698825376458,
            "cdate": 1698825376458,
            "tmdate": 1699636024567,
            "mdate": 1699636024567,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "ZPlGNmo1W5",
            "forum": "cSSHiLnjsJ",
            "replyto": "cSSHiLnjsJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission986/Reviewer_TdFy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission986/Reviewer_TdFy"
            ],
            "content": {
                "summary": {
                    "value": "The paper aims to interpret the mechanisms of transformers and establishes an explanation for the effect of layer normalization from a geometric viewpoint. The interpretation is validated via probing a GPT-2 model."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The paper provides an intuitive, geometry perspective for interpreting the Transformer architecture. \n* Empirical probing experiments on GPT-2 validated some claims in the paper."
                },
                "weaknesses": {
                    "value": "* Some ideas discussed in the paper, such as interpreting LayerNorm as surface projection have been discussed in prior works and are not novel. A discussion on the novelty of the proposed paper and how it compares with prior works will help clarify this concern. \n* The paper provides an interesting perspective on the specific architecture in popular implementations of Transformers, but its applications or insights for further results are not fully discussed in the paper."
                },
                "questions": {
                    "value": "* Figure 1 and Figure 4 suggest that work particles travel along the path determined by residual updates, but such a description is very general. Are there more specific properties within the residual updates?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission986/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698841975640,
            "cdate": 1698841975640,
            "tmdate": 1699636024498,
            "mdate": 1699636024498,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "9gzbhgbZvZ",
            "forum": "cSSHiLnjsJ",
            "replyto": "cSSHiLnjsJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission986/Reviewer_rASQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission986/Reviewer_rASQ"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors introduce a novel geometric perspective to shed light on the inner workings of transformers. Their main contribution is the findings on how layer normalization constrains the latent features of transformers to a hyper-sphere, which in turn allows attention mechanisms to shape the semantic representation of words on this surface. This geometric viewpoint connects various established properties of transformers, including iterative refinement and contextual embeddings. To validate their insights, the authors analyze a pre-trained GPT-2 model with 124 million parameters. Their findings unveil distinct query-key attention patterns in early layers and confirm prior observations about the specialization of attention heads in deeper layers. By leveraging these geometric insights, the paper offers an intuitive understanding of transformers, depicting iterative refinement as a process that models the trajectory of word particles along the surface of a hyper-sphere."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper presents a study on understanding transformers through the lens of layer normalization, a key component in transformers, and the matrices $W_{QK}, W_{VO}$ used in the attention mechanism.\n\n\n2. The main insights are that in each layer, the layer normalization projects the features to a shared hyper-sphere. The proposed interpretation of attention is similar to the feed-forward module by Geva et al. (2021) in that both calculate relevance scores and aggregate sub-updates for the residual stream. However, the key difference lies in how scores and updates are computed: attention relies on dynamic context, while the feed-forward module depends on static representations.\n\n3. The authors validate these insights by probing a pre-trained 124M parameter GPT-2 mode"
                },
                "weaknesses": {
                    "value": "1. The presentation can be improved significantly. I find it hard to see the differences from prior works and what exactly are the main contributions of this paper. \n\n2. Most of the emprical results are using some selected examples and I do not quite follow these results. Could you list the main points that you are making from these experiments and how the evidence justifies them?  What is the trajectory in figure 4 trying to show?"
                },
                "questions": {
                    "value": "In A.1  $\\mu$? is the average of components of a feature vector $x_i$? Can you provide clear definitions of what the features, mean, and std. deviation are? I would imagine $\\mathbf{\\mu}$ to be either an expectation of $\\mathbf{x}$ or an average of $\\mathbf{x}_i$."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission986/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699138075954,
            "cdate": 1699138075954,
            "tmdate": 1699636024416,
            "mdate": 1699636024416,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]