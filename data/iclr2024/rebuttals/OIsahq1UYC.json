[
    {
        "title": "Diffusion Generative Flow Samplers: Improving learning signals through partial trajectory optimization"
    },
    {
        "review": {
            "id": "yRB7P3v0uI",
            "forum": "OIsahq1UYC",
            "replyto": "OIsahq1UYC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5779/Reviewer_hxZf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5779/Reviewer_hxZf"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces Diffusion Generative Flow Samplers (DGFS), a neural-based sampling algorithm that utilizes diffusion models. DGFS draws heavy inspiration from GFlowNets (typically constrained to discrete data spaces) and offers a practical and scalable solution for sampling in continuous spaces. The primary message conveyed by this paper is that DGFS demonstrates the ability to leverage partial trajectory segments, thereby enabling a more efficient approach to the learning problem when compared to existing methods, which require learning over complete trajectories (from start to finish), as seen in denoising diffusion samplers (DDS). Consequently, DGFS can sample from target unnormalized probability distributions by updating its parameters during a time-dependent stochastic training process. In other words, it can do so without requiring a full trajectory specification and can accommodate intermediate signals injected between time steps before the sampling process is complete. To accomplish this, a neural network denoted as $F_n(\\theta)$ is trained to approximate the unnormalized density of the $n-th$ step.\n\nThe authors argue that DGFS should lead to more stable training, reduce variance in gradient computations, and ultimately provide access to informative intermediate training signals."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well-written, scientifically sound, and highly rigorous, featuring a compelling theoretical framework. The main text is effectively complemented by the appendix materials, which offer additional mathematical details and experimental results.\n\n- I particularly appreciate the table that discusses notation on the first page of the appendix. I believe this should be considered standard practice, as it greatly aids readers in navigating the mathematical content.\n\n- The paper extends the theory of GFlowNets to address sampling problems in continuous space. This extension holds significant relevance for numerous applications, especially in the field of physical and chemical sciences, where unnormalized target densities in continuous space are frequently encountered.\n\n- The concepts of leveraging information from partial trajectory paths and training intermediate layers offer benefits from both theoretical and practical standpoints."
                },
                "weaknesses": {
                    "value": "- **Limited Experiments:** I consider the experiments presented in the paper to be its primary weakness. While the theory is well-presented and comprehensible, the experiments reported in the paper appear to fall short in terms of comparison with more sophisticated baseline methods, particularly in the context of flow-based samplers.\n\n- **Related Works:** Expanding the related work section to include discussions of prior research addressing similar problems, such as gradient variance reduction and efficient sampling of multimodal densities [1, 2, 3], would add significant value. More details are provided below."
                },
                "questions": {
                    "value": "- On the bottom of page 5, the authors state, \"On the other hand, notice that our objectives in Equation 9 or Equation 14 do not require the training samples to follow any particular distribution (only have full support).\" While I believe that in the asymptotic regime of infinitely many samples this holds, it may not be true for practical cases with a finite number of samples. In such cases, amortized variational inference schemes might encounter issues, as discussed in recent works [4]. I'm interested in whether this assumption can be relaxed or if the authors can provide arguments for the general validity of the 'full support' assumption in their case.\n\n- In the discussion about Variance Gradient Updates (section 3.3A), the authors cite works like Roederer et al. (2017), which reported that the variance of the gradient does not necessarily vanish even when the optimal distribution is achieved. However, in Refs. [1,2], authors use information-theoretical arguments to demonstrate that the variance can indeed approach zero when the target distribution is perfectly learned, using the so-called path gradient method for normalizing flows. It would be interesting to see a comparison of DGFS with such a method (specifically designed for continuous normalizing flows in [2]). Additionally, given the discussion in Sec. 3.3A, it would be beneficial to include these references and expand the discussion.\n\n- The concept of injecting information between a sequence of transformations strongly reminds me of simulated annealing and annealed importance sampling (as seen in Stochastic Normalizing Flow [4,5]). Could the authors expand on this, highlighting the significant similarities and differences, if any? There appears to be a notable overlap worth exploring.\n\n- I'm interested in seeing how the lower bound estimator for the log partition function used in this work compares to the asymptotically unbiased estimator for the partition function proposed in [7,8].\n\n- As pointed out in the **Weaknesses** section, I'm curious about how the results for the many-well task compare to [3], which has reported notable results in data-free training for multimodal target densities for quantum chemical tasks as well as the many-well problem. \n\n**Side remark**: While I acknowledge the significant contribution and potential impact of the paper, I believe additional robust experimental evidence and a broader discussion, as outlined above, would strengthen its acceptance. Adding new baselines, particularly [2,3], to the comparison in Figure 2 and Table 1 would be a valuable addition to the paper.\n\n**Minor:**\n\n- Page 2: below eq (4) the authors state: \"It can be shown that the marginal distribution of P at the terminal time N is exactly proportional to the target \u03bc(\u00b7).\". While it might be self-evident to some, it might be helpful to provide a hint for an explicit derivation in the appendix.\n\n- Page 3: Above eq (7), the authors mention, \"[\u2026] stochastic optimal control formulation seen in prior works.\" It would be beneficial to include explicit references to these prior works.\n\n- Page 7: to the best of my knowledge, what the authors call \"Hamilton Monte Carlo\" is more often found in the literature as \"Hamiltonian Monte Carlo\".\n\n- Page 7: In the related work section, the authors might consider citing some of the references listed below, being relevant within the context of NF-based samplers. Notably, some of these references have shown how to derive an asymptotically unbiased estimator for the normalizing constant [7,8,9], also known as the partition function in the realm of Boltzmann distributions. This is a quantity of interest for the present work as well.\n\n**References:**\n\n- [1] [Vaitl, Lorenz, et al. \"Gradients should stay on path: better estimators of the reverse-and forward KL divergence for normalizing flows.\"\u00a0Machine Learning: Science and Technology\u00a03.4 (2022): 045006](https://iopscience.iop.org/article/10.1088/2632-2153/ac9455/pdf)\n- [2] [Vaitl, Lorenz, et al. \"Path-gradient estimators for continuous normalizing flows.\"\u00a0International Conference on Machine Learning. PMLR, 2022.](https://proceedings.mlr.press/v162/vaitl22a/vaitl22a.pdf)\n- [3] [Midgley, Laurence Illing, et al. \"Flow annealed importance sampling bootstrap.\"\u00a0arXiv preprint arXiv:2208.01893\u00a0(2022).](https://arxiv.org/pdf/2208.01893)\n- [4] [Wu, Hao, Jonas K\u00f6hler, and Frank No\u00e9. \"Stochastic normalizing flows.\"\u00a0Advances in Neural Information Processing Systems\u00a033 (2020): 5933-5944.](https://proceedings.neurips.cc/paper/2020/hash/41d80bfc327ef980528426fc810a6d7a-Abstract.html)\n- [5] [Caselle, Michele, et al. \"Stochastic normalizing flows as non-equilibrium transformations.\"\u00a0Journal of High Energy Physics\u00a02022.7 (2022): 1-31.](https://arxiv.org/pdf/2201.08862.pdf)\n- [6] [Nicoli, Kim A., et al. \"Detecting and Mitigating Mode-Collapse for Flow-based Sampling of Lattice Field Theories.\"\u00a0arXiv preprint arXiv:2302.14082\u00a0(2023).](https://arxiv.org/pdf/2302.14082)\n- [7] [Nicoli, Kim A., et al. \"Asymptotically unbiased estimation of physical observables with neural samplers.\"\u00a0Physical Review E\u00a0101.2 (2020): 023304.](https://link.aps.org/accepted/10.1103/PhysRevE.101.023304)\n- [8] [Wirnsberger, Peter, et al. \"Targeted free energy estimation via learned mappings.\"\u00a0The Journal of Chemical Physics\u00a0153.14 (2020).](https://pubs.aip.org/aip/jcp/article/153/14/144112/316574)\n- [9] [Nicoli, Kim A., et al. \"Estimation of thermodynamic observables in lattice field theories with deep generative models.\"\u00a0Physical review letters\u00a0126.3 (2021): 032001.](https://link.aps.org/pdf/10.1103/PhysRevLett.126.032001)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5779/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5779/Reviewer_hxZf",
                        "ICLR.cc/2024/Conference/Submission5779/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5779/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698161928468,
            "cdate": 1698161928468,
            "tmdate": 1700666199585,
            "mdate": 1700666199585,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8ZDUzJWYtC",
                "forum": "OIsahq1UYC",
                "replyto": "yRB7P3v0uI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5779/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5779/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful review. We have updated the writing of our submission and we sincerely hope the following will answer your points and further improve the score. \n\n**Regarding experiment limitation.** We understand the reviewer's point about the limitations in our experiments. To address this, we have carried out further experiments, which we hope will answer the reviewer's concerns. The details are provided below.\n\n\n**Regarding related works.** We appreciate the reviewer's suggestion to include additional references. We have read these papers and agree that they are very related to our paper, therefore we have added a discussion about prior works [1-3, 5-9] in the related work section in the updated draft. We believe these additions have enriched our writing by providing more descriptions especially on normalizing flow based methods. \n\n\n> On the bottom of page 5, the authors state, \"On the other hand, notice that our objectives in Equation 9 or Equation 14 do not require the training samples to follow any particular distribution (only have full support).\" While I believe that in the asymptotic regime of infinitely many samples this holds, it may not be true for practical cases with a finite number of samples. In such cases, amortized variational inference schemes might encounter issues, as discussed in recent works [4]. I'm interested in whether this assumption can be relaxed or if the authors can provide arguments for the general validity of the 'full support' assumption in their case.\n\nYour statement is grounded in prior GFlowNet theory [10, 11]. These theories propose that when a GFlowNet achieves zero training loss across the board, it is assured to sample objects with probability proportional to the given reward (density) function. In essence, the learned probabilistic model is capable of generating samples that perfectly align with the target density. For additional details, we direct you to Section 3.2 in [10] and Section 3.4 in [11]. In our paper, we are actually making the assumption that, given a neural network with infinite capacity and training data encompassing the entire space, the network can be trained to achieve zero training loss. As per GFlowNet theory, under the conditions that the GFlowNet model possesses sufficient capacity and the training trajectories are fully supportive, the GFlowNet can be accurately trained to sample. However, in practical scenarios, both two assumptions are not fully met: neural network architectures have limited expressiveness, and training data coverage is incomplete. When these assumptions are relaxed, it is no longer certain that our model will still learn to sample correctly. In real-world applications, our aim is to update the model efficiently with limited training data. A key insight, therefore, is to ensure that the training trajectories cover significant modes of the target distribution, reflecting the most important aspects despite data limitations.\n\n\n> In the discussion about Variance Gradient Updates (section 3.3A), the authors cite works like Roederer et al. (2017), which reported that the variance of the gradient does not necessarily vanish even when the optimal distribution is achieved. However, in Refs. [1,2], authors use information-theoretical arguments to demonstrate that the variance can indeed approach zero when the target distribution is perfectly learned, using the so-called path gradient method for normalizing flows. \n> It would be interesting to see a comparison of DGFS with such a method (specifically designed for continuous normalizing flows in [2]). \n> Additionally, given the discussion in Sec. 3.3A, it would be beneficial to include these references and expand the discussion.\n\nThank you for bringing up the important work of [1,2], which we have indeed missed in the initial writing. Similar to the analysis in Roeder et al. (2017), [1,2] propose to decompose the total derivative into the summation of a path-gradient term and a score term, and only conisder the path-gradient term when calculating loss objectives. The path-gradient term indeed has zero gradient variance at the optimal solution. We think this is very related to our work, thus we have added reference and discussion in the updated draft.\n\nWe agree with the reviewer's suggestion for doing experimental comparison, so we implement the path-gradient version of neural ODE algorithm in [2] with PyTorch. We train the neural ODE to learn to sample from the unnormalized densities in the benchmark in our submission. On the manywell task, this method obtains a log normalizing factor bias of 2.577, which is outperformed by DGFS. We also find it challenging to scale to hard tasks. We are performing more experiments on this algorithm and will add this baseline into the final version of our work. For completeness we provide our anonymous implementation in [this](https://file.io/vEQo2YFry6vv) link."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5779/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700187146511,
                "cdate": 1700187146511,
                "tmdate": 1700187146511,
                "mdate": 1700187146511,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iLX4ABn1FB",
                "forum": "OIsahq1UYC",
                "replyto": "yRB7P3v0uI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5779/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5779/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors, 2nd part"
                    },
                    "comment": {
                        "value": "> The concept of injecting information between a sequence of transformations strongly reminds me of simulated annealing and annealed importance sampling (as seen in Stochastic Normalizing Flow [4,5]). Could the authors expand on this, highlighting the significant similarities and differences, if any? There appears to be a notable overlap worth exploring.\n\nWe agree that modeling complex distribution through a sequence of transformations is an old yet powerful idea. Representative works include annealed importance sampling [12] and sequential Monte Carlo sampler [13], followed by many learning augmented variants [4,5]. These methods explicitly define a series of densities by linearly interpolating between a reference distribution (usually a standard Gaussian) and the target density in the log probability scale. These methods then design different kinds of transformations between these intermediate level; these transformations are usually complex and are based on MCMC kernels, sometimes involving normalizing flows and rejection-acceptance steps. On the other hand, in our work and other diffusion-based sampling methods, we do not explicitly define the densities in the intermediate level. Instead, we define clear conditional probability for the transition between adajacent levels. In our case, they are all simple Gaussian distributions. We are thus not able to know the exact density functions in the intermediate levels, and thus we use an amortized way to approximate them (Eq. 11-12). In summary, the former kind of modeling explicitly define the intermediate densities and implicitly define the transformations, while the latter modeling does the opposite.\n\n\n\n> I'm interested in seeing how the lower bound estimator for the log partition function used in this work compares to the asymptotically unbiased estimator for the partition function proposed in [7,8].\n\nThank you for pointing out references [7, 8], which we have now included in our revised draft. The asymptotically unbiased estimator in these works relates to Eq. 17 in the DGFS paper, in that both utilize importance sampling to estimate the normalizing factor. In [7, 8], the proposal is a learned normalizing flow. In our work, the proposal is the forward process \n$\\mathcal{Q}$ described in Eq. 1, and our aim is to calculate the integration of the target process $\\mathcal{P}$, which has the same value to the integration of the unnormalized density function. We have incorporated a discussion about these papers in Section 3.2 of our updated draft.\n\n\n\n> As pointed out in the Weaknesses section, I'm curious about how the results for the many-well task compare to [3], which has reported notable results in data-free training for multimodal target densities for quantum chemical tasks as well as the many-well problem.\n \nThank you for bringing up this work. We do have include this work in our original writing, and we point to Appendix C.5 for our results.\n\n\n\n\n**Regarding minor points.** Thank you for pointing out these points. We have fixed them in the revision.\n\n\n\n[1] Gradients should stay on path: better estimators of the reverse-and forward KL divergence for normalizing flows\n\n[2] Path-gradient estimators for continuous normalizing flows\n\n[3] Flow annealed importance sampling bootstrap\n\n[4] Stochastic normalizing flows\n\n[5] Stochastic normalizing flows as non-equilibrium transformations\n\n[6] Detecting and Mitigating Mode-Collapse for Flow-based Sampling of \nLattice Field Theories\n\n[7] Asymptotically unbiased estimation of physical observables with neural samplers\n\n[8] Targeted free energy estimation via learned mappings\n\n[9] Estimation of thermodynamic observables in lattice field theories with deep generative models\n\n[10] GFlowNet Foundations\n\n[11] A theory of continuous generative flow networks\n\n[12] Annealed importance sampling\n\n[13] Sequential Monte Carlo sampler\n\n\nThank you for your constructive review. Your suggestions have been instrumental in refining our paper, and we hope that our response can increase your score evaluation of our work. Please let us know if there is any additional thing we can do to improve your evaluation of our work."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5779/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700187175034,
                "cdate": 1700187175034,
                "tmdate": 1700187175034,
                "mdate": 1700187175034,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CPiHwrdlKP",
                "forum": "OIsahq1UYC",
                "replyto": "yRB7P3v0uI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5779/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5779/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your response"
                    },
                    "comment": {
                        "value": "Dear reviewer hxZf,\n\nThanks again for your detailed comments. We hope our responses have addressed your concerns, and thus request for a reconsideration of the score. We would appreciate it if we can get your further feedback at your earliest convenience."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5779/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700493581093,
                "cdate": 1700493581093,
                "tmdate": 1700493581093,
                "mdate": 1700493581093,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FTqQeTOVId",
                "forum": "OIsahq1UYC",
                "replyto": "yRB7P3v0uI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5779/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5779/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Request for an update of reviews for our submission"
                    },
                    "comment": {
                        "value": "Dear reviewer hxZf,\n\nWe thank for your effort for reviewing our paper. We have demonstrated the benefit of our method and the improvement upon other methods. Since the deadline of discussion period is about to end, and other reviewers have given their feedbacks, we are very expectant to see your re-evaluation of our latest replies.\n\nThank you very much!"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5779/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700574812366,
                "cdate": 1700574812366,
                "tmdate": 1700574812366,
                "mdate": 1700574812366,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YqnBcBwrhz",
                "forum": "OIsahq1UYC",
                "replyto": "yRB7P3v0uI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5779/Reviewer_hxZf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5779/Reviewer_hxZf"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Dear Authors, \n\nMy apologies for the delayed reply. I unfortunately got very sick since last week, and I haven't been able to carefully re-read your manuscript and your replies until now. \n\nI am sorry for having let you wait. \n\n**Feedback on the revised version**\n\n\nI appreciate the great effort the authors have put into re-writing part of the paper to enhance clarity and add further ablation studies and experiments. Most of my concerns have been addressed (I still have to wrap my head around some concepts though I believe this does not depend on the paper itself). For this reason, I am happy to increase my score. In particular, I find the broader discussion of related works very useful, as it would help the general audience to contextualize the present work. \n\nI believe the extensive analysis and efforts in re-implementing the path-gradient estimator from Vaitl et al. will substantially improve the clarity of the paper in comparison to existing methods. I look forward to seeing the extended analysis in the final version of this work."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5779/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666119418,
                "cdate": 1700666119418,
                "tmdate": 1700666182651,
                "mdate": 1700666182651,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xXdH35OA4K",
            "forum": "OIsahq1UYC",
            "replyto": "OIsahq1UYC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5779/Reviewer_Tb6D"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5779/Reviewer_Tb6D"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers the problem of sampling from a distribution given its unnormalized density. Recent work proposes training a diffusion process sampler to match the target distribution. At present such methods are trained by minimizing the Kullback-Leibler (KL) divergence computed using terminal states of the diffusion process as samples. As a result, complete trajectories need to be sampled during training, while only the terminal states get direct learning signal. In this paper, authors propose learning an additional neural net to provide learning signal for intermediate steps. Authors note that the resulting objective matches the detailed balance objective in GFlowNet literature, and tap into this literature to further improve the learning objective. Authors evaluate the method on several target distributions, showing improved performance compared to common baselines, including existing diffusion-based samplers (PIS/DDS)."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Significance: diffusion models are state of the art in generative modelling, where we can train them efficiently using the step-wise de-noising objective. Finding similarly effective objectives for training diffusion models as samplers is an important research direction.\n\nMotivation/soundness: the method is well-motivated and theoretically sound. Authors start with a clear problem with existing diffusion-based samplers (weak training signal at intermediate steps), and propose to define intermediate targets for the diffusion process using a neural network trained in parallel. The learning objective for the additional network neatly avoids having to estimate the integral in Eq. (11). The developed connection with GFlowNets allows the method to benefit from developments in this literature.\n\nResults: authors evaluate the propose method on several benchmarks, including the high-dimensional Cox distribution. A good selection of baselines is used, including MCMC, methods based on normalizing flows, and diffusion-based methods. The proposed method demonstrates improved performance across the board, as well as less variance in its results when compared to existing diffusion-based methods.\n\nQuality: the write-up is of high quality, with only a few minor typos, clean mathematical notation, and high-quality figures."
                },
                "weaknesses": {
                    "value": "Originality/novelty: continuous GFlowNets and their connection to diffusion models have been explored by Lahlou et al. (2023) and Zhang et al. (2022a/2023b). The same authors have explored training diffusion models using alternative consistency-based objectives. While the setting in this paper is different (other work considers generative modeling, not sampling), it raises questions about the novelty of the method. Additional discussion with crisp comparisons to Lahlou et al./Zhang et al. would help.\n\nSignificance of the GFlowNet connection: Figure 10 in the appendix suggests that the effect of the forward-looking trick is relatively minor. It would be useful to see the effect of using Eqs. (14-15) instead of the originally proposed objective in Eq. (12). In other words, the practical effect of the GFlowNet connection (and associated improvements) is not clear from the results. \n\nClarity: Sections 2 and 3 are difficult to parse in places. The GFlowNet description in Section 2.2 is extremely dense. The separation of \"learning from intermediate steps\" vs. \"learning with incomplete trajectories\" in Section 3 is confusing. More generally, Section 3 could be re-structured to separate proposed methods, existing methods, and discussion more clearly. Section 4 is dense, with a lot of the references already introduced earlier in the paper."
                },
                "questions": {
                    "value": "At the end of page 5 authors say that the method does not \"require the training sample to follow any particular distribution (only to have full support)\". Have authors measured the effect of choosing the sampling policy? Is it worthwhile to try to explore intelligently?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5779/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5779/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5779/Reviewer_Tb6D"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5779/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698763270035,
            "cdate": 1698763270035,
            "tmdate": 1700575354512,
            "mdate": 1700575354512,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PsSFXQ7DFT",
                "forum": "OIsahq1UYC",
                "replyto": "xXdH35OA4K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5779/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5779/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful review. We have updated the writing of our submission and we hope the following will answer your points. \n\n> Additional discussion with crisp comparisons to Lahlou et al. [2] / Zhang et al. [1] would help.\n\nThank you for suggesting a discussion on related works. We clarify that [2] is an important precursor to DGFS, as it first established a sound theoretical framework for continuous state space GFlowNets and conducted an initial numerical study on sampling simple densities. DGFS extend on [2] and propose more scalable algorithm that obtain better performance in challenging benchmarks. [1] is another important prior work that initially established the connection between GFlowNets and diffusion models, a foundational concept that [2] also relies upon. [1] conducts experiments in the generative modeling setup with given datasets, training diffusion models using GFlowNet-inspired consistency-based methods, but it does not experiment with sampling given unnormalized densities like [2] and this work. \nWe put high value on thorough discussion with prior works, and we have update our introduction section and related work section accordingly to clarify these connections.\n\n[1] Unifying generative models with GFlowNets. (\"Zhang et al. (2022)\")\n\n[2] A theory of continuous generative flow networks. (\"Lahlou et al. (2023)\")\n\n\n> It would be useful to see the effect of using Eqs. (14-15) instead of the originally proposed objective in Eq. (12). In other words, the practical effect of the GFlowNet connection (and associated improvements) is not clear from the results.\n\n\nTo better show the effect, we implement an algorithm based on Eq. 12 rather than Eq. 15.  We keep the same training architecture, hyperparameters, and evaluation protocol for a fair comparison. This includes the use of the technique in Eq. 16 to incorporate intermediate signals. In the manywell task, this algorithm achieves a log normalizing factor estimation bias of 1.082 \u00b1 0.061 compared to 0.904\u00b10.067 of DGFS. We remark that this is also an competitive result compared to other baseline methods. We will add this result to the final version of this paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5779/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700187029325,
                "cdate": 1700187029325,
                "tmdate": 1700187088131,
                "mdate": 1700187088131,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pukvPkvv8T",
                "forum": "OIsahq1UYC",
                "replyto": "xXdH35OA4K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5779/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5779/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors, 2nd part"
                    },
                    "comment": {
                        "value": "> Clarity: Sections 2 and 3 are difficult to parse in places. The GFlowNet description in Section 2.2 is extremely dense. The separation of \"learning from intermediate steps\" vs. \"learning with incomplete trajectories\" in Section 3 is confusing. More generally, Section 3 could be re-structured to separate proposed methods, existing methods, and discussion more clearly. Section 4 is dense, with a lot of the references already introduced earlier in the paper.\n\nThank you for your suggestions regarding the structure of our writing. We recognize that our introduction of GFlowNet in Section 2.2 is quite dense, a consequence of space constraints; we plan to include a more comprehensive version in the final version of this paper. Regarding the terms 'intermediate steps' and 'learning with incomplete trajectories', we understand the need for greater clarity. 'Intermediate steps' refers to the application of Eq. 16, which sets our method apart from other diffusion-based sampling methods that only derive signals from the terminal step reward. 'Learning with incomplete trajectories' denotes DGFS's ability to update parameters without needing the full specification of complete trajectories. In the final manuscript, we will refine our explanations to more clearly delineate these concepts. Additionally, we intend to expand the related work section to offer a deeper discussion on how our work relates to and differs from previous studies.\n\n\n> At the end of page 5 authors say that the method does not \"require the training sample to follow any particular distribution (only to have full support)\". Have authors measured the effect of choosing the sampling policy? Is it worthwhile to try to explore intelligently?\n\n\nThank you for bringing up this point. Here we perform an intial trial.\nDuring the initial writing of this work, we plan to systematically study the effect of the off-policy ability. However during the deadline rush, as stated in Appendix C.7, we do not have enough time for this and thus we only keep a case study in Appendix C.7. During the rebuttal period, we perform an initial study on this. We linearly decay the $\\sigma$ coefficient in DGFS from 1.1 to 1 during the first 1000 training steps, and denote this strategy as DGFS+. We present the results here.\n\n|       | MoG         | Funnel      | Manywell    | VAE         | Cox         |\n|-------|-------------|-------------|-------------|-------------|-------------|\n| DGFS  | 0.019\u00b10.008 | 0.274\u00b10.014 | 0.904\u00b10.067 | 0.180\u00b10.083 | 8.974\u00b11.169 |\n| DGFS+ | 0.012\u00b10.002 | 0.322 \u00b1 0.229 | 0.932\u00b10.123 | 0.162\u00b10.015 | 10.23\u00b11.237 |\n\nAs can be seen from this table, this exploration strategy can bring improvement under some scenarios. This indicates that off-policy can be introduced as a hyperparameter to help the sampling. We see this as a promising direction to further improve the performance of our methods in future. We have updated our writing to incorporate this study with details in the Appendix.\n\n\n\nWe are thankful for your expert review and would welcome any more advice you could provide to help us improve our work further."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5779/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700187050736,
                "cdate": 1700187050736,
                "tmdate": 1700187050736,
                "mdate": 1700187050736,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eJlPAcX4vZ",
                "forum": "OIsahq1UYC",
                "replyto": "PsSFXQ7DFT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5779/Reviewer_Tb6D"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5779/Reviewer_Tb6D"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their response. I have found the comparison to prior work to not be very precise still. Some concrete clarifying questions below.\n\n> DGFS extend on [2] and propose more scalable algorithm that obtain better performance in challenging benchmarks.\n\nCould you clarify what \"more scalable\" means in this context? Why is DGFS more scalable? What evidence do authors have for the \"better performance\" claim: have they compared DGFS to the methods of Lahlou et al.?\n\n> [1] conducts experiments in the generative modeling setup with given datasets, training diffusion models using GFlowNet-inspired consistency-based methods, but it does not experiment with sampling given unnormalized densities like [2] and this work. \n\nHow do these \"GFlowNet-inspired consistency-based methods\" differ from DGFS? Are these methods not applicable to sampling, or have Zhang et al. simply not evaluated them on sampling?"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5779/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700480230339,
                "cdate": 1700480230339,
                "tmdate": 1700480230339,
                "mdate": 1700480230339,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hj68UBIlLR",
                "forum": "OIsahq1UYC",
                "replyto": "uV9xen0bbH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5779/Reviewer_Tb6D"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5779/Reviewer_Tb6D"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for further clarifications and additional results. Several of my points have been addressed, hence I increase my score. I encourage the authors to include the additional comparisons, extended discussion of prior work and ablation studies in the final version of the paper, to make the contribution and its context crystal clear."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5779/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700575544986,
                "cdate": 1700575544986,
                "tmdate": 1700575544986,
                "mdate": 1700575544986,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fc6bfynelU",
            "forum": "OIsahq1UYC",
            "replyto": "OIsahq1UYC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5779/Reviewer_oYBq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5779/Reviewer_oYBq"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, a novel training strategy for diffusion-based samplers is proposed, where the sampler is trained with learning signals for incomplete trajectories. Such a signal is amortized with a novel flow function. Training the sampler with these intermediate learning signals results in reduced variance with respect to similar modes trained only on full trajectories, and shows improved results on a wide set of benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The paper combines recent improvements to the credit assignment problem for partial trajectories in GFlowNets with recent diffusion-based sampler methods. The introduction of the amortized flow function is novel and well-supported with both theoretical and empirical results. The experimental section presents strong empirical results, as well as insightful analysis of the gradient variance and learned drift function which confirm the claims made by the authors. Overall, the proposed method presents strong performance on challenging tasks, making it a potentially high-impact contribution to the scientific community."
                },
                "weaknesses": {
                    "value": "In my opinion, the paper could improve in terms of clarity and in separating the previous methods from the proposed one. The paper is heavily based on previous contributions, such as denoising diffusion sampler (DDS) and path integral sampler (PIS), as well as recent contributions in GFlowNets like [1,2]. While the authors provide brief explanations of these previous methods, I think it would be beneficial to first give a clearer introduction of what such methods do (especially for DDS and PIS), and then clearly outline how the ideas are combined in DGFS.\n\nThe off-policy training strategy is mentioned but could also benefit from additional explanations and perhaps an ablation study, for example, the empirical benefits of training DGFS off-policy vs on-policy. From the text, it is not immediately clear whether DGFS can only be trained off-policy, or if it has the possibility to be trained off-policy as opposed to on-policy. \n\n[1] Madan, Kanika, et al. \"Learning GFlowNets from partial episodes for improved convergence and stability.\" International Conference on Machine Learning. PMLR, 2023.\n\n[2] Pan, Ling, et al. \"Better training of gflownets with local credit and incomplete trajectories.\" arXiv preprint arXiv:2302.01687 (2023)."
                },
                "questions": {
                    "value": "In the experiments, there is no comparison with GFlowNets methods. Is that because GFlowNets perform poorly on continuous sampling benchmarks? And on the other hand, can DGFS be used on discrete data space? And if so, how does it perform?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5779/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5779/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5779/Reviewer_oYBq"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5779/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698770468747,
            "cdate": 1698770468747,
            "tmdate": 1699636607483,
            "mdate": 1699636607483,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4qXTeIeqCN",
                "forum": "OIsahq1UYC",
                "replyto": "fc6bfynelU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5779/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5779/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful review. We have updated the writing of our submission and we hope the following will answer your points. \n\n\n> While the authors provide brief explanations of these previous methods, I think it would be beneficial to first give a clearer introduction of what such methods do (especially for DDS and PIS), and then clearly outline how the ideas are combined in DGFS.\n\nThank you for the suggestion. We agree that our method is indeed based on previous works, which is why we provide an introduction about PIS \\& DDS in Section 2.1 and an introduction about GFlowNets in Section 2.2. We acknowledge that this might be limited and not clear enough for the rationale, and we will improve related writing by adding more introductory sections in both the main text and the Appendix in the final version.\n\n\n> The off-policy training strategy is mentioned but could also benefit from additional explanations and perhaps an ablation study, for example, the empirical benefits of training DGFS off-policy vs on-policy. From the text, it is not immediately clear whether DGFS can only be trained off-policy, or if it has the possibility to be trained off-policy as opposed to on-policy.\n\nThank you for bringing up this point. During the initial writing of this work, we plan to systematically study the effect of the off-policy ability. However during the deadline rush, as stated in Appendix C.7, we do not have enough time for this and thus we only keep a case study in Appendix C.7. We have updated the writing to remove related paragraphs from the main text to ensure the text not being misleading. \n\nDuring the rebuttal period, we perform an initial study on this. We linearly decay the $\\sigma$ coefficient in DGFS from 1.1 to 1 during the first 1000 training steps, and denote this strategy as DGFS+. We present the results here.\n\n|       | MoG         | Funnel      | Manywell    | VAE         | Cox         |\n|-------|-------------|-------------|-------------|-------------|-------------|\n| DGFS  | 0.019\u00b10.008 | 0.274\u00b10.014 | 0.904\u00b10.067 | 0.180\u00b10.083 | 8.974\u00b11.169 |\n| DGFS+ | 0.012\u00b10.002 | 0.322 \u00b1 0.229 | 0.932\u00b10.123 | 0.162\u00b10.015 | 10.23\u00b11.237 |\n\nAs can be seen from this table, this exploration strategy can bring improvement under some scenarios. This indicates that off-policy can be introduced as a hyperparameter to help the sampling. We see this as a promising direction to further improve the performance of our methods in future. We have updated our writing to incorporate this study with details in the Appendix.\n\n \n\n> In the experiments, there is no comparison with GFlowNets methods. Is that because GFlowNets perform poorly on continuous sampling benchmarks? And on the other hand, can DGFS be used on discrete data space? And if so, how does it perform?\n\nPrevious work [1] has done initial experiment with trajectory balance algorithm on relatively simple continuous sampling tasks to support the validity of continuous GFlowNet theory. The performance in high dimensional tasks of the algorithm is not competitive against the baselines used in this work, as the reviewer supposes. During rebuttal period, we conduct experiment to systematically test the algorithm in [1] and show the results here.\n\n|       | MoG         | Funnel      | Manywell    | VAE         | Cox         |\n|-------|-------------|-------------|-------------|-------------|-------------|\n|  [1]     | 1.891 \u00b1 0.042    | 0.398\u00b10.061     | 3.669 \u00b1 0.653    | 4.563 \u00b1 1.029     | 2728. \u00b1 51.54      |\n|  [1] w/ off policy | 0.024 \u00b1 0.189    |  0.373 \u00b10.020 | 6.257 \u00b1 2.449    | 2.529 \u00b1 0.737     | 2722. \u00b1 31.64      |\n\n\nAll the evaluation protocols and hyperparameter setups are keep consistent with the setup in this work. We can see that the performance of both the variants are far from comparable to the baselines in this work. \n\n\n\nAs for discrete data space, we refer to a related previous work [2], where the author use a similar modeling to this work to train a GFlowNet for sampling in a categorical discrete data space. [2] designs a Markovian decision process to sequentially generate categorical data, and use a deep neural network to model the policy that navigates in this decision process. The model can be trained with signals from the reward function (which is parametrized as an energy function in that paper) and is a valid inference machine that can sample with probability proportional to this reward function if the training is complete.\n\n\n[1] A theory of continuous generative flow networks\n\n[2] Generative Flow Neteworks for Discrete Probabilistic Modeling\n\n\nWe thank you for your review and we are grateful for any additional insights you might have for our paper."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5779/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700186927804,
                "cdate": 1700186927804,
                "tmdate": 1700186927804,
                "mdate": 1700186927804,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oiWTrRMJVb",
                "forum": "OIsahq1UYC",
                "replyto": "4qXTeIeqCN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5779/Reviewer_oYBq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5779/Reviewer_oYBq"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for answering my questions and including clarifications to the manuscripts. I confirm my previous score."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5779/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587045158,
                "cdate": 1700587045158,
                "tmdate": 1700587045158,
                "mdate": 1700587045158,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BGz7rQVYvE",
            "forum": "OIsahq1UYC",
            "replyto": "OIsahq1UYC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5779/Reviewer_1u4d"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5779/Reviewer_1u4d"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an algorithm for better sampling from intractable high-dimensional density functions. The sampling procedure is formulated as a time sequence. The improvement then stems from the usage of partial trajectories instead of merely relying on the end-time variable. Empirical results show the superior performance of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I haven't done any research in the sampling area, nor do I have a strong background of the sampling methods and their advantages and disadvantages. However, from reading the paper, I have gained an understanding of the problem to be solved in the paper. Moreover, the methodology makes sense to me. That being said, the paper is well written with a clear logic flow, even for people who is new to the field.\n\nThe experiments cover a wide range of data, including high-dimensional settings. The reported results look very promising and indicating the strength of the proposed method."
                },
                "weaknesses": {
                    "value": "I am not very confident in evaluating the novelty of the proposed method. From the introduction, it seems like a combination of the existing ideas in constructing a forward Markov chain and exploitation of the detailed balance in GFlowNets. I would appreciate a discussion regarding the novelty.\n\nWhat is the running time comparison to existing baseline methods?\n\nThe reported results in table 1 uses a different metric for baseline methods. I am curious how does the proposed method compares to baseline methods using the results at the best checkpoint?"
                },
                "questions": {
                    "value": "See weakness above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5779/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5779/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5779/Reviewer_1u4d"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5779/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698802028998,
            "cdate": 1698802028998,
            "tmdate": 1700697888187,
            "mdate": 1700697888187,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fDqPXBXyXV",
                "forum": "OIsahq1UYC",
                "replyto": "BGz7rQVYvE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5779/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5779/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful review. We have updated the writing of our submission and we hope the following will answer your points. \n\n> From the introduction, it seems like a combination of the existing ideas in constructing a forward Markov chain and exploitation of the detailed balance in GFlowNets. I would appreciate a discussion regarding the novelty.\n \nWe appreciate the reviewer's interest in the novelty of our approach. Our work is indeed based on prior GFlowNet foundations  and previous diffusion-based sampling methods. Though the concept of detailed balance is not new, our work is the first to achieve learning without full trajectory specification in the field of continuous sampling from given unnormalized densities with diffusion. This could potentially inspire future sampling research with similar ideas. Furthermore, we propose the use of an intermediate signal (Eq. 16) which is novel and specifically designed for the chosen SDE-based modeling. This largely enhances learning efficiency compared to other diffusion-based sampling methods, which rely solely on terminal signals. Last but not least, our proposed algorithm achieves state-of-the-art performance on challenging sampling benchmarks, demonstrating the significance of our method. In summary, while DGFS integrates existing concepts, its unique contributions lie in its novel training approach, its flexibility in accommodating different formulations, and its solid theoretical backing, all of which mark a significant advancement in the domain of diffusion-based sampling methods.\n\n> What is the running time comparison to existing baseline methods?\n\nWe conduct an evaluation to compare the running times of all diffusion modeling methods. On the manywell task, the training time per batch (with a batch size of 256) is 3.24 seconds for PIS, 3.34 seconds for DDS, and 3.74 seconds for DGFS; the inference time for generating a batch (with a batch size of 2000) is 1.52 seconds for PIS, 1.48 seconds for DDS, and 1.50 seconds for DGFS. All tests were performed on a single NVIDIA RTX 8000 GPU. This comparison provides clear insights into the comparable speed of our proposed methods relative to the existing baselines while having better performance.\n\n\n> The reported results in table 1 uses a different metric for baseline methods. I am curious how does the proposed method compares to baseline methods using the results at the best checkpoint?\n\nWe conduct experiments on the manywell task. We test every a hundred training step and report the best value for each algorithm. The log normalizing factor estimation bias under this protocol is 22.36 for SMC, 2.079 for VI-NF, 0.055 for CRAFT, 0.088 for PIS, 0.129 for DDS, and 0.058 for DGFS. Notice that there SMC result stays the same as the table, as there is no training involved in it. We remark that the best checkpoint during the training is an unstable metric, as the algorithm's estimate can ocsillate over the course of learning due to randomness. If \"good luck\" occurs, then a poorly performing algorithm may still obtain an estimate close to the ground truth log normalizing factor at some point during training. Therefore, we use a more stable way in the paper as stated in Section 5.1.\n\n\nWe sincerely thank you for your feedback and remain open to any additional suggestions that could further improve our work."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5779/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700186322980,
                "cdate": 1700186322980,
                "tmdate": 1700187452486,
                "mdate": 1700187452486,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KCLnUBydQt",
                "forum": "OIsahq1UYC",
                "replyto": "fDqPXBXyXV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5779/Reviewer_1u4d"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5779/Reviewer_1u4d"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response and I am willing to keep the positive review and raise my confidence."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5779/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700697855498,
                "cdate": 1700697855498,
                "tmdate": 1700697855498,
                "mdate": 1700697855498,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]