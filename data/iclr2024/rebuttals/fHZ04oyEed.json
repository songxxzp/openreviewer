[
    {
        "title": "Representation Learning from Interventional Data"
    },
    {
        "review": {
            "id": "n1S83NqA4y",
            "forum": "fHZ04oyEed",
            "replyto": "fHZ04oyEed",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3823/Reviewer_WWQo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3823/Reviewer_WWQo"
            ],
            "content": {
                "summary": {
                    "value": "This paper first introduces a problem where the data is a mixture of observational and interventional data. The authors proposed a method that can limit the dependency between variables for interventional data based on NHSIC measure. Experiments demonstrate superior performance against current methods on synthetic datasets and other tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Many case studies and examples are given to motivate the problem, and the results are clearly presented. The RepLin method seems to be a novel use of the NHSIC dependence measure. There are a lot of experiments spanning a variety of tasks including robustness to image corruption, which is very interesting. All experimental results demonstrated superior performance against benchmarks."
                },
                "weaknesses": {
                    "value": "It seems that RepLin algorithm assumes knowledge of which datapoint is interventional, whereas the ERM and other methods do not use this assumption. ERM-Resampled uses that knowledge, but not as directly as RepLin. However, experimental comparisons with ERM-Resampled is absent. In Section 3, it is unclear what the author(s) are trying to discuss, especially the phrase \u201cit is advisable to remove spurious information from the representations entirely\u201d. The title of the paper is about representation learning, but the paper evaluates the quality of the representation only via a single downstream task, making the problem more like supervised learning."
                },
                "questions": {
                    "value": "- What is the test dataset for the problem in Figure 3? If in Figure 3(b) the test is on the full support, then it is a significantly harder problem then Figure 3(a), making the motivation unclear.\n- Why is there no comparison between RepLin and ERM-Resampled?\n- Does the method work when there are more than two features that directly affect X?  What about when the intervention is on two variables?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3823/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3823/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3823/Reviewer_WWQo"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3823/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698679965247,
            "cdate": 1698679965247,
            "tmdate": 1699636339928,
            "mdate": 1699636339928,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TFSZuv9SCd",
                "forum": "fHZ04oyEed",
                "replyto": "n1S83NqA4y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3823/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3823/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Clarity on the method, results and terminology + additional results"
                    },
                    "comment": {
                        "value": "**Comparison to ERM-Resampled:** Please refer to Table T1 for the comparison between RepLIn and ERM-Resampled.\n\n**Use of resampling:** We observed that resampling improves robustness in general. A comparison between our method without resampling and other methods can be found in Table T1. Additionally, resampling will help our method obtain better gradients from dependence-related losses as the estimate of the dependence improves with the number of samples.\n\n**Clarity for Section 3:** The phrase from Section 3 that the reviewer found unclear was intended to argue about the danger of leaving spurious information in the representation. In a recent line of work, it was proposed to fine-tune the classifier to achieve robustness after observing that the learned representation contained both invariant and spurious features [R12-15]. However, in the absence of a fine-tuning dataset that exactly matches the test distribution, some spurious correlations may still slip through. We demonstrate this in a case where the support of the interventional distribution does not match that of the test distribution (Sections 3 and 4.3 in the main paper).\n\n**Terminology in the title:** The larger goal of our work is to learn useful representations from interventional data. The term ``representation learning\" is not limited to unsupervised/self-supervised learning. For example, representations learned from ImageNet classification have proved useful in several downstream applications.\n\n**Test dataset in Fig. 3:** In Fig. 3, $A$ and $B$ are binary variables. During observations, $B\\coloneqq A$ and during interventions, $B$ is set to values independent of $A$. The training data consists of both observational and interventional data, so that the model may learn the true invariant features. Since we are interested in evaluating the robustness of our models to interventional distributional shifts, our test set is built entirely from interventional data.\n\n**Multi-variate case:** To demonstrate the advantage of our method in the multi-variate case, we construct a causal graph with five binary variables. The latents are generated according to the following SCM.\n\n$$\nA = \\text{Bern}(0.4) \\\\\n$$\n$$\nB = \\text{Bern}(0.6) \\\\\n$$\n$$\nC = A \\vee B \\\\\n$$\n$$\nD = A \\wedge C \\\\\n$$\n$$\nE = \\neg B \\wedge C\n$$\n\nwhere $\\text{Bern}(p)$ indicates a Bernoulli distribution parameterized by $p$. The observed data $X$ is obtained as,\n\n$$\nX\\_A = \\text{NN}\\_{4,3}(A)\n$$\n$$\nX\\_B = \\text{NN}\\_{4,3}(B)\n$$\n$$\nX\\_C = \\text{NN}\\_{2,0.1}(C)\n$$\n$$\nX\\_D = \\text{NN}\\_{2,0.1}(D)\n$$\n$$\nX\\_E = \\text{NN}\\_{2,0.1}(E)\n$$\n$$\nX = \\text{concat}\\left(X\\_A, X\\_B, X\\_C, X\\_D, X\\_E\\right)\n$$\n\n*where* $\\text{NN}_{l,\\sigma}$ indicates a randomly initialized neural network with $l$ hidden layers, each with 100 units, and an added noise layer which adds a Gaussian noise with zero mean and $\\sigma$ standard deviation. Each latent variable adds information about itself to the observed signal. Our intuition here is that, due to more hidden layers and larger added noise, the model may struggle to use the invariant information from $X$ and instead learn spurious features.\n\nTo collect interventional data, we intervene on $C$, $D$, and $E$ separately. Our evaluation metrics, therefore, will be the accuracy of the model in predicting $A$ and $B$ during these interventions. We compare ERM-Resampled and RepLIn-Resampled on their predictive accuracy during intervention on $C$, $D$, and $E$ in Tables T2, T3, and T4."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3823/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700158558993,
                "cdate": 1700158558993,
                "tmdate": 1700158558993,
                "mdate": 1700158558993,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "81C15RB3WJ",
            "forum": "fHZ04oyEed",
            "replyto": "fHZ04oyEed",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3823/Reviewer_XFMF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3823/Reviewer_XFMF"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers the problem of representation learning under distribution shift. Specifically, the paper considers the setting where the data generating process is described by a causal DAG, and the learner is able to observe both observational data and data generated by a specific, known intervention. The paper shows that on a toy example, both ERM and a resampled-version of ERM fail to generalize when given a large amount of observational data but only a small amount of interventional data. The paper uses this example to motivate their representation learning approach which, in addition to minimizing predictive error, also seeks to minimize a statistical dependency measure between features that are known to be independent on the observational data. The paper then goes on to compare their method against ERM and the resampled version of ERM on a synthetic toy dataset and an image classification dataset where smiling and gender have been subsampled to induce dependence. The paper also compares against ERM and fine-tuning on an ImageNet subset where noise is added to correlate with certain labels. On all experiments, the paper shows that their method leads to improvement over the baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper considers an important area of study: learning under distribution shift. The paper distills the particular type of distribution shift that they interested in into an easily understood model, and illustrates the issue with common heuristics in addressing this problem on a toy example. Finally, the paper considers both toy and realistic data in their experiments."
                },
                "weaknesses": {
                    "value": "The paper has a few weaknesses that should be addressed.\n\n1. It is unclear exactly where the dependency and self-dependency losses come from. In particular, they seem to be only heuristically motivated, and it's unclear what minimizers of this loss should look like. Why should the \"self-dependence loss\" prevent the model from learning only irrelevant features, and how do we know that this is the right loss to do so? A rigorous mathematical treatment of the proposed loss would be very helpful here.\n\n2. The paper only considers ERM-based baselines. The field of domain generalization is currently rich with approaches to problems that subsume the current work, but this paper does not compare against any of those. E.g., invariant risk minimization is one approach to domain generalization that would seem to easily apply to the present problem.\n\n3. The paper doesn't really motivate the problem setting. It's not clear to me when we would be in a setting where (a) we know the true causal DAG, (b) we get to observe interventional data, and (c) we want to learn representations of our data for some downstream purpose, e.g. prediction in this setting."
                },
                "questions": {
                    "value": "Building on the weaknesses I pointed out above:\n\n1. Can you show that minimizing the RepLin loss leads to provably optimal properties for the minimizing function?\n\n2. Can you justify why you did not compare against methods from domain generalization?\n\n3. Can you present a concrete setting that motivates the paper setting?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3823/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3823/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3823/Reviewer_XFMF"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3823/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698696432108,
            "cdate": 1698696432108,
            "tmdate": 1699636339846,
            "mdate": 1699636339846,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QrQ1SlpKfJ",
                "forum": "fHZ04oyEed",
                "replyto": "81C15RB3WJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3823/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3823/Authors"
                ],
                "content": {
                    "title": {
                        "value": "About loss functions"
                    },
                    "comment": {
                        "value": "**Common responses:** The relevance and applications of our problem setting can be found in common response A. We discuss the difference between our goal and that in domain generalization in common response B.\n\n**Origin of dependence loss function:** Our dependence loss is motivated by the observed correlation between the drop in accuracy and feature dependence during interventions. Since dependence between learned features is commonly measured and adjusted using kernel-based measures of dependence such as HSIC and KCC, we use the same to design our loss function.\n\n**Origin of self-dependence loss function:** A caveat in learning independent features is that the presence of irrelevant/degenerate information in the representation can minimize the dependence loss while maintaining strong performance. To prevent this, we use our self-dependency loss which penalizes the representations for not fully aligned with its corresponding attribute of interest."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3823/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700157823168,
                "cdate": 1700157823168,
                "tmdate": 1700157823168,
                "mdate": 1700157823168,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "r8bX51DVFN",
                "forum": "fHZ04oyEed",
                "replyto": "QrQ1SlpKfJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3823/Reviewer_XFMF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3823/Reviewer_XFMF"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you to the authors for the detailed responses to my concerns. I still have a couple of reservations about the paper in its present form, however.\n\nFirst, a common concern among the reviewers, myself included, is that the assumptions of the paper seem overly restrictive. In particular, perfect knowledge of the causal graph and perfect interventions are often not possible in interesting settings. For example, in the mRNA expression example given in the author response, the full biological causal graph may not be known ahead of time and the interventions are often noisy. I think the paper could be improved by demonstrating some robustness of their method to causal graphical misspecification.\n\nSecond, I'm still not sure that I totally believe that minimizing the loss functions leads to the optimal causal representation. Is it possible for the authors to construct even a toy example where minimizing these loss functions in the high sample limit provably   leads to the optimal causal representation? Also, the appendix points out that there is a \"warm-up\" phase in the learning process where one of the regularization parameters is set to 0 and then gradually increased to its full value. Is this because the loss functions lead to unstable learning dynamics?"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3823/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576462949,
                "cdate": 1700576462949,
                "tmdate": 1700576462949,
                "mdate": 1700576462949,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mR4n7ISKHn",
            "forum": "fHZ04oyEed",
            "replyto": "fHZ04oyEed",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3823/Reviewer_Uyzd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3823/Reviewer_Uyzd"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers the problem of prediction under distribution shifts as a result of interventions on one of the label and proposes an approach inspired by the conditional independence in the intervened graph. The authors consider a setup with multiple labels with causal relationships between the labels, and known hard-interventions that act on one of the labels to remove its dependence from other relevant labels. The proposed approach enforces statistical independence between intervened labels and their parent labels in the original graph; and highlights its benefits over several baselines like reweighting, classifier fine-tuning, etc. on multiple benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "* The claims made in the paper are supported by rigorous experimentation over multiple synthetic, semi-synthetic, and realistic benchmarks. Further, the authors also compare with important baselines of reweighting, classifier fine-tuning, pre-training, etc.\n\n* The paper is well-written with good motivation for the proposed approach using synthetic benchmarks, and the presentation of the results is clear and well-organized.\n\n* The proposed approach uses loss objectives from prior work to enforce independence between features with minor modifications, though the application for the task of multi-label prediction with interventions is novel."
                },
                "weaknesses": {
                    "value": "* My major concern with the work is that the technical contribution of the work is weak since the problem setup is very simple. The authors assume they know exactly the label that has been intervened and its non-descendants, which makes the proposed approach a simple application of the d-separation criteria in causal graphs. In contrast, the common setups in OOD generalization involving distribution shifts [1, 2] do not assume that we observe the intervened variables (and their causes) explicitly. \n\n* I appreciate the rigorous experimentation in the work, however, the significance of the results is highly limited as the application of d-separation criteria in the intervened graph with known variables and causal relationships is supposed to work. Further, the experiments on the corrupted CIFAR and ImageNet require the knowledge of corruption labels, which would simplify the problem. \n\nReferences:\n\n[1] Martin Arjovsky, L\u00e9on Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization.\narXiv preprint arXiv:1907.02893, 2019\n\n[2] Polina Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson. Last layer re-training is sufficient\nfor robustness to spurious correlations. arXiv preprint arXiv:2204.02937, 2022"
                },
                "questions": {
                    "value": "My suggestion to the authors is to consider the case with unknown causal relationships and intervened nodes, as that is a more practical and challenging setup. I understand that would require substantial efforts with a completely different approach in principle to solve the problem, but I am interested in hearing the thoughts of the authors regarding this point.\n\nMinor points:\n\n* The description of the prior work Ahuja et al. (2022) in the learning using interventional data section is incorrect. Their work is not limited to independent latent factors, rather they allowed for general causal relationships between the latents."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3823/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3823/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3823/Reviewer_Uyzd"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3823/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698793222902,
            "cdate": 1698793222902,
            "tmdate": 1699636339755,
            "mdate": 1699636339755,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "N2oemTGYjJ",
                "forum": "fHZ04oyEed",
                "replyto": "mR4n7ISKHn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3823/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3823/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Approach and Related Works"
                    },
                    "comment": {
                        "value": "**Common responses:** The existing works on OOD generalization are designed for general distribution shifts. However, we are interested in a specific form of distribution shift induced by causal interventions. Please refer to common response B, where we describe how our goals and approach differ from standard invariant learning.\n\n**Our approach and d-separation:** d-separation only tells us what the true dependence relations in the ideal causal model are. It does not translate to learning systems where no checks are employed during training to enforce true causal relations. One of our contributions is the observation that d-separation is not inherently followed by these models. Our method was driven by the observed correlation between the drop in accuracy and dependence between the features during interventions, as discussed in Section 2.1 of our paper. This observation is non-trivial and has not been previously discussed in the literature on learning from interventional data, although the question of whether learned models respect causal relationships has been asked [R21-23]. \n\n**Knowledge of corruption labels:** Robustness to label-dependent corruption is one of the several applications of our work. As we mentioned in common response A, we may often have this information. In practice, it is possible to infer the type of image corruption [R11].\n\n**Correction in related works:** We apologize for the error in the description of Ahuja et al. (2022). Their paper indeed says it \u201cdoes not require any structural assumptions about the dependency between the latents\u201d. We have made this correction in the newly added section that contrasts our setting with that of identifiable causal representation learning."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3823/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700157792218,
                "cdate": 1700157792218,
                "tmdate": 1700157792218,
                "mdate": 1700157792218,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "G9Rp4lIKKB",
                "forum": "fHZ04oyEed",
                "replyto": "mR4n7ISKHn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3823/Reviewer_Uyzd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3823/Reviewer_Uyzd"
                ],
                "content": {
                    "comment": {
                        "value": "**Common Response B**\n\nRegarding the common response B, I'm afraid I have to disagree that datasets like PACS are not a result of intervention. Changes in the style of the image (photo, sketch, etc.) can be attributed to intervening on the latent variables that control to the style variations in the image. The goal of domain generalization methods is more difficult since they do not observe these latent variables and the causal graph for the data generation process, but the proposed approach in this work relies heavily on them. I do understand the difference between the proposed setup and the domain generalization setup, but the former does not seem challenging due to the reasons I mentioned before. \n\n**Comparison with related works from Causal Representation Learning**\n\nAfter reading the common response A, perhaps a case could be made that assuming the knowledge of intervened variables can be practical in some cases. But then a proper comparison needs to be made with other approaches from causal representation learning that can do so. For example, some works enforce support independence via Hausdorff distance [1, 2] between latent representations, which is a richer penalty than enforcing statistical independence as it can handle the case of correlation due to unobserved confounders as well. The proposed setup of authors can be been similar to the work of Ahuja et al. [2], the only difference being that the authors consider a supervised setup with a classification loss; while the setup of Ahuja et al. is unsupervised and they have a reconstruction objective. However, the access to interventional data is similar and the penalty of enforcing support distance in Ahuja et al. can be used by the authors in the proposed setup as well.\n\nI do agree with the authors when they say: \"The general objective in identifiable causal representation learning is to \u201clearn both the true joint distribution over both observed and latent variables. The objective of this work is to provide a method to learn representations that are robust to interventional distribution shifts under the assumption of known interventional targets and their parents.\"   \n\nHowever, there are rich ideas in the causal representation learning about enforcing (statistical/support) independence, etc, which can serve as competitive baselines in the proposed setup. \n\nI do appreciate the effort made by the authors but at this time the submission indeed requires more work and unfortunately I would not be able to increase my score. In the current form, the authors have technically sound results but I think the overall contribution is limited. I encourage authors to incorporate my feedback about causal rep learning baselines since I think it can make a strong case for downstream applications of the various causal rep learning works that have mostly dealt with identification and small-scale experiments for now. \n\nReferences\n\n[1] Roth, Karsten, Mark Ibrahim, Zeynep Akata, Pascal Vincent, and Diane Bouchacourt. \"Disentanglement of correlated factors via hausdorff factorized support.\" arXiv preprint arXiv:2210.07347 (2022).\n\n[2] Ahuja, Kartik, Divyat Mahajan, Yixin Wang, and Yoshua Bengio. \"Interventional causal representation learning.\" In International conference on machine learning, pp. 372-407. PMLR, 2023."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3823/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700605508998,
                "cdate": 1700605508998,
                "tmdate": 1700605606628,
                "mdate": 1700605606628,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FPwTU83AoS",
            "forum": "fHZ04oyEed",
            "replyto": "fHZ04oyEed",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3823/Reviewer_Pdav"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3823/Reviewer_Pdav"
            ],
            "content": {
                "summary": {
                    "value": "In this work the authors propose the RepLIn method for representation learning that aims to be more robust under distribution shift obtained via a perfect intervention on a single feature. For a model that has a causal data generating process, standard techniques like ERM propose to learn the generative process and the recent field of causal representation learning has brought along an array of sophisticated tools to handle this. In general, when the data generating process is modified by a hard intervention on a causal variable, this brings a distribution shift and the learned representations are not robust. The authors propose to modify the loss function of representation learning methods using their regularization term to handle such issues.\n\nInitial experiments on the windmill datasets (with 2 causal variables) suggest that there is a correlation between accuracy drop on interventional data and independence of the features corresponding to the intervened variable. This situation is somewhat mitigated by using additional interventional data. However, the authors propose a different fix which is to define a modified regularization term that aims to minimize the dependence between features on interventional data. To avoid pathological feature learning, an additional regularization term is added. The authors call this the RepLIn method.\n\nThey validate their observation of correlation on the windmill dataset. Experiments on CelebA dataset (using the causal variables smile and gender) show that RepLIn is more robust than the baseline algorithms by 2%. Additional experiments on 3-variable causal data generating process is also shown on label prediction. These experiments weakly validate the method and suggest that spurious correlations are mitigated.\n\n### References:\n\n- [1] Interventional causal representation learning.\n\n- [2] Learning Linear Causal Representations from Interventions under General Nonlinear Mixing"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The statistical correlation between drop in representation learning accuracy and independence of the causal covariates is a nice observation. Retrospectively, it's not surprising considering the data generating process but the authors suggest to exploit this via explicit regularization.\n\n- Experiments show that the proposed method RepLIn can beat methods such as classifier fine-tuning to handle distribution shift on various image datasets."
                },
                "weaknesses": {
                    "value": "- The kind of distribution shift considered, i.e. hard perfect interventions on causal features, maybe too restrictive for real-life applications. In fact, IRM aims to be as general as possible, unfortunately at the overhead of significant additional complexity. Therefore, this model loses this generality.\n\n- Related to the above, even if we have weak dependence among the causal variables after intervention, the proposed regularization method will still be feasilbe to apply, but it's not clear how much the hyperparameter tuning will be needed in the experiments. Could the authors comment on this?\n\n- The authors should have a detailed discussion on the array of 10+ works on causal representation learning from interventional data, e.g. [1], [2] (see also related works section in [2] which contains a wide list of such works). I'm surprised the authors seem unaware of this very closely related line of work.\n\n- Related to the above point, while the authors compare to IRM in app. F and highlight the differences, I feel like the works on causal representation learning are more apt to contrast with, due to various modeling assumptions."
                },
                "questions": {
                    "value": "Some questions were raised above.\n\n- The authors consider perfect interventions. How robust are their methods to imperfect or soft interventions (with the usual meanings from the causal literature)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3823/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3823/Reviewer_Pdav",
                        "ICLR.cc/2024/Conference/Submission3823/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3823/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698804739324,
            "cdate": 1698804739324,
            "tmdate": 1700543191429,
            "mdate": 1700543191429,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "56Rspx6URF",
                "forum": "fHZ04oyEed",
                "replyto": "FPwTU83AoS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3823/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3823/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Assumptions and Relevance of our Problem Setting"
                    },
                    "comment": {
                        "value": "**Common responses:** We justify the relevance and practicality of our setting in common response A. We apologize for the confusion due to the term \"causal representation learning,\u201d which is more commonly used in the identifiable learning community. Our response to your questions about the missing related works from identifiable causal representation learning can be found in common response C.\n\n**About assumptions:** The primary objective of our work is to develop a method that can efficiently exploit the limited amounts of interventional data that may be available for training. To this end, we make a few assumptions. However, our assumptions are justified given that our work is the first to explicitly exploit interventions to achieve robustness to interventional distribution shift.\n\n**Imperfect/soft interventions:** We developed our method under the assumption of perfect interventions. Constraining the dependence between intervened variables can be posed as a constrained optimization problem for which closed-form solutions may be obtained. For example, [R10] models the allowed sensitive information leakage as a constraint and obtains a closed-form solution. Nonetheless, we believe minimizing dependence between the interventional features can improve robustness against such distribution shifts."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3823/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700157737516,
                "cdate": 1700157737516,
                "tmdate": 1700157737516,
                "mdate": 1700157737516,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nbcHhPx1iX",
                "forum": "fHZ04oyEed",
                "replyto": "56Rspx6URF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3823/Reviewer_Pdav"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3823/Reviewer_Pdav"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their response. Rereading the work in light of the other reviews, I also agree with the other reviewers that the contributions are very limited (given the strong assumptions made) and the writing needs to be significantly improved."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3823/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700543538751,
                "cdate": 1700543538751,
                "tmdate": 1700543538751,
                "mdate": 1700543538751,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]